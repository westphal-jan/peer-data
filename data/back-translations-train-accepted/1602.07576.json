{"id": "1602.07576", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Group Equivariant Convolutional Networks", "abstract": "We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. By convolving over groups larger than the translation group, G-CNNs build representations that are equivariant to these groups, which makes it possible to greatly increase the degree of parameter sharing. We show how G-CNNs can be implemented with negligible computational overhead for discrete groups such as the group of translations, reflections and rotations by multiples of 90 degrees. G-CNNs achieve state of the art results on rotated MNIST and significantly improve over a competitive baseline on augmented and non-augmented CIFAR-10.", "histories": [["v1", "Wed, 24 Feb 2016 16:17:15 GMT  (105kb)", "http://arxiv.org/abs/1602.07576v1", null], ["v2", "Fri, 11 Mar 2016 18:26:26 GMT  (106kb)", "http://arxiv.org/abs/1602.07576v2", null], ["v3", "Fri, 3 Jun 2016 10:54:16 GMT  (454kb)", "http://arxiv.org/abs/1602.07576v3", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["taco cohen", "max welling"], "accepted": true, "id": "1602.07576"}, "pdf": {"name": "1602.07576.pdf", "metadata": {"source": "CRF", "title": "Group Equivariant Convolutional Networks", "authors": ["Taco S. Cohen"], "emails": ["T.S.COHEN@UVA.NL", "M.WELLING@UVA.NL"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.07 576v 1 [cs.L G] 24 Fe"}, {"heading": "1. Introduction", "text": "Although a strong theory of the neural network is currently lacking, there is a great deal of empirical evidence that both weight distribution and depth (among other factors) are important for good predictive power. Conventional weight distribution is effective because there is a translation symmetry in most educational problems: the label function and data distribution are both roughly invariant in terms of displacement. By using the same weights to analyze or model each part of the image, far fewer parameters are used than a fully interconnected layer, while the ability to learn many useful translations is preserved. (s) Convolution layers can be used effectively in a deep network because all layers in such a network are equivalent: shifting the image and feeding through a series of layers is the same."}, {"heading": "2. Equivariant Representations", "text": "In the current generation of neural networks, representation spaces are usually equipped with very minimal internal structures, such as linear space Rn. Although vector addition has proven surprisingly effective in modeling semantic relationships of words (Mikolov et al., 2013), it does have inherent limitations, such as being commutative and non-periodic structures. In this paper, we construct representations that exhibit the structure of linear G space for some groups G. This means that an object or feature represented by the network represents a related pose that incorporates a canonical frame of reference into the local framework of the object or feature. This additional structure allows us to model data more efficiently: A filter in a G-CNN recognizes common occurrences features that have a preferred relative position, and can correspond to any feature."}, {"heading": "3. Related Work", "text": "Lenc & Vedaldi (2015) show that AlexNet CNN (Krizhevsky et al., 2012), which specializes in imagenet, spontaneously learns representations equivalent to flips, scaling, and rotation, supporting the idea that equivariance is a good inductive bias for deep networks; there is a large literature on learning-invariant representations that can be achieved by normalization (Lowe, 2004; Jaderberg et al., 2015) or by averting a possible nonlinear function across a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach is at least applicable to Hurwitz et al., 2015); both approaches are largely compatible with our paper, but we focus on the more general notion of equivalence that is critical in building deep networks."}, {"heading": "4. Mathematical Framework", "text": "In this section, we will define a few mathematical concepts that we will apply in defining and analyzing group equivariant Convolutionary Networks. We will begin by defining symmetry groups and examine in particular two groups used in the G-CNNs that we have built so far. Then, we will take a look at feature maps that we model as vector-like functions on a group (a standard CNN feature map is a vector value-oriented function on the group of integer translations, Z2)."}, {"heading": "4.1. Symmetry Groups", "text": "A symmetry of a (mathematical) object is a transformation that leaves that object invariant. For example, if we take the sample grid of our image, Z2, and turn it around, we get \u2212 Z2 = {(\u2212 n, \u2212 m) | (n, m) \u0445Z2} = Z2. Thus, the rotation operation is a symmetry of the sample grid. i If we have two symmetry transformations g and h and put them together, the result is another symmetry transformation (i.e., it leaves the object invariant). Moreover, the reverse transformation g \u2212 1 for each symmetry is also a symmetry. In other words, the set of all symmetries of an object is closed under composition and inverse. A series of transformations with these properties is called a symmetry group."}, {"heading": "4.2. The group p4", "text": "The group p4 consists of all compositions of ratios and rotations by 90 degrees over each center of rotation in a square grid. A convenient parameterization of this group in relation to three integer numbers r, u, v isg (r, u, v) = cos (r\u03c0 / 2) \u2212 sin (r\u03c0 / 2) u sin (r\u03c0 / 2) cos (r\u03c0 / 2) v0 1, (2) where 0 \u2264 r < 4 and (u, v) Z2. The group operation is given by matrix multiplication. The group operation and inversion can also be represented directly in relation to integers (r, u, v), but the equations are cumbersome. Therefore, our preferred method, two group elements represented by integer tuples, is the conversion into matrices, multiplication of these matrices, and then the conversion of the resulting matrices into squares."}, {"heading": "4.4. Feature maps", "text": "We model images and stacks of characteristics of a conventional CNN as functions f: Z2 \u2192 RK, which are based on a limited domain supp (f) (usually a square centered on the origin). We think of the integers (p, q) in Z2 as the coordinates of the pixels. At each sampling point, a stack of characteristics must have a finite size, in which K has the number of color channels of the input image, or the number of characteristics at higher levels. Although the arrays used to store the characteristics must always have a finite size, modeling characteristics must be functions that extend to infinity (which do not extend to a finite region). We will deal with transformations of the characteristics maps so that we introduce the following notation for a transformation of the characteristics maps, so we place them on a set of characteristics: [Lf] (x)."}, {"heading": "5. Equivariance properties of CNNs", "text": "In this section we recall the definitions of the constellation and the correlation operations used in conventional CNNs, even though these operations are equivalent for translations, but not for other transformations such as the rotation. This is certainly well known and easy to recognize through mental visualization, but the derivation from it will make it easier to follow the derivation of the group equivariance of the group equivariance, which is defined in the next sector. \u2212 At each level, a regular convent takes as input a stack of characteristic cards f: Z2 \u2192 RKl and constellations, or correlates it with a series of Kl + 1 filters. \u2212 In the series of Kl + 1 filters, the constellation of the constellation x: [f-constellation] (x) = constellation y constellation of the constellation y (x-constellation of the constellation of the constellation of the constellation), the constellation of the constellation of the constellation of the constellation of the constellation (constellation of the constellation of the constellation of the constellation of the constellation of the constellation), the constellation of the constellation of the (constellation of the constellation of the constellation of the constellation of the constellation of the constellation of the constellation of the (constellation of the constellation) and the constellation of the constellation of the (constellation of the constellation of the constellation of the constellation of the constellation of the constellation of the) constellation of the"}, {"heading": "6. Group Equivariant Networks", "text": "In this section we define the three layers used in a GCNN (G-folding, G-pooling, non-linearity) and show that each oscillates with G-transformations of the image area."}, {"heading": "6.1. G-Equivariant correlation", "text": "\"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \"f.\" f. \""}, {"heading": "6.2. Pointwise non-linearities", "text": "Equation 14 shows that the G correlation preserves the transformation properties of the previous layer. What about the nonlinearity and pooling operations commonly used in convective networks? Nonlinearity \u03bd: R \u2192 R is applied to a characteristic map f: G \u2192 R through its composition to provide the corrected characteristic map \u03bd \u0445 f; evaluated at g, we have [\u043c f] (g) = \u03bd (f (g). Since Lhf = f \u0445 h \u2212 1 and the functional composition is associative, we can see that the transformation properties of the previous layer are taken over in the reflected characteristic map."}, {"heading": "6.3. Pooling", "text": "In order to simplify the analysis, we divide the pooling process into two steps: the pooling procedure itself (without sampling) and a subsampling procedure (without sampling).The non-striped maxpooling procedure will be modelled on a feature procedure f: G \u2192 R can be modelled as an operator P, which on f asPf (g) = max k sampling procedure f (k) = max k sampling procedure f (k) = max k sampling procedure f (k) = max k sampling procedure f (k) = max k sampling procedure f (k) = max k sampling procedure sampler sampling procedure f (k) Sampling procedure f (k) Sampling procedure f (k) Sampling procedure f (k) = max k sampling procedure sampling procedure f (k) sampling procedure f (k) sampling procedure f (k) sampling procedure f (k) sampling procedure f sampling procedure f (k) sampling procedure f (k) sampling procedure f (k) sampling procedure sampling procedure sampling procedure Sampling procedure sampling procedure Sampling procedure Sampling procedure sampling procedure sampling procedure sampling procedure sampling procedure U sampling procedure (k) sampling procedure (k) Sampling procedure sampling procedure sampling procedure sampling procedure (k) Sampling procedure sampling procedure (k) Sampling procedure sampling procedure sampling procedure sampling procedure sampling procedure (k) Sampling procedure sampling procedure sampling procedure sampling procedure sampling procedure (gwo)."}, {"heading": "7. Backpropagating through G-convolutions", "text": "In order to train a group equivalent pleated network, we must calculate the course of a loss function with respect to the parameters of the filters. As usual, this is accomplished by multiplying the Jacobians of the operations calculated in each plane in right-associative order (i.e. reverse propagation). Let us have the function card k calculated at level l as f lk = f l \u2212 1, where f l \u2212 1 is the previous function card. At some point in the backprop algorithm, we will have calculated the derived card L / 6 f lk for all k, and we must calculate the total map L / 6 f l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l (g), where the superscript denotes the involution (of the parameter)."}, {"heading": "8. Efficient Implementation", "text": "Calculating a G-fold involves nothing more than indexing arithmetic and internal products, so that it can be easily implemented using a loop or as a parallel GPU kernel. However, without significant investment in code optimization, such an approach is likely to be slower than comparable planar folding, as there are now extremely efficient algorithms for Z2 convolutions (Mathieu et al., 2014; Vasilache et al., 2015; Lavin & Gray, 2015). In this section, we show how G-convolution can be divided into a very cheap filter indexing operation followed by a standard planar folding for which we can use existing implementations. In a divided group, any transformation g can be split into a translation t-Z2 and a transformation r that leaves the origin invariant. For group p4, we can write g = tr for a translation and r for a rotation."}, {"heading": "8.1. Filter rotation", "text": "The first part of the calculation is to evaluate Lr\u043d for each of the four rotations r-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-v-"}, {"heading": "8.2. Planar convolution", "text": "The second part of the calculation is to evaluate the inner products based on an optimized implementation of the 2D convolution. For the first layer correlation, the sum of X in size 22 is a sum of the translation coordinates u, v. Thus, if we simply transform the form K1 \u00b7 4 \u00b7 K0 \u00b7 n into 4K1 \u00b7 K0 \u00b7 n, we can perform the entire p4 correlation with a single call to a planar correlation routine. For further layers, we must sum up using three coordinates i, u, v. This can be done by transforming the entire p4 correlation from K l \u00b7 4 \u00b7 K l \u2212 1 \u00b7 4 \u00b7 n \u00b7 n to 4K l \u00b7 4K l \u2212 1 \u00b7 n and then calling the planar correlation routine."}, {"heading": "9. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1. Rotated MNIST", "text": "The rotated MNIST dataset (Larochelle et al., 2007) contains 62,000 randomly rotated handwritten digits. The dataset is divided into training, validation, and test sets of size 10000, 2000, and 50000, respectively. Since data amplification could potentially reduce the benefit of using G convolutions, all the experiments reported in this section use random rotations in each instance presentation. We found that this does not make much difference, at least for the rotated MNIST datasets. We performed the model selection using the validation set and produced a CNN architecture (Z2CNN, see Table 1) that exceeds the models tested by Larochelle et al. (2007) (when trained to 12k and evaluated to 50k), but does not match the prior art using prior knowledge of rotations (Schmidt & Roth, 2012)."}, {"heading": "9.2. CIFAR-10", "text": "The CIFAR-10 dataset consists of 60k images of size 32 x 32, divided into 10 classes. The dataset is divided into 40k training, 10k validation and 10k test splits. We took the All-CNN-C by Springenberg et al. (2015) as a starting point. Until recently, this architecture represented the state of the art on CIFAR-10 without data augmentation and gives competing results even in the extended environment (by adding translated and inverted copies to the dataset) The architecture consists of a sequence of striped and non-striped conversions interspersed with rectified linear activation units, and nothing else. We have the pipeline of (Springenberg et al., 2015) and used the same architectural, data augmentation, pre-processing, regulation and optimization parameters. We were unable to produce the reported error of 7.25% with augmentation and 9.07% without system reatization."}, {"heading": "10. Discussion & Future work", "text": "Our results show that p4 folding layers can be used as drop-in substitutes for standard convolutions, improving the statistical efficiency of a CNN. While GCNNs take advantage of geometric priors, other approaches such as exponential linear units and highway networks achieve good results through better optimization, so it is entirely possible that the benefits of these methods are synergistic. Interestingly, it seems that G-CNNs can benefit from data augmentation in the same way as Constitutional Networks, as long as the augmentation comes from a group larger than G. For example, augmentation with flips and subpixel translations seems to improve the results for the p4-CNN. Furthermore, the fact that rotation-equivalent networks improve the results on the CIFAR-10 dataset shows that there is no complete symmetry in the data distributions that are usable for these networks."}, {"heading": "11. Conclusion", "text": "We have introduced G-CNNs, a generalization of revolutionary networks that can significantly reduce the number of parameters required per layer without losing much expressiveness, or increase the expressiveness of a network without increasing the number of parameters. We have developed the general theory for G-CNNs in the case of discrete groups and demonstrated that all layer types of action of the chosen group G are equivalent. Our experimental results show that G-coils can be used as drop substitutes in revolutionary networks, improving their performance without further coordination."}, {"heading": "Acknowledgements", "text": "We would like to thank Chris Olah, Joan Bruna, Robert Gens, Sander Dieleman and Stefano Soatto for their helpful discussions, which were supported by the NWO (grant number NAI.14.108)."}], "references": [{"title": "Learning to See by Moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning", "author": ["F. Anselmi", "J.Z. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio"], "venue": null, "citeRegEx": "Anselmi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anselmi et al\\.", "year": 2014}, {"title": "Invariant scattering convolution networks. IEEE transactions on pattern analysis and machine intelligence", "author": ["J. Bruna", "S. Mallat"], "venue": null, "citeRegEx": "Bruna and Mallat,? \\Q2013\\E", "shortCiteRegEx": "Bruna and Mallat", "year": 2013}, {"title": "Learning intermediatelevel representations of form and motion from natural movies", "author": ["C.F. Cadieu", "B.A. Olshausen"], "venue": "Neural computation,", "citeRegEx": "Cadieu and Olshausen,? \\Q2012\\E", "shortCiteRegEx": "Cadieu and Olshausen", "year": 2012}, {"title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)", "author": ["D. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": null, "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Learning the Irreducible Representations of Commutative Lie Groups", "author": ["T. Cohen", "M. Welling"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Cohen and Welling,? \\Q2014\\E", "shortCiteRegEx": "Cohen and Welling", "year": 2014}, {"title": "Transformation Properties of Learned Visual Representations", "author": ["T.S. Cohen", "M. Welling"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Cohen and Welling,? \\Q2015\\E", "shortCiteRegEx": "Cohen and Welling", "year": 2015}, {"title": "Rotationinvariant convolutional neural networks for galaxy morphology prediction", "author": ["S. Dieleman", "K.W. Willett", "J. Dambre"], "venue": "Monthly Notices of the Royal Astronomical Society,", "citeRegEx": "Dieleman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2015}, {"title": "Deep Symmetry Networks", "author": ["R. Gens", "P. Domingos"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Gens and Domingos,? \\Q2014\\E", "shortCiteRegEx": "Gens and Domingos", "year": 2014}, {"title": "Emergence of Phase- and Shift-Invariant Features by Decomposition of Natural Images into Independent Feature Subspaces", "author": ["A. Hyvarinen", "P. Hoyer"], "venue": "Neural Computation,", "citeRegEx": "Hyvarinen and Hoyer,? \\Q2000\\E", "shortCiteRegEx": "Hyvarinen and Hoyer", "year": 2000}, {"title": "Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": null, "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Spatial Transformer Networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman", "K. Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Transformation equivariant Boltzmann machines", "author": ["Kivinen", "Jyri J", "Williams", "Christopher K I"], "venue": "In 21st International Conference on Artificial Neural Networks,", "citeRegEx": "Kivinen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kivinen et al\\.", "year": 2011}, {"title": "Emergence of invariant-feature detectors in the adaptive-subspace self-organizing map", "author": ["T. Kohonen"], "venue": "Biological Cybernetics,", "citeRegEx": "Kohonen,? \\Q1996\\E", "shortCiteRegEx": "Kohonen", "year": 1996}, {"title": "A novel set of rotationally and translationally invariant features for images based on the noncommutative bispectrum", "author": ["R. Kondor"], "venue": null, "citeRegEx": "Kondor,? \\Q2007\\E", "shortCiteRegEx": "Kondor", "year": 2007}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "Proceedings of the 24th International Conference on Machine Learning (ICML),", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Deeply-Supervised Nets", "author": ["C. Lee", "S. Xie", "P.W. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Understanding image representations by measuring their equivariance and equivalence", "author": ["K. Lenc", "A. Vedaldi"], "venue": "In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Lenc and Vedaldi,? \\Q2015\\E", "shortCiteRegEx": "Lenc and Vedaldi", "year": 2015}, {"title": "Distinctive Image Features from ScaleInvariant Keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Lowe,? \\Q2004\\E", "shortCiteRegEx": "Lowe", "year": 2004}, {"title": "Fast Training of Convolutional Networks through FFTs", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Mathieu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2014}, {"title": "Learning to Represent Spatial Transformations with Factored HigherOrder Boltzmann Machines", "author": ["R. Memisevic", "G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Memisevic and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Memisevic and Hinton", "year": 2010}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning rotation-aware features: From invariant priors to equivariant descriptors", "author": ["U. Schmidt", "S. Roth"], "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Schmidt and Roth,? \\Q2012\\E", "shortCiteRegEx": "Schmidt and Roth", "year": 2012}, {"title": "Spherical Tensor Algebra for Biomedical Image Analysis", "author": ["H. Skibbe"], "venue": "PhD thesis, Albert-Ludwigs-Universitat Freiburg im Breisgau,", "citeRegEx": "Skibbe,? \\Q2013\\E", "shortCiteRegEx": "Skibbe", "year": 2013}, {"title": "Learning Invariant Representations with Local Transformations", "author": ["K. Sohn", "H. Lee"], "venue": "Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Sohn and Lee,? \\Q2012\\E", "shortCiteRegEx": "Sohn and Lee", "year": 2012}, {"title": "Striving for Simplicity: The All Convolutional Net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "Training Very Deep Networks", "author": ["Srivastava", "Rupesh Kumar", "Greff", "Klaus", "Schmidhuber", "J\u00fcrgen"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Fast convolutional nets with fbfft: A GPU performance evaluation", "author": ["N. Vasilache", "J. Johnson", "M. Mathieu", "S. Chintala", "S. Piantino", "Y. LeCun"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Vasilache et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vasilache et al\\.", "year": 2015}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 27, "context": "In section 9 we report on our experiments with rotated MNIST, where G-CNNs significantly outperform the previous state of the art, and CIFAR-10, where we show a considerable improvement over a competitive baseline (the All-CNN-C architecture of (Springenberg et al., 2015)), thus outperforming DropConnect (Wan et al.", "startOffset": 245, "endOffset": 272}, {"referenceID": 30, "context": ", 2015)), thus outperforming DropConnect (Wan et al., 2013), Maxout (Goodfellow et al.", "startOffset": 41, "endOffset": 59}, {"referenceID": 18, "context": ", 2014), deeply supervised nets (Lee et al., 2015).", "startOffset": 32, "endOffset": 50}, {"referenceID": 23, "context": "Although vector addition has been shown to be surprisingly effective at modeling semantic relationships of words (Mikolov et al., 2013), it has inherent limitations such as being commutative and non-periodic.", "startOffset": 113, "endOffset": 135}, {"referenceID": 15, "context": "Lenc & Vedaldi (2015) show that the AlexNet CNN (Krizhevsky et al., 2012) trained on imagenet spontaneously learns representations that are equivariant to flips, scaling and rotation.", "startOffset": 48, "endOffset": 73}, {"referenceID": 20, "context": "There is a large literature on learning invariant representations, which can be achieved by pose normalization (Lowe, 2004; Jaderberg et al., 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al.", "startOffset": 111, "endOffset": 147}, {"referenceID": 11, "context": "There is a large literature on learning invariant representations, which can be achieved by pose normalization (Lowe, 2004; Jaderberg et al., 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al.", "startOffset": 111, "endOffset": 147}, {"referenceID": 25, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)).", "startOffset": 68, "endOffset": 140}, {"referenceID": 14, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)).", "startOffset": 68, "endOffset": 140}, {"referenceID": 1, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)).", "startOffset": 68, "endOffset": 140}, {"referenceID": 17, "context": "Cohen & Welling (2015) showed that under certain conditions such disentangled representations are also decorrelated, thus relating this line of work to statistical approaches such as ICA and ISA (Hyvarinen & Hoyer, 2000; Le et al., 2011).", "startOffset": 195, "endOffset": 237}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)).", "startOffset": 119, "endOffset": 200}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012).", "startOffset": 119, "endOffset": 553}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012).", "startOffset": 119, "endOffset": 614}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012).", "startOffset": 119, "endOffset": 636}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant.", "startOffset": 119, "endOffset": 694}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant.", "startOffset": 119, "endOffset": 718}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant. Dieleman et al. (2015) showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation.", "startOffset": 119, "endOffset": 880}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant. Dieleman et al. (2015) showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation. By diagonalizing or block-diagonalizing the transformation operators Tg, the representation becomes disentangled or \u201cirreducible\u201d in a certain precise sense (Cohen & Welling, 2014). This idea is implicit in a long line of work, going back at least to Kohonen (1996), and including Memisevic & Hinton (2010); Hinton et al.", "startOffset": 119, "endOffset": 1345}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant. Dieleman et al. (2015) showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation. By diagonalizing or block-diagonalizing the transformation operators Tg, the representation becomes disentangled or \u201cirreducible\u201d in a certain precise sense (Cohen & Welling, 2014). This idea is implicit in a long line of work, going back at least to Kohonen (1996), and including Memisevic & Hinton (2010); Hinton et al.", "startOffset": 119, "endOffset": 1386}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant. Dieleman et al. (2015) showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation. By diagonalizing or block-diagonalizing the transformation operators Tg, the representation becomes disentangled or \u201cirreducible\u201d in a certain precise sense (Cohen & Welling, 2014). This idea is implicit in a long line of work, going back at least to Kohonen (1996), and including Memisevic & Hinton (2010); Hinton et al. (2011); Cadieu & Olshausen (2012).", "startOffset": 119, "endOffset": 1408}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant. Dieleman et al. (2015) showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation. By diagonalizing or block-diagonalizing the transformation operators Tg, the representation becomes disentangled or \u201cirreducible\u201d in a certain precise sense (Cohen & Welling, 2014). This idea is implicit in a long line of work, going back at least to Kohonen (1996), and including Memisevic & Hinton (2010); Hinton et al. (2011); Cadieu & Olshausen (2012). Cohen & Welling (2015) showed that under certain conditions such disentangled representations are also decorrelated, thus relating this line of work to statistical approaches such as ICA and ISA (Hyvarinen & Hoyer, 2000; Le et al.", "startOffset": 119, "endOffset": 1435}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant. Dieleman et al. (2015) showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation. By diagonalizing or block-diagonalizing the transformation operators Tg, the representation becomes disentangled or \u201cirreducible\u201d in a certain precise sense (Cohen & Welling, 2014). This idea is implicit in a long line of work, going back at least to Kohonen (1996), and including Memisevic & Hinton (2010); Hinton et al. (2011); Cadieu & Olshausen (2012). Cohen & Welling (2015) showed that under certain conditions such disentangled representations are also decorrelated, thus relating this line of work to statistical approaches such as ICA and ISA (Hyvarinen & Hoyer, 2000; Le et al.", "startOffset": 119, "endOffset": 1459}, {"referenceID": 0, "context": "Agrawal et al. (2015) show that equivariance to ego-motion can be learned, and that this can be used as an unsupervised", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "The sum of two G-equivariant feature maps is also G-equivariant, thus G-conv layers can be used in highway networks and residual networks (Srivastava et al., 2015; He et al., 2015).", "startOffset": 138, "endOffset": 180}, {"referenceID": 29, "context": "So we see that both the forward and backward passes involve convolution or correlation operations, as is the case in the current generation of convnets (Vasilache et al., 2015).", "startOffset": 152, "endOffset": 176}, {"referenceID": 21, "context": "However, without a significant investment in code optimization, such an approach is likely to be slower than a comparable planar convolution, because there are now extremely efficient algorithms for Zconvolutions (Mathieu et al., 2014; Vasilache et al., 2015; Lavin & Gray, 2015).", "startOffset": 213, "endOffset": 279}, {"referenceID": 29, "context": "However, without a significant investment in code optimization, such an approach is likely to be slower than a comparable planar convolution, because there are now extremely efficient algorithms for Zconvolutions (Mathieu et al., 2014; Vasilache et al., 2015; Lavin & Gray, 2015).", "startOffset": 213, "endOffset": 279}, {"referenceID": 16, "context": "The rotated MNIST dataset (Larochelle et al., 2007) contains 62000 randomly rotated handwritten digits.", "startOffset": 26, "endOffset": 51}, {"referenceID": 16, "context": "The rotated MNIST dataset (Larochelle et al., 2007) contains 62000 randomly rotated handwritten digits. The dataset is split into a training, validation and test sets of size 10000, 2000 and 50000, respectively. Because data augmentation could potentially reduce the benefits of using G-convolutions, all experiments reported in this section use random rotations on each instance presentation. We found that at least for the rotated MNIST dataset this does not make much difference. We performed model selection using the validation set, yielding a CNN architecture (Z2CNN, see table 1) that outperforms the models tested by Larochelle et al. (2007) (when trained on 12k and evaluated on 50k), but does not match the previous state of the art, which uses prior knowledge about rotations (Schmidt & Roth, 2012) (see table 2).", "startOffset": 27, "endOffset": 650}, {"referenceID": 16, "context": "The dataset is split into 40k Model Test Error (%) Larochelle et al. (2007) 10.", "startOffset": 51, "endOffset": 76}, {"referenceID": 16, "context": "The dataset is split into 40k Model Test Error (%) Larochelle et al. (2007) 10.38 \u00b1 0.27 Sohn & Lee (2012) 4.", "startOffset": 51, "endOffset": 107}, {"referenceID": 16, "context": "The dataset is split into 40k Model Test Error (%) Larochelle et al. (2007) 10.38 \u00b1 0.27 Sohn & Lee (2012) 4.2 Schmidt & Roth (2012) 3.", "startOffset": 51, "endOffset": 133}, {"referenceID": 27, "context": "We recreated the pipeline of (Springenberg et al., 2015) and used the same architectural, data augmentation, preprocessing, regularization and optimization parameters.", "startOffset": 29, "endOffset": 56}, {"referenceID": 27, "context": "This discrepancy indicates that our pipeline may differ slightly from the one used by (Springenberg et al., 2015), for example in terms of weight initialization, but our results are in line with another reproduction by (NervanaSystems, 2016) who report 10.", "startOffset": 86, "endOffset": 113}, {"referenceID": 27, "context": "We took the All-CNN-C by Springenberg et al. (2015) as our baseline.", "startOffset": 25, "endOffset": 52}, {"referenceID": 4, "context": "units by Clevert et al. (2015) achieve better performance, while in the augmented case the 19-layer highway networks perform about equal and fractional max pooling with massive data augmentation clearly outperforms our P4-AllCNN-BN architecture.", "startOffset": 9, "endOffset": 31}, {"referenceID": 18, "context": "41 DSN (Lee et al., 2015) 9.", "startOffset": 7, "endOffset": 25}, {"referenceID": 27, "context": "82 All-CNN (Springenberg et al., 2015) 9.", "startOffset": 11, "endOffset": 38}, {"referenceID": 4, "context": "84 Elu (Clevert et al., 2015) 6.", "startOffset": 7, "endOffset": 29}, {"referenceID": 30, "context": "38 DropConnect (Wan et al., 2013) 9.", "startOffset": 15, "endOffset": 33}, {"referenceID": 18, "context": "81 DSN (Lee et al., 2015) 7.", "startOffset": 7, "endOffset": 25}, {"referenceID": 27, "context": "6 All-CNN (Springenberg et al., 2015) 7.", "startOffset": 10, "endOffset": 37}], "year": 2017, "abstractText": "We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. By convolving over groups larger than the translation group, G-CNNs build representations that are equivariant to these groups, which makes it possible to greatly increase the degree of parameter sharing. We show how G-CNNs can be implemented with negligible computational overhead for discrete groups such as the group of translations, reflections and rotations by multiples of 90 degrees. G-CNNs achieve state of the art results on rotated MNIST and significantly improve over a competitive baseline on augmented and non-augmented CIFAR-10.", "creator": "LaTeX with hyperref package"}}}