{"id": "1602.07714", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Learning values across many orders of magnitude", "abstract": "Learning non-linear functions can be hard when the magnitude of the target function is unknown beforehand, as most learning algorithms are not scale invariant. We propose an algorithm to adaptively normalize these targets. This is complementary to recent advances in input normalization. Importantly, the proposed method preserves the unnormalized outputs whenever the normalization is updated to avoid instability caused by non-stationarity. It can be combined with any learning algorithm and any non-linear function approximation, including the important special case of deep learning. We empirically validate the method in supervised learning and reinforcement learning and apply it to learning how to play Atari 2600 games. Previous work on applying deep learning to this domain relied on clipping the rewards to make learning in different games more homogeneous, but this uses the domain-specific knowledge that in these games counting rewards is often almost as informative as summing these. Using our adaptive normalization we can remove this heuristic without diminishing overall performance, and even improve performance on some games, such as Ms. Pac-Man and Centipede, on which previous methods did not perform well.", "histories": [["v1", "Wed, 24 Feb 2016 21:14:52 GMT  (604kb,D)", "http://arxiv.org/abs/1602.07714v1", null], ["v2", "Tue, 16 Aug 2016 05:27:17 GMT  (659kb,D)", "http://arxiv.org/abs/1602.07714v2", "Paper accepted for publication at NIPS 2016. This version includes the appendix"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE stat.ML", "authors": ["hado p van hasselt", "arthur guez", "matteo hessel", "volodymyr mnih", "david silver"], "accepted": true, "id": "1602.07714"}, "pdf": {"name": "1602.07714.pdf", "metadata": {"source": "CRF", "title": "Learning functions across many orders of magnitudes", "authors": ["Hado van Hasselt", "Arthur Guez", "Matteo Hessel", "David Silver"], "emails": ["HADO@GOOGLE.COM", "AGUEZ@GOOGLE.COM", "MTTHSS@GOOGLE.COM", "DAVIDSILVER@GOOGLE.COM"], "sections": [{"heading": "1. Learning with arbitrary magnitudes", "text": "This year, it has come to the point where it is only a matter of time before a solution is found, in which a solution is found."}, {"heading": "2. Related work", "text": "In this paper, we focus only on the objective, not on the objective, but on the objective. (LeCun et al., 1998) This has led to several publications on how to achieve a scale-based invariance of input factors (e.g. Ross et al., 2013; Ioffe and Szegedy, 2015; Desjardins et al., 2015). On the other hand, the production or objective of normalization is not associated with the same attention. In classification, normalization of objectives is not necessary. In joint, monitored regression setting, where a complete set of data is available, we start learning, it is straightforward and commonplace to determine an appropriate normalization."}, {"heading": "3. Preliminaries", "text": "We consider learning from a data stream {(Xt, Yt)} \u221e t = 1, where the inputs Xt, Rn and targets Yt, Rk come in some way close to the corresponding target Yt, for example measured by the (expected) square difference. A canonical and popular example of a learning algorithm is stochastic gradient descent (SGD) on a square loss sl (fIS). 1 2 E [(fIS) \u2212 Yt) > (fIS) \u2212 Yt)]. For a given tuple (XT, Yt), the update of the square lots Rumsl (fIS) is on a square loss sl (fIS)."}, {"heading": "4. Adaptive normalization with Pop-Art", "text": "We propose to normalize targets Yt, where normalization is learned separately from the approximate function. We consider an affine transformation of targets Y-t = \u03a3 -1 t (Yt-\u00b5t), where Electrit and \u00b5t are scale and shift parameters learned from the data. The scale matrix can be defined more densely, diagonally, or by a scalar approach, like the results of a parameterized function f-\u03c3tI (Xt) toward the normalized target Y-t can then contain separate components, or can be defined by a scalar value \u00b5t = \u00b5t1. We discuss the relative advantages of each approach in Section 4.3.We update the output of a parameterized function f-cipated, as if the shift of another target can contain Y-t, or be defined by a scalar value defined as \u00b5t = \u00b5t1."}, {"heading": "4.1. Preserving outputs precisely", "text": "The only way to avoid a change in all the output of the unnormalized function when updating the scale and shift is to change the normalized function for itself at the same time. The goal is to get the results of the time before the normalization change exactly for all inputs, which prevents normalization from affecting the approximation, which is appropriate because its goal is solely to make learning easier and leave the solution of the approximation itself to the internal optimization algorithm. \u2212 Without losing generality, we assume that the normalized function can be written. \u2212 Normalization can be asf, W, b, (x) \u2261 Wg, where the approximation itself is applied to the internal optimization algorithm. \u2212 Rn \u2192 Rm is any (non-linear) function, for example, a deep neural network. It is not unusual for deep neural networks to end in a linear layer, and so g that they are considered as the output of the last (hidden) layer of nonlinearity."}, {"heading": "4.2. Adaptively rescaling targets", "text": "For clarity and algorithm 1 SGD with pop art For a given differentiable function g + 0. \u2212 \u03b2\u03b2t For a given differentiable function g + 0. \u2212 \u03b2t For a given differentiable function g + 0. \u2212 \u03b2t For a given differentiated function g + 0, b = 0, \u03a3 = I, and \u00b5 = 0. while learning to do observation of input X and target Y to calculate new scale effects and new shifts g + 0, p \u2212 W (rescale W + 1 to get outputs) b, p + 0, p = 0, p = 0, p = 0, p = p, p. (rescale b) g, p. (rescale b) g, p."}, {"heading": "4.3. Multi-variate outputs", "text": "In many applications, the objectives are higher order vectors or tensors. Then, there are at least three ways to normalize, depending on how these objectives interact. The appropriate choice depends on the domain. First, we can calculate combined statistics to find a global scale and displacement. This is useful if all objectives have the same natural scale and units and we want to preserve relative differences.One example is learning actions in the field of learning amplification. Second, we can normalize each element of the target separately. This is useful if the results have different natural scales, for example when predicting sensory readings of different modalities. Normalization per component makes it easier to weigh each output appropriately in the desired target, since we can untangle the relative importance of each initial component from its magnitude. Finally, we can normalize all elements together, for example by defining normalization as a goal that the normalized target vector is distributed according to a multivariant normalization (this is a more common scalar for 2015)."}, {"heading": "4.4. An equivalence for stochastic gradient descent", "text": "This analysis suggests a different normalization algorithm, which is an interesting analogy to Pop Art SGD. We consider SGD updates for an unnormalized multilayer function of the form f\u03b8, W, b (X) = Wg (X) + b). The update for the weight matrix W isWt = Wt \u2212 1 + 1 + 2 (Xt) >, where the unnormalized targets (X) are the unnormalized errors. The magnitude of this update depends on the magnitude of the errors that are appropriate when the inputs are normalized, because then the ideal scale of weights depends on the size of the SGD."}, {"heading": "5. Binary regression experiments", "text": "In our first experiment, we analyze the effect of rare events in online learning, when rarely a target \u03b2 is observed with a much higher magnitude. Such events are expected, for example, when we try to learn from a sensor that is mostly white noise, but sometimes captures an actual signal. Moreover, such settings occur in amplification learning with sparse rewards. We empirically compare three variants of SGD: without normalization, with normalization, but without obtaining results accurately (i.e. with \"kind,\" but without \"pop\"), and with Pop-Art. The setup is as follows: The inputs are binary representations of integers, uniformly ranging from 0 to n = 210 \u2212 1. The desired results are the corresponding integer values. We present a much larger goal for each 1000 sample by presenting the binary representation of 216 \u2212 1 (i.e., all 16 inputs are 1) and as target 216 \u2212 1."}, {"heading": "6. Atari 2600 experiments", "text": "This is an important motivation for this work, in which no reward is higher than 1 or lower than 1, but it is not the factory that introduces it. The goal is to predict and optimize action values, which are defined as the expected sum of future rewards. (2015) These rewards can vary arbitrarily from one domain to the next, and the rewards can be sparse. As a successful example, Mnih et al. (2015) will learn to play many games with a single set of parameters, a form of Q-learning (Watkins, 1989) along with a deep neural network (LeCun et al., 2015) in an algorithm called DQN. However, in order to handle the different reward structures with a single system, DQN clips all rewards for the interval [\u2212 1, 1]. This is harmless in some games where no reward is higher than 1 or lower than 1."}, {"heading": "7. Discussion", "text": "We have shown that Pop Art methods can be used to adapt to different and non-stationary targets. As discussed above, this seems to be particularly useful for combining with deep reinforcement learning, although PopArt is not specific to this environment. We have seen that Pop Art can successfully replace the cutting of rewards performed by DQN to cope with the different orders of magnitude of the targets used in the Q Learning Update. Now that the real problem is exposed to the learning algorithm, we can hope for further progress, for example by combining Pop Art with other newer advances, such as dual architectures (Wang et al., 2015) and prioritized replay (Schaul et al., 2015). Furthermore, it is important to investigate methods to escape the suboptimal strategies in which these algorithms currently seem to be stuck and for which we need better research. Fortunately, such research can now be solved by the real problem we are trying to inform."}, {"heading": "Appendix", "text": "In this appendix, we present various extensions and variations and analyze them, including normalization based on percentiles or minibatches. In addition, we prove all the facts in the main text and in the appendix."}, {"heading": "Experiment setup", "text": "For the experiments described in Section 6, we closely followed the procedure described in Mnih et al. (2015) and van Hasselt et al. (2016), which was achieved by 30 minutes of simulated play (or 108,000 frames) with the trained agent, repeated 100 times, ensuring diversity across different runs by a low probability of exploration at each step (-greedy exploration with = 0.01), and by performing up to 30 \"no-op\" actions, as also used and described by Mnih et al. (2015). In summary, the evaluation procedure was the same as for Mnih et al., except that we allowed more evaluation time per game (30 minutes instead of 5 minutes) than for Wang et al. (2015). The results in Figure 3 were obtained by normalizing the raw results."}, {"heading": "Generalizing normalization by variance", "text": "We can change the variance of the normalized targets to influence the magnitude of the updates. For a desired standard deviation of s > 0 we can use either \u03c3t = \u221a \u03bdt \u2212 \u00b52t s, with the updates being normal for \u03bdt and \u00b5t. It is easy to show that a generalization of fact 2 with a limit of \u2212 s \u221a 1 \u2212 \u03b2t \u03b2t \u2264 Yt \u2212 \u00b5t \u2264 s 270 \u2212 \u03b2t is allowed. This additional parameter is useful, for example, if we want to quickly track non-stationary problems. We then want a large step size \u03b1 without risking excessive updates. The new parameter s may seem superfluous, because the increase in the normalization step size also reduces the hard limits of the normalized objectives. \u03b2, however, additionally affects the distribution of the normalized targets without normalizing the histograms in the most left graph. The histograms in Figure 4 show what happens when we try to limit the magnitudes of use."}, {"heading": "Adaptive normalization by percentiles", "text": "Instead of normalization by mean and variance, we can normalize this fact only if two other targets are within the given interval. Fact 4. If scalar targets {Yt} t = 1 are distributed according to a normal distribution with arbitrary finite averages and variance, then there is a direct correspondence to normalization by means and variance.Fact 4. If scalar targets are distributed according to a normal distribution with arbitrary finite averages and variance, then there is a direct correspondence to normalization by means and variance.Fact 4. If scalar targets are distributed according to the common goal E [Y \u2212 \u00b5] = 0 and E [Y \u2212 \u00b5] 2 = s2 with arbitrary finite averages and variance.For example, perceptive values of p = 0.99 and p = 0.95 correspond to the targets 0.4 and s = 0.5, respectively. Conversely, s = 1 p 0.68."}, {"heading": "A note on initialization", "text": "When using constant increments, it is useful to be aware of the beginning of learning to trust the data and not arbitrary initial values. This can be done by using an increment as defined below. Fact 7. Consider a recurrence-weighted running average z-t updated from a data stream. (9) Then, 1) the relative weights of the data in Zt are the same as when using a constant increment size \u03b2, and 2) the estimate z-t is not from the initial value z-0.A similar result was derived to remove the effect of initializing certain parameters by Kingma and Ba (2014), the relative weights of the data in Zt are the same as when using a constant increment size \u03b2, and 2) the estimate z-t is not from the initial value z-0.A similar result was derived to remove the effect of initializing certain parameters by Kingma and Ba (2014) for an algorithm called an optimization algorithm."}, {"heading": "Deep Pop-Art", "text": "Sometimes it is useful to apply normalization not to the output of the network, but to a lower level. Thus, for example, the output of a neural network with a Soft-Max at the top, where W is the weight matrix of the last linear layer before the Soft-Max, can be written. Actual yields are already normalized by the Soft-Max, but the yields Wg-\u03b8 (X) + b of the layer below the Soft-Max can still benefit from normalization. To determine the targets to be normalized, we can either reverse the gradient of our loss through the Soft-Max or invert the function. More generally, we can consider applying normalization at each level of a hierarchical, non-linear layer."}, {"heading": "Proofs", "text": "Fact 1. Consider a function f: \u2212 \u2212 \u2212 \u2212 \u03b2\u03b2\u03b2\u03b2\u03b2\u03b2s = \u03b2\u03b2s = \u03b2s = \u03b2s, W, b (x). (Wg). (x). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs. (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs. (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs). (xs. (xs). (xs). (xs). (xs.). (xs. (xs). (xs). (xs). (xs).). (xs. (xs."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["S.I. Amari"], "venue": "Neural computation,", "citeRegEx": "Amari.,? \\Q1998\\E", "shortCiteRegEx": "Amari.", "year": 1998}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Random search for hyperparameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bergstra and Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "Algorithms for hyper-parameter optimization", "author": ["J.S. Bergstra", "R. Bardenet", "Y. Bengio", "B. K\u00e9gl"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q1995\\E", "shortCiteRegEx": "Bishop.", "year": 1995}, {"title": "Natural neural networks", "author": ["G. Desjardins", "K. Simonyan", "R. Pascanu", "K. Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Desjardins et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Regression percentiles using asymmetric squared error loss", "author": ["B. Efron"], "venue": "Statistica Sinica,", "citeRegEx": "Efron.,? \\Q1991\\E", "shortCiteRegEx": "Efron.", "year": 1991}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["S. Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "Hochreiter.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter.", "year": 1998}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Stochastic approximation and recursive algorithms and applications, volume 35", "author": ["H.J. Kushner", "G. Yin"], "venue": "Springer Science & Business Media,", "citeRegEx": "Kushner and Yin.,? \\Q2003\\E", "shortCiteRegEx": "Kushner and Yin.", "year": 2003}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Tuning-free step-size adaptation", "author": ["A.R. Mahmood", "R.S. Sutton", "T. Degris", "P.M. Pilarski"], "venue": "In Proceedings of the IEEE International Conference on Acoustics,", "citeRegEx": "Mahmood et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2012}, {"title": "Optimizing neural networks with kronecker-factored approximate curvature", "author": ["J. Martens", "R.B. Grosse"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Martens and Grosse.,? \\Q2015\\E", "shortCiteRegEx": "Martens and Grosse.", "year": 2015}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["W.S. McCulloch", "W. Pitts"], "venue": "The bulletin of mathematical biophysics,", "citeRegEx": "McCulloch and Pitts.,? \\Q1943\\E", "shortCiteRegEx": "McCulloch and Pitts.", "year": 1943}, {"title": "Asymmetric least squares estimation and testing", "author": ["W.K. Newey", "J.L. Powell"], "venue": "Econometrica: Journal of the Econometric Society,", "citeRegEx": "Newey and Powell.,? \\Q1987\\E", "shortCiteRegEx": "Newey and Powell.", "year": 1987}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Robbins and Monro.,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro.", "year": 1951}, {"title": "Principles of Neurodynamics", "author": ["F. Rosenblatt"], "venue": "Spartan, New York,", "citeRegEx": "Rosenblatt.,? \\Q1962\\E", "shortCiteRegEx": "Rosenblatt.", "year": 1962}, {"title": "Normalized online learning", "author": ["S. Ross", "P. Mineiro", "J. Langford"], "venue": "In Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Ross et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2013}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "In Parallel Distributed Processing,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "No More Pesky Learning Rates", "author": ["T. Schaul", "S. Zhang", "Y. LeCun"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Schaul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2013}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Adapting bias by gradient descent: An incremental version of delta-bar-delta", "author": ["R.S. Sutton"], "venue": "In Proceedings of the Tenth National Conference on Artificial Intelligence,", "citeRegEx": "Sutton.,? \\Q1992\\E", "shortCiteRegEx": "Sutton.", "year": 1992}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Deep reinforcement learning with Double Q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": null, "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Insights in Reinforcement Learning", "author": ["H.P. van Hasselt"], "venue": "PhD thesis, Utrecht University,", "citeRegEx": "Hasselt.,? \\Q2011\\E", "shortCiteRegEx": "Hasselt.", "year": 2011}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "N. de Freitas", "M. Lanctot"], "venue": "CoRR, abs/1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Learning from delayed rewards", "author": ["C.J.C.H. Watkins"], "venue": "PhD thesis,", "citeRegEx": "Watkins.,? \\Q1989\\E", "shortCiteRegEx": "Watkins.", "year": 1989}, {"title": "Adaptive switching circuits", "author": ["B. Widrow", "M.E. Hoff"], "venue": "IRE WESCON Convention Record,", "citeRegEx": "Widrow and Hoff.,? \\Q1960\\E", "shortCiteRegEx": "Widrow and Hoff.", "year": 1960}], "referenceMentions": [{"referenceID": 3, "context": "Many current learning algorithms rely on a priori access to actual or sufficiently similar data to properly tune relevant hyper-parameters (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012).", "startOffset": 139, "endOffset": 209}, {"referenceID": 2, "context": "Many current learning algorithms rely on a priori access to actual or sufficiently similar data to properly tune relevant hyper-parameters (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012).", "startOffset": 139, "endOffset": 209}, {"referenceID": 23, "context": "Many current learning algorithms rely on a priori access to actual or sufficiently similar data to properly tune relevant hyper-parameters (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012).", "startOffset": 139, "endOffset": 209}, {"referenceID": 29, "context": "(2015), which combines Q-learning (Watkins, 1989) with a deep convolutional neural network (cf.", "startOffset": 34, "endOffset": 49}, {"referenceID": 1, "context": "The resulting deep Q network (DQN) algorithm was shown to be able to learn to play a varied set of Atari 2600 games from the Arcade Learning Environment (ALE) (Bellemare et al., 2013), by learning action values corresponding to expected sums of future rewards.", "startOffset": 159, "endOffset": 183}, {"referenceID": 12, "context": "Input normalization has long been recognized as important for efficient learning of non-linear approximations such as neural networks (LeCun et al., 1998).", "startOffset": 134, "endOffset": 154}, {"referenceID": 9, "context": "This has led to several publications about how to achieve scale-invariance on the inputs (e.g., Ross et al., 2013; Ioffe and Szegedy, 2015; Desjardins et al., 2015).", "startOffset": 89, "endOffset": 164}, {"referenceID": 5, "context": "This has led to several publications about how to achieve scale-invariance on the inputs (e.g., Ross et al., 2013; Ioffe and Szegedy, 2015; Desjardins et al., 2015).", "startOffset": 89, "endOffset": 164}, {"referenceID": 0, "context": "More generally there has been work on normalizing the whole optimization process, for instance by using natural gradients (Amari, 1998).", "startOffset": 122, "endOffset": 135}, {"referenceID": 5, "context": "Sometimes, these approximations focus on a complementary aspect, such as input normalization in the case of Natural Neural Networks (Desjardins et al., 2015) and Batch Normalization (Ioffe and Szegedy, 2015).", "startOffset": 132, "endOffset": 157}, {"referenceID": 9, "context": ", 2015) and Batch Normalization (Ioffe and Szegedy, 2015).", "startOffset": 32, "endOffset": 57}, {"referenceID": 14, "context": "In other cases, these algorithms directly approximate the full natural gradient (Martens and Grosse, 2015), but then necessarily have to make trade offs in terms of accuracy to obtain computational gains.", "startOffset": 80, "endOffset": 106}, {"referenceID": 24, "context": "In a different strand of research, several algorithms have been proposed to automatically adapt the step size during learning to handle non-stationarity problems (Sutton, 1992; Mahmood et al., 2012; Schaul et al., 2013).", "startOffset": 162, "endOffset": 219}, {"referenceID": 13, "context": "In a different strand of research, several algorithms have been proposed to automatically adapt the step size during learning to handle non-stationarity problems (Sutton, 1992; Mahmood et al., 2012; Schaul et al., 2013).", "startOffset": 162, "endOffset": 219}, {"referenceID": 21, "context": "In a different strand of research, several algorithms have been proposed to automatically adapt the step size during learning to handle non-stationarity problems (Sutton, 1992; Mahmood et al., 2012; Schaul et al., 2013).", "startOffset": 162, "endOffset": 219}, {"referenceID": 24, "context": "In addition, these algorithms often come with additional assumptions, such as requiring linear function approximation (Sutton, 1992; Mahmood et al., 2012).", "startOffset": 118, "endOffset": 154}, {"referenceID": 13, "context": "In addition, these algorithms often come with additional assumptions, such as requiring linear function approximation (Sutton, 1992; Mahmood et al., 2012).", "startOffset": 118, "endOffset": 154}, {"referenceID": 25, "context": "In addition, the algorithms typically assume stochastic gradient descent updates, or are themselves specific variants thereof, such as RMSprop (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014), which are not themselves target-scale invariant.", "startOffset": 143, "endOffset": 170}, {"referenceID": 10, "context": "In addition, the algorithms typically assume stochastic gradient descent updates, or are themselves specific variants thereof, such as RMSprop (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014), which are not themselves target-scale invariant.", "startOffset": 180, "endOffset": 201}, {"referenceID": 15, "context": "An important special case is when f\u03b8 is a neural network (McCulloch and Pitts, 1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986; Bishop, 1995).", "startOffset": 57, "endOffset": 163}, {"referenceID": 30, "context": "An important special case is when f\u03b8 is a neural network (McCulloch and Pitts, 1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986; Bishop, 1995).", "startOffset": 57, "endOffset": 163}, {"referenceID": 18, "context": "An important special case is when f\u03b8 is a neural network (McCulloch and Pitts, 1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986; Bishop, 1995).", "startOffset": 57, "endOffset": 163}, {"referenceID": 20, "context": "An important special case is when f\u03b8 is a neural network (McCulloch and Pitts, 1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986; Bishop, 1995).", "startOffset": 57, "endOffset": 163}, {"referenceID": 4, "context": "An important special case is when f\u03b8 is a neural network (McCulloch and Pitts, 1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986; Bishop, 1995).", "startOffset": 57, "endOffset": 163}, {"referenceID": 22, "context": "Especially for deep neural networks (LeCun et al., 2015; Schmidhuber, 2015) an update that is too large may harm learning, because these networks are highly non-linear and a large update may \u2018bump\u2019 the parameters to regions with high error.", "startOffset": 36, "endOffset": 75}, {"referenceID": 9, "context": "This is a common approach for input normalization (Ioffe and Szegedy, 2015; Desjardins et al., 2015).", "startOffset": 50, "endOffset": 100}, {"referenceID": 5, "context": "This is a common approach for input normalization (Ioffe and Szegedy, 2015; Desjardins et al., 2015).", "startOffset": 50, "endOffset": 100}, {"referenceID": 9, "context": "In general care should be taken that the inputs are wellbehaved; this is exactly the point of recent work on input normalization (Ioffe and Szegedy, 2015; Desjardins et al., 2015).", "startOffset": 129, "endOffset": 179}, {"referenceID": 5, "context": "In general care should be taken that the inputs are wellbehaved; this is exactly the point of recent work on input normalization (Ioffe and Szegedy, 2015; Desjardins et al., 2015).", "startOffset": 129, "endOffset": 179}, {"referenceID": 25, "context": "Consider RMSprop (Tieleman and Hinton, 2012), AdaGrag (Duchi et al.", "startOffset": 17, "endOffset": 44}, {"referenceID": 6, "context": "Consider RMSprop (Tieleman and Hinton, 2012), AdaGrag (Duchi et al., 2011), and Adam (Kingma and Ba, 2014), all of which divide the updates by the square root of a recency-weighted empirical second moment such that", "startOffset": 54, "endOffset": 74}, {"referenceID": 10, "context": ", 2011), and Adam (Kingma and Ba, 2014), all of which divide the updates by the square root of a recency-weighted empirical second moment such that", "startOffset": 18, "endOffset": 39}, {"referenceID": 29, "context": "(2015) used a form of Q-learning (Watkins, 1989) together with a deep neural network (LeCun et al.", "startOffset": 33, "endOffset": 48}, {"referenceID": 28, "context": "Now that the true problem is exposed to the learning algorithm we can hope to make further progress, for instance by combining Pop-Art with other recent advances, such as dueling architectures (Wang et al., 2015) and prioritized replay (Schaul et al.", "startOffset": 193, "endOffset": 212}], "year": 2016, "abstractText": "Learning non-linear functions can be hard when the magnitude of the target function is unknown beforehand, as most learning algorithms are not scale invariant. We propose an algorithm to adaptively normalize these targets. This is complementary to recent advances in input normalization. Importantly, the proposed method preserves the unnormalized outputs whenever the normalization is updated to avoid instability caused by non-stationarity. It can be combined with any learning algorithm and any non-linear function approximation, including the important special case of deep learning. We empirically validate the method in supervised learning and reinforcement learning and apply it to learning how to play Atari 2600 games. Previous work on applying deep learning to this domain relied on clipping the rewards to make learning in different games more homogeneous, but this uses the domainspecific knowledge that in these games counting rewards is often almost as informative as summing these. Using our adaptive normalization we can remove this heuristic without diminishing overall performance, and even improve performance on some games, such as Ms. Pac-Man and Centipede, on which previous methods did not perform well. 1. Learning with arbitrary magnitudes Many current learning algorithms rely on a priori access to actual or sufficiently similar data to properly tune relevant hyper-parameters (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012). It is much harder to learn efficiently from a stream of data when we do not know the magnitude of the function we seek to approximate beforehand, or if these magnitudes can change over time. Concretely, we are motivated by the work by Mnih et al. (2015), which combines Q-learning (Watkins, 1989) with a deep convolutional neural network (cf. LeCun et al., 2015). The resulting deep Q network (DQN) algorithm was shown to be able to learn to play a varied set of Atari 2600 games from the Arcade Learning Environment (ALE) (Bellemare et al., 2013), by learning action values corresponding to expected sums of future rewards. The ALE was proposed as an evaluation framework to test general learning algorithms on solving many different interesting tasks, and DQN was proposed as a singular solution, using a single set of hyperparameters. However, to achieve this feat the rewards and temporal-difference errors were clipped to [\u22121, 1] because the magnitudes and frequencies of rewards vary wildly between different games. For instance, in Pong the rewards are sparsely distributed and bounded by \u22121 and +1 while in Ms. Pac-Man eating a single ghost can yield a reward of up to +1600, but DQN clips the latter to +1 as well. This is not a satisfying solution for two reasons. The first reason is that the clipping introduces a form of domain knowledge. Most games have sparse non-zero rewards outside of [\u22121, 1]. Clipping the rewards then implies that the algorithm optimizes the frequency of rewards, rather than their sum. This is a good heuristic in many Atari games, but it does not generalize to the full setting of reinforcement learning. More importantly, the clipping changes the problem that the learning agent is solving and in some games the learned behaviour is clearly affected by this. In this paper, we propose a method that adaptively normalizes the targets used in the learning updates. If we know that these targets are guaranteed to fall in some predetermined range, for instance [\u22121, 1], it is much easier to find suitable hyperparameters. If the magnitudes of incoming targets can vary greatly over time, then a naive adaptive normalization can result in a highly non-stationary and unstable learning problem because it would continually change the outputs of our approximation for all inputs, thereby invalidating earlier learning. To avoid this, the proposed method includes an additional step that ensures that the outputs of the approximation are kept fixed whenever we change the normalization. The proposed technique is not specific to the application to DQN and is applicable more generally in supervised learning and reinforcement learning. There are several reasons ar X iv :1 60 2. 07 71 4v 1 [ cs .L G ] 2 4 Fe b 20 16 Learning functions across many orders of magnitudes normalization of the targets can be desirable. First, it is generally useful when we want a single system to be able to solve multiple different problems with varying natural magnitudes. Second, when learning a multi-variate function we can use the adaptive normalization to disentangle the natural magnitude of each target component from its relative importance in the loss function. This is particularly useful when the targets have different units, such as when we simultaneously predict signals from sensors with different modalities. Finally, adaptive scaling can help in problems that are non-stationary. For instance, this is common in reinforcement learning when the policy of behavior, and therefore the distribution and magnitude of the targets, can change repeatedly during learning.", "creator": "TeX"}}}