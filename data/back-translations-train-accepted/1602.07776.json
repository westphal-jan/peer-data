{"id": "1602.07776", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2016", "title": "Recurrent Neural Network Grammars", "abstract": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.", "histories": [["v1", "Thu, 25 Feb 2016 02:42:58 GMT  (166kb,D)", "http://arxiv.org/abs/1602.07776v1", "11 pages"], ["v2", "Fri, 1 Apr 2016 23:28:08 GMT  (170kb,D)", "http://arxiv.org/abs/1602.07776v2", "11 pages"], ["v3", "Thu, 6 Oct 2016 14:22:02 GMT  (222kb,D)", "http://arxiv.org/abs/1602.07776v3", "Proceedings of NAACL 2016 (contains corrigendum)"], ["v4", "Wed, 12 Oct 2016 04:47:45 GMT  (395kb,D)", "http://arxiv.org/abs/1602.07776v4", "Proceedings of NAACL 2016 (contains corrigendum)"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["chris dyer", "adhiguna kuncoro", "miguel ballesteros", "noah a smith"], "accepted": true, "id": "1602.07776"}, "pdf": {"name": "1602.07776.pdf", "metadata": {"source": "CRF", "title": "Recurrent Neural Network Grammars", "authors": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith"], "emails": ["cdyer@cs.cmu.edu,", "akuncoro@cs.cmu.edu,", "miguel.ballesteros@upf.edu,", "nasmith@cs.washington.edu"], "sections": [{"heading": null, "text": "We introduce recurring grammars of neural networks, probabilistic models of sentences with an explicit phrase structure. We explain efficient sequencing techniques that allow both parsing and speech modeling. Experiments show that they provide better parsing in English than any previously published monitored generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese."}, {"heading": "1 Introduction", "text": "In the last few years, language model results that improve substantially over long-established state-of-the-art baselines are RNNs (Zaremba et al., 2015), and dialogue generations (Wen et al., 2015), as well as various conditional language models that perform tasks such as machine translation (Bahdanau et al., 2015), caption generation (Xu et al., 2015), and dialogue generations (Wen et al., 2015). Despite these impressive results, sequential models of natural language are inappropriate because relationships between words are organized in terms of latent nested structures, rather than sequential surface order (Chomsky, 1957).In this paper, we introduce recurrent neural network grammars (RNGs; \u00a7 2), a new generative probability model of sentences that exhibit explicitly hierarchical relationships between words and phrases."}, {"heading": "2 RNN Grammars", "text": "Formally, an RNNG is a triple (N, \u03a3, \u044b), consisting of a finite set of non-terminal symbols (N), a finite set of terminal symbols (\u03a3), so that N \u03a3 = \u2205, and a collection of neural network parameters. The algorithm that grammar uses to generate trees and strings in the language is characterized by a transition-based algorithm, which is outlined in the next section. In the following section, the semantics of the parameters used to turn it into a stochastic algorithm that generates pairs of trees and strings are discussed."}, {"heading": "3 Top-down Parsing and Generation", "text": "RNNGs are based on a top-down generation algorithm based on a stack data structure of partially completed syntactical components. To highlight the similarity of our algorithm to more familiar bottom-up shift-reduce detection algorithms, we first present the parsing version of our algorithm (not the generation) (\u00a7 3.1) and then present modifications to turn it into a generator (\u00a7 3.2)."}, {"heading": "3.1 Parser Transitions", "text": "The parser algorithm transforms a sequence of words x into a parser tree y with two data structures (a stack, an input buffer). Similar to the bottom-up algorithm of Sagae and Lavie (2005), our algorithm starts empty with the stack (S) and the complete sequence of words on the input buffer (B). The buffer contains unprocessed terminal symbols, and the stack contains terminal symbols that have \"open\" nonterminal symbols and closed constitutions. One of the following three operations (Fig. 1) is applied successively, based on the evolving contents of the stack and the buffer: \u2022 NT (X) introduces an \"open nonterminal.\" Open nonterminals are written as a non-terminal symbol preceded by a non-terminal symbol."}, {"heading": "3.2 Generator Transitions", "text": "The parsing algorithm that maps from word strings to trees can be modified with minor modifications to create an algorithm that generates trees and word strings. There are two primary modifications: (i) There is no input buffer of unprocessed words, instead there is an output buffer (T), and (ii) instead of a SHIFT operation there is a GEN (x) operation that generates a terminal symbol x and adds it to the top of the stack and the output buffer. The algorithm ends when a single tree remains on the stack. Fig. 4 shows an exemplary generation sequence. Restrictions on generator transitions. Since the generator algorithm state has replaced the input buffer with an output buffer, the parser constraints for the generator must be reformed. These are: \u2022 The GEN (x) operation can only be applied when n value is exceeded. Since our parsing buffer has been replaced by an output buffer, &lt. There is a problem with < there is a non-permissible parameter for trees."}, {"heading": "3.3 Transition Sequences from Trees", "text": "Each parse tree can be transformed into a sequence of transitions by crossing a parse tree most deeply, from left to right. Since there is a distinct, most depth crossing of a tree, from left to right, there is exactly one transition sequence for each tree. For a tree y and a sequence of symbols x, we write a (x, y) to indicate the corresponding sequence of generation transitions, and b (x, y) to indicate the parse transitions."}, {"heading": "3.4 Runtime Analysis", "text": "A detailed analysis of the algorithmic properties of our top-down parser goes beyond the scope of this essay; however, we will briefly give some facts. Assuming the availability of constant time push and pop operations, the runtime is linear in the number of nodes in the parse tree generated by the parser / generator (intuitively this is true, because although a single REDUCE operation may require the application of a number of pops linear in the number of input symbols, the total number of pops performed over an entire parse / generation pass will also be linear).Since there is no way to bind the number of output nodes in a parse tree depending on the number of input words, specifying the runtime complexity of the parse algorithm as a function of the input size, further assumptions are required."}, {"heading": "3.5 Comparison to Other Models", "text": "Our generation algorithm differs from earlier stack-based parsing / generation algorithms in two ways: First, it constructs rooted tree structures from above (instead of from bottom to top), and second, the transition operators are able to directly create arbitrary tree structures, rather than adopting, for example, binary trees, as was the case in previous work that used transition-based algorithms to generate phrase structure trees (Sagae and Lavie, 2005; Zhang and Clark, 2011; Zhu et al., 2013), although left-angle algorithms can also create arbitrary tree structures in a bottom-up order (Henderson, 2004)."}, {"heading": "4 Generative Model", "text": "RNNGs use the generator transition that has just been introduced to define a common distribution between syntax trees (y) and words (x), which is defined as a sequence model of generator transitions parameterized by a continuous embedding of the algorithm state at each time step (ut); i.e., p (x, y) = | a (x, y) | a (at) < t = | a (x, y) | t = 1 exp r > atut + bat \u2211 a \u2032 AG (Tt, St, nt) exp > a \u2032 ut + ba \u2032, and where action-specific embedding ra and bias vector b are parameters. Representation of the algorithm state at the time t, ut, ut, is calculated by combining the representation of the three data structures of the generator."}, {"heading": "4.1 Syntactic Composition Function", "text": "To calculate an embedding of this new sub-tree, we use a composition function based on bidirectional LSTMs shown in Figure 6.An embedding of the resulting component is the first element that the LSTM reads both in the forward and backward motion of the embedding of the infant sub-trees. Intuitively, this sequence serves to \"tell\" each LSTM what type of head it should look for when processing the child-node embeddings. Similarly, the final state of the LSTMs is calculated (if it corresponds to an affinity transformation process) and a tangible nonlinearity to become a subtree embedded.3 Since each of the child-node embeddings (u, v, w in Figure 6) is a recursive composition (if it corresponds to an internal node)."}, {"heading": "4.2 Word Generation", "text": "To reduce the size of the AG (S, T, n), word formation is divided into two parts: First, the decision is made to predict GEN as an action (3We found the many previously proposed syntactic composition functions insufficient for our purposes. First, we have to deal with an unlimited number of children, and many previously proposed functions are limited to binary branch nodes (Socher et al., 2013b; Dyer et al., 2015). Second, those who could handle n-nodes have misused the non-terminal information (Tai et al., 2015), which is crucial for our task. And then we choose the word, depending on the current parser state. To further reduce the computer complexity of modeling a word, we use a class-factored softmax (Baltescu and Blunsom, 2015; Goodman, 2001)."}, {"heading": "4.3 Training", "text": "The parameters in the model are learned to maximize the probability of a tree body."}, {"heading": "4.4 Discriminative Parsing Model", "text": "A discriminatory parsing model can be achieved by replacing the embedding of Tt in each time step with an embedding of the input buffer Bt. To train this model, the conditional probability of each action sequence in view of the input string is maximized."}, {"heading": "5 Inference via Importance Sampling", "text": "Our generative model p (x, y) defines a common distribution of trees (y) and q word sequences (x). To evaluate this as a language model, we need to be able to find the MAP analysis tree w, i.e., the tree y Y (x) that maximizes p (x, y). However, to evaluate the model as a parser, we need to be able to find the MAP analysis tree w, i.e., the tree y analysis tree Y (x, y) that maximizes p (x, y). Due to the unlimited dependencies within the order of analysis actions in our model, the exact solution to one of these sequence problems is intractable. To get estimates of this, we use a variant of the importance sample (Doucet and Johansen, 2011).Our sampling algorithm uses a conditional suggestion distribution x (y | x) with the following properties: (i), y (x) > (i) and q (x)."}, {"heading": "6 Experiments", "text": "We present the results of our two models both in parsing (discriminative and generative) and as a language model (generative only) in English and Chinese. Data. for English, sections 2-21 of Penn Treebank are used as a training corpus for both, using section 24 as the discrimination rate, and section 23 is used for evaluation. Singleton words in the training corpus version 5.1 (CTB) with unknown word classes using the Berkeley mapping rule.4 Orthographic case distinctions are retained, and numbers (beyond singletons) are not standardized. For Chinese, we use Penn Chinese Treebank version 5.1 (CTB) with unknown word classes using the Berkeley mapping rule.5 For the Chinese experiments, we use a single unknown word class. Corpus statistics are presented in Table 1.6 Model and training parameters. For the discriminatory model, we have used hidden dimensions of 128 and 2-layer STMs (number of reduced dimension sets)."}, {"heading": "7 Discussion", "text": "From our experiments, it is clear that the proposed generative model is quite effective both as a parser and as a language model, which is the result of (i) the loosening of conventional assumptions of independence (e.g. context freedom) and (ii) the conclusion of continuous symbol representations alongside nonlinear models of their syntactic relationships. The main question that remains is why the discriminatory model - which has more information than the generative model - performs worse than the generative model. This pattern has previously been observed in the neural parsing of Henderson (2004), which hypothesized that larger, unstructured conditioning contexts are harder to learn and offer opportunities to overlap. Our discriminatory model conditions across history, stack and buffer, while our generative model accesses only the history and stack, the fully discriminatory model of Vinyals et al. (2015) was able to achieve results that are similar to those of our generative model, the stack and buffer, while our generative model uses only the history and stack."}, {"heading": "8 Related Work", "text": "Our language model combines work from two modeling traditions: (i) relapsing neural network language models and (ii) syntactic language modeling. Recurrent neural network language models use RNs to calculate representations of an unlimited history of words in a left-to-right language model (Zaremba et al., 2015; Mikolov et al., 2010; Elman, 1990). Syntactic language models collectively generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is extensive literature on this subject, but one strand emphasizes a bottom-up generation of the tree by using variants of shift-reducing parser actions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neural network-based model of Henderson (2004) is particularly similar when it uses a 2001 unrestricted year based on a neural architecture (2010), based only on a neural architecture."}, {"heading": "9 Outlook", "text": "RNNGs can be combined with a particle filter inference scheme (and not with that on a discriminatory parser, \u00a7 5) to create a left-to-right marginalization algorithm that runs in the expected linear time. Thus, they could be used in applications that require language models. A second possibility is to replace the sequential generation architectures found in many transduction problems of neural networks that produce sentences tied to certain inputs. Previous work in machine translation has shown that conditional syntactic models can work quite well without the random samples of the LSTM sequential language model: Analysts say they are opposed to UNK-LC-y countries for the equipment that shows that some of the lowest giraffe patrols plan the diligence purchase. \"We do not have that someone has passed the UNK-LC-ed\" or pushed up \"and has not specified the numbers,\" so we have \"new experience.\""}, {"heading": "10 Conclusion", "text": "We introduced recurring grammars of neural networks, a probabilistic model of phrase structure trees that can be trained generatively and used as a language model or parser, and a corresponding discrimination model that can be used as a parser. Apart from pre-processing phrase structures outside of vocabulary, the approach does not require feature design or transformations of treebank data. The generative model surpasses any previously published parser based on a single monitored generative model in English, and lags slightly behind the best reported generative model in Chinese. As language models, RNNGs outperform the best single-sentence language models."}, {"heading": "Acknowledgments", "text": "We thank Brendan O'Connor and Swabha Swayamdipta for their feedback on the drafts of this paper and Jan Buys, Phil Blunsom and Yue Zhang for their help in data preparation. This work was partly coordinated by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under DARPA / I2O's Low Resource Languages for Emergent Incidents (LORELEI) program under Contract No HR0011-15C-0114; partly supported by Contract No W911NF-15-1-0543 with DARPA and the Army Research Office (ARO) for publication and unlimited distribution. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. Miguel Ballesteros was supported by the European Commission under Contract No. FP7-ICT-610411 (Project MULTIA-64A) of the U.S. Government."}], "references": [{"title": "Neural machine transation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proc. ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Trainable grammars for speech recognition", "author": ["James K. Baker"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "Baker.,? \\Q1979\\E", "shortCiteRegEx": "Baker.", "year": 1979}, {"title": "Pragmatic neural modelling in machine translation", "author": ["Baltescu", "Blunsom2015] Paul Baltescu", "Phil Blunsom"], "venue": "In Proc. NAACL", "citeRegEx": "Baltescu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Baltescu et al\\.", "year": 2015}, {"title": "On the parameter space of generative lexicalized statistical parsing models", "author": ["Dan Bikel"], "venue": "Ph.D. thesis,", "citeRegEx": "Bikel.,? \\Q2004\\E", "shortCiteRegEx": "Bikel.", "year": 2004}, {"title": "An efficient implementation of a new DOP model", "author": ["Rens Bod"], "venue": "In Proc. EACL", "citeRegEx": "Bod.,? \\Q2003\\E", "shortCiteRegEx": "Bod.", "year": 2003}, {"title": "Class-based n-gram models of natural language", "author": ["Brown et al.1992] Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "2015a. A Bayesian model for generative transitionbased dependency parsing. CoRR, abs/1506.04334", "author": ["Buys", "Blunsom2015a] Jan Buys", "Phil Blunsom"], "venue": null, "citeRegEx": "Buys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Buys et al\\.", "year": 2015}, {"title": "Generative incremental dependency parsing with neural networks", "author": ["Buys", "Blunsom2015b] Jan Buys", "Phil Blunsom"], "venue": "In Proc. ACL", "citeRegEx": "Buys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Buys et al\\.", "year": 2015}, {"title": "A maximumentropy-inspired parser", "author": ["Eugene Charniak"], "venue": "In Proc. NAACL", "citeRegEx": "Charniak.,? \\Q2000\\E", "shortCiteRegEx": "Charniak.", "year": 2000}, {"title": "Top-down nearly-context-sensitive parsing", "author": ["Eugene Charniak"], "venue": "In Proc. EMNLP", "citeRegEx": "Charniak.,? \\Q2010\\E", "shortCiteRegEx": "Charniak.", "year": 2010}, {"title": "Structured language modeling", "author": ["Chelba", "Jelinek2000] Ciprian Chelba", "Frederick Jelinek"], "venue": "Computer Speech and Language,", "citeRegEx": "Chelba et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2000}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proc. EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A tutorial on particle filtering and smoothing: Fifteen years later", "author": ["Doucet", "Johansen2011] Arnaud Doucet", "Adam M. Johansen"], "venue": "In Handbook of Nonlinear Filtering. Oxford", "citeRegEx": "Doucet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Doucet et al\\.", "year": 2011}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proc. ACL", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "An efficient contextfree parsing algorithm", "author": ["Jay Earley"], "venue": "Communications of the ACM,", "citeRegEx": "Earley.,? \\Q1970\\E", "shortCiteRegEx": "Earley.", "year": 1970}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "A neural syntactic language model", "author": ["Emami", "Jelinek2005] Ahmad Emami", "Frederick Jelinek"], "venue": "Machine Learning,", "citeRegEx": "Emami et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Emami et al\\.", "year": 2005}, {"title": "Scalable inference and training of context-rich syntactic translation models", "author": ["Galley et al.2006] Michel Galley", "Jonathan Graehl", "Kevin Knight", "Daniel Marcu", "Steve DeNeefe", "Wei Wang", "Ignacio Thayer"], "venue": "In Proc. ACL", "citeRegEx": "Galley et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2006}, {"title": "Phrase dependency machine translation with quasi-synchronous tree-to-tree features", "author": ["Gimpel", "Smith2014] Kevin Gimpel", "Noah A. Smith"], "venue": "Computational Linguistics,", "citeRegEx": "Gimpel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Bengio2010] Xavier Glorot", "Yoshua Bengio"], "venue": "In Proc. ICML", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Classes for fast maximum entropy training. CoRR, cs.CL/0108006", "author": ["Joshua Goodman"], "venue": null, "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Discriminative training of a neural network statistical parser", "author": ["James Henderson"], "venue": "In Proc. ACL", "citeRegEx": "Henderson.,? \\Q2004\\E", "shortCiteRegEx": "Henderson.", "year": 2004}, {"title": "Self-training PCFG grammars with latent annotations across languages", "author": ["Huang", "Harper2009] Zhongqiang Huang", "Mary Harper"], "venue": "In Proc. EMNLP", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Computation of the probability of initial substring generation by stochastic context-free grammars", "author": ["Jelinek", "Lafferty1991] Frederick Jelinek", "John D. Lafferty"], "venue": "Computational Linguistics,", "citeRegEx": "Jelinek et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Jelinek et al\\.", "year": 1991}, {"title": "Joint and conditional estimation of tagging and parsing models", "author": ["Mark Johnson"], "venue": "In Proc. ACL", "citeRegEx": "Johnson.,? \\Q2001\\E", "shortCiteRegEx": "Johnson.", "year": 2001}, {"title": "Effective self-training for parsing", "author": ["Eugene Charniak", "Mark Johnson"], "venue": "In Proc. NAACL", "citeRegEx": "McClosky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In Proc. Interspeech", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Improved inference for unlexicalized parsing", "author": ["Petrov", "Klein2007] Slav Petrov", "Dan Klein"], "venue": "In Proc. NAACL", "citeRegEx": "Petrov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2007}, {"title": "Probabilistic top-down parsing and language modeling", "author": ["Brian Roark"], "venue": "Computational Linguistics,", "citeRegEx": "Roark.,? \\Q2001\\E", "shortCiteRegEx": "Roark.", "year": 2001}, {"title": "A classifier-based parser with linear run-time complexity", "author": ["Sagae", "Lavie2005] Kenji Sagae", "Alon Lavie"], "venue": "In Proc. IWPT", "citeRegEx": "Sagae et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sagae et al\\.", "year": 2005}, {"title": "Bayesian symbol-refined tree substitution grammars for syntactic parsing", "author": ["Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata"], "venue": "In Proc. ACL", "citeRegEx": "Shindo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "Parsing with compositional vectors", "author": ["John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proc. ACL", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proc. EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A latent variable model for generative dependency parsing", "author": ["Titov", "Henderson2007] Ivan Titov", "James Henderson"], "venue": "In Proc. IWPT", "citeRegEx": "Titov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2007}, {"title": "Grammar as a foreign language", "author": ["Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Proc. ICLR", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Joint POS tagging and transition-based constituent parsing in Chinese with non-local features", "author": ["Wang", "Xue2014] Zhiguo Wang", "Nianwen Xue"], "venue": "In Proc. ACL", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Feature optimization for constituent parsing via neural networks", "author": ["Wang et al.2015] Zhiguo Wang", "Haitao Mi", "Nianwen Xue"], "venue": "In Proc. ACL-IJCNLP", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": "In Proc. EMNLP", "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Lei Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "In Proc. ICML", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus", "author": ["Xue et al.2005] Naiwen Xue", "Fei Xia", "Fu-dong Chiou", "Marta Palmer"], "venue": null, "citeRegEx": "Xue et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "Recurrent neural network regularization", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": "In Proc. ICLR", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Syntactic processing using the generalized perceptron and beam search", "author": ["Zhang", "Clark2011] Yue Zhang", "Stephen Clark"], "venue": "Computational Linguistics,", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "Fast and accurate shift-reduce constituent parsing", "author": ["Zhu et al.2013] Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu"], "venue": "In Proc. ACL", "citeRegEx": "Zhu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 41, "context": "In the last few years, language model results that substantially improve over long-established state-ofthe-art baselines have been obtained using RNNs (Zaremba et al., 2015; Mikolov et al., 2010) as well as in various conditional language modeling tasks such as machine translation (Bahdanau et al.", "startOffset": 151, "endOffset": 195}, {"referenceID": 26, "context": "In the last few years, language model results that substantially improve over long-established state-ofthe-art baselines have been obtained using RNNs (Zaremba et al., 2015; Mikolov et al., 2010) as well as in various conditional language modeling tasks such as machine translation (Bahdanau et al.", "startOffset": 151, "endOffset": 195}, {"referenceID": 0, "context": ", 2010) as well as in various conditional language modeling tasks such as machine translation (Bahdanau et al., 2015), image caption generation (Xu et al.", "startOffset": 94, "endOffset": 117}, {"referenceID": 39, "context": ", 2015), image caption generation (Xu et al., 2015), and dialogue generation (Wen et al.", "startOffset": 34, "endOffset": 51}, {"referenceID": 38, "context": ", 2015), and dialogue generation (Wen et al., 2015).", "startOffset": 33, "endOffset": 51}, {"referenceID": 21, "context": "While several transition-based neural models of syntactic generation exist (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these rely on bottom-up construction of syntactic structure.", "startOffset": 75, "endOffset": 169}, {"referenceID": 28, "context": "Bottom-up construction orders are appealing since they permit well-known shift-reduce or leftcorner parsing algorithms to be exploited for inference; however, they limit the use of top-down grammar information, which is useful for generation (Roark, 2001).", "startOffset": 242, "endOffset": 255}, {"referenceID": 21, "context": "Surprisingly\u2014although in line with previous parsing results showing the effectiveness of generative models (Henderson, 2004; Johnson, 2001)\u2014 parsing with the generative model obtains significantly better results than parsing with the discriminative model.", "startOffset": 107, "endOffset": 139}, {"referenceID": 24, "context": "Surprisingly\u2014although in line with previous parsing results showing the effectiveness of generative models (Henderson, 2004; Johnson, 2001)\u2014 parsing with the generative model obtains significantly better results than parsing with the discriminative model.", "startOffset": 107, "endOffset": 139}, {"referenceID": 14, "context": "Our transition set is closely related to the operations used in Earley\u2019s algorithm which likewise introduces nonterminals symbols with its PREDICT operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN (Earley, 1970).", "startOffset": 240, "endOffset": 254}, {"referenceID": 12, "context": "Our transition set is closely related to the operations used in Earley\u2019s algorithm which likewise introduces nonterminals symbols with its PREDICT operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN (Earley, 1970). It is likewise closely related to the \u201clinearized\u201d parse trees proposed by Vinyals et al. (2015) and to the top-down, left-to-right decompositions of trees used in by Roark (2001) and by Charniak (2010).", "startOffset": 64, "endOffset": 353}, {"referenceID": 12, "context": "Our transition set is closely related to the operations used in Earley\u2019s algorithm which likewise introduces nonterminals symbols with its PREDICT operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN (Earley, 1970). It is likewise closely related to the \u201clinearized\u201d parse trees proposed by Vinyals et al. (2015) and to the top-down, left-to-right decompositions of trees used in by Roark (2001) and by Charniak (2010).", "startOffset": 64, "endOffset": 436}, {"referenceID": 8, "context": "(2015) and to the top-down, left-to-right decompositions of trees used in by Roark (2001) and by Charniak (2010).", "startOffset": 97, "endOffset": 113}, {"referenceID": 43, "context": ", assuming binarized trees, as is the case in prior work that has used transition-based algorithms to produce phrase-structure trees (Sagae and Lavie, 2005; Zhang and Clark, 2011; Zhu et al., 2013).", "startOffset": 133, "endOffset": 197}, {"referenceID": 21, "context": "arbitrary tree structures in a bottom-up order (Henderson, 2004).", "startOffset": 47, "endOffset": 64}, {"referenceID": 11, "context": "The output buffer, stack, and history are sequences that grow unboundedly, and to obtain representations of them we use recurrent neural networks to \u201cencode\u201d their contents (Cho et al., 2014).", "startOffset": 173, "endOffset": 191}, {"referenceID": 13, "context": "To efficiently obtain representations of S under push and pop operations, we use stack LSTMs (Dyer et al., 2015).", "startOffset": 93, "endOffset": 112}, {"referenceID": 13, "context": "First, we must contend with an unbounded number of children, and many previously proposed functions are limited to binary branching nodes (Socher et al., 2013b; Dyer et al., 2015).", "startOffset": 138, "endOffset": 179}, {"referenceID": 33, "context": "Second, those that could deal with n-ary nodes made poor use of nonterminal information (Tai et al., 2015), which is crucial for our task.", "startOffset": 88, "endOffset": 106}, {"referenceID": 20, "context": "To further reduce the computational complexity of modeling the generation of a word, we use a class-factored softmax (Baltescu and Blunsom, 2015; Goodman, 2001).", "startOffset": 117, "endOffset": 160}, {"referenceID": 5, "context": "To obtain clusters, we use the greedy agglomerative clustering algorithm of Brown et al. (1992).", "startOffset": 76, "endOffset": 96}, {"referenceID": 40, "context": "1 (CTB) (Xue et al., 2005).", "startOffset": 8, "endOffset": 26}, {"referenceID": 20, "context": "Model type F1 Henderson (2004) D 89.", "startOffset": 14, "endOffset": 31}, {"referenceID": 20, "context": "Model type F1 Henderson (2004) D 89.4 Socher et al. (2013a) D 90.", "startOffset": 14, "endOffset": 60}, {"referenceID": 20, "context": "Model type F1 Henderson (2004) D 89.4 Socher et al. (2013a) D 90.4 Zhu et al. (2013) D 90.", "startOffset": 14, "endOffset": 85}, {"referenceID": 20, "context": "Model type F1 Henderson (2004) D 89.4 Socher et al. (2013a) D 90.4 Zhu et al. (2013) D 90.4 Vinyals et al. (2015) \u2013 WSJ only D 90.", "startOffset": 14, "endOffset": 114}, {"referenceID": 20, "context": "Model type F1 Henderson (2004) D 89.4 Socher et al. (2013a) D 90.4 Zhu et al. (2013) D 90.4 Vinyals et al. (2015) \u2013 WSJ only D 90.5 Petrov and Klein (2007) G 90.", "startOffset": 14, "endOffset": 156}, {"referenceID": 4, "context": "1 Bod (2003) G 90.", "startOffset": 2, "endOffset": 13}, {"referenceID": 4, "context": "1 Bod (2003) G 90.7 Shindo et al. (2012) \u2013 single G 91.", "startOffset": 2, "endOffset": 41}, {"referenceID": 4, "context": "1 Bod (2003) G 90.7 Shindo et al. (2012) \u2013 single G 91.1 Shindo et al. (2012) \u2013 ensemble G 92.", "startOffset": 2, "endOffset": 78}, {"referenceID": 4, "context": "1 Bod (2003) G 90.7 Shindo et al. (2012) \u2013 single G 91.1 Shindo et al. (2012) \u2013 ensemble G 92.4 Zhu et al. (2013) S 91.", "startOffset": 2, "endOffset": 114}, {"referenceID": 4, "context": "1 Bod (2003) G 90.7 Shindo et al. (2012) \u2013 single G 91.1 Shindo et al. (2012) \u2013 ensemble G 92.4 Zhu et al. (2013) S 91.3 McClosky et al. (2006) S 92.", "startOffset": 2, "endOffset": 144}, {"referenceID": 4, "context": "1 Bod (2003) G 90.7 Shindo et al. (2012) \u2013 single G 91.1 Shindo et al. (2012) \u2013 ensemble G 92.4 Zhu et al. (2013) S 91.3 McClosky et al. (2006) S 92.1 Vinyals et al. (2015) \u2013 single S 92.", "startOffset": 2, "endOffset": 173}, {"referenceID": 4, "context": "1 Bod (2003) G 90.7 Shindo et al. (2012) \u2013 single G 91.1 Shindo et al. (2012) \u2013 ensemble G 92.4 Zhu et al. (2013) S 91.3 McClosky et al. (2006) S 92.1 Vinyals et al. (2015) \u2013 single S 92.5 Vinyals et al. (2015) \u2013 ensemble S 92.", "startOffset": 2, "endOffset": 211}, {"referenceID": 38, "context": "1 Model type F1 Zhu et al. (2013) D 82.", "startOffset": 16, "endOffset": 34}, {"referenceID": 33, "context": "6 Wang et al. (2015) D 83.", "startOffset": 2, "endOffset": 21}, {"referenceID": 33, "context": "6 Wang et al. (2015) D 83.2 Huang and Harper (2009) D 84.", "startOffset": 2, "endOffset": 52}, {"referenceID": 7, "context": "2 Charniak (2000) G 80.", "startOffset": 2, "endOffset": 18}, {"referenceID": 3, "context": "8 Bikel (2004) G 80.", "startOffset": 2, "endOffset": 15}, {"referenceID": 3, "context": "8 Bikel (2004) G 80.6 Petrov and Klein (2007) G 83.", "startOffset": 2, "endOffset": 46}, {"referenceID": 3, "context": "8 Bikel (2004) G 80.6 Petrov and Klein (2007) G 83.3 Zhu et al. (2013) S 85.", "startOffset": 2, "endOffset": 71}, {"referenceID": 3, "context": "8 Bikel (2004) G 80.6 Petrov and Klein (2007) G 83.3 Zhu et al. (2013) S 85.6 Wang and Xue (2014) S 86.", "startOffset": 2, "endOffset": 98}, {"referenceID": 3, "context": "8 Bikel (2004) G 80.6 Petrov and Klein (2007) G 83.3 Zhu et al. (2013) S 85.6 Wang and Xue (2014) S 86.3 Wang et al. (2015) S 86.", "startOffset": 2, "endOffset": 124}, {"referenceID": 21, "context": "This pattern has been observed before in neural parsing by Henderson (2004), who hypothesized that larger, unstructured conditioning contexts are harder to learn from, and provide opportunities to overfit.", "startOffset": 59, "endOffset": 76}, {"referenceID": 21, "context": "This pattern has been observed before in neural parsing by Henderson (2004), who hypothesized that larger, unstructured conditioning contexts are harder to learn from, and provide opportunities to overfit. Our discriminative model conditions on the entire history, stack, and buffer, while our generative model only accesses the history and stack. The fully discriminative model of Vinyals et al. (2015) was able to obtain results similar to those of our generative model (albeit using much larger training sets obtained through semisupervision) but similar results to those of our discriminative parser using the same data.", "startOffset": 59, "endOffset": 404}, {"referenceID": 41, "context": "Recurrent neural network language models use RNNs to compute representations of an unbounded history of words in a left-to-right language model (Zaremba et al., 2015; Mikolov et al., 2010; Elman, 1990).", "startOffset": 144, "endOffset": 201}, {"referenceID": 26, "context": "Recurrent neural network language models use RNNs to compute representations of an unbounded history of words in a left-to-right language model (Zaremba et al., 2015; Mikolov et al., 2010; Elman, 1990).", "startOffset": 144, "endOffset": 201}, {"referenceID": 15, "context": "Recurrent neural network language models use RNNs to compute representations of an unbounded history of words in a left-to-right language model (Zaremba et al., 2015; Mikolov et al., 2010; Elman, 1990).", "startOffset": 144, "endOffset": 201}, {"referenceID": 1, "context": "Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991).", "startOffset": 89, "endOffset": 130}, {"referenceID": 1, "context": "Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser actions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neuralnetwork\u2013based model of Henderson (2004) is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize a generative parsing model based on a leftcorner model.", "startOffset": 90, "endOffset": 426}, {"referenceID": 1, "context": "Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser actions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neuralnetwork\u2013based model of Henderson (2004) is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize a generative parsing model based on a leftcorner model. Dependency-only language models have also been explored (Titov and Henderson, 2007; Buys and Blunsom, 2015a; Buys and Blunsom, 2015b). Modeling generation top-down as a rooted branching process that recursively rewrites nonterminals has been explored by Charniak (2000) and Roark (2001).", "startOffset": 90, "endOffset": 863}, {"referenceID": 1, "context": "Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser actions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neuralnetwork\u2013based model of Henderson (2004) is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize a generative parsing model based on a leftcorner model. Dependency-only language models have also been explored (Titov and Henderson, 2007; Buys and Blunsom, 2015a; Buys and Blunsom, 2015b). Modeling generation top-down as a rooted branching process that recursively rewrites nonterminals has been explored by Charniak (2000) and Roark (2001). Of particular note is the work of Charniak (2010), which uses random forests and hand-engineered features over the entire syntactic derivation history to make decisions over the next action to take.", "startOffset": 90, "endOffset": 880}, {"referenceID": 1, "context": "Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser actions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neuralnetwork\u2013based model of Henderson (2004) is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize a generative parsing model based on a leftcorner model. Dependency-only language models have also been explored (Titov and Henderson, 2007; Buys and Blunsom, 2015a; Buys and Blunsom, 2015b). Modeling generation top-down as a rooted branching process that recursively rewrites nonterminals has been explored by Charniak (2000) and Roark (2001). Of particular note is the work of Charniak (2010), which uses random forests and hand-engineered features over the entire syntactic derivation history to make decisions over the next action to take.", "startOffset": 90, "endOffset": 931}, {"referenceID": 17, "context": "computationally expensive marginalization process at decoding time (Galley et al., 2006; Gimpel and Smith, 2014).", "startOffset": 67, "endOffset": 112}, {"referenceID": 8, "context": "While an RNNG is not a processing model (it is a grammar), the fact that it is left-to-right opens up several possibilities for developing new sentence processing models based on an explicit grammars, similar to the processing model of Charniak (2010).", "startOffset": 236, "endOffset": 252}], "year": 2016, "abstractText": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.", "creator": "LaTeX with hyperref package"}}}