{"id": "1602.07844", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2016", "title": "Fast Nonsmooth Regularized Risk Minimization with Continuation", "abstract": "In regularized risk minimization, the associated optimization problem becomes particularly difficult when both the loss and regularizer are nonsmooth. Existing approaches either have slow or unclear convergence properties, are restricted to limited problem subclasses, or require careful setting of a smoothing parameter. In this paper, we propose a continuation algorithm that is applicable to a large class of nonsmooth regularized risk minimization problems, can be flexibly used with a number of existing solvers for the underlying smoothed subproblem, and with convergence results on the whole algorithm rather than just one of its subproblems. In particular, when accelerated solvers are used, the proposed algorithm achieves the fastest known rates of $O(1/T^2)$ on strongly convex problems, and $O(1/T)$ on general convex problems. Experiments on nonsmooth classification and regression tasks demonstrate that the proposed algorithm outperforms the state-of-the-art.", "histories": [["v1", "Thu, 25 Feb 2016 08:34:59 GMT  (258kb,D)", "http://arxiv.org/abs/1602.07844v1", "AAAI-2016"]], "COMMENTS": "AAAI-2016", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["shuai zheng 0004", "ruiliang zhang", "james t kwok"], "accepted": true, "id": "1602.07844"}, "pdf": {"name": "1602.07844.pdf", "metadata": {"source": "CRF", "title": "Fast Nonsmooth Regularized Risk Minimization with Continuation", "authors": ["Shuai Zheng", "Ruiliang Zhang", "James T. Kwok"], "emails": ["jamesk}@cse.ust.hk"], "sections": [{"heading": "Introduction", "text": "It is not easy to minimize the sum of an empirical loss, but it is the sum of an empirical loss and a regularizer. (If both are so easy to implement and highly scalable (Kushner and Yin 2003), it is not easy to efficiently minimize the regulatory mechanisms (such as the \"1\" and the \"nuclear\" regulators), the corresponding regulatory risks, however, by using the proximal gradient algorithms and their accelerated variants (Kushner and Yin 2003) efficiently (e.g. the hung loss and the absolute loss), or if both the loss and the absolute loss are not smooth, proximal gradient algorithms are not directly applicable."}, {"heading": "Related Work", "text": "Consider the nonsmooth functions of Formg (x) = g (x) + max u (g) = g (g) = g) [< Ax, u > \u2212 Q (u)], (1) where g \u2212 ig is convex, continuously differentiable with L-Lipschitz continuous gradient, U Rp is convex, A-Rp \u00b7 d, and Q is a continuous convex function. Nesterov (2005b) suggested the following smooth approximation: g-ig (x) = g-value (x) + max u-value U [< Ax, u > \u2212 Q (u) \u2212 quantitative change (u)], (2) where g-value is a smoothing parameter and n-value is a non-strongly convex function. For example, consider the hinge losses g (x) = max (0, 1 \u2212 yiz T i) = maximum value (1 z), where yzi-value is linear and yzi-value."}, {"heading": "Nesterov Smoothing with Continuation", "text": "In machine learning, x usually corresponds to the model parameter, f is the loss and r is the regulator. We assume that the loss f on a set of n samples can be broken down as f (x) = 1n \u00b2 n = 1 fi (x), where fi is the loss value on the ith sample. Furthermore, any fi can be written as in (1), i.e. fi (x) = f \u00b2 i (x) + maxu \u00b2 U [< Aix, u > \u2212 Q (u)]. One can then apply Nesterov's smoothing, and P (x) in (5) is smoothed down to P (x) = f \u00b2 i (x) + r (x), (6) where f \u00b2 < ikarn i (x) = 1 \u00b7 f \u00b7 i (> i) that it can be smoothed (x)."}, {"heading": "Strongly Convex Objectives", "text": "In this section, we assume that P's is an approximate solution, which is then used for the warm start."}, {"heading": "General Convex Objectives", "text": "If P is not strongly convex, we add a small \"2-term\" (with weight dependencies) to it. We then gradually reduce the number of iterations T1, smoothing parameters and strong convectivity parameters (1 and 2 respectively). \u2212 The revised procedure is shown in Algorithm 2.Algorithm 2.Algorithm 2 CNS algorithm for general convex problems (1 each). \u2212 Target number: Number of iterations T1, smoothing parameters \u04211 and strong convectivity parameters \u04211 for stage 1, and shrinkage parameters. \u2212 Target number: x 2: for s = 1, 2,. \u2212 Target number: 4: P. \u2212 Target number: Smooth P with smoothing parameters. \u2212 Target number: 5: x. \u2212 Target number: 1 and shrinkage parameters. \u2212 Target number: x."}, {"heading": "Experiments", "text": "Due to the lack of space, we report only two data sets (Table 3) from the LIBSVM archive: (i) the popular classification data set rcv1; and (ii) YearPredictionMSD, the largest regression data in the LIBSVM archive, and is a subset of the Million Song data set. We use the hinge loss for classification and \"1 loss for regression.\" Both can be smoothed with Nesterov's smoothing (to (3) and (4), respectively. As for the regulator, we use the 1st elastic mesh regulator r (x) = 1, 2 (Zou andHastie 2005) and the problem (5) is strongly convex; and 2. '1 regulator r (x) = 1, and (5) is (general) convex."}, {"heading": "Strongly Convex Objectives", "text": "Figure 1 shows the convergence of the lens and the test performance (classification error at rcv1 and \"1 loss at YearPredictionMSD), trends are in line with Theorem 1. CNS-A is fastest (with a of O (1 / T 2)), followed by CNS-NA and Poly-SGD, both at O (1 / T) rate (from Theorem 1 and (Shamir and Zhang 2013)). The slowest are FOBOS and RDA, which converge at a rate of O (log T / T) (Duchi and Singer 2009; Xiao 2009). Figure 2 compares to the case where no continuation is used. Two fixed smoothing settings are used, \u03b3 = 10 \u2212 2 and \u03b3 = 10 \u2212 3. As can be seen, they are much slower (Propositions 2 and 3). In addition, a smaller nail leads to a slower convergence, while a larger nail leads to a worse solution, but a larger nail leads to a faster convergence."}, {"heading": "General Convex Objectives", "text": "As shown in Figure 3, the trends are again in line with Theorem 3. CNS-A is the fastest (O (1 / T) convergence rate, while the others all have a rate of O (1 / \u221a T) (Duchi and Singer 2009; Xiao 2009; Shamir and Zhang 2013). Furthermore, RDA performs better than FOBOS and Poly-SGD. Remember that Poly-SGD outperforms FOBOS and RDA for strongly convex problems. However, for general convex problems, Poly-SGD is worst, as its rate is only as good as others and does not exploit the composite structure of the problem."}, {"heading": "Conclusion", "text": "In this paper, we have proposed a continuation algorithm (CNS) for regulated risk minimization problems where both the loss and the regulator cannot be smooth. At each of its stages, the smoothed partial problem can easily be solved by existing accelerated or non-accelerated solvers. Theoretical analyses establish convergence results for the entire continuation algorithm, not just for one of its stages. Especially when using accelerated solvers, the proposed CNS algorithm achieves the rate of O (1 / T 2) for strongly convex problems and O (1 / T) for general convex problems. These are the fastest known rates for non-smooth optimization. However, CNS is advantageous as it allows the use of a regularizer (as opposed to the fastest batch algorithm) and can exploit the composite structure of the optimization problem (as opposed to the fastest stochastic algorithm)."}, {"heading": "Acknowledgments", "text": "This research was partially supported by the Hong Kong Special Administrative Region Research Grants Council (Grant 614513)."}], "references": [{"title": "Informationtheoretic lower bounds on the oracle complexity of convex optimization", "author": ["Agarwal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Agarwal,? \\Q2009\\E", "shortCiteRegEx": "Agarwal", "year": 2009}, {"title": "NESTA: A fast and accurate first-order method for sparse recovery", "author": ["Bobin Becker", "S. Cand\u00e8s 2011] Becker", "J. Bobin", "E.J. Cand\u00e8s"], "venue": "SIAM Journal on Imaging Sciences", "citeRegEx": "Becker et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2011}, {"title": "Smoothing proximal gradient method for general structured sparse regression", "author": ["Chen"], "venue": "Annals of Applied Statistics", "citeRegEx": "Chen,? \\Q2012\\E", "shortCiteRegEx": "Chen", "year": 2012}, {"title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Bach Defazio", "A. Lacoste-Julien 2014] Defazio", "F. Bach", "S. Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Defazio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Defazio et al\\.", "year": 2014}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["Duchi", "J. Singer 2009] Duchi", "Y. Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Duchi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2009}, {"title": "A fixed-point continuation method for `1regularized minimization with applications to compressed sensing", "author": ["Yin Hale", "E. Zhang 2007] Hale", "W. Yin", "Y. Zhang"], "venue": "Technical Report CAAM TR07-07,", "citeRegEx": "Hale et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hale et al\\.", "year": 2007}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Johnson", "R. Zhang 2013] Johnson", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2013}, {"title": "Stochastic Approximation and Recursive Algorithms and Applications, volume 35", "author": ["Kushner", "H.J. Yin 2003] Kushner", "G. Yin"], "venue": null, "citeRegEx": "Kushner et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kushner et al\\.", "year": 2003}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["Hastie Mazumder", "R. Tibshirani 2010] Mazumder", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["Nemirovski", "A. Yudin 1983] Nemirovski", "D. Yudin"], "venue": null, "citeRegEx": "Nemirovski et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 1983}, {"title": "PRISMA: Proximal iterative smoothing algorithm. Preprint arXiv:1206.2372", "author": ["Argyriou Orabona", "F. Srebro 2012] Orabona", "A. Argyriou", "N. Srebro"], "venue": null, "citeRegEx": "Orabona et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2012}, {"title": "Stochastic smoothing for nonsmooth minimizations: Accelerating SGD by exploiting structure", "author": ["Ouyang", "H. Gray 2012] Ouyang", "A.G. Gray"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Ouyang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ouyang et al\\.", "year": 2012}, {"title": "Proximal algorithms. Foundations and Trends in Optimization 1(3):127\u2013239", "author": ["Parikh", "N. Boyd 2014] Parikh", "S. Boyd"], "venue": null, "citeRegEx": "Parikh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2014}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Shamir Rakhlin", "A. Sridharan 2012] Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Convergence rates of inexact proximal-gradient methods for convex optimization", "author": ["Roux Schmidt", "M. Bach 2011] Schmidt", "N.L. Roux", "F.R. Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Schmidt et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2011}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Roux Schmidt", "M. Bach 2013] Schmidt", "N.L. Roux", "F. Bach"], "venue": "Preprint arXiv:1309.2388", "citeRegEx": "Schmidt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2013}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["Shalev-Shwartz", "S. Zhang 2014] Shalev-Shwartz", "T. Zhang"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2014}, {"title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes", "author": ["Shamir", "O. Zhang 2013] Shamir", "T. Zhang"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Shamir et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2013}, {"title": "A fast algorithm for sparse reconstruction based on shrinkage, subspace optimization, and continuation", "author": ["Wen"], "venue": "SIAM Journal on Scientific Computing", "citeRegEx": "Wen,? \\Q2010\\E", "shortCiteRegEx": "Wen", "year": 2010}, {"title": "A proximal-gradient homotopy method for the `1-regularized least-squares problem", "author": ["Xiao", "L. Zhang 2012] Xiao", "T. Zhang"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Xiao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2012}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Xiao", "L. Zhang 2014] Xiao", "T. Zhang"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Xiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2014}, {"title": "Regularization and variable selection via the elastic net", "author": ["Zou", "H. Hastie 2005] Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B", "citeRegEx": "Zou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 18, "context": "of gradually changing the regularization parameter in (Hale, Yin, and Zhang 2007; Wen et al. 2010; Mazumder, Hastie, and Tibshirani 2010). Empirically, continuation converges much faster than the use of a fixed smoothing parameter (Becker, Bobin, and Cand\u00e8s 2011). However, the theoretical convergence rate obtained in (Becker, Bobin, and Cand\u00e8s 2011) is only for one stage of the continuation algorithm (i.e., on the smoothed problem with a particular smoothing parameter), while the convergence properties for the whole algorithm are not clear. Recently, Xiao and Zhang (2012) obtained a linear convergence rate for their continuation algorithm, though only for the special case of `1-regularized least squares regression.", "startOffset": 82, "endOffset": 579}], "year": 2016, "abstractText": "In regularized risk minimization, the associated optimization problem becomes particularly difficult when both the loss and regularizer are nonsmooth. Existing approaches either have slow or unclear convergence properties, are restricted to limited problem subclasses, or require careful setting of a smoothing parameter. In this paper, we propose a continuation algorithm that is applicable to a large class of nonsmooth regularized risk minimization problems, can be flexibly used with a number of existing solvers for the underlying smoothed subproblem, and with convergence results on the whole algorithm rather than just one of its subproblems. In particular, when accelerated solvers are used, the proposed algorithm achieves the fastest known rates of O(1/T ) on strongly convex problems, and O(1/T ) on general convex problems. Experiments on nonsmooth classification and regression tasks demonstrate that the proposed algorithm outperforms the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}