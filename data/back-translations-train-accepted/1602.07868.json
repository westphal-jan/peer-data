{"id": "1602.07868", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2016", "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks", "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.", "histories": [["v1", "Thu, 25 Feb 2016 10:13:45 GMT  (91kb,D)", "http://arxiv.org/abs/1602.07868v1", null], ["v2", "Fri, 26 Feb 2016 08:53:23 GMT  (91kb,D)", "http://arxiv.org/abs/1602.07868v2", null], ["v3", "Sat, 4 Jun 2016 01:21:52 GMT  (61kb,D)", "http://arxiv.org/abs/1602.07868v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["tim salimans", "diederik p kingma"], "accepted": true, "id": "1602.07868"}, "pdf": {"name": "1602.07868.pdf", "metadata": {"source": "META", "title": "Weight Normalization: A Simple Reparameterization  to Accelerate Training of Deep Neural Networks", "authors": ["Tim Salimans", "Diederik P. Kingma"], "emails": ["SALIMANSTIM@GMAIL.COM", "D.P.KINGMA@UVA.NL"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. Most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves."}, {"heading": "2. Weight Normalization", "text": "We consider standard artificial neural networks, in which the calculation of each neuron consists of taking a weighted sum of input characteristics, followed by an elementary nonlinearity: y = \u03c6 (w \u00b7 x + b), (1) where w is a k-dimensional weight vector, b is a scalar bias term, x is a k-dimensional vector of input characteristics, \u03c6 (.) denotes an elementary nonlinearity such as the rectifier max (., 0) and y is the scalar output of the neuron. After we have associated a loss function with one or more neuron outputs, such a neural network is usually trained by stochastic gradient lowering in the parameters w, b, each neuron. In an effort to accelerate the convergence of this optimization process, we propose to make each weight vector a vector in relation to a parameter vector more defined and a vector a vector to a vchastic parameter."}, {"heading": "2.1. Gradients", "text": "The formation of a neural network in the new parameterization takes place with the help of the usual stochastic gradient descending methods. Here, we distinguish by (2), around the gradient of a loss function L in relation to the new parameters v, g. This results in the gradient at which \"wL\" is as usual in relation to the weights w of the gradients. Therefore, the propagation by means of weight normalization requires only a minor modification above the standard software for neural networks. The above expressions are independent of the minibatch size and cause only a minimal computational effort."}, {"heading": "2.2. Relation to batch normalization", "text": "An important source of inspiration for this repair parameterization is batch normalization (Ioffe & Szegedy, 2015), which corresponds to our method in the specific case where our network has only a single layer and the input functions for that layer are white (independently distributed with a variance of zero mean and unit).Although the exact equivalence does not normally apply to deeper architectures, we note that our weight normalization method provides much of the acceleration of full batch normalization, while offering several advantages: the computational overhead of weight normalization is much lower and does not increase stochasticity by the loud estimates of minibatch statistics. This means that our method can also be applied to models such as RNNNs and LSTMs, as well as to applications such as Reinforcement Learning, for which batch normalization is less suitable."}, {"heading": "3. Data-Dependent Initialization of Parameters", "text": "In addition to a repair parameter effect, batch normalization also has the advantage of defining the scale of characteristics generated by each level of the neural network, which makes optimization robust to parameter initializations for which these scales vary across layers. As weight normalization lacks this property, we find it important to properly initialize our parameters. We propose to extract the elements of v from a simple distribution with a fixed scale, which in our experiments represents a normal distribution with a mean zero and a standard deviation of 0.05. Before starting the training, we then initialize the b and g parameters by running an initial feedback pass through our network with a minibatch of data, using the following calculation on each neuron: t = v \u00b7 x | |, y =? (t \u2212 \u00b5 [t] \u03c3 [t] t), (4) where batch [micron and minicellular] examples are enabled."}, {"heading": "4. Mean-only Batch Normalization", "text": "The weight normalization introduced in Section 2 makes the scale of neuron activations more or less independent of the parameters. However, unlike batch normalization, the means of neuron activation still depend on v. Therefore, we are also investigating the idea of combining the weight normalization with a special version of batch normalization, which we call pure batch normalization: In this normalization method, we subtract the minibatch agents as in full batch normalization, but we do not divide them by the standard deviations of the minibatch. That is, we calculate neuron activations etc. = w \u00b7 x, t \u00b7 x, t \u00b7 p, y \u2212 p (t) (6), where w is the weight vector, parameterized by the weight normalization, and \u00b5 [t] is the minibatch mean of the preactivation operation. During training, we keep an ongoing mean of the minibatch test value, which we replace in the normalization."}, {"heading": "5. Experiments", "text": "We are experimentally testing the usefulness of our method using four different models for a wide range of applications in the areas of monitored image recognition, generative modelling and in-depth amplification learning."}, {"heading": "5.1. Supervised Classification: CIFAR-10", "text": "In fact, most of them are able to play by the rules they have established in the past, and they are able to play by the rules they have established in the past."}, {"heading": "5.2. Generative Modelling: Convolutional VAE", "text": "Next, we test the effect of weight normalization applied to deep revolutionary variational auto-encoders (CVAEs) (Kingma & Welling, 2013; Rezende et al., 2014; Salimans et al., 2015), trained on the MNIST dataset of handwritten numerical images and the CIFAR-10 dataset of small natural images. Variational auto-encoders are generative models that explain the data vector x resulting from a set of latent variables z by a common distribution of the form p (z, x) = p (z) p (x | z) p (x | z), specifying the decoder p (x | z) as a sequence sequence sequence of a neuronal network. A lower limit on the protocol logging marginal probability p (x) can be defined by approximate conclusions on the latent variable z (z, x) = p (z) (z) (z) as a sequence sequence (z) p (p) (p) (x) p (p) p (p)."}, {"heading": "5.3. Generative Modelling: DRAW", "text": "Next, we look at DRAW, a recursive generative model of (Gregor et al., 2015).DRAW is a variable auto encoder with a generative model p (z) p (x | z) and encoder q (z | x), similar to the model in Section 5.2, but with an encoder and decoder that consists of a recursive neural network consisting of Long Short-Term Memory (LSTM) units (Hochreiter & Schmidhuber, 1997).LSTM units consist of a memory cell with additive dynamics combined with input, forgetting and output gates that determine what information flows into and out of memory. Additive dynamics allows learning of extensive dependencies in the data. At each step of the model, DRAW uses the same amount of weight vectors to update the cell states of the LSTM units in its encoder and decoder."}, {"heading": "5.4. Reinforcement Learning: DQN", "text": "Next, we apply weight normalization to the problem of Reinforcement Learning for games on the Atari Learning Environment (Bellemare et al., 2013). The approach we are using is the Deep Q Network (DQN), proposed by (Mnih et al., 2015), an application for which batch normalization is not well suited: the noise introduced by estimating the minibatch statistics destabilizes the learning process. We could not get batch normalization to work for DQN without using an impractically large minibatch size. In contrast, weight normalization is easy to apply in this context, as is the initialization method of Section 3. Stochastic gradient learning is performed with Adamax (Kingma & Ba, 2014) with a dynamic of 0.5. We are looking for optimal learning rates in {0.0001, 0.0003, 0.0003, 0.0003}, 0.0003} and we find 0.0001 to work well with normal weight 001, and we generally find it to work well with 0.0001 of normal weight 001."}, {"heading": "6. Extensions", "text": "We examined several extensions of the weight normalization method described above. At this point, we are discussing two extensions that we did not use in our previous experiments, but which could be interesting for further investigation in future work."}, {"heading": "6.1. Exponential parameterization of the scale parameter g", "text": "An alternative parameterization of the weight scale is g = ecs, where s is a log scale parameter that can be learned by stochastic gradient descent, and c is a constant that throttles the effective learning rate of s. Parameterization of the g parameter in the log scale is more natural and allows it to cover a wide range of different orders of magnitude. We conducted various experiments with this parameterization with different values of c. We found that a value of c between 1 and 5 worked well, with a value of 3 appearing approximately optimal for our experiments. However, the final performance of the test sets was not significantly better or worse than the results obtained with direct learning of g in its original parameterization, which is why we did not use log scale parameterization in our recent experiments."}, {"heading": "6.2. Fixing the norm of v", "text": "If we learn a neural network with weight normalization using standard gradient parentage, the norm of v is monotonously increased by the number of weight updates, which reduces the effective learning rate on the weights in our network. This effect occurs because the non-normalized weight vector v only penetrates into our model by inserting it into the expression for the gradient given in (3). Suppose that an optimizer updates the weight vector v for each loss function V, which is necessarily orthogonal to v, where the value of vL = 0 can be verified by inserting it into the expression for the gradient given in (3). Suppose that an optimizer has to ascend / descend the weight vector v to v = v +, and that the relative norm of update that we know updates."}, {"heading": "7. Conclusion", "text": "We introduced weight normalization, a simple repair parameterization of weight vectors in a neural network that accelerates the convergence of stochastic gradient descent optimization. Weight normalization has been applied to four different models of supervised image recognition, generative modeling, and deep gain learning, and shows a consistent advantage between applications. The repair method is easy to use, requires little computing effort, and introduces no dependencies between the examples in a minibatch, making it our standard choice for developing new deep learning architectures."}], "references": [{"title": "Neural learning in structured parameter spaces natural Riemannian gradient", "author": ["S. Amari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Amari,? \\Q1997\\E", "shortCiteRegEx": "Amari", "year": 1997}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Deep learning. Book in preparation for", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-Encoding Variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "Proceedings of the 2nd International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Data-dependent initializations of convolutional neural networks", "author": ["Kr\u00e4henb\u00fchl", "Philipp", "Doersch", "Carl", "Donahue", "Jeff", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1511.06856,", "citeRegEx": "Kr\u00e4henb\u00fchl et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kr\u00e4henb\u00fchl et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Network in network", "author": ["Lin", "Min", "Qiang", "Chen", "Yan", "Shuicheng"], "venue": "In ICLR: Conference Track,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Deep learning via hessian-free optimization", "author": ["Martens", "James"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Martens and James.,? \\Q2010\\E", "shortCiteRegEx": "Martens and James.", "year": 2010}, {"title": "All you need is a good init", "author": ["Mishkin", "Dmytro", "Matas", "Jiri"], "venue": "arXiv preprint arXiv:1511.06422,", "citeRegEx": "Mishkin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mishkin et al\\.", "year": 2015}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["Polyak", "Boris T", "Juditsky", "Anatoli B"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Polyak et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Polyak et al\\.", "year": 1992}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Markov chain Monte Carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICML,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Springenberg", "Jost Tobias", "Dosovitskiy", "Alexey", "Brox", "Thomas", "Riedmiller", "Martin"], "venue": "In ICLR Workshop Track,", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "Deep learning (Goodfellow et al., 2016) is a subfield of machine learning that consists in learning models that are wholly or partially specified by a class of flexible differentiable functions called neural networks.", "startOffset": 14, "endOffset": 39}, {"referenceID": 20, "context": "If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature, and first-order gradient descent will have trouble making progress (Martens, 2010; Sutskever et al., 2013).", "startOffset": 208, "endOffset": 247}, {"referenceID": 0, "context": "The amount of curvature, and thus the success of our optimization, is not invariant to reparameterization (Amari, 1997): There may be multiple equivalent ways of parameterizing the same model, some of which are much easier to optimize than others.", "startOffset": 106, "endOffset": 119}, {"referenceID": 11, "context": "Independently from our work, this type of initialization was recently proposed by different authors (Mishkin & Matas, 2015; Kr\u00e4henb\u00fchl et al., 2015) who found such data-based initialization to work well for use with the normal parameterization.", "startOffset": 100, "endOffset": 148}, {"referenceID": 19, "context": "The model we are using is based on the ConvPool-CNN-C architecture of (Springenberg et al., 2015), with some small modifications: we replace the first dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 2\u00d7 2 max-pooling, rather than 3\u00d73.", "startOffset": 70, "endOffset": 97}, {"referenceID": 13, "context": "68% Network in Network (Lin et al., 2014) 10.", "startOffset": 23, "endOffset": 41}, {"referenceID": 19, "context": "6% ConvPool-CNN-C (Springenberg et al., 2015) 9.", "startOffset": 18, "endOffset": 45}, {"referenceID": 19, "context": "31% ALL-CNN-C (Springenberg et al., 2015) 9.", "startOffset": 14, "endOffset": 41}, {"referenceID": 17, "context": "Next, we test the effect of weight normalization applied to deep convolutional variational auto-encoders (CVAEs) (Kingma & Welling, 2013; Rezende et al., 2014; Salimans et al., 2015), trained on the MNIST dataset of images of handwritten digits and the CIFAR-10 dataset of small natural images.", "startOffset": 113, "endOffset": 182}, {"referenceID": 18, "context": "Next, we test the effect of weight normalization applied to deep convolutional variational auto-encoders (CVAEs) (Kingma & Welling, 2013; Rezende et al., 2014; Salimans et al., 2015), trained on the MNIST dataset of images of handwritten digits and the CIFAR-10 dataset of small natural images.", "startOffset": 113, "endOffset": 182}, {"referenceID": 18, "context": "We follow a similar implementation of the CVAE as in (Salimans et al., 2015) with some modifications, mainly that the encoder and decoder are parameterized with ResNet (He et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 6, "context": ", 2015) with some modifications, mainly that the encoder and decoder are parameterized with ResNet (He et al., 2015) blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.", "startOffset": 99, "endOffset": 116}, {"referenceID": 5, "context": "Next, we consider DRAW, a recurrent generative model by (Gregor et al., 2015).", "startOffset": 56, "endOffset": 77}, {"referenceID": 2, "context": "Next we apply weight normalization to the problem of Reinforcement Learning for playing games on the Atari Learning Environment (Bellemare et al., 2013).", "startOffset": 128, "endOffset": 152}], "year": 2016, "abstractText": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.", "creator": "LaTeX with hyperref package"}}}