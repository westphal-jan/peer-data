{"id": "1602.08210", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2016", "title": "Architectural Complexity Measures of Recurrent Neural Networks", "abstract": "In this paper, we systematically analyse the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN's over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the \"depth\" in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems, as we improve the state-of-the-art for sequential MNIST dataset.", "histories": [["v1", "Fri, 26 Feb 2016 06:16:27 GMT  (423kb,D)", "http://arxiv.org/abs/1602.08210v1", "19 pages, 15 figures. Under review as a conference paper at ICML 2016"], ["v2", "Mon, 29 Feb 2016 17:18:21 GMT  (423kb,D)", "http://arxiv.org/abs/1602.08210v2", "19 pages, 15 figures; comments fixed"], ["v3", "Sat, 12 Nov 2016 19:38:43 GMT  (577kb,D)", "http://arxiv.org/abs/1602.08210v3", "17 pages, 8 figures; To appear in NIPS2016"]], "COMMENTS": "19 pages, 15 figures. Under review as a conference paper at ICML 2016", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["saizheng zhang", "yuhuai wu", "tong che", "zhouhan lin", "roland memisevic", "ruslan salakhutdinov", "yoshua bengio"], "accepted": true, "id": "1602.08210"}, "pdf": {"name": "1602.08210.pdf", "metadata": {"source": "META", "title": "Architectural Complexity Measures of Recurrent Neural Networks", "authors": ["Saizheng Zhang", "Yuhuai Wu", "Tong Che", "Zhouhan Lin", "Roland Memisevic", "Ruslan Salakhutdinov", "Yoshua Bengio"], "emails": ["SAIZHENG.ZHANG@UMONTREAL.CA", "YWU@CS.TORONTO.EDU", "TONGCHE@IHES.FR", "LIN.ZHOUHAN@GMAIL.COM", "ROLAND.UMONTREAL@GMAIL.COM", "RSALAKHU@CS.TORONTO.EDU", "YOSHUA.BENGIO@GMAIL.COM"], "sections": [{"heading": "1. Introduction", "text": "This year it has come to the point that it is a purely reactionary project, in which it is a reactionary project, in which it is a reactionary, reactionary and reactionary project."}, {"heading": "2. General RNN", "text": "RNNs are learning machines that recursively compute new states by applying transition functions to previous states and inputs. It has two components: the connection architecture, which describes how information flows between different nodes, and the transition function, which describes the nonlinear transformation at each node. Normally, the connection architecture is informally illustrated by an infinitely directed acyclic graph, which in turn can be considered a finitely directed cyclic graph that unfolds over time. In this section, we first present a general definition of the connection architecture and its underlying calculation, followed by a general definition of an RNN."}, {"heading": "2.1. The Connecting Architecture", "text": "We have the idea of connecting the architecture by extending the traditional graph-based figures to a more general definition with a finite directed multigraph and its unfolded version. (There is only a cyclic representation of RNNs.) We attach \"weights\" to the edges of the cyclic graph in which Vc = Vin Vin Vhid represents a finite time delay between the source and the target node, is a directed graph that allows a variety of nodes, and Vin, Vhid are not empty. Ec \u00b7 Vc \u00b7 Z is a finite series of directed edges. Each E = (u, v,)."}, {"heading": "2.2. A General Definition of RNN", "text": "The connection architecture in section 2.1 describes how information flows between the RNN units. Let us assume that v-Vc is a node in Gc, let In (v-V) denotes the number of incoming nodes from v-V, In (v-V) = {u-V-V-V-V-Ec}. In the flow of the RNN, the transition function Fv-V-V can take the outputs of the nodes in (v-V) as inputs and calculate a new output. For example, vanilla RNs units with different activation functions, LSTMs and GRUs can all be regarded as units with specific transition functions. We now give the general definition of a RNN: Definition 2.1.A RNN is a tuple (Gc, Gun, V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V), in which Gun = (Vun, Eun) the unfolding of a cyclic graph is V-V, and {v-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V"}, {"heading": "3. Measures of Architectural Complexity", "text": "In this section, we will develop various metrics for the architectural complexity of RNNs, focusing mainly on the graphical properties of RNNs. To analyze an RNN solely from its architectural point of view, we will start from the mild assumption that the RNN is homogeneous. We will continue with the assumption that the RNN is unidirectional. It is more natural for a bidirectional RNN to measure the complexity of its unidirectional components."}, {"heading": "3.1. Recurrent Depth", "text": "Unlike other models, in which the number of increases is increased within a timeframe, RNN's map is inputs to outputs over multiple time steps. In a sense, an RNN transforms along both feeder and recurring dimensions. This fact suggests that we should examine its architectural complexity from these two different perspectives. First, we consider the recurring perspective. The conventional definition of depth is the maximum number of nonlinear transformations from inputs to outputs. Note that a guided path in an unfolded representation gun corresponds to a sequence of nonlinear transformations. In the face of an unfolded RNN graphic gun, which can be a maximum number of nonlinear transformations from inputs to outputs. Di (n) is the length of the longest path from any node start time i to any node in time i + n.From the recurrent perspective, it is natural to examine (Di) changes over time."}, {"heading": "3.3. Feedforward Depth", "text": "The recurrent depth is not completely characterized by the nature of the nonlinearity of an RNN. As the previous work suggests (Sutskever et al., 2014), stacked RNNs are not fully pronounced flat with the same hidden size on problems where a more immediate input and output process is modeled. This is not surprising, since the growth rate of Di (n) only captures the number of nonlinear transformations in the time direction, not in the depth of the feedback forward process, putting more emphasis on the specific paths that connect inputs to outputs. Since an RNN unfolded graph gun, D (n) lets the length of the longest path from each input node to each output depth i + n. Clearly, if n is small, the recurrent depth cannot serve as a good description for D (n)."}, {"heading": "3.5. Recurrent Skip Coefficient", "text": "Because models with great recurring depths have more nonlinearities over time, gradients can explode or disappear more easily. On the other hand, it is known that adding skip links over several time steps can help improve the performance of long-term dependency problems (Lin et al. (1996); Sutskever & Hinton (2010)). To measure such a \"skip effect,\" we should instead consider the length of the shortest path of time i + n. In Gun, n, let di (n) be the length of the shortest path. Similar to recurring depth, we consider the growth rate of di (n).Theorem 3.6 (Recurring Skip Coefficient).RNN and its two graphs Gun and Gc, under mild assumptions Gun 3.1), the growth rate of di (n) is zero."}, {"heading": "4. Experiments and Results", "text": "In this section, we will conduct a series of experiments to examine the following questions: (1) Is recurring depth a trivial measurement? (2) Can the improvement in depth return be improved? (3) Can increasing the recurring skip coefficient improve performance in long-term dependency tasks? (4) Does the recurring skip coefficient suggest more than simply adding skip connections? We will first show evaluations of RNNs with tanh nonlinearity and then present similar results for LSTMs."}, {"heading": "4.1. Tasks and Training Settings", "text": "Text8 dataset: We evaluate our models using character-level language modeling using the Text8 dataset, which contains 100M characters from Wikipedia, with an alphabet of the letters a-z and a space. We follow the setting of Mikolov et al. (2012): 90M for training, 5M for validation, and the remaining 5M for the test. We also divide the training sets into non-overlapping sequences, each with a length of 180. The quality of fit is evaluated by the bits-per-character metric (BPC), which is log2 of perplexity. Sequential MNIST datasets: We also evaluate our models with a modified version of the MNIST dataset. Each MNIST image dataset is transformed into a 784 x 1 sequence, turning the number classification task into a ratification."}, {"heading": "4.2. Recurrent Depth is Non-trivial", "text": "To examine the first question, we compare 4 similar connection architectures: 1-layer (flat) \"sh,\" 2-layer stacked \"st,\" 2-layer stacked with an additional connection from bottom to top \"bu,\" and 2-layer stacked with an additional connection from top to bottom \"td,\" as shown in Figure 2 (a). sh has a layer size of 512, while the rest of the networks have a layer size of 256, which means that the number of hidden parameters in each architecture remains roughly the same. We also compare our results with those in Pascanu et al. (2012) (referred to as \"P-sh\"), since they used the same model architecture as the one shown in our illustration. Although the four architectures look quite similar, they have different recurring depths: sh, st, and bu have dr = 1, whereas dd has the specific construction of the additional nonlinear transformations in the illustration, rather than simply adding a layer to the second layer."}, {"heading": "4.3. Comparing Depths", "text": "From the previous experiment, we found some evidence that performance could be improved with greater recursive depth. To further investigate the various implications of depth, we perform a systematic analysis of both recurring depth dr and advancing depth df on text8 and sequential MNIST datasets. We ensure that all models have approximately the same number of parameters (e.g. the model with dr = 1 and df = 2 has a hidden layer size of 360).Table 2 and Figure 4 show results on the text8 dataset. We observe that all models have approximately the same number of parameters (e.g. the model with dr = 1 and df = 2 has a hidden layer size of 360."}, {"heading": "4.4. Recurrent Skip Coefficients", "text": "To investigate whether increasing a recursive skip coefficient improves the performance of the model in long-term dependency tasks, we compare models with increasing the sequential MNIST problem (without / with permutation, referred to as MNIST and pMNIST). Our base model is the flat architecture proposed in Le et al. (2015). To increase the recursive skip coefficient, we add connections from time step t to time step t + k for some solid integers k, shown in Figure 5 (a). By using this specific construction, the recursive skip coefficient increases from 1 (i.e., baseline) to k. We also ensure that each model has approximately the same number of parameters: the hidden layer size of the baseline model is set to 90. Models with additional connections have 2 hidden matrices (one from t to t + 1 and the other from t to t + k)."}, {"heading": "4.5. Recurrent Skip Coefficients vs. Skip Connections", "text": "In the next series of experiments, we investigated whether the recursive skip coefficient can in fact suggest more than just adding skip connections. We design 4 specific architectures, which are shown in Figure 5 (b): (1) is the base model with a 2-layer stacked architecture, the other three models add additional skip connections in different ways. Note that these additional skip connections all exceed the same time period k, and in particular (2) and (3) share very similar architectures. Although (2), (3) and (4) all add additional skip connections, the fact that their recursive skip coefficients are different makes a big difference in terms of their recursive skip shapes: (2) has s = k2 and (4) has s = k. Therefore, although (3) and (4) all add additional skip connections, the fact that their recursive skip coefficients are different from each other is that their recursive skip shapes consist of: (3) has s = k2 and (3) has s = k."}, {"heading": "4.6. Results on LSTMs", "text": "For the first experiment, we compared 4 architectures, st, bu and td9, as defined in Figure 2 (a). LSTMs have performance advantages similar to Tanh units, see Table 6.LSTMs also showed performance gains and a much faster convergence rate when using larger s, as shown in Table 7 and Figure 6. In sequential MNIST, the LSTM already performs reasonably well with s = 3 and an increase in s did not lead to a significant improvement, while performance in pMNIST gradually improves when s increases from 4 to 6. We also observed that the LSTM network performs worse when performing MNIST compared to a Tanh RNN. A similar result was reported in Le et al. (2015)."}, {"heading": "5. Conclusion", "text": "In this paper, we first introduced a general definition of RNNs, which allows you to construct more general architectures and provides a solid framework for architectural complexity analysis. Subsequently, we proposed three architectural complexity measures: recurring depth, feedback depth and recurring skip coefficients, each covering the complexity in the long term, complexity in the short term, and the speed of information flow. We also find empirical evidence that increasing recurring depth could bring performance improvements, increasing the feedback depth might9bu and td within the conventional definition of an LSTM network are not well defined, with one node receiving only two inputs. We overcome this limitation by considering a multidimensional LSTM (Graves et al., 2007), which is explained in detail in Appendix B. 2, cannot help with long-term dependency tasks, while increasing recurring skip efficiency analysis of new architectural tasks may improve the overall architectural performance by largely addressing these long-term dependency measures."}, {"heading": "Acknowledgments", "text": "The authors thank the following agencies for funding and support: NSERC, Canada Research Chairs, CIFAR, Calcul Quebec, Compute Canada, Samsung and IARPA Raytheon BBN Contract No. D11PC20071. The authors thank the developers of Theano (Bastien et al., 2012) and Keras (Chollet, 2015) and also Nicolas Ballas, Tim Cooijmans, Ryan Lowe, Mohammad Pezeshki, Roger Grosse and Alex Schwing for their insightful comments."}, {"heading": "A. Proofs", "text": "To show Theorem 3.2, let us first consider the most general case in which dr is defined (Theorem A.1.), then discuss the mild assumptions under which we can reduce ourselves to the original limit (Proposition A.1.1), and introduce some notations used during the proof. If v = (t, p) \"Gun\" is a node in the unfolded graph, we have a corresponding node in the folded graph, denoted by v = (t, p). Theorem A.1. Given an RNN cycle graph and its unfolded representation (Gun), the node C (Gc) of the series of directed cycles in Gc."}, {"heading": "B. Experiment Details", "text": "In this section we explain the multidimensional dependence between the nodes (W (u) hu + W (p) hp + W (q) hq), which we use with the nodes (W) hu + W (p) hp + W (c) hq) hq (n)."}], "references": [{"title": "Unitary evolution recurrent neural networks", "author": ["Arjovsky", "Martin", "Shah", "Amar", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1511.06464,", "citeRegEx": "Arjovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras. GitHub repository: https:// github.com/fchollet/keras", "author": ["Chollet", "Fran\u00e7ois"], "venue": null, "citeRegEx": "Chollet and Fran\u00e7ois.,? \\Q2015\\E", "shortCiteRegEx": "Chollet and Fran\u00e7ois.", "year": 2015}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["El Hihi", "Salah", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hihi et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Hihi et al\\.", "year": 1996}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Multi-dimensional recurrent neural networks", "author": ["Graves", "Alex", "Fern\u00e1ndez", "Santiago", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the 17th International Conference on Artificial Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2007}, {"title": "Lstm: A search space odyssey", "author": ["Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Hermans", "Michiel", "Schrauwen", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermans et al\\.", "year": 2013}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["Hochreiter", "Sepp"], "venue": "Diploma, Technische Universita\u0308t Mu\u0308nchen,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1991}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Jozefowicz", "Rafal", "Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Kiros", "Ryan", "Zhu", "Yukun", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Torralba", "Antonio", "Urtasun", "Raquel", "Fidler", "Sanja"], "venue": "In NIPS,", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "A clockwork rnn", "author": ["Koutnik", "Jan", "Greff", "Klaus", "Gomez", "Faustino", "Schmidhuber", "Juergen"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Koutnik et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koutnik et al\\.", "year": 2014}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Le", "Quoc V", "Jaitly", "Navdeep", "Hinton", "Geoffrey E"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Learning longterm dependencies is not as difficult with NARX recurrent neural networks", "author": ["T. Lin", "B.G. Horne", "P. Tino", "C.L. Giles"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Lin et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Lin et al\\.", "year": 1996}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["Martens", "James", "Sutskever", "Ilya"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Martens et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2011}, {"title": "Subword language modeling with neural networks", "author": ["Mikolov", "Tom\u00e1\u0161", "Sutskever", "Ilya", "Deoras", "Anoop", "Le", "Hai-Son", "Kombrink", "Stefan"], "venue": "preprint, (http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf),", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Understanding the exploding gradient problem", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "Computing Research Repository (CoRR) abs/1211.5063,", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "How to construct deep recurrent neural networks", "author": ["Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1312.6026,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["Raiko", "Tapani", "Valpola", "Harri", "LeCun", "Yann"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Raiko et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2012}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1992}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["Srivastava", "Nitish", "Mansimov", "Elman", "Salakhutdinov", "Ruslan"], "venue": "In ICML,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Temporal-kernel recurrent neural networks", "author": ["Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "Neural Networks,", "citeRegEx": "Sutskever et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015).", "startOffset": 125, "endOffset": 231}, {"referenceID": 28, "context": "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015).", "startOffset": 125, "endOffset": 231}, {"referenceID": 26, "context": "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015).", "startOffset": 125, "endOffset": 231}, {"referenceID": 15, "context": "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015).", "startOffset": 125, "endOffset": 231}, {"referenceID": 3, "context": ", 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al.", "startOffset": 56, "endOffset": 95}, {"referenceID": 9, "context": ", 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015).", "startOffset": 101, "endOffset": 196}, {"referenceID": 4, "context": ", 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015).", "startOffset": 101, "endOffset": 196}, {"referenceID": 13, "context": ", 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015).", "startOffset": 101, "endOffset": 196}, {"referenceID": 1, "context": "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015). There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues (Martens & Sutskever, 2011; Pascanu et al., 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015). This paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since Schmidhuber (1992); El Hihi & Bengio (1996) introduced different forms of \u201cstacked RNNs\u201d, researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures.", "startOffset": 140, "endOffset": 823}, {"referenceID": 1, "context": "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015). There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues (Martens & Sutskever, 2011; Pascanu et al., 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015). This paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since Schmidhuber (1992); El Hihi & Bengio (1996) introduced different forms of \u201cstacked RNNs\u201d, researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures.", "startOffset": 140, "endOffset": 848}, {"referenceID": 1, "context": "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015). There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues (Martens & Sutskever, 2011; Pascanu et al., 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015). This paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since Schmidhuber (1992); El Hihi & Bengio (1996) introduced different forms of \u201cstacked RNNs\u201d, researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures. Some examples include Raiko et al. (2012); Graves (2013) who explored the use of skip connections; Hermans & Schrauwen (2013) who proposed the deep RNNs which are stacked RNNs with skip connections; Pascanu et al.", "startOffset": 140, "endOffset": 1074}, {"referenceID": 1, "context": "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015). There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues (Martens & Sutskever, 2011; Pascanu et al., 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015). This paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since Schmidhuber (1992); El Hihi & Bengio (1996) introduced different forms of \u201cstacked RNNs\u201d, researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures. Some examples include Raiko et al. (2012); Graves (2013) who explored the use of skip connections; Hermans & Schrauwen (2013) who proposed the deep RNNs which are stacked RNNs with skip connections; Pascanu et al.", "startOffset": 140, "endOffset": 1089}, {"referenceID": 1, "context": "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015). There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues (Martens & Sutskever, 2011; Pascanu et al., 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015). This paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since Schmidhuber (1992); El Hihi & Bengio (1996) introduced different forms of \u201cstacked RNNs\u201d, researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures. Some examples include Raiko et al. (2012); Graves (2013) who explored the use of skip connections; Hermans & Schrauwen (2013) who proposed the deep RNNs which are stacked RNNs with skip connections; Pascanu et al.", "startOffset": 140, "endOffset": 1158}, {"referenceID": 1, "context": "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015). There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues (Martens & Sutskever, 2011; Pascanu et al., 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015). This paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since Schmidhuber (1992); El Hihi & Bengio (1996) introduced different forms of \u201cstacked RNNs\u201d, researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures. Some examples include Raiko et al. (2012); Graves (2013) who explored the use of skip connections; Hermans & Schrauwen (2013) who proposed the deep RNNs which are stacked RNNs with skip connections; Pascanu et al. (2013a) who pointed out the distinction of constructing a \u201cdeep\u201d RNN from the view of the recurrent paths and the view of the input-to-hidden and hidden-to-output maps.", "startOffset": 140, "endOffset": 1254}, {"referenceID": 19, "context": "These two depths can be viewed as general extensions of the work of Pascanu et al. (2013a). We also explore a quantity called the recurrent skip coefficient which measures how quickly information propagates over time.", "startOffset": 68, "endOffset": 91}, {"referenceID": 17, "context": "Skip connections crossing different timescales have also been studied by Lin et al. (1996); El Hihi & Bengio (1996); Sutskever & Hinton (2010); Koutnik et al.", "startOffset": 73, "endOffset": 91}, {"referenceID": 17, "context": "Skip connections crossing different timescales have also been studied by Lin et al. (1996); El Hihi & Bengio (1996); Sutskever & Hinton (2010); Koutnik et al.", "startOffset": 73, "endOffset": 116}, {"referenceID": 17, "context": "Skip connections crossing different timescales have also been studied by Lin et al. (1996); El Hihi & Bengio (1996); Sutskever & Hinton (2010); Koutnik et al.", "startOffset": 73, "endOffset": 143}, {"referenceID": 16, "context": "(1996); El Hihi & Bengio (1996); Sutskever & Hinton (2010); Koutnik et al. (2014). Instead of specific architecture design, we focus on analyzing the graph-theoretic properties of recurrent skip coefficients, revealing the fundamental difference between the regular skip connections and the ones which truly increase the recurrent skip coefficients.", "startOffset": 60, "endOffset": 82}, {"referenceID": 16, "context": "Most traditional RNNs have m = 1, while some special structures like hierarchical or clockwork RNN (El Hihi & Bengio, 1996; Koutnik et al., 2014) have m > 1.", "startOffset": 99, "endOffset": 145}, {"referenceID": 28, "context": "As previous work suggests (Sutskever et al., 2014), stacked RNNs do outperform shallow ones with the same hidden size on problems where a more immediate input and output process is modeled.", "startOffset": 26, "endOffset": 50}, {"referenceID": 18, "context": "On the other hand, it is known that adding skip connections across multiple time steps may help improve the performance on long-term dependency problems (Lin et al. (1996); Sutskever & Hinton (2010)).", "startOffset": 154, "endOffset": 172}, {"referenceID": 18, "context": "On the other hand, it is known that adding skip connections across multiple time steps may help improve the performance on long-term dependency problems (Lin et al. (1996); Sutskever & Hinton (2010)).", "startOffset": 154, "endOffset": 199}, {"referenceID": 17, "context": "Each MNIST image data is reshaped into a 784 \u00d7 1 sequence, turning the digit classification task into a sequence classification one with long-term dependencies (Le et al., 2015; Arjovsky et al., 2015).", "startOffset": 160, "endOffset": 200}, {"referenceID": 0, "context": "Each MNIST image data is reshaped into a 784 \u00d7 1 sequence, turning the digit classification task into a sequence classification one with long-term dependencies (Le et al., 2015; Arjovsky et al., 2015).", "startOffset": 160, "endOffset": 200}, {"referenceID": 18, "context": "We follow the setting from Mikolov et al. (2012): 90M for training, 5M for validation and the remaining 5M for test.", "startOffset": 27, "endOffset": 49}, {"referenceID": 0, "context": ", 2015; Arjovsky et al., 2015). The model has access to 1 pixel per time step and predicts the class label at the end. A slight modification of the dataset is to permute the image sequences by a fixed random order beforehand (permuted MNIST). Since spatially local dependencies are no longer local (in the sequence), this task may be harder in terms of modelling long-term dependencies. Results in Le et al. (2015) have shown that both tanh RNNs and LSTMs did not achieve satisfying performance, which also highlights the difficulty of this task.", "startOffset": 8, "endOffset": 415}, {"referenceID": 21, "context": "We also compare our results to the ones reported in Pascanu et al. (2012) (denoted as \u201cP-sh\u201d), since they used the same model architecture as the one specified by our sh.", "startOffset": 52, "endOffset": 74}, {"referenceID": 21, "context": "We also compare our results to the ones reported in Pascanu et al. (2012) (denoted as \u201cP-sh\u201d), since they used the same model architecture as the one specified by our sh. Although the four architectures look quite similar, they have different recurrent depths: sh, st and bu have dr = 1, while td has dr = 2. Note that the specific construction of the extra nonlinear transformations in td is not conventional. Instead of simply adding intermediate layers in hidden-to-hidden connection, as reported in Pascanu et al. (2013a), more nonlinearities are gained by a recurrent flow from the first layer to the second layer and then back to the first layer at each time step (see the red path in Figure 2(a)).", "startOffset": 52, "endOffset": 526}, {"referenceID": 21, "context": "Table 1 clearly shows that the td architecture outperforms all the other architectures, including the one reported in Pascanu et al. (2012). The validation curve, displayed in Figure 3(a), further shows the gap between td and the other three architectures.", "startOffset": 118, "endOffset": 140}, {"referenceID": 17, "context": "Our baseline model is the shallow architecture proposed in Le et al. (2015). To increase the recurrent skip coefficient s, we add connections from time step t to time step t + k for some fixed integer k, shown in Figure 5 (a).", "startOffset": 59, "endOffset": 76}, {"referenceID": 0, "context": "6% on pMNIST, and achieves almost the same performance as LSTM on the MNIST dataset with only 25% number of parameters (Arjovsky et al., 2015).", "startOffset": 119, "endOffset": 142}, {"referenceID": 16, "context": "We note that our model is the first tanh RNN model that achieves good performance on this task, even improving upon the method proposed in Le et al. (2015). In addition, we also formally compare with the previous results reported in Le et al.", "startOffset": 139, "endOffset": 156}, {"referenceID": 16, "context": "We note that our model is the first tanh RNN model that achieves good performance on this task, even improving upon the method proposed in Le et al. (2015). In addition, we also formally compare with the previous results reported in Le et al. (2015); Arjovsky et al.", "startOffset": 139, "endOffset": 250}, {"referenceID": 0, "context": "(2015); Arjovsky et al. (2015), where our model (called as sTANH) has a hidden-layer size of 95, which is about the same number of parameters as in the tanh model of Arjovsky et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 0, "context": "(2015); Arjovsky et al. (2015), where our model (called as sTANH) has a hidden-layer size of 95, which is about the same number of parameters as in the tanh model of Arjovsky et al. (2015). Table 4 shows that our simple architecture improves upon the state-of-theart by 2.", "startOffset": 8, "endOffset": 189}, {"referenceID": 17, "context": "Similar result was also reported in Le et al. (2015).", "startOffset": 36, "endOffset": 53}, {"referenceID": 8, "context": "We overcome such limitation by considering a Multidimensional LSTM (Graves et al., 2007), which is explained in detail in Appendix B.", "startOffset": 67, "endOffset": 88}], "year": 2017, "abstractText": "In this paper, we systematically analyse the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graphtheoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN\u2019s over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the \u201cdepth\u201d in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems, as we improve the state-of-the-art for sequential MNIST dataset.", "creator": "LaTeX with hyperref package"}}}