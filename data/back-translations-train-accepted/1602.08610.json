{"id": "1602.08610", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2016", "title": "Scalable Bayesian Rule Lists", "abstract": "We present an algorithm for building rule lists that is two orders of magnitude faster than previous work. Rule list algorithms are competitors for decision tree algorithms. They are associative classifiers, in that they are built from pre-mined association rules. They have a logical structure that is a sequence of IF-THEN rules, identical to a decision list or one-sided decision tree. Instead of using greedy splitting and pruning like decision tree algorithms, we fully optimize over rule lists, striking a practical balance between accuracy, interpretability, and computational speed. The algorithm presented here uses a mixture of theoretical bounds (tight enough to have practical implications as a screening or bounding procedure), computational reuse, and highly tuned language libraries to achieve computational efficiency. Currently, for many practical problems, this method achieves better accuracy and sparsity than decision trees; further, in many cases, the computational time is practical and often less than that of decision trees.", "histories": [["v1", "Sat, 27 Feb 2016 16:29:24 GMT  (1106kb,D)", "http://arxiv.org/abs/1602.08610v1", "31 pages, 18 figures"], ["v2", "Mon, 3 Apr 2017 07:01:26 GMT  (943kb,D)", "http://arxiv.org/abs/1602.08610v2", "31 pages, 19 figures"]], "COMMENTS": "31 pages, 18 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hongyu yang", "cynthia rudin", "margo seltzer"], "accepted": true, "id": "1602.08610"}, "pdf": {"name": "1602.08610.pdf", "metadata": {"source": "CRF", "title": "Scalable Bayesian Rule Lists", "authors": ["Hongyu Yang", "Cynthia Rudin"], "emails": ["hongyuy@mit.edu", "rudin@mit.edu", "margo@eecs.harvard.edu"], "sections": [{"heading": "1. Introduction", "text": "This year it is so far that in the first half of the year, in the second half of the year, in the second half of the year, there will be an increase in unemployment in the second half of the year. \"We have to be in the first half of the year, in the second half of the year, in the second half of the year,\" he said. \"We have to be in the second half of the year, in the second half of the year, in the first half of the year, in the second half of the year,\" he said."}, {"heading": "2. Previous Work: Review of Bayesian Rule Lists of Letham et al. (2015)", "text": "The scalable list uses the posterior distribution of the Bayesian Rule Lists algorithm = otherwise. Our training set is {(xi, yi) ni = 1, where the xi-X encoding characteristics, and yi are terms that are binary in our case, either 0 or 1. A Bayesian decision list has the following form: if x follows the a1 rule, then y-Binomial rule, then y-1-condition Beta (\u03b1 + N1) otherwise, if x obeys, then y-Binomial-condition, then y-2-condition Beta (\u03b1 + N2)... otherwise, x obeys the then y-Binomial condition, then y-condition Beta (\u03b1 + Nm) otherwise y-condition, then y-condition Binomial-2, then y-condition Beta-2). Here, the condition-cedents condition (+ Condition + 2), then the condition-\u03b1-condition, then the condition-\u03b1-condition-\u03b1-condition, then the condition-\u03b1-\u03b1-condition, then y-condition-\u03b1-condition, then y-condition"}, {"heading": "3. Markov Chain Monte Carlo", "text": "Considering the previous parameters \u03bb, which determine the length of the list, \u03b7, which determine the desired number of conditions in the list, and \u03b1, which provides a preference over labels (normally we set all \u03b1's to 1), the algorithm, together with the set of preset rules A, must select which rules A should use, along with their sequence. We start with a simple update scheme to maximize the list of rules iteratively, which could be used for both inferences (MCMC) and simulated annealing. We only use the MCMC iterations in our experiments, but choose the MAP solution among the iterations. To define a rule list, the algorithm selects a subset of precursors from A, along with a permutation of precursors. Let's define the neighborhood of a rule list, which are all rule lists that handle spacing 1 from the original list."}, {"heading": "4. Implementation Techniques", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Expressing computation as bit vectors", "text": "The vast majority of computational time spent creating rule sets is spent determining which rules capture which observations in a particular rule sequence. Remember, the first rule for which an observation evaluates this observation requires different set operations - checking whether a set contains an element, adding an element to a set, and removing an element from a set. However, set operations are typically slow, and hardware contributes little to efficiency. We convert all set operations into logical operations to bit vectors for which hardware support is readily available. Bitvector representation is both memory and computationally efficient, and the vectors have a length that corresponds to the total number of data samples. Before starting the algorithm, we calculate for each rule the bit vector that represents the samples for which the rule generates a true value. For one million samples (or, more precisely, up to 1,048,576 observations, each containing 128 bytes)."}, {"heading": "4.2 Representing Intermediate State as Bit Vectors", "text": "Within a rule list, each observation is captured by one rule and only one rule - the first rule for which the condition applies. Presenting the rules and rule lists in this way allows us to explore the rule list state space and to reuse important calculations in the process. Consider, for example, a rule list with m rules. Suppose we want to delete rule k from the rule set. Naive implementation recalculates the \"Captures\" vector for each rule in the rule set. Our implementation updates only the rules j > k, using logical operators that are used on the rule list \"Captures\" vector for k and the \"Captures\" vector of the rule for each rule j > k. This reduces the runtime of the algorithm in practice by about 50%."}, {"heading": "4.3 An Algebra for Computation Reuse", "text": "Our use of bit vectors transforms the large number of sentence operations performed in a traditional implementation into a series of boolean operations on bit vectors, which are summarized below. In our notation, the rule list contains n rules; k and j are used to represent certain rules in the rule list. Let k.captures point to the captures vector for rule k and k.init, which is associated with each rule and specifies all observations for which the rule applies. Note that k.captures for rule k.init. Below, we show these bit vector operations for the possible MCMC steps. 1. Remove rule kfor j = k + 1 to m do tmp."}, {"heading": "4.4 High Performance Bit Manipulation", "text": "After converting expensive set operations into bit vector operations, we can now use both hardware vector statements and optimized software libraries. We examined three alternative implementations, each of which improved computing efficiency over the previous one. \u2022 First, we retained our Python implementation with additional bit operations. \u2022 Next, we used the Python Gmpy library to perform population counting. \u2022 Then, we moved the implementation from Python to C, displaying the bit vectors as packed arrays of longs and reusing the information from the previous evaluation of posteriors. \u2022 Finally, we used the GMP library, which is slow in practice for small datasets but faster for big data sets. To evaluate how each of these steps improved the computation time of the algorithm, we ran a controlled experiment where each version of the algorithm received two sets of data (according to the above three pythons) of the same set of (three steps)."}, {"heading": "5. Theoretical Bounds with Practical Implications", "text": "We demonstrate two limitations: First, we provide an upper limit on the number of rules in a maximum a posteriori rule list. This allows us to limit our search space to rule lists below a certain size if desired; second, we provide a branch and binding restriction that eliminates certain prefixes of rule lists if desired, preventing our algorithm from searching in regions of the room that have not been proven to contain the maximum a posteriori rule list."}, {"heading": "5.1 Upper bound on the number of rules in the list", "text": "Considering the number of characters, the parameters \u03bb for the size of the list and the parameters \u03b10 = \u03b1j = = all values, we can derive an upper limit for the size of a maximum a posteriori rule list. This formalizes how the previous number of rules is strong enough to exceed the probability. We consider binary rules and binary characters so that the total number of possible rules of each size can be calculated directly. In creating the upper limit, within the evidence, we will hypothetically exhaust all rules from each size category in turn, starting with the smallest sizes. We discuss this below. Let us give the number of rules that remain in the stack that have c logical conditions. The sequence of b's that we define next is a lower limit for the possible sequence of | Qc | s. Specifically, b represents the sequence of sizes of rules that would provide the smallest possible | Qc | s."}, {"heading": "5.2 Prefix Bound", "text": "Next, we provide a limit that excludes certain regions of the rulespace from consideration. If we look at a rule list starting with rules a1,.., ap., if the best possible rule list starting with a1,.., ap., cannot be surpassed the rear rule list we have found so far, then we know any rule list starting with a1,.., ap., we should stop researching rule lists starting with a1,.., ap., this is a kind of branch and bound strategy, in which we now have the entire set of lists starting with a1,.., ap., we formalize this intuition below.Denote the rule list at iteration t of dt = (at1, a t 2, a t mt, a0). The current best posteriority probability has a value of a1 that isv."}, {"heading": "5.3 Demonstrations of Theorem 2", "text": "Demonstration 1: We use the tic tac toe data set from the UCI repository (see Bache & Lichman, 2013). Each observation is a tic tac toe board after the game is over. If the X player wins, the label of the observation is 1, otherwise it is 0. Consider a rule list that begins with the following two rules: ifothen... otherwise, if o then... otherwise, if... The first rule states that the board contains an \"O\" in the lowest middle position, and the rule says nothing about other places. Intuitively, this is a particularly bad rule, as it covers a lot of possible tic tac toe boards, and cannot distinguish between winning and losing boards for the \"X\" player alone. Likewise, the second rule does not distinguish well. Therefore, we expect that any rule list that starts with these two rules works poorly. We can show this by means of the theorem. In one of three folds of the data, this rule list has a log posterior that is posted."}, {"heading": "6. Experiments", "text": "We offer a comparison of algorithms along three dimensions: solution quality (AUC range under the ROC curve), thrift and scalability. Thrift will be measured as the number of leaves in a decision tree or as the number of rules in a rule list. Scalability will be measured in the computation time whether there is a reasonable balance between these three quantities. Let us describe the number of experiments we have chosen as the basis to represent the sets of uninterpretable methods and the set of \"interpretable\" methods. To represent the class of uninterpretable methods, we have chosen logistic regression, SVM RBF, Random Forests (RF), and Increased Decision Trees (ADA). None of these methods is designed to provide thrifty classifiers; they are designed to make the yields scalable and accurately classifiable. To represent the class of \"interpretable\" algorithms, we chose CART and CART."}, {"heading": "7. Related Works and Discussion", "text": "This method is really a direct competitor to CART. The algorithm proposed here strikes a balance between accuracy, scalability and interpretability. Interpretability has long been a fundamental issue in artificial intelligence (see Ru \ufffd ping, 2006; Bratko, 1997; Dawes, 1979; Vellido, Mart \ufffd n-Guerrero, & Lisboa, 2012; Giraud-Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans, Dejaeger, Vanthienen, & Baesens, 2011; Freitas, 2014) Since the rule lists created by our method are designed in such a way that they are unlikely to be promoted or otherwise combined to form more complicated models."}, {"heading": "Acknowledgments", "text": "The authors want the partial funding from Philips, Wistron and Siemen.CodeCode for SBRL is available at the following link: https: / / github.com / Hongyuy / sbrlmod"}, {"heading": "Appendix: Proof of Theorem 1", "text": "To prove this, we will show that each rule list with more than mmax-rules has a lower addendum than the trivial empty rule list. This means that each rule list with more than mmax-terms cannot be an MAP rule list. Do you refer to this list as a trivial rule list with only the standard rule. According to the definition of d * as MAP-rule list, it has an addendum that is at least as high as the MAP-rule list. Note: Posterior (d = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "Appendix A. Appendix: Proof of Theorem 2", "text": "For rule list d = {dp, ap + 1, ap + 2, am, a0}, the following rules apply: (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (2): (1): (2): (1): (1): (1): (2): (2): (2): (2): (2): (2:::: (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2) (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2: (2): (2): (2): (2: (2): (2): (2: (2): (2): (2): (2: (2): (2: (2): (2: (): (2: (2): (2: (2): (2: (2): (): (2: (2): (2: (2): (2: (): (2: (2): (2: (2): (2: (2): (2: (2): (): (2: (2): (2): (): (2: (2: (2): (2: (2): (2): (2: (2): (2: (2):"}], "references": [{"title": "Decision lists", "author": ["M. Anthony"], "venue": "Tech. rep., CDAM Research Report LSE-CDAM-2005-", "citeRegEx": "Anthony,? 2005", "shortCiteRegEx": "Anthony", "year": 2005}, {"title": "Theory and applications of agnostic pac-learning with small decision trees", "author": ["P. Auer", "R.C. Holte", "W. Maass"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "UCI machine learning repository", "author": ["K. Bache", "M. Lichman"], "venue": null, "citeRegEx": "Bache and Lichman,? \\Q2013\\E", "shortCiteRegEx": "Bache and Lichman", "year": 2013}, {"title": "Optimal decision trees", "author": ["K.P. Bennett", "J.A. Blue"], "venue": "Tech. rep., R.P.I. Math Report No. 214,", "citeRegEx": "Bennett and Blue,? \\Q1996\\E", "shortCiteRegEx": "Bennett and Blue", "year": 1996}, {"title": "Machine learning: between accuracy and interpretability", "author": ["I. Bratko"], "venue": "Della Riccia, G., Lenz, H.-J., & Kruse, R. (Eds.), Learning, Networks and Statistics, Vol. 382 of International Centre for Mechanical Sciences, pp. 163\u2013177. Springer Vienna.", "citeRegEx": "Bratko,? 1997", "shortCiteRegEx": "Bratko", "year": 1997}, {"title": "Bayesian treed models", "author": ["H.A. Chipman", "E.I. George", "R.E. McCulloch"], "venue": "Machine Learning,", "citeRegEx": "Chipman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chipman et al\\.", "year": 2002}, {"title": "BART: Bayesian additive regression trees", "author": ["H.A. Chipman", "E.I. George", "R.E. McCulloch"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Chipman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chipman et al\\.", "year": 2010}, {"title": "The robust beauty of improper linear models in decision making", "author": ["R.M. Dawes"], "venue": "American Psychologist, 34 (7), 571\u2013582.", "citeRegEx": "Dawes,? 1979", "shortCiteRegEx": "Dawes", "year": 1979}, {"title": "Induction of shallow decision trees", "author": ["D. Dobkin", "T. Fulton", "D. Gunopulos", "S. Kasif", "S. Salzberg"], "venue": null, "citeRegEx": "Dobkin et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Dobkin et al\\.", "year": 1996}, {"title": "A fast way to produce optimal fixeddepth decision trees", "author": ["A. Farhangfar", "R. Greiner", "M. Zinkevich"], "venue": "In International Symposium on Artificial Intelligence and Mathematics (ISAIM", "citeRegEx": "Farhangfar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Farhangfar et al\\.", "year": 2008}, {"title": "Comprehensible classification models: a position paper", "author": ["A.A. Freitas"], "venue": "ACM SIGKDD Explorations Newsletter, 15 (1), 1\u201310.", "citeRegEx": "Freitas,? 2014", "shortCiteRegEx": "Freitas", "year": 2014}, {"title": "Predictive learning via rule ensembles", "author": ["J.H. Friedman", "B.E. Popescu"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Friedman and Popescu,? \\Q2008\\E", "shortCiteRegEx": "Friedman and Popescu", "year": 2008}, {"title": "Beyond predictive accuracy: what", "author": ["C. Giraud-Carrier"], "venue": "Proceedings of the ECML98 Workshop on Upgrading Learning to Meta-Level: Model Selection and Data Transformation, pp. 78\u201385.", "citeRegEx": "Giraud.Carrier,? 1998", "shortCiteRegEx": "Giraud.Carrier", "year": 1998}, {"title": "Directional decision lists. ArXiv e-prints 1508.07643", "author": ["M. Goessling", "S. Kang"], "venue": null, "citeRegEx": "Goessling and Kang,? \\Q2015\\E", "shortCiteRegEx": "Goessling and Kang", "year": 2015}, {"title": "Very simple classification rules perform well on most commonly used datasets", "author": ["R.C. Holte"], "venue": "Machine Learning, 11 (1), 63\u201391.", "citeRegEx": "Holte,? 1993", "shortCiteRegEx": "Holte", "year": 1993}, {"title": "An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models", "author": ["J. Huysmans", "K. Dejaeger", "C. Mues", "J. Vanthienen", "B. Baesens"], "venue": "Decision Support Systems,", "citeRegEx": "Huysmans et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huysmans et al\\.", "year": 2011}, {"title": "Computer-assisted keyword and document set discovery from unstructured text", "author": ["G. King", "P. Lam", "M. Roberts"], "venue": null, "citeRegEx": "King et al\\.,? \\Q2014\\E", "shortCiteRegEx": "King et al\\.", "year": 2014}, {"title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model", "author": ["B. Letham", "C. Rudin", "T.H. McCormick", "D. Madigan"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Letham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Letham et al\\.", "year": 2015}, {"title": "CMAR: accurate and efficient classification based on multiple class-association rules", "author": ["W. Li", "J. Han", "J. Pei"], "venue": "In IEEE International Conference on Data Mining,", "citeRegEx": "Li et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Li et al\\.", "year": 2001}, {"title": "Integrating classification and association rule mining", "author": ["B. Liu", "W. Hsu", "Y. Ma"], "venue": "In Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Liu et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Liu et al\\.", "year": 1998}, {"title": "Learning with decision lists of data-dependent features", "author": ["M. Marchand", "M. Sokolova"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Marchand and Sokolova,? \\Q2005\\E", "shortCiteRegEx": "Marchand and Sokolova", "year": 2005}, {"title": "Bayesian hierarchical rule modeling for predicting medical conditions", "author": ["T.H. McCormick", "C. Rudin", "D. Madigan"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "McCormick et al\\.,? \\Q2012\\E", "shortCiteRegEx": "McCormick et al\\.", "year": 2012}, {"title": "Inductive logic programming: Theory and methods", "author": ["S. Muggleton", "L. De Raedt"], "venue": "The Journal of Logic Programming,", "citeRegEx": "Muggleton and Raedt,? \\Q1994\\E", "shortCiteRegEx": "Muggleton and Raedt", "year": 1994}, {"title": "Optimal constraint-based decision tree induction from itemset lattices", "author": ["S. Nijssen", "E. Fromont"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Nijssen and Fromont,? \\Q2010\\E", "shortCiteRegEx": "Nijssen and Fromont", "year": 2010}, {"title": "Teleo-reactive programs for agent control", "author": ["N.J. Nilsson"], "venue": "Journal of Artificial Intelligence Research, 1, 139\u2013158.", "citeRegEx": "Nilsson,? 1994", "shortCiteRegEx": "Nilsson", "year": 1994}, {"title": "Learning decision lists", "author": ["R.L. Rivest"], "venue": "Machine Learning, 2 (3), 229\u2013246.", "citeRegEx": "Rivest,? 1987", "shortCiteRegEx": "Rivest", "year": 1987}, {"title": "Learning optimized lists of classification rules", "author": ["C. Rudin", "S. Ertekin"], "venue": null, "citeRegEx": "Rudin and Ertekin,? \\Q2015\\E", "shortCiteRegEx": "Rudin and Ertekin", "year": 2015}, {"title": "Learning theory analysis for association rules and sequential event prediction", "author": ["C. Rudin", "B. Letham", "D. Madigan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rudin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rudin et al\\.", "year": 2013}, {"title": "Learning interpretable models", "author": ["S. R\u00fcping"], "venue": "Ph.D. thesis, Universit\u00e4t Dortmund.", "citeRegEx": "R\u00fcping,? 2006", "shortCiteRegEx": "R\u00fcping", "year": 2006}, {"title": "To explain or to predict", "author": ["G. Shmueli"], "venue": "Statistical Science, 25 (3), 289\u2013310.", "citeRegEx": "Shmueli,? 2010", "shortCiteRegEx": "Shmueli", "year": 2010}, {"title": "The decision list machine", "author": ["M. Sokolova", "M. Marchand", "N. Japkowicz", "J. Shawe-Taylor"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sokolova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sokolova et al\\.", "year": 2003}, {"title": "Learning classification models of cognitive conditions from subtle behaviors in the digital clock drawing test. Machine Learning, First Online, 1\u201349", "author": ["W. Souillard-Mandar", "R. Davis", "C. Rudin", "R. Au", "D.J. Libon", "R. Swenson", "C.C. Price", "M. Lamar", "D.L. Penney"], "venue": null, "citeRegEx": "Souillard.Mandar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Souillard.Mandar et al\\.", "year": 2015}, {"title": "Structure of association rule classifiers: a review", "author": ["K. Vanhoof", "B. Depaire"], "venue": "In Proceedings of the International Conference on Intelligent Systems and Knowledge Engineering,", "citeRegEx": "Vanhoof and Depaire,? \\Q2010\\E", "shortCiteRegEx": "Vanhoof and Depaire", "year": 2010}, {"title": "Making machine learning models interpretable", "author": ["A. Vellido", "J.D. Mart\u0301\u0131n-Guerrero", "P.J. Lisboa"], "venue": "In Proceedings of the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning", "citeRegEx": "Vellido et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vellido et al\\.", "year": 2012}, {"title": "Causal falling rule lists", "author": ["F. Wang", "C. Rudin"], "venue": null, "citeRegEx": "Wang and Rudin,? \\Q2015\\E", "shortCiteRegEx": "Wang and Rudin", "year": 2015}, {"title": "Falling rule lists", "author": ["F. Wang", "C. Rudin"], "venue": "In Proceedings of Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "Wang and Rudin,? \\Q2015\\E", "shortCiteRegEx": "Wang and Rudin", "year": 2015}, {"title": "Trading interpretability for accuracy: Oblique treed sparse additive models", "author": ["J. Wang", "R. Fujimaki", "Y. Motohashi"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Bayesian or\u2019s of and\u2019s for interpretable classification with application to context aware recommender systems", "author": ["T. Wang", "C. Rudin", "F. Doshi", "Y. Liu", "E. Klampfl", "P. MacNeille"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Cpar: classification based on predictive association rules", "author": ["X. Yin", "J. Han"], "venue": "In Proceedings of the 2003 SIAM International Conference on Data Mining,", "citeRegEx": "Yin and Han,? \\Q2003\\E", "shortCiteRegEx": "Yin and Han", "year": 2003}, {"title": "Using decision lists to construct interpretable and parsimonious treatment", "author": ["Y. Zhang", "E. Laber", "A. Tsiatis", "M. Davidian"], "venue": "regimes. ArXiv e-prints,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 17, "context": "(i) A principled objective, which is the posterior distribution for the Bayesian Rule List (BRL) model of Letham et al.\u2019s (2015). We optimize this objective over rule lists.", "startOffset": 106, "endOffset": 129}, {"referenceID": 17, "context": "Previous Work: Review of Bayesian Rule Lists of Letham et al. (2015) Scalable Rule Lists uses the posterior distribution of the Bayesian Rule Lists algorithm.", "startOffset": 48, "endOffset": 69}, {"referenceID": 4, "context": "Interpretability has long since been a fundamental topic in artificial intelligence (see R\u00fcping, 2006; Bratko, 1997; Dawes, 1979; Vellido, Mart\u0301\u0131n-Guerrero, & Lisboa, 2012; Giraud-Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans, Dejaeger, Mues, Vanthienen, & Baesens, 2011; Freitas, 2014).", "startOffset": 84, "endOffset": 292}, {"referenceID": 7, "context": "Interpretability has long since been a fundamental topic in artificial intelligence (see R\u00fcping, 2006; Bratko, 1997; Dawes, 1979; Vellido, Mart\u0301\u0131n-Guerrero, & Lisboa, 2012; Giraud-Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans, Dejaeger, Mues, Vanthienen, & Baesens, 2011; Freitas, 2014).", "startOffset": 84, "endOffset": 292}, {"referenceID": 12, "context": "Interpretability has long since been a fundamental topic in artificial intelligence (see R\u00fcping, 2006; Bratko, 1997; Dawes, 1979; Vellido, Mart\u0301\u0131n-Guerrero, & Lisboa, 2012; Giraud-Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans, Dejaeger, Mues, Vanthienen, & Baesens, 2011; Freitas, 2014).", "startOffset": 84, "endOffset": 292}, {"referenceID": 14, "context": "Interpretability has long since been a fundamental topic in artificial intelligence (see R\u00fcping, 2006; Bratko, 1997; Dawes, 1979; Vellido, Mart\u0301\u0131n-Guerrero, & Lisboa, 2012; Giraud-Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans, Dejaeger, Mues, Vanthienen, & Baesens, 2011; Freitas, 2014).", "startOffset": 84, "endOffset": 292}, {"referenceID": 29, "context": "Interpretability has long since been a fundamental topic in artificial intelligence (see R\u00fcping, 2006; Bratko, 1997; Dawes, 1979; Vellido, Mart\u0301\u0131n-Guerrero, & Lisboa, 2012; Giraud-Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans, Dejaeger, Mues, Vanthienen, & Baesens, 2011; Freitas, 2014).", "startOffset": 84, "endOffset": 292}, {"referenceID": 10, "context": "Interpretability has long since been a fundamental topic in artificial intelligence (see R\u00fcping, 2006; Bratko, 1997; Dawes, 1979; Vellido, Mart\u0301\u0131n-Guerrero, & Lisboa, 2012; Giraud-Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans, Dejaeger, Mues, Vanthienen, & Baesens, 2011; Freitas, 2014).", "startOffset": 84, "endOffset": 292}, {"referenceID": 17, "context": "Rule lists and their variants are currently being used for text processing (King, Lam, & Roberts, 2014), discovering treatment regimes (Zhang, Laber, Tsiatis, & Davidian, 2015), and creating medical risk assessments (Letham et al., 2015; Souillard-Mandar, Davis, Rudin, Au, Libon, Swenson, Price, Lamar, & Penney, 2015), among other applications.", "startOffset": 216, "endOffset": 319}, {"referenceID": 25, "context": "Inductive logic programming (Muggleton & De Raedt, 1994), greedy top-down decision list algorithms (Rivest, 1987; Sokolova, Marchand, Japkowicz, & Shawe-Taylor, 2003; Anthony, 2005; Marchand & Sokolova, 2005; Rudin, Letham, & Madigan, 2013; Goessling & Kang, 2015), associative classification (Vanhoof & Depaire, 2010; Liu, Hsu, & Ma, 1998; Li, Han, & Pei, 2001; Yin & Han, 2003) and its Bayesian counterparts (McCormick, Rudin, & Madigan, 2012) all fall into this category.", "startOffset": 99, "endOffset": 264}, {"referenceID": 0, "context": "Inductive logic programming (Muggleton & De Raedt, 1994), greedy top-down decision list algorithms (Rivest, 1987; Sokolova, Marchand, Japkowicz, & Shawe-Taylor, 2003; Anthony, 2005; Marchand & Sokolova, 2005; Rudin, Letham, & Madigan, 2013; Goessling & Kang, 2015), associative classification (Vanhoof & Depaire, 2010; Liu, Hsu, & Ma, 1998; Li, Han, & Pei, 2001; Yin & Han, 2003) and its Bayesian counterparts (McCormick, Rudin, & Madigan, 2012) all fall into this category.", "startOffset": 99, "endOffset": 264}, {"referenceID": 3, "context": "Interpretability has long since been a fundamental topic in artificial intelligence (see R\u00fcping, 2006; Bratko, 1997; Dawes, 1979; Vellido, Mart\u0301\u0131n-Guerrero, & Lisboa, 2012; Giraud-Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans, Dejaeger, Mues, Vanthienen, & Baesens, 2011; Freitas, 2014). Because the rule lists created by our method are designed to be interpretable, one would probably not want to boost them, or combine them in other ways to form more complicated models. This contrasts with, for instance, Friedman and Popescu (2008), who linearly combine pre-mined rules.", "startOffset": 103, "endOffset": 542}, {"referenceID": 3, "context": "Interpretability has long since been a fundamental topic in artificial intelligence (see R\u00fcping, 2006; Bratko, 1997; Dawes, 1979; Vellido, Mart\u0301\u0131n-Guerrero, & Lisboa, 2012; Giraud-Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans, Dejaeger, Mues, Vanthienen, & Baesens, 2011; Freitas, 2014). Because the rule lists created by our method are designed to be interpretable, one would probably not want to boost them, or combine them in other ways to form more complicated models. This contrasts with, for instance, Friedman and Popescu (2008), who linearly combine pre-mined rules. This work enables us to globally control decision trees in a sense, which could lead to more interesting styles of trees, and different forms of interpretability. For example, one cannot easily construct a Falling Rule List with a greedy splitting method, but can construct one with a global optimization approach. A Falling Rule List Wang and Rudin (2015b) is a decision list where the probabilities of success decrease as we descend along the list.", "startOffset": 103, "endOffset": 939}, {"referenceID": 3, "context": "Interpretability has long since been a fundamental topic in artificial intelligence (see R\u00fcping, 2006; Bratko, 1997; Dawes, 1979; Vellido, Mart\u0301\u0131n-Guerrero, & Lisboa, 2012; Giraud-Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans, Dejaeger, Mues, Vanthienen, & Baesens, 2011; Freitas, 2014). Because the rule lists created by our method are designed to be interpretable, one would probably not want to boost them, or combine them in other ways to form more complicated models. This contrasts with, for instance, Friedman and Popescu (2008), who linearly combine pre-mined rules. This work enables us to globally control decision trees in a sense, which could lead to more interesting styles of trees, and different forms of interpretability. For example, one cannot easily construct a Falling Rule List with a greedy splitting method, but can construct one with a global optimization approach. A Falling Rule List Wang and Rudin (2015b) is a decision list where the probabilities of success decrease as we descend along the list. This means we can target the highest probability subgroup by checking only a few conditions. A Causal Falling Rule List (CFRL) Wang and Rudin (2015a) is another such example.", "startOffset": 103, "endOffset": 1182}, {"referenceID": 24, "context": "Teleo-reactive programs (Nilsson, 1994) use a decision list structure and could benefit from learning this structure from data.", "startOffset": 24, "endOffset": 39}, {"referenceID": 14, "context": ", Bennett & Blue, 1996; Auer, Holte, & Maass, 1995; Dobkin, Fulton, Gunopulos, Kasif, & Salzberg, 1996), mainly working with only fixed depth trees. None of these works use the systems level techniques we use to speed up computation. Farhangfar, Greiner, and Zinkevich (2008) use a screening step that reduces the number of features, using the N\u00e4\u0131ve Bayes assumption that the features are independent, given the class, and then uses dynamic programming to construct an optimal fixed-depth tree.", "startOffset": 30, "endOffset": 276}, {"referenceID": 14, "context": ", Bennett & Blue, 1996; Auer, Holte, & Maass, 1995; Dobkin, Fulton, Gunopulos, Kasif, & Salzberg, 1996), mainly working with only fixed depth trees. None of these works use the systems level techniques we use to speed up computation. Farhangfar, Greiner, and Zinkevich (2008) use a screening step that reduces the number of features, using the N\u00e4\u0131ve Bayes assumption that the features are independent, given the class, and then uses dynamic programming to construct an optimal fixed-depth tree. One particularly interesting work following this literature is that of Nijssen and Fromont (2010), which allows for pre-mined rules to form trees, but in a different way than our method or associative classifiers.", "startOffset": 30, "endOffset": 593}, {"referenceID": 14, "context": ", Bennett & Blue, 1996; Auer, Holte, & Maass, 1995; Dobkin, Fulton, Gunopulos, Kasif, & Salzberg, 1996), mainly working with only fixed depth trees. None of these works use the systems level techniques we use to speed up computation. Farhangfar, Greiner, and Zinkevich (2008) use a screening step that reduces the number of features, using the N\u00e4\u0131ve Bayes assumption that the features are independent, given the class, and then uses dynamic programming to construct an optimal fixed-depth tree. One particularly interesting work following this literature is that of Nijssen and Fromont (2010), which allows for pre-mined rules to form trees, but in a different way than our method or associative classifiers. Nijssen and Fromont (2010) has the user pre-mine all possible leaves, enumerating all conditions leading to that leaf.", "startOffset": 30, "endOffset": 736}, {"referenceID": 14, "context": ", Bennett & Blue, 1996; Auer, Holte, & Maass, 1995; Dobkin, Fulton, Gunopulos, Kasif, & Salzberg, 1996), mainly working with only fixed depth trees. None of these works use the systems level techniques we use to speed up computation. Farhangfar, Greiner, and Zinkevich (2008) use a screening step that reduces the number of features, using the N\u00e4\u0131ve Bayes assumption that the features are independent, given the class, and then uses dynamic programming to construct an optimal fixed-depth tree. One particularly interesting work following this literature is that of Nijssen and Fromont (2010), which allows for pre-mined rules to form trees, but in a different way than our method or associative classifiers. Nijssen and Fromont (2010) has the user pre-mine all possible leaves, enumerating all conditions leading to that leaf. (By contrast, in our work and in associative classification, we mine only small conjunctions, and their ordered combination creates leaves.) Nijssen and Fromont (2010) warn about issues related to running out of memory.", "startOffset": 30, "endOffset": 996}, {"referenceID": 14, "context": ", Bennett & Blue, 1996; Auer, Holte, & Maass, 1995; Dobkin, Fulton, Gunopulos, Kasif, & Salzberg, 1996), mainly working with only fixed depth trees. None of these works use the systems level techniques we use to speed up computation. Farhangfar, Greiner, and Zinkevich (2008) use a screening step that reduces the number of features, using the N\u00e4\u0131ve Bayes assumption that the features are independent, given the class, and then uses dynamic programming to construct an optimal fixed-depth tree. One particularly interesting work following this literature is that of Nijssen and Fromont (2010), which allows for pre-mined rules to form trees, but in a different way than our method or associative classifiers. Nijssen and Fromont (2010) has the user pre-mine all possible leaves, enumerating all conditions leading to that leaf. (By contrast, in our work and in associative classification, we mine only small conjunctions, and their ordered combination creates leaves.) Nijssen and Fromont (2010) warn about issues related to running out of memory. As a possible extension, the work proposed here could be modified to handle regularized empirical risk minimization, in particular it could use the objective of Rudin and Ertekin (2015), which is a balance between accuracy and sparsity of rule lists.", "startOffset": 30, "endOffset": 1234}, {"referenceID": 14, "context": ", Bennett & Blue, 1996; Auer, Holte, & Maass, 1995; Dobkin, Fulton, Gunopulos, Kasif, & Salzberg, 1996), mainly working with only fixed depth trees. None of these works use the systems level techniques we use to speed up computation. Farhangfar, Greiner, and Zinkevich (2008) use a screening step that reduces the number of features, using the N\u00e4\u0131ve Bayes assumption that the features are independent, given the class, and then uses dynamic programming to construct an optimal fixed-depth tree. One particularly interesting work following this literature is that of Nijssen and Fromont (2010), which allows for pre-mined rules to form trees, but in a different way than our method or associative classifiers. Nijssen and Fromont (2010) has the user pre-mine all possible leaves, enumerating all conditions leading to that leaf. (By contrast, in our work and in associative classification, we mine only small conjunctions, and their ordered combination creates leaves.) Nijssen and Fromont (2010) warn about issues related to running out of memory. As a possible extension, the work proposed here could be modified to handle regularized empirical risk minimization, in particular it could use the objective of Rudin and Ertekin (2015), which is a balance between accuracy and sparsity of rule lists. It could also be modified to handle disjunctive normal form classifiers, for which there are now Bayesian models analogous to the ones studied in this work (Wang, Rudin, Doshi, Liu, Klampfl, & MacNeille, 2015b). Bayesian tree models may also be able to be constructed using our setup, where one would mine rules and create a globally optimal tree (Dension, Mallick, & Smith, 1998; Chipman, George, & McCulloch, 2002, 2010). It may be logistically more difficult to code trees than lists in order to take advantage of the fast lower level computations, but this is worth further investigation. A theoretical result of Rudin et al. (2013) states that the VC (Vapnik-Chervonenkis) dimension of the set of rule lists created using pre-mined rules is exactly the size of the set of pre-mined rules.", "startOffset": 30, "endOffset": 1936}], "year": 2017, "abstractText": "We present an algorithm for building rule lists that is two orders of magnitude faster than previous work. Rule list algorithms are competitors for decision tree algorithms. They are associative classifiers, in that they are built from pre-mined association rules. They have a logical structure that is a sequence of IF-THEN rules, identical to a decision list or one-sided decision tree. Instead of using greedy splitting and pruning like decision tree algorithms, we fully optimize over rule lists, striking a practical balance between accuracy, interpretability, and computational speed. The algorithm presented here uses a mixture of theoretical bounds (tight enough to have practical implications as a screening or bounding procedure), computational reuse, and highly tuned language libraries to achieve computational efficiency. Currently, for many practical problems, this method achieves better accuracy and sparsity than decision trees; further, in many cases, the computational time is practical and often less than that of decision trees.", "creator": "LaTeX with hyperref package"}}}