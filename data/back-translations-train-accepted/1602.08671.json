{"id": "1602.08671", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2016", "title": "Lie Access Neural Turing Machine", "abstract": "Recently, Neural Turing Machine and Memory Networks have shown that adding an external memory can greatly ameliorate a traditional recurrent neural network's tendency to forget after a long period of time. Here we present a new design of an external memory, wherein memories are stored in an Euclidean key space $\\mathbb R^n$. An LSTM controller performs read and write via specialized structures called read and write heads, following the design of Neural Turing Machine. It can move a head by either providing a new address in the key space (aka random access) or moving from its previous position via a Lie group action (aka Lie access). In this way, the \"L\" and \"R\" instructions of a traditional Turing Machine is generalized to arbitrary elements of a fixed Lie group action. For this reason, we name this new model the Lie Access Neural Turing Machine, or LANTM.", "histories": [["v1", "Sun, 28 Feb 2016 04:55:19 GMT  (1474kb,D)", "http://arxiv.org/abs/1602.08671v1", null], ["v2", "Tue, 23 Aug 2016 01:23:46 GMT  (781kb,D)", "http://arxiv.org/abs/1602.08671v2", null], ["v3", "Tue, 6 Sep 2016 14:42:56 GMT  (14976kb,AD)", "http://arxiv.org/abs/1602.08671v3", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["greg yang"], "accepted": true, "id": "1602.08671"}, "pdf": {"name": "1602.08671.pdf", "metadata": {"source": "CRF", "title": "Lie Access Neural Turing Machine", "authors": ["Greg Yang"], "emails": ["gyang@college.harvard.edu"], "sections": [{"heading": null, "text": "In several basic experiments, we tested two different configurations of LANTM against an LSTM baseline. Since LANTM is end-to-end differentiable, we trained with RMSProp. We found the right configuration of LANTM to learn various permutation and arithmetic tasks and extrapolate to at least twice the input size, with the number of parameters 2 orders of magnitude below the LSTM baseline. Specifically, we trained LANTM on adding k digits for 2 \u2264 k \u2264 16, but it was able to generalize almost perfectly to 17 \u2264 k \u2264 32."}, {"heading": "1 Introduction", "text": "Recurrent neural networks (RNNs) are powerful devices that, unlike traditional neural networks, allow smooth access to external storage versions; they have achieved profound performance in various areas such as machine translation [15, 3, 1], speech recognition [4, 2], captions [13, 10, 16], and so on. However, despite such advances, RNNs still cannot maintain their memory for long periods of time, which is an obstacle to the achievement of human general intelligence. This ability enabled NTM to learn and generalize simple algorithms, and the storage network of Graves et al. [5] and storage networks of Sukhbaatar et al. [14] achieved breakthroughs toward this goal through the use of external storage, which enabled NTM to learn and generalize simple algorithms, and the storage network to complete simple QGreural tasks [6] led to long queues [6]."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Lie groups", "text": "b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b."}, {"heading": "2.2 Recurrent Neural Networks", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3 Lie Access Memory", "text": "As mentioned in the introduction, Lie Access Neural Turing Machine (LANTM) is based on the external storage architecture of the Neural Turing Machine (NTM), which can be summarized by Figure 2: a neural network controller reads and writes into a memory structure using specially designed functions called \"heads\" that respect terminology from Turing machine literature. Heads themselves have no traceable parameters, so the only learning is done by the controller. In a LANTM, the memory structure is a dictionary, with keys in a Euclidean space called Rn for a fixed n, the key space or address space; and with values (so-called memory vectors) in another Euclidean space called Rm for a fixed m (m is the memory width). In time step t, each read head converts statements from the controller into a read address k (t) r Rn, which retrieves a read address from the memory."}, {"heading": "3.1 Read", "text": "In this section we present two weighting schemes for retrieving a value from memory via an address, the main idea of which is represented by Figure 3.The read key k (t) r produces weights w (t) r (i) over all memory vectors M (t), each with addressa (t) (i), by standardizing their inverse squared distances: w (t) r (i). The weighting of the two keys k (t) is represented by Figure 3.The read key k (t) r produces weights w (t) r (i) over all memory vectors M (t), each with addressa (t) (i), by normalizing their inverse squared distances: w (t) r (i)."}, {"heading": "3.2 Write", "text": "There is no additional ingredient in writing except to add the produced memory vector m (t), its strength s (t) and its address k (t) w to the collection of memory vectors, strengths and addresses. To ensure that the memory selection works well according to the weighted average, we squeeze the values from m (t) to [\u2212 1, 1] after tanh, but a squeeze by the logistic sigmoid function is also conceivable. Without such a squeeze, a memory vector M (t) (i) with large values can dominate the output of a weight method, although it has a low weight w (t) r (i)."}, {"heading": "3.3 Addressing procedure", "text": "Here we describe how the keys k (t) r and k (t) w are produced. The procedure is the same for both read and write keys (b) (b), so we assume that we have to calculate a single key k (t). We first describe the abstraction of the process for each fixed Lie group G, which acts smoothly on the key space Rn. Specifically, the exact mechanism for making an element of G is left unspecified at the moment. In the next subsections, we discuss the special decisions of G. The controller outputs 3 things: a candidate key k (t), a mixing coefficient, or a gate, g (t), a gate key, g (t), and an action v (t), which we also call step. Tor g mixes the previous key k (t \u2212 1) with the candidate key k (t) to create a preaction key k (t), which is transforged by group c (t), which is transformed from k (t) to the final key k (t)."}, {"heading": "3.3.3 Example: The translation group R2 acting on R2", "text": "The case of the translation group is simpler: Again, the controller outputs 2 numbers a = a (t), b = b (t), so that v = (a, b) acts on a key k = (x, y) byv \u00b7 k = (x, y) + (a, b) = (x + a, y + b)."}, {"heading": "3.4 Interpolation of Lie action", "text": "For groups such as (Rn, +) there is a well-defined convex interpolation between two elements that remain in the group. For some others, such as R * \u00b7 SO (2), the rectilinear interpolation tv + (1 \u2212 t) w for t * [0, 1], v, w * G sometimes produce elements outside the group (in this case, pick up the elements and get 0), but does so with the probability of zero in an appropriate sense. Then, as for keys, we can produce the controller a candidate action v (t) and a mixing coefficient h (t) to blend smoothly with the previous action v (t \u2212 1) to produce a final action v (t): = h (t) v (t)) v (t \u2212 1)).This allows the controller to move in an interaction line (t)."}, {"heading": "4 Experiments", "text": "In fact, it is such that it is a matter of a way in which people move in the most diverse areas of the world, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live."}, {"heading": "5 Results", "text": "The results are listed in Table 3. LANTM-InvNorm was able to complete all the tasks and generalize almost perfectly to 2 x the training sizes. LANTM-SoftMax also did on the copy and duplicate tasks, but completely failed with all others that performed worse than the LSTM baseline. The baseline itself learned tasks with smaller training input sizes (bigramFlip, double, additionally) almost flawless, but the generalization to 2 x training size was pitiful for all tasks, with coarse scores of no more than 6%. We tested the learned InvNorm model on larger, arbitrarily selected input sizes. Results are summarized in Table 4. On permutation tasks, it was fairly well generalized when the training size is questioned, able to correct more than 90% of the test problems."}, {"heading": "6 Related Works and Comparison of Results", "text": "This year, the time has come for us to find a solution that is capable, that we are able, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution. \""}, {"heading": "7 Discussion", "text": "Two things about the memory strength of memory M (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i) i (t) i (t) i) i (t) i i (t) i i i i (i) i i i i (i) i (i) i (i) i (i) i (t) i (t) i (t) i) i (t) i (t) i (i) i (i) i (i) i (i) i (t) i (i) i (i) i (t) i (i) i (i) i (t) i (i) i (i) i (i) i (t) i (i) i (i) i (i) i (i) i (t) i (i) i (t) i (i) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i) i (i) i (i (i (i) i (i (i) i (i (i (i) i (i (i) i (i) i (i (i) i (i (i) i (i (i) i (i) i i (i (i) i i (i (i) i (i) i) i (i (i) i) i (i (i (i) i) i (t) i (t) i (i (i (i i i"}, {"heading": "8 Conclusion", "text": "Our experiments showed that in the 5 selected tasks, a small LANTM InvNorm was able to learn and generalize almost perfectly, while the large LSTM baseline failed in many cases. Furthermore, InvNorm as a weight scheme was significantly better than SoftMax because LANTM-SoftMax could not learn or generalize any other tasks other than copying and duplicating. Our results suggest that a smooth key space without fixed binding to the number of memories dramatically increases the learning ability and generalizability of problems with long-term correlations - but only with the correct weight scheme."}, {"heading": "A Gifs", "text": "For each task and each of LANTM-InvNorm and LANTM-SoftMax, we created a GIF of examples read and written during the learning process; the entire album is available at https: / / gfycat.com / geyang / lie _ access _ neural _ turing _ machine. Each GIF was created as follows: 1. At the end of each epoch, we randomly selected an input of the maximum training length specific to that task (in the case of an additional task, for example, two 16-digit numbers).2. We executed the model with all previously set weights on that input, and recorded the reading and writing positions in the key space, as well as the strength of each memory vector. 3. When the training is complete, we record each epoch in a separate frame and string it into a GIF file. The writing positions are marked with red circles and filled so that a darker fill color means a higher memory thickness."}, {"heading": "B Close analysis", "text": "This year, it is more than ever before in the history of the city, in which it has come as far as never before in the history of the city."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Describing Multimedia Content using Attention-based Encoder\u2013Decoder Networks", "author": ["Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio"], "venue": "[cs],", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Learning Phrase Representations using RNN Encoder- Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Speech Recognition with Deep Recurrent Neural Networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "[cs],", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Neural Turing Machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Learning to Transduce with Unbounded Memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "Jrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Grid Long Short-Term Memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "[cs],", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "[cs],", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Richard Socher"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Introduction to Smooth Manifolds", "author": ["John Lee"], "venue": "Graduate Texts in Mathematics 218. Springer,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille"], "venue": "[cs],", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "End-To-End Memory Networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "[cs],", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "[cs],", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J. Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1990}, {"title": "Memory Networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Learning to Execute", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 84, "endOffset": 94}, {"referenceID": 2, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 84, "endOffset": 94}, {"referenceID": 0, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 84, "endOffset": 94}, {"referenceID": 3, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 115, "endOffset": 121}, {"referenceID": 1, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 115, "endOffset": 121}, {"referenceID": 12, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 140, "endOffset": 152}, {"referenceID": 9, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 140, "endOffset": 152}, {"referenceID": 15, "context": "They have obtained profound achievements in diverse arenas like machine translation [15, 3, 1], speech recognition [4, 2], image captioning [13, 10, 16], and so on.", "startOffset": 140, "endOffset": 152}, {"referenceID": 4, "context": "[5] and memory networks of Sukhbaatar et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] achieved breakthroughs toward this goal by utilizing external memories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] introduced neural stacks, queues, and deques.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": ") The reader hoping to learn the basics of smooth manifolds and Lie groups can consult John Lee\u2019s excellent Introduction to Smooth Manifolds [12].", "startOffset": 141, "endOffset": 145}, {"referenceID": 16, "context": "\u03c4 is usually differentiable everywhere or almost everywhere, so that the RNN can be trained by backpropagation through time (BPTT) [17].", "startOffset": 131, "endOffset": 135}, {"referenceID": 6, "context": "In this work, we use a particular variant of RNN called the Long Short Term Memory (LSTM) [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "In models with external memories, LSTM often serves as the controller [5, 6, 20].", "startOffset": 70, "endOffset": 80}, {"referenceID": 5, "context": "In models with external memories, LSTM often serves as the controller [5, 6, 20].", "startOffset": 70, "endOffset": 80}, {"referenceID": 0, "context": "and a new address k (t) w \u2208 R, along with a scalar s \u2208 [0, 1], called the memory strength of the vector.", "startOffset": 55, "endOffset": 61}, {"referenceID": 0, "context": "The controller emits 3 things: a candidate key k\u0303 \u2208 R, a mixing coefficient, or gate, g \u2208 [0, 1], and an action v \u2208 G that we also call step.", "startOffset": 90, "endOffset": 96}, {"referenceID": 0, "context": "For some others like R\u2217\u00d7SO(2), the straight-line interpolation tv+(1\u2212t)w for t \u2208 [0, 1], v, w \u2208 G sometimes produce elements outside the group (in this case sometimes the elements cancel out and get 0), but does so with probability zero in a suitable sense.", "startOffset": 81, "endOffset": 87}, {"referenceID": 14, "context": "The baselines of our experiments are LSTMs in an encoder-decoder setup as described in [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "Let x be an integer in the range [0, 10], with zero padding in front (on the right) to make up k digits.", "startOffset": 33, "endOffset": 40}, {"referenceID": 9, "context": "Let x and y be integers in the range [0, 10], with zero padding in front (on the right) to make up k digits.", "startOffset": 37, "endOffset": 44}, {"referenceID": 18, "context": "Thus this is a different encoding of the addition problem from previous works like [19] and [8].", "startOffset": 83, "endOffset": 87}, {"referenceID": 7, "context": "Thus this is a different encoding of the addition problem from previous works like [19] and [8].", "startOffset": 92, "endOffset": 95}, {"referenceID": 5, "context": "We tested the models by drawing 100 batches of random problems and computing fine and coarse scores as in [6].", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "This is akin to restricting the head shifts of an NTM to +1 and \u22121 [5].", "startOffset": 67, "endOffset": 70}, {"referenceID": 18, "context": "[19] taught LSTM to evaluate simple python programs via curriculum learning, which included copy and addition tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "They also relied on teacher forcing, that is, \u201ceven if the LSTM made an incorrect prediction in the ith output digit, the LSTM will be provided as input the correct ith output digit for predicting the (i + 1)th digit\u201d [19].", "startOffset": 218, "endOffset": 222}, {"referenceID": 8, "context": "In Grid Long Short-Term Memory [9], Kalchbrenner et al.", "startOffset": 31, "endOffset": 34}, {"referenceID": 18, "context": "Copy Vocab Addition LSTM [19] 35 10 9 Grid LSTM [9] 20 64 15 NTM [5] 20;20* 256\u2020 Neural DeQue [6] 64;128 128 LANTM-InvNorm 64;128 128 16;32", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "Copy Vocab Addition LSTM [19] 35 10 9 Grid LSTM [9] 20 64 15 NTM [5] 20;20* 256\u2020 Neural DeQue [6] 64;128 128 LANTM-InvNorm 64;128 128 16;32", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "Copy Vocab Addition LSTM [19] 35 10 9 Grid LSTM [9] 20 64 15 NTM [5] 20;20* 256\u2020 Neural DeQue [6] 64;128 128 LANTM-InvNorm 64;128 128 16;32", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "Copy Vocab Addition LSTM [19] 35 10 9 Grid LSTM [9] 20 64 15 NTM [5] 20;20* 256\u2020 Neural DeQue [6] 64;128 128 LANTM-InvNorm 64;128 128 16;32", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "The asterisk (*) indicates that [5] gave no statistics on the actual performance comparable to the fine and coarse scores here, but here we assume that sample input/output of figure 4 of that paper is illustrative of NTM\u2019s performance on the copy task.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "[5] constructed the Neural Turing Machine by tying an LSTM or feedforward controller to a fixed size memory array via read and write heads.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "One caveat here is that each symbol in NTM\u2019s copy task is an 8-bit 3[5] gave no statistics on the actual performance comparable to the fine and coarse scores here, but here we assume that sample input/output of figure 4 of that paper is illustrative of NTM\u2019s performance on the copy task.", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "[6] designed smooth versions of stack, queue, and deque as external memories to an LSTM controller, and these models were respectively named the Neural Stack, Queue, and DeQue.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] wrote about ideas similar to those in Neural Turing Machines in the paper Memory Networks, released around the same time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] improved upon the paper to give a memory network trainable via gradient descent end-to-end, and obtained good results in language modelling along with weaker results in QA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] added a episodic memory module on top of a memory network to be able to recursively focus on parts of specific facts relevant to the question at hand.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "For example, the NTM of [5] used a memory array of 128 memory vectors each of dimension 20 in order to learn length 20 copy task, but LANTM used only half the memory size (about 64 \u00d7 20, as the input is only about length 64, and a new memory vector is created for each input character) to learn length 64 copy task.", "startOffset": 24, "endOffset": 27}], "year": 2016, "abstractText": "Recently, Neural Turing Machine and Memory Networks have shown that adding an external memory can greatly ameliorate a traditional recurrent neural network\u2019s tendency to forget after a long period of time. Here we present a new design of an external memory, wherein memories are stored in an Euclidean key space R. An LSTM controller performs read and write via specialized structures called read and write heads, following the design of Neural Turing Machine. It can move a head by either providing a new address in the key space (aka random access) or moving from its previous position via a Lie group action (aka Lie access). In this way, the \u201cL\u201d and \u201cR\u201d instructions of a traditional Turing Machine is generalized to arbitrary elements of a fixed Lie group action. For this reason, we name this new model the Lie Access Neural Turing Machine, or LANTM. We tested two different configurations of LANTM against an LSTM baseline in several basic experiments. As LANTM is differentiable end-to-end, training was done with RMSProp. We found the right configuration of LANTM to be capable of learning different permutation and arithmetic tasks and extrapolating to at least twice the input size, all with the number of parameters 2 orders of magnitude below that for the LSTM baseline. In particular, we trained LANTM on addition of k-digit numbers for 2 \u2264 k \u2264 16, but it was able to generalize almost perfectly to 17 \u2264 k \u2264 32.", "creator": "LaTeX with hyperref package"}}}