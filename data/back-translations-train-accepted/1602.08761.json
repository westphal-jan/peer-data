{"id": "1602.08761", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2016", "title": "Resource Constrained Structured Prediction", "abstract": "We study the problem of structured prediction under test-time budget constraints. We propose a novel approach applicable to a wide range of structured prediction problems in computer vision and natural language processing. Our approach seeks to adaptively generate computationally costly features during test-time in order to reduce the computational cost of prediction while maintaining prediction performance. We show that training the adaptive feature generation system can be reduced to a series of structured learning problems, resulting in efficient training using existing structured learning algorithms. This framework provides theoretical justification for several existing heuristic approaches found in literature. We evaluate our proposed adaptive system on two real-world structured prediction tasks, optical character recognition (OCR) and dependency parsing. For OCR our method cuts the feature acquisition time by half coming within a 1% margin of top accuracy. For dependency parsing we realize an overall runtime gain of 20% without significant loss in performance.", "histories": [["v1", "Sun, 28 Feb 2016 19:44:57 GMT  (458kb,D)", "https://arxiv.org/abs/1602.08761v1", null], ["v2", "Wed, 8 Jun 2016 01:31:01 GMT  (446kb,D)", "http://arxiv.org/abs/1602.08761v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.CV cs.LG", "authors": ["tolga bolukbasi", "kai-wei chang", "joseph wang", "venkatesh saligrama"], "accepted": true, "id": "1602.08761"}, "pdf": {"name": "1602.08761.pdf", "metadata": {"source": "CRF", "title": "Resource Constrained Structured Prediction", "authors": ["Tolga Bolukbasi", "Kai-Wei Chang", "Joseph Wang", "Venkatesh Saligrama"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It is time for a new beginning in the history of the European Union."}, {"heading": "2 Budgeted Structured Learning", "text": "We begin by examining the structured prediction problem and its formulation under an expected budget constraint. We then expand the formulation to include structured predictions at any time. Structured prediction: The goal of structured prediction is to learn a function F that maps from an input space X to a structured space Y. In contrast to multi-class classification, the space of output Y is not simply categorical, but is instead assumed to be an exponential output space (often of varying sizes depending on the attribute space) containing an underlying structure that is generally represented by multiple parts and relationships between parts. For example, in dependency spares x x are properties that represent a sentence (e.g. words, pos tags), and y-Y is a park tree. In a structured prediction model, the mapping function F is often modeled as the F edge of the component, with the park factor Y (x, y) being a park function."}, {"heading": "2.1 Structured Prediction Under an Expected Budget", "text": "Our goal is to reduce the cost of prediction during the test period (which represents computing time, energy consumption, etc.).We consider the case where a variety of scoring functions are available to be used for each component.In addition, each scoring function involves an evaluation problem (such as the time or energy consumption required to extract the features for the scoring function).For each example, we define a state S-S in which the space of states is defined S = {0, 1} K-C |, which is used for the K components during the forecast. In the state, the element S (k) = 1 indicates that the kth function is used for component c during the forecast. For each state S, we define the evaluation costs: c (S) = c c c c c c components Y."}, {"heading": "2.2 Anytime Structured Prediction", "text": "In many areas of application, budget constraints are unknown a priori or vary from example to example due to changing resource availability and an expected budget system as described in Section 2.1, this does not result in a practicable system. Instead, we look at the problem of learning a system that can be implemented at any time [6]. In this context, a single system is designed in such a way that when a new example arrives during the test period, the characteristics of a single system per example are learned until a single system is applied to all practicable budgets, as opposed to unique systems for each budget. We model the learning problem at any time as sequential state selection. The goal is to select a system of states starting with an initial state S0 = 0k."}, {"heading": "3 Related Work", "text": "In this context, it should be noted that this is an attempt to get to grips with the causes of the crisis. In this context, it should be noted that the crisis is a crisis that has come to a head in recent years, a crisis that has come to a head in recent years. In this context, it should be noted that the crisis has broken out in recent years not only in the United States, but also in many other countries of the world."}, {"heading": "4 Experiments", "text": "In this section, we demonstrate the effectiveness of the proposed algorithm on two structured prediction tasks in different areas: the equivalence of parsing and OCR. We report on the results both at all times and on the expected case guidelines, pointing to the latter as a one-shot policy. Our focus is mainly on policy, not on reaching the state of the art in both of these areas. At a high level, the resource cost and overhead cost strategies of three resources are applied, namely the intermediate cost of inference and the political overhead costs that decide between acquiring functions and inferencing. Some methods, as they are described, are not relevant to the functional cost and overhead cost. Other methods include inference in their policies (meta features) for selecting new functions, but do not take into account the resulting policy overhead measures."}, {"heading": "A Proofs", "text": "Theory 2.1 The target in (1) can be expressed as: n \u2211 i = 1 \u2211 S (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (max S) (Xi) + Xi (Xi) + Xi (Xi) + Xi (Xi) + Xi (Xi), S (Xi), S (Xi), S (Xi), S (S), S (Xi), S (S) max (S), S max (S), S (S), S max, S (Xi), S (S) max (S), S (Xi), S max (S), S (S) max, S (Xi), S max (S), S (Xi), S (max (S), S (S) max (Xi), S (max (S), S (Xi), S (max (S), S (Xi), S (max (S), S (max (max S), S (Xi), S (max (max S) (max S (max S) (max S) (max S (max S) (max S) (max S (max S) (max S) (max S (max S) (max S) (max S) (max S (max S) (max S (max S) (max S) (max S (max S) (max S) (max S) (max S (max S) (max S (max S) (max S (max S) (max S) (max S (max S) (max S) (max) (max S) (max S) (max S (max S) (max) (max S) (max S (max S) (max S) (max S) (max S (max) (max) (max S) (max) ("}], "references": [{"title": "Fast classification using sparse decision DAGs", "author": ["R. Busa-Fekete", "D. Benbouzid", "B. K\u00e9gl"], "venue": "29th International Conference on Machine Learning ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "IllinoisSL: A JAVA Library for Structured Prediction", "author": ["K.-W. Chang", "S. Upadhyay", "M.-W. Chang", "V. Srikumar", "D. Roth"], "venue": "arXiv preprint arXiv:1509.07179", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Classifier cascade for minimizing feature evaluation cost", "author": ["M. Chen", "K.Q. Weinberger", "O. Chapelle", "D. Kedem", "Z. Xu"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 218\u2013226", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(9):1627\u2013 1645", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "An efficient algorithm for easy-first non-directional dependency parsing", "author": ["Y. Goldberg", "M. Elhadad"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 742\u2013750. Association for Computational Linguistics", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Speedboost: Anytime prediction with uniform near-optimality", "author": ["A. Grubb", "D. Bagnell"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 458\u2013466", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamic Feature Selection for Dependency Parsing", "author": ["H. He", "H. Daum\u00e9 III", "J. Eisner"], "venue": "EMNLP, pages 1455\u20131464", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning, 37(2):183\u2013233", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Dynamic feature selection for classification on a budget", "author": ["S. Karayev", "M. Fritz", "T. Darrell"], "venue": "International Conference on Machine Learning (ICML): Workshop on Prediction with Sequential Models", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature-Cost Sensitive Learning with Submodular Trees of Classifiers", "author": ["M.J. Kusner", "W. Chen", "Q. Zhou", "Z.E. Xu", "K.Q. Weinberger", "Y. Chen"], "venue": "AAAI, pages 1939\u20131945", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, 19(2):313\u2013330", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1993}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["R. McDonald", "F. Pereira", "K. Ribarov", "J. Haji\u010d"], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 523\u2013530. Association for Computational Linguistics", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Vine pruning for efficient multi-pass dependency parsing learned prioritization for trading off accuracy and speed", "author": ["A. Rush", "S. Petrov"], "venue": "NAACL", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning Where to Sample in Structured Prediction", "author": ["T. Shi", "J. Steinhardt", "P. Liang"], "venue": "AISTATS", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Easy-first Coreference Resolution", "author": ["V. Stoyanov", "J. Eisner"], "venue": "COLING, pages 2519\u20132534. Citeseer", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning Dynamic Feature Selection for Fast Sequential Prediction", "author": ["E. Strubell", "L. Vilnis", "K. Silverstein", "A. McCallum"], "venue": "arXiv preprint arXiv:1505.06169", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Max-Margin Markov Networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "Advances in Neural Information Processing Systems, page None", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Supervised sequential classification under budget constraints", "author": ["K. Trapeznikov", "V. Saligrama"], "venue": "Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, pages 581\u2013589", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research, pages 1453\u20131484", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust real-time object detection", "author": ["P. Viola", "M. Jones"], "venue": "International Journal of Computer Vision, 4", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Model selection by linear programming", "author": ["J. Wang", "T. Bolukbasi", "K. Trapeznikov", "V. Saligrama"], "venue": "Computer Vision\u2013ECCV 2014, pages 647\u2013662. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "An LP for Sequential Learning Under Budgets", "author": ["J. Wang", "K. Trapeznikov", "V. Saligrama"], "venue": "Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, pages 987\u2013995", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic structured model selection", "author": ["D. Weiss", "B. Sapp", "B. Taskar"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2656\u20132663", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Structured Prediction Cascades", "author": ["D. Weiss", "B. Taskar"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 916\u2013923", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning adaptive value of information for structured prediction", "author": ["D.J. Weiss", "B. Taskar"], "venue": "Advances in Neural Information Processing Systems, pages 953\u2013961", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Cost-Sensitive Tree of Classifiers", "author": ["Z. Xu", "M. Kusner", "M. Chen", "K.Q. Weinberger"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 133\u2013141", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Active deformable part models inference", "author": ["M. Zhu", "N. Atanasov", "G.J. Pappas", "K. Daniilidis"], "venue": "Computer Vision\u2013ECCV 2014, pages 281\u2013296. Springer", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": ", variational inference [8]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 11, "context": "The task can be formulated as a structured prediction problem, where the inference problem concerns finding the maximum spanning trees (MSTs) in a directed graphs [12].", "startOffset": 163, "endOffset": 167}, {"referenceID": 5, "context": "We instead consider the problem of learning an anytime system [6].", "startOffset": 62, "endOffset": 65}, {"referenceID": 19, "context": ", [20, 3, 1, 9, 26, 18, 10, 21, 22]).", "startOffset": 2, "endOffset": 35}, {"referenceID": 2, "context": ", [20, 3, 1, 9, 26, 18, 10, 21, 22]).", "startOffset": 2, "endOffset": 35}, {"referenceID": 0, "context": ", [20, 3, 1, 9, 26, 18, 10, 21, 22]).", "startOffset": 2, "endOffset": 35}, {"referenceID": 8, "context": ", [20, 3, 1, 9, 26, 18, 10, 21, 22]).", "startOffset": 2, "endOffset": 35}, {"referenceID": 25, "context": ", [20, 3, 1, 9, 26, 18, 10, 21, 22]).", "startOffset": 2, "endOffset": 35}, {"referenceID": 17, "context": ", [20, 3, 1, 9, 26, 18, 10, 21, 22]).", "startOffset": 2, "endOffset": 35}, {"referenceID": 9, "context": ", [20, 3, 1, 9, 26, 18, 10, 21, 22]).", "startOffset": 2, "endOffset": 35}, {"referenceID": 20, "context": ", [20, 3, 1, 9, 26, 18, 10, 21, 22]).", "startOffset": 2, "endOffset": 35}, {"referenceID": 21, "context": ", [20, 3, 1, 9, 26, 18, 10, 21, 22]).", "startOffset": 2, "endOffset": 35}, {"referenceID": 15, "context": "[16] improve the speed of a parser that operates on search-based structured prediction models, where joint prediction is decomposed to a sequence of decisions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23, 25], who present a scheme for adaptive feature selection assuming the computational costs of policy execution and inference are negligible.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[23, 25], who present a scheme for adaptive feature selection assuming the computational costs of policy execution and inference are negligible.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "3 in [25]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "[7] use imitation learning to adaptively select features for dependency parsing.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Methods to increase the speed of inference (predicting the given part responses) have been proposed [24, 14].", "startOffset": 100, "endOffset": 108}, {"referenceID": 13, "context": "Methods to increase the speed of inference (predicting the given part responses) have been proposed [24, 14].", "startOffset": 100, "endOffset": 108}, {"referenceID": 3, "context": "More focused research has improved the speed of individual algorithms such as object detection using deformable parts models [4, 27] and dependency parsing [7, 16].", "startOffset": 125, "endOffset": 132}, {"referenceID": 26, "context": "More focused research has improved the speed of individual algorithms such as object detection using deformable parts models [4, 27] and dependency parsing [7, 16].", "startOffset": 125, "endOffset": 132}, {"referenceID": 6, "context": "More focused research has improved the speed of individual algorithms such as object detection using deformable parts models [4, 27] and dependency parsing [7, 16].", "startOffset": 156, "endOffset": 163}, {"referenceID": 15, "context": "More focused research has improved the speed of individual algorithms such as object detection using deformable parts models [4, 27] and dependency parsing [7, 16].", "startOffset": 156, "endOffset": 163}, {"referenceID": 4, "context": "Adaptive features approaches have been designed to improve accuracy, including easy-first decoding strategies [5, 15], however these methods focus on performance as opposed to computational cost.", "startOffset": 110, "endOffset": 117}, {"referenceID": 14, "context": "Adaptive features approaches have been designed to improve accuracy, including easy-first decoding strategies [5, 15], however these methods focus on performance as opposed to computational cost.", "startOffset": 110, "endOffset": 117}, {"referenceID": 24, "context": "We compare our system to the Q-learning approach in [25] and two baselines: a uniform policy and a myopic policy.", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "The equivalent policy of [16] applied to our inference algorithm is marked as the myopic policy in our experiments.", "startOffset": 25, "endOffset": 29}, {"referenceID": 24, "context": "[25] for the OCR dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] to the structured prediction case.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Finally, we compare against the Q-learning method proposed by [25].", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "We adopt Structured-SVM [19] to solve the policy learning problems for expected and anytime cases defined in (4) and (8), respectively.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "Optical Character Recognition We tested our algorithm on a sequence-label problem, the OCR dataset [17] composed of 6,877 handwritten words, where each word is represented as a sequence of 16x8 binary letter images.", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "We use a linear-chain Markov model, and similar to the setup in [23, 21], use raw pixel values and HOG features with 3x3 cell size as our feature templates.", "startOffset": 64, "endOffset": 72}, {"referenceID": 20, "context": "We use a linear-chain Markov model, and similar to the setup in [23, 21], use raw pixel values and HOG features with 3x3 cell size as our feature templates.", "startOffset": 64, "endOffset": 72}, {"referenceID": 6, "context": "Dependency Parsing We follow the setting in [7] and conduct experiments on English Penn Treebank (PTB) corpus [11].", "startOffset": 44, "endOffset": 47}, {"referenceID": 10, "context": "Dependency Parsing We follow the setting in [7] and conduct experiments on English Penn Treebank (PTB) corpus [11].", "startOffset": 110, "endOffset": 114}, {"referenceID": 11, "context": "All algorithms are implemented based on the graph-based dependency parser [12] in Illinois-SL library [2], where the code is optimized for speed.", "startOffset": 74, "endOffset": 78}, {"referenceID": 1, "context": "All algorithms are implemented based on the graph-based dependency parser [12] in Illinois-SL library [2], where the code is optimized for speed.", "startOffset": 102, "endOffset": 105}, {"referenceID": 11, "context": "3 The first (\u03c8Full) considers the part-of-speech (POS) tags and lexicons of xi, xj , and their surrounding words (see [12]).", "startOffset": 118, "endOffset": 122}, {"referenceID": 6, "context": "3 in [7]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 15, "context": "In addition, greedy-style parser such as [16] might be faster by nature.", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "When we apply the length dictionary filtering heuristic in [7, 13], our parser achieves 89.", "startOffset": 59, "endOffset": 66}, {"referenceID": 12, "context": "When we apply the length dictionary filtering heuristic in [7, 13], our parser achieves 89.", "startOffset": 59, "endOffset": 66}, {"referenceID": 6, "context": "In contrast, the baseline system in [7] is slow than us by about three times.", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "When operating in the accuracy level of 90%, Figure 3 in [7] shows that their final system takes about 20s.", "startOffset": 57, "endOffset": 60}, {"referenceID": 6, "context": "We acknowledge that [7] use different features, policy settings, and hardware from ours; therefore these numbers might not be comparable.", "startOffset": 20, "endOffset": 23}], "year": 2016, "abstractText": "We study the problem of structured prediction under test-time budget constraints. We propose a novel approach applicable to a wide range of structured prediction problems in computer vision and natural language processing. Our approach seeks to adaptively generate computationally costly features during test-time in order to reduce the computational cost of prediction while maintaining prediction performance. We show that training the adaptive feature generation system can be reduced to a series of structured learning problems, resulting in efficient training using existing structured learning algorithms. This framework provides theoretical justification for several existing heuristic approaches found in literature. We evaluate our proposed adaptive system on two structured prediction tasks, optical character recognition (OCR) and dependency parsing and show strong performance in reduction of the feature costs without degrading accuracy.", "creator": "LaTeX with hyperref package"}}}