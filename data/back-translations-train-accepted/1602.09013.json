{"id": "1602.09013", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Feb-2016", "title": "Beyond CCA: Moment Matching for Multi-View Models", "abstract": "We introduce three novel semi-parametric extensions of probabilistic canonical correlation analysis with identifiability guarantees. We consider moment matching techniques for estimation in these models. For that, by drawing explicit links between the new models and a discrete version of independent component analysis (DICA), we first extend the DICA cumulant tensors to the new discrete version of CCA. By further using a close connection with independent component analysis, we introduce generalized covariance matrices, which can replace the cumulant tensors in the moment matching framework, and, therefore, improve sample complexity and simplify derivations and algorithms significantly. As the tensor power method or orthogonal joint diagonalization are not applicable in the new setting, we use non-orthogonal joint diagonalization techniques for matching the cumulants. We demonstrate performance of the proposed models and estimation techniques on experiments with both synthetic and real datasets.", "histories": [["v1", "Mon, 29 Feb 2016 15:51:50 GMT  (117kb,D)", "http://arxiv.org/abs/1602.09013v1", "26 pages, under review"], ["v2", "Fri, 3 Jun 2016 14:06:23 GMT  (130kb,D)", "http://arxiv.org/abs/1602.09013v2", "Appears in: Proceedings of the 33rd International Conference on Machine Learning (ICML 2016). 22 pages"]], "COMMENTS": "26 pages, under review", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["anastasia podosinnikova", "francis r bach", "simon lacoste-julien"], "accepted": true, "id": "1602.09013"}, "pdf": {"name": "1602.09013.pdf", "metadata": {"source": "CRF", "title": "Beyond CCA: Moment Matching for Multi-View Models", "authors": ["Anastasia Podosinnikova", "Francis Bach", "Simon Lacoste-Julien"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Examples of such data include the presentation of some texts in two languages (e.g., Vinokourov et al., 2002) or images that interact with text data (e.g., Hardoon et al., 2004; Gong et al.) Given two multidimensional variables (or datasets), CCA finds two linear transformations (factor loading matrices) that maximize each other the correlations between the transformed variables (or datasets). Along with its kernel-based version (see, Cristianini & Shawe-Taylor, 2000; Bach & Jordan, 2003), CCA has a wide range of applications (see, e.g., Hardoon et al. (2004) for an overview)."}, {"heading": "2 Multi-view models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Extensions of Gaussian CCA", "text": "Gaussian CCA (Hotelling, 1936) aims to find projections D1 (RM1) \u00b7 K and D2 (RM2) \u00b7 K of the two observation vectors x1 (RM1) and x2 (RM2) The following probable interpretations of CCA are known (Browne, 1979; Bach & Jordan, 2005; Klami et al., 2013). Considering that K sources are generally normal random variables, the solution amounts to the solution of a generalized SVD problem. The following probable interpretations of CCA are well known (Browne, 1979; Klami et al., 2013). Considering that K sources are generally normal random variables, \u03b1-N (0, IK), the Gaussian CCA model is based on Byx1, \u00b51 (Micro1) and 1 (M1)."}, {"heading": "2.2 Identifiability", "text": "This section discusses the identifiability of factor load matrices D1 and D2. In general, the unidentifiability of permutation and scaling cannot be avoided for the type of models considered. In practice, however, this unidentifiability is easy to handle, and in the following we will consider identifiability only up to permutation and scaling. The factor load matrix of FA / PPCA can be considered an identifiable analogue of FA / PPCA. In fact, it is known that the mixing matrix D of ICA is identifiable if there is Gauss at most at one source (Comon, 1994). The factor load matrix of FA / PPCA is not identifiable because it is defined only up to multiplication by an orthogonal rotation matrix."}, {"heading": "3 The cumulants and generalized covariances", "text": "In this section we first derive the cumulative tensors for the discrete CCA model (section 3.1) and then generalized covariance matrices (section 3.2) for the models (4) - (6). We show that both cumulants and generalized covariances have a special diagonal shape and can therefore be used efficiently within the moment matching framework (section 4)."}, {"heading": "3.1 From discrete ICA to discrete CCA", "text": "In this section, we derive the DCCA cumulants as an extension of the cumulants of discrete component analysis (DICA; Podosinnikova et al., 2015).Discrete ICA = > Then we consider the discrete ICA model (9), in which x-RM has conditional independent Poisson components with medium dalgorithms and non-negative components: x-RK has independent non-negative components. (9) For estimating the factor that matrix D, Podosinnikova et al. (2015), we propose an algorithm component based on the moment matching method of the DICA model. Specifically, they define the DICA scovariance matrix and T-cumulant tensor asS: = cov (x) \u2212 diag."}, {"heading": "3.2 Generalized covariance matrices", "text": "We show that (a) the generalized covariance matrix is analogous to (11) and, in the CCA case, it has a diagonal shape analogous to (16). (b) The generalized covariance matrices can therefore be considered a substitute for the generalized covariance matrices. (a) The derivatives and final expressions used for implementing resulting algorithms and (b) potentially improved complexity are greatly simplified."}, {"heading": "4 Joint diagonalization algorithms", "text": "This year, the number of working women living in the US has skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed and skyrocketed."}, {"heading": "5 Experiments", "text": "We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample.We, sample."}, {"heading": "Conclusion", "text": "We have proposed the first identifiable versions of CCA, along with moment matching algorithms that allow the identification of load matrices in a semi-parametric framework, without making assumptions about the distribution of the source or noise. We also introduce a new set of moments (our general covariance matrices) that may prove useful in other environments."}, {"heading": "6 Appendix", "text": "The appendix is organized as follows. - In appendix A we present the proof of theorem 1, which shows the identifiability of the CCA models (4) - (6).- In appendix B we provide some details for the generalized covariance matrices: the form of the generalized covariance matrices of the independent variables (appendix B.1), the derivatives of the diagonal form of the generalized covariance matrices ICA (appendix B.3), the derivatives of the diagonal form of the generalized covariance matrices of the CCA models (4) - (6) (appendix B.4), and the approximation of the T-cumulants with the generalized covariance matrix (appendix B.5).- In appendix C we offer expressions for the natural covariance matrices and the T-cumulants."}, {"heading": "B The generalized expectation and covariance matrix", "text": "B.1 The generalized expectation and covariance matrix of the sources.xx = > Expectation matrix = > The sources \u03b1 = (\u03b11,., \u03b1K) are mutually independent of each other. Therefore, their CGF (19) K\u03b1 (h) = > logE (e\u03b1 > h) takes the formK\u03b1 (h) = \u2211 k log [E (e\u03b1khk)]. Therefore, the k-th element of the generalized expectation (20) of \u03b1 (divisible in \u03b1k) [E\u03b1 (h)] k = \u03b1 (\u03b1) k = E (\u03b1ke\u03b1khk) E (e\u03b1khk) (41) and the generalized covariance (21) of \u03b1 \u2212 \u2212 \u2212 ag is diagonal due to the separability and its k-th diagonal element [C\u03b1 (h)] kk = E (\u03b1)] khk = \u03b1 (\u03b1)] khk = \u03b1 (\u03b1) khk) E (e\u03b1khk) E (e\u03b1khk) hk (e\u03b1khk) hk) hk (e\u03b1khk) hk hk)."}, {"heading": "B.4 The generalized CCA S-covariance matrix", "text": "In this section we will outline the derivative of the diagonal form (27) (27) (>) of the generalized S-covariance matrix (1) (1).Expressions (25) and (26) can be obtained in a similar way.Denoting x = [x1; x2] and t = [t1; t2] (i.e. stacking of vectors as in (8), the CGF (19) of the mixed CCA (6) can then be used as Kx (t) = logE (et > 1 x1 + t > 2 x2) = logE [E (et > 1 x1 + t > 2], the CGF (19) of the mixed CCA (a) (a) = logE (et > 1 x1)."}, {"heading": "C Finite sample estimators", "text": "[1] Finite estimators of generalized expectation and covariance matrix (2000); Slapak & Yeredor (2012b), we use the most direct way of defining generalized expectation (20) and covariance matrix (21). (2) Finite sample X = {2, x2,. (2), an estimator of generalized expectation isE x (t) = [2), an estimator of generalized expectation isE x (t). (2), an estimator of generalized covariance isC x (t). (2), an estimator of generalized expectation isE x (t) >. (2), an estimator of generalized expectation wn x (t). (t) > An estimator of generalized S covariance wn wn wn wn wn wn wn wn = et >. (1), an estimator of generalized covariance S (2), (1)."}, {"heading": "D.1 Computation of whitening matrices", "text": "You can use these matrix (31) via the S12 (12) to the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the S12 (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12), the (12, the (12), the (12), the (12), the (12), the (12, the (12), the (12), the (12, the (12), the (12), the (12 (12), the (12), the (12, the (12), the (12), the (12 (12), 12 (12), the (12, 12 (12), the (12, 12, 12 (12), 12 (12, 12, 12, 12 (12), 12 (12, 12 (12, 12), 12 (12, 12, 12, 12, 12, 12, 12 (12, 12, 12, 12, 12, 12, 12 (12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12 (12), 12 (12, 12, 12), 12 (12, 12, 12, 12, 12, 12, 12 (12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,"}, {"heading": "D.4 Finalizing estimation of D1 and D2", "text": "The non-orthogonal diagnostic algorithm outputs an invertable matrix Q., if the estimated factor load matrices are not to be negative (continuous case of NCCA (4)), then D1 = W \u2020 1Q, D2 = W \u2020 2Q \u2212 1, (47), where \u2020 stands for the pseudo-inverse. For the spectral algorithm, where Q are eigenvectors of a non-symmetrical matrix and are not guaranteed to be real, only real parts are retained after the evaluation of matrices D1 and D2 in accordance with (47). If the matrices D1 and / or D2 do not have to be negative (the discrete case of DCCA (5) and MCCA (6), they must be further mapped. For this, we select the character of each column so that the vector (column) has less negative than positive components, which is measured by the sum of the squares of the individual components of that unit."}, {"heading": "E Jacobi-like joint diagonalization of non-symmetric matrices", "text": "Given N-defective (a.k.a. diagonalizable) does not necessarily normalize 7 matrices (Q = > Q's) defined (A = {A1, A2,., AN}, where each matrix (A = > Q = > Q's) is defined (A = > Q's), where each matrix (A \u2212 RM \u00b7 M) can be considered together as diagonal. We refer to this problem as non-orthogonal joint diagonalization (NOJD) problematic. 8Algorithm 1 non-orthogonal joint diagonalization (NOJD) 1: Initialize: A (0) \u2190 A and Q (0) iterations' = 0 2: for sweeps k = 1, 2. do 3: for p = 1.,., M \u2212 1 do 4: for q = p,."}, {"heading": "F Supplementary experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "F.1 Continuous synthetic data", "text": "This experiment is essentially a continuous analogy to the synthetic experiment with the discrete data from Section 5.Synthetic data. We sample synthetic data from the linear non-Gaussian CCA (NCCA) model (7) with each topic xj = Dj\u03b1 + Fj\u03b2j. The (non-Gaussian) sources are \u03b1-z\u03b1Gamma (c, b), where z\u03b1 is a wheel maker random variable (i.e., takes the values \u2212 1 or 1 with the same probabilities).The noise sources are \u03b2j \u0445 z\u03b2jGamma (cj, bj), for z\u03b2j = 1, 2, where again z\u03b2j is a wheel maker random variable. Parameters of the gamma distribution are initialized by analogy with the discrete case (see Section 5).The elements of the matrices Dj and Fj, for j = 1, 2, are ampsampled id. for uniform distribution in [1] \u2212 1."}, {"heading": "F.4 Data preprocessing", "text": "For the experiment, we use House Debate Training Set of the Hansard collection. To process this text data, we perform case conversion, pedigree and removal of some stop words. For the pedigree, the SnowballStemmer of the NLTK Toolbox by Bird et al. (2009) was used for both English and French languages. Although these words have certain problems (such as the mapping of several different forms of a word on a single stem in one language but not in the other), they are left out of our consideration. In addition, we have also removed the following words that we consider to be stop words for our task12 (and their possible forms): - from English can, will, believe, come, cost, cut, do, follow, get, give, know, let, like, hear, live, look, lose, do, can, meet, move, see, take, think, speak, use, another, will, want to use, another, another, use, another, another, use, another, another, use, another."}, {"heading": "Supplementary References", "text": "Jacod, J. and Protter, P. Probability Essentials. Springer, 2004."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We introduce three novel semi-parametric extensions of probabilistic canonical correlation analysis with identifiability guarantees. We consider moment matching techniques for estimation in these models. For that, by drawing explicit links between the new models and a discrete version of independent component analysis (DICA), we first extend the DICA cumulant tensors to the new discrete version of CCA. By further using a close connection with independent component analysis, we introduce generalized covariance matrices, which can replace the cumulant tensors in the moment matching framework, and, therefore, improve sample complexity and simplify derivations and algorithms significantly. As the tensor power method or orthogonal joint diagonalization are not applicable in the new setting, we use non-orthogonal joint diagonalization techniques for matching the cumulants. We demonstrate performance of the proposed models and estimation techniques on experiments with both synthetic and real datasets.", "creator": "LaTeX with hyperref package"}}}