{"id": "1603.00162", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2016", "title": "Convolutional Rectifier Networks as Generalized Tensor Decompositions", "abstract": "Convolutional rectifier networks, i.e. convolutional neural networks with rectified linear activation and max or average pooling, are the cornerstone of modern deep learning. However, despite their wide use and success, our theoretical understanding of the expressive properties that drive these networks is partial at best. On other hand, we have a much firmer grasp of these issues in the world of arithmetic circuits. Specifically, it is known that convolutional arithmetic circuits posses the property of \"complete depth efficiency\", meaning that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be implemented (or even approximated) by a shallow network. In this paper we describe a construction based on generalized tensor decompositions, that transforms convolutional arithmetic circuits into convolutional rectifier networks. We then use mathematical tools available from the world of arithmetic circuits to prove new results. First, we show that convolutional rectifier networks are universal with max pooling but not with average pooling. Second, and more importantly, we show that depth efficiency is weaker with convolutional rectifier networks than it is with convolutional arithmetic circuits. This leads us to believe that developing effective methods for training convolutional arithmetic circuits, thereby fulfilling their expressive potential, may give rise to a deep learning architecture that is provably superior to convolutional rectifier networks but has so far been overlooked by practitioners.", "histories": [["v1", "Tue, 1 Mar 2016 06:44:34 GMT  (827kb,D)", "http://arxiv.org/abs/1603.00162v1", null], ["v2", "Mon, 23 May 2016 12:52:16 GMT  (828kb,D)", "http://arxiv.org/abs/1603.00162v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["nadav cohen", "amnon shashua"], "accepted": true, "id": "1603.00162"}, "pdf": {"name": "1603.00162.pdf", "metadata": {"source": "CRF", "title": "Convolutional Rectifier Networks as Generalized Tensor Decompositions", "authors": ["Nadav Cohen", "Amnon Shashua"], "emails": ["cohennadav@cs.huji.ac.il", "shashua@cs.huji.ac.il"], "sections": [{"heading": null, "text": "In this paper, we describe a construction based on generalized tensor decompositions that transforms revolutionary arithmetic circuits into revolutionary rectifier networks. We then use mathematical tools available from the world of arithmetic circuits to prove new results. First, we show that revolutionary rectifier networks are universal with maximum pooling, but not average pooling. Second, and more importantly, we show that depth efficiency in revolutionary rectifier networks is weaker than in revolutionary arithmetic circuits. This leads us to believe that the development of effective methods of forming folding arithmetic circuits, and thus exploiting their expression potential, can lead to a deeper learning architecture that is demonstrably superior to revolutionary rectifier networks but has been overlooked by users."}, {"heading": "1. Introduction", "text": "It is the manner in which the various types of \"real\" types of \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"real,\" \"\" real, \"\" \"real,\" \"\" real, \"\" \"\" real, \"\" \"\" real, \"\" \"\" \"real,\" \"\" \"\" \"\", \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\""}, {"heading": "2. Related Work", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "3. Generalized Tensor Decompositions", "text": "rE \"s tis rf\u00fc ide rf\u00fc the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "4. From Networks to Tensors", "text": "The KonvNet architecture that is analyzed in this publication is shown in Fig. 1. Entering into a network that is designated as a network starts with 1, 2, 3, 4, 5, 5, 5, 5, 6, 6, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,"}, {"heading": "5. Capacity Analysis", "text": "In this section, we analyze score functions that can be expressed by the flat and deep ConvNets (fig. 2 or fig. 1 with L = log2N) under ReLU activation with maximum or average pooling (Convolutionary Rectifier Networks), and compare these settings with linear activation with product pooling (Convolutionary arithmetic circuits). Score functions are analyzed by grid sorters (eq. 3), which are represented by the generalized tensor decompositions established in the previous section: the generalized CP decomposition (eq. 6) corresponding to the flat network and the generalized HT decomposition (eq. 7) corresponding to the deep network. The analysis is organized as follows. In sec. 5.1, we present preliminary material required to follow our evidence."}, {"heading": "5.1. Preliminaries", "text": "In order to evaluate the completeness of depth efficiency, and also for other purposes, we are often interested in the \"volume\" of sentences in a Euclidean space, or more formally, in their Lebesgue scale. While an introduction to the Lebesgue measurement theory goes beyond the scope of this paper (the interested reader is referred to [13], here we present several concepts and results on which our proofs will be based. A zero unit of measurement can intuitively be thought of as a zero band. A union of numerable zero units is itself a zero unit of measurement. If we randomize a point in space by any continuous distribution, the probability of hitting a zero unit of measurement is always zero. A useful fact (proven in [1] for example) is that the zero unit of measurement of a polynomial, i.e. the amount of points on which a polynomic concept disappears, is either the entire space (if the polynomial is in question), or it must have a zero unit of measurement."}, {"heading": "5.2. Templates and Representation Functions", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "5.3. Matricization", "text": "In the analysis of the matrix, we will often consider its arrangement as matrices. A tensor A, referred to as [A], is its arrangement as a matrix with lines corresponding to odd modes and columns. In particular, if A-M1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "5.4. Universality", "text": "Universality refers to the ability of a network to perform (or approximate) any function of choice if no limitations are imposed on its size. It is known that fully connected neural networks are universal among all types of non-linear activations that are used in practice, even if the number of hidden layers is limited to one ([6, 12, 18]). To understand the best of our knowledge, the universality of our shallow and deep ConvNets (fig. 2, and fig. 1 with L = log2N, respectively) is under ReLU activation and average pooling. We start with a result similar to that given in [5], depending on which Constitutional arithmetic circuits are universal: assertion of templates exist, with linear activation and product pooling."}, {"heading": "5.5. Depth Efficiency", "text": "It is generally assumed that deep networks with non-linear layers efficiently express functions that cannot be efficiently expressed by flat networks, i.e. that the question of depth efficiency implicitly presupposes universality, i.e. that there are some (possibly exponential) quantities with which the flat network is able to express the target functionality. To achieve the best of our knowledge, the question of depth efficiency has to be formally analyzed in the context of ConvNets. This work focuses on revolutionary arithmetic circuits that show depth efficiency with such networks, i.e. in addition to a negligible set, all functions are realizable through deep network efficiency."}, {"heading": "5.5.1 Approximation", "text": "In its current form, the results of our depth efficiency analysis (Claims 8, 9, 11 and the analogues in paragraph 5,6) relate only to the exact realization; specifically, they represent a lower limit on the size of a flat ConvNet required to realize exactly a grid tensor generated by a deep ConvNet. Practically, the size required for an approximation would be that of a flat ConvNet in order to approximate the calculation of a deep ConvNet. Primarily, it could be that the size required for an exact realization is exponential, but the size required for an approximation is only polynomic. As we briefly discuss below, this is not the case, and in fact, all the lower limits we provide apply not only to the exact realization, but also to arbitrarily well-approximated information."}, {"heading": "5.5.2 On the Incidence of Depth Efficiency", "text": "In claim 8 we saw this depth efficiency with linear activation and product pooling. That is, with linear activation and product pooling, apart from a negligible set, all weight settings for the deep ConvNet size (fig. 1 with size 2 pooling windows and L = log2N hidden levels) will result in score functions that cannot be realized by the shallow ConvNet level (fig. 2), unless the latter requires super polynomial size (claims 9 and 10), which replace the activation and pooling operators with ReLU and Max, makes depth efficiency incomplete. There are still weight settings that perform the deep ConvNet functions that require the shallow ConvNet functions, but these do not occupy the entire space."}, {"heading": "5.6. Shared Coefficients for Convolution", "text": "To this end, our analysis has focused on the non-shared environment, where the coefficients of 1 \u00b7 1 Conv filters = net weight (see Figure 1) can vary according to spatial conditions. \u2212 g In practice, ConvNets typically force sharing, which in our framework means that the coefficients of 1 \u00b7 1 Conv filters in channels with hidden layer l are the same for all locations. \u2212 g In this section, we analyze the shared environment by following a line6. To understand this, let's just assume that the choice g (a, b) = max."}, {"heading": "5.6.1 Universality", "text": "In the non-divided environment, we saw (Sec. 5,4) that linear activation with product pooling and ReLU activation with maximum pooling both lead to universality, while ReLU activation with average pooling does not. We will now see that in the shared environment, no matter how the activation and pooling operators are selected, universality is never metric. A flat ConvNet with divided weights generates grid sorters by the split generalized CP decomposition (eq. 15). A TensorA generated by this decomposition is necessarily symmetrical, i.e. for each permutation: [N] \u2192 [N] and indexes d1... dN it is true: Ad1... dN = A\u03b4 (d1)... Although not all tensors share this property, a flat ConvNet with divided weights is not universal."}, {"heading": "5.6.2 Depth Efficiency", "text": "In order to answer this question, we have to deal with the question of distributional justice. (The distributional justice of distributional justice in distributional justice and the distributional justice of distributional justice in distributional justice of distributional justice in distributional justice in distributional justice and the distributional justice in distributional justice in distributional justice in distributional justice in distributional justice in distributional justice and in distributional justice in distributional justice in distributional justice in distributional justice in distributional justice. (The distribution justice in distributional justice in distributional justice in distributional justice in distributional justice in distributional justice in distributional justice in distributional justice and in distributional justice in distributional justice in distributional justice in distributional justice in distributional justice in distributional justice.)"}, {"heading": "6. Discussion", "text": "The contribution of this paper is twofold: firstly, we are introducing a construction in the form of generalized tensor decompositions that enables the conversion of conventional to conventional rectifier networks, opening the door to various mathematical tools from the world of arithmetic that are now available for the analysis of conventional rectifier networks; secondly, we are using such tools to demonstrate new properties that drive this important class of networks; and, secondly, our analysis shows that conventional rectifier networks are universal, but not with average pooling systems, implying that non-linearity results only from activation, but also from networking."}, {"heading": "Acknowledgments", "text": "This work is partly funded by the Intel Grant ICRI-CI No. 9- 2012-6133 and the ISF Center 1790 / 12. Nadav Cohen is supported by a Google Fellowship in Machine Learning."}, {"heading": "A. Existence of Covering Templates", "text": "This year it is as if it is a reactionary, but not a reactionary, reactionary, reactionary, reactionary-eaJrh-eaJrh-eaJrh-eaJrh-eaJngr-eaJngr-eaJrmhsrsrsrteeaJrh-eaJrh-eaJngr-eaeaeSrmtlrsrteeaeaeaeaeaeaeaeSrlhc-nlrlrrteaeaeaeaeaeaeoi.nlrgn"}], "references": [{"title": "The zero set of a polynomial", "author": ["Richard Caron", "Tim Traynor"], "venue": "WSMR Report 05-02,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Teaching deep convolutional neural networks to play go", "author": ["Christopher Clark", "Amos Storkey"], "venue": "arXiv preprint arXiv:1412.3409,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "SimNets: A Generalization of Convolutional Networks", "author": ["Nadav Cohen", "Amnon Shashua"], "venue": "NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "On the expressive power of deep learning: a tensor analysis", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": "arXiv preprint arXiv:1509.05009,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G Cybenko"], "venue": "Mathematics of Control, Signals and Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1989}, {"title": "Shallow vs. deep sumproduct networks", "author": ["Olivier Delalleau", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1512.03965,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Deep learning. Book in preparation for MIT Press, 2016", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": "URL http://goodfeli.github.io/dlbook/", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "A New Scheme for the Tensor Representation", "author": ["W Hackbusch", "S K\u00fchn"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Tensor Spaces and Numerical Tensor Calculus, volume 42 of Springer Series in Computational Mathematics", "author": ["Wolfgang Hackbusch"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Kurt Hornik", "Maxwell B Stinchcombe", "Halbert White"], "venue": "Neural networks,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1989}, {"title": "Tensor Decompositions and Applications", "author": ["Tamara G Kolda", "Brett W Bader"], "venue": "SIAM Review (),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function", "author": ["Moshe Leshno", "Vladimir Ya Lin", "Allan Pinkus", "Shimon Schocken"], "venue": "Neural networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "On the expressive efficiency of sum product networks", "author": ["James Martens", "Venkatesh Medabalimi"], "venue": "arXiv preprint arXiv:1411.7717,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML-", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "author": ["Razvan Pascanu", "Guido Montufar", "Yoshua Bengio"], "venue": "arXiv preprint arXiv,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Itheory on depth vs width: hierarchical function composition", "author": ["Tomaso Poggio", "Fabio Anselmi", "Lorenzo Rosasco"], "venue": "Technical report, Center for Brains, Minds and Machines (CBMM),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Sum-product networks: A new deep architecture", "author": ["Hoifung Poon", "Pedro Domingos"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil"], "venue": "In Proceedings of the companion publication of the 23rd international conference on World wide web companion,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Arithmetic circuits: A survey of recent results and open questions", "author": ["Amir Shpilka", "Amir Yehudayoff"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Going Deeper with Convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Benefits of depth in neural networks", "author": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1602.04485,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Atomnet: A deep convolutional neural network for bioactivity prediction in structure-based drug discovery", "author": ["Izhar Wallach", "Michael Dzamba", "Abraham Heifets"], "venue": "arXiv preprint arXiv:1510.02855,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Deep neural networks are repeatedly proving themselves to be extremely effective machine learning models, providing state of the art accuracies on a wide range of tasks (see [17, 9]).", "startOffset": 174, "endOffset": 181}, {"referenceID": 13, "context": "Arguably, the most successful deep learning architecture to date is that of convolutional neural networks (ConvNets, [16]), which prevails in the field of computer vision, and is recently being harnessed for many other application domains as well (e.", "startOffset": 117, "endOffset": 121}, {"referenceID": 21, "context": "[25, 31, 2]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 27, "context": "[25, 31, 2]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 1, "context": "[25, 31, 2]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 17, "context": "Modern ConvNets are formed by stacking layers one after the other, where each layer consists of a linear convolutional operator followed by Rectified Linear Unit (ReLU [21]) activation (\u03c3(z) = max{0, z}), which in turn is followed by max or average pooling (P{cj} = max{cj} or P{cj} = mean{cj} respectively).", "startOffset": 168, "endOffset": 172}, {"referenceID": 12, "context": "Such models, which we refer to as convolutional rectifier networks, have driven the resurgence of deep learning ([15]), and represent the cutting edge of the ConvNet architecture ([28, 27]).", "startOffset": 113, "endOffset": 117}, {"referenceID": 24, "context": "Such models, which we refer to as convolutional rectifier networks, have driven the resurgence of deep learning ([15]), and represent the cutting edge of the ConvNet architecture ([28, 27]).", "startOffset": 180, "endOffset": 188}, {"referenceID": 23, "context": "Such models, which we refer to as convolutional rectifier networks, have driven the resurgence of deep learning ([15]), and represent the cutting edge of the ConvNet architecture ([28, 27]).", "startOffset": 180, "endOffset": 188}, {"referenceID": 20, "context": "Arithmetic circuits (also known as Sum-Product Networks, [24]) are networks with two types of nodes: sum nodes, which compute a weighted sum of their inputs, and product nodes, computing the product of their inputs.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Recently, [5] analyzed convolutional arithmetic circuits through ten-", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "Although convolutional arithmetic circuits are known to be equivalent to SimNets ([3]), a new deep learning architecture that has recently demonstrated promising empirical performance ([4]), they are fundamentally different from convolutional rectifier networks.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "Accordingly, the result established in [5] does not apply to models commonly used in practice.", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "In this paper we present a construction, based on the notion of generalized tensor decompositions, that transforms convolutional arithmetic circuits of the type described in [5] into convolutional rectifier networks.", "startOffset": 174, "endOffset": 177}, {"referenceID": 22, "context": "We refer the interested reader to [26] for a survey written in 2010, and mention here the more recent works [7] and [19] studying depth efficiency of arithmetic circuits in the context of deep learning (Sum-Product Networks).", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "We refer the interested reader to [26] for a survey written in 2010, and mention here the more recent works [7] and [19] studying depth efficiency of arithmetic circuits in the context of deep learning (Sum-Product Networks).", "startOffset": 108, "endOffset": 111}, {"referenceID": 15, "context": "We refer the interested reader to [26] for a survey written in 2010, and mention here the more recent works [7] and [19] studying depth efficiency of arithmetic circuits in the context of deep learning (Sum-Product Networks).", "startOffset": 116, "endOffset": 120}, {"referenceID": 17, "context": "Compared to arithmetic circuits, the literature on depth efficiency of neural networks with ReLU activation is far less developed, primarily since these models were only introduced several years ago ([21]).", "startOffset": 200, "endOffset": 204}, {"referenceID": 18, "context": "[22] and [20] use combinatorial arguments to characterize the maximal number of linear regions in functions generated by ReLU networks, thereby establishing existence of depth efficiency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[22] and [20] use combinatorial arguments to characterize the maximal number of linear regions in functions generated by ReLU networks, thereby establishing existence of depth efficiency.", "startOffset": 9, "endOffset": 13}, {"referenceID": 26, "context": "[30] uses semi-algebraic geometry to analyze the number of oscillations in functions realized by neural networks with semi-algebraic activations, ReLU in particular.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "The fundamental result proven in [30] is the existence, for every k \u2208 N, of functions realizable by networks with \u0398(k) layers and \u0398(1) nodes per layer, which cannot be approximated by networks withO(k) layers unless these are exponentially large (have \u03a9(2) nodes).", "startOffset": 33, "endOffset": 37}, {"referenceID": 6, "context": "The work in [8] makes use of Fourier analysis to show existence of functions that are efficiently computable by depth-3 networks, yet require exponential size in order to be approximated by depth-2 networks.", "startOffset": 12, "endOffset": 15}, {"referenceID": 19, "context": "[23] also compares the computational abilities of deep vs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "However, the complexity measure considered in [23] is the VC dimension, whereas our interest lies in network size.", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "Recently, [5] introduced convolutional arithmetic circuits, which may be viewed as ConvNets with linear activation and product pooling.", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": "These networks are shown to correspond to hierarchical tensor decompositions (see [11]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 3, "context": "Accordingly, the analysis carried out in [5] does not apply to the networks at the forefront of deep learning.", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "Closing the gap between the networks analyzed in [5] and convolutional rectifier networks is the topic of this paper.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "We achieve this by generalizing tensor decompositions, thereby opening the door to mathematical machinery as used in [5], harnessing it to analyze, for the first time, the depth efficiency of convolutional rectifier networks.", "startOffset": 117, "endOffset": 120}, {"referenceID": 11, "context": "Tensor decompositions (see [14] for a survey) may be viewed as schemes for expressing tensors using tensor products and weighted sums.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "This is often referred to in the deep learning community as a locally-connected layer (see [29]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "Convolutional arithmetic circuits as analyzed in [5] correspond to linear activation (\u03c3(z) = z) and product pooling (P{cj} = \u220f cj).", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "vation (\u03c3(z) = z) and product pooling (P{cj} = \u220f cj) we get a convolutional arithmetic circuit as analyzed in [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 11, "context": "6 generalizes the classic CP (CANDECOMP/PARAFAC) decomposition (see [14] for a historic survey), and we accordingly refer to it as the generalized CP decomposition.", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "7 generalizes the Hierarchical Tucker decomposition introduced in [10], and is accordingly referred to as the generalized HT decomposition.", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "A useful fact (proven in [1] for example) is that the zero set of a polynomial, i.", "startOffset": 25, "endOffset": 28}, {"referenceID": 9, "context": "Here too, a full introduction to the topic is beyond our scope (we refer the interested reader to [11]), and we only list some concepts and results that will be used.", "startOffset": 98, "endOffset": 102}, {"referenceID": 4, "context": "It is well-known that fullyconnected neural networks are universal under all types of non-linear activations typically used in practice, even if the number of hidden layers is restricted to one ([6, 12, 18]).", "startOffset": 195, "endOffset": 206}, {"referenceID": 10, "context": "It is well-known that fullyconnected neural networks are universal under all types of non-linear activations typically used in practice, even if the number of hidden layers is restricted to one ([6, 12, 18]).", "startOffset": 195, "endOffset": 206}, {"referenceID": 14, "context": "It is well-known that fullyconnected neural networks are universal under all types of non-linear activations typically used in practice, even if the number of hidden layers is restricted to one ([6, 12, 18]).", "startOffset": 195, "endOffset": 206}, {"referenceID": 3, "context": "We begin by stating a result similar to that given in [5], according to which convolutional arithmetic circuits are universal:", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "To the best of our knowledge, at the time of this writing the only work to formally analyze depth efficiency in the context of ConvNets is [5].", "startOffset": 139, "endOffset": 142}, {"referenceID": 3, "context": "Claim 8 (adaptation of theorem 1 in [5]).", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "The proof of theorem 1 in [5] shows that when arranged as matrices, such tensors have rank at least min{r0,M}N almost always, i.", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "Such tensors, when arranged as matrices, are shown in the proof of theorem 1 in [5] to have rank at most Z.", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "In the case of linear activation and product pooling (g(a, b) = a\u00b7b), the generalized Kronecker product g reduces to the standard Kronecker product , and the rank-multiplicative property of the latter (rank(A B) = rank(A)\u00b7rank(B)) can be used to show (see [5]) that besides in negligible (zero measure) cases, rank grows rapidly through the levels of the matricized generalized HT decomposition (eq.", "startOffset": 256, "endOffset": 259}, {"referenceID": 1, "context": "fr e q u e n cy numbers of hidden channels: [2, 2, 2]", "startOffset": 44, "endOffset": 53}, {"referenceID": 1, "context": "fr e q u e n cy numbers of hidden channels: [2, 2, 2]", "startOffset": 44, "endOffset": 53}, {"referenceID": 1, "context": "fr e q u e n cy numbers of hidden channels: [2, 2, 2]", "startOffset": 44, "endOffset": 53}, {"referenceID": 6, "context": "fr e q u e n cy numbers of hidden channels: [8, 8, 8]", "startOffset": 44, "endOffset": 53}, {"referenceID": 6, "context": "fr e q u e n cy numbers of hidden channels: [8, 8, 8]", "startOffset": 44, "endOffset": 53}, {"referenceID": 6, "context": "fr e q u e n cy numbers of hidden channels: [8, 8, 8]", "startOffset": 44, "endOffset": 53}, {"referenceID": 0, "context": "Since a non-zero polynomial vanishes only on a set of zero measure (see [1] for example), the set of weight vectors \u03b1 for which p(\u03b1) = 0 has measure zero.", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "For linear activation with product pooling, the following claim, which is essentially a derivative of theorem 1 in [5], tells us that in the shared setting, as in the unshared setting, depth efficiency holds completely:", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "Such result is provided by the proof of theorem 1 in [5].", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "Convolutional arithmetic circuits on the other hand received far less attention, although they have been successfully trained in recent works on the SimNet architecture ([3, 4]), demonstrating", "startOffset": 170, "endOffset": 176}], "year": 2017, "abstractText": "Convolutional rectifier networks, i.e. convolutional neural networks with rectified linear activation and max or average pooling, are the cornerstone of modern deep learning. However, despite their wide use and success, our theoretical understanding of the expressive properties that drive these networks is partial at best. On other hand, we have a much firmer grasp of these issues in the world of arithmetic circuits. Specifically, it is known that convolutional arithmetic circuits posses the property of \u201dcomplete depth efficiency\u201d, meaning that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be implemented (or even approximated) by a shallow network. In this paper we describe a construction based on generalized tensor decompositions, that transforms convolutional arithmetic circuits into convolutional rectifier networks. We then use mathematical tools available from the world of arithmetic circuits to prove new results. First, we show that convolutional rectifier networks are universal with max pooling but not with average pooling. Second, and more importantly, we show that depth efficiency is weaker with convolutional rectifier networks than it is with convolutional arithmetic circuits. This leads us to believe that developing effective methods for training convolutional arithmetic circuits, thereby fulfilling their expressive potential, may give rise to a deep learning architecture that is provably superior to convolutional rectifier networks but has so far been overlooked by practitioners.", "creator": "LaTeX with hyperref package"}}}