{"id": "1603.00375", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2016", "title": "Easy-First Dependency Parsing with Hierarchical Tree LSTMs", "abstract": "We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders. To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving state-of-the-art accuracies for English and Chinese, without relying on external word embeddings. The parser's implementation is available for download at the first author's webpage.", "histories": [["v1", "Tue, 1 Mar 2016 17:43:37 GMT  (155kb,D)", "http://arxiv.org/abs/1603.00375v1", null], ["v2", "Thu, 26 May 2016 10:11:44 GMT  (155kb,D)", "http://arxiv.org/abs/1603.00375v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["eliyahu kiperwasser", "yoav goldberg"], "accepted": true, "id": "1603.00375"}, "pdf": {"name": "1603.00375.pdf", "metadata": {"source": "CRF", "title": "Easy-First Dependency Parsing with Hierarchical Tree LSTMs", "authors": ["Eliyahu Kiperwasser", "Yoav Goldberg"], "emails": ["elikip@gmail.com", "yoav.goldberg@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to move to a different world in which they are able to live than in a world in which they are able to live, in which they are able to live, to live and to live."}, {"heading": "2 Background and Notation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Dependency-based Representation", "text": "A dependency-based syntactical representation is centered on syntactical modification relationships between headers and modifiers, resulting in trees in which each node is a word in the sentence and each node has a parent node except for a particular root node. A dependency tree above a sentence of n words w1,... wn can be represented as a list of n pairs of the form (h, m), with 0 \u2264 h \u2264 n and 1 \u2264 m \u2264 n. Each such pair represents an edge in the tree in which h is the index of a header (including the special ROOT node 0), and m is the index of a modifier word. To make the dependency trees useful for actual downstream language processing tasks, each edge is labeled with a syntactical relationship. The tree representation then becomes a list of trebles (h, m, '), where 1 \u2264 L is the index of a dependency relationship from a particular group of L-tactics."}, {"heading": "2.2 Recurrent Networks and LSTMs", "text": "In this paper, we use the RNN abstraction as a building block, and recursively combine multiple RNNs to obtain our tree representation. We briefly describe the RNN abstraction below. For more details on RNNs, the reader will refer to sources such as (Goldberg, 2015; Bengio et al.; Cho, 2015).The RNN abstraction is a function of RNN that results in a sequence of input vectors x1,. xn (xi-Rdin), and produces a sequence of state vectors (also referred to as output vectors).The RNN abstraction is a function occurring in a sequence of input vectors x1,."}, {"heading": "3 Tree Representation", "text": "We assume that the trees in which the children are located are separated from the children, and there are the children who stand in front of the parent node, and the children who have relied on the parent node. (That is) The trees correspond well to the dependency tree structures. (We refer to the parent node as head, and to its children as modifiers. (.) The clues of the modifier are always treated by the parental modifiers as t.l1,., t.lkl and its right modifiers as t.r1 t.r3 t.l2t.l3The gist the idea of treating the modifiers of a node as a sequence, and encode this sequence with an RNN. (.l1t.l2t.l2t.l3The Gis the idea of treating the modifiers of a node as a sequence."}, {"heading": "3.1 Representing words", "text": "In the discussion above, we start from a vector representation vi associated with the sentence word. What does vi look like? A sensible approach would be to consider vi as a function of the word form and the speech portion (POS) of the word, i.e.: vi = g (W v \u00b7 pi) + bv), where wi and pi are the embedded vectors of the word form and the POS tag of the word. This encodes each word in isolation, without regard to its context. The context of a word can be very informative in terms of its meaning. One way to integrate the context is the bi-directional RNN (Schuster and Paliwal, 1997). Bidirectional RNNNs are shown to be an effective representation for sequence tagging (Irsoy and Cardie, 2014). Bidirectional RNNNs represent a word in the sentence using a concatenation of the end states of two RNs, running from the beginning of the sentence to the structure of the word cells:"}, {"heading": "3.2 A note on the head-outward generation", "text": "Why did we choose to encode the children from the head outwards and not vice versa? In order to facilitate incremental tree formation and allow efficient parsing, the sequence of generations outwards is required, as we show in Section 4 below. In addition to efficiency aspects, the use of head-external encoding puts more emphasis on the outermost dependents, who are known to be most informative in predicting the parse structure.2 We rely on the ability of RNN to extract information from arbitrary positions in the sequence to include information about the header itself that appears at the beginning of the sequence. This seems to work well, which is expected, considering that the average maximum number of siblings in one direction in the PTB is 4.1, and LSTMs have shown that they capture much broader interactions. Nevertheless, if you use tree encoding in a situation where the tree is fully pre-defined from the sequence, we may specify future sentence assignments (i.e., work for a small set of sentences), or even a small set of tasks."}, {"heading": "4 Parsing Algorithm", "text": "We start by describing our bottom-up parsing algorithm and then show how the 2 features in transition-based dependency parsers often look at the current dependencies to the left and right of a particular node and almost never look further than the second most to the left or right. Second-order graph-based dependency parsers (McDonald, 2006; Eisner, 2000) also condition the generation of their siblings from the current extreme dependency. The encoded vector representation can be established and maintained throughout the parsing process."}, {"heading": "4.1 Bottom-up Parsing", "text": "We follow a (projective) bottom-up analysis strategy similar to the easy-first analysis algorithm of Goldberg and Elhadad (2010). The main data structure in the parser is a list of partially constructed parse trees, which we call pending. For a sentence of words w1,.., wn, the pending list of n nodes is initialized, where pending [i] corresponds to the word wi. The algorithm then selects two adjacent trees in the pending list pending [i] and pending [i + 1], and then either appends the root of the pending [i + 1] as the most right-sided modifier of the pending [i], or appends the root of the pending [i] as the most distant [i + 1]. The tree that was treated as a modifier is then removed from the pending list: [i + 1, shortens it by one. The process ends after n \u2212 1 steps at which we remain on the pending list with a single tree."}, {"heading": "4.2 Bottom-up Tree-Encoding", "text": "Algorithm 2 shows how the vector encodings can be maintained together with the parsing algorithm, so that at each stage of the parsing process each element that is [i] pending is associated with a vector encoding of the corresponding tree algorithm 2. Parsing while retaining the tree representations: Input: Sentence w = w1,.., wn 2: Input: Vectors vi, which is the word wi 3: Arc encoding [] 4: for i-1,., n do 5: pend [i].id i 6: pend [i].el \u2190 RNL.init ().append () 7: pend [i].er \u2190 RNNR.init ().append ().mappend (vi).mappend (vi).mappend (vi).mappend | > 1 do 9:.. append (vi).el \u2190 RNNL.init ().append ().pend (i), pend [i].c [i].er \u2190 RNNR.init ().append ().append (vi).mappend (vi).mappend (vi).mappend (vi).mappend (vi).mappend (vi).mappend (vi).mappend (vi).mappend (vi):.mappend (vi).mappend (vi).mappend (vi).mappend (vi).mappend (vi):.mend (.mend (vi):.mend (.mc):.D:.mc (.mc), mc (.mc (.pi): (.pend):.D:.D:.mc:.mend (.mc),.mc:.mend (.mc: (.D),.mc:.mc: (.pi):.end (.mc:.D),.mend (.mc:.D:.mc:.mend (.mc),.mc:.mend (.mc:."}, {"heading": "4.3 Labeled Tree Representation", "text": "The tree representation described above does not take into account the relation labels \"the parsing algorithm assigns to each edge. In cases where the tree is fully specified in advance, the relationship of each word to its header can be added to the word representations. However, in the context of the parsing algorithm, the labels are not known until the modifier is appended to its parent tree. Therefore, we extend the tree representation by linking the node vector representation to a vector representation assigned to the label that connects the sub-tree to its parent tree. Formally, only the final Enc (t) equation changes: enc (t) = g (W e \u00b7 (el) er () + be), where\" is a learned embed vector associated with the given label."}, {"heading": "4.4 Scoring Function", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4.5 Computational Complexity", "text": "The Easy-First-Parsing algorithm works in O (n log n) time (Goldberg and Elhadad, 2010). The parser differs in three aspects: executing a BI-LSTM encoder before parsing (O (n)); maintaining the tree representation during parsing (lines 11-22 in algorithm 2), which takes a constant amount of time at each parsing step; and local evaluation using an MLP instead of a linear classifier (again a constant time process)."}, {"heading": "5 Training Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Loss and Parameter Updates", "text": "The goal of the training is to define the score function so that correct actions are rated better than wrong actions. We use a marginal goal that aims to maximize the margin between the correct action with the highest score and the amount of erroneous actions. Formally, we define a hinge loss for each analysis step as follows: Max {0, 1 \u2212 max (i, d, ') \"GScore\" (pend, i, d,') + Max (i \u2032, d \u2032) \"A\\ GScore (pend, i, d\"), where A is the set of all possible actions and G the set of correct actions in the current stage.Since the scoring function depends on the vector encoding of all trees in the window, and any tree encoding depends on the parameters of the network."}, {"heading": "5.2 Error-Exploration and Dynamic Oracle Training", "text": "In this case, it is as if it were a reactionary act."}, {"heading": "5.3 Out-of-vocabulary items and word-dropout", "text": "Due to the rarity of natural language, we are likely to encounter a significant number of words that do not appear in the training data (OOV words) at the test date. OOV words are likely even if we prepare the word representations on a large, uncommented corpus. A common approach is to designate a special \"unknown word\" symbol whose associated vector is used as a word representation whenever an OOV word is encountered at the test date. To train the unknown word vector, one possible approach is to replace all words occurring in the training corpus with the unknown word symbol less than a certain number of times. This approach provides a good vector representation of unknown words, but at the expense of ignoring many words from the training corpus. Instead, we suggest a variant of the word dropout approach (Iyyer et al., 2015). During the training, we replace a word symbol with the inverse of the probability of the word symbols in the training."}, {"heading": "6 Implementation Details", "text": "Our Python implementation is made available on the first author's website. We use the PyCNNwrapper of CNN library 3 to build the network calculation diagram, calculate the gradients using automatic differentiation, and perform parameter updates. We noticed that the error in the development set did not improve after 20 iterations over the course of the training, so we performed the training for 20 iterations, the sentences were mixed between iterations. Non-projective sentences were skipped during the training. We use the default parameters initialization, step sizes, and regulation values provided by the PyCNN toolkit, and the hyperparameters of the last networks used for all reported experiments are described in detail in Table 1.White et al (2015), emphasizing the importance of careful hyperparameter setting to achieve the highest accuracy in neural network-based parameters. We followed this advice and did not undertake just a few experiments for the rest of the parameters that seemed to work."}, {"heading": "7 Experiments and Results", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "8 Related Work", "text": "In fact, most of them are able to determine for themselves what they want and what they don't want."}, {"heading": "9 Conclusions and Future Work", "text": "We propose a compositional vector representation of parse trees based on a recursive combination of recursive-neuronal network encoders, demonstrating its effectiveness by integrating it into a bottom-up easy-first parser. Future enhancements to parsing include the addition of bar search, handling of unknown words using character embedding, and adapting the algorithm to constituency trees. We also plan to determine the effectiveness of our hierarchical tree LSTM encoder by applying it to more semantic vector representation tasks, such as training tree representation to capture emotions (Socher et al., 2013b; Tai et al., 2015), semantic sentence similarity (Marelli et al., 2014) or textual inference (Bowman et al., 2015).Acknowledgements This research is supported by the Intel Collaborative Research Institute for Cputative Research Computative Institute (1555-Intelligence I, Israeli Science Foundation)."}, {"heading": "Appendix: Training Algorithm Pseudocode", "text": "(1), (2), (2), (2), (2), (2), (3), (3), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (), (5), (), (), (), (), ()."}], "references": [{"title": "Deep Learning", "author": ["Yoshua Bengio", "Ian J. Goodfellow", "Aaron Courville."], "venue": "Book in preparation for MIT Press.", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740\u2013750, Doha, Qatar, October. Association for", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Natural Language Understanding with Distributed Representation", "author": ["Kyunghyun Cho."], "venue": "arXiv:1511.07916 [cs, stat], November.", "citeRegEx": "Cho.,? 2015", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "Three Generative, Lexicalised Models for Statistical Parsing", "author": ["Michael Collins."], "venue": "Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16\u201323, Madrid, Spain, July. Association for Computational Linguistics.", "citeRegEx": "Collins.,? 1997", "shortCiteRegEx": "Collins.", "year": 1997}, {"title": "Stanford dependencies manual", "author": ["Marie-Catherine de Marneffe", "Christopher D. Manning."], "venue": "Technical report, Stanford University.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Neural CRF Parsing", "author": ["Greg Durrett", "Dan Klein."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 302\u2013312,", "citeRegEx": "Durrett and Klein.,? 2015", "shortCiteRegEx": "Durrett and Klein.", "year": 2015}, {"title": "TransitionBased Dependency Parsing with Stack Long ShortTerm Memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Three New Probabilistic Models for Dependency Parsing: An Exploration", "author": ["Jason M. Eisner."], "venue": "COLING 1996 Volume 1: The 16th International Conference on Computational Linguistics.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Bilexical grammars and their cubictime parsing algorithms", "author": ["J. Eisner."], "venue": "Advances in Probabilistic and Other Parsing Technologies.", "citeRegEx": "Eisner.,? 2000", "shortCiteRegEx": "Eisner.", "year": 2000}, {"title": "Finding Structure in Time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive Science, 14(2):179\u2013211, March.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing", "author": ["Yoav Goldberg", "Michael Elhadad."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Goldberg and Elhadad.,? 2010", "shortCiteRegEx": "Goldberg and Elhadad.", "year": 2010}, {"title": "A dynamic oracle for the arc-eager system", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "Proc. of COLING 2012.", "citeRegEx": "Goldberg and Nivre.,? 2012", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2012}, {"title": "Easy-First Chinese POS Tagging and Dependency Parsing", "author": ["Ji Ma", "Tong Xiao", "Jingbo Zhu", "Feiliang Ren."], "venue": "Proceedings of COLING 2012, pages 1731\u20131746, Mumbai, India, December. The COLING 2012 Organizing Committee.", "citeRegEx": "Ma et al\\.,? 2012", "shortCiteRegEx": "Ma et al\\.", "year": 2012}, {"title": "Easy-First POS Tagging and Dependency Parsing with Beam Search", "author": ["Ji Ma", "Jingbo Zhu", "Tong Xiao", "Nan Yang."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 110\u2013114, Sofia,", "citeRegEx": "Ma et al\\.,? 2013", "shortCiteRegEx": "Ma et al\\.", "year": 2013}, {"title": "Building a large annotated corpus of English: The penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marchinkiewicz."], "venue": "Computational Linguistics, 19.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entail", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Turning on the turbo: Fast third-order nonprojective turbo parsers", "author": ["Andre Martins", "Miguel Almeida", "Noah A. Smith."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 617\u2013622,", "citeRegEx": "Martins et al\\.,? 2013", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Discriminative Training and Spanning Tree Algorithms for Dependency Parsing", "author": ["Ryan McDonald."], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "McDonald.,? 2006", "shortCiteRegEx": "McDonald.", "year": 2006}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "An Effective Neural Network Model for Graph-based Dependency Parsing", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-", "citeRegEx": "Pei et al\\.,? 2015", "shortCiteRegEx": "Pei et al\\.", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "Kuldip K. Paliwal."], "venue": "IEEE Transactions on Signal Processing, 45(11):2673\u20132681, November.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks", "author": ["Richard Socher", "Christopher Manning", "Andrew Ng."], "venue": "Proceedings of the Deep Learning and Unsupervised Feature Learning Workshop of {NIPS}", "citeRegEx": "Socher et al\\.,? 2010", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "Parsing with Compositional Vector Grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455\u2013465,", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 Conference on Empiri-", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive Deep Learning For Natural Language Processing and Computer Vision", "author": ["Richard Socher."], "venue": "Ph.D. thesis, Stanford University, August.", "citeRegEx": "Socher.,? 2014", "shortCiteRegEx": "Socher.", "year": 2014}, {"title": "Transition-based Dependency Parsing Using Recursive Neural Networks", "author": ["Pontus Stenetorp."], "venue": "Deep Learning Workshop at the 2013 Conference on Neural Information Processing Systems (NIPS), Lake Tahoe, Nevada, USA, December.", "citeRegEx": "Stenetorp.,? 2013", "shortCiteRegEx": "Stenetorp.", "year": 2013}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A fast, effective, non-projective, semantically-enriched parser", "author": ["Stephen Tratz", "Eduard Hovy."], "venue": "Proc. of EMNLP.", "citeRegEx": "Tratz and Hovy.,? 2011", "shortCiteRegEx": "Tratz and Hovy.", "year": 2011}, {"title": "Transitionbased Neural Constituent Parsing", "author": ["Taro Watanabe", "Eiichiro Sumita."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume", "citeRegEx": "Watanabe and Sumita.,? 2015", "shortCiteRegEx": "Watanabe and Sumita.", "year": 2015}, {"title": "Structured Training for Neural Network Transition-Based Parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beamsearch", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proc. of EMNLP.", "citeRegEx": "Zhang and Clark.,? 2008", "shortCiteRegEx": "Zhang and Clark.", "year": 2008}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Yue Zhang", "Joakim Nivre."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188\u2013193.", "citeRegEx": "Zhang and Nivre.,? 2011", "shortCiteRegEx": "Zhang and Nivre.", "year": 2011}, {"title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network", "author": ["Chenxi Zhu", "Xipeng Qiu", "Xinchi Chen", "Xuanjing Huang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "Recurrent neural networks (RNNs) (Elman, 1990), and in particular methods based on the LSTM architecture (Hochreiter and Schmidhuber, 1997), work very well for modeling sequences, and constantly obtain state-of-the-art results on both language-modeling and prediction tasks (see, e.", "startOffset": 33, "endOffset": 46}, {"referenceID": 19, "context": "(Mikolov et al., 2010)).", "startOffset": 0, "endOffset": 22}, {"referenceID": 22, "context": "Several works attempt to extend recurrent neural networks to work on trees (see Section 8 for a brief overview), giving rise to the so-called recursive neural networks (Goller and K\u00fcchler, 1996; Socher et al., 2010).", "startOffset": 168, "endOffset": 215}, {"referenceID": 11, "context": "We use our tree representation for encoding the partially-build parse trees in a greedy, bottom-up dependency parser which is based on the easy-first transition-system of Goldberg and Elhadad (2010).", "startOffset": 171, "endOffset": 199}, {"referenceID": 24, "context": "tic tasks such as sentiment analysis (Socher et al., 2013b; Tai et al., 2015), sentence similarity judgements (Marelli et al.", "startOffset": 37, "endOffset": 77}, {"referenceID": 27, "context": "tic tasks such as sentiment analysis (Socher et al., 2013b; Tai et al., 2015), sentence similarity judgements (Marelli et al.", "startOffset": 37, "endOffset": 77}, {"referenceID": 16, "context": ", 2015), sentence similarity judgements (Marelli et al., 2014) and textual entailment (Bowman et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 1, "context": ", 2014) and textual entailment (Bowman et al., 2015).", "startOffset": 31, "endOffset": 52}, {"referenceID": 0, "context": "For further detail on RNNs, the reader is referred to sources such as (Goldberg, 2015; Bengio et al., 2015; Cho, 2015).", "startOffset": 70, "endOffset": 118}, {"referenceID": 3, "context": "For further detail on RNNs, the reader is referred to sources such as (Goldberg, 2015; Bengio et al., 2015; Cho, 2015).", "startOffset": 70, "endOffset": 118}, {"referenceID": 8, "context": "Recurrent neural networks (RNNs), first proposed by Elman (1990) are statistical learners for modeling sequential data.", "startOffset": 52, "endOffset": 65}, {"referenceID": 21, "context": "One way of incorporating context is the Bidirectional RNN (Schuster and Paliwal, 1997).", "startOffset": 58, "endOffset": 86}, {"referenceID": 7, "context": "When using pretrained word embeddings, we follow (Dyer et al., 2015) and use embedding vectors which are trained using positional context (Ling et al.", "startOffset": 49, "endOffset": 68}, {"referenceID": 7, "context": "The head-outward modifier generation approach has a long history in the parsing literature, and goes back to at least Eisner (1996) and Collins (1997).", "startOffset": 118, "endOffset": 132}, {"referenceID": 4, "context": "The head-outward modifier generation approach has a long history in the parsing literature, and goes back to at least Eisner (1996) and Collins (1997). In contrast to previous work in which each modifier could condition only on a fixed small number of modifiers preceding it, and in which the left- and right- sequences of modifiers were treated as independent from one another for computational efficiency reasons, our approach allows the model to access information from the entirety of both the left and the right sequences jointly.", "startOffset": 136, "endOffset": 151}, {"referenceID": 18, "context": "Second-order graph based dependency parsers (McDonald, 2006; Eisner, 2000) also condition on the current outermost dependent when generating its sibling.", "startOffset": 44, "endOffset": 74}, {"referenceID": 9, "context": "Second-order graph based dependency parsers (McDonald, 2006; Eisner, 2000) also condition on the current outermost dependent when generating its sibling.", "startOffset": 44, "endOffset": 74}, {"referenceID": 11, "context": "We follow a (projective) bottom-up parsing strategy, similar to the easy-first parsing algorithm of Goldberg and Elhadad (2010).", "startOffset": 100, "endOffset": 128}, {"referenceID": 11, "context": "This parsing algorithm is both sound and complete with respect to the class of projective dependency trees (Goldberg and Elhadad, 2010).", "startOffset": 107, "endOffset": 135}, {"referenceID": 28, "context": "2010) and works that extend it (Tratz and Hovy, 2011; Ma et al., 2012; Ma et al., 2013).", "startOffset": 31, "endOffset": 87}, {"referenceID": 13, "context": "2010) and works that extend it (Tratz and Hovy, 2011; Ma et al., 2012; Ma et al., 2013).", "startOffset": 31, "endOffset": 87}, {"referenceID": 14, "context": "2010) and works that extend it (Tratz and Hovy, 2011; Ma et al., 2012; Ma et al., 2013).", "startOffset": 31, "endOffset": 87}, {"referenceID": 11, "context": "The Easy-First parsing algorithm works in O(n log n) time (Goldberg and Elhadad, 2010).", "startOffset": 58, "endOffset": 86}, {"referenceID": 3, "context": "Which action should be chosen? A sensible option is to defineG as the set of actions that can lead to the gold tree, and following the highest scoring actions in this set. However, using training in this manner tends to suffer from error propagation at test time. The parser sees only states that result from following correct actions. The lack of examples containing errors in the training phase makes it hard for the parser to infer the best action given partly erroneous trees. In order to cope with this, we follow the error exploration training strategy, in which we let the parser follow the highest scoring action in A during training even if this action is incorrect, exposing it to states that result from erroneous decisions. This strategy requires defining the set G such that the correct actions to take are well-defined also for states that cannot lead to the gold tree. Such a set G is called a dynamic oracle. Error-exploration and dynamic-oracles were introduced by Goldberg and Nivre (2012).", "startOffset": 23, "endOffset": 1008}, {"referenceID": 7, "context": "For comparison purposes we followed the setup of (Dyer et al., 2015).", "startOffset": 49, "endOffset": 68}, {"referenceID": 15, "context": "Data For English, we used the Stanford Dependency (SD) (de Marneffe and Manning, 2008) conversion of the Penn Treebank (Marcus et al., 1993), using the standard train/dev/test splitswith the same predicted POS-tags as used in (Dyer et al.", "startOffset": 119, "endOffset": 140}, {"referenceID": 31, "context": "1 (CTB5), using the train/test/dev splits of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof-speech tags, also following (Dyer et al.", "startOffset": 45, "endOffset": 87}, {"referenceID": 7, "context": "1 (CTB5), using the train/test/dev splits of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof-speech tags, also following (Dyer et al.", "startOffset": 45, "endOffset": 87}, {"referenceID": 7, "context": ", 2015) with gold partof-speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014).", "startOffset": 53, "endOffset": 96}, {"referenceID": 2, "context": ", 2015) with gold partof-speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014).", "startOffset": 53, "endOffset": 96}, {"referenceID": 7, "context": "When using external word embeddings, we also use the same data as (Dyer et al., 2015).", "startOffset": 66, "endOffset": 85}, {"referenceID": 32, "context": "The different systems and the numbers reported from them are taken from: ZhangNivre11: (Zhang and Nivre, 2011); Martins13: (Martins et al.", "startOffset": 87, "endOffset": 110}, {"referenceID": 17, "context": "The different systems and the numbers reported from them are taken from: ZhangNivre11: (Zhang and Nivre, 2011); Martins13: (Martins et al., 2013); Weiss15 (Weiss et al.", "startOffset": 123, "endOffset": 145}, {"referenceID": 30, "context": ", 2013); Weiss15 (Weiss et al., 2015); Pei15: (Pei et al.", "startOffset": 17, "endOffset": 37}, {"referenceID": 20, "context": ", 2015); Pei15: (Pei et al., 2015); LeZuidema14 (Le and Zuidema, 2014); Zhu15: (Zhu et al.", "startOffset": 16, "endOffset": 34}, {"referenceID": 33, "context": ", 2015); LeZuidema14 (Le and Zuidema, 2014); Zhu15: (Zhu et al., 2015).", "startOffset": 52, "endOffset": 70}, {"referenceID": 22, "context": "tors is using recursive neural networks (Goller and K\u00fcchler, 1996; Socher et al., 2010; Tai et al., 2015).", "startOffset": 40, "endOffset": 105}, {"referenceID": 27, "context": "tors is using recursive neural networks (Goller and K\u00fcchler, 1996; Socher et al., 2010; Tai et al., 2015).", "startOffset": 40, "endOffset": 105}, {"referenceID": 22, "context": "However, the functions are usually restricted to having a fixed maximum arity (usually two) (Socher et al., 2010; Tai et al., 2015; Socher, 2014).", "startOffset": 92, "endOffset": 145}, {"referenceID": 27, "context": "However, the functions are usually restricted to having a fixed maximum arity (usually two) (Socher et al., 2010; Tai et al., 2015; Socher, 2014).", "startOffset": 92, "endOffset": 145}, {"referenceID": 25, "context": "However, the functions are usually restricted to having a fixed maximum arity (usually two) (Socher et al., 2010; Tai et al., 2015; Socher, 2014).", "startOffset": 92, "endOffset": 145}, {"referenceID": 27, "context": "To cope with the vanishing gradients, (Tai et al., 2015) enrich the composition function with a gating mechanism similar to that of the LSTM, resulting in the so-called Tree-LSTM model.", "startOffset": 38, "endOffset": 56}, {"referenceID": 27, "context": "by using a bag-of-modifiers representation or a convolutional layer (Tai et al., 2015; Zhu et al., 2015).", "startOffset": 68, "endOffset": 104}, {"referenceID": 33, "context": "by using a bag-of-modifiers representation or a convolutional layer (Tai et al., 2015; Zhu et al., 2015).", "startOffset": 68, "endOffset": 104}, {"referenceID": 20, "context": "In terms of parsing with vector representations, there are four dominant approaches: search based parsers that use local features that are fed to a neural-network classifier (Pei et al., 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al.", "startOffset": 174, "endOffset": 217}, {"referenceID": 6, "context": "In terms of parsing with vector representations, there are four dominant approaches: search based parsers that use local features that are fed to a neural-network classifier (Pei et al., 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al.", "startOffset": 174, "endOffset": 217}, {"referenceID": 2, "context": ", 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al., 2015), sometimes coupled with a node composition function (Dyer et al.", "startOffset": 135, "endOffset": 179}, {"referenceID": 30, "context": ", 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al., 2015), sometimes coupled with a node composition function (Dyer et al.", "startOffset": 135, "endOffset": 179}, {"referenceID": 7, "context": ", 2015), sometimes coupled with a node composition function (Dyer et al., 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al.", "startOffset": 60, "endOffset": 106}, {"referenceID": 29, "context": ", 2015), sometimes coupled with a node composition function (Dyer et al., 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al.", "startOffset": 60, "endOffset": 106}, {"referenceID": 22, "context": ", 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produced a k-best list of parses using a traditional parsing technique, and then scores the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al.", "startOffset": 124, "endOffset": 184}, {"referenceID": 26, "context": ", 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produced a k-best list of parses using a traditional parsing technique, and then scores the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al.", "startOffset": 124, "endOffset": 184}, {"referenceID": 23, "context": ", 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produced a k-best list of parses using a traditional parsing technique, and then scores the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al.", "startOffset": 124, "endOffset": 184}, {"referenceID": 33, "context": ", 2013a); and parse-reranking approaches that first produced a k-best list of parses using a traditional parsing technique, and then scores the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al., 2015).", "startOffset": 200, "endOffset": 262}, {"referenceID": 22, "context": "Unlike the bottom-up parser in (Socher et al., 2010) who only parses sentences of up to 15 words and the parser of (Stenetorp, 2013) who achieves very low parsing accuracies, we parse arbitrary sentences with state-of-the-art accuracy.", "startOffset": 31, "endOffset": 52}, {"referenceID": 26, "context": ", 2010) who only parses sentences of up to 15 words and the parser of (Stenetorp, 2013) who achieves very low parsing accuracies, we parse arbitrary sentences with state-of-the-art accuracy.", "startOffset": 70, "endOffset": 87}, {"referenceID": 23, "context": "Unlike the bottom up parser in (Socher et al., 2013a) we do not make use of a grammar.", "startOffset": 31, "endOffset": 53}, {"referenceID": 30, "context": "The parser of (Weiss et al., 2015) obtains exceptionally high results using local features and no composition function.", "startOffset": 14, "endOffset": 34}, {"referenceID": 7, "context": "Finally, perhaps closest to our approach is the greedy, transition-based parser of (Dyer et al., 2015) that also works in a bottom-up fashion, and incorporates and LSTM encoding of the input tokens and hierarchical vector composition into its scoring mechanism.", "startOffset": 83, "endOffset": 102}, {"referenceID": 24, "context": "training tree representation for capturing sentiment (Socher et al., 2013b; Tai et al., 2015), semantic sentence similarity (Marelli et al.", "startOffset": 53, "endOffset": 93}, {"referenceID": 27, "context": "training tree representation for capturing sentiment (Socher et al., 2013b; Tai et al., 2015), semantic sentence similarity (Marelli et al.", "startOffset": 53, "endOffset": 93}, {"referenceID": 16, "context": ", 2015), semantic sentence similarity (Marelli et al., 2014) or textual inference (Bowman et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 1, "context": ", 2014) or textual inference (Bowman et al., 2015).", "startOffset": 29, "endOffset": 50}], "year": 2017, "abstractText": "We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders. To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving state-of-the-art accuracies for English and Chinese, without relying on external word embeddings. The parser\u2019s implementation is available for download at the first author\u2019s webpage.", "creator": "TeX"}}}