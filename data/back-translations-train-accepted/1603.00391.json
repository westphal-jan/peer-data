{"id": "1603.00391", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2016", "title": "Noisy Activation Functions", "abstract": "Common activation functions used in neural networks can yield to training difficulties due to the saturation behavior of the activation function, which may hide dependencies which are not visible to first order (using only gradients). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that some gradients may sometimes flow, even if the noiseless application of the activation function would yield zero gradient. Large noise will dominate the noise-free gradient and allow stochastic gradient descent to be more exploratory. By adding noise only to the problematic parts of the activation function we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by by noisy variants helps training in many contexts, yielding state-of-the-art results on several datasets, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results.", "histories": [["v1", "Tue, 1 Mar 2016 18:30:15 GMT  (784kb,D)", "http://arxiv.org/abs/1603.00391v1", null], ["v2", "Sun, 6 Mar 2016 20:51:57 GMT  (784kb,D)", "http://arxiv.org/abs/1603.00391v2", null], ["v3", "Sun, 3 Apr 2016 21:41:47 GMT  (788kb,D)", "http://arxiv.org/abs/1603.00391v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["\u00e7aglar g\u00fcl\u00e7ehre", "marcin moczulski", "misha denil", "yoshua bengio"], "accepted": true, "id": "1603.00391"}, "pdf": {"name": "1603.00391.pdf", "metadata": {"source": "META", "title": "Noisy Activation Functions", "authors": ["Caglar Gulcehre", "Marcin Moczulski", "Misha Denil", "Yoshua Bengio"], "emails": ["GULCEHRC@IRO.UMONTREAL.CA", "MARCIN.MOCZULSKI@STCATZ.OX.AC.UK", "MISHA.DENIL@GMAIL.COM", "BENGIOY@IRO.UMONTREAL.CA"], "sections": [{"heading": "1. Introduction", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "2. Saturating Activation Functions", "text": "It is not in such a way as if it were about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, and in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way and in which it is about a way, in which it is about which it is about a way and in which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which is about which it is about which is about which it is about which it is about which is about which it is about which it is about which it is about which is about which it is about which is about which is about which it is about which it is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which"}, {"heading": "3. Annealing with Noisy Activation Functions", "text": "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "4. Adding Noise Where the Unit Would Saturate", "text": "A novel idea behind the proposed noise activation is that the noise added to nonlinearity is proportional to the magnitude of the saturation of nonlinearity. For Hardsigmoid (x) and Hard-tanh (x), due to our parameterization of noise, which is reflected in the fact that noise is only added when the hardlinearity is saturated. This differs from previous proposals such as the Bengio Noisy Rectifier (2013), where noise is added just before a rectifier unit (ReLU), regardless of whether the input is in the linear mode or in the saturating regime of nonlinearity.The motivation is to keep the training signal clean when the unit is not saturated (typically linear) and to provide a loud signal when the unit is in the saturated regime. In this paper, h (x) refers to the hard function activator-like the signature."}, {"heading": "4.1. Derivatives in the saturated regime", "text": "In the simplest case of our method, we rely on an unbiased distribution, e.g. a standard standard standard. In this case, we choose \u00b5 = 0 to fulfill Equation 7, and therefore have \u03c6 (x) = h (x) + \u03c3 (x) Because of our parameterization of \u03c3 (x), if our stochastic activation function behaves exactly like the persistent activation h that leads to familiar territory. Because we will be 0. For the moment, let us limit our attention to the case when | x | xt and h are saturated. In this case, however, the derivative of h (x) is exactly zero when we condition what we have \"(x) = 4 (x) = 3 (x) = 3 (x) = 3 (x), which is not zero, which is almost certain. In the unsaturated regime, where we can use the optimization regime, the structure depends on the linear formation of the information."}, {"heading": "4.2. Pushing Activations Towards Linear Regime", "text": "An unsatisfactory aspect of the unbiased noise formulation is that, depending on the value of noise, occasionally the gradient of noise points in the wrong direction. This can cause a backward-looking message that would push x in a direction that would degrade the objective function in which we would prefer these messages to push the saturated unit back toward an unsaturated state in which the gradient of h (x) can be used. An easy way to achieve this is to ensure that the noise is always positive, and to manually adjust its character to the character of x. Specifically, we could adjust the character of x (x) = \u2212 sgn (x) sgn (x) s = \u00b5 (x). Where we have the noise as before and sgn is the drawing function, so that sgn (x) is greater than or equal to 0."}, {"heading": "5. Adding Noise to Input of the Function", "text": "Adding noise with a specified standard deviation to the input of the activation function was investigated for ReLU activation functions (Nair & Hinton, 2010; Bengio et al., 2013).\u03c6 (x, \u0442) = h (x + \u03c3\u0442) and \u0435\u043d\u044b\u0435 N (0, 1) (14) In Eqn 14 we offer parameterization of the loud activation function. \u03c3 can be learned either as in Eqn 9 or fixed as hyperparameter. The condition in Eqn 5 is only met if \u03c3 is learned. Experimentally, we found small values that work better. At fixed and small values, if x is greater and farther from the threshold xt, noise will be less likely to push the activations back into the linear regime. We also investigated the effect of injecting input noise when the activations are saturated:? (x + 1) = h (x + 1 | x \u2265 | 15) (0) and vice versa."}, {"heading": "6. Experimental Results", "text": "In our experiments, we used noise only during training: at test time, we replaced the noise variable with its expected value. We performed all our experiments with only a drop replacement of the activation functions in existing experimental arrangements, without changing the previously set hyperparameters. Therefore, it is plausible that better results could be achieved by carefully adjusting the models with loud activation functions to hyperparameters. In all our experiments, we randomly initialized p-uniform from the range [\u2212 1, 1]. We provide experimental results by noisy activations with normal (NAN), semi-normal noise (NAH), normal noise during function input (NANI) and normal noise injected into the input of the function when the unit is saturated (NANIS)."}, {"heading": "6.1. Exploratory Analysis", "text": "In Figure 5, we showed the learning curves of different types of activations with different types of noise as opposed to the tanh and hard tanh units. Models are single-layer MLPs trained on MNIST for classification, and we show the average negative log probability \u2212 logP (correct class | input). Models that use loud activations converge faster than those that use tanh and hard tanh activation functions and rank NLL lower than the tanh network.We trained 3-layer MLP on a dataset generated from a mixture of 3 Gaussian distributions with different means and standard deviations. Each layer of the MLP contains 8 hidden units. Both the model with tanh and noisy \u2212 tanh activations was able to solve this task almost perfectly."}, {"heading": "6.2. Learning to Execute", "text": "The problem of predicting the results of a short program introduced in (Zaremba & Sutskever, 2014) 1 turned out to be chal-1The code can be found at https: / / github.com / wojciechz / learning _ to _ execute. We thank authorship for modern deep-learning architectures. Authors had to use curricula (Bengio et al., 2009) to first capture knowledge of the simpler examples and increase the difficulty of the examples as the training progressed. We replaced all sigmoid and tanh nonlinearity in the reference model with their noisy counterparts. We changed the default gradient clipping to 5 out of 10 to avoid numerical stability problems.When evaluating a network, the length (number of rows) of the executed programs was set to 6 and nesting to 3, which are default settings in the released code for these tasks. Both the reference model and the teaching model were best illustrated with activations that \"the most important is when the results are combined with the activation of the public.\""}, {"heading": "6.3. Penntreebank Experiments", "text": "We trained a two-layer word-level LSTM language model at the Penntreebank. We used the same model that Zaremba et al. (2014) 2. We simply replaced all sigmoid and tanh units with loud hard sigmoid and hard tanh units. The reference model is a very finely tuned strong baseline (Zaremba et al., 2014). For the noisy experiments, we used exactly the same setting, but lowered the threshold for gradient clipping to 5 out of 10. We provide the results of various models in Table 2. In terms of validation and test performance, we did not observe much difference between the additive noise from normal and semi-normal distributions, but there is a significant improvement due to noise, which, as far as we know, makes this result a new state of the art in this task."}, {"heading": "6.4. Neural Machine Translation Experiments", "text": "We have used a neural machine translation (NMT) model on the Europarl dataset with the neural attention model2We have used the code from https: / / github.com / wojzaremba / lstmTable 3. Image Caption Generation on Flickr8k. This time we have added noisy activations in the code from (Xu et al., 2015) and obtained significant improvements in the higher quality BLEU results and the METEOR metric as well as in NLL. Soft attention and hard attention refers here to the use of backprop compared to REINFORCE in the formation of the attention mechanism. BLEU -2 BLEU-3 BLEU-4 METEOR test NLLSoft Attention (Sigmoid and Tanh) (Reference) 67 44.8 19.5 18.9 40.33 Soft Attention (Noisy Sigmoid and Tanh)."}, {"heading": "6.5. Image Caption Generation Experiments", "text": "As a reference model, we used the soft neural attention model proposed in (Xu et al., 2015). 4 We reduced the orthogonally initialized weight matrices by a factor of 0.01. As shown in Table 3, we achieved better results than the reference model and our model also performed better than the best in terms of the meteor score (Xu et al., 2015)."}, {"heading": "6.6. Experiments with Continuation", "text": "We conducted experiments to validate the effect of annealing noise to obtain a sequence method for neural networks.We designed a new task in which, given a random sequence of integers, the goal is to predict the number of unique elements in the sequence. We used an LSTM network via the input sequence and performed a time-average merge of the hidden states of the LSTM to obtain a fixed-size vector. We fed the pooled LSTM representation into a simple (a hidden layer) ReLU-MLP to predict the unique number of elements in the input sequence. In the experiments, we set the length of the input sequence to 26 and the input values are between 0 and 10. To activate the noise, we started training with the Hyperparameter Scale 4We used the code available at https: / github.com / Test vinkelu / STARK 59.5% error LP."}, {"heading": "7. Conclusion", "text": "Nonlinearities in neural networks are both a blessing and a curse. A blessing, because they represent more complicated functions, and a curse, because it makes optimization more difficult. Thus, in several of our experiments, we found that the use of a hard version (i.e. non-linear) of sigmoid and tanh nonlinearities often led to improved results. In the past, various strategies were proposed to solve the difficult optimization problem associated with the formation of some deep networks, including learning according to curricula, which is an approximate form of continuation. Previous work also included attenuated versions of nonlinearities, which would be progressively made more difficult during training. Motivated by this earlier work, we introduce and formalize the concept of loud activations as a general framework for the injection of noise in nonlinear functions, so that large noise allows the SGD to be more exploratory. We propose a specific noisy activation unit, which would only emit noise in the region that would otherwise affect the gradient level of the unit."}, {"heading": "Acknowledgements", "text": "The authors would like to acknowledge the support of the following research funding and computer support agencies: NSERC, Calcul Que'bec, Compute Canada, Samsung, the Canada Research Chairs and CIFAR. We would also like to thank the developers of Theano 5 for developing such a powerful tool for scientific computing. Caglar Gulcehre also thanks IBM Watson Research and the Statistical Knowledge Discovery Group at IBM Research for supporting this work during his internship. 5http: / / deeplearning.net / software / theano /"}], "references": [{"title": "Numerical Continuation Methods", "author": ["E.L. Allgower", "K. Georg"], "venue": "An Introduction. Springer-Verlag,", "citeRegEx": "Allgower and Georg,? \\Q1980\\E", "shortCiteRegEx": "Allgower and Georg", "year": 1980}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Estimating or propagating gradients through stochastic neurons", "author": ["Bengio", "Yoshua"], "venue": "Technical Report arXiv:1305.2982, Universite de Montreal,", "citeRegEx": "Bengio and Yoshua.,? \\Q2013\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2013}, {"title": "Curriculum learning", "author": ["Bengio", "Yoshua", "Louradour", "Jerome", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In ICML\u201909,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Bengio", "Yoshua", "L\u00e9onard", "Nicholas", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Visualizing and understanding recurrent networks", "author": ["Karpathy", "Andrej", "Johnson", "Justin", "Fei-Fei", "Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Neelakantan", "Arvind", "Vilnis", "Luke", "Le", "Quoc V", "Sutskever", "Ilya", "Kaiser", "Lukasz", "Kurach", "Karol", "Martens", "James"], "venue": "arXiv preprint arXiv:1511.06807,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "It is thanks to ReLU that for the first time a few years ago it was shown (Glorot et al., 2011) that very deep purely supervised networks can be trained, whereas using tanh nonlinearity only allowed to train shallow networks.", "startOffset": 74, "endOffset": 95}, {"referenceID": 6, "context": "A plausible hypothesis about the recent surge of interest on these piecewise-linear activation functions (Glorot et al., 2011), is due to the fact that they are easier to optimize with SGD and backpropagation than smooth activation functions, such as sigmoid and tanh.", "startOffset": 105, "endOffset": 126}, {"referenceID": 4, "context": "Adding noise to the activation function has been considered for ReLU units and was explored in (Bengio et al., 2013; Nair & Hinton, 2010) for feed-forward networks and Boltzmann machines to encourage units to explore more and make the optimization easier.", "startOffset": 95, "endOffset": 137}, {"referenceID": 5, "context": "More recently there has been a resurgence of interest in more elaborate \u201cgated\u201d architectures such as LSTMs (Hochreiter & Schmidhuber, 1997) and GRUs (Cho et al., 2014), but also encompassing neural attention mechanisms that have been used in the NTM (Graves et al.", "startOffset": 150, "endOffset": 168}, {"referenceID": 7, "context": ", 2014), but also encompassing neural attention mechanisms that have been used in the NTM (Graves et al., 2014), Memory Networks (Weston et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 13, "context": ", 2014), automatic image captioning (Xu et al., 2015) and wide areas of applications (LeCun et al.", "startOffset": 36, "endOffset": 53}, {"referenceID": 10, "context": "Although gates often operate in the soft-saturated regime (Karpathy et al., 2015; Bahdanau et al., 2014; Hermann et al., 2015) the architecture prevents them from being fully open or closed.", "startOffset": 58, "endOffset": 126}, {"referenceID": 1, "context": "Although gates often operate in the soft-saturated regime (Karpathy et al., 2015; Bahdanau et al., 2014; Hermann et al., 2015) the architecture prevents them from being fully open or closed.", "startOffset": 58, "endOffset": 126}, {"referenceID": 8, "context": "Although gates often operate in the soft-saturated regime (Karpathy et al., 2015; Bahdanau et al., 2014; Hermann et al., 2015) the architecture prevents them from being fully open or closed.", "startOffset": 58, "endOffset": 126}, {"referenceID": 12, "context": "A related approach of adding noise to gradients and annealing the noise was investigated in (Neelakantan et al., 2015) as well.", "startOffset": 92, "endOffset": 118}, {"referenceID": 4, "context": "Adding noise with fixed standard deviation to the input of the activation function has been investigated for ReLU activation functions (Nair & Hinton, 2010; Bengio et al., 2013).", "startOffset": 135, "endOffset": 177}, {"referenceID": 3, "context": "The authors had to use curriculum learning (Bengio et al., 2009) to let the model capture knowledge about the easier examples first and increase the level of difficulty of the examples further down the training.", "startOffset": 43, "endOffset": 64}, {"referenceID": 14, "context": "The reference model is a well-finetuned strong baseline from (Zaremba et al., 2014).", "startOffset": 61, "endOffset": 83}, {"referenceID": 14, "context": "We used the same model proposed by Zaremba et al. (2014)2.", "startOffset": 35, "endOffset": 57}, {"referenceID": 13, "context": "This time we added noisy activations in the code from (Xu et al., 2015) and obtain substantial improvements on the higher-order BLEU scores and the METEOR metric, as well as in NLL.", "startOffset": 54, "endOffset": 71}, {"referenceID": 14, "context": "We only replaced in the code from Zaremba et al. (2014) the sigmoid and tanh by corresponding noisy variants and observe a substantial improvement in perplexity, which makes this the state-of-theart on this task.", "startOffset": 34, "endOffset": 56}, {"referenceID": 1, "context": "(Bahdanau et al., 2014)3.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Using existing code from (Bahdanau et al., 2014) with nonlinearities replaced by their noisy versions, we find much improved performance (2 BLEU points is considered a lot for machine translation).", "startOffset": 25, "endOffset": 48}, {"referenceID": 13, "context": "We used the soft neural attention model proposed in (Xu et al., 2015) as our reference model.", "startOffset": 52, "endOffset": 69}, {"referenceID": 13, "context": "As shown in Table 3, we were able to obtain better results than the reference model and our model also outperformed the best model provided in (Xu et al., 2015) in terms of Meteor score.", "startOffset": 143, "endOffset": 160}, {"referenceID": 7, "context": "As a second test, we used the same annealing procedure in order to train a neural turing machine (NTM) on the associative recall task (Graves et al., 2014).", "startOffset": 134, "endOffset": 155}], "year": 2016, "abstractText": "Common activation functions used in neural networks can yield to training difficulties due to the saturation behavior of the activation function, which may hide dependencies which are not visible to first order (using only gradients). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that some gradients may sometimes flow, even if the noiseless application of the activation function would yield zero gradient. Large noise will dominate the noise-free gradient and allow stochastic gradient descent to be more exploratory. By adding noise only to the problematic parts of the activation function we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by by noisy variants helps training in many contexts, yielding state-of-the-art results on several datasets, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results.", "creator": "LaTeX with hyperref package"}}}