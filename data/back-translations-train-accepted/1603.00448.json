{"id": "1603.00448", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2016", "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization", "abstract": "Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.", "histories": [["v1", "Tue, 1 Mar 2016 20:35:56 GMT  (4434kb,D)", "https://arxiv.org/abs/1603.00448v1", null], ["v2", "Thu, 3 Mar 2016 07:30:36 GMT  (4434kb,D)", "http://arxiv.org/abs/1603.00448v2", null], ["v3", "Fri, 27 May 2016 16:53:46 GMT  (4706kb,D)", "http://arxiv.org/abs/1603.00448v3", "International Conference on Machine Learning (ICML), 2016, to appear"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO", "authors": ["chelsea finn", "sergey levine", "pieter abbeel"], "accepted": true, "id": "1603.00448"}, "pdf": {"name": "1603.00448.pdf", "metadata": {"source": "META", "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization", "authors": ["Chelsea Finn", "Sergey Levine", "Pieter Abbeel"], "emails": ["CBFINN@EECS.BERKELEY.EDU", "SVLEVINE@EECS.BERKELEY.EDU", "PABBEEL@EECS.BERKELEY.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2. Related Work", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3. Preliminaries and Overview", "text": "We build on the probabilistic maximum entropy inverse optimal control frameworks (Ziebart et al., 2008). The demonstrated behavior is assumed to be the result of an expert acting stochastically and almost optimally in relation to an unknown cost function. Specifically, the model assumes that the expert samples are the detected trajectories {\u03c4i} from the distribution p (\u03c4) = 1Z exp (\u2212 c\u03b8))), (1) where \u03c4 = {x1, u1,.., xT} is a trajector sample, c\u03b8 (\u03c4) = \u2211 t c\u03b8 (xt, ut) is an unknown cost function, parameterized by \u03b8, and xt and ut are the state and action at the appropriate time. Under this model, the expert is most likely to act optimally, and can generate suboptimal trajectories with a probability that decreases exponentially as the trajectories become costly.The partition function Z is difficult to calculate or perform as a persistent challenge to linear functions."}, {"heading": "4. Guided Cost Learning", "text": "In this section, we will describe the controlled cost-learning algorithm that combines sample-based maximum entropy IOC with forward-looking amplification learning using temporally varying linear models. The central idea behind this method is to adapt the sample distribution to the maximum entropy cost distribution p (\u03c4) = 1Z exp (\u2212 c\u03b8 (\u03c4) by directly optimizing a trajectory distribution with respect to the current cost c\u03b8 (\u03c4) using a sample-efficient amplification learning algorithm. Samples generated on the physical system are used both to improve policy and to more accurately estimate the partition function Z. In this way, the amplification learning step \"steers\" the sample distribution in regions where the samples are more useful for estimating the partition function. We will first describe how the IOC target can be estimated in Equation (1) using samples, and then describe how the sample distribution can fit the sample learning."}, {"heading": "4.1. Sample-Based Inverse Optimal Control", "text": "In the sample-based approach to maximum entropy IOC, the partition function Z = sed exp (\u2212 cctuas et al., 2011; Kalakrishnan et al., 2013) is estimated. Instead, in this section, a sample-based approximation for the IOC target for a general nonlinear parameterization of the cost function is derived. Negative log probability corresponding to the IOC model in Equation (1) is given by: LIOC (\u03b8) = 1N."}, {"heading": "4.2. Adaptive Sampling via Policy Optimization", "text": "Choosing the background sample distribution q (\u03c4) to estimate the objective LIOC is critical to the successful application of the sample-based IOC method. Therefore, the optimal importance of the sample distribution to estimate the partition function is relatively difficult when the cost distribution is unknown. Instead, we can adaptively reproduce the behavior q (\u03c4) to generate more samples in these regions of the development area that are good according to the current cost function. To this end, we rely on the IOC optimization, which seeks to find the cost function that maximizes the probability of demonstrations, with a policy optimization process that improves the distribution practice. Since one of the main benefits of the sample-based IOC approach is the ability to handle unknown dynamics, we must choose a method that improves the distribution practice."}, {"heading": "4.3. Cost Optimization and Importance Weights", "text": "The IOC target can be optimized by using the nonlinear optimization methods and the gradient dLIOCd\u03b8. Stochastic gradient methods are often preferred for high-dimensional functional approximation methods such as neural networks. Such methods are easy to apply to targets factorized via the training samples, but the partition function does not factorize in this way. Nevertheless, we found that our goal could still be optimized using stochastic gradient methods by selecting a subset of demonstrations and background samples in each iteration. If the number of samples in the batch is small, we found it necessary to add the sampled demonstrations of the background sample as well. The goal can be unlimited and often does in practice. The stochastic optimization method is presented in Algorithm 2 and is easy to implement with most neural network libraries based on backpropagation. Estimulation."}, {"heading": "4.4. Learning Costs and Controllers", "text": "Unlike many previous methods of the IOC and IRL, our approach can be used to learn costs and at the same time to optimize policies for a new task, not in Algorithm 2 Nonlinear IOC with stochastic gradients 1: for iteration k = 1 to K do 2: Example demonstration batch D: Example demonstration batch D: Example background batch D: Example background batch D: Example sample D: Example sample sample D: Example sample sample D: Example sample sample D: Example sample D: Example sample D: Example sample D: Example sample D: Example sample D: Example sample D: Example sample D: Example sample 7: End of 8: Return optimized cost parameters for the demos, such as a new position of a target item for a casting task as shown in our experiments. Since the algorithm produces both a cost function control (xt, ut) and a controller (return), we can directly optimize this task."}, {"heading": "5. Representation and Regularization", "text": "Our experiments in Section 6.2 confirm that an affinity cost function is not meaningful enough to learn some behaviors. Neural network parameterization is particularly useful for learning visual representation on raw image pixels. (2016) Learning cost functions on raw pixels is an interesting direction for future work, which we propose in Section 7.While the expressiveness of nonlinear cost functions offers a number of benefits, they introduce significant model complexity into an already under-specified IOC target. To mitigate this challenge, we propose two regulatory methods for IOC. Prior methods regulate the IOC target by adopting the \"1 or\" 2 standard of cost parameterization functions (Calametrisation and Ziebart 2010), applying costs directly to the state of a robotic system without any handcrafted functions."}, {"heading": "6. Experimental Evaluation", "text": "We evaluated our random IOC algorithm against a range of robot control tasks, both in simulation and on a real robotic platform. Each of the experiments involved complex second-order dynamics with force or torque control and no manually designed cost function features, with the raw state provided as input for the learned cost function. We also tested the consistency of our algorithm using an example of a toy point mass distribution for which the basic truth distribution is known. These experiments, which are fully discussed in Appendix B, show that both the use of a maximum entropy version of the policy optimization goal (see Section 4.2) and the use of weights of importance are necessary to restore the true distribution."}, {"heading": "6.1. Simulated Comparisons", "text": "We focus on the task and the example complexity and also make comparisons about two different distribution methods and regulations. The first task is 2D navigation around obstacles, modelled on the task set by Levine & Koltun (2012). This task has a simple, linear dynamics and a low-dimensional state space, but a complex cost function, which we visualize in Figure 2. The second task involves a 3-link arm, which leads to a target position in 2D, in which the presence of physical obstacles is achieved. The third, most challenging task is 3D peg insertion with a 7 DOF. This task is much more difficult than tasks that were previously evaluated."}, {"heading": "6.2. Real-World Robotic Control", "text": "In fact, most of us are able to set out in search of new ways to conquer the world."}, {"heading": "7. Discussion and Future Work", "text": "We have an inverse, optimal control algorithm that can learn complex, nonlinear cost representations, such as neural networks, and apply it to high-dimensional systems with unknown dynamics, using a sample-based approach to the IOC's maximum entropy target, with samples generated directly from the raw state of the system (Levine & Abbeel, 2014). To our knowledge, this approach is the first to combine the benefits of a sample-based IOC with unknown dynamics with non-linear cost representations that directly exploit the raw state of the system without the need for manual feature engineering, allowing us to apply our method to a variety of robotic manipulation tasks."}, {"heading": "Acknowledgements", "text": "This research was partially funded by the ONR through a Young Investigator Program Award, the Army Research Office through the MAST Program and an NSF Scholarship. We thank Anca Dragan for thoughtful discussions."}, {"heading": "A. Policy Optimization under Unknown Dynamics", "text": "The method of linearity optimisation described by Levine and Abbeel (Levine & Abbeel, 2014), which we summarize in this Annex, follows the method described by Levine and Abbeel (Levine & Abbeel, 2014), which can be optimised by iterative optimisation Eq (x1) q (xt + 1) q (xt) q (ut) q (ut) q (ut) q (ut) q (ut) q (t) q (t) q (t) q (t) q (t) q (t) q (ut) q (t) q (t) q (t) q (t) q (t) x (t) c) x (e) c) x (c) c c c c c c c c c c c (c) c) c (c) c) c (c) c) (c) x (x) c (x) c) c (c) x (c) c (c) c) c (c) (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c (c) c) c (c) c) c (c) c) c (c) c (c) c) c (c) c) c) c (c (c) c) c (c) c (c) c) c (c) c) c (c) c (c) c) c (c) c (c) (c) c) c (c) c (c) c) c) c (c (c) c) c (c) c) c (c) (c) c (c) c) c (c) c) c (c) c) c (c (c) c) c) c (c (c) c) c (c) c) c (c (c) c) c (c) c) c (c) c (c) c) (c) c) c) c ("}, {"heading": "B. Consistency Evaluation", "text": "We evaluated the consistency of our algorithm by generating 40 demonstrations from 4 known linear Gaussian trajectory distributions of a second order point system, each of which travels to the origin from different starting positions. The purpose of this experiment is to verify that our method is able to successfully restore the true cost function in simple areas where the exact cost function can be learned. To do this, we measured the KL divergence between the trajectories generated by our method and the true distribution underlying the demonstrations, very close to the true distribution, with a KL divergence of 230.66 added up to over 100 time pauses. Empirically, we estimated the importance weights of the demos, calculating the trajectories with a slightly higher KL divergence of 272.71."}, {"heading": "C. Neural Network Parametrization and Initialization", "text": "We use expressive neural network function approximators to represent the cost, using the form: c\u03b8 (xt, ut) = \u0442 Ayt + b + wu-ut-2This parameterization can be considered as a cost that is square in a series of learned nonlinear features yt = f\u03b8 (xt), where f\u03b8 is a multilayered neural network with corrective nonlinearity of the form max (z, 0). Since simpler cost functions are generally preferred, we initialize f\u03b8 to the identity function by setting the parameters of the first fully connected layer to contain the identity matrix and negative identity matrix (generating hidden units twice the size of the input) and all subsequent layers of the identity matrix. We found that this initialization improved the generalization of the learned costs."}, {"heading": "D. Detailed Description of Task Setup", "text": "This year it has been the case that we will be able to put ourselves at the top, \"he said in an interview with the German Press Agency.\" I am very happy that it has come this far, \"he said.\" I am very happy that it has come this far, \"he said."}, {"heading": "E. Regularization Evaluation", "text": "We evaluated performance with and without each of the two regularization terms proposed in Section 5 on the simulated reach and insert tasks. As shown in Figure 5, both regularization terms support performance. It is noteworthy that the learned paths fail to insert the peg into the hole when the costs are learned at constant rates without local regularization."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.", "creator": "LaTeX with hyperref package"}}}