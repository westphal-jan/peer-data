{"id": "1603.00786", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2016", "title": "Improving Named Entity Recognition for Chinese Social Media with Word Segmentation Representation Learning", "abstract": "Named entity recognition, and other information extraction tasks, frequently use linguistic features such as part of speech tags or chunkings. For languages where word boundaries are not readily identified in text, word segmentation is a key first step to generating features for an NER system. While using word boundary tags as features are helpful, the signals that aid in identifying these boundaries may provide richer information for an NER system. New state-of-the-art word segmentation systems use neural models to learn representations for predicting word boundaries. We show that these same representations, jointly trained with an NER system, yield significant improvements in NER for Chinese social media. In our experiments, jointly training NER and word segmentation with an LSTM-CRF model yields nearly 5% absolute improvement over previously published results.", "histories": [["v1", "Wed, 2 Mar 2016 16:41:40 GMT  (18kb,D)", "http://arxiv.org/abs/1603.00786v1", null], ["v2", "Tue, 28 Mar 2017 19:14:40 GMT  (485kb,D)", "http://arxiv.org/abs/1603.00786v2", "This is the camera ready version of our ACL'16 paper. We also added a supplementary material containing the results of our systems on a cleaner dataset (much higher F1 scores). More information please refer to the repothis https URL"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nanyun peng", "mark dredze"], "accepted": true, "id": "1603.00786"}, "pdf": {"name": "1603.00786.pdf", "metadata": {"source": "CRF", "title": "Learning Word Segmentation Representations to Improve Named Entity Recognition for Chinese Social Media", "authors": ["Nanyun Peng", "Mark Dredze"], "emails": ["npeng1@jhu.edu,", "mdredze@cs.jhu.edu"], "sections": [{"heading": "1 Introduction", "text": "The full mention of this gap and the more specific naming of entities (NER) (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012) has become a popular task for social media analytics (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2012; Liu et al., 2012a). Many downstream applications that use social media, such as relationship extraction (Bunescu and Mooney, 2005) and linking entities (Dredze et al., 2010; Ratinov et al., 2011), rely on the first mentions of entities. Unsurprisingly, the accuracy of NER systems in social media that function as state systems (Dredze et al., 2011)."}, {"heading": "2 Model", "text": "We propose a model that integrates the best Chinese word segmentation system (Chen et al., 2015) and uses a neural LSTM model that learns representations, and the best NER model for Chinese social media (Peng and Dredze, 2015) that supports the training of neural representations by a logbilinear CRF. We start with a brief review of each system."}, {"heading": "2.1 LSTM for Word Segmentation", "text": "Chen et al. (2015) proposed a single layer, left to right LSTM for Chinese word segmentation. An LSTM is a recursive neural network (RNN) that uses a series of gates (input, forgetting, and output gate) to control how memory is propagated in the hidden states of the model. In the Chinese word segmentation task, each Chinese character is initialized as a d-dimensional vector that the LSTM will modify during its training. Furthermore, the model learns a hidden vector h. These vectors are then used with a biased-linear transformation to predict the output labels, which in this case are Begin, Inside, End, and Singleton. A prediction for the position t is used as: y (t) = Woh (t) + bo (1), where the transformation parameters are, bo the bias parameters, and h (t) the hidden state of the tag to modulate them."}, {"heading": "2.2 Log-bilinear CRF for NER", "text": "Peng and Dredze (2015) proposed a logbilinear model for Chinese social media. They used standard NER features along with additional features based on lexical embeddings. By fine-tuning these embeddings and sharing training with a word2vec target (Mikolov et al., 2013), the resulting model is logbilinear. Typical lexical embeddings offer a single embedding vector for each word type. However, Chinese text is not word-segmented, making the correlation between the input and the embedding vector unclear. Peng and Dredze (2015) examined several types of representations for Chinese, including presegmentation of input to obtain words, using character embeddings and a combined approach where embeddings for characters are learned based on their position in the word. This final presentation yielded the greatest improvements."}, {"heading": "2.3 Using Segmentation Representations to Improve NER", "text": "The improvements in character position embeddings presented by Peng and Dredze (2015) suggest that word segmentation information can be helpful for NR. Embeddings aside, one simple way to incorporate this information into an NR system would be to add features to the CRF that use the predicted segmentation markers as traits. 2The same functionality as Aij in Chen et al. (2015) models.However, these features alone could overlook useful information from the segmentation model.Previous work has shown that joint learning of different stages of the NLP pipeline for Chinese (Liu et al., 2012b; Zheng et al., 2013) has helped. Therefore, we are looking for approaches for deeper interaction between word segmentation and NR. LSTM word segmentations learn two different types of representations: 1) embeddings for each character and 2) hidden segmentation prepositioning."}, {"heading": "3 Parameter Estimation", "text": "We train all our models with stochastic gradient descent (SGD), we train for up to 30 epochs, and we stop when the results of NER converge with the data of Dev. We use a separate learning rate for each part of the common goal, with a timetable that reduces the learning rate by half, if the dev results do not improve after 5 consecutive epochs. In the input layer of LSTM according to Chen et al. (2015), a dropout rate is introduced. We optimize two hyperparameters using held dev data: the common coefficient \u03bb in the interval [0.5, 1] and the dropout rate in the interval [0, 0.5]. All other hyperparameters are based on the values of Chen et al. (2015) for the LSTM and Peng and Dredze (2015) for the CRF. We train the common model with an alternating optimization strategy. Since the segment subset is substantially larger than the sample data set of 3NER, we each provide the 4th."}, {"heading": "4 Experiments and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "We use the same training, development, and test splits as Chen et al. (2015) for word segmentation, and Peng and Dredze (2015) for NER.Word segmentation The segmentation data comes from the joint task SIGHAN 2005. We used the PKU part, which includes 43,963 word sets for training and 4,278 sentences for testing. We did not use any special pre-processing; this data set contains 1,890 Sina Weibo messages, which are provided with four entity types (person, organization, location, and geopolitical entity), including name and nominal mentions. We note that the word segmentation data set is significantly larger than the NER data that motivates our subsampling during training (\u00a7 3)."}, {"heading": "4.2 Results and Analysis", "text": "Hyperparameters are matched to the developer data and then applied to the test. We will explain the results.We will start by defining a CRF baseline (# 1) and show that adding segmentation characteristics is helpful (# 2).However, adding these characteristics to the full model (with embedding) in Peng and Dredze (2015) (# 3) does not improve results (# 4).This is probably because the segmentation characteristics already contain segmentation information.Replacing character position embedding with character embedding (# 5) leads to poorer results but benefits from additional segmentation characteristics (# 6).This shows that both word segmentation helps and character position embedding effectively delimits words."}, {"heading": "5 Discussion", "text": "Different interpretations of our results suggest a direction for future functioning. First, we can consider our method as multi-task learning (Caruana, 1997; Collobert and Weston, 2008), in which we use the same learned representations (embedding and hidden vectors) for two tasks: segmentation and NER, which use different prediction and decryption levels. Result 8 shows the result of excluding the additional NER characteristics and sharing commonly trained LSTM. Although this does not work as well as adding the additional NER characteristics (# 11), it is impressive that this simple architecture has produced similar F1 results to the best results in Peng and Dredze (2015). While we can expect both the NER and word segmentation results to improve, we have found that the segmentation performance of the best common model matched to NER is better than the best results in the standalone segment model. While we can expect both the NER- and word segmentation results to improve, we have found that the segmentation performance of the best common model matched to NER is better than the single segment model."}], "references": [{"title": "A shortest path dependency kernel for relation extraction", "author": ["Bunescu", "Mooney2005] Razvan C Bunescu", "Raymond J Mooney"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Chen et al.2015] Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "The unreasonable effectiveness of word representations for twitter named entity recognition. In North America Chapter of Association for Computational Linguistics (NAACL)", "author": ["Cherry", "Guo2015] Colin Cherry", "Hongyu Guo"], "venue": null, "citeRegEx": "Cherry et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cherry et al\\.", "year": 2015}, {"title": "Unsupervised models for named entity classification", "author": ["Collins", "Singer1999] Michael Collins", "Yoram Singer"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Collins et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Collins et al\\.", "year": 1999}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Entity disambiguation for knowledge base population", "author": ["Dredze et al.2010] Mark Dredze", "Paul McNamee", "Delip Rao", "Adam Gerber", "Tim Finin"], "venue": "In Conference on Computational Linguistics (Coling)", "citeRegEx": "Dredze et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dredze et al\\.", "year": 2010}, {"title": "The cips-sighan clp 2012 chineseword segmentation onmicroblog corpora bakeoff", "author": ["Duan et al.2012] Huiming Duan", "Zhifang Sui", "Ye Tian", "Wenjie Li"], "venue": "In Second CIPS-SIGHAN Joint Conference on Chinese Language Processing,", "citeRegEx": "Duan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2012}, {"title": "Annotating named entities in twitter data with crowdsourcing", "author": ["Finin et al.2010] Tim Finin", "William Murnane", "Anand Karandikar", "Nicholas Keller", "Justin Martineau", "Mark Dredze"], "venue": "In NAACL Workshop on Creating Speech and Language Data With", "citeRegEx": "Finin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Finin et al\\.", "year": 2010}, {"title": "Crowdsourcing and annotating NER for Twitter# drift", "author": ["Dirk Hovy", "Anders S\u00f8gaard"], "venue": null, "citeRegEx": "Fromreide et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fromreide et al\\.", "year": 2014}, {"title": "Entity linking and name disambiguation using SVM in chinese micro-blogs", "author": ["Fu et al.2015] JinLan Fu", "Jie Qiu", "Yunlong Guo", "Li Li"], "venue": null, "citeRegEx": "Fu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2015}, {"title": "Chinese word segmentation and named entity recognition: A pragmatic approach", "author": ["Gao et al.2005] Jianfeng Gao", "Mu Li", "Andi Wu", "Chang-Ning Huang"], "venue": "Comput. Linguist.,", "citeRegEx": "Gao et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2005}, {"title": "The task 2 of cips-sighan 2012 named entity recognition and disambiguation in chinese bakeoff", "author": ["He et al.2012] Zhengyan He", "Houfeng Wang", "Sujian Li"], "venue": "In Second CIPS-SIGHAN Joint Conference on Chinese Language Processing,", "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "The fourth international chinese language processing bakeoff: Chinese word segmentation, named entity recognition and chinese pos tagging", "author": ["Jin", "Chen2008] Guangjin Jin", "Xiao Chen"], "venue": "In Sixth SIGHAN Workshop on Chinese Language Process-", "citeRegEx": "Jin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2008}, {"title": "Twiner: Named entity recognition in targeted twitter stream", "author": ["Li et al.2012] Chenliang Li", "Jianshu Weng", "Qi He", "Yuxia Yao", "Anwitaman Datta", "Aixin Sun", "BuSung Lee"], "venue": "In SIGIR Conference on Research and Development in Information", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Recognizing named entities in tweets. In Association for Computational Linguistics (ACL), pages 359\u2013367", "author": ["Liu et al.2011] Xiaohua Liu", "Shaodian Zhang", "Furu Wei", "Ming Zhou"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Joint inference of named entity recognition and normalization for tweets", "author": ["Liu et al.2012a] Xiaohua Liu", "Ming Zhou", "Furu Wei", "Zhongyang Fu", "Xiangyang Zhou"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Joint inference of named entity recognition and normalization for tweets", "author": ["Liu et al.2012b] Xiaohua Liu", "Ming Zhou", "Furu Wei", "Zhongyang Fu", "Xiangyang Zhou"], "venue": "In Proceedings of the 50th Annual Meeting of the Association", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Chinese word segmentation and named entity recognition based on conditional random fields", "author": ["Mao et al.2008] Xinnian Mao", "Yuan Dong", "Saike He", "Sencheng Bao", "Haila Wang"], "venue": "In IJCNLP,", "citeRegEx": "Mao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2008}, {"title": "Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons", "author": ["McCallum", "Li2003] Andrew McCallum", "Wei Li"], "venue": null, "citeRegEx": "McCallum et al\\.,? \\Q2003\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2003}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A survey of named entity recognition and classification", "author": ["Nadeau", "Sekine2007] David Nadeau", "Satoshi Sekine"], "venue": "Lingvisticae Investigationes,", "citeRegEx": "Nadeau et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nadeau et al\\.", "year": 2007}, {"title": "Lexicon infused phrase embeddings for named entity resolution. CoRR, abs/1404.5367", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": null, "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Named entity recognition for chinese social media with jointly trained embeddings", "author": ["Peng", "Dredze2015] Nanyun Peng", "Mark Dredze"], "venue": null, "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Local and global algorithms for disambiguation to wikipedia. In Association for Computational Linguistics (ACL), pages 1375\u20131384", "author": ["Ratinov et al.2011] Lev Ratinov", "Dan Roth", "Doug Downey", "Mike Anderson"], "venue": null, "citeRegEx": "Ratinov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2011}, {"title": "Named entity recognition in tweets: an experimental study", "author": ["Ritter et al.2011] Alan Ritter", "Sam Clark", "Oren Etzioni"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Word representations: a simple and general method for semi-supervised learning. In Association for Computational Linguistics (ACL), pages 384\u2013394", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Unsupervised multi-domain adaptation with feature embeddings", "author": ["Yang", "Eisenstein2015] Yi Yang", "Jacob Eisenstein"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Word segmentation and named entity recognition for sighan bakeoff3", "author": ["Zhang et al.2006] Suxiang Zhang", "Ying Qin", "Juan Wen", "Xiaojie Wang"], "venue": "In Fifth SIGHAN Workshop on Chinese Language Processing,", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}, {"title": "Deep learning for Chinese word segmentation and POS tagging", "author": ["Zheng et al.2013] Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Zheng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "Entity mention detection, and more specifically named entity recognition (NER) (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012), has become a popular task for social media analysis (Finin et al.", "startOffset": 79, "endOffset": 190}, {"referenceID": 7, "context": ", 2012), has become a popular task for social media analysis (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012a).", "startOffset": 61, "endOffset": 180}, {"referenceID": 14, "context": ", 2012), has become a popular task for social media analysis (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012a).", "startOffset": 61, "endOffset": 180}, {"referenceID": 24, "context": ", 2012), has become a popular task for social media analysis (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012a).", "startOffset": 61, "endOffset": 180}, {"referenceID": 8, "context": ", 2012), has become a popular task for social media analysis (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012a).", "startOffset": 61, "endOffset": 180}, {"referenceID": 13, "context": ", 2012), has become a popular task for social media analysis (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012a).", "startOffset": 61, "endOffset": 180}, {"referenceID": 5, "context": "Many downstream applications that use social media, such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011), rely on first identifying mentions of entities.", "startOffset": 126, "endOffset": 169}, {"referenceID": 23, "context": "Many downstream applications that use social media, such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011), rely on first identifying mentions of entities.", "startOffset": 126, "endOffset": 169}, {"referenceID": 24, "context": "While this gap is shrinking in English (Ritter et al., 2011; Cherry and Guo, 2015), it remains large in other languages, such as Chinese (Peng and Dredze, 2015; Fu et al.", "startOffset": 39, "endOffset": 82}, {"referenceID": 9, "context": ", 2011; Cherry and Guo, 2015), it remains large in other languages, such as Chinese (Peng and Dredze, 2015; Fu et al., 2015).", "startOffset": 84, "endOffset": 124}, {"referenceID": 24, "context": "Ritter et al. (2011) annotated Twitter data for these systems to improve a Twitter NER tagger, however, these systems do not exist for so-", "startOffset": 0, "endOffset": 21}, {"referenceID": 25, "context": "The same approach was also found helpful for NER in the news domain (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014)", "startOffset": 68, "endOffset": 138}, {"referenceID": 21, "context": "The same approach was also found helpful for NER in the news domain (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014)", "startOffset": 68, "endOffset": 138}, {"referenceID": 10, "context": "In Asian languages like Chinese, Japanese and Korean, word segmentation is a critical first step (Gao et al., 2005; Zhang et al., 2006; Mao et al., 2008).", "startOffset": 97, "endOffset": 153}, {"referenceID": 27, "context": "In Asian languages like Chinese, Japanese and Korean, word segmentation is a critical first step (Gao et al., 2005; Zhang et al., 2006; Mao et al., 2008).", "startOffset": 97, "endOffset": 153}, {"referenceID": 17, "context": "In Asian languages like Chinese, Japanese and Korean, word segmentation is a critical first step (Gao et al., 2005; Zhang et al., 2006; Mao et al., 2008).", "startOffset": 97, "endOffset": 153}, {"referenceID": 10, "context": "In Asian languages like Chinese, Japanese and Korean, word segmentation is a critical first step (Gao et al., 2005; Zhang et al., 2006; Mao et al., 2008). Peng and Dredze (2015) showed the value of word segmentation to Chinese NER in social media by using character positional embeddings.", "startOffset": 98, "endOffset": 178}, {"referenceID": 1, "context": "We combine state-of-the-art Chinese word segmentation (Chen et al., 2015) with the best Chinese social media NER system (Peng and Dredze, 2015).", "startOffset": 54, "endOffset": 73}, {"referenceID": 1, "context": "We propose a model that integrates the best Chinese word segmentation system (Chen et al., 2015) using an LSTM neural model that learns representations, with the best NER model for Chinese social media (Peng and Dredze, 2015), that supports training neural representations by a log-bilinear CRF.", "startOffset": 77, "endOffset": 96}, {"referenceID": 1, "context": "We used the same model as Chen et al. (2015) trained on the same data (segmented Chinese news article) We employed a different training objective.", "startOffset": 26, "endOffset": 45}, {"referenceID": 1, "context": "We used the same model as Chen et al. (2015) trained on the same data (segmented Chinese news article) We employed a different training objective. Chen et al. (2015) employed a maxmargin, however, while they found this objective yielded better results, we observed that maximumlikelihood yielded better segmentation results in our experiments1.", "startOffset": 26, "endOffset": 166}, {"referenceID": 1, "context": "We were unable to match the results reported by Chen et al. (2015). Our implementation achieved nearly identical results on development data (as inferred from their published figure), but lagged in test accuracy by 2.", "startOffset": 48, "endOffset": 67}, {"referenceID": 19, "context": "By fine-tuning these embeddings, and jointly training them with a word2vec (Mikolov et al., 2013) objective, the resulting model is log-bilinear.", "startOffset": 75, "endOffset": 97}, {"referenceID": 1, "context": "The same functionality as Aij in Chen et al. (2015) model.", "startOffset": 33, "endOffset": 52}, {"referenceID": 28, "context": "Previous work showed that jointly learning different stages of the NLP pipeline helped for Chinese (Liu et al., 2012b; Zheng et al., 2013).", "startOffset": 99, "endOffset": 138}, {"referenceID": 1, "context": "Dropout is introduced in the input layer of LSTM following Chen et al. (2015). We optimize two hyper-parameters using held out dev data: the joint coefficient \u03bb in the interval [0.", "startOffset": 59, "endOffset": 78}, {"referenceID": 1, "context": "Dropout is introduced in the input layer of LSTM following Chen et al. (2015). We optimize two hyper-parameters using held out dev data: the joint coefficient \u03bb in the interval [0.5, 1] and the dropout rate in the interval [0, 0.5]. All other hyper-parameters were set to the values given by Chen et al. (2015) for the LSTM and Peng and Dredze (2015) for the CRF.", "startOffset": 59, "endOffset": 311}, {"referenceID": 1, "context": "Dropout is introduced in the input layer of LSTM following Chen et al. (2015). We optimize two hyper-parameters using held out dev data: the joint coefficient \u03bb in the interval [0.5, 1] and the dropout rate in the interval [0, 0.5]. All other hyper-parameters were set to the values given by Chen et al. (2015) for the LSTM and Peng and Dredze (2015) for the CRF.", "startOffset": 59, "endOffset": 351}, {"referenceID": 19, "context": "We used word2vec (Mikolov et al., 2013) with the same parameter settings as Peng and Dredze (2015) to pre-train the embeddings.", "startOffset": 17, "endOffset": 39}, {"referenceID": 19, "context": "We used word2vec (Mikolov et al., 2013) with the same parameter settings as Peng and Dredze (2015) to pre-train the embeddings.", "startOffset": 18, "endOffset": 99}, {"referenceID": 1, "context": "We use the same training, development and test splits as Chen et al. (2015) for word segmentation and Peng and Dredze (2015) for NER.", "startOffset": 57, "endOffset": 76}, {"referenceID": 1, "context": "We use the same training, development and test splits as Chen et al. (2015) for word segmentation and Peng and Dredze (2015) for NER.", "startOffset": 57, "endOffset": 125}, {"referenceID": 6, "context": "While it is well known that segmentation systems trained on news do worse on social media (Duan et al., 2012), we still show large improvements in applying our model to these different domains.", "startOffset": 90, "endOffset": 109}], "year": 2017, "abstractText": "Named entity recognition, and other information extraction tasks, frequently use linguistic features such as part of speech tags or chunkings. For languages where word boundaries are not readily identified in text, word segmentation is a key first step to generating features for an NER system. While using word boundary tags as features are helpful, the signals that aid in identifying these boundaries may provide richer information for an NER system. New state-of-the-art word segmentation systems use neural models to learn representations for predicting word boundaries. We show that these same representations, jointly trained with an NER system, yield significant improvements in NER for Chinese social media. In our experiments, jointly training NER and word segmentation with an LSTM-CRF model yields nearly 5% absolute improvement over previously published results.", "creator": "LaTeX with hyperref package"}}}