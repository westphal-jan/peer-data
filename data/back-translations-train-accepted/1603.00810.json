{"id": "1603.00810", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2016", "title": "Character-based Neural Machine Translation", "abstract": "Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme completely avoids the problem of unknown source words and provides improved results even when the source language is not morphologically rich. The number of target words is still limited by the standard word-based softmax output layer. However the number of unknowns at the output of the translation network is dramatically reduced (by a relative 66%) with a significant overall improvement over both neural and phrase-based baselines. Improvements up to 3 BLEU points are obtained in the German-English WMT task.", "histories": [["v1", "Wed, 2 Mar 2016 18:01:57 GMT  (76kb,D)", "http://arxiv.org/abs/1603.00810v1", "5 pages, 1 figure"], ["v2", "Thu, 19 May 2016 14:02:48 GMT  (77kb,D)", "http://arxiv.org/abs/1603.00810v2", "Accepted for publication at ACL 2016"], ["v3", "Thu, 30 Jun 2016 10:28:36 GMT  (77kb,D)", "http://arxiv.org/abs/1603.00810v3", "Accepted for publication at ACL 2016"]], "COMMENTS": "5 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE stat.ML", "authors": ["marta r costa-juss\u00e0", "jos\u00e9 a r fonollosa"], "accepted": true, "id": "1603.00810"}, "pdf": {"name": "1603.00810.pdf", "metadata": {"source": "CRF", "title": "Character-based Neural Machine Translation", "authors": ["Marta R. Costa-juss\u00e0", "Jos\u00e9 A. R. Fonollosa"], "emails": ["marta.ruiz@upc.edu", "jose.fonollosa@upc.edu"], "sections": [{"heading": null, "text": "One of the biggest challenges that neural MT still faces is dealing with very large vocabulary and morphologically rich languages. In this paper, we propose a neural MT system that uses character-based embedding in combination with convolutionary and highway layers to replace standard reference-based word representations. The proposed MT scheme completely avoids the problem of unknown source words and delivers improved results even if the source language is not morphologically rich. The number of target words is still limited by the standard word-based Softmax output layer, but the number of unknowns at the output of the translation network is drastically reduced (by a relative 66%), with the number of target words significantly improved compared to the English and German standard word-based Softmax output layer up to the EU 3 dots."}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live."}, {"heading": "2 Neural Machine Translation", "text": "The approach used in this paper (Bahdanau et al., 2015) follows the encoder decoder architecture. First, the encoder reads the source sentence s = (s1,.. sI) and encodes it into a sequence of hidden states h = (h1,.. hI). Then the decoder generates a corresponding translation t = t1,..., tJ based on the encoded sequence of hidden states h. Both encoder and decoder are jointly trained to maximize the conditional log probability of the correct translation. This baseline of the autoencoder architecture is improved by an attention-based mechanism (Bahdanau et al., 2015) in which the encoder neural factor uses a neural word (GRU)."}, {"heading": "3 Character-based Machine Translation", "text": "In fact, it is in such a way that most of the people who are in a position are able to move in the world, to stay in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in"}, {"heading": "4 Experimental framework", "text": "This section reports on the data used, its pre-processing, basic data and results with the advanced character-based neural MT system."}, {"heading": "4.1 Data", "text": "Pre-processing consisted of tokenization, truecasing, 1http: / / www.statmt.org / wmt15 / translation-task.htmlnormalizes punctuation and filters sentences with more than 5% of their words in a language other than English or German. Statistics are shown in Table 1."}, {"heading": "4.2 Baseline systems", "text": "The phrase-based system was built using Moses (Koehn et al., 2007), with standard parameters such as Grow-final-diag for alignment, GoodTuring smoothing of relative frequencies, 5 gram language modeling using Kneser-Ney discounting and lexicalized reordering, among others. The neural system was created using DL4MT2 software available in github. We generally used settings from previous work (Jean et al., 2015): networks have an embedding of 620 and a dimension of 1024, a stack size of 32 and no suspending. We used a vocabulary of 90,000 words in German-English. Likewise, as suggested in (Jean et al., 2015), we replaced unknown words (UNKs) with the corresponding source word using the orientation information."}, {"heading": "4.3 Results", "text": "Table 3 shows the BLEU results for the base systems (including phrases and neural words, NN) and the character-based neural MT (CHAR). We also include the results for the CHAR and NN systems with post-processing of unknown words, which consists of replacing the UNKs with the corresponding source word (+ Src), as proposed in (Jean et al., 2015). BLEU results improve by almost 1.5 points in German-English and more than 3 points in English-German. The reduction in the number of unknown words is 66% relative in German-English and 15% relative in English-German. This reduction in unknown2http: / dl4mt.computing.dcu.ie / words is not proportional to the improvement in the BLEU, but it is relevant in both directions. Note the number of imperfect vocabulary words in the test sentence shows that in cases with post-processing of unknown words a better translation can be achieved than in others."}, {"heading": "5 Conclusions", "text": "Neural MT offers a new perspective on the way MT is managed. Compared to previous approaches, e.g. based on statistical phrases, its main advantage is that the translation is confronted with traceable features and optimized in an end-to-end scheme. However, many challenges remain to be solved, such as dealing with the limitation of the vocabulary size. In this essay, we have proposed a modification of the standard encoder / decoder architecture of neural MT to use character embeddings with unlimited vocabulary, which allows a drastic reduction in the number of unknown words and a large improvement in translation quality. The reduction of unknown words in German-English direction is relative and the improvement in BLEU is about 1.5 points in German-English and more than 3 points in English-German.As further work, we are currently examining various alternatives to extend the character-based approach to the target side of the neural MT system."}, {"heading": "Acknowledgements", "text": "This work is supported by the 7th Framework Programme of the European Commission through the Marie Curie Action International Scholarship (IMTraP-2011-29951) and also by the Spanish Ministry of Economy and Competitiveness, Contract TEC2015-69266-P."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Improved transitionbased parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Compositional Morphology for Word Representations and Language Modelling", "author": ["Botha", "Blunsom2014] Jan A. Botha", "Phil Blunsom"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Botha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Botha et al\\.", "year": 2014}, {"title": "Joint learning of character and word embeddings", "author": ["Xinxiong Chen", "Lei Xu", "Zhiyuan Liu", "Maosong Sun", "Huan-Bo Luan"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Proc. of the Eighth Workshop on Syntax,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Normalizing tweets with edit scripts and recurrent neural embeddings", "author": ["Grzegorz Chrupala"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Chrupala.,? \\Q2014\\E", "shortCiteRegEx": "Chrupala.", "year": 2014}, {"title": "Multi-language image description with neural sequence models. CoRR, abs/1510.04709", "author": ["Stella Frank", "Eva Hasler"], "venue": null, "citeRegEx": "Elliott et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Montreal neural machine translation systems for wmt15", "author": ["Jean et al.2015] Sebastien Jean", "Orhan Firat", "Kyunghun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proc. of the 10th Workshop on Statistical Machine Translation,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing, Seattle", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Characteraware neural language models", "author": ["Kim et al.2016] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI\u201916)", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Statistical Phrase-Based Translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Joseph Och", "Daniel Marcu"], "venue": "In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: Open Source Toolkit for Statistical Machine Translation", "author": ["Herbst."], "venue": "Proc. of the 45th Annual Meeting of the Association for Computational Linguistics, pages 177\u2013180.", "citeRegEx": "Herbst.,? 2007", "shortCiteRegEx": "Herbst.", "year": 2007}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D. Santos", "Bianca Zadrozny"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units. CoRR, abs/1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "the most popular approaches has been statistical phrase-based MT, which uses a combination of features to maximise the probability of the target sentence given the source sentence (Koehn et al., 2003).", "startOffset": 180, "endOffset": 200}, {"referenceID": 15, "context": "Just recently, the neural MT approach has appeared (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) and obtained state-of-the-art results.", "startOffset": 51, "endOffset": 148}, {"referenceID": 4, "context": "Just recently, the neural MT approach has appeared (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) and obtained state-of-the-art results.", "startOffset": 51, "endOffset": 148}, {"referenceID": 0, "context": "Just recently, the neural MT approach has appeared (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) and obtained state-of-the-art results.", "startOffset": 51, "endOffset": 148}, {"referenceID": 15, "context": "beddings (Sutskever et al., 2014) so that words (or minimal units) are not independent anymore; and is easily extendable to multimodal sources of information (Elliott et al.", "startOffset": 9, "endOffset": 33}, {"referenceID": 6, "context": ", 2014) so that words (or minimal units) are not independent anymore; and is easily extendable to multimodal sources of information (Elliott et al., 2015).", "startOffset": 132, "endOffset": 154}, {"referenceID": 0, "context": "system from (Bahdanau et al., 2015), which follows an encoder-decoder architecture with attention, and introduce elements from the characterbased neural language model (Kim et al.", "startOffset": 12, "endOffset": 35}, {"referenceID": 9, "context": ", 2015), which follows an encoder-decoder architecture with attention, and introduce elements from the characterbased neural language model (Kim et al., 2016).", "startOffset": 140, "endOffset": 158}, {"referenceID": 14, "context": "from previous work (Sennrich et al., 2015) which uses the neural MT architecture from (Bahdanau et al.", "startOffset": 19, "endOffset": 42}, {"referenceID": 0, "context": ", 2015) which uses the neural MT architecture from (Bahdanau et al., 2015) without modification to deal with subword units (but not including unigram characters).", "startOffset": 51, "endOffset": 74}, {"referenceID": 1, "context": "for POS tagging (Santos and Zadrozny, 2014), name entity recognition (Santos and aes, 2015), parsing (Ballesteros et al., 2015), normalization (Chrupala, 2014) or learning word representations (Botha and Blunsom, 2014; Chen et al.", "startOffset": 101, "endOffset": 127}, {"referenceID": 5, "context": ", 2015), normalization (Chrupala, 2014) or learning word representations (Botha and Blunsom, 2014; Chen et al.", "startOffset": 23, "endOffset": 39}, {"referenceID": 3, "context": ", 2015), normalization (Chrupala, 2014) or learning word representations (Botha and Blunsom, 2014; Chen et al., 2015).", "startOffset": 73, "endOffset": 117}, {"referenceID": 12, "context": "In our case, with the new characterbased neural MT architecture, we take advantage of intra-word information, which is proven to be extremely useful in other NLP applications (Santos and Zadrozny, 2014; Ling et al., 2015), especially when dealing with morphologically rich languages.", "startOffset": 175, "endOffset": 221}, {"referenceID": 4, "context": "Neural MT uses a neural network approach to compute the conditional probability of the target sentence given the source sentence (Cho et al., 2014; Bahdanau et al., 2015).", "startOffset": 129, "endOffset": 170}, {"referenceID": 0, "context": "Neural MT uses a neural network approach to compute the conditional probability of the target sentence given the source sentence (Cho et al., 2014; Bahdanau et al., 2015).", "startOffset": 129, "endOffset": 170}, {"referenceID": 0, "context": "The approach used in this work (Bahdanau et al., 2015) follows the encoder-decoder architecture.", "startOffset": 31, "endOffset": 54}, {"referenceID": 0, "context": "This baseline autoencoder architecture is improved with a attention-based mechanism (Bahdanau et al., 2015), in which the encoder uses a bi-directional gated recurrent unit (GRU).", "startOffset": 84, "endOffset": 107}, {"referenceID": 7, "context": "sults against the standard phrase-based system in the WMT 2015 evaluation (Jean et al., 2015).", "startOffset": 74, "endOffset": 93}, {"referenceID": 9, "context": "ing (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al.", "startOffset": 4, "endOffset": 41}, {"referenceID": 12, "context": "ing (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al.", "startOffset": 4, "endOffset": 41}, {"referenceID": 1, "context": ", 2015), parsing (Ballesteros et al., 2015) or POS tagging (Ling et al.", "startOffset": 17, "endOffset": 43}, {"referenceID": 12, "context": ", 2015) or POS tagging (Ling et al., 2015; Santos and Zadrozny, 2014).", "startOffset": 23, "endOffset": 69}, {"referenceID": 9, "context": "(Kim et al., 2016) for language modeling.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "However, the addition of two highway layers was shown to improve the quality of the language model in (Kim et al., 2016) so we also kept these additional layers in our case.", "startOffset": 102, "endOffset": 120}, {"referenceID": 7, "context": "Also, as proposed in (Jean et al., 2015) we replaced unknown words (UNKs) with the corresponding source word using the alignment information.", "startOffset": 21, "endOffset": 40}, {"referenceID": 7, "context": "We also include the results for the CHAR and NN systems with post-processing of unknown words, which consists in replacing the UNKs with the corresponding source word (+Src), as suggested in (Jean et al., 2015).", "startOffset": 191, "endOffset": 210}], "year": 2016, "abstractText": "Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affixaware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme completely avoids the problem of unknown source words and provides improved results even when the source language is not morphologically rich. The number of target words is still limited by the standard word-based softmax output layer. However the number of unknowns at the output of the translation network is dramatically reduced (by a relative 66%) with a significant overall improvement over both neural and phrase-based baselines. Improvements up to 3 BLEU points are obtained in the German-English", "creator": "LaTeX with hyperref package"}}}