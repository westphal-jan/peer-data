{"id": "1603.00892", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2016", "title": "Counter-fitting Word Vectors to Linguistic Constraints", "abstract": "In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors' capability for judging semantic similarity. Applying this method to publicly available pre-trained word vectors leads to a new state of the art performance on the SimLex-999 dataset. We also show how the method can be used to tailor the word vector space for the downstream task of dialogue state tracking, resulting in robust improvements across different dialogue domains.", "histories": [["v1", "Wed, 2 Mar 2016 21:19:36 GMT  (1103kb,D)", "http://arxiv.org/abs/1603.00892v1", "Paper accepted for the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016)"]], "COMMENTS": "Paper accepted for the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016)", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["nikola mrksic", "diarmuid \u00f3 s\u00e9aghdha", "blaise thomson", "milica gasic", "lina maria rojas-barahona", "pei-hao su", "david vandyke", "tsung-hsien wen", "steve j young"], "accepted": true, "id": "1603.00892"}, "pdf": {"name": "1603.00892.pdf", "metadata": {"source": "CRF", "title": "Counter-fitting Word Vectors to Linguistic Constraints", "authors": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Lina Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "emails": ["nm480@cam.ac.uk", "mg436@cam.ac.uk", "phs26@cam.ac.uk", "djv27@cam.ac.uk", "thw28@cam.ac.uk", "sjy@cam.ac.uk", "blaisethom}@apple.com"], "sections": [{"heading": "1 Introduction", "text": "This hypothesis supports unattended learning of meaningful word representations from large corpora (Curran, 2003; O \"SE\" aghdha and Korhonen, 2014; Mikolov et al., 2013; Pennington et al., 2014). Word vectors using these methods have proven useful for many downstream tasks, including machine translation (Zou et al., 2013) and parsing dependencies (Bansal et al., 2014). One drawback of learning word embedding from common occurrences in corpora is that it tends to be useful to parse notions of semantic similarity and conceptual association (Hill et al., 2014b). We even have methods that can distinguish similarity from associations."}, {"heading": "2 Related Work", "text": "Most work on improving word vector representations using lexical resources has focused on bringing together words that are known to be semantically related to each other in vector space. Some methods modify the previous or regularization of the original training process (Yu and Dredze, 2014; Bian et al., 2014; Kiela et al., 2015). Wieting et al. (2015) use the Paraphrase database (Ganitkevitch et al., 2013) to train word vectors that emphasize word similarity over word kinship, which reach the current state of the art by exploiting the SimLex 999 datasets and using them as input for counter-fitting in our experiments.1When we \"improve,\" we refer to improving vector space for a specific purpose. We do not expect a vector space to be tuned for semantic similarity."}, {"heading": "3 Counter-fitting Word Vectors to Linguistic Constraints", "text": "Our starting point is an indexed set of word vectors V = {v1, v2,., vN} with a vector for each word in the vocabulary. We will inject semantic relationships into this vector space to produce new word vectors. Elements of each sentence are pairs of word indexes; for example, each pair (i, j) in S is so that the i-th and the j-th word in the vocabulary are synonymous. The objective function used to move the pre-trained word vectors V to the sentences of linguistic constraints A and S contains three different terms: 1. Antonym Repel (AR): This term is used to move antonymous words away from each other."}, {"heading": "3.1 Injecting Dialogue Domain Ontologies into Vector Space Representations", "text": "One disadvantage of this approach is that the RNN model can only perform exact string matching operations to detect the slot names and values mentioned by the user. It cannot detect synonymous words such as expensive and expensive, or even subtle deviations from certain values. The targets are presented as sets of constraints expressed by slot-value pairs such as [food: Indian] or [parking: allowed]. The set of slots S and the set of values Vs for each slot form the ontology of a dialogue domain. In this essay, we adopt the resulting recurring neural network (RNN) framework for tracking that is used in (Henderson et al., 2014d; Henderson et al., 2014c; Mrks."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Word Vectors and Semantic Lexicons", "text": "Two different collections of pre-trained word vectors were used as input for the cross-pass method: 1. Glove Common Crawl 300-dimensional vectors provided by Pennington et al. (2014).2. Paragram-SL999 300-dimensional vectors provided by Wieting et al. (2015).The synonymic and antonymic constraints were derived from two semantic lexicographies: 1. PPDB 2.0 (Pavlick et al., 2015): the latest version of the Paraphrase database. A new feature of this version is that it assigns conversion types to its word pairs. We identify the equivalence relationship with synonymy and exclusion with antonymy. We used the largest available (XXXL) version of the database and considered only single token terms."}, {"heading": "4.2 Improving Lexical Similarity Predictions", "text": "s rank correlation coefficient with the SimLex-999 dataset, which contains word pairs evaluated by a large number of commenters who are instructed to consider only semantic similarity. Table 2 provides a summary of the recently reported competitive values for SimLex-999, as well as the performance of the unchanged, retrofitted and mutually matched GloVe and Paragram-SL999 word vectors. To the best of our knowledge, the 0.685 figure shown for the latter represents the current high score, which is above the average interannotator agreement of 0.67, which is considered ceiling performance in most work to date. In our opinion, the average interannotator agreement is not the only meaningful measure of ceiling performance."}, {"heading": "4.3 Improving Dialogue State Tracking", "text": "These data sets are taken from the Dialogue State Tracking Challenges 2 and 3 (Henderson et al., 2014a; Henderson et al., 2014b).We used four different sets of word vectors to construct semantic dictionaries: the original GloVe and Paragram SL999 vectors, as well as versions opposed to any domain ontology.The optimal value of t was determined by a grid search: We created a dictionary and trained a model for each potential t, which was then evaluated on the development set. Table 6 shows the performance of RNN models using the constructed dictionary.The dictionaries were determined by a grid search: We created a dictionary and trained a model for each potential t, which was then evaluated on the development set."}, {"heading": "5 Conclusion", "text": "We have presented a novel method for counterfitting linguistic constraints in word vector space representations that efficiently post-processes word vectors to improve their usefulness for tasks involving semantic similarity judgments. Their focus on separating vector representations of antonymous word pairs leads to significant improvements in real similarity estimation tasks. We have also shown that counterfitting can tailor word vectors for downstream tasks by injecting domain ontologies into word vectors used to construct semantic dictionaries for dialogue systems."}, {"heading": "Acknowledgements", "text": "We would like to thank Felix Hill for his help with the SimLex 999 evaluation and the anonymous reviewers for their helpful suggestions."}], "references": [{"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of ACL", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Knowledge-powered deep learning for word embedding. In Machine Learning and Knowledge Discovery in Databases", "author": ["Bian et al.2014] Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": null, "citeRegEx": "Bian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bian et al\\.", "year": 2014}, {"title": "From Distributional to Semantic Similarity", "author": ["James Curran"], "venue": "Ph.D. thesis, School of Informatics,", "citeRegEx": "Curran.,? \\Q2003\\E", "shortCiteRegEx": "Curran.", "year": 2003}, {"title": "Finding contradictions in text", "author": ["Anna N. Rafferty", "Christopher D. Manning"], "venue": "In Proceedings of ACL", "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Non-distributional word vector representations", "author": ["Faruqui", "Dyer2015] Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of ACL", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Retrofitting Word Vectors to Semantic Lexicons", "author": ["Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "venue": "In Proceedings of NAACL HLT", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "PPDB: The Paraphrase Database", "author": ["Benjamin Van Durme", "Chris Callison-burch"], "venue": "In Proceedings of NAACL HLT", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Excitatory or inhibitory: A new semantic orientation extracts contradiction and causality from the Web", "author": ["Kentaro Torisawa", "Stijn De Saeger", "Jong-Hoon Oh", "Junichi Kazama"], "venue": "Proceedings of EMNLP-CoNLL", "citeRegEx": "Hashimoto et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2012}, {"title": "The Second Dialog State Tracking Challenge", "author": ["Blaise Thomson", "Jason D. Wiliams"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "The Third Dialog State Tracking Challenge", "author": ["Blaise Thomson", "Jason D. Wiliams"], "venue": "In Proceedings of IEEE SLT", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Robust Dialog State Tracking using Delexicalised Recurrent Neural Networks and Unsupervised Adaptation", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of IEEE SLT", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Word-Based Dialog State Tracking with Recurrent Neural Networks", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Embedding word similarity with neural machine translation", "author": ["Hill et al.2014a] Felix Hill", "Kyunghyun Cho", "Sbastien Jean", "Coline Devin", "Yoshua Bengio"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Specializing word embeddings for similarity or relatedness", "author": ["Kiela et al.2015] Douwe Kiela", "Felix Hill", "Stephen Clark"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kiela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Identifying synonyms among distributionally similar words", "author": ["Lin et al.2003] Dekang Lin", "Shaojun Zhao", "Lijuan Qin", "Ming Zhou"], "venue": "In Proceedings of IJCAI", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Learning semantic word embeddings based on ordinal knowledge constraints", "author": ["Liu et al.2015] Quan Liu", "Hui Jiang", "Si Wei", "Zhen-Hua Ling", "Yu Hu"], "venue": "Proceedings of ACL", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "An unsupervised approach to recognizing discourse relations", "author": ["Marcu", "Echihabi2002] Daniel Marcu", "Abdsemmad Echihabi"], "venue": "In Proceedings of ACL", "citeRegEx": "Marcu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Marcu et al\\.", "year": 2002}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "Proceedings of NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A Lexical Database for English. Communications of the ACM", "author": ["George A. Miller"], "venue": null, "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Computing word-pair antonymy", "author": ["Bonnie Dorr", "Graeme Hirst"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Mohammad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2008}, {"title": "Computing lexical contrast", "author": ["Bonnie J. Dorr", "Graeme Hirst", "Peter D. Turney"], "venue": null, "citeRegEx": "Mohammad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "author": ["Mrk\u0161i\u0107 et al.2015] Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "Proceedings of ACL", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2015}, {"title": "Word Embedding-based Antonym Detection using Thesauri and Distributional Information", "author": ["Ono et al.2015] Masataka Ono", "Makoto Miwa", "Yutaka Sasaki"], "venue": "In Proceedings of NAACL HLT", "citeRegEx": "Ono et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ono et al\\.", "year": 2015}, {"title": "Probabilistic distributional semantics", "author": ["\u00d3 S\u00e9aghdha", "Anna Korhonen"], "venue": "Computational Linguistics,", "citeRegEx": "S\u00e9aghdha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "S\u00e9aghdha et al\\.", "year": 2014}, {"title": "PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "author": ["Pushpendre Rastogi", "Juri Ganitkevich", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": null, "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Symmetric pattern based word embeddings for improved word similarity prediction", "author": ["Roi Reichart", "Ari Rappoport"], "venue": "In Proceedings of CoNLL", "citeRegEx": "Schwartz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schwartz et al\\.", "year": 2015}, {"title": "A uniform approach to analogies, synonyms, antonyms, and associations", "author": ["Peter D. Turney"], "venue": "In Proceedings of COLING", "citeRegEx": "Turney.,? \\Q2008\\E", "shortCiteRegEx": "Turney.", "year": 2008}, {"title": "From paraphrase database to compositional paraphrase model and back. Transactions of the Association for Computational Linguistics", "author": ["Wieting et al.2015] John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": null, "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Polarity inducing Latent Semantic Analysis", "author": ["Yih et al.2012] Wen-Tau Yih", "Geoffrey Zweig", "John C. Platt"], "venue": "In Proceedings of ACL", "citeRegEx": "Yih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2012}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": "In Proceedings of ACL", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "A machine learning approach to textual entailment recognition", "author": ["Marco Pennachiotti", "Alessandro Moschitti"], "venue": "Journal of Natural Language Engineering,", "citeRegEx": "Zanzotto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2009}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y. Zou", "Richard Socher", "Daniel M. Cer", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "This hypothesis supports unsupervised learning of meaningful word representations from large corpora (Curran, 2003; \u00d3 S\u00e9aghdha and Korhonen, 2014; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 101, "endOffset": 193}, {"referenceID": 17, "context": "This hypothesis supports unsupervised learning of meaningful word representations from large corpora (Curran, 2003; \u00d3 S\u00e9aghdha and Korhonen, 2014; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 101, "endOffset": 193}, {"referenceID": 25, "context": "This hypothesis supports unsupervised learning of meaningful word representations from large corpora (Curran, 2003; \u00d3 S\u00e9aghdha and Korhonen, 2014; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 101, "endOffset": 193}, {"referenceID": 32, "context": "Word vectors trained using these methods have proven useful for many downstream tasks including machine translation (Zou et al., 2013) and dependency parsing (Bansal et al.", "startOffset": 116, "endOffset": 134}, {"referenceID": 0, "context": ", 2013) and dependency parsing (Bansal et al., 2014).", "startOffset": 31, "endOffset": 52}, {"referenceID": 19, "context": ", based on syntactic co-occurrences) will generally fail to tell synonyms from antonyms (Mohammad et al., 2008).", "startOffset": 88, "endOffset": 111}, {"referenceID": 27, "context": "1 By applying counter-fitting to the Paragram-SL999 word vectors provided by Wieting et al. (2015), we achieve new state-of-the-art performance on SimLex-999, a dataset designed to measure how well different models judge semantic similarity between words (Hill et al.", "startOffset": 77, "endOffset": 99}, {"referenceID": 1, "context": "Some methods modify the prior or the regularization of the original training procedure (Yu and Dredze, 2014; Bian et al., 2014; Kiela et al., 2015).", "startOffset": 87, "endOffset": 147}, {"referenceID": 13, "context": "Some methods modify the prior or the regularization of the original training procedure (Yu and Dredze, 2014; Bian et al., 2014; Kiela et al., 2015).", "startOffset": 87, "endOffset": 147}, {"referenceID": 6, "context": "(2015) use the Paraphrase Database (Ganitkevitch et al., 2013) to train word vectors which emphasise word similarity over word relatedness.", "startOffset": 35, "endOffset": 62}, {"referenceID": 1, "context": "Some methods modify the prior or the regularization of the original training procedure (Yu and Dredze, 2014; Bian et al., 2014; Kiela et al., 2015). Wieting et al. (2015) use the Paraphrase Database (Ganitkevitch et al.", "startOffset": 109, "endOffset": 171}, {"referenceID": 17, "context": "As Mohammad et al. (2008) observe, antonymous concepts are related but not similar.", "startOffset": 3, "endOffset": 26}, {"referenceID": 4, "context": "Faruqui et al.\u2019s (2015) retrofitting approach uses similarity constraints from WordNet and other resources to pull similar words closer together.", "startOffset": 0, "endOffset": 24}, {"referenceID": 14, "context": "Most prior work focuses on extracting antonym pairs from text rather than exploiting them (Lin et al., 2003; Mohammad et al., 2008; Turney, 2008; Hashimoto et al., 2012; Mohammad et al., 2013).", "startOffset": 90, "endOffset": 192}, {"referenceID": 19, "context": "Most prior work focuses on extracting antonym pairs from text rather than exploiting them (Lin et al., 2003; Mohammad et al., 2008; Turney, 2008; Hashimoto et al., 2012; Mohammad et al., 2013).", "startOffset": 90, "endOffset": 192}, {"referenceID": 27, "context": "Most prior work focuses on extracting antonym pairs from text rather than exploiting them (Lin et al., 2003; Mohammad et al., 2008; Turney, 2008; Hashimoto et al., 2012; Mohammad et al., 2013).", "startOffset": 90, "endOffset": 192}, {"referenceID": 7, "context": "Most prior work focuses on extracting antonym pairs from text rather than exploiting them (Lin et al., 2003; Mohammad et al., 2008; Turney, 2008; Hashimoto et al., 2012; Mohammad et al., 2013).", "startOffset": 90, "endOffset": 192}, {"referenceID": 20, "context": "Most prior work focuses on extracting antonym pairs from text rather than exploiting them (Lin et al., 2003; Mohammad et al., 2008; Turney, 2008; Hashimoto et al., 2012; Mohammad et al., 2013).", "startOffset": 90, "endOffset": 192}, {"referenceID": 31, "context": "The most common use of antonymy information is to provide features for systems that detect contradictions or logical entailment (Marcu and Echihabi, 2002; de Marneffe et al., 2008; Zanzotto et al., 2009).", "startOffset": 128, "endOffset": 203}, {"referenceID": 3, "context": "The most common use of antonymy information is to provide features for systems that detect contradictions or logical entailment (Marcu and Echihabi, 2002; de Marneffe et al., 2008; Zanzotto et al., 2009). As far as we are aware, there is no previous work on exploiting antonymy in dialogue systems. The modelling work closest to ours are Liu et al. (2015), who use antonymy and WordNet hierarchy information to modify the heavyweight Word2Vec training objective; Yih et al.", "startOffset": 158, "endOffset": 356}, {"referenceID": 3, "context": "The most common use of antonymy information is to provide features for systems that detect contradictions or logical entailment (Marcu and Echihabi, 2002; de Marneffe et al., 2008; Zanzotto et al., 2009). As far as we are aware, there is no previous work on exploiting antonymy in dialogue systems. The modelling work closest to ours are Liu et al. (2015), who use antonymy and WordNet hierarchy information to modify the heavyweight Word2Vec training objective; Yih et al. (2012), who use a Siamese neural network to improve the quality of Latent Semantic Analysis vectors; Schwartz et al.", "startOffset": 158, "endOffset": 481}, {"referenceID": 3, "context": "The most common use of antonymy information is to provide features for systems that detect contradictions or logical entailment (Marcu and Echihabi, 2002; de Marneffe et al., 2008; Zanzotto et al., 2009). As far as we are aware, there is no previous work on exploiting antonymy in dialogue systems. The modelling work closest to ours are Liu et al. (2015), who use antonymy and WordNet hierarchy information to modify the heavyweight Word2Vec training objective; Yih et al. (2012), who use a Siamese neural network to improve the quality of Latent Semantic Analysis vectors; Schwartz et al. (2015), who build a standard distributional model from co-occurrences based on symmetric patterns, with specified antonymy patterns counted as negative co-occurrences; and Ono et al.", "startOffset": 158, "endOffset": 598}, {"referenceID": 3, "context": "The most common use of antonymy information is to provide features for systems that detect contradictions or logical entailment (Marcu and Echihabi, 2002; de Marneffe et al., 2008; Zanzotto et al., 2009). As far as we are aware, there is no previous work on exploiting antonymy in dialogue systems. The modelling work closest to ours are Liu et al. (2015), who use antonymy and WordNet hierarchy information to modify the heavyweight Word2Vec training objective; Yih et al. (2012), who use a Siamese neural network to improve the quality of Latent Semantic Analysis vectors; Schwartz et al. (2015), who build a standard distributional model from co-occurrences based on symmetric patterns, with specified antonymy patterns counted as negative co-occurrences; and Ono et al. (2015), who use thesauri and distributional data to train word embeddings specialised for capturing antonymy.", "startOffset": 158, "endOffset": 781}, {"referenceID": 26, "context": "52 Symmetric Patterns (Schwartz et al., 2015) 0.", "startOffset": 22, "endOffset": 45}, {"referenceID": 25, "context": "58 GloVe vectors (Pennington et al., 2014) 0.", "startOffset": 17, "endOffset": 42}, {"referenceID": 28, "context": "58 Paragram-SL999 (Wieting et al., 2015) 0.", "startOffset": 18, "endOffset": 40}, {"referenceID": 25, "context": "Glove Common Crawl 300-dimensional vectors made available by Pennington et al. (2014).", "startOffset": 61, "endOffset": 86}, {"referenceID": 28, "context": "Paragram-SL999 300-dimensional vectors made available by Wieting et al. (2015).", "startOffset": 57, "endOffset": 79}, {"referenceID": 24, "context": "0 (Pavlick et al., 2015): the latest release of the Paraphrase Database.", "startOffset": 2, "endOffset": 24}, {"referenceID": 18, "context": "WordNet (Miller, 1995): a well known semantic lexicon which contains vast amounts of high quality human-annotated synonym and antonym pairs.", "startOffset": 8, "endOffset": 22}], "year": 2016, "abstractText": "In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors\u2019 capability for judging semantic similarity. Applying this method to publicly available pre-trained word vectors leads to a new state of the art performance on the SimLex-999 dataset. We also show how the method can be used to tailor the word vector space for the downstream task of dialogue state tracking, resulting in robust improvements across different dialogue domains.", "creator": "LaTeX with hyperref package"}}}