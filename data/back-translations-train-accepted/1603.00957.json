{"id": "1603.00957", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Question Answering on Freebase via Relation Extraction and Textual Evidence", "abstract": "Existing knowledge-based question answering systems often rely on small annotated training data. While shallow methods like information extraction techniques are robust to data scarcity, they are less expressive than deep understanding methods, thereby failing at answering questions involving multiple constraints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We first present a novel neural network based relation extractor to retrieve the candidate answers from Freebase, and then develop a refinement model to validate answers using Wikipedia. We achieve 53.3 F1 on WebQuestions, a substantial improvement over the state-of-the-art.", "histories": [["v1", "Thu, 3 Mar 2016 03:22:01 GMT  (636kb,D)", "http://arxiv.org/abs/1603.00957v1", null], ["v2", "Wed, 8 Jun 2016 11:05:53 GMT  (1341kb,D)", "http://arxiv.org/abs/1603.00957v2", null], ["v3", "Thu, 9 Jun 2016 15:12:19 GMT  (1335kb,D)", "http://arxiv.org/abs/1603.00957v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kun xu", "siva reddy", "yansong feng", "songfang huang", "dongyan zhao"], "accepted": true, "id": "1603.00957"}, "pdf": {"name": "1603.00957.pdf", "metadata": {"source": "CRF", "title": "Enhancing Freebase Question Answering Using Textual Evidence", "authors": ["Kun Xu", "Yansong Feng", "Siva Reddy", "Songfang Huang", "Dongyan Zhao"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own."}, {"heading": "2 Framework", "text": "Considering the question \"who Shaq played for first,\" we first identify the entity contained in the question and then use an MCCNN model to identify the KB relationship between the entity and the answer. We maintain up to 5 first-place entities and relationships for both entity linkage and relationship extraction components, and develop a common conclusion model to find the best entity relationship configuration and retrieve candidate responses from Freebase accordingly. Finally, we refine these candidate responses by applying a refinement model that takes into account the Wikipedia page of the topic unit."}, {"heading": "3 Question Decomposition", "text": "A question often has several constraints on its answers, for example, who plays anakin Skywalker in Star Wars 1. All actors should meet the following two constraints as answers to this question: (1) the actor played anakin Skywalker; and (2) the actor played in Star Wars 1. Inspired by (Bao et al., 2014) we design a dependency tree-based method for handling such multiple constraints. In general, we first break down the original question into a series of sub-questions using syntactic patterns. Then, the final answers to the original question can be obtained by cutting through the answers of all the sub-questions. For example, the two sub-questions are who plays anakin Skywalker and who plays in Star Wars 1."}, {"heading": "4 KB-based Question Answering", "text": "In view of a subset of questions, we assume that question 1, which is the answer, has a clear KB relationship to the entity in the question, and we predict a single KB triple for each subset. The QA problem is therefore formulated as an information extraction problem comprising two subtasks, namely linking entities and extracting relationships that are challenging enough in themselves and whose results are even expected to be compatible with each other."}, {"heading": "4.1 Entity Linking", "text": "For each question, we use handmade sequences of language categories to identify all possible mention ranges, for example, the NN (shaq) sequence can indicate an entity. For each mention span, we use an entity-linking tool S-MART (Yang and Chang, 2015) to retrieve the top 5 entities from Freebase. These entities are treated as candidates that are ultimately unique in the common inference step."}, {"heading": "4.2 Relation Extraction", "text": "Recently, a large number of methods have been proposed to learn the mapping of relational phrases to KB relationships, such as naive bayes based (Yao and Van Durme, 2014), logistic regression based (Berant et al., 2013), and neural networks based (Yih et al., 2015). These methods usually take up the whole question to predict the relationship that can be triggered by noisy information that has nothing to do with the relationship, especially with these short questions. Syntactic 1 who, when, what, where, how, why, who, whose, features such as the shortest path of dependence have proven to be more concise and representative in terms of extraction (Liu et al., 2015; Xu et al., 2015). Therefore, we are developing multi-channel Convolutionary Neural Networks (MCCNNs) to learn the representation of the relationship both at the syntactic level and at the sentence level."}, {"heading": "4.2.1 Syntactic Level Features", "text": "We use the shortest path between mentioning an entity and the question word in the dependency tree m2 as input in the first channel. We treat the path as a concatenation of words, dependency edges, and dependency names. Note that mentioning the entity and the question word are excluded from the dependency path to learn a more general representation of relationships at the syntactical level. As shown in Figure 2, the dependency path between Shaq and Who is \u2190 nsubj - play - dobj \u2192."}, {"heading": "4.2.2 Sentence Level Features", "text": "The other channel takes the context of the entities in the question as input, i.e. the words of the question about removing the entity mention and the ques-2We word Stanford CoreNLP dependency parser. As shown in Figure 2, the game is played first and then fed into the other channel to learn the relation representation at the sentence level."}, {"heading": "4.2.3 MCCNNs for Relation Classification", "text": "For each channel, the network structure shown in Figure 2 is used to handle the input of different lengths, which provides a vector representation of fixed length. Finally, the two feature vectors are concatenated and fed into a Softmax classifier, whose output dimension corresponds to the number of predefined relation types. The value of each dimension indicates the confidence value of the corresponding relation. In the syntactic feature representations, the vectors of the direction of the dependency edge and the dependency designation are randomly initialized and optimized by backpropagation. Word embedding is divided into two channels and also updated in the training process."}, {"heading": "4.2.4 Objective Function and Learning", "text": "The model is learned from the training data on the basis of question pairs and the corresponding simulated gold relationship. In case of an input question x mentioning a commented entity, the network outputs a vector o (x), where the entry ok (x) is the probability that the k-th relationship between the entity and the expected answer exists. We designate t (x) as a target distribution vector in which the value for the gold relationship is set to 1, others to 0. We calculate the cross-entropy error between t (x) and o (x) and further define the objective function across all training data: J (\u03b8) = \u2212 \u2211 x K \u00b2 k = 1 tk (x) log ok (x) + \u03bb | | 2, which is the set of model parameters to be learned, and \u03bb is a vector of regulation parameters. The model parameters can be efficiently calculated by backpropagation through network structures. To minimize J (Celsius), we apply to Ducent (D)."}, {"heading": "4.3 Joint Inference", "text": "As we know, entities and relationships have strong selective preferences that certain entities with certain relationships do not appear, and vice versa. Locally optimized models are not able to exploit these implicit bidirectional preferences. Therefore, we use a ranking-based common inference model to derive a globally optimal mapping of entity relationships from local predictions. The basic idea is to use various cues from the two local models and the available KB to place a correct mapping of entity relationships higher than the problematic ones."}, {"heading": "4.3.1 Features", "text": "For a given entity relationship mapping (e, r), we use the following clues from the KB: Entity Clues The value of the predicted entity e returned by the entity concatenation system is used directly as a attribute. Furthermore, the text of the mention overlaps with the freebase name for entity e as attribute. In Freebase, most entities have a relationship fb: description that describes the basic introduction of this entity. This description serves as an important clue to differentiate the entity, as the description could in a sense be treated like a bag of relational phrases related to this entity. For example, in the current example, Shaq is referred to by three potential entities m.06 ttvh (an American reality TV show), m.05n7bp (a multiplayer video game), and m.012xdf (a famous NBA basketball player) pointing to the entity."}, {"heading": "4.3.2 Learning", "text": "Let's assume that (egold, rgold) is the gold entity / relation pair for question q. Let's let (ea, ra) be a candidate entity relation assignment for question q. Specifically, if both ea and ra are correct (i.e. ea = egold, ra = rgold), we assign them with the highest score 3. If, on the other hand, only the entity or relation is equal to the gold assignment (i.e. ea = egold, ra = egold, ra = rgold), we assign it with note 2. If both entity and relation assignments are wrong, we assign it with note 1. The underlying intuition is that even if an assignment is not completely correct, it is still preferred over some other completely wrong assignments."}, {"heading": "5 Refining Answers Using Wikipedia", "text": "To refine the answers further, we use Wikipedia as an unstructured source of knowledge in which most of the statements are validated by multiple people. Our refinement model is based on the intuition of how people refine their answers. If you ask someone: Who played Shaq first and gave him four candidate answers (Los Angeles Lakers, Boston Celtics, Orlando Magic and Miami Heat), as well as access to Wikipedia, that person might first find that the question is Shaquille O'Neal, then go to O'Neal's Wikipedia page and look for the sentences that contain the candidate's answers as evidence. By analyzing these sentences, you can find out whether a candidate's answer is correct or not."}, {"heading": "5.1 Finding Evidence from Wikipedia", "text": "As mentioned above, we should first link the theme unit selected by a common follow-up model, and the answers retrieved from Freebase, to Wikipedia3. On the Wikipedia page of the theme unit, we use Wikifier (Cheng and Roth, 2013) to recognize the Wikipedia units in the sentences. Then, we collect the sentences that mention the candidates \"answers as evidence that are the input of the refinement model. For example, on O'Neal's Wikipedia page, we find a sentence\" O'Neal was drafted by the Orlando Magic with the first overall pick in the 1992 NBA draft, \"which is treated as proof of the Orlando Magic's answer. This sentence is then taken into account by the refinement model to determine whether Orlando Magic is the answer to the question."}, {"heading": "5.2 Lexical Features", "text": "We treat the refinement process as a binary classification task, i.e. as right (positive) and wrong (negative), relying in particular on the lexical characteristics extracted from the question and the evidence. Formally, for a question q = < q1,.., qn > and an evidence object set s = < s1,.., sm >, we designate the character of q or s by Qi or sj. For each pair (q, s), we identify a set of all possible symbol pairs (Qi, sj) whose occurrences are used as characteristics. In this way, we hope to obtain a higher weight for characteristics such as: (initially designed) and lower weight for (initially played)."}, {"heading": "5.3 Learning", "text": "We prepare the training data for the Refinement Model as follows: On the training data set, we first use our KB-based approach to retrieve the answers from the KB. Then, we use the gold answers of these questions to monitor the training of the Refinement Model. Specifically, we treat the sentences that contain right / wrong answers as positive / negative examples of the Refinement Model. Then, we use the Freebase API to retrieve the corresponding Wikipedia ID of a Freebase Unit. LIBSVM (Chang and Lin, 2011) to learn the weights for all symbol pairs. Note that on the Wikipedia page of the Topic Unit, we can collect more than one sentence containing a candidate answer. However, some of these sentences may be irrelevant to the question, so we consider this candidate answer correct if at least one proof can be positively predicted by our Refinement Model. On the other hand, we sometimes find no evidence for the candidate's answer. In these cases, we resort to the results of the Refinement Model Kit."}, {"heading": "6 Experiments", "text": "In this section we present the experimental setup, the main results and the detailed analysis of our system."}, {"heading": "6.1 Data", "text": "We use the WEBQUESTIONS dataset, which contains 5,810 questions searched via the Google Suggest service, commenting on the responses to Amazon Mechanical Turk. Questions are broken down into training and test sets containing 3,778 questions (65%) and 2,032 questions (35%), respectively. We further divide the training questions into the training set and development sets by 80% / 20%. Note that in order to train the MCCNNs and the Common Inference Model, we need the gold standard relationships of the questions. As this dataset contains only question-answer pairs and commented topic units, we determine the simulated gold relationships in three steps: For a given question, we first locate the e topic unit in the freebase chart, and then consider the 1 and 2-hop predicates associated with the topic as gold relationship candidates. For each relationship candidate r, we type the query (e, r,?) which compares KB and the answer with the highest value in the gold index."}, {"heading": "6.2 Parameter Settings", "text": "The word embedding dimension is set to 50. We initialized the word embedding with the word vectors trained with the model of Turian et al. (2010). The hyperparameters in our model are matched with the development set. The window size of MCCNNs is 3. The sizes of hidden layer 1 and hidden layer 2 of two channels in MCCNNs are set to 200 and 100 respectively."}, {"heading": "6.3 Results and Discussion", "text": "We use Berant's official evaluation script to calculate the average question-oriented value F1 and compare our DeepJRQA model, which performs both the common inference and the refinement of responses, with existing models in the literature. Note that DeepJRQA treats the executed results of two top-notch KB triples generated by the common inference model as candidate responses. In addition, we use three baseline. The first baseline is DeepQA, which finds the answers by taking the best predictions from a pipeline of EL and RE to query the KB. The second baseline is DeepJQA, which takes the common inference approach but without refining the answer. The third baseline is DeepRQA, which performs the refinement of responses but without reaching a common conclusion. Table 1 summarizes the outcomes. We can see that our performance is improved by 9.2% over DeepJRQA, including the current systems."}, {"heading": "6.3.1 Detailed analysis", "text": "The only question is how much time we have left to help us, and how much time we have left to help us put the world in order, \"he said in an interview with the\" New York Times. \"\" I believe we are able to put the world in order, \"he said in an interview with the\" New York Times. \"\" I don't think we will be able to put the world in order, \"he said.\" I don't think we will be able to keep the world in order. \""}, {"heading": "7 Related Work", "text": "Over time, the QA task has evolved into two main streams - QA to unstructured data and QA to structured data. TREC QA assessments (Voorhees and Tice, 1999) have been an important impetus for unstructured QA leading to richer data sets and more sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015) While initial progress has been made in structured QA with small domains such as GeoQuery (Cell and Mooney, 1996), the focus has shifted to large-scale structured KBs such as Freebase, DBPedia et al., 2012; Cai and Yates, 2013; Berant et al., al., 2013; Kwiatkowski et al., al., al., 2013."}, {"heading": "8 Conclusion and Future Work", "text": "In this paper, we introduced a new method that uses both Freebase and Wikipedia to answer questions of natural language. We introduced a common method of linking and extracting relationships to retrieve answers from Freebase, and then refined those answers with additional evidence from Wikipedia. Our method surpasses the state of the art on the WEBQUESTIONS datasets. Although the refinement model helps, our performance is limited by the coverage of Freebase. Furthermore, in the future, we should answer subjective questions, such as what is the most popular attraction in America, which can not only be answered with Freebase, but also draw on unstructured knowledge."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer et al.2007] Sren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary G. Ives"], "venue": "In ISWC/ASWC", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Open information extraction for the web", "author": ["Banko et al.2007] Michele Banko", "Michael J Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni"], "venue": "In IJCAI", "citeRegEx": "Banko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Knowledge-based question answering as machine translation", "author": ["Bao et al.2014] Junwei Bao", "Nan Duan", "Ming Zhou", "Tiejun Zhao"], "venue": null, "citeRegEx": "Bao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bao et al\\.", "year": 2014}, {"title": "More accurate question answering on freebase", "author": ["Bast", "Haussmann2015] Hannah Bast", "Elmar Haussmann"], "venue": "In CIKM", "citeRegEx": "Bast et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bast et al\\.", "year": 2015}, {"title": "Semantic parsing via paraphrasing", "author": ["Berant", "Liang2014] Jonathan Berant", "Percy Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Andrew Chou", "Roy Frostig", "Percy Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": null, "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Question answering with subgraph embeddings", "author": ["Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory networks. CoRR, abs/1506.02075", "author": ["Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Large-scale semantic parsing via schema matching and lexicon extension", "author": ["Cai", "Yates2013] Qingqing Cai", "Alexander Yates"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "Toward an architecture for never-ending language learning", "author": ["Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R Hruschka Jr.", "Tom M Mitchell"], "venue": null, "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "Lin2011] Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM TIST,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Relational inference for wikification", "author": ["Cheng", "Roth2013] Xiao Cheng", "Dan Roth"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2013}, {"title": "Question answering over freebase with multi-column convolutional neural networks. In ACLIJCNLP", "author": ["Dong et al.2015] Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John C. Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Fader et al.2013] Anthony Fader", "Luke S. Zettlemoyer", "Oren Etzioni"], "venue": null, "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Fader et al.2014] Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni"], "venue": "In SIGKDD", "citeRegEx": "Fader et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Tree edit models for recognizing textual entailments, paraphrases, and answers to questions", "author": ["Heilman", "Smith2010] Michael Heilman", "Noah A Smith"], "venue": null, "citeRegEx": "Heilman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2010}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": null, "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Training linear svms in linear time. In SIGKDD", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "Joachims.,? \\Q2006\\E", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "Knowledge graph and corpus driven segmentation and answer inference for telegraphic entity-seeking", "author": ["Joshi et al.2014] Mandar Joshi", "Uma Sawant", "Soumen Chakrabarti"], "venue": null, "citeRegEx": "Joshi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2014}, {"title": "Weakly supervised training of semantic parsers", "author": ["Krishnamurthy", "Tom M Mitchell"], "venue": "EMNLPCoNLL", "citeRegEx": "Krishnamurthy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2012}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Eunsol Choi", "Yoav Artzi", "Luke S. Zettlemoyer"], "venue": null, "citeRegEx": "Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "A dependency-based neural network for relation classification", "author": ["Liu et al.2015] Yang Liu", "Furu Wei", "Sujian Li", "Heng Ji", "Ming Zhou", "Houfeng WANG"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": null, "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Grounded semantic parsing for complex knowledge extraction", "author": ["Hoifung Poon", "Kristina Toutanova"], "venue": null, "citeRegEx": "Parikh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2015}, {"title": "Large-scale semantic parsing without question-answer pairs", "author": ["Reddy et al.2014] Siva Reddy", "Mirella Lapata", "Mark Steedman"], "venue": "Transactions of the Association of Computational Linguistics,", "citeRegEx": "Reddy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": null, "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Yago: a core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": null, "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Template-based question answering over rdf data", "author": ["Lorenz B\u00fchmann", "Jens Lehmann", "Axel-Cyrille Ngonga Ngomo", "Daniel Gerber", "Philipp Cimiano"], "venue": null, "citeRegEx": "Unger et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Unger et al\\.", "year": 2012}, {"title": "The trec-8 question answering track report", "author": ["Voorhees", "Tice1999] Ellen M Voorhees", "Dawn M. Tice"], "venue": null, "citeRegEx": "Voorhees et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Voorhees et al\\.", "year": 1999}, {"title": "What is the jeopardy model? a quasi-synchronous grammar for qa", "author": ["Wang et al.2007] Mengqiu Wang", "Noah A Smith", "Teruko Mitamura"], "venue": "EMNLPCoNLL", "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Building a semantic parser overnight", "author": ["Wang et al.2015] Yushi Wang", "Jonathan Berant", "Percy Liang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Semantic relation classification via convolutional neural networks with simple negative sampling", "author": ["Xu et al.2015] Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Natural language questions for the web of data", "author": ["Yahya et al.2012] Mohamed Yahya", "Klaus Berberich", "Shady Elbassuoni", "Maya Ramanath", "Volker Tresp", "Gerhard Weikum"], "venue": null, "citeRegEx": "Yahya et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yahya et al\\.", "year": 2012}, {"title": "S-mart: Novel tree-based structured learning algorithms applied to tweet entity linking", "author": ["Yang", "Chang2015] Yi Yang", "Ming-Wei Chang"], "venue": "ACLIJNLP", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Wikiqa: A challenge dataset for opendomain question answering", "author": ["Yang et al.2015] Yi Yang", "Wen-tau Yih", "Christopher Meek"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Information extraction over structured data: Question answering with freebase", "author": ["Yao", "Van Durme2014] Xuchen Yao", "Benjamin Van Durme"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Answer extraction as sequence tagging with tree edit distance", "author": ["Yao et al.2013] Xuchen Yao", "Benjamin Van Durme", "Peter Clark"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Lean question answering over freebase from scratch", "author": ["Xuchen Yao"], "venue": "In NAACL", "citeRegEx": "Yao.,? \\Q2015\\E", "shortCiteRegEx": "Yao.", "year": 2015}, {"title": "Question answering using enhanced lexical semantic models", "author": ["Yih et al.2013] Wen-tau Yih", "Ming-Wei Chang", "Christopher Meek", "Andrzej Pastusiak"], "venue": null, "citeRegEx": "Yih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2013}, {"title": "Semantic parsing for single-relation question answering", "author": ["Yih et al.2014] Wen-tau Yih", "Xiaodong He", "Christopher Meek"], "venue": null, "citeRegEx": "Yih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base. In ACL-IJCNLP", "author": ["Yih et al.2015] Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao"], "venue": null, "citeRegEx": "Yih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "Deep learning for answer sentence selection. arXiv preprint arXiv:1412.1632", "author": ["Yu et al.2014] Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["Zelle", "Mooney1996] John M Zelle", "Raymond J Mooney"], "venue": null, "citeRegEx": "Zelle et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Zelle et al\\.", "year": 1996}, {"title": "A joint model for question answering over multiple knowledge bases", "author": ["Zhang et al.2016] Yuanzhe Zhang", "Shizhu He", "Kang Liu", "Jun Zhao"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": ", Freebase (Bollacker et al., 2008), YAGO (Suchanek et al.", "startOffset": 11, "endOffset": 35}, {"referenceID": 29, "context": ", 2008), YAGO (Suchanek et al., 2007) and DBpedia (Auer et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 0, "context": ", 2007) and DBpedia (Auer et al., 2007), KBs have become a new source of potential answers for people to mine.", "startOffset": 20, "endOffset": 39}, {"referenceID": 5, "context": "The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated meaning representation language.", "startOffset": 39, "endOffset": 86}, {"referenceID": 23, "context": "The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated meaning representation language.", "startOffset": 39, "endOffset": 86}, {"referenceID": 23, "context": "Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014).", "startOffset": 103, "endOffset": 173}, {"referenceID": 27, "context": "Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014).", "startOffset": 103, "endOffset": 173}, {"referenceID": 42, "context": "On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al.", "startOffset": 173, "endOffset": 253}, {"referenceID": 40, "context": "On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al.", "startOffset": 173, "endOffset": 253}, {"referenceID": 7, "context": ", 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015).", "startOffset": 76, "endOffset": 116}, {"referenceID": 13, "context": ", 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015).", "startOffset": 76, "endOffset": 116}, {"referenceID": 8, "context": "Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015).", "startOffset": 71, "endOffset": 117}, {"referenceID": 32, "context": "Such words with multiple latent constraints have been a pain-in-the-neck for both semantic parsing and relation extraction approaches, and requires larger training data (this phenomenon is coined as sub-lexical compositionally by Wang et al. (2015)).", "startOffset": 230, "endOffset": 249}, {"referenceID": 2, "context": "Inspired by (Bao et al., 2014), we design a dependency tree-based method to handle such multiple-constraint questions.", "startOffset": 12, "endOffset": 30}, {"referenceID": 5, "context": "Recently, a large number of methods have been proposed to learn the mapping from relational phrases to KB relations, such as Naive Bayes based (Yao and Van Durme, 2014), logistic regression based (Berant et al., 2013), and neural network based (Yih et al.", "startOffset": 196, "endOffset": 217}, {"referenceID": 43, "context": ", 2013), and neural network based (Yih et al., 2015).", "startOffset": 34, "endOffset": 52}, {"referenceID": 24, "context": "features such as the shortest dependency path have been proven to be more concise and representative in relation extraction (Liu et al., 2015; Xu et al., 2015).", "startOffset": 124, "endOffset": 159}, {"referenceID": 34, "context": "features such as the shortest dependency path have been proven to be more concise and representative in relation extraction (Liu et al., 2015; Xu et al., 2015).", "startOffset": 124, "endOffset": 159}, {"referenceID": 14, "context": "To minimize J(\u03b8), we apply stochastic gradient descent (SGD) with AdaGrad (Duchi et al., 2011) in our experiments.", "startOffset": 74, "endOffset": 94}, {"referenceID": 20, "context": "This gives the training data for our svm-ranker (Joachims, 2006), which we hope the gold-standard entity-relation assignments rank higher than incorrect ones.", "startOffset": 48, "endOffset": 64}, {"referenceID": 5, "context": "(Berant et al., 2013) 48.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "9 (Bao et al., 2014) - - 37.", "startOffset": 2, "endOffset": 20}, {"referenceID": 7, "context": "5 (Bordes et al., 2014) - - 39.", "startOffset": 2, "endOffset": 23}, {"referenceID": 13, "context": "2 (Dong et al., 2015) - - 40.", "startOffset": 2, "endOffset": 21}, {"referenceID": 40, "context": "8 (Yao, 2015) - - 44.", "startOffset": 2, "endOffset": 13}, {"referenceID": 43, "context": "3 (Yih et al., 2015) 52.", "startOffset": 2, "endOffset": 20}, {"referenceID": 43, "context": "5 (Yih et al., 2015)(w/o ClueWeb) 50.", "startOffset": 2, "endOffset": 20}, {"referenceID": 43, "context": "2% over DeepQA, outperforming all systems including the current stateof-the-art (Yih et al., 2015).", "startOffset": 80, "endOffset": 98}, {"referenceID": 32, "context": "TREC QA evaluations (Voorhees and Tice, 1999) were a major boost to unstructured QA which lead to richer datasets and sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015).", "startOffset": 140, "endOffset": 278}, {"referenceID": 39, "context": "TREC QA evaluations (Voorhees and Tice, 1999) were a major boost to unstructured QA which lead to richer datasets and sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015).", "startOffset": 140, "endOffset": 278}, {"referenceID": 41, "context": "TREC QA evaluations (Voorhees and Tice, 1999) were a major boost to unstructured QA which lead to richer datasets and sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015).", "startOffset": 140, "endOffset": 278}, {"referenceID": 44, "context": "TREC QA evaluations (Voorhees and Tice, 1999) were a major boost to unstructured QA which lead to richer datasets and sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015).", "startOffset": 140, "endOffset": 278}, {"referenceID": 36, "context": "TREC QA evaluations (Voorhees and Tice, 1999) were a major boost to unstructured QA which lead to richer datasets and sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015).", "startOffset": 140, "endOffset": 278}, {"referenceID": 18, "context": "TREC QA evaluations (Voorhees and Tice, 1999) were a major boost to unstructured QA which lead to richer datasets and sophisticated methods (Wang et al., 2007; Heilman and Smith, 2010; Yao et al., 2013; Yih et al., 2013; Yu et al., 2014; Yang et al., 2015; Hermann et al., 2015).", "startOffset": 140, "endOffset": 278}, {"referenceID": 30, "context": "While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent focus has shifted to large scale structured KBs like Freebase, DBPedia (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on noisy KBs (Banko et al.", "startOffset": 189, "endOffset": 277}, {"referenceID": 5, "context": "While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent focus has shifted to large scale structured KBs like Freebase, DBPedia (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on noisy KBs (Banko et al.", "startOffset": 189, "endOffset": 277}, {"referenceID": 23, "context": "While initial progress on structured QA started with small toy domains like GeoQuery (Zelle and Mooney, 1996), recent focus has shifted to large scale structured KBs like Freebase, DBPedia (Unger et al., 2012; Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), and on noisy KBs (Banko et al.", "startOffset": 189, "endOffset": 277}, {"referenceID": 1, "context": ", 2013), and on noisy KBs (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015).", "startOffset": 26, "endOffset": 143}, {"referenceID": 10, "context": ", 2013), and on noisy KBs (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015).", "startOffset": 26, "endOffset": 143}, {"referenceID": 15, "context": ", 2013), and on noisy KBs (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015).", "startOffset": 26, "endOffset": 143}, {"referenceID": 26, "context": ", 2013), and on noisy KBs (Banko et al., 2007; Carlson et al., 2010; Krishnamurthy and Mitchell, 2012; Fader et al., 2013; Parikh et al., 2015).", "startOffset": 26, "endOffset": 143}, {"referenceID": 35, "context": "An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016).", "startOffset": 138, "endOffset": 198}, {"referenceID": 16, "context": "An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016).", "startOffset": 138, "endOffset": 198}, {"referenceID": 46, "context": "An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly (Yahya et al., 2012; Fader et al., 2014; Zhang et al., 2016).", "startOffset": 138, "endOffset": 198}, {"referenceID": 5, "context": "Though earlier methods exploited unstructured data for KB-QA (Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Yao and Van Durme, 2014; Reddy et al., 2014; Yih et al., 2015), these methods do not rely on unstructured data at test time.", "startOffset": 61, "endOffset": 179}, {"referenceID": 27, "context": "Though earlier methods exploited unstructured data for KB-QA (Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Yao and Van Durme, 2014; Reddy et al., 2014; Yih et al., 2015), these methods do not rely on unstructured data at test time.", "startOffset": 61, "endOffset": 179}, {"referenceID": 43, "context": "Though earlier methods exploited unstructured data for KB-QA (Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Yao and Van Durme, 2014; Reddy et al., 2014; Yih et al., 2015), these methods do not rely on unstructured data at test time.", "startOffset": 61, "endOffset": 179}, {"referenceID": 4, "context": "Though earlier methods exploited unstructured data for KB-QA (Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Yao and Van Durme, 2014; Reddy et al., 2014; Yih et al., 2015), these methods do not rely on unstructured data at test time. Our work is closely related to Joshi et al. (2014) who aim to answer noisy telegraphic queries using both structured and unstructured data.", "startOffset": 96, "endOffset": 293}, {"referenceID": 25, "context": "While these methods aim to predict a relation between two entities in order to populate KBs (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013), we work with sentence level relation extraction for question answering.", "startOffset": 92, "endOffset": 156}, {"referenceID": 19, "context": "While these methods aim to predict a relation between two entities in order to populate KBs (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013), we work with sentence level relation extraction for question answering.", "startOffset": 92, "endOffset": 156}, {"referenceID": 28, "context": "While these methods aim to predict a relation between two entities in order to populate KBs (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013), we work with sentence level relation extraction for question answering.", "startOffset": 92, "endOffset": 156}, {"referenceID": 40, "context": "Closest to our extraction method is (Yao and Van Durme, 2014; Yao, 2015) who also uses sentence level relation extraction for QA.", "startOffset": 36, "endOffset": 72}, {"referenceID": 17, "context": ", 2009; Hoffmann et al., 2011; Riedel et al., 2013), we work with sentence level relation extraction for question answering. Krishnamurthy and Mitchell (2012) and Fader et al.", "startOffset": 8, "endOffset": 159}, {"referenceID": 15, "context": "Krishnamurthy and Mitchell (2012) and Fader et al. (2014) adopt open relation extraction methods for QA but they require hand-coded grammar for parsing queries.", "startOffset": 38, "endOffset": 58}], "year": 2017, "abstractText": "Existing knowledge-based question answering systems often rely on small annotated training data. While shallow methods like information extraction techniques are robust to data scarcity, they are less expressive than deep understanding methods, thereby failing at answering questions involving multiple constraints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We first present a novel neural network based relation extractor to retrieve the candidate answers from Freebase, and then develop a refinement model to validate answers using Wikipedia. We achieve 53.3 F1 on WEBQUESTIONS, a substantial improvement over the state-of-theart.", "creator": "LaTeX with hyperref package"}}}