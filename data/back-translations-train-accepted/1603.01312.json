{"id": "1603.01312", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Learning Physical Intuition of Block Towers by Example", "abstract": "Wooden blocks are a common toy for infants, allowing them to develop motor skills and gain intuition about the physical behavior of the world. In this paper, we explore the ability of deep feed-forward models to learn such intuitive physics. Using a 3D game engine, we create small towers of wooden blocks whose stability is randomized and render them collapsing (or remaining upright). This data allows us to train large convolutional network models which can accurately predict the outcome, as well as estimating the block trajectories. The models are also able to generalize in two important ways: (i) to new physical scenarios, e.g. towers with an additional block and (ii) to images of real wooden blocks, where it obtains a performance comparable to human subjects.", "histories": [["v1", "Thu, 3 Mar 2016 22:59:35 GMT  (8028kb,D)", "http://arxiv.org/abs/1603.01312v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["adam lerer", "sam gross", "rob fergus"], "accepted": true, "id": "1603.01312"}, "pdf": {"name": "1603.01312.pdf", "metadata": {"source": "META", "title": "Learning Physical Intuition of Block Towers by Example", "authors": ["Adam Lerer", "Sam Gross", "Rob Fergus"], "emails": ["ALERER@FB.COM", "SGROSS@FB.COM", "ROBFERGUS@FB.COM"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that most of us are able to abide by the rules that we have imposed on ourselves. (...) It is so that we see ourselves able to understand the rules. (...) It is not so that we see ourselves able to understand the rules. (...) It is not so that we understand them. (...) It is so that we are able to understand them. (...) It is as if we do not understand them. (...) It is as if we understand them. (...). (...). (...). (...). (...). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.).). (.). (.).). (.). (.). (.). (.). (.). (.).). (.). (.). (.). (.). (.).). (.). (.).). (. (.). (. (.).). (.).). (.). ().). (. ().). (). (). (). (). (). ().). (). ().). (). (). (). ().). (). (). (). (). (). ().). (). (). (). (). (). (). (). ().). ().). (). ().). (). (). ().). (). ()."}, {"heading": "1.1. Related Work", "text": "A generative simulation model is used to predict the outcome of a variety of block configurations with different physical properties, and it is found to be closely in line with human judgment. This work complements our work by using a top-down approach based on a sophisticated graphics engine that incorporates explicit prior knowledge of Newtonian mechanics. In contrast, our model is purely bottom-up, estimating stability directly from image pixels and learning from examples. Our combination of top-down rendering engines for data generation with high-performance feed-forward regressors is similar to the work of the Kinect body, the estimation work of (Shotton et al, 2013), although the application is very different. (Wu et al, 2015) has recently studied learning simple kinematics in the context of objects moving downwards."}, {"heading": "2. Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. UETorch", "text": "UETorch is a package that integrates the Lua / Torch learning environment directly into the UE4 game loop, enabling fine-grained scripting and online control of UE4 simulations through Torch. Torch is well suited for integrating game engines, as Lua is the dominant scripting language for games, and many games, including UE4, support Lua scripting. UETorch adds additional interfaces to capture screenshots, segmentation masks, optical flow data, and game control through user input or direct game state modifications. As Torch runs within the UE4 process, new capabilities can easily be added through FFI without defining additional interfaces / protocols for communication between processes."}, {"heading": "2.2. Data Collection", "text": "SyntheticA simulation was developed in UETorch, which generated vertical stacks of 2, 3, or 4 colored blocks in random configurations. Block position and orientation, camera position, background textures, and lighting were randomized at each attempt to improve the transferability of the learned features. In each simulation, the result was recorded (did it fall?) and screenshots and segmentation masks were taken at 8 frames / sec. Images and masks from a representative 4 block simulation are shown in Fig. 2. A total of 180,000 simulations were performed, which were balanced across the number of blocks and stable / unstable configurations. 12,288 examples were reserved for testing. RealFour wooden cubes were manufactured and painted red, green, blue, and yellow. Manufacturing imperfections added a certain degree of randomness to the stability of the real stacked blocks, so that the stability of the real blocks was stable."}, {"heading": "2.3. Human Subject Methodology", "text": "To better understand the challenge posed by our real and synthetic datasets, we asked 10 subjects to evaluate the images in a controlled experiment. Participants were asked to make a binary prediction of the outcome of the blocks (i.e. falling or not). During the training phase, which consisted of 50 randomly drawn examples, participants were shown the final framework of each example, along with feedback on whether their choice was correct or not (see Fig. 3). Subsequently, they were tested using 100 randomly drawn examples (separated from the training set). During the test phase, subjects were given no feedback as to the accuracy of their answers."}, {"heading": "2.4. Model Architectures", "text": "We trained several convolutional network (CNN) architectures on the synthetic blocks. We trained some ar-chitectures on binary case prediction only, and others on case prediction and mask prediction tasks.Fall PredictionWe trained the ResNet-34 (He et al., 2015) and Googlenet (Szegedy et al., 2014) networks on case prediction. These models were pre-trained on the Imagenet dataset., We replaced the final linear plane with a single logistic output and fine-tuned the entire network structure with the SGD dataset."}, {"heading": "2.5. Evaluation", "text": "We compare the case prediction accuracy on synthetic and real images, both between models and between model and human performance. We also train models with a block tower size pre-set and test them on the tower size pre-set to evaluate the transferability of these models to different block tower sizes. We evaluate mask predictions using two criteria: center mask IoU and protocol probability per pixel. We define the center mask IoU as the intersection-over-union of the mask designation with the binarized prediction for the t = 4s mask averaged over each foreground class present in the mask lettering. MIoU (m, q) = 1N N = 1 [Cn, c].Cn IoU (mnc, q nc)] (1), where mnc is the set of pixels of class c in the mask n, Cn = {c: c, 3, 4, n.c} the probability is prefabricated."}, {"heading": "3. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Fall Prediction Results", "text": "Table 1 compares the accuracy for case prediction of several deep networks and baselines described in Section 2.4. Winding networks perform well in case prediction, whether isolated or combined with the mask prediction. The best accuracy for synthetic data is achieved with PhysNet, which is jointly trained on masks and fall prediction. Accuracy for real data for all convector networks is within its standard deviation. As an ablation study, we also measured the performance of Googlenet without Imagenet pre-training. Interestingly, while the model performs equally well with synthetic data with and without pre-training, only the pre-formed model is generalizable with real images (Table 1). We conducted occlusion experiments to determine which regions of the block images affected the case predictions of the models. A Gaussian patch of gray pixels with standard deviations 20% of the image width gives a comparative accuracy between the image."}, {"heading": "3.2. Mask Prediction Results", "text": "Table 2 compares the mask prediction accuracy of the DeepMask and PhysNet networks described in Section 2.4. PhysNet achieves the best performance on both the mid-mask IoU and the log likelihood per pixel (see Section 2.5), essentially outperforming DeepMask and baselines. Predicting the mask as equal to the initial mask (t = 0) has a high mask IoU due to the deficiencies in this metric described in Section 2.5. Examples of PhysNet mask outputs on synthetic and real data are shown in Fig. 7. We only show masks for examples whose fall is predicted because predicting masks for stable towers is simple and the outputs are typically perfect. PhysNet mask outputs are typically quite reasonable for falling 2- and 3-block synthetic towers, but exhibit more errors and uncertainties on 4-block synthetic towers that show very high levels of physical uncertainty in most of these cases."}, {"heading": "3.3. Evaluation on Held-Out Number of Blocks", "text": "Table 3 compares the performance of networks that had either 3- or 4-block configurations that were excluded from the training set. Although the accuracy of these networks is lower in the untrained class compared to a fully trained model, it is still relatively high - comparable to human performance. Also, the predicted masks on the untrained number of blocks continue to capture case dynamics with reasonably high accuracy. Some examples are shown in Fig. 8."}, {"heading": "4. Discussion", "text": "Our results suggest that deep CNN models, from bottom to top, can achieve human-level performance when predicting how towers fall from blocks. We also find that the performance of these models can be generalized well to real-world images if the models are prepared for real-world data (Table 1). Several experiments provide evidence that the deep models we train gain insights into the dynamics of towers, rather than simply storing a mapping of configurations to the results. Most convincingly, the relatively slight deterioration in the performance of the models on a tower size that is not shown during training (Table 3 & Fig 8) indicates that the model must base its prediction on local characteristics rather than store accurate block configurations. Moreover, the occlusion experiments in Fig. 5 suggest that models focus on specific regions that transfer stability or instability to a block configuration."}, {"heading": "Acknowledgements", "text": "The authors would like to thank: Soumith Chintala and Arthur Szlam for early feedback on experimental design, Sainbayar Sukhbaatar for their support in collecting block examples from the real world, Y-Lan Boureau for useful advice on human experiments and Piotr Dollar for feedback on the manuscript."}], "references": [{"title": "Learning to see by moving", "author": ["Agrawal", "Pulkit", "Carreira", "Joao", "Malik", "Jitendra"], "venue": null, "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Humans predict liquid dynamics using probabilistic simulation", "author": ["C.J. Bates", "I. Yildirim", "J.B. Tenenbaum", "P.W. Battaglia"], "venue": "In In Proc. 37th Ann. Conf. Cognitive Science Society,", "citeRegEx": "Bates et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bates et al\\.", "year": 2015}, {"title": "Simulation as an engine of physical scene understanding", "author": ["Battaglia", "Peter W", "Hamrick", "Jessica B", "Tenenbaum", "Joshua B"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Battaglia et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Battaglia et al\\.", "year": 2013}, {"title": "The origin of concepts", "author": ["Carey", "Susan"], "venue": null, "citeRegEx": "Carey and Susan.,? \\Q2009\\E", "shortCiteRegEx": "Carey and Susan.", "year": 2009}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "The scope and limits of simulation in automated reasoning", "author": ["Davis", "Ernest", "Marcus", "Gary"], "venue": "Artificial Intelligence,", "citeRegEx": "Davis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2016}, {"title": "Relevant and robust a response to marcus and davis", "author": ["Goodman", "Noah D", "Frank", "Michael C", "Griffiths", "Thomas L", "Tenenbaum", "Joshua B", "Battaglia", "Peter W", "Hamrick", "Jessica B"], "venue": "Psychological science,", "citeRegEx": "Goodman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2013}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ArXiv e-prints,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Mechanical reasoning by mental simulation", "author": ["Hegarty", "Mary"], "venue": "Trends in cognitive sciences,", "citeRegEx": "Hegarty and Mary.,? \\Q2004\\E", "shortCiteRegEx": "Hegarty and Mary.", "year": 2004}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "shift. CoRR,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Consistent physics underlying ballistic motion prediction", "author": ["K.A. Smith", "P.W. Battaglia", "E. Vul"], "venue": "In Proc. 35th Ann. Conf. Cognitive Science Society,", "citeRegEx": "Smith et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2013}, {"title": "Scene understanding by reasoning stability and safety", "author": ["Zheng", "Bo", "Zhao", "Yibiao", "Yu", "Joey", "Ikeuchi", "Katsushi", "Zhu", "Song-Chun"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "(Agrawal et al., 2015), but inherent complexities limit the diversity and quantity of data that can be acquired.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "We integrate the Torch (Collobert et al., 2011) machine learning framework directly into the UE4 game loop, allowing for online interaction with the UE4 world.", "startOffset": 23, "endOffset": 47}, {"referenceID": 7, "context": "ResNets (He et al., 2015).", "startOffset": 8, "endOffset": 25}, {"referenceID": 2, "context": "The most closely related work to ours is (Battaglia et al., 2013) who explore the physics involved with falling blocks.", "startOffset": 41, "endOffset": 65}, {"referenceID": 2, "context": "Similar to (Battaglia et al., 2013), they also used a top-down 3D physics engine to map from a hypothesis of object mass, shape, friction etc.", "startOffset": 11, "endOffset": 35}, {"referenceID": 1, "context": "A number of works in cognitive science have explored intuitive physics, for example, in the context of liquid dynamics (Bates et al., 2015), ballistic motion (Smith et al.", "startOffset": 119, "endOffset": 139}, {"referenceID": 10, "context": ", 2015), ballistic motion (Smith et al., 2013) and gears and pulleys (Hegarty, 2004).", "startOffset": 26, "endOffset": 46}, {"referenceID": 11, "context": "In computer vision, a number of works have used physical reasoning to aid scene understanding (Zheng et al., 2015; Koppula & Saxena, 2016).", "startOffset": 94, "endOffset": 138}, {"referenceID": 7, "context": "We trained the ResNet-34 (He et al., 2015) and Googlenet (Szegedy et al.", "startOffset": 25, "endOffset": 42}, {"referenceID": 2, "context": "Compared to top-down, simulation-based models such as (Battaglia et al., 2013), deep models require far more training data \u2013 many thousands of examples \u2013 to achieve a high level of performance.", "startOffset": 54, "endOffset": 78}], "year": 2016, "abstractText": "Wooden blocks are a common toy for infants, allowing them to develop motor skills and gain intuition about the physical behavior of the world. In this paper, we explore the ability of deep feedforward models to learn such intuitive physics. Using a 3D game engine, we create small towers of wooden blocks whose stability is randomized and render them collapsing (or remaining upright). This data allows us to train large convolutional network models which can accurately predict the outcome, as well as estimating the block trajectories. The models are also able to generalize in two important ways: (i) to new physical scenarios, e.g. towers with an additional block and (ii) to images of real wooden blocks, where it obtains a performance comparable to human subjects.", "creator": "LaTeX with hyperref package"}}}