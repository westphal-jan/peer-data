{"id": "1603.01354", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks on different languages. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER.", "histories": [["v1", "Fri, 4 Mar 2016 05:55:02 GMT  (142kb,D)", "http://arxiv.org/abs/1603.01354v1", "10 pages, 3 figures. Submission on ACL 2016"], ["v2", "Mon, 7 Mar 2016 06:19:37 GMT  (142kb,D)", "http://arxiv.org/abs/1603.01354v2", "10 pages, 3 figures. Submission on ACL 2016"], ["v3", "Tue, 8 Mar 2016 05:16:17 GMT  (143kb,D)", "http://arxiv.org/abs/1603.01354v3", "10 pages, 3 figures. Submission on ACL 2016"], ["v4", "Mon, 14 Mar 2016 21:46:13 GMT  (143kb,D)", "http://arxiv.org/abs/1603.01354v4", "10 pages, 3 figures. Submission on ACL 2016"], ["v5", "Sun, 29 May 2016 00:42:15 GMT  (143kb,D)", "http://arxiv.org/abs/1603.01354v5", "10 pages, 3 figures. To appear on ACL 2016"]], "COMMENTS": "10 pages, 3 figures. Submission on ACL 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CL stat.ML", "authors": ["xuezhe ma", "eduard h hovy"], "accepted": true, "id": "1603.01354"}, "pdf": {"name": "1603.01354.pdf", "metadata": {"source": "CRF", "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "authors": ["Xuezhe Ma"], "emails": ["xuezhem@cs.cmu.edu,", "ehovy@andrew.cmu.edu"], "sections": [{"heading": null, "text": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of handcrafted features and data pre-processing. In this article, we introduce a novel neutral network architecture that automatically benefits from text and character representation by combining bi-directional LSTM, CNN, and CRF. Our system is truly end-to-end and does not require feature engineering or data pre-processing, making it applicable to a wide range of sequence labeling tasks in multiple languages. We evaluate our system using two sets of data for two sequence labeling tasks - Penn Treebank WSJ corpus for Part-of-Speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER)."}, {"heading": "1 Introduction", "text": "It is one of the first phases of deep language understanding and its importance has been well recognized in the natural language processing community. However, natural language processing (NLP) systems, such as syntactic parsing (Nivre and Scholz, 2004; Koo and Collins, 2010; Ma and Zhao, 2012; Chen and Manning, 2014) and entity correspondence resolution (Ng, 2010) are becoming increasingly complex because they tag the output information from POS or not. Most traditional high-performance labeling models are linear statistical models, including Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, Passos et al., 2014; Luo et al.), which rely heavily on craftsmanship characteristics and task-specific resources."}, {"heading": "2 Neural Network Architecture", "text": "In this section we describe the components (layers) of our neural network architecture. We introduce the neural layers in our neural network individually from bottom to top."}, {"heading": "2.1 CNN for Character-level Representation", "text": "Previous studies (Santos and Zadrozny, 2014; Chiu and Nichols, 2015) have shown that CNN is an effective approach to extracting morphological information (such as the prefix or suffix of a word) from letters of words and encoding it into neural representations. Figure 1 shows the CNN we use to extract the representation of a word at character level. CNN is similar to that in Chiu and Nichols (2015) except that we only use character embeddings as CNN inputs, without character types. Before entering character embeddings on CNN, a default layer is applied (Srivastava et al., 2014)."}, {"heading": "2.2 Bi-directional LSTM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1 LSTM Unit", "text": "Recursive neural networks (RNNs) are a powdery family of connection models that capture time dynamics over cycles in the diagram. Although RNNs are theoretically capable of detecting long dependencies, they fail in practice due to the problems of gradient varnishing / explosion (Bengio et al., 1994; Pascanu et al., 2012).LSTMs (Hochreiter and Schmidhuber, 1997) are a variant of RNNs designed to address the gradient varnishing problems inherent in RNs. Essentially, an LSTM unit consists of three multiplicative gates that control the information portions in order to forget them and move on to the next time step. Figure 2 gives the basic structure of an LSTM unit. Formally, the formulas for updating an LSTM unity time element are t: it is the LSTM time element (iht + ixt)."}, {"heading": "2.2.2 BLSTM", "text": "An elegant solution, the efficacy of which has been proven in previous work (Graves and Schmidhuber, 2005; Dyer et al., 2015), is bi-directional LSTM (BLSTM). The basic idea is to present each sequence forwards and backwards in two separate hidden states in order to capture past and future information, respectively. Then, the two hidden states are concatenated to form the final output."}, {"heading": "2.3 CRF", "text": "For sequence labeling (or general structured prediction tasks), it is advantageous to consider the correlations between labels in neighborhoods and jointly decipher the best chain of labels for a given input set. For example, in POS labeling, an adjective can be followed by a noun rather than a verb, and in NER with standard BIO2 annotations (Tjong Kim Sang and Veenstra, 1999), I-ORG cannot follow the I-PER. Therefore, we jointly model the label sequence using a conditional random field (CRF) (Lafferty et al., 2001) instead of deciphering each label sequence independently. Formally, we use z = {z1, \u00b7, zn} to represent a generic input sequence where zi is the input vector of the ith word y."}, {"heading": "2.4 BLSTM-CNNs-CRF", "text": "Finally, we construct our neural network model by feeding BLSTM output vectors into a CRF layer. Figure 3 illustrates the architecture of our network in detail. For each word, CNN calculates the character-level representation with character embedded inputs in Figure 1. Then, the character-level vector is linked to the word embed vector to feed into the BLSTM network. Finally, BLSTM output vectors are fed into the CRF layer to jointly decipher the best label sequence. As shown in Figure 3, failure layers are applied to both BLSTM input and output vectors. Experimental results show that the use of suspensions significantly improves the performance of our model (see Section 4.5 for details)."}, {"heading": "3 Network Training", "text": "We implement the neural network using the Theano library (Bergstra et al., 2010). Calculations for a single model are performed on a GeForce GTX TITAN X GPU. Using the settings described in this section, model training takes about 12 hours for the POS marker and 8 hours for the NI."}, {"heading": "3.1 Parameter Initialization", "text": "Word Embeddings. We use publicly available Stanford's GloVe 100 dimensional embeddings11http: / / nlp.stanford.edu / projects / glove / dressed on 6 billion words from Wikipedia and web text (Pennington et al., 2014) We also conduct experiments with two other sets of published embeddings, namely Senna 50 dimensional embeddings2 trained on Wikipedia and Reuters RCV-1 corpus (Collobert et al., 2011), and Google's Word2Vec 300 dimensional embeddings3 trained on 100 billion words from Google News (Mikolov et al., 2013). To test the effectiveness of prefabricated word embeddings, we experimented with randomly initialized embedings with 100 dimensions where embedding is uni-formally out of reach [\u2212 \u221a 3 dim, + 3dim] where dim is the dimension of embedings (He et al., 2015)."}, {"heading": "3.2 Optimization Algorithm", "text": "Parameter optimization is performed with minibatch stochastic gradient descent (SGD) with batch size 10 and dynamics 0.9. We choose an initial learning rate of \u03b70 (\u03b70 = 0.01 for POS tagging, and 0.015 for NER, see Section 3.3.), and the learning rate is updated to each epoch of education as \u03b7t = \u03b7t = 0 / (1 + 0), with decay rate \u03c1 = 0.05 and t is the number of completed epochs. To reduce the effects of the \"gradient explosion,\" we use a gradient cutout of 5.0 (Pascanu et al., 2012). We explored other more refined optimization algorithms such as AdaDelta (Zeiler, 2012), Adam (Kingma and Ba, 2014) or RMSProp (Dauphin et al., 2015), none of them significantly improving SGD with dynamics and gradient cutouts in our preliminary experiments. We use Early Stopping."}, {"heading": "3.3 Tuning Hyper-Parameters", "text": "Table 1 summarizes the selected hyperparameters for all experiments. We match the hyperparameters to the development sets by random search. Due to time constraints, it is not possible to perform a random search across the entire hyperparameter space. Therefore, for the tasks of POS tagging and NERs, we try to use as many common hyperparameters as possible. Note that the final hyperparameters for these two tasks are almost identical, except for the initial learning rate. We set the state size of the LSTM to 200. Adjusting this parameter has no significant effect on the performance of our model. For CNN, we use 30 filters with window width 3."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data Sets", "text": "As already mentioned, we evaluate our neural network model using two sequence tagging tasks: POS tagging and NER. POS tagging. For English POS tagging, we use the section of the Wall Street Journal (WSJ) of Penn Treebank (PTB) (Marcus et al., 1993), which contains 45 different POS tags. To compare with previous work, we use the standard splits - sections 0-18 as training data, section 19-21 as development data, and sections 22-24 as test data (Manning, 2011; S\u00f8gaard, 2011). For NER, we conduct experiments with the English data from the joint task of CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). This data set contains four different types of named units: PERSON, LOCATION, ORGANIZATION, and MISC. We use the BIOES tagging scheme instead of standard BIO2, as previous studies have reported significant improvements with this scheme."}, {"heading": "4.2 Main Results", "text": "First, we conduct experiments to analyze the effectiveness of each component (layer) of our neural network architecture through ablation studies. We compare performance with three base systems - BRNN, the bidirectional RNN; BLSTM, the bidirectional LSTM and BLSTM-CNNs, the combination of BLSTM and CNN for modeling character level information. All of these models run with Stanford's GloVe 100-dimensional word embedding and the same hyperparameters as in Table 1. According to the results, BLSTM performs better than BRNN in all evaluation metrics of both tasks. BLSTMCNN models significantly exceed the BLSTM model and show that character level representations are important for linguistic sequencing tasks. This is consistent with the results of previous work (Santos and Zadrozny, 2014; Chiu and Nichols, 2015). Finally, by adding modules to both the STCRM and STCRM are significant improvements to the decoding properties of all networks."}, {"heading": "4.3 Comparison with Previous Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1 POS Tagging", "text": "Table 4 illustrates the results of our POS marking model, together with seven previous top performance systems for comparison. Our model performs significantly better than Senna (Collobert et al., 2011), which is an upstream neural network model based on capital and discrete suffix characteristics and data pre-processing. In addition, our model achieves 0.23% improvements in accuracy over the \"CharWNN\" (Santos and Zadrozny, 2014), a neural network model based on Senna and also using CNNs to represent character levels, demonstrating the effectiveness of BLSTM in modeling sequential data and the importance of shared decryption with structured prediction models. Compared to conventional statistical models, our system achieves state-of-the-art accuracy and achieves a 0.05% improvement over the previously best reported results from S\u00f8gaard (2011)."}, {"heading": "4.3.2 NER", "text": "Table 5 shows the F1 values of previous models for NER using the test data set from the joint task CoNLL-2003. For comparison, we list their results together with ours. Similar to the observations of POS marking, our model achieves significant improvements over Senna and the other two neural models, namely the LSTM CRF and LSTM CNNs proposed by Huang et al. (2015) and the LSTM CRF and LSTM CNNs proposed by Chiu and Nichols (2015). Huang et al. (2015) used discrete spelling, pos and context features, and Chiu and Nichols (2015) used character, capitalization and lexicon features, as well as some task-specific data pre-processing features, while our model does not require carefully designed features or data pre-processing. We must point out that the result reported by Chiu and Nichols (2015) is incomparable to ours (90.77%) as their final model is based on the combination of training data and development data."}, {"heading": "4.4 Word Embeddings", "text": "As mentioned in Section 3.1, to initialize our model, we conducted experiments with different groups of publicly published word embeddings and with a random sampling method. Table 6 shows the performance of three different word embeddings and the randomly selected word embeddings. According to the results in Table 6.4We conduct experiments with the same setting and get a% -1 scale. 5The numbers come from Table 3 of the original paper (Luo et al., 2015). Although there are clearly inconsistencies in precision, recall, and F1 values, it is unclear how they are faulty. Models that use pre-trained word embeddings achieve a significant improvement compared to those that use random embeddings. Comparing the two tasks, NER relies more heavily on pre-tracted embeddings than on POS scriptions. This is consistent with the results obtained from previous work (Colloet al., 2011; better, al., al., than Huang; al., al."}, {"heading": "4.5 Effect of Dropout", "text": "Table 7 compares the results with and without failure layers for each data set. All other hyperparameters remain the same as in Table 1. We observe a significant improvement for both tasks. It shows the effectiveness of failure layers in reducing overfit."}, {"heading": "5 Related Work", "text": "In recent years, several different neural network architectures have been proposed and successfully applied to linguistic sequence labeling such as POS tagging, chunking, and NER. The two most similar approaches to our model, among these neural architectures, are the BLSTM-CRF model proposed by Huang et al. (2015) and the LSTMCNNs model presented by Chiu and Nichols (2015).Huang et al. (2015) uses bidirectional LSTM for word-level representations and CRF for joint labeling decryption, which is similar to our model. However, there are two main differences between their models. First, they did not use CNNs to model sequence information at the character level. Second, they combined their neural network model with handcrafted functions to improve their performance, making their model not an end-to-end system."}, {"heading": "6 Conclusion", "text": "In this paper, we proposed an architecture of neural networks for sequence labeling. We model both character and word representations using CNN or bidirectional LSTM. In addition to BLSTM, we use a sequential CRF to jointly decipher the sequence of labels for the given sentence. It is a true end-to-end model that does not rely on task-specific resources, feature engineering or data pre-processing. Experimental results on two linguistic sequence marking tasks - POS tagging and NER - show the effectiveness of our model. We achieved state-of-the-art performance on both tasks compared to previously state-of-the-art systems developed with manual functions, data pre-processing and external resources. There are several potential directions for future work. First, our model can be further improved by exploring learning approaches for multiple tasks to combine more useful and correlated information."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proceedings of EMNLP-2014,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Named entity recognition: a maximum entropy approach using global information", "author": ["Chieu", "Ng2002] Hai Leong Chieu", "Hwee Tou Ng"], "venue": "In Proceedings of CoNLL-2003,", "citeRegEx": "Chieu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chieu et al\\.", "year": 2002}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Chiu", "Nichols2015] Jason PC Chiu", "Eric Nichols"], "venue": "arXiv preprint arXiv:1511.08308", "citeRegEx": "Chiu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches. Syntax, Semantics and Structure in Statistical Translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Kuksa.,? \\Q2011\\E", "shortCiteRegEx": "Kuksa.", "year": 2011}, {"title": "Enhancing of chemical compound and drug name recognition using representative tag scheme and fine-grained tokenization", "author": ["Dai et al.2015] Hong-Jie Dai", "Po-Ting Lai", "Yung-Chun Chang", "Richard Tzong-Han Tsai"], "venue": "Journal of cheminformat-", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Rmsprop and equilibrated adaptive learning rates for non-convex optimization. arXiv preprint arXiv:1502.04390", "author": ["Harm de Vries", "Junyoung Chung", "Yoshua Bengio"], "venue": null, "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Victor Guimaraes", "RJ Niter\u00f3i", "Rio de Janeiro"], "venue": "In Proceedings of NEWS 2015 The Fifth Named Entities Workshop,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of ACL-2015", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "In Proceedings of HLT-NAACL-2003,", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers et al.2000] Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Gers et al.2003] Felix A Gers", "Nicol N Schraudolph", "J\u00fcrgen Schmidhuber"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gers et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping", "author": [], "venue": "In Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference,", "citeRegEx": "Giles.,? \\Q2001\\E", "shortCiteRegEx": "Giles.", "year": 2001}, {"title": "Svmtool: A general pos tagger generator based on support vector machines", "author": ["Gim\u00e9nez", "M\u00e0rquez2004] Jes\u00fas Gim\u00e9nez", "Llu\u0131\u0301s M\u00e0rquez"], "venue": "Proceedings of LREC-2004", "citeRegEx": "Gim\u00e9nez et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gim\u00e9nez et al\\.", "year": 2004}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Bengio2010] Xavier Glorot", "Yoshua Bengio"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Learning task-dependent distributed representations by backpropagation through", "author": ["Goller", "Kuchler1996] Christoph Goller", "Andreas Kuchler"], "venue": null, "citeRegEx": "Goller et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Goller et al\\.", "year": 1996}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves et al.2013] Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "In Proceedings of ICASSP-2013,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He et al.2015] Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Efficient third-order dependency parsers", "author": ["Koo", "Collins2010] Terry Koo", "Michael Collins"], "venue": "In Proceedings of ACL-2010,", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Non-lexical neural architecture for finegrained pos tagging", "author": ["Kevin L\u00f6ser", "Alexandre Allauzen", "Rue John von Neumann"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Labeau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Labeau et al\\.", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando CN Pereira"], "venue": "In Proceedings of ICML2001,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["LeCun et al.1989] Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Phrase clustering for discriminative learning", "author": ["Lin", "Wu2009] Dekang Lin", "Xiaoyun Wu"], "venue": "In Proceedings of ACL-2009,", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Joint entity recognition and disambiguation", "author": ["Luo et al.2015] Gang Luo", "Xiaojiang Huang", "ChinYew Lin", "Zaiqing Nie"], "venue": "In Proceedings of EMNLP-2015,", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Fourth-order dependency parsing", "author": ["Ma", "Zhao2012] Xuezhe Ma", "Hai Zhao"], "venue": "In Proceedings of COLING 2012: Posters,", "citeRegEx": "Ma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2012}, {"title": "Partof-speech tagging from 97% to 100%: is it time for some linguistics", "author": ["Christopher D Manning"], "venue": "In Computational Linguistics and Intelligent Text Processing,", "citeRegEx": "Manning.,? \\Q2011\\E", "shortCiteRegEx": "Manning.", "year": 2011}, {"title": "Building a large annotated corpus of English: the Penn Treebank", "author": ["Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Online largemargin training of dependency parsers", "author": ["Koby Crammer", "Fernando Pereira"], "venue": "In Proceedings of ACL-2005,", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Supervised noun phrase coreference research: The first fifteen years", "author": ["Vincent Ng"], "venue": "In Proceedings of ACL-2010,", "citeRegEx": "Ng.,? \\Q2010\\E", "shortCiteRegEx": "Ng.", "year": 2010}, {"title": "Deterministic dependency parsing of English text", "author": ["Nivre", "Scholz2004] Joakim Nivre", "Mario Scholz"], "venue": "In Proceedings of COLING-2004,", "citeRegEx": "Nivre et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2004}, {"title": "On the difficulty of training recurrent neural networks. arXiv preprint arXiv:1211.5063", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": "In Proceedings of CoNLL-2014,", "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Named entity recognition for chinese social media with jointly trained embeddings", "author": ["Peng", "Dredze2015] Nanyun Peng", "Mark Dredze"], "venue": "In Proceedings of EMNLP-2015,", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of EMNLP-2014,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In Proceedings of CoNLL-2009,", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D Santos", "Bianca Zadrozny"], "venue": "In Proceedings of ICML-2014,", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Guided learning for bidirectional sequence classification", "author": ["Shen et al.2007] Libin Shen", "Giorgio Satta", "Aravind Joshi"], "venue": "In Proceedings of ACL-2007,", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Semisupervised condensed nearest neighbor for part-ofspeech tagging", "author": ["Anders S\u00f8gaard"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "S\u00f8gaard.,? \\Q2011\\E", "shortCiteRegEx": "S\u00f8gaard.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Structure regularization for structured prediction", "author": ["Xu Sun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sun.,? \\Q2014\\E", "shortCiteRegEx": "Sun.", "year": 2014}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of CoNLL-2003 - Volume", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Representing text chunks", "author": ["Tjong Kim Sang", "Jorn Veenstra"], "venue": "In Proceedings of EACL\u201999,", "citeRegEx": "Sang et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sang et al\\.", "year": 1999}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of NAACL-HLT2003,", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 35, "context": "Natural language processing (NLP) systems, like syntactic parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Koo and Collins, 2010; Ma and Zhao, 2012; Chen and Manning, 2014) and entity coreference resolution (Ng, 2010), are becoming more sophisticated, in part because of utilizing output information of POS tagging or NER systems.", "startOffset": 66, "endOffset": 179}, {"referenceID": 37, "context": ", 2005; Koo and Collins, 2010; Ma and Zhao, 2012; Chen and Manning, 2014) and entity coreference resolution (Ng, 2010), are becoming more sophisticated, in part because of utilizing output information of POS tagging or NER systems.", "startOffset": 108, "endOffset": 118}, {"referenceID": 40, "context": "Most traditional high performance sequence labeling models are linear statistical models, including Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015), which rely heavily on hand-crafted features and taskspecific resources.", "startOffset": 163, "endOffset": 226}, {"referenceID": 31, "context": "Most traditional high performance sequence labeling models are linear statistical models, including Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015), which rely heavily on hand-crafted features and taskspecific resources.", "startOffset": 163, "endOffset": 226}, {"referenceID": 13, "context": "Recently, recurrent neural networks (RNN) (Goller and Kuchler, 1996), together with its variants such as long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) and gated recurrent unit (GRU) (Cho et al.", "startOffset": 135, "endOffset": 188}, {"referenceID": 6, "context": ", 2000) and gated recurrent unit (GRU) (Cho et al., 2014), have shown great success in modeling sequential data.", "startOffset": 39, "endOffset": 57}, {"referenceID": 20, "context": "Several RNNbased neural network models have been proposed to solve sequence labeling tasks like speech recognition (Graves et al., 2013), POS tagging (Huang et al.", "startOffset": 115, "endOffset": 136}, {"referenceID": 23, "context": ", 2013), POS tagging (Huang et al., 2015) and NER (Chiu and Nichols, 2015), achieving competitive performance against traditional models.", "startOffset": 21, "endOffset": 41}, {"referenceID": 29, "context": "We first use convolutional neural networks (CNNs) (LeCun et al., 1989) to encode character-level information of a word into its character-level representation.", "startOffset": 50, "endOffset": 70}, {"referenceID": 34, "context": "We evaluate our model on two linguistic sequence labeling task \u2014 POS tagging on Penn Treebank WSJ (Marcus et al., 1993), and NER on English data from CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003).", "startOffset": 98, "endOffset": 119}, {"referenceID": 47, "context": "A dropout layer (Srivastava et al., 2014) is applied before character embeddings are input to CNN.", "startOffset": 16, "endOffset": 41}, {"referenceID": 1, "context": "Though, in theory, RNNs are capable to capturing long dependencies, in practice, they fail due to the gradient varnishing/exploding problems (Bengio et al., 1994; Pascanu et al., 2012).", "startOffset": 141, "endOffset": 184}, {"referenceID": 39, "context": "Though, in theory, RNNs are capable to capturing long dependencies, in practice, they fail due to the gradient varnishing/exploding problems (Bengio et al., 1994; Pascanu et al., 2012).", "startOffset": 141, "endOffset": 184}, {"referenceID": 14, "context": "It should be noted that we do not include peephole connections (Gers et al., 2003) in the our LSTM formulation.", "startOffset": 63, "endOffset": 82}, {"referenceID": 11, "context": "An elegant solution whose effectiveness has been proven by previous work (Graves and Schmidhuber, 2005; Dyer et al., 2015) is bi-directional LSTM (BLSTM).", "startOffset": 73, "endOffset": 122}, {"referenceID": 28, "context": "Therefore, we model label sequence jointly using a conditional random fields (CRF) (Lafferty et al., 2001), instead of decoding each label independently.", "startOffset": 83, "endOffset": 106}, {"referenceID": 28, "context": "For a sequence CRF model (only interactions between two successive labels are considered), training and decoding can be solved efficiently by adopting the Viterbi algorithm (Lafferty et al., 2001).", "startOffset": 173, "endOffset": 196}, {"referenceID": 2, "context": "We implement the neural network using Theano library (Bergstra et al., 2010).", "startOffset": 53, "endOffset": 76}, {"referenceID": 42, "context": "glove/ trained on 6 billion words from Wikipedia and web text (Pennington et al., 2014)", "startOffset": 62, "endOffset": 87}, {"referenceID": 36, "context": ", 2011), and Google\u2019s Word2Vec 300 dimensional embeddings3 trained on 100 billion words from Google News (Mikolov et al., 2013).", "startOffset": 105, "endOffset": 127}, {"referenceID": 21, "context": "3 dim ] where dim is the dimension of embeddings (He et al., 2015).", "startOffset": 49, "endOffset": 66}, {"referenceID": 24, "context": "Bias vectors are initialized to zero, expect the bias bf for forget gate in LSTM , which is initialized to one (Jozefowicz et al., 2015).", "startOffset": 111, "endOffset": 136}, {"referenceID": 39, "context": "0 (Pascanu et al., 2012).", "startOffset": 2, "endOffset": 24}, {"referenceID": 52, "context": "We explored other more sophisticated optimization algorithms such as AdaDelta (Zeiler, 2012), Adam (Kingma and Ba, 2014) or RMSProp (Dauphin et al.", "startOffset": 78, "endOffset": 92}, {"referenceID": 9, "context": "We explored other more sophisticated optimization algorithms such as AdaDelta (Zeiler, 2012), Adam (Kingma and Ba, 2014) or RMSProp (Dauphin et al., 2015), none of them meaningfully improve upon SGD with momentum and gradient clipping in our preliminary experiments.", "startOffset": 132, "endOffset": 154}, {"referenceID": 15, "context": "We use early stopping (Giles, 2001; Graves et al., 2013) based on performance", "startOffset": 22, "endOffset": 56}, {"referenceID": 20, "context": "We use early stopping (Giles, 2001; Graves et al., 2013) based on performance", "startOffset": 22, "endOffset": 56}, {"referenceID": 47, "context": "To mitigate overfitting, we apply dropout method (Srivastava et al., 2014) to regularize our model.", "startOffset": 49, "endOffset": 74}, {"referenceID": 34, "context": "For English POS tagging, we use the Wall Street Journal (WSJ) portion of Penn Treebank (PTB) (Marcus et al., 1993), which contains 45 different POS tags.", "startOffset": 93, "endOffset": 114}, {"referenceID": 33, "context": "In order to compare with previous work, we adopt the standard splits \u2014 section 0-18 as training data, section 1921 as development data and section 22-24 as test data (Manning, 2011; S\u00f8gaard, 2011).", "startOffset": 166, "endOffset": 196}, {"referenceID": 46, "context": "In order to compare with previous work, we adopt the standard splits \u2014 section 0-18 as training data, section 1921 as development data and section 22-24 as test data (Manning, 2011; S\u00f8gaard, 2011).", "startOffset": 166, "endOffset": 196}, {"referenceID": 8, "context": "We use the BIOES tagging scheme instead of standard BIO2, as previous studies have reported meaningful improvement with this scheme over others like BIO2 (Ratinov and Roth, 2009; Chiu and Nichols, 2015; Dai et al., 2015).", "startOffset": 154, "endOffset": 220}, {"referenceID": 47, "context": "16 Toutanova et al. (2003) 97.", "startOffset": 3, "endOffset": 27}, {"referenceID": 33, "context": "27 Manning (2011) 97.", "startOffset": 3, "endOffset": 18}, {"referenceID": 33, "context": "27 Manning (2011) 97.28 Collobert et al. (2011)\u2021 97.", "startOffset": 3, "endOffset": 48}, {"referenceID": 33, "context": "27 Manning (2011) 97.28 Collobert et al. (2011)\u2021 97.29 Santos and Zadrozny (2014)\u2021 97.", "startOffset": 3, "endOffset": 82}, {"referenceID": 33, "context": "27 Manning (2011) 97.28 Collobert et al. (2011)\u2021 97.29 Santos and Zadrozny (2014)\u2021 97.32 Shen et al. (2007) 97.", "startOffset": 3, "endOffset": 108}, {"referenceID": 33, "context": "27 Manning (2011) 97.28 Collobert et al. (2011)\u2021 97.29 Santos and Zadrozny (2014)\u2021 97.32 Shen et al. (2007) 97.33 Sun (2014) 97.", "startOffset": 3, "endOffset": 125}, {"referenceID": 33, "context": "27 Manning (2011) 97.28 Collobert et al. (2011)\u2021 97.29 Santos and Zadrozny (2014)\u2021 97.32 Shen et al. (2007) 97.33 Sun (2014) 97.36 S\u00f8gaard (2011) 97.", "startOffset": 3, "endOffset": 146}, {"referenceID": 34, "context": "Chieu and Ng (2002) 88.", "startOffset": 10, "endOffset": 20}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.", "startOffset": 3, "endOffset": 25}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.", "startOffset": 3, "endOffset": 53}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.", "startOffset": 3, "endOffset": 83}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.59 Huang et al. (2015)\u2021 90.", "startOffset": 3, "endOffset": 110}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.59 Huang et al. (2015)\u2021 90.10 Chiu and Nichols (2015)\u2021 90.", "startOffset": 3, "endOffset": 141}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.59 Huang et al. (2015)\u2021 90.10 Chiu and Nichols (2015)\u2021 90.77 Ratinov and Roth (2009) 90.", "startOffset": 3, "endOffset": 172}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.59 Huang et al. (2015)\u2021 90.10 Chiu and Nichols (2015)\u2021 90.77 Ratinov and Roth (2009) 90.80 Lin and Wu (2009) 90.", "startOffset": 3, "endOffset": 196}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.59 Huang et al. (2015)\u2021 90.10 Chiu and Nichols (2015)\u2021 90.77 Ratinov and Roth (2009) 90.80 Lin and Wu (2009) 90.90 Passos et al. (2014) 90.", "startOffset": 3, "endOffset": 223}, {"referenceID": 12, "context": "31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.59 Huang et al. (2015)\u2021 90.10 Chiu and Nichols (2015)\u2021 90.77 Ratinov and Roth (2009) 90.80 Lin and Wu (2009) 90.90 Passos et al. (2014) 90.90 Luo et al. (2015) 91.", "startOffset": 3, "endOffset": 247}, {"referenceID": 45, "context": "05% improvement over the previously best reported results by S\u00f8gaard (2011). It should be noted that Huang et al.", "startOffset": 61, "endOffset": 76}, {"referenceID": 23, "context": "It should be noted that Huang et al. (2015) also evaluated their BLSTM-CRF model for POS tagging on WSJ corpus.", "startOffset": 24, "endOffset": 44}, {"referenceID": 23, "context": "Similar to the observations of POS tagging, our model achieves significant improvements over Senna and the other two neural models, namely the LSTM-CRF proposed by Huang et al. (2015) and LSTM-CNNs proposed by Chiu and Nichols (2015).", "startOffset": 164, "endOffset": 184}, {"referenceID": 23, "context": "Similar to the observations of POS tagging, our model achieves significant improvements over Senna and the other two neural models, namely the LSTM-CRF proposed by Huang et al. (2015) and LSTM-CNNs proposed by Chiu and Nichols (2015). Huang et al.", "startOffset": 164, "endOffset": 234}, {"referenceID": 23, "context": "Similar to the observations of POS tagging, our model achieves significant improvements over Senna and the other two neural models, namely the LSTM-CRF proposed by Huang et al. (2015) and LSTM-CNNs proposed by Chiu and Nichols (2015). Huang et al. (2015) utilized discrete spelling, pos and context features, and Chiu and Nichols (2015) used character-type, capitalization, and lexicon features and some taskspecific data pre-processing, while our model does not require any carefully designed features or data pre-processing.", "startOffset": 164, "endOffset": 255}, {"referenceID": 23, "context": "Similar to the observations of POS tagging, our model achieves significant improvements over Senna and the other two neural models, namely the LSTM-CRF proposed by Huang et al. (2015) and LSTM-CNNs proposed by Chiu and Nichols (2015). Huang et al. (2015) utilized discrete spelling, pos and context features, and Chiu and Nichols (2015) used character-type, capitalization, and lexicon features and some taskspecific data pre-processing, while our model does not require any carefully designed features or data pre-processing.", "startOffset": 164, "endOffset": 337}, {"referenceID": 23, "context": "Similar to the observations of POS tagging, our model achieves significant improvements over Senna and the other two neural models, namely the LSTM-CRF proposed by Huang et al. (2015) and LSTM-CNNs proposed by Chiu and Nichols (2015). Huang et al. (2015) utilized discrete spelling, pos and context features, and Chiu and Nichols (2015) used character-type, capitalization, and lexicon features and some taskspecific data pre-processing, while our model does not require any carefully designed features or data pre-processing. We have to point out that the result (90.77%) reported by Chiu and Nichols (2015) is incomparable with ours since their final model was trained on the combination of training and development data sets4.", "startOffset": 164, "endOffset": 609}, {"referenceID": 31, "context": "20)5 reported on CoNLL 2003 data set is by the joint NER and entity linking model (Luo et al., 2015).", "startOffset": 82, "endOffset": 100}, {"referenceID": 31, "context": "Numbers are taken from the Table 3 of the original paper (Luo et al., 2015).", "startOffset": 57, "endOffset": 75}, {"referenceID": 23, "context": "This is consistent with results reported by previous work (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015).", "startOffset": 58, "endOffset": 126}, {"referenceID": 23, "context": "The two most similar approaches to our model, among these neural architectures, are the BLSTM-CRF model proposed by Huang et al. (2015) and the LSTMCNNs model presented by Chiu and Nichols (2015).", "startOffset": 116, "endOffset": 136}, {"referenceID": 23, "context": "The two most similar approaches to our model, among these neural architectures, are the BLSTM-CRF model proposed by Huang et al. (2015) and the LSTMCNNs model presented by Chiu and Nichols (2015).", "startOffset": 116, "endOffset": 196}, {"referenceID": 26, "context": "Labeau et al. (2015) proposed a RNN-CNNs model for German POS tagging.", "startOffset": 0, "endOffset": 21}, {"referenceID": 26, "context": "Labeau et al. (2015) proposed a RNN-CNNs model for German POS tagging. This model is similar to the LSTM-CNNs model in Chiu and Nichols (2015), with the difference of using vanila RNN instead of LSTM.", "startOffset": 0, "endOffset": 143}], "year": 2017, "abstractText": "State-of-the-art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both wordand character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks on different languages. We evaluate our system on two data sets for two sequence labeling tasks \u2014 Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain stateof-the-art performance on both the two data \u2014 97.55% accuracy for POS tagging and 91.21% F1 for NER.", "creator": "LaTeX with hyperref package"}}}