{"id": "1603.01360", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "Neural Architectures for Named Entity Recognition", "abstract": "State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.", "histories": [["v1", "Fri, 4 Mar 2016 06:36:29 GMT  (123kb,D)", "http://arxiv.org/abs/1603.01360v1", "Proceedings of NAACL 2016"], ["v2", "Wed, 6 Apr 2016 03:11:58 GMT  (124kb,D)", "http://arxiv.org/abs/1603.01360v2", "Proceedings of NAACL 2016"], ["v3", "Thu, 7 Apr 2016 15:09:36 GMT  (124kb,D)", "http://arxiv.org/abs/1603.01360v3", "Proceedings of NAACL 2016"]], "COMMENTS": "Proceedings of NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["guillaume lample", "miguel ballesteros", "sandeep subramanian", "kazuya kawakami", "chris dyer"], "accepted": true, "id": "1603.01360"}, "pdf": {"name": "1603.01360.pdf", "metadata": {"source": "CRF", "title": "Neural Architectures for Named Entity Recognition", "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "emails": ["glample@cs.cmu.edu,", "sandeeps@cs.cmu.edu,", "kkawakam@cs.cmu.edu,", "cdyer@cs.cmu.edu,", "miguel.ballesteros@upf.edu"], "sections": [{"heading": null, "text": "State-of-the-art systems for detecting named entities rely heavily on craftsmanship and domain-specific knowledge to effectively learn from the small, monitored training corpus available. In this paper, we present two new neural architectures - one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments based on an approach inspired by shift-reduction parsers. Our models rely on two sources of information about words: character-based word representations learned from the monitored corpus, and unattended word representations learned from uncommented corpora."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of the people mentioned are a person who is not able to look after a person, but a person who is able to look after a person who is able to look after a person."}, {"heading": "2 LSTM-CRF Model", "text": "We provide a brief description of LSTMs and CRFs and present a hybrid tagging architecture."}, {"heading": "2.1 LSTM", "text": "Recursive neural networks (RNNs) are a family of neural networks that operate on sequential data, taking as input a sequence of vectors (x1, x2,.., xn) and returning a different sequence (h1, h2,.., hn) that provides information about the sequence at each step in the input. Although RNNs can theoretically learn long dependencies, in practice they do not and tend to be biased against their most recent inputs in the sequence (Bengio et al., 1994). Long-term memory networks (LSTMs) were designed to combat this problem by incorporating a memory cell and have shown to capture far-reaching dependencies, using multiple gates that control the share of input to give the memory cell and forget the share from the previous state (Hochreiter and Schmidhuber, 1997)."}, {"heading": "2.2 CRF Tagging Models", "text": "A very simple - but surprisingly effective - marker model is the use of hht's as characteristics to make independent marker decisions for each output yt = (Ling et al., 2015b). Despite the success of this model for simple problems such as POS marking, its independent classification decisions are limited if there are strong dependencies between the output labels. NER is such a task because the \"grammar\" that characterizes interpretable marker sequences imposes several hard constraints (e.g. I-PER cannot follow B-LOC; see \u00a7 2.4 for details) that would be impossible to model with assumptions of independence. Instead of modelling the marker decisions independently, we model them together using a conditional random field (Lafferty et al., 2001). For an input marker Yyn = (x1, x2,), we consider the matrix of the LP-P matrix of the STi where the marker number is."}, {"heading": "2.3 Parameterization and Training", "text": "The results associated with each marker decision for each word in a sentence (i.e., the Pi, y's) are defined as the point product between the embedding of a word context computed with a bidirectional LSTM - just like the POS marker model by Ling et al. (2015b) and these are combined with Bigram compatibility values (i.e. the Ay, y's) This architecture is shown in Figure 1. Circles represent observed variables, diamonds are deterministic functions of their parents, and double circles are random variables. Thus, the parameters of this model are the matrix of the Bigram compatibility values A, and the parameters that lead to the matrix P, namely the parameters of the bidirectional LSTM, the linear property weights, and the word embedding."}, {"heading": "2.4 IOBES Tagging Scheme", "text": "A single designated entity could include several trademarks within a set. Sentences are usually presented in IOB (Inside, Outside, Beginning) format, with each symbol being referred to as a B label if the symbol is the beginning of a designated entity, I label if it is located within a designated entity, but not the first symbol within the designated entity or O otherwise. However, we chose the IOBES labeling scheme, a variant of IOB that encodes information about individual entities (S) and explicitly marks the end of designated entities (E), with which the marking of a word as an I label with high confidence limits the selection of the subsequent word on an I label or E label, but the IOB scheme is only able to determine that the subsequent word cannot be the inside of another label. Ratinov and Roth (2009) and Dai al (2015) showed a similar scheme to BES that improved performance of the EIOB by using appropriate labels."}, {"heading": "3 Transition-Based Chunking Model", "text": "As an alternative to the LSTM-CRF discussed in the previous section, we will examine a new architecture that fragments and labels a sequence of input using an algorithm similar to transition-based dependency sparsing. This model directly constructs representations of multi-token names (e.g. the name Mark Watney consists of a single figure).This model relies on a stack data structure to construct incremental sections of input. To obtain representations of this stack that are used to predict subsequent actions, we will use the stack-LSTM presented by Dyer et al. (2015), in which the LSTM is extended by a \"stack pointer.\" While sequential LSTM model sequences allow model sequences to be embedded from left to right, stack-LSTMs allow the embedding of a stack of objects that are both added (by a push operation) and removed (by a pop operation)."}, {"heading": "3.1 Chunking Algorithm", "text": "We have designed a transition inventory, given in Figure 2, inspired by transition-based parsers, in particular the standard arc parser from Nivre (2004).In this algorithm, we use two stacks (specific output and stacks, each representing finished chunks and scratch space) and a buffer that contains the words that still need to be processed.The transition inventory contains the following transitions: The SHIFT transition moves a word from the buffer to the stack, the OUT transition moves a word from the buffer directly into the output stack, while the REDUCE (y) transition opens all items from the top of the stack that create a \"chunk,\" labels this with Label y and pushes a representation of this chunk onto the output stack. The algorithm closes when the stack and buffer are both blank. The algorithm is shown in Figure 2, which is the sequence of the processing of the sentence required."}, {"heading": "3.2 Representing Labeled Chunks", "text": "When the REDUCE (y) operation is performed, the algorithm moves a sequence of tokens (along with their vector embedding) from the stack into the output buffer as a single completed piece. To calculate an embedding of this sequence, we run a bidirectional LSTM over the embedding of its component tokens together with a token that represents the type of the identified piece (i.e. y). This function is specified as g (u,.., v, ry), where ry is a learned embedding of a label type. Thus, the output buffer contains a single vector representation for each designated chunk that is generated, regardless of its length."}, {"heading": "4 Input Word Embeddings", "text": "The input layers of our two models are vector representations of individual words. Learning independent representations of word types from the limited NER training data is a difficult problem: there are simply too many parameters to be able to reliably estimate them. Since many languages have orthographic or morphological evidence that something is a name (or not a name), we want representations that are sensitive to the spelling of words. Therefore, we use a model that constructs representations of words from representations of the characters they are composed of (4.1). Our second intuition is that names that can be very different from one another occur regularly in large companies. Therefore, we use embeddings learned from a large corpus that responds sensitively to the word order (4.2). Finally, to prevent models from being too dependent on one representation or another, we use dropout training and find that this is crucial for good generalization performance (4.3)."}, {"heading": "4.1 Character-based models of words", "text": "An important distinction of our work from most previous approaches is that we learn character traits while training instead of hand-engineering prefixes and suffix information about words. Learning character embeddings has the advantage of learning specific representations for the task and domain in question. However, they have proven useful for morphologically rich languages and for solving the vocabulary problem for tasks such as part-of-speech tagging and language modeling (Ling et al., 2015b) or dependency sparing (Ballesteros et al., 2015).Figure 4 describes our architecture to generate word embeddings for a word from its characters. A randomly initialized character search table includes an embeddings for each character. The character embeddings that correspond to each character in a word are given in direct and reverse order to a forward and backward-looking word."}, {"heading": "4.2 Pretrained embeddings", "text": "As with Collobert et al. (2011), we use pre-cut word embeddings to initialize our reference table. We observe significant improvements through pre-cut word embeddings compared to randomly initialized ones. Embeddings are pre-trained using Skip-n-gram (Ling et al., 2015a), a variant of word2vec (Mikolov et al., 2013a), which is responsible for word order. These embeddings are fine-tuned during the training. Word embeddings for Spanish, Dutch, German and English are respected on the basis of the Spanish Gigaword version 3, the Leipzig company's collection, the German monolingual training data of the machine translation workshop 2010 and the English Giga Word version 4 (removing parts of the LA Times and NY Times). 2 We use an embedding dimension of 100 for English, 64 for other languages, a minimum word frequency of 4 and a window size of 8."}, {"heading": "4.3 Dropout training", "text": "Initial experiments showed that character-level embedding did not improve our overall performance when used in conjunction with pre-trained word representations. To encourage the model to be dependent on both representations, we use dropout training (Hinton et al., 2012) by placing a dropout mask on the last embedding layer just before entering into the bi-directional LSTM in Figure 1. We observe a significant improvement in the performance of our model after using dropouts (see Table 5)."}, {"heading": "5 Experiments", "text": "In this section, we present the methods we use to train our models, the results we have achieved in various tasks, and the impact of configuring our networks on model performance."}, {"heading": "5.1 Training", "text": "For both models presented, we train our networks using the backward propagation algorithm, which updates our parameters successively on each training example, using stochastic gradient drops (SGD) with a learning rate of 0.01 and gradient clipping of 5.0. Several methods have been suggested to improve the performance of SGD, such as Adadelta (Zeiler, 2012) or Adam (Kingma and Ba, 2014). Although we observe faster convergence with these methods, none of them perform as well as SGD with gradient clipping. Our LSTM CRF model uses a single layer for forward and backward-facing LSTMs, whose dimensions are set to 100. Tuning this dimension did not have a significant impact on model performance. We set the drop-out rate to 0.5. The use of higher rates negatively affected our results, while smaller rates led to longer training time.The Stack-LM model uses two layers for each dimension of 100 ropes."}, {"heading": "5.2 Data Sets", "text": "To demonstrate the ability of our model to generalize into different languages, we present results for the CoNLL-2002 and CoNL2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), which contain independent datasets for English, Spanish, German and Dutch. All datasets contain four different types of named units: locations, persons, organizations and other units that do not belong to any of the three preceding categories. Although POS tags have been provided for all datasets, we have not included them in our models."}, {"heading": "5.3 Results", "text": "To make the comparison between our model and others fair, we report the results of other models with and without external labeled data such as Gazetteers and Knowledge Base. Our models do not use Gazetteers or external labeled resources. Luo et al. (2015) achieved the best value by jointly modeling the NER and Knowledge Base tasks (Hoffart et al., 2011). Their model uses many handmade features, including spelling functions, WordNet clusters, brown clusters, POS tags, chunks tags, and master and external knowledge databases such as Freebase and Wikipedia. Our LSTM CRF model outperforms all other systems, including those that use external labeled data such as Gazetteers. Our StackLSTM model also outperforms all previous models that do not include external features, with the exception of the Dutch, CRF systems presented only the Spanish and the Spanish, the Spanish, the 2015, and the Spanish, and the three D."}, {"heading": "5.4 Network architectures", "text": "Our models included several components that we could optimize to understand their impact on overall performance. We investigated the impact that CRF, character-level display, pretraining of our word embeddings, and drop-out had on our LSTMCRF model. We observed that pretraining of our word embeddings gave us the greatest improvement in overall performance from + 7.31 in F1. CRF layer resulted in an increase of + 1.79, while using drop-out resulted in a difference of + 1.17, and finally learning character-level word embeddings resulted in an increase of about + 0.74. For the stack LSTM, we conducted a similar series of experiments. Results with different architectures are listed in Table 5."}, {"heading": "6 Related Work", "text": "In the joint task CoNLL-2002, Carreras et al. (2002) achieved the best results at both the Dutch and Spanish levels by combining several small decision trees with fixed depth. The next year, in the joint task CoNLL2003, Florian et al. (2003) achieved the best results at the German level by combining the results of four different classifiers. Qi et al. (2009) they later improved this with a neural network by performing sequential learning on a massive, unlabeled corpus.Several other neural architectures were previously proposed for NER. Thus, Collobert et al. (2011) uses a CNN model of a sequence of word embedding with a CRF layer on top. This can be considered as our first model without embedding at the character level and with the bidirectional LSTM architecture being replaced by a CNN model."}, {"heading": "7 Conclusion", "text": "This paper presents two neural sequence labeling architectures that deliver the best NER results ever reported in standard evaluation settings, even compared to models that use external resources such as gazetteers. A key aspect of our models is that they model dependencies on output labels, either using a simple CRF architecture or using a transition-based algorithm to explicitly construct and label portions of the input. Word representations are also critical to success: we use both pre-trained word representations and \"character-based\" representations that capture morphological and orthographic information. To prevent the learner from being too dependent on a representation class, dropout is used."}, {"heading": "Acknowledgments", "text": "Miguel Ballesteros is supported by the European Commission under contract numbers FP7-ICT610411 (Project MULTISENSOR) and H2020-RIA645012 (Project KRISTINA)."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005a] Rie Kubota Ando", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Learning predictive structures", "author": ["Ando", "Zhang2005b] Rie Kubota Ando", "Tong Zhang"], "venue": null, "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Improved transition-based dependency parsing by modeling characters instead of words with LSTMs", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "Proceedings of EMNLP", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "The leipzig corpora collection-monolingual corpora of standard size", "author": ["Gerhard Heyer", "Uwe Quasthoff", "Matthias Richter"], "venue": "Proceedings of Corpus Linguistic", "citeRegEx": "Biemann et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Biemann et al\\.", "year": 2007}, {"title": "Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation", "author": ["Philipp Koehn", "Christof Monz", "Kay Peterson", "Mark Przybocki", "Omar F Zaidan"], "venue": null, "citeRegEx": "Callison.Burch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2010}, {"title": "Named entity extraction using adaboost, proceedings of the 6th conference on natural language learning", "author": ["Llu\u0131\u0301s M\u00e0rquez", "Llu\u0131\u0301s Padr\u00f3"], "venue": null, "citeRegEx": "Carreras et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2002}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Chiu", "Nichols2015] Jason PC Chiu", "Eric Nichols"], "venue": "arXiv preprint arXiv:1511.08308", "citeRegEx": "Chiu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Kuksa.,? \\Q2011\\E", "shortCiteRegEx": "Kuksa.", "year": 2011}, {"title": "Language independent named entity recognition combining morphological and contextual evidence", "author": ["Cucerzan", "Yarowsky1999] Silviu Cucerzan", "David Yarowsky"], "venue": "In Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC,", "citeRegEx": "Cucerzan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Cucerzan et al\\.", "year": 1999}, {"title": "Language independent ner using a unified model of internal and contextual evidence", "author": ["Cucerzan", "Yarowsky2002] Silviu Cucerzan", "David Yarowsky"], "venue": "In proceedings of the 6th conference on Natural language learning-Volume", "citeRegEx": "Cucerzan et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Cucerzan et al\\.", "year": 2002}, {"title": "Enhancing of chemical compound and drug name recognition using representative tag scheme and fine-grained tokenization", "author": ["Dai et al.2015] Hong-Jie Dai", "Po-Ting Lai", "Yung-Chun Chang", "Richard Tzong-Han Tsai"], "venue": "Journal of cheminformatics,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proc. ACL", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Structured databases of named entities from bayesian nonparametrics", "author": ["Tae Yano", "William W Cohen", "Noah A Smith", "Eric P Xing"], "venue": "In Proceedings of the First Workshop on Unsupervised Learning in NLP,", "citeRegEx": "Eisenstein et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2011}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Multilingual language processing from bytes", "author": ["Gillick et al.2015] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": "arXiv preprint arXiv:1512.00103", "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Spanish gigaword third edition (ldc2011t12)", "author": ["David Graff"], "venue": "Linguistic Data Consortium,", "citeRegEx": "Graff.,? \\Q2011\\E", "shortCiteRegEx": "Graff.", "year": 2011}, {"title": "Framewise phoneme classification with bidirectional LSTM networks", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. IJCNN", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Robust disambiguation of named entities in text", "author": ["Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum"], "venue": null, "citeRegEx": "Hoffart et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2011}, {"title": "Bidirectional LSTM-CRF models for sequence tagging. CoRR, abs/1508.01991", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Character-aware neural language models. CoRR, abs/1508.06615", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando CN Pereira"], "venue": "In Proc. ICML", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Phrase clustering for discriminative learning", "author": ["Lin", "Wu2009] Dekang Lin", "Xiaoyun Wu"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "2015a. Not all contexts are created equal: Better word representations with variable attention", "author": ["Ling et al.2015a] Wang Ling", "Lin Chu-Cheng", "Yulia Tsvetkov", "Silvio Amir", "R\u00e1mon Fernandez Astudillo", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015b] Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Joint named entity recognition and disambiguation", "author": ["Luo et al.2015] Gang Luo", "Xiaojiang Huang", "Chin-Yew Lin", "Zaiqing Nie"], "venue": "In Proc. EMNLP", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre"], "venue": "In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together", "citeRegEx": "Nivre.,? \\Q2004\\E", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Learning multilingual named entity recognition from wikipedia", "author": ["Nothman et al.2013] Joel Nothman", "Nicky Ringland", "Will Radford", "Tara Murphy", "James R Curran"], "venue": "Artificial Intelligence,", "citeRegEx": "Nothman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nothman et al\\.", "year": 2013}, {"title": "English gigaword fourth edition (ldc2009t13)", "author": ["Parker et al.2009] Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda"], "venue": "Linguistic Data Consortium,", "citeRegEx": "Parker et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Parker et al\\.", "year": 2009}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": "arXiv preprint arXiv:1404.5367", "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Combining labeled and unlabeled data with word-class distribution learning", "author": ["Qi et al.2009] Yanjun Qi", "Ronan Collobert", "Pavel Kuksa", "Koray Kavukcuoglu", "Jason Weston"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge manage-", "citeRegEx": "Qi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2009}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Santos", "Victor Guimar\u00e3es"], "venue": "arXiv preprint arXiv:1505.05008", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proc. CoNLL", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Introduction to the conll-2002 shared task: Languageindependent named entity recognition", "author": [], "venue": "In Proc. CoNLL", "citeRegEx": "Sang.,? \\Q2002\\E", "shortCiteRegEx": "Sang.", "year": 2002}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proc. ACL", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "dencies, in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence (Bengio et al., 1994).", "startOffset": 111, "endOffset": 132}, {"referenceID": 24, "context": "Therefore, instead of modeling tagging decisions independently, we model them jointly using a conditional random field (Lafferty et al., 2001).", "startOffset": 119, "endOffset": 142}, {"referenceID": 26, "context": ", the Pi,y\u2019s) are defined to be the dot product between the embedding of a wordin-context computed with a bidirectional LSTM\u2014 exactly the same as the POS tagging model of Ling et al. (2015b) and these are combined with bigram compatibility scores (i.", "startOffset": 171, "endOffset": 191}, {"referenceID": 11, "context": "Ratinov and Roth (2009) and Dai et al. (2015) showed that using a more expressive tagging scheme like IOBES improves model performance", "startOffset": 28, "endOffset": 46}, {"referenceID": 12, "context": "To obtain representations of this stack used for predicting subsequent actions, we use the Stack-LSTM presented by Dyer et al. (2015), in which the LSTM is augmented with a \u201cstack pointer.", "startOffset": 115, "endOffset": 134}, {"referenceID": 30, "context": "We designed a transition inventory which is given in Figure 2 that is inspired by transition-based parsers, in particular the arc-standard parser of Nivre (2004). In this algorithm, we make use of two stacks (designated output and stack representing, respectively, completed chunks and scratch space) and a buffer that contains the words that have yet to be processed.", "startOffset": 149, "endOffset": 162}, {"referenceID": 12, "context": "Following Dyer et al. (2015), we use stack LSTMs", "startOffset": 10, "endOffset": 29}, {"referenceID": 2, "context": ", 2015b) or dependency parsing (Ballesteros et al., 2015).", "startOffset": 31, "endOffset": 57}, {"referenceID": 18, "context": "To encourage the model to depend on both representations, we use dropout training (Hinton et al., 2012), applying a dropout mask to the final embedding layer just before the input to the bidirec-", "startOffset": 82, "endOffset": 103}, {"referenceID": 40, "context": "Several methods have been proposed to enhance the performance of SGD, such as Adadelta (Zeiler, 2012) or Adam (Kingma and Ba, 2014).", "startOffset": 87, "endOffset": 101}, {"referenceID": 16, "context": "(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)", "startOffset": 0, "endOffset": 85}, {"referenceID": 4, "context": "(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)", "startOffset": 0, "endOffset": 85}, {"referenceID": 5, "context": "(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)", "startOffset": 0, "endOffset": 85}, {"referenceID": 32, "context": "(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)", "startOffset": 0, "endOffset": 85}, {"referenceID": 20, "context": "2 by jointly modeling the NER and entity linking tasks (Hoffart et al., 2011).", "startOffset": 55, "endOffset": 77}, {"referenceID": 27, "context": "The best score reported on this task is by Luo et al. (2015). They obtained a F1 of 91.", "startOffset": 43, "endOffset": 61}, {"referenceID": 20, "context": "2 by jointly modeling the NER and entity linking tasks (Hoffart et al., 2011). Their model uses a lot of hand-engineered features including spelling features, WordNet clusters, Brown clusters, POS tags, chunks tags, as well as stemming and external knowledge bases like Freebase and Wikipedia. Our LSTM-CRF model outperforms all other systems, including the ones using external labeled data like gazetteers. Our StackLSTM model also outperforms all previous models that do not incorporate external features, apart from the one presented by Chiu and Nichols (2015).", "startOffset": 56, "endOffset": 564}, {"referenceID": 15, "context": "The only exception is Dutch, where the model of Gillick et al. (2015) can perform better by leveraging the information from other NER datasets.", "startOffset": 48, "endOffset": 70}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.", "startOffset": 3, "endOffset": 23}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.10 Passos et al. (2014) 90.", "startOffset": 3, "endOffset": 51}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.10 Passos et al. (2014) 90.05 Passos et al. (2014)* 90.", "startOffset": 3, "endOffset": 78}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.10 Passos et al. (2014) 90.05 Passos et al. (2014)* 90.90 Luo et al. (2015)* + gaz 89.", "startOffset": 3, "endOffset": 103}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.10 Passos et al. (2014) 90.05 Passos et al. (2014)* 90.90 Luo et al. (2015)* + gaz 89.9 Luo et al. (2015)* + gaz + linking 91.", "startOffset": 3, "endOffset": 133}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.10 Passos et al. (2014) 90.05 Passos et al. (2014)* 90.90 Luo et al. (2015)* + gaz 89.9 Luo et al. (2015)* + gaz + linking 91.2 Chiu and Nichols (2015) 90.", "startOffset": 3, "endOffset": 179}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.10 Passos et al. (2014) 90.05 Passos et al. (2014)* 90.90 Luo et al. (2015)* + gaz 89.9 Luo et al. (2015)* + gaz + linking 91.2 Chiu and Nichols (2015) 90.69 Chiu and Nichols (2015)* 90.", "startOffset": 3, "endOffset": 209}, {"referenceID": 14, "context": "Model F1 Florian et al. (2003)* 72.", "startOffset": 9, "endOffset": 31}, {"referenceID": 14, "context": "Model F1 Florian et al. (2003)* 72.41 Ando and Zhang (2005a) 75.", "startOffset": 9, "endOffset": 61}, {"referenceID": 14, "context": "Model F1 Florian et al. (2003)* 72.41 Ando and Zhang (2005a) 75.27 Qi et al. (2009) 75.", "startOffset": 9, "endOffset": 84}, {"referenceID": 14, "context": "Model F1 Florian et al. (2003)* 72.41 Ando and Zhang (2005a) 75.27 Qi et al. (2009) 75.72 Gillick et al. (2015) 72.", "startOffset": 9, "endOffset": 112}, {"referenceID": 14, "context": "Model F1 Florian et al. (2003)* 72.41 Ando and Zhang (2005a) 75.27 Qi et al. (2009) 75.72 Gillick et al. (2015) 72.08 Gillick et al. (2015)* 76.", "startOffset": 9, "endOffset": 140}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002) 77.", "startOffset": 9, "endOffset": 32}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002) 77.05 Nothman et al. (2013) 78.", "startOffset": 9, "endOffset": 60}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002) 77.05 Nothman et al. (2013) 78.6 Gillick et al. (2015) 78.", "startOffset": 9, "endOffset": 87}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002) 77.05 Nothman et al. (2013) 78.6 Gillick et al. (2015) 78.08 Gillick et al. (2015)* 82.", "startOffset": 9, "endOffset": 115}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002)* 81.", "startOffset": 9, "endOffset": 32}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002)* 81.39 Santos and Guimar\u00e3es (2015) 82.", "startOffset": 9, "endOffset": 67}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002)* 81.39 Santos and Guimar\u00e3es (2015) 82.21 Gillick et al. (2015) 81.", "startOffset": 9, "endOffset": 95}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002)* 81.39 Santos and Guimar\u00e3es (2015) 82.21 Gillick et al. (2015) 81.83 Gillick et al. (2015)* 82.", "startOffset": 9, "endOffset": 123}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees.", "startOffset": 31, "endOffset": 54}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers.", "startOffset": 31, "endOffset": 230}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus.", "startOffset": 31, "endOffset": 334}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top.", "startOffset": 31, "endOffset": 553}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features.", "startOffset": 31, "endOffset": 793}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features.", "startOffset": 31, "endOffset": 896}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features. Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers.", "startOffset": 31, "endOffset": 1051}, {"referenceID": 13, "context": "Eisenstein et al. (2011) use Bayesian nonparametrics to construct a database of named entities in an almost unsupervised setting.", "startOffset": 0, "endOffset": 25}, {"referenceID": 13, "context": "Eisenstein et al. (2011) use Bayesian nonparametrics to construct a database of named entities in an almost unsupervised setting. Ratinov and Roth (2009) quantitatively compare several approaches for NER and", "startOffset": 0, "endOffset": 154}], "year": 2016, "abstractText": "State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures\u2014one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers. 1", "creator": "LaTeX with hyperref package"}}}