{"id": "1603.01431", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks", "abstract": "While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- \\textit{Internal Covariate Shift}-- the current solution has multiple drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (specially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size $ 1 $ during training. We address these (and other) drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call \\textit{Normalization Propagation}. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer; thus being computationally cheaper. We exploit the observation that the pre-activation before Rectified Linear Units follow a Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics (using data) for any of the hidden layers.", "histories": [["v1", "Fri, 4 Mar 2016 12:01:58 GMT  (299kb,D)", "http://arxiv.org/abs/1603.01431v1", null], ["v2", "Wed, 9 Mar 2016 16:41:25 GMT  (302kb,D)", "http://arxiv.org/abs/1603.01431v2", null], ["v3", "Mon, 23 May 2016 23:01:55 GMT  (1073kb,D)", "http://arxiv.org/abs/1603.01431v3", "11 pages in ICML 2016"], ["v4", "Mon, 30 May 2016 02:08:06 GMT  (1068kb,D)", "http://arxiv.org/abs/1603.01431v4", "11 pages, ICML 2016"], ["v5", "Sun, 3 Jul 2016 20:17:44 GMT  (906kb,D)", "http://arxiv.org/abs/1603.01431v5", "11 pages, ICML 2016"], ["v6", "Tue, 12 Jul 2016 13:57:19 GMT  (923kb,D)", "http://arxiv.org/abs/1603.01431v6", "11 pages, ICML 2016, appendix added to the last version"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["devansh arpit", "yingbo zhou", "bhargava urala kota", "venu govindaraju"], "accepted": true, "id": "1603.01431"}, "pdf": {"name": "1603.01431.pdf", "metadata": {"source": "META", "title": "Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks ", "authors": ["Devansh Arpit", "Yingbo Zhou", "Bhargava U. Kota", "Venu Govindaraju {DEVANSHA"], "emails": ["GOVIND}@DEVANSHA.EDU"], "sections": [{"heading": "1. Introduction and Motivation", "text": "It is about the problem of shifting the distribution of each hidden layer in a deep neural network. This layer is borrowed from the concept of covariate shift (Shimodaira, 2000), where this problem is addressed by a single input-output learning system. Consider the last layer of a deep network used for classification, which essentially tries to learn P (Y), where the class label is more random (r.v.) and where the layer input r.v However, learning a fixed P (Y) becomes a problem when the P (X) is constantly changing."}, {"heading": "2. Background", "text": "It has long been known in the deep learning community that input whitening and decorrelation help speed up the training process (LeCun et al., 2012). In fact, it is explicitly mentioned in (LeCun et al., 2012) that this whitening should be done before each shift, so that the input to the next layer has zero mean. In the perspective of the internal covariate shift, what is required for the network to learn a fixed hypothesis P (Y | X) at each given layer during training is to fix the distribution P (X) of the input to that layer. While whitening could be used to accomplish this task on each layer, it would be a very costly choice (cubic order of input) size, since whitening dictates the calculation of the Singular Value Decomposition (SVD) of the input data matrix. Desjardins et al (2015) suggest that this problem can be overcome by approximation."}, {"heading": "3.1. Data Normalization", "text": "Therefore, we look at a data distribution X in Rn in such a way that all samples are normalized, i.e., Ex-X [x] = 0 Ex-X [x2j] = 1-j-j-1,..., n-2) Then our goal is to find a way to transfer this normalization to all hidden layers without requiring explicit data-dependent normalization. Depending on whether this input is passed through a revolutionary layer or a fully connected layer, part of the input or the entire input is multiplied to a weight matrix. Regardless of the case, we can use x to mark this input for the sake of notation; which can thus be the whole data vector or a subset of its element, depending on the case. Pre-activation is done by u, Wx, where W-Rm-n and m are the number of filters (we will ignore the distortion for the time being)."}, {"heading": "3.2. Mean and Standard-deviation Normalization for First Hidden Layer", "text": "It is clear that the input data x = 1, i = 1, i = 1, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 1, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = i, i = 2, i = 2, i = i, i = i = 2, i = i = i, i = i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 1, i = 1, i = 1, i = 1, i = 1, i = 1, i = 1, i = 1, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 1, i = 1, i = 1, i = 1, i = 1, i = 1, i = 1, i = 1, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2, i = 2 = 2, i = 2, i = 2 = 2, i = 2, i = 2 = 2, i = 2, i = 2 = 2, i = 2, i = 2 = 2 = 2, i = 2, i = 2 = 2, i = 2 = 2, i = 2, i = 2 = 2 = 2, i = 2, i = 2, i = 2 = 2, i = 2, i = 2 = 2, i = 2, i = 2, i = 2, i = 2, i = 2,"}, {"heading": "3.3. Propagation to Higher Layers", "text": "With the above two operations, the dynamics of the second hidden layer become identical to the dynamics of the first hidden layer. By inducing, repeating these two operations for each layer, namely -1) we divide the pre-ReLU activation of each hidden layer by the corresponding \"Wi-2\" layer, where W is the weight matrix of the corresponding layer, 2) we subtract and divide from the post-ReLU activation of each hidden layer - we ensure that the input to each layer is a canonical distribution. During the training, all these normalization operations are backpropagated."}, {"heading": "3.4. Effect of NormProp on Jacobian of Hidden Layers", "text": "It was in Saxe et al. (2013); Ioffe & Szegedy (2015); Ioffe & Szegedy (2015); Ioffe & Szegedy (2015); Ioffe & Szegedy (2015); Ioffe & Szegedy (2015); Ioffe & Szegedy (2015); Ioffe & Szegedy (2015); Ioffe & Szegedy (2015); (2015); Ioffe & Szegedy (2015); (2015); Ioffe & Szegedy (2015); (2015); (2014); (2014); (2014); (2014); (2014); (2014); (2014); 2014; 2014; 2014; (2014); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2015); (2014); (2014); (2014); (2014); 2014); 2014); 2014 (2014); 2014 (2014); (2014); (2014); (2015); (2015); (2015); (2015); 2015); (2015); (2015); (2015); (2015); (2015); (2015); 2015); 2015); (); 2015); (); 2015); (); 2015); 2015); (); 2015); (); 2015); (); 2015); 2015); (); 2015); 2015); (); 2015); (); 2015); (); 2015); 2015); 2015); (); 2015); (); 2015); 2015); (); 2014); 2014); 2014 (2014); 2014 (2014); 2014 (2014); 2014); 2014); 2014); 2014 (2014); 2014); 2014 (2014); 2014); 2014); 2014"}, {"heading": "4. NormProp: Implementation Details", "text": "We have all the necessary ingredients to filter out the steps to normalize propagation, to train any deep neural network with ReLU activation, even though these are hidden layers. Like BN, NormProp can be used alongside any optimization algorithm (e.g. Stochastic Gradient Descent with / without Momentum) to train deep networks."}, {"heading": "4.1. Normalize Data", "text": "Since the basic idea of NormProp is to propagate data normalization through hidden layers, we offer two alternative options, one of which can be used to normalize input to a NormProp network. As we will describe, both options are justified in their respective scenarios. 1. Global Data Normalization: In cases where the entire dataset - which roughly represents the true data distribution - is available, we calculate the global mean and the standard deviation for each attribute element. Then, the first step for NormProp is to subtract across the entire dataset from each sample. Likewise, we divide each attribute element by the elementary standard deviation. Ideally, NormProp requires that all input dimensions be statistically uncorrelated, a property achieved by softening, for example, we propose elementary normalization as approximation, as it is mathematically cheap."}, {"heading": "4.2. Initialize Network Parameters", "text": "We use Normalized Initialization (Glorot & Bengio, 2010) to determine the initial values of all weight matrices that are both fully connected and wavy."}, {"heading": "4.3. Propagate Normalization", "text": "Similar to BN, we also use gradient-based learnable scaling and bias parameters \u03b3 and \u03b2 in the implementation. We will now describe in detail our normalization for both fully connected and wavy layers."}, {"heading": "4.3.1. FULLY CONNECTED LAYERS", "text": "Consider each fully connected layer characterized by a weight matrix W-Rm-n, bias \u03b2-Rm, scaling \u03b3-Rm, input x-Rn, and activation ReLU. Here m stands for the number of filters and n for the input dimension. Then, the ith output unit oi of the hidden layer would be traditional: oi = ReLU (WTi x + \u03b2i) (8) In the case of NormProp, the output unit oi is now oi, oi = 1 \u221a 1 2 (1 \u2212 1\u03c0) [ReLU (\u03b3i (W T i x) 1.21 \u0445 Wi-2 + \u03b2i) \u2212 \u221a 12\u03c0] (9). Here, we divide the pre-activation by 1.21 to bring the jakobin close to one, as suggested by our analysis in Section 3.4. Therefore, we call this number the jakobin factor. We found that dividing the Pre-activation by learning factor significantly helps us train off with higher objacin the learning factor."}, {"heading": "4.3.2. CONVOLUTIONAL LAYERS", "text": "Let us consider any shaft position marked by a filter matrix W-Rm-d-h-w, preload \u03b2-Rm, scaling \u03b3-Rm, input x-Rd-L-B and activation ReLU, as well as any selection of the step size. In this case, m denotes the number of filters, d-depth, h-height, w-width for input / filter and L, B-height and width of the image. Without NormProp, the ith output function board oi of the hidden layer using the ith filter Wi-Rd-h-w is traditionally: oi = ReLU (Wi-x + \u03b2i) (10), in which \u0435 denotes the folding process. Now, in the case of NormProp, the output function board oi = 1: 1: 1 2 (1 \u2212 1\u03c0) [ReLU (Wi-x) 1,21: Wi-F + \u03b2i) \u2212 1: 2: 1] (11), the output function board oi = 1: 1: 1 is executed in the same way as in each element."}, {"heading": "4.4. Training", "text": "Optimization: We use Stochastic Gradient Descent with Momentum (set to 0.9) for training. Data redeployment also leads to performance improvements (but this generally applies when training deeper networks).Learning Rate: We have found that learning accelerates by halving the learning rate when the training error begins to saturate. In addition, we have found a higher initial learning rate for larger lots that improves performance.Limitations: After each training session, we scale each hidden weight vector / filter card to have unit 2 length, i.e., we use the restriction \"2 on all hidden weights, both confessional and fully connected. This is because the scale of weight vectors does not affect the network representation, so restricting the weights reduces the parameter search space. Regulatory: We use weight loss along with the loss function; we have found a small value of weight vectors that proves efficient during training."}, {"heading": "4.5. Validation and Testing", "text": "Validation and testing methods are identical in our case as opposed to BN due to parametric normalization. During validation / testing, each sample is initially normalized by mean and standard deviation, which are calculated depending on how the train data is normalized during training. On the other hand, if we use Global Data Normalization during training, we simply use the same global estimate of mean and standard deviation to normalize each test / validation sample. On the other hand, if batch data normalization is used during training, an ongoing estimate of mean and standard deviation is maintained during training, which is then used to normalize each test / validation sample. Finally, input through the network of learned parameters is propagated using the same strategy described in Section 4.3."}, {"heading": "4.6. Extension to other Activation Functions", "text": "Although our work shows how to overcome the problem of internal covariate displacement specifically for networks with continuous ReLU activation, we have essentially proposed a general framework for spreading normalization to all hidden levels. All that is required to extend NormProp to other activation functions is the calculation of the distribution mean and the standard deviation of the output after the activation function of the choice, similar to what is shown in Note 1. This activation can be both parameter-based and fixed. For example, a parameter-based activation is parametric ReLU (PReLU, He et al. (2015) (with the parameter a) given by, PReLUa (x) = {x ifx > 0 ax ifx \u2264 0 (12) Then, the post-PReLU distribution statistics are deviated by, remark 2. Letter X \u0445 N (0, 1) and PempUa (X) (1) (1)."}, {"heading": "5. Empirical Results and Observations", "text": "We want to check the following numbers: a) Performance comparison of NormProp using Global Data Normalization vs. Batch Data Normalization; b) NormProp mitigates the problem of internal Covariate Shift more accurately compared to BN; c) Convergence stability of NormProp is better than BN; d) Effect of batch size on the behavior of NormProp, especially on batch size 1 (BN not applicable). Finally, we report on the classification of different datasets using NormProp and BN.Datasets: We use the following datasets, 1) CIFAR-10 (Krizhevsky, 2009) - It consists of 60, 000 color images in orbit and 10, BN.D datasets."}, {"heading": "5.1. Global vs. Batch Data Normalization", "text": "In this context, it should be noted that this is a very complex and complex matter."}, {"heading": "5.3. Convergence Stability of NormProp vs. BN", "text": "Since NormProp reduces internal covariate shift more accurately than BN, NormProp is expected to achieve more stable convergence. We confirm this intuition by recording validation accuracy during network training. We use batch size 50 and initial learning rates of 0.05. The graph is Figure 34. NormProp achieves more stable convergence in general, but especially during initial training, due to a more stable input distribution (especially for lower hidden layers) that NormProp achieves, as well as a more accurate estimate of the resulting statistics on shift entry distribution during initial training."}, {"heading": "5.4. Effect of Batch-size on NormProp", "text": "Since it is also possible to train with batch size 1 (using global data normalization at the data level), we4We observed identical trends in SVHN and CIFAR-100. Compare the validation performance of NormProp during training for different batch sizes, including 1. The diagrams are in Figure 4. NormProp performance is largely unaffected by batch size, although lower batch sizes seem to achieve better performance."}, {"heading": "5.5. Results on various Datasets", "text": "We evaluate NormProp and BN on the basis of CIFAR-10, CIFAR-100 and SVHN data sets, but we also report on existing SOTA results. For all data sets and both methods, we use the same architecture as in the above-mentioned experimental protocol except for CIFAR-100, the last revolutionary layer is C (100, 1, 1, 0) instead of C (10, 1, 1, 0). For CIFAR data sets, we use batch size 50 and an initial learning rate of 0.05 and halve it after 25 epochs and train for 200 epochs. Since SVHN is a much larger data set, we train only for 25 epochs with batch size 100 and an initial learning rate of 0.08 and reduce it by half after all 5 epochs. We use stochastic gradient deviation with dynamics (0.9). For CIFAR-10 and CIFAR-100, we train with no standardization WAR results and no SVHN results."}, {"heading": "6. Conclusion", "text": "We have proposed a novel algorithm for solving the problem of internal covariate displacement, which is useful in training deep neural networks and overcomes several disadvantages of batch normalization (BN). Specifically, we propose a parametric approach (NormProp) that avoids estimating the middle and standard deviation of the input distribution of hidden layers based on input data minibatch statistics (which involve shifting of network parameters), which renders them inaccurate (especially during the introduction phase, when parameters change drastically). Instead, NormProp relies on normalizing the statistics of the given data set and conditioning the weight matrix, which ensures that the normalization of the data set is transferred to all hidden layers. Therefore, NormProp does not need to leave an average estimate of the batch statistics of hidden layer inputs for the test phase and maintain the standard computation in the test phase, as we have shown in the school model 1, even though this allows the use of the reactor size for other networks."}, {"heading": "A. Proofs", "text": "Proposition 1 = Wx, where x-Rn and W-Rm-n lie so far apart that Ex [x] = 0 and Ex [xxT] = \u03c32I (I is the identity matrix). Then the covariance matrix of u = 1 is approximately canonically satisfactory, min-a = 2 (D) -diag (F \u2264 2) -D (D) -D (D) -D (D) -D (D) -D (D) -D (D) -D (D) -D (D) -D (D) -D (D) -D) -D (D) -D (D) -D (D) -D) -D (D) -D (D) -D (D) -D (D) -D (D) -D) -D-D) -D (D) -D-D-D) -D-D-D-D-D-D-D-D-D-D D-D-D. D) D -D (D) D-D-D-D-D-D-D-D -D-D-D-D -D-D-D-D-D-D-D-D (D) -D) -D-D-D-D-D-D-D-D-D-D-D (D) -D-D-D) -D-D-D-D-D-D-D-D-D-D-D (D)."}], "references": [{"title": "Learning activation functions to improve deep neural networks", "author": ["Agostinelli", "Forest", "Hoffman", "Matthew", "Sadowski", "Peter", "Baldi", "Pierre"], "venue": "In ICLR,", "citeRegEx": "Agostinelli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agostinelli et al\\.", "year": 2015}, {"title": "Natural neural networks", "author": ["Desjardins", "Guillaume", "Simonyan", "Karen", "Pascanu", "Razvan"], "venue": "In Advances in Neural Information Processing Systems, pp. 2062\u20132070,", "citeRegEx": "Desjardins et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Independent component analysis: algorithms and applications", "author": ["Hyv\u00e4rinen", "Aapo", "Oja", "Erkki"], "venue": "Neural networks,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2000}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "ICML, volume 37 of JMLR Proceedings,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Krizhevsky", "Alex"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Efficient backprop", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["Shimodaira", "Hidetoshi"], "venue": "Journal of statistical planning and inference,", "citeRegEx": "Shimodaira and Hidetoshi.,? \\Q2000\\E", "shortCiteRegEx": "Shimodaira and Hidetoshi.", "year": 2000}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Sparse representation for computer vision and pattern recognition", "author": ["Wright", "John", "Ma", "Yi", "Mairal", "Julien", "Sapiro", "Guillermo", "Huang", "Thomas S", "Yan", "Shuicheng"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Wright et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wright et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": "It has long been known in Deep Learning community that input whitening and decorrelation helps in speeding up the training process (LeCun et al., 2012).", "startOffset": 131, "endOffset": 151}, {"referenceID": 7, "context": "In fact, it is explicitly mentioned in (LeCun et al., 2012) that this whitening should be performed before every layer so that the input to the next layer has zero mean.", "startOffset": 39, "endOffset": 59}, {"referenceID": 1, "context": "However, Desjardins et al. (2015) suggest to overcome this problem by approximating this SVD by: a) using training mini-batch data to compute this SVD; b) computing it every few number of iteration and relying on the assumption that this SVD approximately holds for the iterations in between.", "startOffset": 9, "endOffset": 34}, {"referenceID": 1, "context": "However, Desjardins et al. (2015) suggest to overcome this problem by approximating this SVD by: a) using training mini-batch data to compute this SVD; b) computing it every few number of iteration and relying on the assumption that this SVD approximately holds for the iterations in between. In addition, each hidden layer\u2019s input is then whitened by re-parametrizing a subset of network parameters that are involved in gradient descent. As mentioned in Ioffe & Szegedy (2015), this re-parametrizing may lead to effectively cancelling/attenuating the effect of the gradient update step since these two operations are done independently.", "startOffset": 9, "endOffset": 478}, {"referenceID": 12, "context": "On the other hand, it is generally observed that useful filters that constitute a good representation of real world data are roughly incoherent (Wright et al., 2010; Makhzani & Frey, 2013); thus ensuring the second term is also small thereby minimizing the overall error bound.", "startOffset": 144, "endOffset": 188}, {"referenceID": 9, "context": "It has been discussed in Saxe et al. (2013); Ioffe & Szegedy (2015) that Jacobian of hidden layers with singular values close to one improves training speed in deep networks.", "startOffset": 25, "endOffset": 44}, {"referenceID": 9, "context": "It has been discussed in Saxe et al. (2013); Ioffe & Szegedy (2015) that Jacobian of hidden layers with singular values close to one improves training speed in deep networks.", "startOffset": 25, "endOffset": 68}, {"referenceID": 9, "context": "21 which, being close to 1, approximately achieves dynamical isometry (Saxe et al., 2013) and should thus prevent the problem of exploding or diminishing gradients while training deep networks suggesting faster convergence.", "startOffset": 70, "endOffset": 89}, {"referenceID": 8, "context": "3) SVHN (Netzer et al., 2011)\u2013 It consists of 32\u00d7 32 color images of house numbers collected by Google Street View.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "41 NIN + ALP units (Agostinelli et al., 2015) 9.", "startOffset": 19, "endOffset": 45}, {"referenceID": 0, "context": "25 NIN + ALP units (Agostinelli et al., 2015) 7.", "startOffset": 19, "endOffset": 45}, {"referenceID": 0, "context": "32 NIN + ALP units (Agostinelli et al., 2015) 34.", "startOffset": 19, "endOffset": 45}, {"referenceID": 0, "context": "26 NIN + ALP units (Agostinelli et al., 2015) 30.", "startOffset": 19, "endOffset": 45}, {"referenceID": 0, "context": "25 NIN + ALP units (Agostinelli et al., 2015) NIN (Lin et al.", "startOffset": 19, "endOffset": 45}], "year": 2017, "abstractText": "While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks\u2013 Internal Covariate Shift\u2013 the current solution has multiple drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (specially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size 1 during training. We address these (and other) drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call Normalization Propagation. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer We exploit the observation that the pre-activation before Rectified Linear Units follow a Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics (using data) for any of the hidden layers.", "creator": "LaTeX with hyperref package"}}}