{"id": "1603.01547", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "Text Understanding with the Attention Sum Reader Network", "abstract": "Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Our model outperforms models previously proposed for these tasks by a large margin.", "histories": [["v1", "Fri, 4 Mar 2016 17:32:42 GMT  (366kb,D)", "http://arxiv.org/abs/1603.01547v1", null], ["v2", "Fri, 24 Jun 2016 13:04:47 GMT  (825kb,D)", "http://arxiv.org/abs/1603.01547v2", "Presented at ACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rudolf kadlec", "martin schmid", "ondrej bajgar", "jan kleindienst"], "accepted": true, "id": "1603.01547"}, "pdf": {"name": "1603.01547.pdf", "metadata": {"source": "CRF", "title": "Text Understanding with the Attention Sum Reader Network", "authors": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "emails": ["kadlec@cz.ibm.com", "martin.schmid@cz.ibm.com", "obajgar@cz.ibm.com", "jankle@cz.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Most of the information mankind has gathered up to this point is stored in the form of plaintext, so teaching machines how to understand this data is of paramount importance in the field of artificial intelligence. One way to test the level of text comprehension is simply to ask the system questions for which the answer can be derived from the text. A well-known example of a system that could use a huge collection of unstructured documents to answer questions is, for example, IBM's Watson system, which is used for the Jeopardy challenge (Ferrucci et al., 2010). Cloze-style questions (Taylor, 1953), i.e. questions that arise by removing a phrase from a sentence, are an appealing form of such questions. While the task is easy to evaluate, the context, the question set, or the specific phrase that is missing in the question can be varied to dramatically change the structure of the task and the difficulty of the task."}, {"heading": "2 Task and datasets", "text": "In this section, we briefly present the task we want to solve and relevant large datasets recently introduced for this task."}, {"heading": "2.1 Formal Task Description", "text": "The task is to answer a Cloze-style question, the answer of which depends on understanding a context document provided with the question. It is also provided with a series of possible answers from which the correct one is to be selected, formalised as follows: The training data consists of tuples (q, d, a, A), where q is a question, d is a document containing the answer to question q, A is a set of possible answers and A is the answer to the basic truth. Both q and d are word sequences from the vocabulary V. We also assume that all possible answers are words from the vocabulary, i.e. A V, and that the basic truth answer a appears in the document, i.e. a sequence of words from the vocabulary V. We also assume that all possible answers are words from the vocabulary, i.e. A V, and that the basic truth answer a appears in the document, i.e. an abbreviation: 160 3.01 547v 1 [cs.C L] 4M 016"}, {"heading": "2.2 Datasets", "text": "We will now briefly summarize important characteristics of the data sets."}, {"heading": "2.2.1 News Articles \u2014 CNN and Daily Mail", "text": "The first two sets of data (Hermann et al., 2015) were constructed from a large number of news articles on the websites of CNN and Daily Mail. The main part of each article provides context, while the cloze-style question consists of a short emphasis sentence appearing at the top of each page of the article. Specifically, the question is created by replacing a designated entity from the summary sentence (e.g. \"Producer X will not bring charges against Jeremy Clarkson,\" says his lawyer), and the designated entities throughout the data set have been replaced by anonymous tokens, which have been further shuffled for each example, so that the model cannot build global knowledge of the entities and therefore must really rely on the context document to find an answer to the question."}, {"heading": "2.2.2 Children\u2019s Book Test", "text": "The third set of data, the Children's Book Test (CBT) (Hill et al., 2015), is based on books that are freely available thanks to the Gutenberg 3 project. Each contextual document consists of 20 consecutive sentences from a children's book story. Due to the lack of summaries, the question of the cloze style is then constructed from the following (21st) sentence. You can also see how task complexity varies with the type of word omitted (named entity, common noun, verb, preposition). (Hill et al., 2015) have shown that standard LSTM language models show human performance in predicting verbs and prepositions, but lag behind named entities and common nouns. In this article, therefore, we focus only on predicting the first two word types. 1The CNN and Daily Mail data sets are available at https: / giubth.com / deepmind / Cdata2dataBT."}, {"heading": "3 Our Model \u2014 Attention Sum Reader", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Motivation", "text": "Our model, called Attention Sum Reader (AS Reader), is tailor-made to take advantage of the fact that the answer is a word from the context document. It is a double-edged sword. Although it achieves current results for all the data sets mentioned (if this assumption is correct), it cannot produce an answer that is not contained in the document. Intuitively, our model is structured as follows: 1. We calculate a vector embedding of the query; 2. We calculate a vector embedding of each word in the context of the entire document (contextual embedding); 3. Using a point product between the embedding of the question and the contextual embedding of each occurrence of a candidate answer in the document, we select the most likely answer."}, {"heading": "3.2 Formal Description", "text": "Our model uses one word embed function and two encoder functions. The word embed function q translates words into vector representations. The first encoder function is a document encoder f, which encodes each word from document d in the context of the entire document. We call this the contextual embedding. For convenience, we call the contextual embedding of the i-word in d as fi (d). The second encoder g is used to translate the query q into a fixed length representation of the same dimensionality as fi (d). Both encoders use word embedding, which is calculated by e as input. Then, we calculate a weight for each word in the document as a dot product of its contextual embedding and embedding of the query. This weight could be considered as attention over the document. To form an orderly probability distribution over the words in the document, we normalize the weights of the way we model the maximum attention to the use of this function."}, {"heading": "3.3 Model instance details", "text": "In our model, the document encoder f is implemented as a bidirectional gated recurrent unit (GRU) network (Cho et al., 2014), whose hidden states form the contextual word embeddings, i.e. fi (d) = \u2212 \u2192 fi (d) | | \u2190 \u2212 fi (d), where | vector concatenation and \u2212 \u2192 fi and \u2190 \u2212 fi mean contextual embeddings from the respective recursive networks. The query encoder g is implemented by another bidirectional GRU network. This time, the last hidden state of the forward network is linked to the last hidden state of the backward network to form the query embeddings, i.e. g (q) = \u2212 \u2192 g | (q) | | \u2190 \u2212 g1 (q) The word embedside function e is usually implemented as a reference table V. V, the lines of which can be contained by the vocabulary from (the vocabulary)."}, {"heading": "4 Related Work", "text": "Two recent Deep Neural Network architectures (Hermann et al., 2015; Hill et al., 2015) have been applied to the task of understanding text. Both architectures use an attention mechanism that allows them to highlight passages in the document that may be relevant to answering the question. We will now briefly describe these architectures and compare them with our approach."}, {"heading": "4.1 Attentive and Impatient Readers", "text": "The simpler impatient reader calculates the attention on the document after reading every word of the query. Empirical evaluations, however, have shown that both models on the CNN and Daily Mail datasets are almost identical. The main difference between the attentive reader and our model is that the attentive reader uses the attention to calculate a fixed length representation r of document d, which corresponds to a weighted sum of contextual embedding of words in d, that is, the sum i sifi (d). A common query and embedding of documents is then a non-linear function of r and the query embedding of g (q). This common embedding is in the end a significant simplification compared to all the candidate's answers. A common query and embedding of documents is then a non-linear function of r and the query embedding of g (q)."}, {"heading": "4.2 Memory Networks", "text": "MemNNs (Weston et al., 2014) were applied to the task of understanding text in (Hill et al., 2015). The best functioning storage network model - the window memory - uses fixed-length windows (8) arranged around the candidate words as memory cells. Due to this limited context window, the model is unable to detect dependencies outside this window. 4In the text comprehension task defined in (Hermann et al., 2015), these designated units are anonymized. However, this problem can still occur with anonymous word embedding. Furthermore, the representation within such a window is calculated simply as the sum of the embedding of words in that window. In contrast, in our model, the representation of each word is calculated using a recurring network that not only allows to capture the context from the entire document, but also the embedding of the calculation within such window is calculated simply to support the self-embedding of words in that window."}, {"heading": "4.3 Pointer Networks", "text": "Our model was inspired by PtrNets (Vinyals et al., 2015) by using an attention mechanism to select the answer in context, rather than merging words from context into a response representation. While a Ptr network consists of an encoder and a decoder that uses attention to select the answer in each step, our model gives the answer in a single step. Furthermore, the pointer networks assume that no input appears more than once in the sequence, which is not the case in our settings."}, {"heading": "4.4 Summary", "text": "Our model combines the best features of the architectures mentioned above. We use recurring networks to \"read\" the document and query, as well as attention-seeking and impatient readers, and we use attention in a similar way to Ptr networks. We also use the summation of asset weights in a similar way to MemNNs (Hill et al., 2015)."}, {"heading": "5 Evaluation", "text": "In this section, we evaluate our model using CNN, Daily Mail, and CBT data sets. We show that, despite its simplicity, the model performs state-of-the-art in each of these data sets."}, {"heading": "5.1 Training Details", "text": "To train the model, we used stochastic gradient lineage with the ADAM update rule (Kingma and Ba, 2015) and learning rates of 0.001 or 0.0005. We used negative log probability as a training target. Initial weights in the word embedding matrix were randomly drawn from the interval [\u2212 0.1, 0.1], weights in the GRU networks were initialized by random orthogonal matrices (Saxe et al., 2014) and distortions were initialized to zero. Gradient truncation (Pascanu et al., 2012) was set to 10 and batch size to 32. During the training, we mixed all examples in each epoch. To speed up the training, we pre-selected 10 batches worth examples and sorted them by the length of the document."}, {"heading": "5.2 Evaluation Method", "text": "We evaluated the proposed model both as a single model and as an ensemble average. For individual models, we report on the results for the best model as well as the average accuracy for the best 20% of the models with the best validation data performance, as individual models vary considerably due to random weight initialization, even with identical hyperparameter values. The performance of individual models can therefore be difficult to reproduce. As far as ensembles are concerned, we used a simple averaging of the response probabilities predicted by ensemble members. The ensemble models were either selected as the top 70% of all models trained or according to the following algorithm: We started with the most powerful model according to validation performance. Then, at each step, we tried to add the most powerful model that had not been tried before."}, {"heading": "5.3 Results", "text": "The performance of our models on the CBT dataset is summarized in Table 5, Table 4 shows the results on the CNN and Daily Mail datasets. The tables also list the performance of other published models evaluated on these datasets. 5 Attempts to use the Restricted Optimization by Linear Approach (COBYLA) method (Powell, 1994) to optimize weights result in consistency with the validation data for which the optimization was performed. Table 8 then measures accuracy as a percentage of test cases where the basic truth was among the best answers proposed by the greedy ensemble model for k = 1, 2, 5, CNN. On the CNN datasets, our single model is matched with the best validation accuracy of NN."}, {"heading": "6 Analysis", "text": "To further analyze the characteristics of our model, we examined the dependence of accuracy on the length of the context document (Figure 2), the number of candidate responses (Figure 3) and the frequency of the correct response in context (Figure 4).On the CNN and Daily Mail datasets, accuracy decreases with increasing document length (Figure 2a).We suspect that this may be due to several factors. Firstly, long documents can make the task more complex. Secondly, such cases are quite rare in the training data (Figure 2b), which motivates the model to specialize in shorter contexts. Finally, the context length is correlated to the number of named units, i.e. the number of possible responses, which itself negatively correlates to the accuracy (see Figure 3).On the CBT dataset, this negative trend appears to disappear (Figure 2c).This supports the later two explanations, since the distribution of document lengths is somewhat more uniform (Figure 2d and the number of responses may be significant) in this model."}, {"heading": "6.1 Comparison to Weighted Average Blending", "text": "In Section 4.1, we hypothesized that the fact that the attentive reader uses attention to create a mixed representation potentially harms his performance. To verify this intuition, we implemented the cross-fading into our model and brought it closer to the attentive reader (Hermann et al., 2015). In this modified model, we calculate the attention weights si in the same way as in our original model (see Equation 1). However, we replace Equation 2 with the following equations: r = \u2211 i she (wi) (3) P (a \u2032 | q, d) \u0438 exp (r \u00b7 e (a \u2032) (4), where r is the mixed response that is embedded, and a \u00b2 A is a possible candidate response. This change in architecture actually leads to a significant reduction in accuracy across all four datasets."}, {"heading": "7 Conclusion", "text": "In this article, we introduced a new neural network architecture for understanding natural language text. Although our model is simpler than previously published models, it offers a new state-of-the-art accuracy for all the data sets evaluated."}], "references": [{"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Nicolas Bouchard", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bastien et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Holger Schwenk", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Nico Schlaefer", "author": ["David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya a. Kalyanpur", "Adam Lally", "J. William Murdock", "Eric Nyberg", "John Prager"], "venue": "and Chris Welty.", "citeRegEx": "Ferrucci et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Mustafa Suleyman", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay"], "venue": "and Phil Blunsom.", "citeRegEx": "Hermann et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sumit Chopra", "author": ["Felix Hill", "Antoine Bordes"], "venue": "and Jason Weston.", "citeRegEx": "Hill et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Tomas Mikolov", "author": ["Razvan Pascanu"], "venue": "and Yoshua Bengio.", "citeRegEx": "Pascanu et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "D", "author": ["J Michael"], "venue": "Powell,", "citeRegEx": "Powell1994", "shortCiteRegEx": null, "year": 1994}, {"title": "James L Mcclelland", "author": ["Andrew M Saxe"], "venue": "and Surya Ganguli.", "citeRegEx": "Saxe et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["Wilson L Taylor"], "venue": "Journalism and Mass Communication", "citeRegEx": "Taylor.,? \\Q1953\\E", "shortCiteRegEx": "Taylor.", "year": 1953}, {"title": "Jan Chorowski", "author": ["Bart van Merrienboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-farley"], "venue": "and Yoshua Bengio.", "citeRegEx": "van Merrienboer et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Meire Fortunato", "author": ["Oriol Vinyals"], "venue": "and Navdeep Jaitly.", "citeRegEx": "Vinyals et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sumit Chopra", "author": ["Jason Weston"], "venue": "and Antoine Bordes.", "citeRegEx": "Weston et al.2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "Several large cloze-style context-questionanswer datasets have been introduced recently: the CNN and Daily Mail news data and the Children\u2019s Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Our model outperforms models previously proposed for these tasks by a large margin.", "creator": "TeX"}}}