{"id": "1603.02010", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2016", "title": "Differentially Private Policy Evaluation", "abstract": "We present the first differentially private algorithms for reinforcement learning, which apply to the task of evaluating a fixed policy. We establish two approaches for achieving differential privacy, provide a theoretical analysis of the privacy and utility of the two algorithms, and show promising results on simple empirical examples.", "histories": [["v1", "Mon, 7 Mar 2016 11:23:57 GMT  (174kb,D)", "http://arxiv.org/abs/1603.02010v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["borja balle", "maziar gomrokchi", "doina precup"], "accepted": true, "id": "1603.02010"}, "pdf": {"name": "1603.02010.pdf", "metadata": {"source": "CRF", "title": "Differentially Private Policy Evaluation\u2217", "authors": ["Borja Balle", "Maziar Gomrokchi", "Doina Precup"], "emails": ["b.deballepigem@lancaster.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to surpass themselves, to surpass themselves and to surpass themselves, go to another world in which they go to another world in which they cannot evade themselves. In the other world, it is so that they are able to survive themselves. In the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, all of us, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, all of us, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third, in the third world, in the third world, all of us, in the third world, in the third world, in the third world, in the third world, in the third, in the third world, in the third world, in the third world, in the third, in the third world, in the third world, all of us, in the third, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, all of us, in the third, in the third world, in the third world, in the third world, in the third, in the third and in the third world, third world, in the third world, in the third world, all of the third world, all of us, in the third world, all of the world, in the world, in the"}, {"heading": "2 Background", "text": "In this section, we provide background information on different privacy and policy assessments from Monte Carlo estimates."}, {"heading": "2.1 Differential Privacy", "text": "The central goal is to limit the loss of privacy that a user may suffer when the result of an analysis is made public on a database with their data. This can encourage users to participate in studies that use sensitive data, such as the evaluation of medical records. In the context of machine learning, differentiated private algorithms are useful because they enable learning models in a way that their parameters do not reveal information about the training data [McSherry and Talwar, 2007]. For example, one can think about using historical medical records to learn prognostic and diagnostic models that can be shared between multiple health care providers without affecting the privacy of the patients whose data was used to train the above discussion. To formalize X, we leave an input room and an output room."}, {"heading": "2.2 Policy Evaluation", "text": "Political evaluation is the problem of obtaining (an approximation to) the value function of a Markov reward process defined by an MDP M and a policy \u03c0 (Sutton and Barto, 1998, Szepesva \"ri, 2010). In many cases of interest M is unknown, but we have access to the pathways that involve state transitions and immediate rewards. If the state space of M is relatively small, the tabular methods that represent the value of each state can be used individually. In this paper, we focus on the political evaluations with linear functional approximation in the batch case, where we have access to a number of trajectories that are typically needed to defeat the curse of finite dimensionality and exploit the fact that similar states have similar values."}, {"heading": "3 Private First-Visit Monte Carlo Algorithms", "text": "In this section, we provide the details of two differentiated private valuation algorithms for policies based on Monte Carlo estimates. Each of these algorithms corresponds to a different stable version of the minimization argmin\u03b8 JX (\u03b8) described in the previous section. A formal privacy analysis of these algorithms is presented in Section 4. The limits showing how privacy requirements affect the usefulness of the estimates are outlined in Section 5."}, {"heading": "3.1 Algorithm DP-LSW", "text": "One way to design the optimization argmin\u03b8 JX (\u03b8) more stable is to consider a similar optimization of the minimum square rate, in which the optimization weights do not change with X, and to guarantee that the optimization problem is always strongly convex. Therefore, we consider a new objective function that is given with respect to a new set of positive regression weights. (Let us consider this as a diagonal matrix, in which the optimization problems with X (s, s) = DP (s, s, s) = S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S,"}, {"heading": "3.2 Algorithm DP-LSL", "text": "The second DP algorithm for policy assessment that we propose is also an output disturbance mechanism. It differs from DP-LSW in the way it promotes the stability of undisturbed solutions. In this case, we opt for the optimization of a regulated version of JX (\u03b8). In particular, we consider the objective function J\u03bbX (\u03b8), which is achieved by adding a ridge penalty to the loss of the smallest squares of (3). The introduction of the ridge penalty results in the objective function J\u03bbX (270) being strongly convex, thus ensuring the existence of a unique solution. X = argminually J \u03bb X (\u03b8 X), which can be achieved in a closed form."}, {"heading": "4 Privacy Analysis", "text": "This section provides a formal privacy analysis for DP-LSW and DP-LSL and shows that both algorithms (\u03b5, \u03b4) -differentiated are private. We rely on the smooth sensitivity framework of [Nissim et al., 2007, 2011], which provides tools for the design of DP mechanisms with data-dependent output disturbances. We rely on the following problem, which provides sufficient conditions for calibrating Gaussian output disturbance mechanisms with a variance proportional to smooth upper limits of local sensitivity. Let A be an algorithm that calculates a vector \u00b5X-Rd deterministically on Input X and then outputs ZX-N (\u00b5X, \u03c32XI), where \u03c32X is a variance given by X. Let there be an algorithm that calculates a vector \u00b5X-Rd deterministically on Input X and then outputs an X-X-N value."}, {"heading": "4.1 Privacy Analysis of DP-LSW", "text": "We start by setting an upper limit for the standard."}, {"heading": "4.2 Privacy Analysis of DP-LSL", "text": "Proving that DP-LSL is private in different ways follows the same strategy as DP-LSW. We start with a problem that affects the local sensitivity of DP-LSL for pairs of adjacent datasets X \"X.\" We use the notation Is-x for an indicator variable that is the same when visiting the state s within orbit x.Lemma 7. Let X \"X\" count as two adjacent datasets of m \"X\" as the vector given by Fx (s) = Fx, s (resp. Fx) and X \"= (x1,., xm \u2212 1, x\"). Let us leave the Fx-RS (resp. Fx) as the vector given by Fx (s)."}, {"heading": "5 Utility Analysis", "text": "Since the promise of differential privacy applies to all possible pairs of adjacent datasets that we do not know, the analysis in the previous section does not assume a generative model for the input dataset X. However, in practical applications, we expect X = (x1,.., xm) to contain multiple trajectories sampled by the same policy on the same MDP. The purpose of this section is to show that the amount of noise added by our algorithms decreases, leading to more accurate estimates of the value function. This is in line with the intuition that when we issue a fixed number of parameters, data from more users will be used to estimate these parameters, resulting in smaller individual contributions from each user, and making the use of privacy constraint easier to satisfactory.To measure the utility of our DP algorithms, we will measure the difference in empirical risks between learned and unlearned."}, {"heading": "6 Experiments", "text": "In this section we illustrate the behavior of the proposed algorithms using synthetic examples. The domain we use consists of a chain of N states in which the agent has a certain probability to stay and increase the probability (1). There is a reward of 1, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7"}, {"heading": "7 Conclusion", "text": "We present the first differentiated private policy assessment algorithms in the full MDP environment. Our algorithms are based on established Monte Carlo methodologies and have supply guarantees that show that the cost of privacy decreases as the training sessions grow. The smoothed sensitivity framework is a key component of our analyses, which differ from previous work on DP mechanisms for ERM and bandit issues in two respects. First, we do not consider optimizations with non-Lipschitz loss functions, which prevents us from applying most established privacy and benefit analysis techniques in ERM algorithms, and complicate some parts of our analysis. Specifically, we cannot use the close benefit analysis of [Jain and Thakurta, 2014] to achieve dimensional limitations. Second, and more importantly, the natural model of adjacent policy assessment data sets is that we can differentiate from whole sets of data that imply multiple sets of data."}, {"heading": "A Smoothed Gaussian Perturbation", "text": "For the sake of completeness, we offer here an elementary proof (albeit with slightly worse constants). In particular, we will prove the following deviation: Lemma 14. Let A be an algorithm that on input X computes a vector \u00b5X, and then output ZX, and then output ZX, and 2XI), where \"2X\" is a deviation, that of X. Let \"X,\" \"L,\" \"L,\" \"L,\" and \"X,\" and \"X,\" and \"X,\" and \"X\" is a deviation, that of X. Let \"A,\" and \"L,\" and. \""}, {"heading": "B Privacy Analysis of DP-LSW", "text": "Lemma 16: 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - 'X' - '-' X '-' - 'X' - '-' X '-' - 'X' - '-' - '-' X '-' - '-' - 'X' - '-' - '-' X '-' - '-' - 'X' - '-' - '-' - 'X' - '-' - '-' - 'X' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '-' - '- -' - '-' - '-' - '-' - '-' - '- -' - '-' - - '-' - '- - -' - '-' - - '-' - '-' - '-' - '-' - - - '-' - '-' - '-' - - '-' - - - '-' - '- - -' - '- -' - - '- - -' - '-' - '-' - '-' - - '-' - '- -' - - '-' - '-' - - '-' - '- -' - '-' - - '-' - '- -' - '- -' - '-' - - '-' - '-' - - '-' - - - '-' - '- -' - '-' - '- -' - '-' - '-' - '-' - '-' - '-'"}, {"heading": "C Privacy Analysis of DP-LSL", "text": "'That's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, that's me, I'm, that's me, that's me, I'm, I, I'm, I'm, I'm, I'm, I'm, I'm, I'm, I'm, I'm, I'm, I'm, that, I'm, that, that, I'm, I'm, that, that, I'm, that, that, I'm, that, that, that, I'm, I'm, I'm, I'm, I'm, I'm, I'm, that, I'm, that, that, that, I'm, I'm, I'm, that, that, I'm, that, I'm, that, I'm, that, I'm, that, I'm, that, I'm, that, I'm, that, I'm, I'm, that, that, I'm, I, I'm, that, that, I'm, that, that, I'm, I'm, I'm, that, that, that, that, that, that,"}, {"heading": "D Utility Analysis of DP-LSW", "text": "The aim of this section is to show that the size m of the dataset X grows = > the differentiated private solution \u03b8wX = > * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "E Utility Analysis of DP-LSL", "text": "The analysis in this section follows a scheme similar to the previous one. We begin by taking into account the expectation of excessive empirical risk in relation to the Gaussian disorder. (...) We assume that it is a random date with m-trajectories. (...) Let us point out that it is a random date with m-trajectories. (...) Let us point out that it is a random date with m-trajectories. (...) Let us point out that it is a random date with m-trajectories. (...) Let us point out that it is a random date with m-trajectories. (...)"}], "references": [{"title": "Privacy-preserving logistic regression", "author": ["Kamalika Chaudhuri", "Claire Monteleoni"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chaudhuri and Monteleoni.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri and Monteleoni.", "year": 2009}, {"title": "Differentially private empirical risk", "author": ["Kamalika Chaudhuri", "Claire Monteleoni", "Anand D Sarwate"], "venue": "minimization. volume 12. JMLR. org,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2011}, {"title": "Differential privacy. In Proceedings of the 33rd international conference on Automata, Languages and Programming-Volume Part II, pages", "author": ["Cynthia Dwork"], "venue": null, "citeRegEx": "Dwork.,? \\Q2006\\E", "shortCiteRegEx": "Dwork.", "year": 2006}, {"title": "The algorithmic foundations of differential privacy", "author": ["Cynthia Dwork", "Aaron Roth"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Dwork and Roth.,? \\Q2014\\E", "shortCiteRegEx": "Dwork and Roth.", "year": 2014}, {"title": "Differentially private learning with kernels", "author": ["Prateek Jain", "Abhradeep Thakurta"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Jain and Thakurta.,? \\Q2013\\E", "shortCiteRegEx": "Jain and Thakurta.", "year": 2013}, {"title": "near) dimension independent risk bounds for differentially private learning", "author": ["Prateek Jain", "Abhradeep Guha Thakurta"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Jain and Thakurta.,? \\Q2014\\E", "shortCiteRegEx": "Jain and Thakurta.", "year": 2014}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["Beatrice Laurent", "Pascal Massart"], "venue": "Annals of Statistics,", "citeRegEx": "Laurent and Massart.,? \\Q2000\\E", "shortCiteRegEx": "Laurent and Massart.", "year": 2000}, {"title": "Mechanism design via differential privacy", "author": ["Frank McSherry", "Kunal Talwar"], "venue": "In Foundations of Computer Science,", "citeRegEx": "McSherry and Talwar.,? \\Q2007\\E", "shortCiteRegEx": "McSherry and Talwar.", "year": 2007}, {"title": "Nearly optimal differentially private stochastic multi-arm bandits", "author": ["Nikita Mishra", "Abhradeep Thakurta"], "venue": "In UAI,", "citeRegEx": "Mishra and Thakurta.,? \\Q2015\\E", "shortCiteRegEx": "Mishra and Thakurta.", "year": 2015}, {"title": "Smooth sensitivity and sampling in private data analysis", "author": ["Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith"], "venue": "In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing,", "citeRegEx": "Nissim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nissim et al\\.", "year": 2007}, {"title": "Smooth sensitivity and sampling in private data", "author": ["Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith"], "venue": "Journal of Privacy and Confidentiality,", "citeRegEx": "Nissim et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nissim et al\\.", "year": 2011}, {"title": "Nearly optimal algorithms for private online learning in full-information and bandit settings", "author": ["Adam Smith", "Abhradeep Thakurta"], "venue": "In NIPS,", "citeRegEx": "Smith and Thakurta.,? \\Q2013\\E", "shortCiteRegEx": "Smith and Thakurta.", "year": 2013}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Richard S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Algorithms for reinforcement learning", "author": ["Csaba Szepesv\u00e1ri"], "venue": null, "citeRegEx": "Szepesv\u00e1ri.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri.", "year": 2010}, {"title": "Differentially private feature selection via stability arguments, and the robustness of the lasso", "author": ["Abhradeep Guha Thakurta", "Adam Smith"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Thakurta and Smith.,? \\Q2013\\E", "shortCiteRegEx": "Thakurta and Smith.", "year": 2013}, {"title": "Algorithms for differentially private multi-armed bandits", "author": ["Aristide C.Y. Tossou", "Christos Dimitrakakis"], "venue": "In International Conference on Artificial Intelligence", "citeRegEx": "Tossou and Dimitrakakis.,? \\Q2016\\E", "shortCiteRegEx": "Tossou and Dimitrakakis.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Reinforcement learning [Sutton and Barto, 1998] provides a variety of algorithms capable of handling such tasks.", "startOffset": 23, "endOffset": 47}, {"referenceID": 2, "context": "Differential privacy (DP) [Dwork, 2006] is a very active research area, originating from cryptography, but which has now been embraced by the machine learning community.", "startOffset": 26, "endOffset": 39}, {"referenceID": 2, "context": "DP is a formal model of privacy used to design mechanisms that reduce the amount of information leaked by the result of queries to a database containing sensitive information about multiple users [Dwork, 2006].", "startOffset": 196, "endOffset": 209}, {"referenceID": 15, "context": ", 2012, Jain and Thakurta, 2013], and the lasso [Thakurta and Smith, 2013].", "startOffset": 48, "endOffset": 74}, {"referenceID": 7, "context": "In the context of machine learning, differentially private algorithms are useful because they allow learning models in such a way that their parameters do not reveal information about the training data [McSherry and Talwar, 2007].", "startOffset": 202, "endOffset": 229}, {"referenceID": 2, "context": "Dwork and Roth [2014]) samples each component of the noise \u03b7 = (\u03b71, .", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "Nissim et al. [2007] showed that approaches based on LSp do not lead to differentially private algorithms, and then proposed an alternative framework for DP mechanisms with data-dependent perturbations based on the idea of smoothed sensitivity.", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "We will use a Monte Carlo approach, in which the returns of the trajectories in X are used as regression targets to fit the parameters in V\u0302 \u03c0 via a least squares approach [Sutton and Barto, 1998].", "startOffset": 172, "endOffset": 196}, {"referenceID": 15, "context": "On the other hand, it is known that differential privacy is tightly related to certain notions of stability [Thakurta and Smith, 2013], and optimization problems with non-unique solutions generally pose a problem to stability.", "startOffset": 108, "endOffset": 134}, {"referenceID": 9, "context": "We use the smooth sensitivity framework of [Nissim et al., 2007, 2011], which provides tools for the design of DP mechanisms with data-dependent output perturbations. We rely on the following lemma, which provides sufficient conditions for calibrating Gaussian output perturbation mechanisms with variance proportional to smooth upper bounds of the local sensitivity. Lemma 1 (Nissim et al. [2011]).", "startOffset": 44, "endOffset": 398}, {"referenceID": 9, "context": "Lemma 4 (Nissim et al. [2007]).", "startOffset": 9, "endOffset": 30}, {"referenceID": 9, "context": "For some functions \u03c6, the upper bound \u03c8 can be hard to compute or even approximate [Nissim et al., 2007].", "startOffset": 83, "endOffset": 104}, {"referenceID": 5, "context": "In particular, we cannot leverage the tight utility analysis of [Jain and Thakurta, 2014] to get dimension independent bounds.", "startOffset": 64, "endOffset": 89}, {"referenceID": 12, "context": "First, we would like to design DP policy evaluation methods based on temporal-difference learning [Sutton, 1988].", "startOffset": 98, "endOffset": 112}, {"referenceID": 9, "context": "A proof of Lemma 1 in the paper can be found in the pre-print Nissim et al. [2011]. For the sake of completeness, we provide here an elementary proof (albeit with slightly worse constants).", "startOffset": 62, "endOffset": 83}, {"referenceID": 6, "context": "On the other hand, X = \u2016Z1 \u2212 \u03bc1\u2016/\u03c3 1 \u223c \u03c7d follows a chi-squared distribution with d degrees of freedom, for which is known Laurent and Massart [2000] that for all t \u2265 0: P[X > d+ 2 \u221a dt+ 2t] \u2264 e\u2212t .", "startOffset": 123, "endOffset": 150}], "year": 2016, "abstractText": "We present the first differentially private algorithms for reinforcement learning, which apply to the task of evaluating a fixed policy. We establish two approaches for achieving differential privacy, provide a theoretical analysis of the privacy and utility of the two algorithms, and show promising results on simple empirical examples.", "creator": "LaTeX with hyperref package"}}}