{"id": "1603.02514", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2016", "title": "Variational Autoencoders for Semi-supervised Text Classification", "abstract": "Semi-supervised learning becomes one of the most significant problems nowadays since the size of datasets is increasing tremendously while labeled data is limited. We propose a new semi-supervised learning method for sequence classification tasks. Our work is based on both deep generative model for semi-supervised learning \\cite{kingma2014semi} and variational auto-encoder for sequence modeling \\cite{bowman2015generating}. We found the introduction of Sc-LSTM is critical to the success in our method. We have obtained some preliminary experimental results on IMDB sentiment classification dataset, showing that the proposed model improves the classification accuracy comparing to pure supervised classifier.", "histories": [["v1", "Tue, 8 Mar 2016 13:24:45 GMT  (169kb,D)", "http://arxiv.org/abs/1603.02514v1", "6 pages, 1 figure"], ["v2", "Mon, 23 May 2016 14:33:50 GMT  (365kb,D)", "http://arxiv.org/abs/1603.02514v2", "6 pages, 1 figure"], ["v3", "Thu, 24 Nov 2016 08:18:31 GMT  (735kb,D)", "http://arxiv.org/abs/1603.02514v3", "8 pages, 4 figure"]], "COMMENTS": "6 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["weidi xu", "haoze sun", "chao deng", "ying tan"], "accepted": true, "id": "1603.02514"}, "pdf": {"name": "1603.02514.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Variational Autoencoders for Sequence Classification", "authors": ["Weidi Xu", "Haoze Sun"], "emails": ["hsu@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Backgrounds", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Variational Autoencoder", "text": "Recently, the variaitonal autoencoder (UAE) has attracted a lot of attention due to its impressive results reported in (Kingma and Welling, 2013) and (Gregor et al., 2015). It is equipped with a top-down generative network \u03b8 and a bottom-up detection network \u03c6. Both networks are trained together to maximize the variational lower limit of data probability. In view of the data set X = {x1, x2,......, xN}, the variational auto encoder aims to maximize the loglikelihood of all datapoints (x1, x2,..., xN) = N i = 1 logp\u03b8 (xi). Different form of the traditional auto-encoder architecture that VAE is in the first sample latent representation z from a learned posterioral distribution model z = z logically z = z distribution condition."}, {"heading": "2.2 Semi-supervised Learning Using VAEs", "text": "(Kingma et al., 2016; Yan et al., 2015) initially proposed a conditional variable auto encoder (CVAE) to successfully separate the image style and content information. Several other papers (Maal\u00f8e et al., 2016) may help improve the discriminatory results by using unlabeled data. Given the label Dataset X, Y = {(x1, y1), (x2, y2)...... (xN, yN)} the lower limit of CVAE is the lower limit of QVAE: QPTB (x, y), the x, y) [logpTB (x, z, z) + logpTB (z), the lower limit of QVAE is: QPTB (x, y), the lower limit of QVAE is: QPTB (x, y)."}, {"heading": "2.3 Variational Autoencoder for Sentence Generation", "text": "Recurrent neural networks (RNNs) are the most successful methods of sequence generation, such as machine translation and caption. However, individual generation mechanisms in RNNs cannot extract high-level features such as theme, style, and sentiment properties. To solve the problem, variable recurring auto encoders (VRAEs) are used to model global features for sequential data such as sentences (Bowman et al., 2015) and music (Fabius and van Amersfoort, 2014). Similar to the models above, VRAEs use encoder decoder structure. In the recognition model, sequence x is processed by encoderRNNs to extract global information, while in generative models, latent variables z are used to initialize the hidden state for decoder RNNNNs."}, {"heading": "3 Model", "text": "In this section, we present our work in detail, based on the networks mentioned above and outlined in Figure 1. Specifically, the inference model of our method is described as follows: q\u03c6 (z | y, x) = N (z | \u00b5 (x, y), diag (\u03c32 (x, y))) (8) x \u00b2 = LSTM (x) (9) \u00b5 (x, y) = W ([x, y]) (10) log\u03c32 (x, y) = Softplus (W ([x, y]))) (11), where the sequence x is encoded by the LSTM network and the output for setting this diagonal Gaussian distribution is linked to y, y is presented as a uniform vector. In this section, we use the notation b = W (a) to denote a linear weight matrix with bias from vector a to vector b."}, {"heading": "3.1 Sc-LSTM", "text": "In order to model the sequence generation, which is based on both the hidden state z and the class name y, we originally simply link y and z as the initial state for LSTM. However, we have found in experiments that this simple implementation has not improved the classification performance since the model has found that ignoring the class feature and minimizing the generation probability according to the language model (i.e. predicting the next word according to a small context window) is the best strategy for optimizing the lens function. This is because the category information is only passed on to the generation network in the first step of time and the conditional generation probability is maximized across all types of categories (each multiplied by a coefficient generated by the classifier q\u03c6 (y | x). If the conditional generation model cannot distinguish between different category characteristics, the model will not be able to use the power of the coupling mechanism in the 2.2, the advantages of the feedback section will not be described."}, {"heading": "3.2 Cost Annealing", "text": "Cost glow is a training trick introduced in (Bowman et al., 2015; Husza \u0301 r, 2015) that gradually increases the weight of the KL costs from zero to one. Without this trick, the model tends to ignore the input x, and most training models probably result in models that consistently set q (z | x) close to q (z). This technique is also taken into account in our implementation."}, {"heading": "4 Experimental Results", "text": "This section shows several preliminary experimental results in the Large Movie Review Dataset, sometimes known as the IMDB Dataset. The dataset consists of 25K labeled data samples for the training, 25K labeled data samples for the test, and 50K unlabeled data samples for semi-supervised learning. The dataset has adequate samples for the training classifier as well as generation models.For the equipment set, we randomly sample 5K samples from the training set. We use a common implementation for the classification model, i.e. the use of the average hidden state generated by the standard LSTM network to predict the label.The averaging of the hidden states is easier for the training reported in (Hong and Fang,).Although there are some other more sophisticated methods for sequence classification, we use this standard model for simplification in subsequent experiments.Our results cannot match the state-of-the IMDB data sets as high-precision ones."}, {"heading": "4.1 Implementation Details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Length Sampling", "text": "If the use of data sets consists of long sequence samples, it is not efficient to use the entire sequence to generate them. On the other hand, assuming that the sub-sequence has enough information to derive the category from, a trusted sequence is sufficient to distinguish the likelihood of generating different class characteristics. On the other hand, the recurring neural network, even LSTM, is unable to model very long sequence generation, which reduces the difficulty for conditional generation models. In the implementation of IMDB data sets, where the average sequence length is about 300, we randomly draw sub-sequences from each data sample to speed up training. In experiments, we found that this trick works well to speed up training."}, {"heading": "4.1.2 Dropout and Word-dropout", "text": "In the conditional sequence generation model, we randomly omit some words and replace them with spaces. This method introduces noise into networks and helps the network to be more general. However, we found it inappropriate to use the word dropout mechanism in Equation 6. Since the word dropout makes the generation probability unstable and thus affects the gradient calculated for the sequence classifier, we use this mechanism in our implementation only for marked data, i.e. Equation 5, but not in Equation 6."}, {"heading": "4.2 Qualitative Analysis", "text": "Although the classification performance is improved with our method, the generation performance is still unknown. To verify this, sample several sentences using a trained conditional generation model. The sampled sentences are shown in Table 4.2. We show several cases when we use the same initial state z and different designations. From the examples, we can see that both sentences generated with the same z share the same sentence structure and words, but the sentimental implication is completely different from each other. On the other hand, the model is still unable to grasp the high meaning of the feeling, but has tried to remember the frequency of the words for each category."}, {"heading": "5 Conclusion", "text": "In this paper we present a new semi-supervised learning method for sequence classification. We explained the reason for the use of Sc-LSTM as a conditional generative model. The resulting model can significantly improve classification performance."}, {"heading": "Acknowledgments", "text": "This work was supported by the Natural Science Foundation of China (NSFC) under grant number 61375119 and the Beijing Natural Science Foundation under grant number 4162029, and partially supported by the National Key Basic Research Development Plan (973 Plan) Project of China under grant number 2015CB352302."}], "references": [{"title": "Greedy layer-wise training of deep networks. Advances in neural information processing", "author": ["Bengio et al.2007] Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349", "author": ["Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Variational recurrent auto-encoders", "author": ["Fabius", "van Amersfoort2014] Otto Fabius", "Joost R van Amersfoort"], "venue": "arXiv preprint arXiv:1412.6581", "citeRegEx": "Fabius et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fabius et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor et al.2015] Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv preprint arXiv:1511.05101", "author": ["Ferenc Husz\u00e1r"], "venue": null, "citeRegEx": "Husz\u00e1r.,? \\Q2015\\E", "shortCiteRegEx": "Husz\u00e1r.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Welling2013] Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Semi-supervised learning with deep generative models", "author": ["Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auxiliary deep generative models. arXiv preprint arXiv:1602.05473", "author": ["Maal\u00f8e et al.2016] Lars Maal\u00f8e", "Casper Kaae S\u00f8nderby", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": null, "citeRegEx": "Maal\u00f8e et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maal\u00f8e et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "Proceedings of the confer-", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": null, "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Attribute2image: Conditional image generation from visual attributes. arXiv preprint arXiv:1512.00570", "author": ["Yan et al.2015] Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee"], "venue": null, "citeRegEx": "Yan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Our work is based on both deep generative model for semisupervised learning (Kingma et al., 2014) and variational auto-encoder for sequence modeling (Bowman et al.", "startOffset": 76, "endOffset": 97}, {"referenceID": 1, "context": ", 2014) and variational auto-encoder for sequence modeling (Bowman et al., 2015).", "startOffset": 59, "endOffset": 80}, {"referenceID": 4, "context": "Previous semi-supervised methods (Hinton et al., 2006; Vincent et al., 2010; Bengio et al., 2007) are often used for weight initialization, i.", "startOffset": 33, "endOffset": 97}, {"referenceID": 11, "context": "Previous semi-supervised methods (Hinton et al., 2006; Vincent et al., 2010; Bengio et al., 2007) are often used for weight initialization, i.", "startOffset": 33, "endOffset": 97}, {"referenceID": 0, "context": "Previous semi-supervised methods (Hinton et al., 2006; Vincent et al., 2010; Bengio et al., 2007) are often used for weight initialization, i.", "startOffset": 33, "endOffset": 97}, {"referenceID": 10, "context": "(Socher et al., 2013) proposed a similar model using semi-supervised method.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "It has also been applied in semi-supervised learning (Kingma et al., 2014; Maal\u00f8e et al., 2016) and achieved the best results in several image semisupervised learning tasks .", "startOffset": 53, "endOffset": 95}, {"referenceID": 9, "context": "It has also been applied in semi-supervised learning (Kingma et al., 2014; Maal\u00f8e et al., 2016) and achieved the best results in several image semisupervised learning tasks .", "startOffset": 53, "endOffset": 95}, {"referenceID": 8, "context": "We combines the variational autoencoders for semi-supervised learning (Kingma et al., 2014) and its application in sequence generation (Bowman et al.", "startOffset": 70, "endOffset": 91}, {"referenceID": 1, "context": ", 2014) and its application in sequence generation (Bowman et al., 2015) for sequence classifiar X iv :1 60 3.", "startOffset": 51, "endOffset": 72}, {"referenceID": 12, "context": "To force the conditional generation model to be aware of category feature, we utilize the similar structure with semantically controlled LSTM (Sc-LSTM), which is proprosed in (Wen et al., 2015).", "startOffset": 175, "endOffset": 193}, {"referenceID": 3, "context": "Recently variaitonal autoencoder (VAE) have drawn a lot of attentions due to its impressive results reported in (Kingma and Welling, 2013) and (Gregor et al., 2015).", "startOffset": 143, "endOffset": 164}, {"referenceID": 8, "context": "(Kingma et al., 2014) firstly proposed a conditional variational auto-encoder (CVAE) to successfully separate the image style and content information.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Several other works (Maal\u00f8e et al., 2016; Yan et al., 2015) also proved CVAE to be powerful in image generation tasks.", "startOffset": 20, "endOffset": 59}, {"referenceID": 13, "context": "Several other works (Maal\u00f8e et al., 2016; Yan et al., 2015) also proved CVAE to be powerful in image generation tasks.", "startOffset": 20, "endOffset": 59}, {"referenceID": 8, "context": "In addition, semi-supervised learning (Kingma et al., 2014; Maal\u00f8e et al., 2016) can help improving the discriminative results by employing unlabeled data.", "startOffset": 38, "endOffset": 80}, {"referenceID": 9, "context": "In addition, semi-supervised learning (Kingma et al., 2014; Maal\u00f8e et al., 2016) can help improving the discriminative results by employing unlabeled data.", "startOffset": 38, "endOffset": 80}, {"referenceID": 1, "context": "To solve the problem, variational recurrent autoencoders (VRAEs) have been employed in modeling global features for sequential data like sentences(Bowman et al., 2015) and music(Fabius and van Amersfoort, 2014).", "startOffset": 146, "endOffset": 167}, {"referenceID": 12, "context": "Simplified semantically controlled LSTM (Sc-LSTM) network proposed in (Wen et al., 2015) is adopted to achieve this goal.", "startOffset": 70, "endOffset": 88}, {"referenceID": 1, "context": "Cost annealing is a training trick introduced in (Bowman et al., 2015; Husz\u00e1r, 2015), which gradually increasing the weight of KL cost from zero to one.", "startOffset": 49, "endOffset": 84}, {"referenceID": 6, "context": "Cost annealing is a training trick introduced in (Bowman et al., 2015; Husz\u00e1r, 2015), which gradually increasing the weight of KL cost from zero to one.", "startOffset": 49, "endOffset": 84}], "year": 2016, "abstractText": "Semi-supervised learning becomes one of the most significant problems nowadays since the size of datasets is increasing tremendously while labeled data is limited. We propose a new semisupervised learning method for sequence classification tasks. Our work is based on both deep generative model for semisupervised learning (Kingma et al., 2014) and variational auto-encoder for sequence modeling (Bowman et al., 2015). We found the introduction of Sc-LSTM is critical to the success in our method. We have obtained some preliminary experimental results on IMDB sentiment classification dataset, showing that the proposed model improves the classification accuracy comparing to pure supervised classifier.", "creator": "LaTeX with hyperref package"}}}