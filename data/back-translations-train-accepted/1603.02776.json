{"id": "1603.02776", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2016", "title": "Implicit Discourse Relation Classification via Multi-Task Neural Networks", "abstract": "Without discourse connectives, classifying implicit discourse relations is a challenging task and a bottleneck for building a practical discourse parser. Previous research usually makes use of one kind of discourse framework such as PDTB or RST to improve the classification performance on discourse relations. Actually, under different discourse annotation frameworks, there exist multiple corpora which have internal connections. To exploit the combination of different discourse corpora, we design related discourse classification tasks specific to a corpus, and propose a novel Convolutional Neural Network embedded multi-task learning system to synthesize these tasks by learning both unique and shared representations for each task. The experimental results on the PDTB implicit discourse relation classification task demonstrate that our model achieves significant gains over baseline systems.", "histories": [["v1", "Wed, 9 Mar 2016 03:13:37 GMT  (177kb,D)", "http://arxiv.org/abs/1603.02776v1", "This is the pre-print version of a paper accepted by AAAI-16"]], "COMMENTS": "This is the pre-print version of a paper accepted by AAAI-16", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["yang liu", "sujian li", "xiaodong zhang", "zhifang sui"], "accepted": true, "id": "1603.02776"}, "pdf": {"name": "1603.02776.pdf", "metadata": {"source": "CRF", "title": "Implicit Discourse Relation Classification via Multi-Task Neural Networks", "authors": ["Yang Liu", "Sujian Li", "Xiaodong Zhang", "Zhifang Sui"], "emails": ["szf}@pku.edu.cn"], "sections": [{"heading": "Introduction", "text": "In this context, it is important that we are able to identify ourselves in a different way than is the case in other countries. In other countries, where explicit markers exist, different tasks may arise. However, without an explicit marker based on implicit discourse relationships, it is much more difficult to rely on. The fact that these implicit relationships are explicit makes the classification of their types a central challenge in discourse analysis.The main line of research approaches the implicit classification of features from the corpus and the design of machine learning algorithms."}, {"heading": "Prerequisite", "text": "In our work, we choose three types of discourse corpora: PDTB, RST-DT, and the natural text with discourse-linking words. In this section, we briefly introduce corporatism (PDTB) (Prasad et al. 2007), known as the largest discourse corpus, consisting of 2159 articles from the Wall Street Journal. PDTB adopts the predicate argumentation structure in which the predicate is discourse-linking (e.g. during) and the arguments are two passages of text around the connective. In PDTB, a relationship is explicit that is presented connectively in the text; otherwise, it is implicit and argumentative."}, {"heading": "Multi-Task Neural Network for Discourse Parsing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Motivation and Overview", "text": "In Table 1 we list some examples that have similar discourse relations in nature but are commented on differently in different corpora; the second row is part of the collaborative relationship in RST-DT; the third and fourth rows are both expansion relations in the PDTB: one is implicit and the other is explicit with the connective \"particular\"; the fifth row comes from the NYT corpus and uses directly the word \"special\" to denote the discourse relationship between two sentences; from these instances we can see that they all reflect the similar discourse relationship, which the second argument gives more details of the first argument; it is intuitive that the classification power in these instances can be increased from each other if we synthesize them appropriately; with this idea we propose adopting the multi-task learning method and designing a specific discourse analysis task."}, {"heading": "CNNs: Modeling Argument Pairs", "text": "Figure 1 illustrates our proposed method of modeling the argument pairs. We associate each word with a vector representation xw-RDe, which is usually preschooled with large unlabeled corpora. We consider an argument as a sequence of these word vectors, and let x1i (x 2 i) be the vector of the ith word in the argument Arg1 (Arg2). Then the argument pair can be represented as \"Arg1.\" Arg2Generally, we let xi: i +, x1m1] arguments (1) Arg2: [x21, x 2, x2m2] (2) where Arg1 has m1 words and Arg2 has words m2 words.Arg2Generally, we refer to the concatenation of the word vectors xi + 1, xi + 1. \""}, {"heading": "Multi-Task Neural Networks: Classifying Discourse Relations", "text": "This year, it will be able to find a solution that adapts to the needs of the people."}, {"heading": "Model Training", "text": "If the instance belongs to class i, only the i-th dimension gt [i] 1 and the other dimensions are set to 0. In our MTNN model, all tasks are classification problems and we use cross-entropy loss as an optimization function. Given the parameters of the neural network and the word embedding, the objective function can for example be written as J (Liu et al. 2015) = \u2212 nt [i] lt [i] lt [i] lt [i] (9) We use mini-batch stochastic gradient descent (SGD) to train the parameters. Referring to the training method in (Liu et al. 2015), we select a task in each epoch and update the model according to its task-specific objectivity. To avoid overadjustment, we use different learning rates to train the neural network parameters and the word embedding, which are roughly referred to as J, and determine that the tasks are best fulfilled."}, {"heading": "Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Datasets", "text": "As outlined above, we use three corpora in our experiments: PDTB, RST-DT, and the NYT corpus to train our multi-task neural network.Since our main goal is to classify implicit discourse relationships (the main task), Table 2 summarizes the statistics of the four implicit discourse relationships at the highest level in PDTB. We follow the structure of previous studies (Pitler, Louis, and Nenkova 2009) by dividing the data set into a training set, development set, and test set. Sections 2-20 are used to train classifiers, Sections 0-1 to develop trait sets and fine-tune models, and Sections 21-22 to test the systems.In task 2, all 17,469 explicit relationships are used in Sections 0-24 in the PDTB. Table 3 shows the distribution of these explicit relationships across four classes. In task 3, we change RST-dependent structures for DTB (we get the most frequent EDT-tree types in 2014)."}, {"heading": "Model Configuration", "text": "We use Word embedding provided by GloVe (Pennington, Socher and Manning 2014), and the dimension of embedding De is 50. We first train these four tasks separately to roughly adjust their hyper parameters, then adjust the multi-task learning system more carefully to the performance of our main task on the development set. Learning rates are set as \u03bb = 0.004, \u03bbe = 0.001. Each task has a number of hyper parameters, including the window size of CNN h, the pooling size np, the number of filters nf, the dimension of task specific representation nr and the regulating ratios \u00b5 and \u00b5e. All tasks share a window size, a pooling size, and a number of filters for learning the common representation called hs, nsp, n s f. Detailed settings are shown in Table 6."}, {"heading": "Evaluation and Analysis", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "Related Work", "text": "They selected several surface features to form four implicit classifiers, each of which was suitable for one of the top PDTB relationship classes. Although other characteristics proved useful, the pairs of words were the main ones responsible for most of these classifiers. Interestingly, they found that training these characteristics on PDTB was more useful than training on an external corpus. Based on this work, Lin, Kan and Ng (2009) extended to four different trait types that represent the context, the constituent parse trees and the raw text. In addition, Park and Cardie enhanced performance by optimizing the trait group."}, {"heading": "Conclusion", "text": "Previous studies on the classification of implicit discourse relationships have always faced two problems: conciseness and representation of arguments. To solve these two problems, we propose to use different types of corpus and to design a Multi-Task Neural Network (MTNN) to synthesize different body-specific discourse classification tasks. In our MTNN model, the Convolutionary Neural Networks with Dynamic Pooling are developed to model the argumentation pairs. Subsequently, different discourse classification tasks can derive their unique and common representations for the argument pairs, through which they can mutually optimize each other without causing useless noise. Experimental results show that our system delivers state-of-of-the-art performance. In our future work, we will design an MTL system based on the syntactic tree that allows each task to share structural information."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Without discourse connectives, classifying implicit discourse relations is a challenging task and a bottleneck for building a practical discourse parser. Previous research usually makes use of one kind of discourse framework such as PDTB or RST to improve the classification performance on discourse relations. Actually, under different discourse annotation frameworks, there exist multiple corpora which have internal connections. To exploit the combination of different discourse corpora, we design related discourse classification tasks specific to a corpus, and propose a novel Convolutional Neural Network embedded multi-task learning system to synthesize these tasks by learning both unique and shared representations for each task. The experimental results on the PDTB implicit discourse relation classification task demonstrate that our model achieves significant gains over baseline systems.", "creator": "LaTeX with hyperref package"}}}