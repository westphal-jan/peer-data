{"id": "1603.02839", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2016", "title": "Starting Small -- Learning with Adaptive Sample Sizes", "abstract": "For many machine learning problems, data is abundant and it may be prohibitive to make multiple passes through the full training set. In this context, we investigate strategies for dynamically increasing the effective sample size, when using iterative methods such as stochastic gradient descent. Our interest is motivated by the rise of variance-reduced methods, which achieve linear convergence rates that scale favorably for smaller sample sizes. Exploiting this feature, we show -- theoretically and empirically -- how to obtain significant speed-ups with a novel algorithm that reaches statistical accuracy on an $n$-sample in $2n$, instead of $n \\log n$ steps.", "histories": [["v1", "Wed, 9 Mar 2016 10:52:53 GMT  (561kb)", "http://arxiv.org/abs/1603.02839v1", null], ["v2", "Fri, 7 Oct 2016 12:33:13 GMT  (774kb)", "http://arxiv.org/abs/1603.02839v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hadi daneshmand", "aurelien lucchi", "thomas hofmann"], "accepted": true, "id": "1603.02839"}, "pdf": {"name": "1603.02839.pdf", "metadata": {"source": "META", "title": "Starting Small \u2013 Learning with Adaptive Sample Sizes", "authors": ["Hadi Daneshmand", "Aurelien Lucchi"], "emails": ["HADI.DANESHMAND@INF.ETHZ.CH", "AURELIEN.LUCCHI@INF.ETHZ.CH", "THOMAS.HOFMANN@INF.ETHZ.CH"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.02 839v 1 [cs.L G] 9M ar"}, {"heading": "1. Introduction", "text": "In empirical risk minimization (ERM) (Vapnik, 1998), the training set S is used to define a sample risk RS, which is then minimized with respect to a predefined function class. Learning algorithms are effectively equated with optimization algorithms. However, for all practical purposes, an approximate solution of the RS will suffice as long as the optimization error is small in relation to statistical accuracy for sample size n: = | S | This is important for massive data sets where optimization for numerical precision is not feasible. Instead of stopping black box optimization early, one should understand the trade-offs between statistical and computational accuracy, cf. (Chandrasekaran & Jordan, 2013). In this paper, we examine a much neglected facet of this topic, namely the dynamic control of effective sample size in optimization."}, {"heading": "1.1. Empirical Risk Minimization", "text": "Formally, we assume that training examples x-S X i.i.d. were drawn from an underlying but unknown probability distribution P. We define a function class F, which is parameterized by weight vectors w-Rd, and define the expected risk as R (w): = Efx (w), where f is an x-indexed family of loss functions, which is often convex. We define the minimum and minimizer of R (w) versus F by R * or w *. Since P is unknown, the ERM suggests relying on empirical (or sample) risk in relation to SRS (w): = 1n x x x-S fx (w), w-S: = argmin w-F RS (w). (1) Note that in defining the loss factor, one can absorb a regulator."}, {"heading": "1.2. Generalization bounds", "text": "The relationship between w * and w * S has been extensively studied in the literature on learning theory and is usually analysed using uniform convergence boundaries that take the general form (Boucheron et al., 2005) ES [sup w * F (w) \u2212 RS (w) |] \u2264 H (n), (2) where the expectation is higher than a random n sample. Here, H is a boundary that depends on n, usually by a ratio n / d, where d is the capacity of F (e.g. VC dimension). In the feasible case, we may observe a favorable H (n) between d / n, whereas in the pessimistic case we may only see weaker boundaries such as H (n) or d / n (e.g. for linear functional classes); see also (Bousquet & Bottou, 2008)."}, {"heading": "1.3. Statistical efficiency", "text": "Suppose, then, that we have an approximate optimization algorithm that produces solutions wS for S that are, on average, um (n) optimal, i.e. ES [RS (wS) \u2212 R \u0445 S] \u2264 (n). We can then give the following quality guarantee in anticipation of sample sets S (Bousquet & Bottou, 2008) ESR (wS) \u2212 R \u0445 H (n) + (n), (3), which represents an additive splitting of the expected solution suboptimality into an estimated (or statistical) error H (n) and an optimization (or computational) error (n). On a given calculation budget, we typically find that Hs (n) increases with n, whereas H (n) decreases with n. This indicates a trade-off, which could indicate a sample size m < n. Intuitively speaking, the concentration of the calculation budget on less data may be better than the distribution of the calculations."}, {"heading": "1.4. Stochastic Gradient Optimization", "text": "However, while the updating directions of the SGD are expected to correspond to the true (negative) gradient direction, high variance typically leads to sublinear convergence. Here, variance-reducing methods for ERM such as SAG (Roux et al., 2012), SVRG (Johnson & Zhang, 2013) and SAGA (Defazio et al., 2014) come into play. We focus on the latter, where the following result can be determined on the convergence rate (see Appendix). Lemma 1. Let all fx be convex with continuous L-Lipschitz gradients and assume that RS is strongly convex. Then, the suboptimality of the SAGA iteration by t-steps becomes a random convergence limited by EA."}, {"heading": "1.5. Contributions", "text": "Our main question is: Can we achieve faster convergence to a statistically precise solution by running SAGA on an initially smaller sample, the size of which is then gradually increased? Motivated by a simple but concise analysis, we present a novel algorithm called DYNASAGA, which implements this idea and reaches the value of H (n) \u2264 H (n) after only 2n iterations."}, {"heading": "2. Related Work", "text": "The Stochastic Approach is a powerful tool for minimizing objective equivalents (1) for convex loss functions; the pioneering work of (Robbins & Monro, 1951) is essentially a streamlined SGD method in which each observation is applied only once; another important milestone was the idea of iteration averaging (Polyak & Juditsky, 1992); a thorough theoretical analysis of asymptotic convergence of SGD can be found in (Kushner & Yin, 2003), while some non-asymptotic results were presented in (Moulines & Bach, 2011); and a number of recent papers, known as variance-reduced SGD, for example (Roux et al., 2012; Shalev-Shwartz & Zhang, 2013; Johnson & Zhang, 2013; Defazio et al., 2014), has used the finite sum structure of empirical risks to establish a linear convergence for strongly convergence objectives."}, {"heading": "3. Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Setting and Assumptions", "text": "We work on the basis of the assumptions made in term 1 and focus on the system of large amounts of data, where n \u0430 and the geometric convergence rate of the SAGA depend from n to \u03c1n = 1 \u2212 1 / hub. This is an interesting regime, since the guaranteed progress per update is greater for smaller samples. This form of \u03c1n implies for the case of performing t = niterations, i.e. performing a pass1: E [RS (wn) \u2212 R \u0445 S] \u2264 (1 \u2212 1 n) nCS \u2264 CS e. (4) This guarantees that we improve the solution suboptimality on average by a factor of 1 / e per pass. This, in turn, implies that in order to achieve a guaranteed accuracy O (n \u2212 \u03b1), we need O (n log n) updating steps."}, {"heading": "3.2. Sample Size Optimization", "text": "For illustration, we use the above result to select a sample size for SAGA that provides the best guarantees. Suggestion 2. Let's assume H (m) = D / m and n is given. Let's define C as an upper limit for CS, as well as S (from Lemma 1), then for m \u0438, V (m): = Dm + Ce \u2212 n m represents a limit for the expected sub-optimality of SAGA. It is minimized for the choicem \u0445 = max {\u0432, nlogn + log CD}.Evidence. The first assertion is directly derived from the assumptions and Lemma 1. In addition, the narrowest limit is reached by differentiating V with respect to 1 / m and the solution for m (see Lemma 9 in the appendix).The result implies that we will perform coarse logn + log CD epochs on the optimally dimensioned sample. Also, the value of the limit is (for simplicity, provided C is optimal logon the appendix), but the result does not imply that the CD is fully encoded (V)."}, {"heading": "3.3. Dynamic Sample Growth", "text": "As we have seen, optimization over a smaller sample can be advantageous (if we believe the significance of the boundaries), but why should a single sample size be chosen once and for all? A smaller sample size seems to be advantageous, but as an optimization algorithm RT approaches the empirical minimizer, it is hit by the statistical accuracy limit, which suggests that we should dynamically increase the size of the sample size. We illustrate this idea in Figure 2. To analyze such a dynamic sample scheme, we need to correlate the suboptimization on a partial sample T with a sub-optimization bound to S. We establish a basic result in the following theorems. \u2212 Theorem 3. Let w be an (ig, T) optimal solution, i.e. RT (w) \u2212 R, T \u2264 T \u2264, where T (S: = | T |, n: | S | |."}, {"heading": "4. Algorithms & Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Computational Limited Learning", "text": "The work of (Bottou, 2010) stresses that for massive data sets, the limiting factor of each learning algorithm is its computational complexity T, not the number of samples n. In SGD, this computational boundary typically translates into the number of stochastic gradients evaluated by the algorithm, i.e. T becomes the number of update steps. An obvious strategy with abundant data is to capture a new data point in each iteration. There are asymptotic results that set boundaries for different SGD variants (Bousquet & Bottou, 2008). However, SAGA and related algorithms rely on memorizing past stochastic gradients, cf. (Hofmann et al., 2015), which makes it advantageous to revisit data points, and which underlies results such as Lemma 1. This leads to qualitatively different behavior, and our results suggest that target conflicts for large-scale learning processes actually need to be revisited."}, {"heading": "4.2. SAGA with Dynamic Sample Sizes", "text": "We define a plan as a monotonic function M: Z + \u2192 Z +, where t is the iteration number and M (t) is the effective sample size used for t. We assume that a sequence of data points X = (x1,.., xn) from P is such that M is a nested sequence of samples Tt: = {xi: 1 \u2264 i \u2264 M (t)}.Algorithm 2 DYNASAGA 1: Input: Training examples X = (x1, x2,.,., xn), xi \u00b2 P Total number of samples T (e.g. T = 2n) Starting point w0 \u0445M (e.g. w0 = 0) Learning rate \u03b7 > 0 (e.g. \u03b7 = 14L) Sample plan M: [1: T] \u2192 [1 \u2212 n] Total number of iterations T (e.g. T = 2n)."}, {"heading": "4.3. Upper Bound Recurrence", "text": "We pursue the strategy of using the fundamental inequalities obtained so far and stitching them together in the form of a repetition. At each iteration t, we allow ourselves the choice of extending the current sample of size m by some increments \u0432 m \u2265 0. We define an upper boundary function U as follows: U (t, n) = min \u03c1nU (t \u2212 1, n) min m < n [U (t, m) + n \u2212 mn H (m)], (7) such that U (0, m) = B, the initial error being defined as follows: \u0432: = 4L\u00b5 [R (w0) \u2212 R (w \u0445)]. (8) We refer the reader to Lemma 8 in the appendix for further details on how to derive the expression for an iteration. (7) The construction of Eq. (7) is motivated by the following result: Production 4th W.h.p via the random n-X sample generated by the ASDGA sequence."}, {"heading": "4.4. Sample Schedules", "text": "In this section, we present and analyze two adaptive sample-size schemes for DYNASAGA.LINEAR We start with sample size \u0430 and perform 2\u043c steps. From there, we add a new sample every second iteration. The effective sample size is thusMLIN (t) = max {2\u0445, t 2} (9) Note that this strategy defines an upper limit for U (2t, t) and U (2t + 1, t). ALTERNATING We have also implemented a variant where we perform alternately updates: with every second iteration, we take a new data point to be added to the set. However, we also force an update of this fresh sample. Alternatively, we simply sample an existing data point uniformly at random. We do not provide a theoretical analysis for this scheme, but experimentally demonstrate that it slightly exceeds the LINEAR strategy (see results in the appendix)."}, {"heading": "4.5. Analysis", "text": "For H (n) = Dn \u2212 \u03b1, 0 < \u03b1 \u2264 1, the LINEAR strategy obtains the following suboptimalityU (2n, n) \u2264 H (n) \u2264 H (n) + 2 (3) subjects (10) proof. The base case results from Cm \u2264 1). Using Eq. (7) and (10) for the inductive case, we can consider getU (2 (n + 1), n + 1) (7) \u2264 2 + 1 [U (2n, n) + 1n H (n)] (10) \u2264 2 (2 + n2) (n + 1) (n + 1) 3 H (n) that by definition of the logarithmic function, log [n + 2) < 2 log (n + 1)."}, {"heading": "5. Experimental Results", "text": "We present experimental results on both synthetic and real data, which largely confirm the above analysis."}, {"heading": "5.1. Baselines", "text": "We compare DYNASAGA (both the LINEAR and ALTERNATING strategies) with various optimization methods presented in Section 2. These include SGD (with constant and decreasing increments), SAGA, Streaming SVRG (SSVRG) and the mixed SGD / SVRG approach presented in (Babanezhad et al., 2015)."}, {"heading": "5.2. Experiment on synthetic data", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "5.3. Experiments on Real Datasets", "text": "The details of the data sets are shown in Table 2. In all experiments, we used the lo-gistic loss with a regularizer \u03bb = 1 \u221a n 2. Figures 4 and 5 show the sub-optimality of empirical risk and expected risk after a single pass through the data sets. The various parameters used for baseline methods are shown in Table 3. A decisive factor for the performance of most baselines, especially SGD, is the selection of the step size. We selected the most powerful step size within the common range guided by existing theoretical analyses, specifically \u03b7 = 1 / L and \u03b7 = CC + \u00b5t for different values of C. Overall, we can see that DYNASAGA performs very well, both as an optimization and as a learning algorithm. SGD is also very competitive and typically achieves faster than all other levels of SYD, with convergence with all other SYS values not very fast."}, {"heading": "6. Conclusion", "text": "We have presented a new method to exploit the trade-off between computational and statistical complexity to achieve rapid convergence to a statistically efficient solution. Specifically, we have focused on modifying the SAGA and proposed a simple dynamic sampling plan that adds a new data point to each of the two update steps. Our analysis shows competitive convergence rates both in terms of suboptimal empirical risk and (more importantly) the expected risk in a one- or two-pass setting. These results have been experimentally validated. Our approach depends on the underlying optimization2We also present some additional results for various regulators of the form \u03bb = 1np, p < 1 in the appendix only by their convergence rate to minimize empirical risk. We therefore suspect that a similar adjustment of sample size to a much broader range of algorithms is applicable, including non-convergence learning for the optimization method."}, {"heading": "A. Appendix", "text": "We start with the convergence rate of the SAGA, which is used in the (Defazio et al., 2014) asEA [2 \u2212 \u2212 \u2212 \u2212 \u2212 Sp] -setpoint (1 \u2212 setpoint) -setpoint (1 \u2212 setpoint) -setpoint (1 \u2212 setpoint) -setpoint (1 \u2212 setpoint) -setpoint (0 \u2212 setpoint) -setpoint (1 \u2212 setpoint) -setpoint (1) -setpoint (1) -setpoint (1 \u2212 setpoint) -setpoint (0 \u2212 setpoint) -setpoint (0 \u2212 setpoint) -setpoint (w) -setpoint (0 \u2212 setpoint) -setpoint (0 \u2212 setpoint) -setpoint (0 \u2212 setpoint) -setpoint (0 \u2212 setpoint) -setpoint (0 \u2212 setpoint) -setpoint (0) -setpoint (0) -0 (setpoint) (0) -0 (setpoint) -0 (-0) -0 (-0) -0 (-0) -0 (-0)"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "For many machine learning problems, data is abundant and it may be prohibitive to make multiple passes through the full training set. In this context, we investigate strategies for dynamically increasing the effective sample size, when using iterative methods such as stochastic gradient descent. Our interest is motivated by the rise of variance-reduced methods, which achieve linear convergence rates that scale favorably for smaller sample sizes. Exploiting this feature, we show \u2013 theoretically and empirically \u2013 how to obtain significant speed-ups with a novel algorithm that reaches statistical accuracy on an n-sample in 2n, instead of n logn steps.", "creator": "LaTeX with hyperref package"}}}