{"id": "1603.03144", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Mar-2016", "title": "Part-of-Speech Tagging for Historical English", "abstract": "With the rise of digital humanities research, natural language processing for historical texts is of increasing interest. However, directly applying standard language processing tools to historical texts often yields unsatisfactory performance, due to language change and genre differences. Spelling normalization is the dominant solution, but it fails to account for changes in usage and vocabulary. In this empirical paper, we assess the capability of do- main adaptation techniques to cope with historical texts, focusing on the classic bench- mark task of part-of-speech tagging. We empirically evaluate several domain adaptation methods on the task of tagging two million- word treebanks of the Penn Corpora of Historical English. We demonstrate that domain adaptation significantly outperforms spelling normalization when adapting modern taggers to older texts, and that domain adaptation is complementary with spelling normalization, yielding better results in combination.", "histories": [["v1", "Thu, 10 Mar 2016 04:27:15 GMT  (29kb)", "http://arxiv.org/abs/1603.03144v1", "Accepted to NAACL 2016"], ["v2", "Mon, 4 Apr 2016 16:59:38 GMT  (31kb)", "http://arxiv.org/abs/1603.03144v2", "Accepted to NAACL 2016"]], "COMMENTS": "Accepted to NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.DL", "authors": ["yi yang", "jacob eisenstein"], "accepted": true, "id": "1603.03144"}, "pdf": {"name": "1603.03144.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["yiyang+jacobe@gatech.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.03 144v 1 [cs.C L] 10 M"}, {"heading": "1 Introduction", "text": "There is a growing interest in applying natural language processing (NLP) techniques to historical texts (Piotrowski, 2012), with applications in information retrieval (Dougherty, 2010; Jurish, 2011), linguistics (Baron et al., 2009; Rayson et al., 2007), and the digital humanities (Hendrickx et al., 2011; Pettersson and Nivre, 2011). However, these texts differ from contemporary training programs in a number of linguistic aspects, including the lexicon (Giusti et al., 2007), morphology (Borinand Forsberg, 2008), and syntax (Eumeridou et al, 2004), which poses significant challenges to modern NLP tools: for example, the accuracy of the CLAWS language tagger (Smith and Smith, 1997) drops 97% on the British National Corpus."}, {"heading": "2 Data", "text": "The Penn Corpora of Historical English contains text from a variety of genres such as this drama 1ME, second edition (Kroch and Taylor, 2000, PPCME2), the Penn-Helsinki Parsed Corpus of Early Modern English (PPCEME), and the Penn Parsed Corpus of Modern English (PPCMBE), and the corpora are annotated with part-of-speech tags and syntactic parsing trees in an annotation style like that of the PTB. In this work we focus on the PPCMBE and the PPCEMEMEME.1 The Penn Parsed Corpus of Modern British English The PPCMBE is a syntactically annotated corpus of text comprising 948,895 words sampled from the period 1700-1914. It is divided into three 70-year periods according to the composition date of the works."}, {"heading": "3 Unsupervised Domain Adaptation", "text": "In typical use scenarios, the user wants to tag some historical texts but does not have specific data in the target domain (e.g. Muralidharan and Hearst, 2013), which best fits the paradigm of unattended domain adaptation when the data from the source domain (e.g. the PTB) is combined with unnamed data from the target domain. Representative differences between the source and target domains can be a major source of error in domain adaptation (BenDavid et al., 2010), and several representation approaches are suggested."}, {"heading": "4 Experiments", "text": "We evaluate these uncontrolled domain adaptation approaches for historical English (the PPCMBE and the PPCEME) in two constellations: (1) time adjustment within each corpus, where we train POS taggers on the most advanced data in the corpus and test them on ever more distant records; (2) time adjustment of English POS tagging from modern news text to historical texts. The first constellation focuses on time differences and eliminates other factors that can affect tagging performance, such as differing annotations and text genres. The second constellation is the standard and well-studied evaluation scenario for POS tagging, where we train on the Wall Street Journal (WSJ) text from the PTB and test it on historical texts. In addition, we evaluate the effectiveness of the VARD normalization tool (Baron and Rayson, 2008) to improve POS tagging performance on the CEPME corpus."}, {"heading": "4.1 Experimental Settings", "text": "The data sets used in the experiments are described in \u00a7 2. All hyperparameters are matched to development data in the source domain. In cases where there is no specific development data set (adaptation within the historical corpora), sample 10% sets from the training data sets for model selection."}, {"heading": "4.1.1 Baseline systems", "text": "We include two basic systems for POS tagging: a classification-based Support Vector Machine (SVM) tagger and a bi-directional Maximum Entropy Markov Model (MEMM) tagger. Specifically, we use the L2-regulated L2-loss SVM implementation in the scikit-learn package (Pedregosa et al., 2011) and the L2-regulated bi-directional MEMM implementation by Stanford CoreNLP (Toutanova et al., 2003; Manning et al., 2014). After Yang and Eisenstein (2015), we apply the feature templates defined by Ratnaparkhi (1996) to extract the basic features for all taggers. There are three broad types of templates: five lexical feature templates, eight affix feature templates and three orthographic feature templates."}, {"heading": "4.1.2 Domain adaptation systems", "text": "We look at the unattended domain customization methods described in Section 3: SCL, Brown Clustering, word2vec2, and FEMA, which we use both in single embedding mode (FEMA-single), where metadata attributes are ignored, and in multi-attribute mode (FEMA attribute), where metadata attributes are used. Domain customization models are trained to merge source and target data sets. Following Yang and Eisenstein (2015), we do not learn feature embedings for the three orthographic feature templates: since each orthographic feature template is only a binary value, it is unnecessary to replace them with a much longer numerical vector. Then, the learned representations are linked to the basic surface features to form the extended representations. For computational reasons, the domain customization systems are based on a much longer numerical vector."}, {"heading": "4.1.3 Parameter tuning", "text": "We select the SVM regularization parameter by sweeping the range {0.1, 0.3, 0.5, 0.8, 1.0}. Following Blitzer et al. (2006), we consider pivot characteristics that occur more than 50 times in all ranges of SCL. We empirically fix the number of singular vectors of projection matrix K to 25 and also use feature normalizations and recalculations, as these settings provide the best performance in previous work. The number of brown clusters is selected from {50, 100, 200, 400}. For FEMA and word2vec, we select embed sizes of {50, 100, 200, 300} and fix the number of negative samples to 15. The window size for embedding training words is specified as 2https: / / code.google.com / p / word2vec / 5. Finally, we execute the same regulation penalty for all specific attributes of Ett1, 0.0, Ett1, 0.1, 0.1, 0.1."}, {"heading": "4.2 Temporal Adaptation", "text": "For PPCMBE, the source domain is the period from 1840 to 1914; for PPCEME, the source domain is the period from 1640 to 1710. All previous texts are treated as target domains. We transform the tags into the PTB tag set for evaluation so that the results can be compared with the next experiment in which the PTB is used for monitoring. Settings We sample 10% sets from the training data as development data for optimizing hyperparameters and then revise the models based on the complete training data using the best parameters. For FEMA, we consider domain attributes for 70-year periods and genres, resulting in a total of 21 attributes from the training data as development data for optimizing hyperparameters."}, {"heading": "4.3 Adaptation from the Penn Treebank", "text": "Most of them are off-the-shelf (for example, Stanford Taggers), based on the WSJ section of Penn Treebank, which is composed of a professionally written text from 1989, which motivates the evaluation scenario in which we train the Penn Treebank tagger and apply it to historical texts."}, {"heading": "5 Analysis", "text": "As expected, the Early Modern English Dataset (PPCEME) is much more sophisticated than the Modern British English Dataset (PPCMBE): the basic accuracy is 7% lower than the PPCEME. However, the PPCEME is also more vulnerable to domain adaptation, with FEMA offering considerably greater improvements. One reason for this is that the PPCEME has many more out-of-vocabulary (OOV) tokens: 23% compared to 9.2% in the PPCMBE. Both domain adaptation and normalization help to solve this specific problem, and in combination they lead to further improvements. This section provides further insights into the sources of error and how to improve the PPCEME data."}, {"heading": "5.1 Feature Ablation", "text": "The results show that the context characteristics of the word are important for achieving good accuracy on both IV and OOV characters. If we drop the appendages, the accuracy on OOV characters decreases significantly. Suffix characteristics are more critical than prefix characteristics, as many of them can determine the POS characteristics of English words with high precision. Orthographic characteristics prove to be almost irrelevant. Overall, the high proportion of OOV characters can be a major source of error, as tagging accuracy on OOV characters in our best base system is less than 50%. Note that these results are relevant for a classification-based tagger; while the Viterbi-based MEMM tagger performs only slightly better overall (an improvement of 0.2%), its error distribution may be different due to the benefits of structured predictions."}, {"heading": "5.2 Error Analysis", "text": "We believe that the low accuracy of In-Vocabulary (IV) is largely caused by the different annotation schemes between the PTB and the PPCEME. Table 4 shows the SVM accuracy per day and the most common errors accordingly. Most of the errors shown in the table are due to different annotations of the same token in the two corporations. For example, more than 99.9% of the tokens \"to\" in the PTB are labeled as TO, but only 54.6% of the tokens are labeled as TO in the PCEME and 44.3% of them as IN (P in the PCHE tagset). The words \"all,\" any \"and\" every \"are labeled in the PPCEME as quantifiers associated with JJ, while they are all labeled as DT in the PTB. A simple remapping from Q to DT results in an increase in the basic accuracy of 0.78%."}, {"heading": "5.3 Improvements from Normalization", "text": "As shown above, the identification accuracy decreases from 81.7% for IV tokens to 49.0% for OOV tokens. After normalization, the OOV rate for PPCEMEM tokens decreases from 23.0% to 13.5%, corresponding to a reduction of 41.5% for OOV tokens. Normalization is not completely accurate, and the identification performance for IV tokens drops slightly to 81.2% for IV tokens. However, due to the dramatic decline in the number of OOV tokens, normalization improves overall accuracy by more than 2.5%. We also observe performance declines in OV tokens after normalization (49.0% to 48.1%), suggesting that the remaining unnormalized OOV tokens are the most difficult cases for both normalization and POS marking."}, {"heading": "5.4 Improvements from Domain Adaptation", "text": "As shown in Table 5, the marking accuracy for both IV and OOV characters is increased with the domain matching methods. Compared to the base stamp, the FEMA attribute achieves an absolute 14% improvement in the accuracy of OOV characters. SCL performs slightly better than Brown clustering for OOV characters and word2vec for IV characters, but worse than OOOV characters. By including metadata attributes, the FEMA attribute performs better than FEMA single for OOV characters, although the accuracy of IV characters is similar. Interestingly, the venerable method of Brown clustering outperforms (marginally) both word2vec and SCL. We continue to examine the relationship between domain fitting and spelling normalization by looking at the errors that are corrected by both approaches. Domain adaptation leads to greater improvements than both word2vec and SCL. We also examine the relationship between domain fitting and spelling normalization by looking at the errors that are corrected by both approaches."}, {"heading": "6 Related Work", "text": "Domain adaptation Early work on domain adaptation focuses on the monitored setting in which a certain amount of marked instances is available in the target domain (Jiang and Zhai, 2007; Daume \u0301 III, 2007; Finkel and Manning, 2009).Unattended domain adaptation is more difficult but attractive in many applications, and several representative learning methods have been proposed to address this issue. Structural Correspondence Learning (Blitzer et al., 2006, SCL) and marginalized denoising autoencoders (Chen et al., 2012, mDA) seek cross-domain representations that are useful to predict a substance of features in the original instances, called Pivot features. Recent work trains cross-domain representations with neural networks, with additional goals such as minimizing source domain errors and maximizing domain confusion losses (Ganin and Lempitsky, 2015; Tg et al)."}, {"heading": "7 Conclusion", "text": "The construction of NLP systems that can withstand language variations is critical if the processing of natural language in historical texts is to be successful. Domain adaptation techniques have been widely used for similar problems, but rarely used in this application. We empirically evaluate several unattended domain adaptation approaches in the POS marking of historical English texts. Results show that domain adaptation methods significantly improve tagger performance in two historic English tree banks. FEMA outperforms other domain adaptation approaches with large margins, and it also performs better than a spelling standardization system. Normalization and domain adaptation together result in even better performance, with an overall improvement of 5% accuracy over a basic classifier in the most difficult environment. We also analyze common errors and point out that more work needs to be done to resolve the contradictory notes. We hope that our research provides historical evidence for domain adaptation and that our work promotes the use of historical texts."}], "references": [{"title": "Vard2: A tool for dealing with spelling variation in historical corpora", "author": ["Baron", "Rayson", "2008 Alistair Baron", "Paul Rayson"], "venue": "In Postgraduate conference in corpus linguistics", "citeRegEx": "Baron et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Baron et al\\.", "year": 2008}, {"title": "Word frequency and key word statistics in corpus linguistics", "author": ["Baron et al", "2009 Alistair Baron", "Paul Rayson", "Dawn Archer"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "A theory of learning from different domains", "author": ["Ben-David et al", "2010 Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman Vaughan"], "venue": "Machine learning,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Domain adaptation with structural correspondence learning", "author": ["Blitzer et al", "2006 John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Something old, something new: A computational morphological description of old swedish", "author": ["Borin", "Forsberg", "2008 Lars Borin", "Markus Forsberg"], "venue": "In LREC 2008 workshop on language technology for cultural heritage data (LaTeCH", "citeRegEx": "Borin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Borin et al\\.", "year": 2008}, {"title": "Class-based n-gram models of natural language", "author": ["Brown et al", "1992 Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1992\\E", "shortCiteRegEx": "al. et al\\.", "year": 1992}, {"title": "Canterbury Tales. Barron\u2019s Educational Series, Haupage, New York, 3 edition", "author": ["Chaucer", "Hopper", "2012 Geoffrey Chaucer", "Vincent Foster Hopper"], "venue": null, "citeRegEx": "Chaucer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chaucer et al\\.", "year": 2012}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["Chen et al", "2012 Minmin Chen", "Z. Xu", "Killian Weinberger", "Fei Sha"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Frustratingly easy domain adaptation", "author": ["III Daum\u00e9"], "venue": "Hal Daume\u0301 III", "citeRegEx": "Daum\u00e9,? \\Q2007\\E", "shortCiteRegEx": "Daum\u00e9", "year": 2007}, {"title": "The google books project: will it make libraries obsolete? The journal of academic librarianship", "author": ["Dougherty", "2010 William C Dougherty"], "venue": null, "citeRegEx": "Dougherty and Dougherty.,? \\Q2010\\E", "shortCiteRegEx": "Dougherty and Dougherty.", "year": 2010}, {"title": "What to do about bad language on the internet", "author": ["Eisenstein", "2013 Jacob Eisenstein"], "venue": null, "citeRegEx": "Eisenstein and Eisenstein.,? \\Q2013\\E", "shortCiteRegEx": "Eisenstein and Eisenstein.", "year": 2013}, {"title": "An analysis of verb subcategorization frames in three special language corpora with a view towards automatic term recognition", "author": ["Eumeridou et al", "2004 Eugenia Eumeridou", "Blaise Nkwenti-Azeh", "John McNaught"], "venue": "Computers and the Humanities,", "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "Hierarchical bayesian domain adaptation", "author": ["Finkel", "Manning", "2009 Jenny R. Finkel", "Christopher Manning"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Finkel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2009}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Ganin", "Lempitsky", "2015 Yaroslav Ganin", "Victor Lempitsky"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML)", "citeRegEx": "Ganin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ganin et al\\.", "year": 2015}, {"title": "A hybrid grammatical tagger: Claws4. Corpus annotation: Linguistic information from computer text", "author": ["Garside", "Smith", "1997 Roger Garside", "Nicholas Smith"], "venue": null, "citeRegEx": "Garside et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Garside et al\\.", "year": 1997}, {"title": "Svmtool: A general pos tagger generator based on support vector machines", "author": ["Gim\u00e9nez", "Marquez", "2004 Jes\u00fas Gim\u00e9nez", "Lluis Marquez"], "venue": "Proceedings of the 4th International Conference on Language Resources and Evaluation", "citeRegEx": "Gim\u00e9nez et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gim\u00e9nez et al\\.", "year": 2004}, {"title": "Automatic detection of spelling variation in historical corpus", "author": ["Giusti et al", "2007 Rafael Giusti", "A Candido", "Marcelo Muniz", "L\u0131\u0301via Cucatto", "Sandra Alu\u0131\u0301sio"], "venue": "In Proceedings of the Corpus Linguistics Conference (CL)", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "Automatic pragmatic text segmentation of historical letters", "author": ["Hendrickx et al", "2011 Iris Hendrickx", "Michel G\u00e9n\u00e9reux", "Rita Marquilhas"], "venue": "In Language Technology for Cultural Heritage,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Instance weighting for domain adaptation in nlp", "author": ["Jiang", "Zhai", "2007 Jing Jiang", "ChengXiang Zhai"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL), Prague", "citeRegEx": "Jiang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2007}, {"title": "What\u2019s in a domain? multi-domain learning for multiattribute data", "author": ["Joshi et al", "2013 Mahesh Joshi", "Mark Dredze", "William W. Cohen", "Carolyn P. Ros\u00e9"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "The penn-helsinki parsed corpus of middle english (ppcme2). department of linguistics, university of pennsylvania", "author": ["Kroch", "Taylor", "2000 Anthony Kroch", "Ann Taylor"], "venue": "cd-rom. Department of Linguistics,", "citeRegEx": "Kroch et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kroch et al\\.", "year": 2000}, {"title": "The penn-helsinki parsed", "author": ["Kroch et al", "2004 Anthony Kroch", "Beatrice Santorini", "Lauren Delfs"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "The penn-helsinki parsed corpus of modern british english (ppcmbe). Department of Linguistics, University of Pennsylvania, CDROM", "author": ["Kroch et al", "2010 Anthony Kroch", "Beatrice Santorini", "Ariel Diertani"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Statistically significant detection of linguistic change", "author": ["Kulkarni et al", "2015 Vivek Kulkarni", "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "In Proceedings of International Conference on World Wide Web (WWW),", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Syntactic annotations for the google books ngram corpus", "author": ["Lin et al", "2012 Yuri Lin", "Jean-Baptiste Michel", "Erez Lieberman Aiden", "Jon Orwant", "Will Brockman", "Slav Petrov"], "venue": "In Proceedings of the ACL 2012 system demonstrations,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Manning et al", "2014 Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Domain adaptation with multiple sources", "author": ["Mansour et al", "2009 Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Marcus et al", "1993 Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1993\\E", "shortCiteRegEx": "al. et al\\.", "year": 1993}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al", "2013 Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Part-of-speech tagging for middle english through alignment and projection of parallel diachronic texts", "author": ["Moon", "Baldridge", "2007 Taesun Moon", "Jason Baldridge"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),", "citeRegEx": "Moon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Moon et al\\.", "year": 2007}, {"title": "Supporting exploratory text analysis in literature study", "author": ["Muralidharan", "Hearst", "2013 Aditi Muralidharan", "Marti A Hearst"], "venue": "Literary and linguistic computing,", "citeRegEx": "Muralidharan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Muralidharan et al\\.", "year": 2013}, {"title": "Automatic verb extraction from historical swedish texts", "author": ["Pettersson", "Nivre", "2011 Eva Pettersson", "Joakim Nivre"], "venue": "In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities,", "citeRegEx": "Pettersson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pettersson et al\\.", "year": 2011}, {"title": "Crftagger: Crf english pos tagger", "author": ["Phan", "2006 Xuan-Hieu Phan"], "venue": "URL: http://crftagger. sourceforge. net", "citeRegEx": "Phan and Phan.,? \\Q2006\\E", "shortCiteRegEx": "Phan and Phan.", "year": 2006}, {"title": "Natural language processing for historical texts", "author": ["Piotrowski", "2012 Michael Piotrowski"], "venue": "Synthesis Lectures on Human Language Technologies,", "citeRegEx": "Piotrowski and Piotrowski.,? \\Q2012\\E", "shortCiteRegEx": "Piotrowski and Piotrowski.", "year": 2012}, {"title": "Tagging the bard: Evaluating the accuracy of a modern pos tagger on early modern english corpora", "author": ["Rayson et al", "2007 Paul Rayson", "Dawn Archer", "Alistair Baron", "Jonathan Culpeper", "Nicholas Smith"], "venue": "In Corpus Linguistics Conference", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "Evaluating an\u2019off-the-shelf\u2019pos-tagger on early modern german text", "author": ["Scheible et al", "2011 Silke Scheible", "Richard J Whitt", "Martin Durrell", "Paul Bennett"], "venue": "In Proceedings of the 5th ACL-HLT workshop on language technology for cultural heritage,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Parsing early and late modern english corpora", "author": ["Schneider et al", "2014 Gerold Schneider", "Hans Martin Lehmann", "Peter Schneider"], "venue": "Literary and Linguistic Computing", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Toutanova et al", "2003 Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "In Proceedings of the North American Chapter of the Association", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Word Representation: A Simple and General Method for Semi-Supervised Learning", "author": ["Turian et al", "2010 Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL),", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Simultaneous deep transfer across domains and tasks", "author": ["Tzeng et al", "2015 Eric Tzeng", "Judy Hoffman", "Trevor Darrell", "Kate Saenko"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Fast easy unsupervised domain adaptation with marginalized structured dropout", "author": ["Yang", "Eisenstein", "2014 Yi Yang", "Jacob Eisenstein"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL),", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Unsupervised multi-domain adaptation with feature embeddings", "author": ["Yang", "Eisenstein", "2015 Yi Yang", "Jacob Eisenstein"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "With the rise of digital humanities research, natural language processing for historical texts is of increasing interest. However, directly applying standard language processing tools to historical texts often yields unsatisfactory performance, due to language change and genre differences. Spelling normalization is the dominant solution, but it fails to account for changes in usage and vocabulary. In this empirical paper, we assess the capability of domain adaptation techniques to cope with historical texts, focusing on the classic benchmark task of part-of-speech tagging. We empirically evaluate several domain adaptation methods on the task of tagging two millionword treebanks of the Penn Corpora of Historical English. We demonstrate that domain adaptation significantly outperforms spelling normalization when adapting modern taggers to older texts, and that domain adaptation is complementary with spelling normalization, yielding better results in combination.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}