{"id": "1603.03793", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2016", "title": "Training with Exploration Improves a Greedy Stack-LSTM Parser", "abstract": "We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles(Goldberg and Nivre, 2013) instead of cross-entropy minimization. This form of training, which accounts for model predictions at training time rather than assuming an error-free action history, improves parsing accuracies for both English and Chinese, obtaining very strong results for both languages. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural-network.", "histories": [["v1", "Fri, 11 Mar 2016 21:34:20 GMT  (24kb)", "https://arxiv.org/abs/1603.03793v1", null], ["v2", "Tue, 13 Sep 2016 14:54:51 GMT  (26kb)", "http://arxiv.org/abs/1603.03793v2", "In proceedings of EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["miguel ballesteros", "yoav goldberg", "chris dyer", "noah a smith"], "accepted": true, "id": "1603.03793"}, "pdf": {"name": "1603.03793.pdf", "metadata": {"source": "CRF", "title": "Training with Exploration Improves a Greedy Stack LSTM Parser", "authors": ["Miguel Ballesteros", "Yoav Goldberg", "Chris Dyer", "Noah A. Smith"], "emails": ["miguel.ballesteros@upf.edu,", "yoav.goldberg@gmail.com,", "cdyer@google.com,", "nasmith@cs.washington.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.03 793v 2 [cs.C L] 13 SE"}, {"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2 Parsing Model and Parameter Learning", "text": "Our starting point is the parser model described by Dyer et al. (2015). We do not describe the model in detail and refer the reader to the original work. At each stage of the parser process, the parser state is encoded in a vector pt, which is used to calculate the probability of the parser action at the time t as: p (zt | pt) = exp (g'ztpt + qzt) \u2211 z \u00b2 A (S, B) exp (g'z \u2032 pt + qz \u2032), (1) where gz is a column vector that represents the embedding of the parser action z, and qz is a bias term for action z. The set A (S, B) exp (g'z \u2032 pt + qz \u2032) represents the valid transitions that can be taken in the current state. Since pt encodes information about all previous decisions of the parser, the chain rule gives the probability of a valid input sequence of parser action as a prerequisite w = 1."}, {"heading": "2.1 Training with Static Oracles", "text": "Using a static oracle, the training method calculates a canonical reference series of transitions for each gold parse tree. It then guides the parser through this canonical sequence of transitions, while at each step t it keeps in mind the state representation pt and the distribution over transitions p (zt | pt) predicted by the current classifier for state representation. Once the end of the sentence is reached, the parameters are updated to maximize the probability of the reference transition sequence (equation 2), which is equivalent to maximizing the probability of the correct transition p (zgt | pt) at any state along the path."}, {"heading": "2.2 Training with Dynamic Oracles", "text": "This year it is more than ever before."}, {"heading": "3 Experiments", "text": "Using the same settings of Chen and Manning (2014) and Dyer et al (2015), we report on Results 4 in the English PTB and Chinese CTB-5. Table 1 shows the results of the parser in its different configurations. The table also shows the best result obtained with the static oracle (obtained by repetition of Dyer et al. Parser) to compare between static and dynamic training strategies.The score achieved by the dynamic oracle for English is 93.56 UAS. This is noteworthy because the parser applies a completely greedy search procedure. Furthermore, the Chinese score establishes the state of the art by using the same settings as Chen and Manning (2014).3A similar goal was achieved by Riezler et al (2000), Charniak and Johnson (2005) and Goldberg (2013) in the context of loglinearly trained probability models."}, {"heading": "4 Related Work", "text": "The training of greedy parsers to produce results outside of gold, facilitated by dynamic oracles, has been studied by several researchers in different ways (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Honnibal et al., 2013; Honnibal and Johnson, 2014; 5We report on the performance of these parsers in the most comparable constellation, that is, with beam size 1 or greedy search. Go \u0301 mez-Rodr\u00ed guez et al., 2014; Bjo \ufffd rkelund Nivre, 2015; Tokgo \ufffd z and Eryig, 2015; Go \ufffd mez-Rodr\u00ed guez and Ferna \ufffd ndez-Gonza \ufffd lez, 2015; Vaswani and Sagae, 2016). More generally, the training of greedy search systems by devoting attention to the expected classified behavior during the test period to learning and learning in search."}, {"heading": "5 Conclusions", "text": "Dyer et al. (2015) introduced stack LSTMs and used them to implement a transition-based dependency parser. The parser uses a greedy learning strategy that potentially provides a very high parsing speed and yet achieves state-of-the-art results. We have shown that improvement is achieved by training the greedy parser for non-gold results; dynamic oracles improve the stack LSTM parser by reaching 93.56 UAS for English and maintaining the greedy search."}, {"heading": "Acknowledgments", "text": "This work was partly sponsored by the US Army Research Laboratory and the US Army Research Office under contract number W911NF-101-0533 and partly by the NSF CAREER Grant IIS1054319. Miguel Ballesteros was supported by the European Commission under contract number FP7-ICT-610411 (Project MULTISENSOR) and H2020-RIA-645012 (Project KRISTINA). Yoav Goldberg is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), a Google Research Award and the Israeli Science Foundation (grant number 1555 / 15)."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Abbeel", "Ng2004] Pieter Abbeel", "Andrew Y. Ng"], "venue": "In Proc. of ICML", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "Globally normalized transition-based neural networks", "author": ["Andor et al.2016] Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": "In Proc. of ACL", "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks. arXiv:1506.03099", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Non-deterministic oracles for unrestricted non-projective transition-based dependency parsing", "author": ["Bj\u00f6rkelund", "Nivre2015] Anders Bj\u00f6rkelund", "Joakim Nivre"], "venue": "In Proc. of IWPT", "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2015}, {"title": "Learning to search better than your teacher", "author": ["Chang et al.2015] Kai-Wei Chang", "Akshay Krishnamurthy", "Alekh Agarwal", "Hal Daume", "John Langford"], "venue": "In Proc. of ICML", "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Coarse-to-fine n-best parsing and MaxEnt discriminative reranking", "author": ["Charniak", "Johnson2005] Eugene Charniak", "Mark Johnson"], "venue": "In Proc. of ACL", "citeRegEx": "Charniak et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Charniak et al\\.", "year": 2005}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D. Manning"], "venue": "In Proc. of EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["III Daum\u00e9", "III Marcu2005] Hal Daum\u00e9", "Marcu. Daniel"], "venue": "In Proc. of ICML", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2005}, {"title": "Search-based structured prediction", "author": ["John Langford", "Daniel Marcu"], "venue": "Machine Learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proc. of ACL", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Training structural SVMs when exact inference is intractable", "author": ["Finley", "Joachims2008] T. Finley", "T. Joachims"], "venue": "Proc. of ICML", "citeRegEx": "Finley et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finley et al\\.", "year": 2008}, {"title": "A dynamic oracle for arc-eager dependency parsing", "author": ["Goldberg", "Nivre2012] Yoav Goldberg", "Joakim Nivre"], "venue": "In Proc. of COLING", "citeRegEx": "Goldberg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2012}, {"title": "Training deterministic parsers with nondeterministic oracles. Transactions of the Association for Computational Linguistics, 1:403\u2013414", "author": ["Goldberg", "Nivre2013] Yoav Goldberg", "Joakim Nivre"], "venue": null, "citeRegEx": "Goldberg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2013}, {"title": "A tabular method for dynamic oracles in transition-based parsing. Transactions of the association", "author": ["Francesco Sartorio", "Giorgio Satta"], "venue": null, "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "Dynamic-oracle transition-based parsing with calibrated probabilistic output", "author": ["Yoav Goldberg"], "venue": "In Proc. of IWPT", "citeRegEx": "Goldberg.,? \\Q2013\\E", "shortCiteRegEx": "Goldberg.", "year": 2013}, {"title": "A polynomial-time dynamic oracle for non-projective dependency parsing", "author": ["Francesco Sartorio", "Giorgio Satta"], "venue": "In Proc. of EMNLP", "citeRegEx": "G\u00f3mez.Rodr\u0131\u0301guez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "G\u00f3mez.Rodr\u0131\u0301guez et al\\.", "year": 2014}, {"title": "Imitation learning by coaching", "author": ["He et al.2012] He He", "Hal Daum\u00e9 III", "Jason Eisner"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Joint incremental disfluency detection and dependency parsing. Transactions of the Association for Computational Linguistics, 2:131\u2013142", "author": ["Honnibal", "Johnson2014] Matthew Honnibal", "Mark Johnson"], "venue": null, "citeRegEx": "Honnibal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Honnibal et al\\.", "year": 2014}, {"title": "A non-monotonic arc-eager", "author": ["Yoav Goldberg", "Mark Johnson"], "venue": null, "citeRegEx": "Honnibal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Honnibal et al\\.", "year": 2013}, {"title": "Dynamic programming algorithms for transition-based dependency parsers", "author": ["Carlos G\u00f3mezRodr\u0131\u0301guez", "Giorgio Satta"], "venue": "In Proc. of ACL", "citeRegEx": "Kuhlmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kuhlmann et al\\.", "year": 2011}, {"title": "Structured learning with approximate inference", "author": ["Kulesza", "Pereira2007] A. Kulesza", "F. Pereira"], "venue": null, "citeRegEx": "Kulesza et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kulesza et al\\.", "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Lecun et al.1998] Yann Lecun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Polyhedral outer approximations with application to natural language parsing", "author": ["Noah A. Smith", "Eric P. Xing"], "venue": "In Proc. of ICML", "citeRegEx": "Martins et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2009}, {"title": "Pseudo-projective dependency parsing", "author": ["Nivre", "Nilsson2005] Joakim Nivre", "Jens Nilsson"], "venue": "In Proc. of ACL", "citeRegEx": "Nivre et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2005}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre"], "venue": "In Proc. of IWPT", "citeRegEx": "Nivre.,? \\Q2003\\E", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre"], "venue": "In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together", "citeRegEx": "Nivre.,? \\Q2004\\E", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Joakim Nivre"], "venue": "Computational Linguistics,", "citeRegEx": "Nivre.,? \\Q2008\\E", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "Sequence level training with recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "In Proc. of ICLR", "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Lexicalized stochastic modeling of constraint-based grammars using loglinear measures and em training", "author": ["Detlef Prescher", "Jonas Kuhn", "Mark Johnson"], "venue": "In Proc. of ACL", "citeRegEx": "Riezler et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2000}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross et al.2011] St\u00e9phane Ross", "Geoffrey J. Gordon", "J. Andrew Bagnell"], "venue": "In Proc. of AISTAT", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Minimum risk training for neural machine translation", "author": ["Shen et al.2015] Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proc. of ACL", "citeRegEx": "Shen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Efficient structured inference for transition-based parsing with neural networks and error states. Transactions of the Association for Computational Linguistics, 4:183\u2013196", "author": ["Vaswani", "Sagae2016] Ashish Vaswani", "Kenji Sagae"], "venue": null, "citeRegEx": "Vaswani et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "An investigation of imitation learning algorithms for structured prediction", "author": ["Andreas Vlachos"], "venue": "In Proc. of the European Workshop on Reinforcement Learning", "citeRegEx": "Vlachos.,? \\Q2012\\E", "shortCiteRegEx": "Vlachos.", "year": 2012}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Yamada", "Matsumoto2003] Hiroyasu Yamada", "Yuji Matsumoto"], "venue": "In Proc. of IWPT", "citeRegEx": "Yamada et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2003}, {"title": "Incremental recurrent neural network dependency parser with search-based discriminative training", "author": ["Yazdani", "Henderson2015] Majid Yazdani", "James Henderson"], "venue": "In Proc. of CoNLL", "citeRegEx": "Yazdani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yazdani et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "We adapt the greedy stack LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles (Goldberg and Nivre, 2013) instead of assuming an error-free action history.", "startOffset": 52, "endOffset": 71}, {"referenceID": 9, "context": "Dyer et al. (2015) presented a parser in which the parser\u2019s unbounded state is embedded in a fixeddimensional continuous space using recurrent neu-", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "Our departure point is the parsing model described by Dyer et al. (2015). We do not describe the model", "startOffset": 54, "endOffset": 73}, {"referenceID": 9, "context": "The definition of what constitutes a \u201ccorrect\u201d action is the major difference between a static oracle as used by Dyer et al. (2015) and the dynamic oracle explored here.", "startOffset": 113, "endOffset": 132}, {"referenceID": 8, "context": "Since the parser is likely to make mistakes at test time and encounter states it has not seen during training, this training criterion is problematic (Daum\u00e9 III et al., 2009; Ross et al., 2011; Goldberg and Nivre, 2012; Goldberg and Nivre, 2013, inter alia). Instead, we would prefer to train the parser to behave optimally even after making a mistake (under the constraint that it cannot backtrack or fix any previous decision). We thus need to include in the training examples states that result from wrong parsing decisions, together with the optimal transitions to take in these states. To this end we reconsider which training examples to show, and what it means to behave optimally on these training examples. The framework of training with exploration using dynamic oracles suggested by Goldberg and Nivre (2012; 2013) provides answers to these questions. While the application of dynamic oracle training is relatively straightforward, some adaptations were needed to accommodate the probabilistic training objective. These adaptations mostly follow Goldberg (2013).", "startOffset": 157, "endOffset": 1073}, {"referenceID": 14, "context": "Goldberg and Nivre (2013) define the arcdecomposition property of transition systems, and show how to derive efficient dynamic oracles for transition systems that are arc-decomposable.", "startOffset": 0, "endOffset": 26}, {"referenceID": 13, "context": "While it is possible to compute dynamic oracles for the arc-standard system (Goldberg et al., 2014), the computation relies on a dynamic programming algorithm which is polynomial in the length of the stack.", "startOffset": 76, "endOffset": 99}, {"referenceID": 19, "context": "We chose instead to switch to the arc-hybrid transition system (Kuhlmann et al., 2011), which is very similar to the arc-standard system but is arc-decomposable and hence admits an efficient O(1) dynamic oracle, resulting in only negligible increase to training runtime.", "startOffset": 63, "endOffset": 86}, {"referenceID": 11, "context": "While it is possible to compute dynamic oracles for the arc-standard system (Goldberg et al., 2014), the computation relies on a dynamic programming algorithm which is polynomial in the length of the stack. As the dynamic oracle has to be queried for each parser state seen during training, the use of this dynamic oracle will make the training runtime several times longer. We chose instead to switch to the arc-hybrid transition system (Kuhlmann et al., 2011), which is very similar to the arc-standard system but is arc-decomposable and hence admits an efficient O(1) dynamic oracle, resulting in only negligible increase to training runtime. We implemented the dynamic oracle to the arc-hybrid system as described by Goldberg (2013).", "startOffset": 77, "endOffset": 737}, {"referenceID": 14, "context": "A similar objective was used by Riezler et al (2000), Charniak and Johnson (2005) and Goldberg (2013) in the context of log-linear probabilistic models.", "startOffset": 86, "endOffset": 102}, {"referenceID": 1, "context": "Y\u201915 and A\u201916 are beam = 1 parsers from Yazdani and Henderson (2015) and Andor et al. (2016), respectively.", "startOffset": 73, "endOffset": 93}, {"referenceID": 1, "context": "A\u201916-beam is the parser with beam larger than 1 by Andor et al. (2016). Bold numbers indicate the best results among the greedy", "startOffset": 51, "endOffset": 71}, {"referenceID": 1, "context": "In order to be able to compare with similar greedy parsers (Yazdani and Henderson, 2015; Andor et al., 2016)5 we report the performance of the parser on the multilingual treebanks of the CoNLL 2009 shared task (Haji\u010d et al.", "startOffset": 59, "endOffset": 108}, {"referenceID": 1, "context": "In order to be able to compare with similar greedy parsers (Yazdani and Henderson, 2015; Andor et al., 2016)5 we report the performance of the parser on the multilingual treebanks of the CoNLL 2009 shared task (Haji\u010d et al., 2009). Since some of the treebanks contain nonprojective sentences and arc-hybrid does not allow nonprojective trees, we use the pseudo-projective approach (Nivre and Nilsson, 2005). We used predicted partof-speech tags provided by the CoNLL 2009 shared task organizers. We also include results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training setup as Dyer et al. (2015); for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we", "startOffset": 89, "endOffset": 650}, {"referenceID": 32, "context": "More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learningto-search frameworks (Abbeel and Ng, 2004; Daum\u00e9 III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum\u00e9 III et al., 2009; Ross et al., 2011; Chang et al., 2015).", "startOffset": 200, "endOffset": 343}, {"referenceID": 16, "context": "More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learningto-search frameworks (Abbeel and Ng, 2004; Daum\u00e9 III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum\u00e9 III et al., 2009; Ross et al., 2011; Chang et al., 2015).", "startOffset": 200, "endOffset": 343}, {"referenceID": 29, "context": "More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learningto-search frameworks (Abbeel and Ng, 2004; Daum\u00e9 III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum\u00e9 III et al., 2009; Ross et al., 2011; Chang et al., 2015).", "startOffset": 200, "endOffset": 343}, {"referenceID": 4, "context": "More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learningto-search frameworks (Abbeel and Ng, 2004; Daum\u00e9 III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum\u00e9 III et al., 2009; Ross et al., 2011; Chang et al., 2015).", "startOffset": 200, "endOffset": 343}, {"referenceID": 2, "context": "ing (Bengio et al., 2015), expected loss training (Shen et al.", "startOffset": 4, "endOffset": 25}, {"referenceID": 30, "context": ", 2015), expected loss training (Shen et al., 2015), and reinforcement learning have been proposed (Ranzato et al.", "startOffset": 32, "endOffset": 51}, {"referenceID": 27, "context": ", 2015), and reinforcement learning have been proposed (Ranzato et al., 2016).", "startOffset": 55, "endOffset": 77}, {"referenceID": 1, "context": "search offers an alternative solution to the problems with greedy search (Andor et al., 2016), and has", "startOffset": 73, "endOffset": 93}, {"referenceID": 22, "context": "been analyzed as well (Kulesza and Pereira, 2007; Finley and Joachims, 2008), including for parsing (Martins et al., 2009).", "startOffset": 100, "endOffset": 122}], "year": 2016, "abstractText": "We adapt the greedy stack LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles (Goldberg and Nivre, 2013) instead of assuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser.", "creator": "LaTeX with hyperref package"}}}