{"id": "1603.03876", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2016", "title": "Variational Neural Discourse Relation Recognizer", "abstract": "Implicit discourse relation recognition is a crucial component for automatic discourse-level analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse relation recognizer. We refer to this model as VIRILE. VIRILE establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse. In order to perform efficient inference and learning, we introduce a neural discourse relation model to approximate the posterior of the latent variable, and employ this approximated posterior to optimize a reparameterized variational lower bound. This allows VIRILE to be trained with standard stochastic gradient methods. Experiments on the benchmark data set show that VIRILE can achieve competitive results against state-of-the-art baselines.", "histories": [["v1", "Sat, 12 Mar 2016 09:11:30 GMT  (477kb,D)", "http://arxiv.org/abs/1603.03876v1", "7 pages"], ["v2", "Sun, 25 Sep 2016 23:33:44 GMT  (452kb,D)", "http://arxiv.org/abs/1603.03876v2", "10 pages, accepted at emnlp 2016"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["biao zhang", "deyi xiong", "jinsong su", "qun liu", "rongrong ji", "hong duan", "min zhang"], "accepted": true, "id": "1603.03876"}, "pdf": {"name": "1603.03876.pdf", "metadata": {"source": "CRF", "title": "Variational Neural Discourse Relation Recognizer", "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su"], "emails": ["zb@stu.xmu.edu.cn,", "jssu@xmu.edu.cn", "dyxiong@suda.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Work", "text": "There are two lines of research related to our work: implicit discourse relationship recognition and variational neural models, which we describe successively. Implicit discourse relationship insights Due to the publication of Penn Discourse Treebank [Prasad et al., 2008] corpus, ever increasing efforts are being made to implicit DRR. In this corpus, Pilter et al. [2009] exploit several linguistically informed traits, such as polarity labels, modality and lexical traits. Lin et al. [2009] further integrate contextual words, word pairs and discourse analyses into their classification. Following this direction, several more powerful traits were exploited: entities [Louis et al., 2010], word embedding and lexical traits. [Braud and Denis, 2015], Brown cluster pairs and co-reference pattern patterns that refer to each other [Ruford et al., 2014 and Xue]."}, {"heading": "3 Background: Variational Autoencoder", "text": "In this section, we briefly consider the variable autoencoder (UAE) [Kingma and Welling, 2014 | Rezende et al., 2014], one of the most classic variable neural models that forms the basis of our model.Unlike conventional neural autoencoders, VAE is a generative model that can be considered a regulated version of the standard autoencoder. The UAE significantly alters the autoencoder architecture by introducing a latent random variable z, designed to capture the variations in the observed variable x. By including z, the common distribution is formulated as follows: p\u03b8 (x, z) = p\u03b8 (x | z) pctuation (z) pctuable (1), where the previous variable z is over the latent variable z, which is usually equipped with a simple Gaussian distribution; and pctuant (x | z) is the variable that modulates the probability."}, {"heading": "4 The VIRILE Model", "text": "This section presents our proposed VIRILE model. Formally, in VIRILE there are two observed variables, x for a discourse and y for the corresponding relationship, and a latent variable. As shown in Figure 1, the common distribution of the three variables is formulated as follows: p\u03b8 (x, y, z) = p\u03b8 (x, y | z) p\u03b8 (z) (4). We begin with this distribution to elaborate the main components in VIRILE."}, {"heading": "4.1 Neural Discourse Recognizer", "text": "The conditional distribution p (x, y | z) in Eq. (4) shows that both discourse arguments and the corresponding relation are generated by the latent variable. As shown in Figure 1, x is separated from y by z. Therefore, discourse x and the corresponding relation y are independent of the latent variable. Thus, the common probability can be formulated as the following variable, following model (x, y, z) = \"p\" (x, z) after previous work [Kingma and Welling, 2014; Rezende et al., 2014]. With regard to the two conditional distributions, we parameterise them via the neuronal networks as shown in Figure 2.Before explaining the network structure further, it is necessary to briefly introduce how discourse relations are represented in our education. We assume that we represent two conditional distributions between us."}, {"heading": "4.2 Neural Posterior Approximator", "text": "For the common distribution in Equation (5), we can define a lower limit of variation similar to Equation (2). However, given the absence of y during the recognition of discourse relations, we assume that the latent variable can be derived solely from discourse arguments x. This allows us to use q\u03c6 (z | x) instead of q\u03c6 (z | x, y) to approximate the true lower limit. Similar to previous work [Kingma and Welling, 2014; Rezende et al., 2014], we allow q\u03c6 (z | x) to be a multivariate Gaussian distribution with a diagonal covariance structure: q\u03c6 (z | x) = N (z; \u00b5, \u03c32I) (13), where the mean \u00b5 and s.d. \u00b2 rear output of the approximate network horizontal covariance structure can be a variance structure."}, {"heading": "4.3 Variational Reparameterization", "text": "In order to optimize our model, we must continue to calculate an expectation about the approximate posterior, i.e. Eq\u03c6 (z | x) [log p\u03b8 (x, y | z)]. Since this expectation is insoluble, we apply the Monte Carlo method to evaluate it with a repair measurement trick similar to the equation. (3): Equality (z | x) [log p\u03b8 (x, y | z)] '1L L L L-L-L-L-L-L-L-L-L (x, y | z-L), where z-N (0, I) (18) is the number of samples. This repair measurement bridges the gap between the probability and the posterior and allows internal backpropagation in our neural network. When checking new instances, we simply ignore the proposed model in order to avoid uncertainty."}, {"heading": "4.4 Parameter Learning", "text": "In view of a training instance (x (t), y (t)), the common training objective is defined as follows: L (\u03b8, \u03c6) '1 2 dz \u2211 j = 1 (1 + log (\u03c3 (t) j) 2) \u2212 (\u00b5 (t) j) 2 \u2212 (\u03c3 (t) j) 2) + 1L L \u2211 l = 1 log p\u03b8 (x (t), y (t) | z (t, l)), where z (t, l) = p (t) + p (t) (l) and (l) \u0445 N (0, I) (19) The first term is the KL divergence, which can be calculated and differentiated without estimation (see [Kingma and Welling, 2014] for details). Intuitively, it is a conventional neural network with a special regulator. The second term is the approximate expectation shown in Eq. (18)."}, {"heading": "5 Experiments", "text": "We performed a series of experiments on the implicit DRR task in English to validate the effectiveness of VIRILE. First, we briefly review the PDTB dataset we used to train our model. In this section, we then present the setup, results, and analysis of the variable lower limit."}, {"heading": "5.1 Dataset", "text": "We used the largest hand-annotated discourse corpus PDTB 2.02 [Prasad et al., 2008] (PDTB hereinafter), which contains discourse commentaries on 2,312 articles in the Wall Street Journal and is divided into several sections. Following previous work [Pitler et al., 2009; Zhou et al., 2010; Lan et al., 2013; Zhang et al., 2015], we used Sections 2-20 as a training sentence, Sections 21-22 as a test sentence. Sections 0-1 were used as a development thesis for hyperparameter optimization. In the PDTB, discourse relations are commented in a predicate-rubber view. Each discourse is treated as a predicate that takes two spans of text as arguments. Discourse relations in the PDTB are arranged in a three-step hierarchy in which the top-level discourse disformations consist of four major semantic classes: COMPORTINAL (Problems) and COMPORTINTINENT (Problems)."}, {"heading": "5.2 Setup", "text": "For optimization, we used the Adagrad algorithm to update the parameters. In terms of the hyperparameters M, L, A and the dimensionality of all vector representations, we set them according to previous work [Kingma and Welling, 2014; Rezende et al., 2014] and preliminary experiments on the development theorem. Finally, we set M = 100, A = 1000, L = 1, dz = 20, dx1 = dx2 = 10001, dh1 = dh2 = dh \u2032 1 = dh \u2032 2 = 400, dy = 2 for all experiments. Note that there is a dimension in dx1 and dx2 for unknown words. We compared VIRILE with the following two different basic methods: \u2022 SVM: a support vector machine (SVM) trained with several manual functions."}, {"heading": "5.3 Classification Results", "text": "Since the development and testing rates are unbalanced in terms of the ratio of positive and negative instances, we chose F1 score as our main evaluation benchmarks. In addition, we have also provided precision, recall and accuracy metrics for further analysis. Table 2 summarizes the -classification results, with the highest F1 score in four tasks in bold.72 1872 772 101, we note that the proposed VIRILE SVM in terms of EXP / TEM and SCNN in terms of EXP / COM in terms of its results from CON, VIRILE in terms of four tasks in bold.72 achieves the best result in terms of EXP. Overall, VIRILE is competitive in terms of comparison with the two states-of-the-art baselines.12."}, {"heading": "5.4 Variational Lower Bound Analysis", "text": "In addition to the classification performance, efficiency of learning and conclusion is another concern of the variation methods. Figure 4 shows the training sequence for four tasks with regard to the lower limit of variation of the training set. We also provide F1 values on the development set to investigate the relationships between the lower limit of variation and recognition performance. We note that in all experiments (within 100 epochs), our model moves considerably quickly towards the lower limit of variation, which is consistent with the previous results [Kingma and Welling, 2014; Rezende et al., 2014]. However, the change trend of the F1 value does not follow the lower limit. In particular, for the four discourse relationships, we continue to observe that the change paths of the F1 value are completely different, which could indicate that the four discourse relationships exhibit different properties and distributions. In particular, the number of epochs in which the best F1 value is reached, the four relations imply that the four are also different for the DRs."}, {"heading": "6 Conclusion and Future Work", "text": "Unlike traditional discriminatory models, which directly calculate the conditional probability of the relationship y given discourse arguments x, our model assumes that it is a latent variable from an underlying semantic space that generates both x and y. In order to make the conclusion and learning efficient, we introduce a neural discourse recognition mechanism and a neural posterior approximator as our generative or inference model. Using repair ametrization technology, we are able to optimize the entire model using a standard stochastic gradient ascendant algorithm. Experimental results regarding classification and variable lower limits verify the effectiveness of our model. In the future, we would like to exploit the use of discourse instances with explicit relationships for implicit DRR. For this, we can assume two directions: 1) the conversion of explicit instances in our model implicit instances and the other implicit instances are also interested in the reasoning of our model's implicit nature."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Implicit discourse relation recognition is a crucial component for automatic discourse-level analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse relation recognizer. We refer to this model as VIRILE. VIRILE establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse. In order to perform efficient inference and learning, we introduce a neural discourse relation model to approximate the posterior of the latent variable, and employ this approximated posterior to optimize a reparameterized variational lower bound. This allows VIRILE to be trained with standard stochastic gradient methods. Experiments on the benchmark data set show that VIRILE can achieve competitive results against state-of-the-art baselines.", "creator": "LaTeX with hyperref package"}}}