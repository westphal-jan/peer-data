{"id": "1603.04351", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2016", "title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations", "abstract": "We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese.", "histories": [["v1", "Mon, 14 Mar 2016 17:18:27 GMT  (35kb)", "http://arxiv.org/abs/1603.04351v1", null], ["v2", "Thu, 26 May 2016 09:45:18 GMT  (97kb,D)", "http://arxiv.org/abs/1603.04351v2", null], ["v3", "Wed, 20 Jul 2016 15:17:29 GMT  (205kb,D)", "http://arxiv.org/abs/1603.04351v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["eliyahu kiperwasser", "yoav goldberg"], "accepted": true, "id": "1603.04351"}, "pdf": {"name": "1603.04351.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["elikip@gmail.com", "yoav.goldberg@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.04 351v 1 [cs.C L] 14 M"}, {"heading": "1 Introduction", "text": "In fact, it is as if most people are able to understand themselves and understand what it is all about, namely, the question of how they should behave and how they should behave. (...) In fact, it is as if people are able to understand themselves. (...) It is as if people are able to understand themselves. (...) It is as if they are able to understand themselves. (...) It is as if people are able to understand themselves. (...) It is as if they are able to understand themselves. (...) It is as if they are able to understand what they are doing. (...) It is as if they are able to understand themselves. (...) It is as if they are able to understand themselves. (...) It is as if they are able to do it, as if they do it, if they do it, if they do it, if they do it, if they do it, if they do it. (...) It is as if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it."}, {"heading": "2 Background and Notation", "text": "Notation We use x1: n to denote a sequence of n vectors x1, \u00b7 \u00b7, xn. F\u03b8 (\u00b7) is a function parameterized with parameters \u03b8. We write FL (\u00b7) as an abbreviation for F\u03b8L - an instance of F with a certain set of parameters \u03b8L. We use \u043c to denote a vector concatenation, and v [i] to denote an indexing operation that takes the ith element of a vector v."}, {"heading": "2.1 Feature Functions in Dependency Parsing", "text": "Traditionally, state-of-the-art parsers rely on linear models via handmade feature functions. The feature functions consider core components (e.g. \"word on stack,\" \"left child of second totop word on stack,\" \"distance between head and modifier words\") and consist of several templates in which each template instantiates a binary indicator function via a combination of core elements (resulting in characteristics of the form \"word on stack is X and left child is Y and..\"). The design of the feature function - which components to consider and which combinations of components to include - is a major challenge in parser design. Once a good feature function is proposed in a paper, it is usually adopted in later work and sometimes optimized to improve performance (examples of good feature functions are those of Zhang and Nivre (2011) for the transition-based parser (including coarse 20 core components and 72 templates for most of McDonald's features)."}, {"heading": "2.2 Related Research Efforts", "text": "In fact, most of them are able to go in search of a solution that has its origins in the past."}, {"heading": "2.3 Bidirectional Recurrent Neural Networks", "text": "An RNN allows us to model the element in the sequence that is based on the past - the elements that are able to count, as well as complex phenomena such as bracketing and code indentation (Karpathy et al., 2015). Our proposed feature extractors are based on a bidirectional neural network (BiRNN), an extension of RNNs that takes into account both the past and the future: n We use a specific flavor of RNN called long-term storage network (LSTM)."}, {"heading": "3 Our Approach", "text": "We propose to replace the handmade feature functions in favor of minimally defined feature functions that use automatically learned bidirectional LSTM representations. Given an n word input set s with words w1,.., wn along with the corresponding POS tags t1,.., tn, 2, we associate each word wi and POS ti with the embedding of vectors e (wi) and e (ti), and create a sequence of input vectors x1: n in which each xi is a concatenation of the corresponding word and POS vectors: xi = e (wi) and e (pi) The embedding is trained along with the model. This encodes each word in sequence, without considering its context. We then introduce context by representing each input element as its (deep) BiLSTM vector, vi: vi = BILSTM = vector x, i) Our feature function is a concatenation of a small number of STM vectors."}, {"heading": "4 Transition-based Parser", "text": "We start by reading the final configuration. In a greedy parser, the transition to each sentence is chosen. We follow the notation in (Goldberg and Nivre, 2013). The transition-based parsing framework assumes a transition system based on the input set, and transitions are repeatedly applied to this configuration. After a finite number of transitions, the system is 3While the BiLSTM configuration is fairly efficient, as it is when it uses a GPU encoding that can run over many sentences in parallel, its computation costs are almost neglible.comes at a final configuration, and a final configuration tree reads the final configuration. In a greedy parser, a classifier is used to select the transition."}, {"heading": "4.1 Details of the Training Algorithm", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "5 Graph-based Parser", "text": "It's about asking what we need to do to enable the search for the Score feature, which refers to the sum of local scores for each portion of the Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Sc"}, {"heading": "6 Experiments and Results", "text": "We evaluated our parsing model in English and Chinese data. For comparison, we follow the setup of (Dyer et al., 2015).Data For English, we used the Stanford Dependency (SD) (de Marneffe et al., 2008) conversion of the Penn Treebank (Marcus et al., 1993), using the standard train / dev / test splitswith the same predicted POS tags as used in (Dyer et al., 2015; Chen and dev splits of (Zhang and Clark, 2008), this dataset al., 2015) with gold partof speech tags, also follows from the evaluation.For Chinese, we use the Penn Chinese Treebank 5.1 as used in (CTB5), using the train / test / dev words of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014).When using external beddings of the data."}, {"heading": "7 Conclusion", "text": "We presented a frustratingly effective approach to feature extraction for dependency parsing, based on a BiLSTM encoder that is trained with the parer, and demonstrated its effectiveness by integrating it into two simple parsing models: a greedy transition-based parser and a globally optimized firststorder graph-based parser that provides highly competitive parsing accuracies in both cases. Recognition This research is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) and the Israeli Science Foundation (grant number 1555 / 15)."}], "references": [{"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Mach. Learn., 28(1):41\u201375, July.", "citeRegEx": "Caruana.,? 1997", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740\u2013750, Doha, Qatar, October. Association for", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Natural language understanding with distributed representation", "author": ["Kyunghyun Cho."], "venue": "CoRR, abs/1511.07916.", "citeRegEx": "Cho.,? 2015", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "Stanford dependencies manual", "author": ["Marie-Catherine de Marneffe", "Christopher D. Manning."], "venue": "Technical report, Stanford University.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "TransitionBased Dependency Parsing with Stack Long ShortTerm Memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason Eisner."], "venue": "Proc. of COLING.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Finding Structure in Time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive Science, 14(2):179\u2013211, March.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "A dynamic oracle for the arc-eager system", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "Proc. of COLING 2012.", "citeRegEx": "Goldberg and Nivre.,? 2012", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2012}, {"title": "Training deterministic parsers with non-deterministic oracles", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "Transactions of the association for Computational Linguistics, 1.", "citeRegEx": "Goldberg and Nivre.,? 2013", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2013}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg."], "venue": "CoRR, abs/1510.00726.", "citeRegEx": "Goldberg.,? 2015", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["A. Graves."], "venue": "Ph.D. thesis, Technische Universit\u00e4t M\u00fcnchen.", "citeRegEx": "Graves.,? 2008", "shortCiteRegEx": "Graves.", "year": 2008}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Dynamic programming algorithms", "author": ["gio Satta"], "venue": null, "citeRegEx": "Satta.,? \\Q2011\\E", "shortCiteRegEx": "Satta.", "year": 2011}, {"title": "A tutorial on energy-based", "author": ["F. Huang"], "venue": null, "citeRegEx": "Huang.,? \\Q2006\\E", "shortCiteRegEx": "Huang.", "year": 2006}, {"title": "Low-rank tensors for scor", "author": ["Tommi Jaakkola"], "venue": null, "citeRegEx": "Jaakkola.,? \\Q2014\\E", "shortCiteRegEx": "Jaakkola.", "year": 2014}, {"title": "Building a large annotated corpus of English: The penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marchinkiewicz."], "venue": "Computational Linguistics, 19.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Concise integer linear programming formulations for dependency parsing", "author": ["Andre Martins", "Noah Smith", "Eric Xing."], "venue": "Proc. ACL/AFNLP.", "citeRegEx": "Martins et al\\.,? 2009", "shortCiteRegEx": "Martins et al\\.", "year": 2009}, {"title": "Turning on the turbo: Fast third-order nonprojective turbo parsers", "author": ["Andre Martins", "Miguel Almeida", "Noah A. Smith."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 617\u2013622,", "citeRegEx": "Martins et al\\.,? 2013", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proc. of ACL.", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Discriminative Training and Spanning Tree Algorithms for Dependency Parsing", "author": ["Ryan McDonald."], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "McDonald.,? 2006", "shortCiteRegEx": "McDonald.", "year": 2006}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre."], "venue": "Incremental Parsing: Bringing Engineering and Cognition Together, ACL-Workshop.", "citeRegEx": "Nivre.,? 2004", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Algorithms for Deterministic Incremental Dependency Parsing", "author": ["Joakim Nivre."], "venue": "Computational Linguistics, 34(4):513\u2013553, December.", "citeRegEx": "Nivre.,? 2008", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "An Effective Neural Network Model for Graph-based Dependency Parsing", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-", "citeRegEx": "Pei et al\\.,? 2015", "shortCiteRegEx": "Pei et al\\.", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "Kuldip K. Paliwal."], "venue": "IEEE Transactions on Signal Processing, 45(11):2673\u20132681, November.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Learning structured prediction models: a large margin approach", "author": ["Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin."], "venue": "Proceedings of the 22nd international conference on Machine learning, ICML \u201905, pages 896\u2013903, New York, NY, USA.", "citeRegEx": "Taskar et al\\.,? 2005", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Template kernels for dependency parsing", "author": ["Hillel Taub-Tabib", "Yoav Goldberg", "Amir Globerson."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Taub.Tabib et al\\.,? 2015", "shortCiteRegEx": "Taub.Tabib et al\\.", "year": 2015}, {"title": "Structured Training for Neural Network Transition-Based Parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beamsearch", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proc. of EMNLP.", "citeRegEx": "Zhang and Clark.,? 2008", "shortCiteRegEx": "Zhang and Clark.", "year": 2008}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Yue Zhang", "Joakim Nivre."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188\u2013193.", "citeRegEx": "Zhang and Nivre.,? 2011", "shortCiteRegEx": "Zhang and Nivre.", "year": 2011}, {"title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network", "author": ["Chenxi Zhu", "Xipeng Qiu", "Xinchi Chen", "Xuanjing Huang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Graph-based parsers (McDonald, 2006) treat parsing as a search-based structured prediction problem in which the goal is learning a scoring func-", "startOffset": 20, "endOffset": 36}, {"referenceID": 20, "context": "Transition-based parsers (Nivre, 2004; Nivre, 2008) treat parsing as a sequence of actions that produce a parse tree, and a classifier is trained to score the possible actions at each stage of the process and guide the parsing process.", "startOffset": 25, "endOffset": 51}, {"referenceID": 21, "context": "Transition-based parsers (Nivre, 2004; Nivre, 2008) treat parsing as a sequence of actions that produce a parse tree, and a classifier is trained to score the possible actions at each stage of the process and guide the parsing process.", "startOffset": 25, "endOffset": 51}, {"referenceID": 19, "context": "Perhaps the simplest graph-based parsers are arc-factored (first order) models (McDonald, 2006), in which the scoring function for a tree decomposes over the individual arcs of the tree.", "startOffset": 79, "endOffset": 95}, {"referenceID": 16, "context": "More elaborate models look at larger (overlapping) parts, requiring more sophisticated inference and training algorithms (Martins et al., 2009; Koo and Collins, 2010).", "startOffset": 121, "endOffset": 166}, {"referenceID": 27, "context": "More advanced transition-based parsers introduce some search into the process using a beam (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010).", "startOffset": 91, "endOffset": 114}, {"referenceID": 1, "context": "2 for an overview) attempt to alleviate parts of the feature function design problem by moving from linear to non-linear models, enabling the modeler to focus on a small set of \u201ccore\u201d features and leaving it up to the machine-learning machinery to come up with good feature combinations (Chen and Manning, 2014; Pei et al., 2015; Lei et al., 2014; Taub-Tabib et al., 2015).", "startOffset": 287, "endOffset": 372}, {"referenceID": 22, "context": "2 for an overview) attempt to alleviate parts of the feature function design problem by moving from linear to non-linear models, enabling the modeler to focus on a small set of \u201ccore\u201d features and leaving it up to the machine-learning machinery to come up with good feature combinations (Chen and Manning, 2014; Pei et al., 2015; Lei et al., 2014; Taub-Tabib et al., 2015).", "startOffset": 287, "endOffset": 372}, {"referenceID": 25, "context": "2 for an overview) attempt to alleviate parts of the feature function design problem by moving from linear to non-linear models, enabling the modeler to focus on a small set of \u201ccore\u201d features and leaving it up to the machine-learning machinery to come up with good feature combinations (Chen and Manning, 2014; Pei et al., 2015; Lei et al., 2014; Taub-Tabib et al., 2015).", "startOffset": 287, "endOffset": 372}, {"referenceID": 1, "context": "For example, the work of (Chen and Manning, 2014) uses 18 different el-", "startOffset": 25, "endOffset": 49}, {"referenceID": 22, "context": "ements in its feature function, while the work of (Pei et al., 2015) uses 21 different elements.", "startOffset": 50, "endOffset": 68}, {"referenceID": 23, "context": "Our proposal (Section 3) is centered around BiRNNs (Irsoy and Cardie, 2014; Schuster and Paliwal, 1997), and more specifically BiLSTMs (Graves, 2008), which are strong and trainable sequence models (see Section 2.", "startOffset": 51, "endOffset": 103}, {"referenceID": 10, "context": "Our proposal (Section 3) is centered around BiRNNs (Irsoy and Cardie, 2014; Schuster and Paliwal, 1997), and more specifically BiLSTMs (Graves, 2008), which are strong and trainable sequence models (see Section 2.", "startOffset": 135, "endOffset": 149}, {"referenceID": 1, "context": "4 UAS), using a firstorder parser with two features and while training solely on Treebank data, without relying on semisupervised signals such as pre-trained word embeddings (Chen and Manning, 2014), word-clusters (Koo et al.", "startOffset": 174, "endOffset": 198}, {"referenceID": 26, "context": ", 2008), or techniques such as tri-training (Weiss et al., 2015).", "startOffset": 44, "endOffset": 64}, {"referenceID": 20, "context": "amples of good feature functions are the feature-set proposed by Zhang and Nivre (2011) for transition-", "startOffset": 75, "endOffset": 88}, {"referenceID": 19, "context": "based parsing (including roughly 20 core components and 72 feature templates), and the feature-set proposed by McDonald et al (2005) for graph-based parsing, with the paper listing 18 templates for a first-order parser, and the MSTParser\u2019s first-order", "startOffset": 111, "endOffset": 133}, {"referenceID": 1, "context": "Chen and Manning (2014) encode each core feature of a greedy transition-based parser as a dense low-dimensional vector, and the vectors are then concatenated and fed into a nonlinear classifier (multi-layer perceptron) which can potentially capture arbitrary feature combinations.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Chen and Manning (2014) encode each core feature of a greedy transition-based parser as a dense low-dimensional vector, and the vectors are then concatenated and fed into a nonlinear classifier (multi-layer perceptron) which can potentially capture arbitrary feature combinations. Weiss et al (2015) showed further gains using the same approach coupled with a somewhat improved set of core features, a more involved network architecture with skip-layers, beam search-decoding, and careful hyper-parameter tuning.", "startOffset": 0, "endOffset": 300}, {"referenceID": 1, "context": "Chen and Manning (2014) encode each core feature of a greedy transition-based parser as a dense low-dimensional vector, and the vectors are then concatenated and fed into a nonlinear classifier (multi-layer perceptron) which can potentially capture arbitrary feature combinations. Weiss et al (2015) showed further gains using the same approach coupled with a somewhat improved set of core features, a more involved network architecture with skip-layers, beam search-decoding, and careful hyper-parameter tuning. Pei et al (2015)", "startOffset": 0, "endOffset": 530}, {"referenceID": 1, "context": "resentation in (Chen and Manning, 2014) is a concatenation of 18 word vectors, 18 POS vectors and 12 dependency-label vectors.", "startOffset": 15, "endOffset": 39}, {"referenceID": 9, "context": "Finally, in Kiperwasser and Goldberg (2016) we present an Easy-First parser based on a novel hierarchical-LSTM tree encoding.", "startOffset": 28, "endOffset": 44}, {"referenceID": 9, "context": "For further details on RNNs and LSTMs, the reader is referred to (Goldberg, 2015; Cho, 2015).", "startOffset": 65, "endOffset": 92}, {"referenceID": 2, "context": "For further details on RNNs and LSTMs, the reader is referred to (Goldberg, 2015; Cho, 2015).", "startOffset": 65, "endOffset": 92}, {"referenceID": 6, "context": "Historical Notes RNNs were introduced by Elamn (Elman, 1990), and extended to BiRNNs by (Schuster and Paliwal, 1997).", "startOffset": 47, "endOffset": 60}, {"referenceID": 23, "context": "Historical Notes RNNs were introduced by Elamn (Elman, 1990), and extended to BiRNNs by (Schuster and Paliwal, 1997).", "startOffset": 88, "endOffset": 116}, {"referenceID": 11, "context": "The LSTM variant of RNNs is due to (Hochreiter and Schmidhuber, 1997).", "startOffset": 35, "endOffset": 69}, {"referenceID": 6, "context": "Historical Notes RNNs were introduced by Elamn (Elman, 1990), and extended to BiRNNs by (Schuster and Paliwal, 1997). The LSTM variant of RNNs is due to (Hochreiter and Schmidhuber, 1997). BiLSTMs were recently popularized by Graves (2008), and deep BiRNNs were introduced to NLP by Irsoy and Cardie (2014), who used them for sequence tagging.", "startOffset": 48, "endOffset": 240}, {"referenceID": 6, "context": "Historical Notes RNNs were introduced by Elamn (Elman, 1990), and extended to BiRNNs by (Schuster and Paliwal, 1997). The LSTM variant of RNNs is due to (Hochreiter and Schmidhuber, 1997). BiLSTMs were recently popularized by Graves (2008), and deep BiRNNs were introduced to NLP by Irsoy and Cardie (2014), who used them for sequence tagging.", "startOffset": 48, "endOffset": 307}, {"referenceID": 21, "context": "We begin by integrating the feature extractor in a transition-based parser (Nivre, 2008).", "startOffset": 75, "endOffset": 88}, {"referenceID": 8, "context": "We follow the notation in (Goldberg and Nivre, 2013).", "startOffset": 26, "endOffset": 52}, {"referenceID": 20, "context": ", 2011), which is similar to the more popular arcstandard system (Nivre, 2004), but for which an efficient dynamic oracle is available (Goldberg and", "startOffset": 65, "endOffset": 78}, {"referenceID": 1, "context": "We follow Chen and Manning (2014) and replace the linear scoring model with an MLP.", "startOffset": 10, "endOffset": 34}, {"referenceID": 8, "context": "We perform error-exploration training using the dynamic-oracle defined in (Goldberg and Nivre, 2013).", "startOffset": 74, "endOffset": 100}, {"referenceID": 24, "context": "Graph-based parsing follows the common structured prediction paradigm (Taskar et al., 2005; McDonald et al., 2005):", "startOffset": 70, "endOffset": 114}, {"referenceID": 18, "context": "Graph-based parsing follows the common structured prediction paradigm (Taskar et al., 2005; McDonald et al., 2005):", "startOffset": 70, "endOffset": 114}, {"referenceID": 18, "context": "In this work, we focus on arc-factored graph based approach presented in (McDonald et al., 2005).", "startOffset": 73, "endOffset": 96}, {"referenceID": 5, "context": "Given the scores of the arcs the highest scoring projective tree can be efficiently found using Eisner\u2019s decoding algorithm (1996). McDonald et al and most subsequent work estimate the local score of an arc by a linear model parameterized by a weight vector w, and a feature function \u03c6(s, h,m) assigning sparse feature vector for an arc linking modifier m to head h.", "startOffset": 96, "endOffset": 131}, {"referenceID": 5, "context": "Given the scores of the arcs the highest scoring projective tree can be efficiently found using Eisner\u2019s decoding algorithm (1996). McDonald et al and most subsequent work estimate the local score of an arc by a linear model parameterized by a weight vector w, and a feature function \u03c6(s, h,m) assigning sparse feature vector for an arc linking modifier m to head h. We follow Pei et al (2015) and replace the linear scoring function with an MLP.", "startOffset": 96, "endOffset": 394}, {"referenceID": 18, "context": "We use a margin-based objective (McDonald et al., 2005; LeCun et al., 2006), aiming to maximize the margin between the score of the gold tree y and highest scoring incorrect tree y\u2032.", "startOffset": 32, "endOffset": 75}, {"referenceID": 0, "context": "This sharing of parameters can be seen as an instance of multi-task learning (Caruana, 1997).", "startOffset": 77, "endOffset": 92}, {"referenceID": 24, "context": "In order to remedy this, we found it useful to use loss augmented inference (Taskar et al., 2005).", "startOffset": 76, "endOffset": 97}, {"referenceID": 4, "context": "For comparison purposes we follow the setup of (Dyer et al., 2015).", "startOffset": 47, "endOffset": 66}, {"referenceID": 15, "context": "Data For English, we used the Stanford Dependency (SD) (de Marneffe and Manning, 2008) conversion of the Penn Treebank (Marcus et al., 1993), using the standard train/dev/test splitswith the same predicted POS-tags as used in (Dyer et al.", "startOffset": 119, "endOffset": 140}, {"referenceID": 4, "context": ", 1993), using the standard train/dev/test splitswith the same predicted POS-tags as used in (Dyer et al., 2015; Chen and Manning, 2014).", "startOffset": 93, "endOffset": 136}, {"referenceID": 1, "context": ", 1993), using the standard train/dev/test splitswith the same predicted POS-tags as used in (Dyer et al., 2015; Chen and Manning, 2014).", "startOffset": 93, "endOffset": 136}, {"referenceID": 27, "context": "1 (CTB5), using the train/test/dev splits of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof-speech tags, also following (Dyer et al.", "startOffset": 45, "endOffset": 87}, {"referenceID": 4, "context": "1 (CTB5), using the train/test/dev splits of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof-speech tags, also following (Dyer et al.", "startOffset": 45, "endOffset": 87}, {"referenceID": 4, "context": ", 2015) with gold partof-speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014).", "startOffset": 53, "endOffset": 96}, {"referenceID": 1, "context": ", 2015) with gold partof-speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014).", "startOffset": 53, "endOffset": 96}, {"referenceID": 4, "context": "When using external word embeddings, we also use the same data as (Dyer et al., 2015).", "startOffset": 66, "endOffset": 85}, {"referenceID": 20, "context": "The greedy transition based parser with 4 features also matches or outperforms most other parsers, including the beam-based transition parser with heavily engineered features of Zhang and Nivre (2011) and the Stack-LSTM parser of", "startOffset": 188, "endOffset": 201}, {"referenceID": 28, "context": "The different systems and the numbers reported from them are taken from: ZhangNivre11: (Zhang and Nivre, 2011); Martins13: (Martins et al.", "startOffset": 87, "endOffset": 110}, {"referenceID": 17, "context": "The different systems and the numbers reported from them are taken from: ZhangNivre11: (Zhang and Nivre, 2011); Martins13: (Martins et al., 2013); Weiss15 (Weiss et al.", "startOffset": 123, "endOffset": 145}, {"referenceID": 26, "context": ", 2013); Weiss15 (Weiss et al., 2015); Pei15: (Pei et al.", "startOffset": 17, "endOffset": 37}, {"referenceID": 22, "context": ", 2015); Pei15: (Pei et al., 2015); Dyer15 (Dyer et al.", "startOffset": 16, "endOffset": 34}, {"referenceID": 4, "context": ", 2015); Dyer15 (Dyer et al., 2015); LeZuidema14 (Le and Zuidema, 2014); Zhu15: (Zhu et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 29, "context": ", 2015); LeZuidema14 (Le and Zuidema, 2014); Zhu15: (Zhu et al., 2015).", "startOffset": 52, "endOffset": 70}], "year": 2016, "abstractText": "We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}