{"id": "1603.05106", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "One-Shot Generalization in Deep Generative Models", "abstract": "Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples---having seen new examples just once---providing an important class of general-purpose models for one-shot machine learning.", "histories": [["v1", "Wed, 16 Mar 2016 14:10:00 GMT  (6267kb,D)", "http://arxiv.org/abs/1603.05106v1", null], ["v2", "Wed, 25 May 2016 12:57:19 GMT  (77030kb,D)", "http://arxiv.org/abs/1603.05106v2", "8pgs, 1pg references, 1pg appendix, In Proceedings of the 33rd International Conference on Machine Learning, JMLR: W&amp;CP volume 48, 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["danilo jimenez rezende", "shakir mohamed", "ivo danihelka", "karol gregor", "daan wierstra"], "accepted": true, "id": "1603.05106"}, "pdf": {"name": "1603.05106.pdf", "metadata": {"source": "META", "title": "One-Shot Generalization in Deep Generative Models", "authors": ["Danilo J. Rezende", "Shakir Mohamed", "Ivo Danihelka", "Karol Gregor", "Daan Wierstra"], "emails": ["DANILOR@GOOGLE.COM", "SHAKIR@GOOGLE.COM", "DANIHELKA@GOOGLE.COM", "KAROLG@GOOGLE.COM", "WIERSTRA@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "In fact, the fact is that most of them are able to move to another world in which they are able to find themselves."}, {"heading": "2. Varieties of Attention", "text": "Observing parts of a scene, ignoring others, analyzing the parts on which we focus, and sequentially constructing an interpretation and understanding of a scene: these are natural parts of human cognition. This is such a successful strategy of reasoning that it is now also an important part of many machine learning systems, and this repeated process of attention and interpretation, analysis, and synthesis is an important component of the generative models we develop. In its most general form, any mechanism that allows us to selectively channel information from one part of our model to another can be viewed as an attentive mechanism. Attention allows a wide range of inventories to be included, with few additional parameters and low computational costs. Attention is most commonly used for classification tasks, showing that both scalability and generalization are improved (Larochelle & Hinton, 2010; Chikkerur et al., 2010; Jaderberg et al., 2015; Mnih, 2014). Attention."}, {"heading": "3. Iterative and Attentive Generative Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Latent Variable Models and Variational Inference", "text": "Generative models with latent variables describe the probability process by which an observed data point can be generated. The simplest formulations, such as PCA and factor analysis, use Gaussian latent variables z, which are linearly combined to generate Gaussian distributed data points x. In more complex models, the probabilistic description includes a hierarchy of L-planes of latent variables, each layer depending on the above layer in a nonlinear manner (Rezende et al., 2014) - for deep generative models, we specify this non-linear dependence using deep neural networks. To calculate the marginal probability of the data, we must integrate the unobserved variables over a particular layer: p (x) = p (z) p) dz (1) In Gaussian models, the previous distribution p (z) is a Gausserial distribution and the probability function p."}, {"heading": "3.2. Sequential Generative Models", "text": "The generative models as we have described them so far can be characterized as single-stage models, since they are models of, for example, data that evaluate their probability functions by transforming the latent variables by means of a nonlinear, forward-looking transformation. A sequential generative model is a natural extension of the latent variable models used in VAEs. It combines stochastic and deterministic calculations into a multi-stage generative process that uses recursive transformations of the latent variables, i.e. an internal state-space model."}, {"heading": "3.2.1. GENERATIVE MODEL", "text": "In its most general form, sequential generative models describe the observed data on T time steps using a set of latent variables zt at each step. The generative model is represented in the stochastic arithmetic diagram of Figure 2 (a) and described by: Latent variables zt \u0445 N (zt | 0.1) t = 1,.., T (3) Hidden state ht = fh (ht \u2212 1, zt; \u03b8h) (4) Hidden canvas ct = fc (ct \u2212 1, ht; \u03b8c) (5) Observation x \u0445 p (x | fo (cT; \u03b8o)))) (6) Each step generates an independent set of K-dimensional latent variables zt (Equation (3)). A deterministic transition function fh introduces the sequential dependence between each of the latent variables (Equation (4). This allows using any transition mechanism and defining our transition as a long-term short-term memory network (Lreiter, 1997, Hochreiter function, and Schmidhugh function)."}, {"heading": "3.2.2. FREE ENERGY OBJECTIVE", "text": "By applying the variation principle, we obtain the free energy objective: log p (x) = log p (x | z1,..., T) p (z1... T) dz1... T \u2265 FF = Eq (z1,..., T) [log p\u03b8 (x | z1,..., T)] \u2212 \u2211 Tt = 1 KL [q\u03c6 (zt | z < tx) \u0445 p (zt)], (7) where z < t specifies the capture of all latent variables from iteration 1 to t \u2212 1. We can now optimize this objective function for the variation parameters \u03c6 and the model parameters \u03b8 by performing stochastic gradient descendants using a mini-stack of data. As with other AEs, we use a single sample of the latent variables generated from q3 (z | x) when calculating the hidden functions of our Monte Carlo gradients retrospectively."}, {"heading": "3.2.3. HIDDEN CANVAS FUNCTIONS", "text": "The Canvas Transition Function fc (ct \u2212 1, ht; \u03b8c) (5) updates the hidden canvas by first transforming the current hidden state of the LSTM ht (with a function fw) and merging the result with the existing canvas ct \u2212 1. In this work, we use hidden canvases that are the same size as the original images, although they can be either larger or smaller and have any number of channels (four in this paper). We consider two ways to update the hidden canvas: Additive Canvas. As the name implies, an additive canvas updates the canvas by simply transforming the hidden state fw (ht; successc) to the previous canvas state ct \u2212 1. This is a simple but effective (see results) refresh rule: fc (ct \u2212 1, ht; \u03b8c) = an additive canvas that can refresh the canvas by simply refreshing the hidden state fw (ht; successc) (8) Grent canvas recurrent with a canvas function \u2212."}, {"heading": "3.2.4. DEPENDENT POSTERIOR INFERENCE", "text": "We use a structured posterior approximation that has an auto-regressive form, i.e. q (zt | z < t, x). We implement this distribution as an inference network parameterized by a deep network. The specific form we use is: Sprite rt = fr (x, ht \u2212 1; \u03c6r) (10) Example zt \u0445 N (zt | \u00b5 (st, ht \u2212 1; \u03c6\u00b5), \u03c3 (rt, ht \u2212 1; \u03c6\u03c3))) (11) At each step of the calculation, we create a low-dimensional representation rt of the input image using a nonlinear transformation of the input image and the hidden state of the model. This function is a read function and is the counterpart of the write function from the previous section. The read function allows us to transform the input image into a new coordinate space that allows a simpler inference calculation. < How the write function can be compared with the read as a complete network or implemented)."}, {"heading": "3.2.5. MODEL PROPERTIES AND COMPLEXITY", "text": "The above sequential generative model and the conclusion is a generalization of existing models such as DRAW (Gregor et al., 2015) and compound VAEs (Huang & Murphy, 2015).This generalization exhibits a number of differences and important characteristics. One of the major deviations is the introduction of the hidden canvas into the generative model, which provides the model with important richness as it allows to construct a model in a hidden space before using a final corrective transformation using function fo. The generative process has an important property that allows the model to be tested without returning the results of the canvas to the hidden state - such a connection is not required and provides more efficiency by reducing the number of model parameters. The inference network within our framework is similarly simplified as well. We do not use a separate, recurring function within the inference network (such as DRAW), but common parameters of the LSTM from the previous recurrent function - the additional one."}, {"heading": "4. Image Generation and Analysis", "text": "First, we show that our models are state-of-the-art, maintain highly competitive probabilities, and are able to generate high-quality samples across a wide range of data sets with different characteristics. In all of our experiments, our data consists of binary images and we use a Bernoulli probability to model the probability of pixels. In all models, we use 400 hidden LSTM units. We use 12 x 12 cores for the spatial transformer, whether used for detection or generative attention.The latent variables are 4-dimensional Gaussian distributions and we use a number of steps ranging from 20-80. The hidden canvas has dimensions that correspond to the size of the four-channel images. We present the main results here and any additional results in Appendix A. All models have been trained for approximately 800K iterations using mini-batches of size 24. The reported probability limits for the training set are calculated by averages of the last 1K Iterations during training."}, {"heading": "4.1. MNIST and Multi-MNIST", "text": "The first experiment uses the binarized MNIST dataset from Salakhutdinov & Murray (2008), which consists of 28 \u00d7 28 binary images with 50,000 training images and 10,000 test images. Table 1 compares the log probabilities obtained at MNIST with existing models and the models discussed here (with deviations from our estimates in brackets).The sequential generative model, which uses spatially transformed attention with the CGRU hidden canvas, provides the best performance among the existing work on this dataset. We show samples from the model in Figure 3. We form a multifunctional MNIST dataset of 64 \u00d7 64 images consisting of two MNIST digits placed at random locations in the image (using the overloaded MNIST generator from Mnih et al. (2014) to generate the data."}, {"heading": "4.2. Omniglot", "text": "Unlike MNIST, which has a small number of classes with many images of each class and a large amount of data, the omniglot dataset (Lake et al., 2015) consists of 105 x 105 binary images over 1628 classes with only 20 images per class. This dataset allows us to show that attention mechanisms and better generative models allow us to work well even in regimes with larger images and limited amounts of data. There are two versions of omniglot data that were previously used for the evaluation of generative models. A dataset used by Burda et al. (2015) consists of 28 x 28 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf 12345 10 15 20 steps sK LD (nats) \u25cf ST + CGRU ST + Additive Figure 4. Split per-step KL contribution to MNIST.125130135140140150150150x20 40x10 45x5 data."}, {"heading": "4.3. Multi-PIE", "text": "The Multi-PIE dataset (Gross et al., 2010) consists of 48 x 48 RGB facial images from different angles. We converted the images to grayscale and trained our model on a subset that covers all 15 viewpoints, but only 3 of the 19 illumination conditions. Our simplification results in 93, 130 training samples and 10,000 test samples. Examples from this model are shown in Figure 7 and are very convincing, showing faces in different orientations, different genders and are representative of the data. The model was trained using the logit-normal density of the pixels as in Rezende & Mohamed (2015)."}, {"heading": "5. One-Shot Generalization", "text": "Lake et al. (2015) present three tasks to evaluate one-off generalizations that test weaker to stronger forms of generalization: (1) unconditional (free) generation, (2) generation of new variations of a given pattern, and (3) generation of representative samples from a novel alphabet. Lake et al. (2015) perform human evaluations as part of their assessment, which is important for comparing the performance of models with human cognitive ability; we do not perform human benchmarks in this work (human evaluation will be part of our follow-up work), and we focus on machine learning one-off generalization and the associated computational challenges."}, {"heading": "1. Unconditional Generation.", "text": "Figure 8 shows examples reflecting the characteristics of the omniglot data and showing a variety of styles, including rounded patterns, line segments, thick and thin strokes representative of the dataset. Quantitatively, the probabilities given in Tables 3 and 4 confirm this model as state of the art."}, {"heading": "2. Novel variations of a given exemplar.", "text": "This task corresponds to Figure 5 in Lake et al. (2015). To do this, a simple modification of the model is made and shown in Figure 2, in which it is conditioned to an external context. Context is the image we wish the model to generate new examples. To test the limits of our approach, we use a dataset in which the training data consists of all available alphabets."}, {"heading": "3. Representative samples from a novel alphabet.", "text": "This task corresponds to Figure 7 in Lake et al. (2015) and conditions the model to 1 to 10 samples of a novel alphabet and prompts the model to generate new characters consistent with this novel alphabet. Here, we present the most difficult form of this test with only one context image. This test is highly subjective, but the model generations in Figure 11 show that it is able to pick up common characteristics and use them in generations. We have highlighted the usefulness of deep generative models as scalable general-purpose tools for probabilistic thinking, which have the important property of a one-time generalization, but these models have limitations. We have already pointed out the need for adequate amounts of data. Another important consideration is that while our models can perform a one-time generalization, they cannot perform a one-time learning. A one-time learning requires that a model is updated after each new input is presented, such as the non-parametric models that Lake et al (2015) does not update certain data types."}, {"heading": "30-20 40-10 45-5", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6. Conclusion", "text": "We have developed a new class of universal models that are capable of performing one-time generalizations, mimicking an important feature of human cognition.Sequential generative models are natural extensions of variable auto-encoders and offer state-of-the-art models for depth estimation and image generation. The models define a sequential process through groups of latent variables that allow the model to calculate the probability of data points over a number of steps, using the principles of feedback and attention. The use of spatial attention mechanisms significantly improves the model's ability to generalize. The spatial transformer is a highly flexible attention mechanism for both reading and writing, and is now our standard mechanism for attention in generative models. We have highlighted the unique generalization capability of the model over a number of tasks that have shown that the model is capable of avoiding compelling and multifaceted data, and that we still want to see this set of limitations only once we have seen it."}, {"heading": "Acknowledgements", "text": "We thank Brenden Lake and Josh Tenenbaum for their insightful conversations and Theophane Weber, Ali Eslami, Peter Battaglia and David Barrett for their valuable feedback."}, {"heading": "A. Additional Results", "text": "A.1. SVHNThe SVHN dataset (Netzer et al., 2011) consists of 32 x 32 RGB images of house numbers."}, {"heading": "B. Other types of attention", "text": "The simplest attention we have received is that it randomly selects patches from the input image, which is the easiest way to implement a sparse selection mechanism. Applying failure regulation to the input layer of deep models would effectively implement this kind of attention (a hard attention that has no learning). In datasets like MNIST, this attention enables competitive learning of the generative model when the model is allowed to handle a large number of patches; see this video https: / / www.youtube. com / watch? v = W0R394wEUqQ.Error-based attention. One of the difficulties with attention mechanisms is that for large and sparse images there is little gradient information available that can cause the attentive selection to get stuck. To address this problem, previous approaches have used particle-based attention methods (Tang et al., 2014) and exploration techniques that allow for enhanced attention to be achieved (Mh et)."}, {"heading": "C. Other model details", "text": "The CGRU of Kaiser & Sutskever (2015) has the following form: fc (ct \u2212 1, ht; \u03b8c) = CGRU (ct \u2212 1 + fw (ht; \u03b8c)), (12) CGRU (c) = u c + (1 \u2212 u) tanh (U \u0445 (r c) + B)), u = \u03c3 (U \u00b2 c + B \u2032), r = \u03c3 (U \u2032 \u043c c + B \u2032), where the symbols indicate the element-by-element product, \u0445 a resistant folding with increment of 1 \u00d7 1 and \u03c3 (\u00b7) is the sigmoid function. Matrices U, U \u2032 and U \u2032 are 3 \u00d7 3. The number of filters used for the hidden canvas c is specified in section 4."}], "references": [{"title": "Learning wake-sleep recurrent attention models", "author": ["Ba", "Jimmy", "Salakhutdinov", "Ruslan R", "Grosse", "Roger B", "Frey", "Brendan J"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Importance weighted autoencoders", "author": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "What and where: A bayesian inference theory of attention", "author": ["Chikkerur", "Sharat", "Serre", "Thomas", "Tan", "Cheston", "Poggio", "Tomaso"], "venue": "Vision research,", "citeRegEx": "Chikkerur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chikkerur et al\\.", "year": 2010}, {"title": "An analysisby-synthesis approach to multisensory object shape perception", "author": ["G. Erdogan", "I. Yildirim", "R.A. Jacobs"], "venue": "In NIPS,", "citeRegEx": "Erdogan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Erdogan et al\\.", "year": 2015}, {"title": "Deep autoregressive networks", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Mnih", "Andriy", "Blundell", "Charles", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Jimenez Rezende", "Danilo", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Efficient inference in occlusion-aware generative models of images", "author": ["Huang", "Jonathan", "Murphy", "Kevin"], "venue": "arXiv preprint arXiv:1511.06362,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Spatial transformer networks", "author": ["Jaderberg", "Max", "Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In Advances in Neural Information Processing Systems, pp. 2008\u20132016,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Neural gpus learn algorithms", "author": ["Kaiser", "\u0141ukasz", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "Kaiser et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2015}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In ICLR,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Lake", "Brenden M", "Salakhutdinov", "Ruslan", "Tenenbaum", "Joshua B"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Larochelle", "Hugo", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Larochelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2010}, {"title": "Generating images from captions with attention", "author": ["Mansimov", "Elman", "Parisotto", "Emilio", "Ba", "Jimmy Lei", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1511.02793,", "citeRegEx": "Mansimov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mansimov et al\\.", "year": 2015}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "In ICML,", "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Analysis-by-synthesis by learning to invert generative black boxes", "author": ["Nair", "Vinod", "Susskind", "Josh", "Hinton", "Geoffrey E"], "venue": "In Artificial Neural Networks-ICANN", "citeRegEx": "Nair et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2008}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Black box variational inference", "author": ["Ranganath", "Rajesh", "Gerrish", "Sean", "Blei", "David M"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "On the quantitative analysis of deep belief networks", "author": ["Salakhutdinov", "Ruslan", "Murray", "Iain"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Learning with hierarchical-deep models", "author": ["Salakhutdinov", "Ruslan", "Tenenbaum", "Joshua B", "Torralba", "Antonio"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 1958}, {"title": "Mean field theory for sigmoid belief networks", "author": ["Saul", "Lawrence K", "Jaakkola", "Tommi", "Jordan", "Michael I"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Saul et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Saul et al\\.", "year": 1996}, {"title": "Learning generative models with visual attention", "author": ["Tang", "Yichuan", "Srivastava", "Nitish", "Salakhutdinov", "Ruslan R"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Vision as bayesian inference: analysis by synthesis", "author": ["Yuille", "Alan", "Kersten", "Daniel"], "venue": "Trends in cognitive sciences,", "citeRegEx": "Yuille et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yuille et al\\.", "year": 2006}, {"title": "SVHN The SVHN dataset (Netzer et al., 2011) consists of 32\u00d7 32 RGB images from house numbers. B. Other types of attention", "author": ["A. Additional Results A"], "venue": null, "citeRegEx": "A.1.,? \\Q2011\\E", "shortCiteRegEx": "A.1.", "year": 2011}], "referenceMentions": [{"referenceID": 21, "context": "Salakhutdinov et al. (2013) developed a probabilistic model that combines a deep Boltzmann machine with a hierarchical Dirichlet process to learn hierarchies of concept categories as well as provide a powerful generative model.", "startOffset": 0, "endOffset": 28}, {"referenceID": 12, "context": "Recently, Lake et al. (2015) presented a compelling demonstration of the ability of probabilistic models to perform one-shot generalization, using Bayesian program learning, which is able to learn a hierarchical, non-parametric generative model of handwritten characters.", "startOffset": 10, "endOffset": 29}, {"referenceID": 12, "context": "Recently, Lake et al. (2015) presented a compelling demonstration of the ability of probabilistic models to perform one-shot generalization, using Bayesian program learning, which is able to learn a hierarchical, non-parametric generative model of handwritten characters. Their approach incorporates specific knowledge of how strokes are formed and the ways in which they are combined to produce characters of different types, exploiting similar strategies used by humans to perform this task. Lake et al. (2015) see the capacity for one-shot generalization demonstrated by Bayesian programming learning \u2018as a challenge for neural models\u2019.", "startOffset": 10, "endOffset": 513}, {"referenceID": 24, "context": "Models that are directed graphical models have recently become popular in machine learning and include discrete latent variable models such as sigmoid belief networks and deep auto-regressive networks (Saul et al., 1996; Gregor et al., 2014), or continuous latent variable models such as non-linear Gaussian belief networks and deep latent Gausar X iv :1 60 3.", "startOffset": 201, "endOffset": 241}, {"referenceID": 4, "context": "Models that are directed graphical models have recently become popular in machine learning and include discrete latent variable models such as sigmoid belief networks and deep auto-regressive networks (Saul et al., 1996; Gregor et al., 2014), or continuous latent variable models such as non-linear Gaussian belief networks and deep latent Gausar X iv :1 60 3.", "startOffset": 201, "endOffset": 241}, {"referenceID": 20, "context": "sian models (Rezende et al., 2014; Kingma & Welling, 2014).", "startOffset": 12, "endOffset": 58}, {"referenceID": 3, "context": "These principles allow the models we develop to reflect the principles of analysis-by-synthesis, in which the analysis of observed information is continually integrated with constructed interpretations of it (Yuille & Kersten, 2006; Erdogan et al., 2015; Nair et al., 2008).", "startOffset": 208, "endOffset": 273}, {"referenceID": 17, "context": "These principles allow the models we develop to reflect the principles of analysis-by-synthesis, in which the analysis of observed information is continually integrated with constructed interpretations of it (Yuille & Kersten, 2006; Erdogan et al., 2015; Nair et al., 2008).", "startOffset": 208, "endOffset": 273}, {"referenceID": 5, "context": "Models such as DRAW (Gregor et al., 2015) and composited variational auto-encoders (Huang & Murphy, 2015) are existing models in this class, and we will develop a general class of sequential generative models that incorporates these and other latent variable models and variational auto-encoders.", "startOffset": 20, "endOffset": 41}, {"referenceID": 2, "context": "Attention has been most widely used for classification tasks, having been shown to improve both scalability and generalization (Larochelle & Hinton, 2010; Chikkerur et al., 2010; Xu et al., 2015; Jaderberg et al., 2015; Mnih et al., 2014; Ba et al., 2015).", "startOffset": 127, "endOffset": 255}, {"referenceID": 26, "context": "Attention has been most widely used for classification tasks, having been shown to improve both scalability and generalization (Larochelle & Hinton, 2010; Chikkerur et al., 2010; Xu et al., 2015; Jaderberg et al., 2015; Mnih et al., 2014; Ba et al., 2015).", "startOffset": 127, "endOffset": 255}, {"referenceID": 8, "context": "Attention has been most widely used for classification tasks, having been shown to improve both scalability and generalization (Larochelle & Hinton, 2010; Chikkerur et al., 2010; Xu et al., 2015; Jaderberg et al., 2015; Mnih et al., 2014; Ba et al., 2015).", "startOffset": 127, "endOffset": 255}, {"referenceID": 16, "context": "Attention has been most widely used for classification tasks, having been shown to improve both scalability and generalization (Larochelle & Hinton, 2010; Chikkerur et al., 2010; Xu et al., 2015; Jaderberg et al., 2015; Mnih et al., 2014; Ba et al., 2015).", "startOffset": 127, "endOffset": 255}, {"referenceID": 0, "context": "Attention has been most widely used for classification tasks, having been shown to improve both scalability and generalization (Larochelle & Hinton, 2010; Chikkerur et al., 2010; Xu et al., 2015; Jaderberg et al., 2015; Mnih et al., 2014; Ba et al., 2015).", "startOffset": 127, "endOffset": 255}, {"referenceID": 25, "context": "Attention in unsupervised learning is much more recent (Tang et al., 2014; Gregor et al., 2015).", "startOffset": 55, "endOffset": 95}, {"referenceID": 5, "context": "Attention in unsupervised learning is much more recent (Tang et al., 2014; Gregor et al., 2015).", "startOffset": 55, "endOffset": 95}, {"referenceID": 8, "context": "Spatial transformers (Jaderberg et al., 2015) are a more general method for providing such invariance, and is our preferred attentional mechanism.", "startOffset": 21, "endOffset": 45}, {"referenceID": 24, "context": "Tang et al. (2014) take such an approach and use 2D similarity transforms to provide basic affine invariance.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "In more complex models, the probabilistic description consists of a hierarchy of L layers of latent variables, where each layer depends on the layer above in a non-linear way (Rezende et al., 2014) \u2014for deep generative models, we specify this non-linear dependency using deep neural networks.", "startOffset": 175, "endOffset": 197}, {"referenceID": 9, "context": "One popular approximation technique is based on variational inference (Jordan et al., 1999), which transforms the difficult integration into an optimization problem that is typically more scalable and easier to solve.", "startOffset": 70, "endOffset": 91}, {"referenceID": 20, "context": "VAEs allow for a single computational graph to be constructed and straightforward gradient computations: when the latent variables are continuous, gradient estimators based on pathwise derivative estimators are used (Rezende et al., 2014; Kingma & Welling, 2014; Burda et al., 2015) and when they are discrete, score function estimators are used (Mnih & Gregor, 2014; Ranganath et al.", "startOffset": 216, "endOffset": 282}, {"referenceID": 1, "context": "VAEs allow for a single computational graph to be constructed and straightforward gradient computations: when the latent variables are continuous, gradient estimators based on pathwise derivative estimators are used (Rezende et al., 2014; Kingma & Welling, 2014; Burda et al., 2015) and when they are discrete, score function estimators are used (Mnih & Gregor, 2014; Ranganath et al.", "startOffset": 216, "endOffset": 282}, {"referenceID": 19, "context": ", 2015) and when they are discrete, score function estimators are used (Mnih & Gregor, 2014; Ranganath et al., 2014; Mansimov et al., 2015).", "startOffset": 71, "endOffset": 139}, {"referenceID": 14, "context": ", 2015) and when they are discrete, score function estimators are used (Mnih & Gregor, 2014; Ranganath et al., 2014; Mansimov et al., 2015).", "startOffset": 71, "endOffset": 139}, {"referenceID": 5, "context": "The above sequential generative model and inference is a generalization of existing models such as DRAW (Gregor et al., 2015) and composited VAEs (Huang & Murphy, 2015).", "startOffset": 104, "endOffset": 125}, {"referenceID": 8, "context": "We use a much more powerful and general attention mechanism based on spatial transformers (Jaderberg et al., 2015; Huang & Murphy, 2015).", "startOffset": 90, "endOffset": 136}, {"referenceID": 4, "context": "Gregor et al. (2015) use a generative attention based on Gaussian convolutions parameterized by a location and scale, and Tang et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "Gregor et al. (2015) use a generative attention based on Gaussian convolutions parameterized by a location and scale, and Tang et al. (2014) use 2D similarity transformations.", "startOffset": 0, "endOffset": 141}, {"referenceID": 16, "context": "We form a multi-MNIST data set of 64 \u00d7 64 images that consists of two MNIST digits placed at random locations in the image (having adapted the cluttered MNIST generator from Mnih et al. (2014) to procedurally generate the data).", "startOffset": 174, "endOffset": 193}, {"referenceID": 3, "context": "From Gregor et al. (2015) and Burda et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 1, "context": "(2015) and Burda et al. (2015)", "startOffset": 11, "endOffset": 31}, {"referenceID": 12, "context": "Unlike MNIST, which has a small number of classes with many images of each class and a large amount of data, the omniglot data set (Lake et al., 2015) consists of 105\u00d7 105 binary images across 1628 classes with just 20 images per class.", "startOffset": 131, "endOffset": 150}, {"referenceID": 1, "context": "One data set used by Burda et al. (2015) consists of 28\u00d728 \u25cf", "startOffset": 21, "endOffset": 41}, {"referenceID": 11, "context": "images, but is different to that of Lake et al. (2015). We compare the available methods on the dataset from Burda et al.", "startOffset": 36, "endOffset": 55}, {"referenceID": 1, "context": "We compare the available methods on the dataset from Burda et al. (2015) in table 3 and find that the sequential models perform better than all competing approaches, further establishing the effectiveness of these models.", "startOffset": 53, "endOffset": 73}, {"referenceID": 1, "context": "We compare the available methods on the dataset from Burda et al. (2015) in table 3 and find that the sequential models perform better than all competing approaches, further establishing the effectiveness of these models. Our second evaluation uses the dataset of Lake et al. (2015), which we downsampled to 52 \u00d7 52 using a 2 \u00d7 2 max-pooling.", "startOffset": 53, "endOffset": 283}, {"referenceID": 1, "context": "From Burda et al. (2015)", "startOffset": 5, "endOffset": 25}, {"referenceID": 12, "context": "This task corresponds to figure 5 in Lake et al. (2015)).", "startOffset": 37, "endOffset": 56}, {"referenceID": 12, "context": "b) We use exactly the data split used by Lake et al. (2015), which consists of 30 alphabets as the training set and the remaining 20 alphabets as the test set.", "startOffset": 41, "endOffset": 60}, {"referenceID": 12, "context": "Unlike the model of Lake et al. (2015), which uses human stroke information and a model structured around the way in which humans draw images, our model is applicable to any image data, with the only domain specific information that is used being that the data is spatially arranged (which is exploited by the convolution and attention).", "startOffset": 20, "endOffset": 39}, {"referenceID": 12, "context": "This task corresponds to figure 7 in Lake et al. (2015), and conditions the model on anywhere between 1 to 10 samples of a novel alphabet and asks the model to generate new characters consistent with this novel alphabet.", "startOffset": 37, "endOffset": 56}, {"referenceID": 12, "context": ", like the non-parametric models used by Lake et al. (2015) or Salakhutdinov et al.", "startOffset": 41, "endOffset": 60}, {"referenceID": 12, "context": ", like the non-parametric models used by Lake et al. (2015) or Salakhutdinov et al. (2013). Parametric models such as ours require a gradient update of the parameters, which we do not do.", "startOffset": 41, "endOffset": 91}], "year": 2016, "abstractText": "Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples\u2014 having seen new examples just once\u2014providing an important class of general-purpose models for one-shot machine learning.", "creator": "LaTeX with hyperref package"}}}