{"id": "1603.05201", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units", "abstract": "Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CRelu) and theoretically analyze its reconstruction property in CNNs. We integrate CRelu into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification.", "histories": [["v1", "Wed, 16 Mar 2016 18:17:36 GMT  (2926kb,D)", "http://arxiv.org/abs/1603.05201v1", null], ["v2", "Tue, 19 Jul 2016 05:18:36 GMT  (2962kb,D)", "http://arxiv.org/abs/1603.05201v2", "ICML 2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["wenling shang", "kihyuk sohn", "diogo almeida", "honglak lee"], "accepted": true, "id": "1603.05201"}, "pdf": {"name": "1603.05201.pdf", "metadata": {"source": "META", "title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units", "authors": ["Wenling Shang", "Kihyuk Sohn", "Diogo Almeida", "Honglak Lee"], "emails": ["SHANGW@UMICH.EDU", "KSOHN@NEC-LABS.COM", "DIOGO@ENLITIC.COM", "HONGLAK@EECS.UMICH.EDU"], "sections": [{"heading": "1. Introduction", "text": "In recent years, we have seen great successes (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Girshick et al., 2015) and a wide range of techniques have been developed to improve the training of CNNs (Lin et al., 2013; Zeiler & Ferguson, 2013; Ioffe & Szegedy, 2015)."}, {"heading": "2. CRelu and Reconstruction Property", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Conjecture on Convolution Layers", "text": "During our initial research of classic CNNs trained on natural images such as AlexNet (Krizhevsky et al., 2012), we noticed a strange tendency of the first evolution: these filters tend to form \"pairs.\" More precisely, provided that the length of the individual filters is defined by us in such a way that we define a mating filter of the first evolution in the following way: \"I = argminoritary filter layer of AlexNet with its mating filter.\" They appear surprisingly opposite to each other, i.e. for each filter there is another filter that exists almost on the opposite phase. In fact, AlexNet employs the popular, unsaturated linear unit (Nair Hinu, 2010), which generates negative values and activates the two phases."}, {"heading": "2.2. Reconstruction Property", "text": "A notable property of CRelu is its preservation of information: CRelu preserves both negative and positive linear answers to these questions. A direct consequence of preserving information is the reconstructive power of layers equipped with CRelu reconstruction. (2015) This aspect of CNNs has recently gained interest. (2013) Theoretically, we injectively investigate the general conditions followed by the max.pooling layer of Relu and measure the stability of the inverting process by calculating the Lipschitz lower limit. However, their curves are not trivial if the number of filters significantly exceeds the input dimension, which is not realistic."}, {"heading": "3. Benchmark Results", "text": "We evaluate the effectiveness of our proposed CRelu activation scheme using three benchmark datasets: CIFAR-10, CIFAR-100 (Krizhevsky, 2009), and ImageNet (Deng et al., 2009). To directly assess the effects of CRelu, we use existing CNN architectures with Relu that have already demonstrated good detection performance and improved performance by replacing Relu with CRelu. Note that models with CRelu activation do not require significant hyperparameter adjustment from the Relu base model, and in most of our experiments we only adjust the failure rate while other hyperparameters (e.g. learning rate, minibatch size) remain the same. Details of the network architecture and training process are outlined in Section E of the complementary materials."}, {"heading": "3.1. CIFAR-10 and CIFAR-100", "text": "In fact, it is such that most of us will be able to move into another world, in which they are able to put themselves into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in fact, in which they, in which they, in which they, in fact, in which they, in fact, in which they, in which they, live, in which they, in which they, in which they, in which they, live, in which they, in which, in which they, in which they, in which they, live, in which they, in which, in which they, in fact, in which they, in which they, live, in which they, in which,"}, {"heading": "3.2. ImageNet", "text": "To assess the impact of CRelu on large datasets, we conduct experiments on ImageNet datasets that go deeper (Deng et al., 2009), which contain approximately 1.3M images for training and 50,000 for validating 1, 000 object categories. For pre-processing, we subtract the mean and divide it by the dard deviation for each input channel, and follow the data augmentation described in (Krizhevsky et al., 2012). We take the All-CNN-B model (Springenberg et al., 2014) as our base model. AllCNN-B's network architecture is similar to that of AlexNet (Krizhevsky et al., 2012), where the maximum pooling layer is replaced by conversion with the same core size and step size, the fully connected layer is replaced by 1 \u00d7 1 convolution layers, followed by average pooling layers, and local normalization layers."}, {"heading": "4. Discussion", "text": "In this section we will discuss qualitative properties of the CRelu activation scheme from different points of view, such as network regulation and learning invariant representation."}, {"heading": "4.1. A View from Regularization", "text": "In general, a model with better traceable parameters is more susceptible to overadjustment. However, the models with CRelu exhibit much less overadjustment problems than the base models with Relu, although it has twice as many parameters (Table 1), for the purely conventional CIFAR experiments. We consider that maintaining both positive and negative phase information makes training more difficult, and this effect has been used to better regulate deep networks, especially when working on small data sets. Not only from empirical evidence, but also to describe the regulating effect by deriving a limit to the wheel-maker complexity of the CRelu layer, followed by a linear transformation as follows: Theorem 4.1. Let G have the class of real functions Rdin \u2192 R with the input dimension F, that is G = [F] dinj = 1. Let H be a linear transformation function from RR to RW, where B is paramdin-W."}, {"heading": "4.2. Towards Learning Invariant Features", "text": "We measure the inventory values using the evaluation metrics of (Goodfellow et al., 2009) and draw a further comparison between the CRelu models and the Relu models. For a fair evaluation, we compare all 7 Conv layers of the allconv Relu model with those of the CIFAR-10 / 100 model. In the case of ImageNet experiments, we select the model in which CRelu replaces the Relu values for the first 7 Conv layers and compare the inventory values with the first 7 Conv layers of the Relu model. Section C in the supplementary materials describes in detail how the inventory values are measured. Figure 4 shows the inventory values for networks trained on CIFAR-10, CIFAR-100 and ImageNet, respectively the inventory values of the CRelu models are consistently higher than the Relu models."}, {"heading": "4.3. Revisiting the Reconstruction Property", "text": "In section 2.1 we observe that the lower layers folding filters from Relu models form negatively correlated pairs. Is the repair phenomenon still existent for CRelu models? We take our best CRelu model, which was trained on ImageNet (where the first 4 layers are integrated with CRelu) and repeat the histogram experiments to generate Figure 5. In clear contrast to Figure 2, the distributions of \u00b5 wi from CRelu model are well connected with the distributions of \u00b5 ri layers from random layer filters. In other words, each lower layer conversion filter layers clearly extends in its own direction without a negatively correlated repair filter, while CRelu implicitly plays the role of \"pair-grouping\" layers. In section 2.2 we mathematically characterize the reconstruction properties of layers, which we characterize with CRelu layers-repair-layers-layers-layers-layers-layers-layers-layers-layers-layers-layers-layers-by-pair-grouping."}, {"heading": "5. Conclusion", "text": "We propose a new activation scheme, CRelu, that preserves both positive and negative linear responses after folding so that each filter can efficiently represent its unique direction. Our work shows that CRelu improves5Each entry is sampled from standard normal distribution.deep networks with classification objective. By preserving the available information from the input while maintaining the unsaturated nonlinearity, CRelu can potentially facilitate more complicated machine learning tasks such as structured output prediction and image generation."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Erik Brinkman, Harry Altman and Mark Rudelson for their helpful comments. We also thank Yuting Zhang and Anna Gilbert for the discussions during the preliminary phase of this work. We thank Technicolor Research for providing resources and support and NVIDIA for donating GPUs."}, {"heading": "A. Reconstruction Property Proofs", "text": "Proposition A.1. Lass x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x."}, {"heading": "B. Proof of Model Complexity Bound", "text": "Definition B.1. (Rademacher Complexity) For a sample S = {x1, \u00b7 \u00b7 L (F) = Evolution D on Set X and a really evaluated function classF on DomainX, \u00b7 \u00b7 \u00b7, xL, where the empirical Rademacher complexity of F is the random variable. The Rademacher complexity of F is RL (F) = ES [R] - L (F) - L (F) - L (F) - L (F) - L (F) - L (F) - L (F) - L (F) - L (F) - L (F) - L (F) - L (F) - L (F) - L) - L (F) - L) - L (F) - L)."}, {"heading": "E. Details of Network Architecture", "text": "Baseline Baseline Baseline (double) Layer kernel, stride, padding activation kernel, stride, padding activation conven1 3 x 3 x 3 x 96, 1, 1 Relu pool1 3 x 3 x x x \u00d7 3 x x x x 3 x \u00d7 192, 1, 1 Relu conver2 3 x 96, 1, 1 Relu 3 x 192 x 192, 1 Relu pool1 3 x 3 x \u00d7 3, 2, 0 x 3 x 384, 1 Relu conver3 x 3 x 96 x 192, 1, 1 Relu conver3 x 192, 1 Relu 3 x 192, 1 Relu 3 x 384, 1 Relu convertivo 3 x 192 x 192 x 192, 1, 1 Relu converting 1 x 192 x 192, 1 Relu converting 3 x 192 x 192 x 192 x 192, 1 x 192, 1 x 192, 1 x 192, 1 x 192, 1 x 192, 1 x 192, 1 x 194, 1 Relu convertivo 3 x 194, 1 \u00d7 192 x 194, 1 \u00d7 194, 192, 1 x 192, 1, 1 x 194, 1, 1 x 192, 1, 1 x 192, 1, 1 x 192, 1, 1 x 192, 1 x 192, 1, 1 x 192, 1, 1 x 192, 1, 1 x 192, 1, 1 x 192, 1, 1 x 192, 1, 1 x 192, 1, 1, 1, 1 x 192, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1"}], "references": [{"title": "Invariant scattering convolution networks", "author": ["Bruna", "Joan", "Mallat", "St\u00e9phane"], "venue": null, "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "Signal recovery from pooling representations", "author": ["Bruna", "Joan", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "In ICML,", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "An introduction to frames and Riesz bases", "author": ["Christensen", "Ole"], "venue": "Birkhuser Basel,", "citeRegEx": "Christensen and Ole.,? \\Q2003\\E", "shortCiteRegEx": "Christensen and Ole.", "year": 2003}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-jia", "Kai", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Inverting convolutional networks with convolutional networks", "author": ["Dosovitskiy", "Alexey", "Brox", "Thomas"], "venue": "In CVPR,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "In CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Measuring invariances in deep networks", "author": ["Goodfellow", "Ian", "Lee", "Honglak", "Le", "Quoc V", "Saxe", "Andrew", "Ng"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2009}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William J"], "venue": "In NIPS,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": null, "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "lya", "Hinton", "Geoffrey"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree", "author": ["Lee", "Cen-yu", "Gallagher", "Patrick W", "Tu", "Zhuowen"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["Liang", "Ming", "Hu", "Xiaolin"], "venue": "In CVPR,", "citeRegEx": "Liang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2015}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Maas", "Andrew", "Hannun", "Awni Y", "Ng"], "venue": "In ICML,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Understanding deep image representations by inverting them", "author": ["Mahendran", "Aravindh", "Vedaldi", "Andrea"], "venue": "In CVPR,", "citeRegEx": "Mahendran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey"], "venue": "In ICML,", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Spectral representations for convolutional neural networks", "author": ["Rippel", "Oren", "Snoek", "Jasper", "Adams", "Ryan"], "venue": "In NIPS,", "citeRegEx": "Rippel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["Snoek", "Jasper", "Rippel", "Oren", "Swersky", "Kevin", "Kiros", "Ryan", "Satish", "Nadathur", "Sundaram", "Narayanan", "Patwary", "Md. Mostofa Ali", "Adams"], "venue": "In ICML,", "citeRegEx": "Snoek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2015}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Springenberg", "Jost", "Dosovitskiy", "Alexey", "Brox", "Thomas", "Riedmiller", "Martin"], "venue": "In ICLR Workshop,", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Training very deep networks", "author": ["Srivastava", "Rupesh", "Greff", "Klaus", "Schmidhuber", "Jurgen"], "venue": "In NIPS,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In ICML,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["Xu", "Bing", "Wang", "Naiyan", "Chen", "Tianqi", "Li", "Mu"], "venue": "In ICML Workshop,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Deep fried convnets", "author": ["Yang", "Zichao", "Moczulski", "Marcin", "Denil", "Misha", "de Freitas", "Nndo de", "Smola", "Alex", "Song", "Le", "Wang", "Ziyu"], "venue": "In ICCV,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In ICLR,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}, {"title": "Stacked what-where auto-encoders", "author": ["Zhao", "Junbo", "Mathieu", "Michael", "Goroshin", "Ross", "Lecun", "Yann"], "venue": "In ICLR,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "In recent years, convolutional neural networks (CNNs) have achieved great success in many problems of machine learning and computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Girshick et al., 2014).", "startOffset": 139, "endOffset": 237}, {"referenceID": 21, "context": "In recent years, convolutional neural networks (CNNs) have achieved great success in many problems of machine learning and computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Girshick et al., 2014).", "startOffset": 139, "endOffset": 237}, {"referenceID": 5, "context": "In recent years, convolutional neural networks (CNNs) have achieved great success in many problems of machine learning and computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Girshick et al., 2014).", "startOffset": 139, "endOffset": 237}, {"referenceID": 13, "context": "In addition, a wide range of techniques has been developed to enhance the performance or ease the training of CNNs (Lin et al., 2013; Zeiler & Fergus, 2013; Maas et al., 2013; Ioffe & Szegedy, 2015).", "startOffset": 115, "endOffset": 198}, {"referenceID": 10, "context": "To better comprehend the internal operations of CNNs, we investigate the well-known AlexNet (Krizhevsky et al., 2012) and thereafter discover that the network learns highly negatively-correlated pairs of filters for the first few convolution layers (Section 2.", "startOffset": 92, "endOffset": 117}, {"referenceID": 10, "context": "During our initial exploration of classic CNNs trained on natural images such as AlexNet (Krizhevsky et al., 2012), we have noted a curious tendency of the first convolution layer filters: these filters tend to form \u201cpairs\u201d.", "startOffset": 89, "endOffset": 114}, {"referenceID": 23, "context": "Another way to allow negative activation is to employ the broader class of non-saturated activation functions including Leaky Relu and its variants (Xu et al., 2015; Maas et al., 2013).", "startOffset": 148, "endOffset": 184}, {"referenceID": 13, "context": "Another way to allow negative activation is to employ the broader class of non-saturated activation functions including Leaky Relu and its variants (Xu et al., 2015; Maas et al., 2013).", "startOffset": 148, "endOffset": 184}, {"referenceID": 24, "context": "This aspect of CNNs has gained interest recently: Mahendran & Vedaldi (2015) invert CNN features back to the input under simple natural image priors; Zhao et al. (2015) stack autoencoders with reconstruction objective to build better classifiers.", "startOffset": 150, "endOffset": 169}, {"referenceID": 0, "context": "Bruna et al. (2013) theoretically investigate general conditions unstride size: s 0.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "We evaluate the effectiveness of our proposed CRelu activation scheme on three benchmark datasets: CIFAR-10, CIFAR-100 (Krizhevsky, 2009) and ImageNet (Deng et al., 2009).", "startOffset": 151, "endOffset": 170}, {"referenceID": 19, "context": "We use the ConvPool-CNN-C model (Springenberg et al., 2014) as our baseline model, which is composed of convolution and pooling followed by Relu without fullyconnected layers.", "startOffset": 32, "endOffset": 59}, {"referenceID": 16, "context": "Model CIFAR-10 CIFAR-100 (Rippel et al., 2015) 8.", "startOffset": 25, "endOffset": 46}, {"referenceID": 18, "context": "60 (Snoek et al., 2015) 6.", "startOffset": 3, "endOffset": 23}, {"referenceID": 11, "context": "75 (Lee et al., 2016) 6.", "startOffset": 3, "endOffset": 21}, {"referenceID": 20, "context": "37 (Srivastava et al., 2015) 7.", "startOffset": 3, "endOffset": 28}, {"referenceID": 3, "context": "To assess the impact of CRelu on large scale dataset, we perform experiments on ImageNet dataset (Deng et al., 2009), which contains about 1.", "startOffset": 97, "endOffset": 116}, {"referenceID": 24, "context": "We compare with AlexNet and other variants, such as FastFood-32-AD (FriedNet) (Yang et al., 2015) and pruned AlexNet (PrunedNet) (Han et al.", "startOffset": 78, "endOffset": 97}, {"referenceID": 7, "context": ", 2015) and pruned AlexNet (PrunedNet) (Han et al., 2015), which are modifications of AlexNet aiming at reducing the number of parameters, as well as All-CNN-B, the baseline model (Springenberg et al.", "startOffset": 39, "endOffset": 57}, {"referenceID": 19, "context": ", 2015), which are modifications of AlexNet aiming at reducing the number of parameters, as well as All-CNN-B, the baseline model (Springenberg et al., 2014).", "startOffset": 130, "endOffset": 157}, {"referenceID": 10, "context": "dard deviation for each input channel, and follow the data augmentation as described in (Krizhevsky et al., 2012).", "startOffset": 88, "endOffset": 113}, {"referenceID": 19, "context": "We take the All-CNN-B model (Springenberg et al., 2014) as our baseline model.", "startOffset": 28, "endOffset": 55}, {"referenceID": 10, "context": "The network architecture of AllCNN-B is similar to that of AlexNet (Krizhevsky et al., 2012), where the max-pooling layer is replaced by convolution with the same kernel size and stride, the fully connected layer is replaced by 1 \u00d7 1 convolution layers followed by average pooling, and the local response normalization layers are discarded.", "startOffset": 67, "endOffset": 92}, {"referenceID": 10, "context": "We report the top-1 and top-5 error rates with center crop only and by averaging scores over 10 patches from the center crop and four corners and with horizontal flip (Krizhevsky et al., 2012).", "startOffset": 167, "endOffset": 192}, {"referenceID": 19, "context": "We note that Springenberg et al. (2014) reported slightly better result (41.", "startOffset": 13, "endOffset": 40}, {"referenceID": 24, "context": "6M parameters (CRelu + all) outperforms FastFood-32-AD (FriedNet) (Yang et al., 2015) and Pruned AlexNet (PrunedNet) (Han et al.", "startOffset": 66, "endOffset": 85}, {"referenceID": 7, "context": ", 2015) and Pruned AlexNet (PrunedNet) (Han et al., 2015), whose designs directly aim at parameter reduction.", "startOffset": 39, "endOffset": 57}, {"referenceID": 22, "context": "1 says that the complexity bound of CRelu + linear transformation is the same as that of Relu + linear transformation, which is proved by Wan et al. (2013). In other words, although the number of model parameters are doubled by CRelu, the model complexity does not necessarily increase.", "startOffset": 138, "endOffset": 156}, {"referenceID": 6, "context": "We measure the invariance scores using the evaluation metrics from (Goodfellow et al., 2009) and draw another comparison between the CRelu models and the Relu models.", "startOffset": 67, "endOffset": 92}, {"referenceID": 6, "context": "We also observe that although as a general trend, the invariance scores increase while going deeper into the networks\u2013consistent with the observations from (Goodfellow et al., 2009), rather unexpectedly, the progression is not monotonic.", "startOffset": 156, "endOffset": 181}], "year": 2016, "abstractText": "Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CRelu) and theoretically analyze its reconstruction property in CNNs. We integrate CRelu into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification.", "creator": "LaTeX with hyperref package"}}}