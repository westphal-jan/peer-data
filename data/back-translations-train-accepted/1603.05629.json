{"id": "1603.05629", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2016", "title": "Discriminative Embeddings of Latent Variable Models for Structured Data", "abstract": "Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced in a number of interdisciplinary areas such as computational biology and drug design. Typically, kernel functions are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations.", "histories": [["v1", "Thu, 17 Mar 2016 19:29:46 GMT  (598kb,D)", "http://arxiv.org/abs/1603.05629v1", "22 pages, 8 figures"], ["v2", "Sat, 19 Mar 2016 19:26:55 GMT  (751kb,D)", "http://arxiv.org/abs/1603.05629v2", "22 pages, 8 figures"], ["v3", "Sun, 29 May 2016 19:12:05 GMT  (722kb,D)", "http://arxiv.org/abs/1603.05629v3", "ICML 2016"], ["v4", "Mon, 26 Sep 2016 23:52:45 GMT  (753kb,D)", "http://arxiv.org/abs/1603.05629v4", "ICML 2016"]], "COMMENTS": "22 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hanjun dai", "bo dai", "le song"], "accepted": true, "id": "1603.05629"}, "pdf": {"name": "1603.05629.pdf", "metadata": {"source": "CRF", "title": "Discriminative Embeddings of Latent Variable Models for Structured Data", "authors": ["Hanjun Dai", "Bo Dai"], "emails": ["bodai}@gatech.edu,", "lsong@cc.gatech.edu"], "sections": [{"heading": null, "text": "We propose an effective and scalable approach to structured data representation, based on the idea of embedding latent variable models in feature spaces and learning such feature spaces using discriminatory information. Furthermore, our feature learning algorithm performs a sequence of function mappings in a similar way to graphical methods of model inference, such as middle- and belief propagation. In real-world applications involving sequences and diagrams, we have shown that the proposed approach is much more scalable than alternatives, while delivering comparable state-of-the-art results in terms of classification and regression."}, {"heading": "1 Introduction", "text": "However, in order to learn from such complex data, we must first explicitly or implicitly transform this data into some vector representations, and then apply machine learning algorithms in the resulting vector space. Kernel methods have emerged as one of the most effective tools for dealing with structured data, and we have achieved state-of-the-art classification and regression results in many sequences (Leslie et al., 2002a; Vishwanathan and Smola, 2003). The success of kernel methods on structured data is largely based on the design of kernel functions - positive semi-definitive similarity between data pairs (Schoolkopf Smola, 2002)."}, {"heading": "2 Backgrounds", "text": "We will also deal with multiple random variables, X1, X2,., X, \"with common density p (X1, X2,.), but the methodology applies to cases where they have different domains. In the case where X is a discrete domain, the density notation should be interpreted as probability, and integral principle should be interpreted as summation. In addition, we will denote a hidden variable with domain H and distribution p (H). We use similar notation conventions for variable H and X.nel methods."}, {"heading": "3 Model for a Structured Data Point", "text": "Without loss of generality, we assume that each structured data point is a graph, with a series of nodes V = {1,.., V} and a series of edges E. We will use xi to determine the value of the node name for nodes i. We note that the node names may differ from the label of the entire data point. For example, each atom in a molecule will correspond to a node in the graph, and the node name will be the atomic number, while the label for the entire molecule may be whether the molecule is a good drug or not. Other structures, such as sequences and trees, may be considered special cases of general graphics. We will model the structured data as an example from a graphic model. More specifically, we will model the label of each node in the graph with a variable Xi variable and, in addition, associate an additional hidden variable Hi with it. Then we will assign these random fields to a pairing one."}, {"heading": "4 Embedding Latent Variable Models", "text": "We will embed the posterior marginal p (Hi | {xi}) of a hidden variable using a characteristic map \u03c6 (Hi), i.e., we will learn it later using monitoring signals. (6) The exact shape of \u03c6 (Hi) is not currently determined, and we will learn it later using monitoring signals. At the moment, we will assume that \u03c6 (Hi) Rd is a finite dimensional attribute space, and the exact value of d is determined by cross-validation in later experiments. However, the calculation of embedding is a very challenging task for general diagrams: it is about performing an inference in a graphical model in which we must integrate all variables that Hi expect, i.e. p (Hi | {xi}) = HV \u2212 1 p (Hi, {hj}), which update the free diagrams with respect to optical algorithms. (7) Only if the structure is a tree, we can perform essentially irical calculations."}, {"heading": "4.1 Embedding Mean-Field Inference", "text": "The Hvanilla midfield conclusion tries to use a product of the independent density components p (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi) - (Hi -) (Hi -) (Hi -) (Hi - (Hi) (Hi) (Hi) - (Hi) - (Hi -) - (Hi -) (Hi -) (Hi - (Hi -) (Hi -) (Hi -) (Hi -) (Hi -) (Hi -) (Hi) (Hi -) (Hi -) (Hi -) (Hi - (Hi) (Hi) (Hi) (Hi -) (Hi -) (Hi -) (Hi -) (Hi -) (Hi -) (Hi -) (Hi -) (Hi -) (Hi -) (Hi -) (Hi -) (Hi -) (Hi -) (Hi -) (Hi - (Hi) (Hi) (Hi -) (Hi -) (Hi - (Hi) (Hi) (Hi -) (Hi -) (Hi -) (Hi -) (Hi) (Hi) (Hi) (Hi -) (Hi -) (Hi -) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi - (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi) (Hi"}, {"heading": "4.2 Embedding Loopy Belief Propagation", "text": "There is another method of variable inference, which essentially aims at the optimization of free energy, which generalizes the variable free energy by taking into account the different interactions. (1) There is another method that refers to the different forms of free energy. (2) There is another method that refers to the different forms of free energy. (2) There is another method that applies to the different forms of free energy. (3) There is another method that applies to the different forms of free energy. (3) There is another method that refers to the different forms of free energy. (4) There is another method that can be applied to. (4) There is another method that can be applied to. (4) There is another method that can be applied to. (4) There is another method that can be applied to free energy."}, {"heading": "4.3 Embedding Generalized Belief Propagation", "text": "The Kikuchi free energy is the generalization of the Bethe free energy through the inclusion of the Bethe special interactions. More specifically, we refer to R as a set of regions, i.e., some basic clusters of nodes, their intersections, the intersections of nodes and so on. We refer to the sub (r) or sub (r) s free energy is the sub (r) sub (hr) loge (hr) n, which are fully contained in r or r. Let hr be the state of the nodes in region r, then the Kikuchi free energy is the sub (r) s free energy."}, {"heading": "5 Discriminative Training", "text": "Similar to the models we introduced in Section 3, we are also with the models we introduced in Section 3 on which we rely. (Jitkrittum et al., 2015), our current work results are chosen before choosing the core function, and only the parameters in the messages are learned from us by learning both the feature spaces and the transformation phases, as well as the ultimate regressor or or classifier for the target discrimination task using the supervision sigalsor sigalsor sigalsor sigalsor sigalsor sigalsor sigalsor sigalsor sigalsor sigalsor sigalsor sigalsor. Specifically, we are dataset with a training D = {n} Nn = 1, where we have a structured data point and a structured data collection."}, {"heading": "6 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Comparison with Neural Networks on Graphs", "text": "Scarselli et al. (2009) proposed a neural network that generates properties by iteratively solving a heuristic nonlinear system, and which is learned using the Almeida-Pineda algorithm. In order to guarantee the existence of the solution for the nonlinear system, there are additional requirements for the function of generating properties. From this perspective, the model in (Li et al., 2015) can be considered an extension of (Scarselli et al., 2009), in which the gated recurrent unit is used for the generation of characteristics. Instead of these heuristic models, our model is based on the principle-driven graphic model, which leads to flexible embedding functions for the generation of characteristics. Meanwhile, the model can be efficiently learned through traditional stochastic gradient decentration. Some works transfer the concept of localization of revolutionary neural networks (CNN) from the Euclidean domain, in order to connect with hierarchical cluster formation, Henlacian et structure (2015)."}, {"heading": "6.2 Comparison with Learning Message Estimator", "text": "By recognizing conclusions as computational expressions, inference machines (Ross et al., 2011) integrate learning into the messages that draw conclusions about CRFs. More recently, Hershey et al. (2014); Zheng et al. (2015); Lin et al. (2015) developed specific recurring neural networks and revolutionary neural networks to mimic messages in CRFs. Although these methods share the similarity, i.e. the evasion of learning potential function, with the proposed framework, there are significant differences compared to the proposed framework. The most important difference is in the learning environment. In these existing messages, learning works (Hershey et al., 2014; Zheng et al., 2015; Lin et al., 2015), with the learning task still being to estimate graphic models with designed functional forms (e.g. RNN or CNN) by maximizing loglicality."}, {"heading": "7 Experiments", "text": "Below, we will first compare our algorithm with kernel methods on string and graph benchmark datasets; then, we will focus on the Harvard Clean Energy Project dataset, which contains 2.3 million samples; we will show that while we achieve comparable performance on medium-sized datasets, we are able to handle millions of data points and become much better when more training samples are given; the two proposed algorithms are referred to as DE-MF and DE-LBP, which stands for discriminatory embedding using medium field or loopy belief; some structured data kernel methods will be included as baseline algorithms; in particular, we will compare datasets with the spectrum string kernel (Leslie et al, 2002a) and mismatch string kernel (Leslie et al, 2002b) on string datasets; on graph benchmark datasets, we will compare datasets (Kernel for 2003 Subtree Kernel & GUEA)."}, {"heading": "7.1 Benchmark structure datasets", "text": "Without being explicitly mentioned, we perform cross-validation for all methods and report on average performance. In structured core methods, we coordinate the degree in {1, 2,.., 10} (for inmatch kernels, we also coordinate the maximum inmatch length in {1, 2, 3}) and train SVM classifiers (Chang & Lin, 2001) above, with the target parameter C also selected in {0.01, 0.1, 1, 10} by cross-validation. For our methods, we simply use a hot vector (the vector representation of discrete node names) as an embedding for observed nodes and use a two-layer neural network to embed (predict) the target value. The hidden layer size b (16, 32, 64) of the neural network avoids embedding the dimension d (16, 32, 64} of hidden variables and the number of iterations (1, 2, 3, 4) by cross-validation."}, {"heading": "7.1.1 String Dataset", "text": "The first (referred to as SCOP) contains 7329 sequences derived from the Structural Classification of Proteins (SCOP) 1.59 database (An-dreeva et al., 2004). Methods are evaluated for their ability to detect members of a SCOP target family (positive test set) belonging to the same SCOP superfamily as the positive training sequences, and no members of the target family are available during the training. We use the same 54 target families and the same training / test splits as for remote homology detection (Kuang et al., 2005). The second is the FC and RES dataset (referred to as FC RES) provided by the CRISPR / Cas9 system. It asks the algorithm whether the RNA Cas9 guideline will lead to the target DNA, and a total of 5310 guidelines on this dataset are explicitly available in Doenal."}, {"heading": "7.1.2 Graph Dataset", "text": "We use the following five commonly used datasets in the literature of the graph core: MUTAG, NCI1, NCI109, ENZYMES and D & D. MUTAG (Debnath et al., 1991), NCI1 and NCI109 (Wale et al., 2008) are chemical compound datasets, while ENZYMES (Borgwardt & Kriegel, 2005) and D & D (Dobson & Doig, 2003) consist of proteins. The task is to perform a multi-class or binary classification; the detailed statistics of these datasets are shown in Table 2.The results of the basic algorithms come from Shervashidze et al. (2011), as we use exactly the same setting here. From the accuracy comparison shown in Figure 3, we can see that the proposed embedding methods are comparable to other graph cores, on different graphs with different numbers of labels, nodes and edges."}, {"heading": "7.2 Harvard Clean Energy Project(CEP) dataset", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to hide ourselves, and that we are able, that we will be able, that we will be able, that we will be able, that we will be able."}, {"heading": "8 Conclusion", "text": "Motivated by prevailing structural data learning tasks, we propose a new approach based on latent embedding of variable models. We use the graphical models to effectively capture the structural information in each data point. Meanwhile, we represent it through learned embedding to achieve both statistical and computational and storage efficiency. In particular, the nonlinear mappings for embedding graphical models and the ultimate regressor or classifier are learned along with the supervisory information, leading to more accurate predictions. Secondly, stochastic approximation is used for optimization, making the algorithm scalable to millions of data. Finally, we apply discriminatory embedding to nine structured data sets containing thousands of data for embedding, resulting in a small but efficient representation to achieve efficient storage efficiency. Empirically, we apply the discriminatory embedding to nine structured data sets, or to the pre-structured state of the nine data sets, in relation to the pre-structured state of the structured data."}, {"heading": "A Derivation of the Fixed-Point Condition for Loopy BP", "text": "The derivation of the fixed point condition for loopy BP is in Yedidia (hi-hi) (hi-hi) (hi-ji) (hi-ji) (hi-ji) (hi-ji) (hi-ji) (hi-ji) (hi-ji) (hi-ji) (hi-ji) (hi-ji) (hi) log qi (hi) (hi) (hi-xi) (hi-xi) dhidhjs (hi-ji) (hi-ji) dhj (hi-hi) qij (hi-ji) (hi-ji) (hi-ji) (hi-ji) log qi (hi) (hi-xi) dhidhidhjs (hi-hi) (hi qij) (hi) log (hi-hi-hi) (hi-qij) (hi) (hi qij) (hi) (hi qij) (hi) (hi) (hi) (hi) qij (hi) (hi) (hi) (hi) (hi) qij (hi) (hi) (hi) (hi) (hi) qij (hi) (hi) (hi) (hi) (hi) qij (hi) (hi) (hi) (hi) (hi) qij (hi) (hi) (hi) (hi (hi) (hi) (hi) (hi (hi) qij (hi) (hi) (hi (hi) (hi) (hi (hi) (hi) qij (hi (hi) (hi) (hi) (hi (hi) (hi) (hi (hi) qij (hi (hi) (hi) (hi) (hi (hi) (hi (hi) (hi (hi) (hi (hi) (hi) (hi) qij (hi (hi) (hi (hi) (hi (hi) (hi (hi) (hi) (hi (hi) (hi (hi) (hi (hi) (hi) (hi (hi) (hi (hi) (hi (hi) (hi) (hi) (hi (hi (hi) (hi) (hi (hi) (hi (hi) (hi (hi) (hi) (hi"}, {"heading": "B Message Updates for other Inference methods", "text": "In Section 4 we discuss the embedding of several alternatives to optimizing bethe-free energy (Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-Hi-H"}, {"heading": "C Derivatives Computation in Algorithm 3", "text": "We can apply the chain rule to get the derivatives with respect to UT = {VT, WT, uT). (According to Eq.23 and Eq.24, the message passed to the parent label for n \u2212 th samplept can be called mny = p = p = p = p = p = p = p = p = p = p (p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = h p = h p = h p = h p = h p p = h p p = h p p = p p p = p p p = p p = p p = p = p = p = h p = h p = h p = p = h p = p = h p = h = p = h p = p = h p = p = h p = p = h p = h = p = p = p = h = p = h = p = p = h = p = p = h = p = p = p = h = p = p = h = p = p = h = p = p = h = p = p = p = h = p = p = h = p = p = h = p = p = p = h = p = h = p = p = p = h = p = h = h = p = h = h = p = h = p = h = h = h = p = h = h = p = h = p = h = h = h = h = p = h p = h = p = h p = p = h = h p = h p = h p = h p = h = h p = h p = p = h = h p = h p = h p = h p = h p = p = h p = p = p p p p p p = p p p = p = p = p p p = p = p p p p p p = p p = p = p = p p = p p p p p p p = p p = p p p = p p p = p p p p p p = p = p p p p p p p p p p p = p = p p p p = p p p p p p p = p p = p p p p p = p = p p"}], "references": [{"title": "Scop database in 2004: refinements integrate structure and sequence family data", "author": ["Andreeva", "Antonina", "Howorth", "Dave", "Brenner", "Steven E", "Hubbard", "Tim JP", "Chothia", "Cyrus", "Murzin", "Alexey G"], "venue": "Nucleic acids research,", "citeRegEx": "Andreeva et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Andreeva et al\\.", "year": 2004}, {"title": "Shortest-path kernels on graphs", "author": ["Borgwardt", "Karsten M", "Kriegel", "Hans-Peter"], "venue": "In Proc. Intl. Conf. Data Mining, pp", "citeRegEx": "Borgwardt et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Borgwardt et al\\.", "year": 2005}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["Bruna", "Joan", "Zaremba", "Wojciech", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6203,", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": null, "citeRegEx": "Chang and Lin,? \\Q2001\\E", "shortCiteRegEx": "Chang and Lin", "year": 2001}, {"title": "Structureactivity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity", "author": ["A.K. Debnath", "R.L. Lopez de Compadre", "G. Debnath", "A.J. Shusterman", "C. Hansch"], "venue": "J Med Chem,", "citeRegEx": "Debnath et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Debnath et al\\.", "year": 1991}, {"title": "Distinguishing enzyme structures from non-enzymes without alignments", "author": ["P.D. Dobson", "A.J. Doig"], "venue": "J Mol Biol,", "citeRegEx": "Dobson and Doig,? \\Q2003\\E", "shortCiteRegEx": "Dobson and Doig", "year": 2003}, {"title": "Rational design of highly active sgrnas for crispr-cas9-mediated gene inactivation", "author": ["Doench", "John G", "Hartenian", "Ella", "Graham", "Daniel B", "Tothova", "Zuzana", "Hegde", "Mudra", "Smith", "Ian", "Sullender", "Meagan", "Ebert", "Benjamin L", "Xavier", "Ramnik J", "Root", "David E"], "venue": "Nature biotechnology,", "citeRegEx": "Doench et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Doench et al\\.", "year": 2014}, {"title": "Convolutional networks on graphs for learning molecular fingerprints", "author": ["Duvenaud", "David K", "Maclaurin", "Dougal", "Iparraguirre", "Jorge", "Bombarell", "Rafael", "Hirzel", "Timothy", "AspuruGuzik", "Al\u00e1n", "Adams", "Ryan P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Duvenaud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2015}, {"title": "In silico predictive modeling of crispr/cas9 guide efficiency", "author": ["Fusi", "Nicolo", "Smith", "Ian", "Doench", "John", "Listgarten", "Jennifer"], "venue": "bioRxiv,", "citeRegEx": "Fusi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fusi et al\\.", "year": 2015}, {"title": "On graph kernels: Hardness results and efficient alternatives", "author": ["T. G\u00e4rtner", "P.A. Flach", "S. Wrobel"], "venue": "Proc. Annual Conf. Computational Learning Theory,", "citeRegEx": "G\u00e4rtner et al\\.,? \\Q2003\\E", "shortCiteRegEx": "G\u00e4rtner et al\\.", "year": 2003}, {"title": "Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping", "author": ["Giles", "Rich Caruana Steve Lawrence Lee"], "venue": "In Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference,", "citeRegEx": "Giles and Lee.,? \\Q2001\\E", "shortCiteRegEx": "Giles and Lee.", "year": 2001}, {"title": "The harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community", "author": ["Hachmann", "Johannes", "Olivares-Amaya", "Roberto", "Atahan-Evrenk", "Sule", "Amador-Bedolla", "Carlos", "S\u00e1nchezCarrera", "Roel S", "Gold-Parker", "Aryeh", "Vogt", "Leslie", "Brockway", "Anna M", "Aspuru-Guzik", "Al\u00e1n"], "venue": "grid. The Journal of Physical Chemistry Letters,", "citeRegEx": "Hachmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hachmann et al\\.", "year": 2011}, {"title": "Deep convolutional networks on graph-structured data", "author": ["Henaff", "Mikael", "Bruna", "Joan", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1506.05163,", "citeRegEx": "Henaff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2015}, {"title": "Deep unfolding: Model-based inspiration of novel deep architectures", "author": ["Hershey", "John R", "Roux", "Jonathan Le", "Weninger", "Felix"], "venue": "arXiv preprint arXiv:1409.2574,", "citeRegEx": "Hershey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hershey et al\\.", "year": 2014}, {"title": "Stable fixed points of loopy belief propagation are local minima of the bethe free energy", "author": ["Heskes", "Tom"], "venue": null, "citeRegEx": "Heskes and Tom.,? \\Q2002\\E", "shortCiteRegEx": "Heskes and Tom.", "year": 2002}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["T.S. Jaakkola", "D. Haussler"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Jaakkola and Haussler,? \\Q1999\\E", "shortCiteRegEx": "Jaakkola and Haussler", "year": 1999}, {"title": "Probability product kernels", "author": ["T. Jebara", "R. Kondor", "A. Howard"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Jebara et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jebara et al\\.", "year": 2004}, {"title": "Kernel-based just-in-time learning for passing expectation propagation messages", "author": ["Jitkrittum", "Wittawat", "Gretton", "Arthur", "Heess", "Nicolas", "Eslami", "S.M. Ali", "Lakshminarayanan", "Balaji", "Sejdinovic", "Dino", "Szab\u00f3", "Zolt\u00e1n"], "venue": "In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Jitkrittum et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jitkrittum et al\\.", "year": 2015}, {"title": "Profilebased string kernels for remote homology detection and motif extraction", "author": ["Kuang", "Rui", "Ie", "Eugene", "Wang", "Ke", "Kai", "Siddiqi", "Mahira", "Freund", "Yoav", "Leslie", "Christina"], "venue": "Journal of bioinformatics and computational biology,", "citeRegEx": "Kuang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kuang et al\\.", "year": 2005}, {"title": "Rdkit: Open-source cheminformatics", "author": ["G. Landrum"], "venue": null, "citeRegEx": "Landrum,? \\Q2013\\E", "shortCiteRegEx": "Landrum", "year": 2013}, {"title": "The spectrum kernel: A string kernel for SVM protein classification", "author": ["C. Leslie", "E. Eskin", "W.S. Noble"], "venue": "In Proceedings of the Pacific Symposium on Biocomputing,", "citeRegEx": "Leslie et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Leslie et al\\.", "year": 2002}, {"title": "Mismatch string kernels for SVM protein classification", "author": ["C. Leslie", "E. Eskin", "J. Weston", "W.S. Noble"], "venue": "Advances in Neural Information Processing Systems 15,", "citeRegEx": "Leslie et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Leslie et al\\.", "year": 2002}, {"title": "Gated graph sequence neural networks", "author": ["Li", "Yujia", "Tarlow", "Daniel", "Brockschmidt", "Marc", "Zemel", "Richard"], "venue": "arXiv preprint arXiv:1511.05493,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Deeply learning the messages in message passing inference", "author": ["G. Lin", "C. Shen", "I. Reid", "A. van den Hengel"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "The EP energy function and minimization schemes", "author": ["T. Minka"], "venue": "See www. stat. cmu. edu/minka/papers/learning. html,", "citeRegEx": "Minka,? \\Q2001\\E", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Convolutional neural networks over tree structures for programming language processing", "author": ["Mou", "Lili", "Li", "Ge", "Zhang", "Lu", "Wang", "Tao", "Jin", "Zhi"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["Murphy", "Kevin P", "Weiss", "Yair", "Jordan", "Michael I"], "venue": "In UAI, pp", "citeRegEx": "Murphy et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 1999}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Learning from the harvard clean energy project: The use of neural networks to accelerate materials discovery", "author": ["Pyzer-Knapp", "Edward O", "Li", "Kewei", "Aspuru-Guzik", "Alan"], "venue": "Advanced Functional Materials,", "citeRegEx": "Pyzer.Knapp et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pyzer.Knapp et al\\.", "year": 2015}, {"title": "Expressivity versus efficiency of graph kernels", "author": ["J. Ramon", "T. G\u00e4rtner"], "venue": "Technical report, First International Workshop on Mining Graphs, Trees and Sequences (held with ECML/PKDD\u201903),", "citeRegEx": "Ramon and G\u00e4rtner,? \\Q2003\\E", "shortCiteRegEx": "Ramon and G\u00e4rtner", "year": 2003}, {"title": "Learning message-passing inference machines for structured prediction", "author": ["Ross", "Stephane", "Munoz", "Daniel", "Hebert", "Martial", "Bagnell", "J Andrew"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "The graph neural network model", "author": ["Scarselli", "Franco", "Gori", "Marco", "Tsoi", "Ah Chung", "Hagenbuchner", "Markus", "Monfardini", "Gabriele"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Scarselli et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Scarselli et al\\.", "year": 2009}, {"title": "Kernel Methods in Computational Biology", "author": ["B. Sch\u00f6lkopf", "K. Tsuda", "Vert", "J.-P"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2004}, {"title": "Learning with Kernels", "author": ["Sch\u00f6lkopf", "Bernhard", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2002}, {"title": "Efficient graphlet kernels for large graph comparison", "author": ["Shervashidze", "Nino", "S.V.N. Vishwanathan", "Petri", "Tobias", "Mehlhorn", "Kurt", "Borgwardt", "Karsten"], "venue": "Proc. Intl. Conference on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics,", "citeRegEx": "Shervashidze et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shervashidze et al\\.", "year": 2009}, {"title": "Weisfeiler-lehman graph kernels", "author": ["Shervashidze", "Nino", "Schweitzer", "Pascal", "Van Leeuwen", "Erik Jan", "Mehlhorn", "Kurt", "Borgwardt", "Karsten M"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Shervashidze et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shervashidze et al\\.", "year": 2011}, {"title": "A Hilbert space embedding for distributions", "author": ["A.J. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In Proceedings of the International Conference on Algorithmic Learning Theory,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Hilbert space embeddings of conditional distributions", "author": ["L. Song", "J. Huang", "A.J. Smola", "K. Fukumizu"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "Nonparametric tree graphical models", "author": ["L. Song", "A. Gretton", "C. Guestrin"], "venue": "In 13th Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "Kernel belief propagation", "author": ["L. Song", "A. Gretton", "D. Bickson", "Y. Low", "C. Guestrin"], "venue": "In Proc. Intl. Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}, {"title": "Injective Hilbert space embeddings of probability measures", "author": ["B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "G. Lanckriet", "B. Sch\u00f6lkopf"], "venue": "In Proc. Annual Conf. Computational Learning Theory,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2008}, {"title": "Halting in random walk kernels", "author": ["Sugiyama", "Mahito", "Borgwardt", "Karsten"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sugiyama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2015}, {"title": "Fast kernels for string and tree matching", "author": ["S.V.N. Vishwanathan", "A.J. Smola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Vishwanathan and Smola,? \\Q2003\\E", "shortCiteRegEx": "Vishwanathan and Smola", "year": 2003}, {"title": "Tree-reweighted belief propagation and approximate ML estimation by pseudo-moment matching", "author": ["M. Wainwright", "T. Jaakkola", "A. Willsky"], "venue": "In 9th Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Wainwright et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2003}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Comparison of descriptor spaces for chemical compound retrieval and classification", "author": ["Wale", "Nikil", "Watson", "Ian A", "Karypis", "George"], "venue": "Knowledge and Information Systems,", "citeRegEx": "Wale et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wale et al\\.", "year": 2008}, {"title": "Generalized belief propagation", "author": ["Yedidia", "Jonathan S", "Freeman", "William T", "Weiss", "Yair"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Yedidia et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2001}, {"title": "Bethe free energy, kikuchi approximations and belief propagation algorithms", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "Technical report, Mitsubishi Electric Research Laboratories,", "citeRegEx": "Yedidia et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2001}, {"title": "Constructing free-energy approximations and generalized belief propagation algorithms", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Yedidia et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2005}, {"title": "Cccp algorithms to minimize the bethe and kikuchi free energies: Convergent alternatives to belief propagation", "author": ["A.L. Yuille"], "venue": "Neural Computation,", "citeRegEx": "Yuille,? \\Q2002\\E", "shortCiteRegEx": "Yuille", "year": 2002}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Zheng", "Shuai", "Jayasumana", "Sadeep", "Romera-Paredes", "Bernardino", "Vineet", "Vibhav", "Su", "Zhizhong", "Du", "Dalong", "Huang", "Chang", "Torr", "Philip"], "venue": "arXiv preprint arXiv:1502.03240,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "However, to keep the paper self-contained, we provide the details here", "author": ["Appendix A Derivation of the Fixed-Point Condition for Loopy BP The derivation of the fixed-point condition for loopy BP can be found in Yedidia"], "venue": "The objective of loopy BP is min", "citeRegEx": "Yedidia,? 2001b", "shortCiteRegEx": "Yedidia", "year": 2001}], "referenceMentions": [{"referenceID": 32, "context": "Structured data, such as sequences, trees and graphs, are prevalent in a number of interdisciplinary areas such as protein design, genomic sequence analysis, and drug design (Sch\u00f6lkopf et al., 2004).", "startOffset": 174, "endOffset": 198}, {"referenceID": 9, "context": ", 2002a; Vishwanathan & Smola, 2003) and graph datasets (G\u00e4rtner et al., 2003; Borgwardt, 2007).", "startOffset": 56, "endOffset": 95}, {"referenceID": 34, "context": ", 2002a), subtree kernel (Ramon & G\u00e4rtner, 2003), graphlet kernel (Shervashidze et al., 2009) and Weisfeiler-lehman graph kernel (Shervashidze et al.", "startOffset": 66, "endOffset": 93}, {"referenceID": 35, "context": ", 2009) and Weisfeiler-lehman graph kernel (Shervashidze et al., 2011) all follow this design principle.", "startOffset": 43, "endOffset": 70}, {"referenceID": 16, "context": "The Fisher kernel (Jaakkola & Haussler, 1999) and probability product kernel (Jebara et al., 2004) are two representative instances within the family.", "startOffset": 77, "endOffset": 98}, {"referenceID": 36, "context": "Our idea is to model each structured data point as a latent variable model, then embed the graphical model into feature spaces (Smola et al., 2007; Song et al., 2009), and use inner product in the embedding space to define kernels.", "startOffset": 127, "endOffset": 166}, {"referenceID": 37, "context": "Our idea is to model each structured data point as a latent variable model, then embed the graphical model into feature spaces (Smola et al., 2007; Song et al., 2009), and use inner product in the embedding space to define kernels.", "startOffset": 127, "endOffset": 166}, {"referenceID": 34, "context": "Similarly, the graphlet kernel (Shervashidze et al., 2009) for two graphs x and x\u2032 can be defined as in the same form as (1), where S is now the set of possible subgraphs, #(s \u2208 x) counts the number occurrence of subgraphs.", "startOffset": 31, "endOffset": 58}, {"referenceID": 16, "context": "Another classical example along the same line is the probability product kernel (Jebara et al., 2004).", "startOffset": 80, "endOffset": 101}, {"referenceID": 36, "context": "Hilbert space embeddings of distributions are mappings of distributions into potentially infinite dimensional feature spaces (Smola et al., 2007), \u03bcX := EX [\u03c6(X)] = \u222b", "startOffset": 125, "endOffset": 145}, {"referenceID": 40, "context": "Some feature map can make the mapping injective (Sriperumbudur et al., 2008), meaning that if two distributions, p(X) and q(X), are different, they are mapped to two distinct points in the feature space.", "startOffset": 48, "endOffset": 76}, {"referenceID": 27, "context": "Only when the graph structure is a tree, exact computation can be carried out efficiently via a message passing algorithm (Pearl, 1988).", "startOffset": 122, "endOffset": 135}, {"referenceID": 26, "context": "In many applications, however, the resulting mean fields and loopy BP algorithm exhibits excellent empirical performance (Murphy et al., 1999).", "startOffset": 121, "endOffset": 142}, {"referenceID": 49, "context": ", convex-concave decomposition (Yuille, 2002) and duality (Minka, 2001).", "startOffset": 31, "endOffset": 45}, {"referenceID": 24, "context": ", convex-concave decomposition (Yuille, 2002) and duality (Minka, 2001).", "startOffset": 58, "endOffset": 71}, {"referenceID": 43, "context": "By optimizing a convexified Bethe free energy, Wainwright et al. (2003) propose the tree-reweighted version BP with weight for each message, {vij}i,j\u2208E , in spanning tree polytope.", "startOffset": 47, "endOffset": 72}, {"referenceID": 24, "context": "The algorithms proposed for minimizing the Bethe free energy (Minka, 2001; Heskes, 2002; Yuille, 2002) can also be extended for Kikuchi free energy, resulting in different embedding forms.", "startOffset": 61, "endOffset": 102}, {"referenceID": 49, "context": "The algorithms proposed for minimizing the Bethe free energy (Minka, 2001; Heskes, 2002; Yuille, 2002) can also be extended for Kikuchi free energy, resulting in different embedding forms.", "startOffset": 61, "endOffset": 102}, {"referenceID": 45, "context": "We start from the messages into the smallest region first (Yedidia et al., 2001b). Remark: The choice of basis clusters and the form of messages determine the dependency in the embedding. Please refer to Yedidia et al. (2005) for details about the principles to partition the graph structure, and several other generalized BP variants with different messages.", "startOffset": 59, "endOffset": 226}, {"referenceID": 17, "context": ", 2010, 2011) and kernel EP (Jitkrittum et al., 2015), our current work exploits embedding to reformulate the graphical models and avoids explicitly learning the potential functions of the graphical models.", "startOffset": 28, "endOffset": 53}, {"referenceID": 22, "context": "From this perspective, the model in (Li et al., 2015) can be considered as an extension of (Scarselli et al.", "startOffset": 36, "endOffset": 53}, {"referenceID": 31, "context": ", 2015) can be considered as an extension of (Scarselli et al., 2009) where the gated recurrent unit is used for feature generation.", "startOffset": 45, "endOffset": 69}, {"referenceID": 2, "context": "Some works transfer locality concept of convolutional neural networks (CNN) from Euclidean domain to graph case, using hierarchical clustering, graph Laplacian (Bruna et al., 2013), or graph Fourier transform (Henaff et al.", "startOffset": 160, "endOffset": 180}, {"referenceID": 12, "context": ", 2013), or graph Fourier transform (Henaff et al., 2015).", "startOffset": 36, "endOffset": 57}, {"referenceID": 26, "context": "Scarselli et al. (2009) proposed a neural network which generates features by solving a heuristic nonlinear system iteratively, and is learned using Almeida-Pineda algorithm.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Some works transfer locality concept of convolutional neural networks (CNN) from Euclidean domain to graph case, using hierarchical clustering, graph Laplacian (Bruna et al., 2013), or graph Fourier transform (Henaff et al., 2015). These models are still restricted to problems with the same graph structure, which is not suitable for learning with molecules. Mou et al. (2016) proposed a convolution operation on trees, while the locality are defined based on parent-child relations.", "startOffset": 161, "endOffset": 378}, {"referenceID": 2, "context": "Some works transfer locality concept of convolutional neural networks (CNN) from Euclidean domain to graph case, using hierarchical clustering, graph Laplacian (Bruna et al., 2013), or graph Fourier transform (Henaff et al., 2015). These models are still restricted to problems with the same graph structure, which is not suitable for learning with molecules. Mou et al. (2016) proposed a convolution operation on trees, while the locality are defined based on parent-child relations. Duvenaud et al. (2015) used CNN to learn the circulant fingerprints for graphs from end to end.", "startOffset": 161, "endOffset": 508}, {"referenceID": 30, "context": "2 Comparison with Learning Message Estimator By recognizing inference as computational expressions, inference machines (Ross et al., 2011) incorporate learning into the messages passing inference for CRFs.", "startOffset": 119, "endOffset": 138}, {"referenceID": 13, "context": "More recently, Hershey et al. (2014); Zheng et al.", "startOffset": 15, "endOffset": 37}, {"referenceID": 13, "context": "More recently, Hershey et al. (2014); Zheng et al. (2015); Lin et al.", "startOffset": 15, "endOffset": 58}, {"referenceID": 13, "context": "More recently, Hershey et al. (2014); Zheng et al. (2015); Lin et al. (2015) designed specific recurrent neural networks and convolutional neural networks for imitating the messages in CRFs.", "startOffset": 15, "endOffset": 77}, {"referenceID": 13, "context": "In these existing messages learning work (Hershey et al., 2014; Zheng et al., 2015; Lin et al., 2015), the learning task is still estimating the messages represented graphical models with designed function forms, e.", "startOffset": 41, "endOffset": 101}, {"referenceID": 50, "context": "In these existing messages learning work (Hershey et al., 2014; Zheng et al., 2015; Lin et al., 2015), the learning task is still estimating the messages represented graphical models with designed function forms, e.", "startOffset": 41, "endOffset": 101}, {"referenceID": 23, "context": "In these existing messages learning work (Hershey et al., 2014; Zheng et al., 2015; Lin et al., 2015), the learning task is still estimating the messages represented graphical models with designed function forms, e.", "startOffset": 41, "endOffset": 101}, {"referenceID": 9, "context": "On graph benchmark datasets, we compare with subtree kernel (Ramon & G\u00e4rtner, 2003) (R&G, for short), random walk kernel(G\u00e4rtner et al., 2003; Vishwanathan et al., 2010), shortest path kernel (Borgwardt & Kriegel, 2005), graphlet kernel(Shervashidze et al.", "startOffset": 120, "endOffset": 169}, {"referenceID": 34, "context": ", 2010), shortest path kernel (Borgwardt & Kriegel, 2005), graphlet kernel(Shervashidze et al., 2009) and the family of WeisfeilerLehman kernels (Shervashidze et al.", "startOffset": 74, "endOffset": 101}, {"referenceID": 35, "context": ", 2009) and the family of WeisfeilerLehman kernels (Shervashidze et al., 2011) (WL kernel, for short).", "startOffset": 51, "endOffset": 78}, {"referenceID": 9, "context": "On graph benchmark datasets, we compare with subtree kernel (Ramon & G\u00e4rtner, 2003) (R&G, for short), random walk kernel(G\u00e4rtner et al., 2003; Vishwanathan et al., 2010), shortest path kernel (Borgwardt & Kriegel, 2005), graphlet kernel(Shervashidze et al., 2009) and the family of WeisfeilerLehman kernels (Shervashidze et al., 2011) (WL kernel, for short). We didn\u2019t compare with fisher kernel and probabilistic product kernel in the experiments since they are taking too much time to run, and some of our baselines (e.g., Leslie et al. (2002a)) has already shown better results than those methods.", "startOffset": 121, "endOffset": 547}, {"referenceID": 18, "context": "We use the same 54 target families and the same training/test splits as in remote homology detection (Kuang et al., 2005).", "startOffset": 101, "endOffset": 121}, {"referenceID": 6, "context": "Details of this dataset can be found in Doench et al. (2014); Fusi et al.", "startOffset": 40, "endOffset": 61}, {"referenceID": 6, "context": "Details of this dataset can be found in Doench et al. (2014); Fusi et al. (2015). We use two variants for spectrum string kernel: 1) kmer-single, where the constructed kernel matrix K (s) k only consider patterns of length k; 2) kmer-concat, where kernel matrix K = \u2211k i=1K (s) k .", "startOffset": 40, "endOffset": 81}, {"referenceID": 4, "context": "MUTAG (Debnath et al., 1991), NCI1 and NCI109 (Wale et al.", "startOffset": 6, "endOffset": 28}, {"referenceID": 45, "context": ", 1991), NCI1 and NCI109 (Wale et al., 2008) are chemical compounds dataset, while ENZYMES (Borgwardt & Kriegel, 2005) and D&D (Dobson & Doig, 2003) are", "startOffset": 25, "endOffset": 44}, {"referenceID": 34, "context": "The results of baseline algorithms are taken from Shervashidze et al. (2011) since we use exactly the same setting here.", "startOffset": 50, "endOffset": 77}, {"referenceID": 34, "context": "The results of baseline algorithms are taken from Shervashidze et al. (2011) since we use exactly the same setting here. From the accuracy comparison shown in Figure 3, we can see the proposed embedding methods are comparable to other graph kernels, on different graphs with different number of labels, nodes and edges. Also, in dataset D&D which consists of 82 different types of labels, our algorithm performs much better. As reported in Shervashidze et al. (2011), the time required for constructing dictionary for the graph kernel can take up to more than a year of CPU time in this dataset, while our algorithm can learn the discriminative embedding from structured data directly without the construction of dictionary.", "startOffset": 50, "endOffset": 467}, {"referenceID": 11, "context": "2 Harvard Clean Energy Project(CEP) dataset The Harvard Clean Energy Project (Hachmann et al., 2011) is a theory-driven search for the next generation of organic solar cell materials.", "startOffset": 77, "endOffset": 100}, {"referenceID": 11, "context": "2 Harvard Clean Energy Project(CEP) dataset The Harvard Clean Energy Project (Hachmann et al., 2011) is a theory-driven search for the next generation of organic solar cell materials. One of the most important properties of molecule for this task is the overall efficiency of the energy conversion process in a solar cell, which is determined by the power conversion efficiency (PCE). The Clean Energy Project (CEP) performed expensive simulations for the 2.3 million candidate molecules on IBMs World Community Grid, in order to get this property value. So using machine learning approach to accurately predict the PCE values is a promising direction for the high throughput screening and discovering new materials. In this experiment, we randomly select 90% of the data for training, and the rest 10% for testing. This setting is similar to Pyzer-Knapp et al. (2015), except that we use entire 2.", "startOffset": 78, "endOffset": 869}], "year": 2017, "abstractText": "Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced in a number of interdisciplinary areas such as computational biology and drug design. Typically, kernel functions are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations. We propose an effective and scalable approach for structured data representation which is based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Furthermore, our feature learning algorithm runs a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In real world applications involving sequences and graphs, we showed that the proposed approach is much more scalable than alternatives while at the same time produce comparable results to the state-of-the-art in terms of classification and regression.", "creator": "LaTeX with hyperref package"}}}