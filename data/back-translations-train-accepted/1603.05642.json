{"id": "1603.05642", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2016", "title": "Optimal Black-Box Reductions Between Optimization Objectives", "abstract": "The diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods, finely tuned to the smoothness, convexity, and other parameterizations of the objective. In this paper we attempt to simplify and reduce the complexity of algorithm design for machine learning by reductions: we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity found in practice.", "histories": [["v1", "Thu, 17 Mar 2016 19:51:59 GMT  (3863kb,D)", "http://arxiv.org/abs/1603.05642v1", null], ["v2", "Thu, 24 Mar 2016 05:11:42 GMT  (3864kb,D)", "http://arxiv.org/abs/1603.05642v2", "corrected a few typos in version 2"], ["v3", "Fri, 20 May 2016 17:03:15 GMT  (3658kb,D)", "http://arxiv.org/abs/1603.05642v3", "new applications of our optimal reductions are obtained in this version 3"]], "reviews": [], "SUBJECTS": "math.OC cs.DS cs.LG stat.ML", "authors": ["zeyuan allen zhu", "elad hazan"], "accepted": true, "id": "1603.05642"}, "pdf": {"name": "1603.05642.pdf", "metadata": {"source": "CRF", "title": "Optimal Black-Box Reductions Between Optimization Objectives", "authors": ["Zeyuan Allen-Zhu", "Elad Hazan"], "emails": ["zeyuan@csail.mit.edu", "ehazan@cs.princeton.edu"], "sections": [{"heading": null, "text": "We show how these new reductions lead to faster runtimes of linear classifiers for certain families of loss functions and that our reductions are optimal and generally cannot be improved. We conclude with experiments that show that our reductions successfully transform methods between areas and achieve the desired performance predicted by theory."}, {"heading": "1 Introduction", "text": "The basic mechanical learning problem of minimizing a loss function and regularizing over a number of examples of a specific hypothesis class occurs in numerous different variations and designations. Examples include: \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"\" A, \"\" \"B,\" \"B,\" \"B,\" B, \"\" B, \"B,\" B, \"\" B, \"B,\" B, \"\" B, \"B,\" B, \"B,\" B, \"B,\" B, \"B,\" B, \"B,\" B, \"B,\" B, \"B,\" B, \"B,\" B, \"B,\" B, \"B,\" B, \"B,\" B, \"B,\" B, \"B,\" B, \"B,\", \"B,\" B, \"B,\", \"B,\" B, \"B,\" B, \"\", \",\" B, \"B,\" \",\" B, \",\", \"B,\", \"B,\", \"B,\", \"B,\", \"B,\", \"B,\", \",\" B, B, \"\", \",\" B, \",\", \"B,\", \",\" B, \"B,\", \"B,\" B, \"\", \",\", \"B,\" B, \"\", \", B, B,\", \",\", \",\" B, \"B,\", \"B,\", B, \", B,\" \",\" B, \",\", B, \"\", \",\", \",\" B, B, \", B,\" \",\", \",\", B, \",\" B, \",\", \"B,\", \"B,\", \"B, B,\", \"\", \",\" \",\", \", B,\" B, \"\", \",\", \",\" B, \",\", \", B,\", \", B,\", \",\", \", B,\", B, \",\","}, {"heading": "2 Preliminaries", "text": "In this paper we call the full gradient vector of function f if it is differentiable (1,1), or the subgradient vector if f is only lipschitz continuous. \u2022 Let us remember some classic definitions of strong convexity and smoothness. Definition 2.1 (smoothness and strong convexity). Definition 2.1 (smoothness and strong convexity). Definition f (x), y \u2212 x 2.2 (smoothness and strong convexity). Definition f (smoothness and strong convexity). Definition 2.2 (smoothness and strong convexity). Definition 2.2 (smoothness and strong convexicity).Definition f (smoothness and strong convexicity). Definition 2.2 (smoothness and strong convexicity). Definition 2.2 (smoothness and strong convexicity)."}, {"heading": "6 Experiments", "text": "We are conducting experiments to test the theoretical speed-ups we have received for regulating and smoothing this issue. (In particular, we are working on empirical risk minimization for the following three datasets, which can be downloaded from the LibSVM website.) However, we cannot simply apply this method (581, 012 samples and 54 features) to make comparison between the individual datasets easier, for each dataset we show using the average Euclidean norm of all vectors. In other words, we are making sure that the data vectors have an average Euclidean norm 1. This step is only for comparison and not necessary in practice."}], "references": [{"title": "Linear coupling: An ultimate unification of gradient and mirror descent", "author": ["Allen-Zhu", "Zeyuan", "Orecchia", "Lorenzo"], "venue": "ArXiv e-prints,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A geometric alternative to Nesterov\u2019s accelerated gradient descent", "author": ["Bubeck", "S\u00e9bastien", "Lee", "Yin Tat", "Singh", "Mohit"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "author": ["Chambolle", "Antonin", "Pock", "Thomas"], "venue": "Journal of Mathematical Imaging and Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives", "author": ["Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "LIBSVM Data: Classification, Regression and Multi-label", "author": ["Fan", "Rong-En", "Lin", "Chih-Jen"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "DRAFT: Introduction to online convex optimimization", "author": ["Hazan", "Elad"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Beyond the regret minimization barrier: Optimal algorithms for stochastic strongly-convex optimization", "author": ["Hazan", "Elad", "Kale", "Satyen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A simpler approach to obtaining an o(1/t) convergence rate for the projected stochastic subgradient method", "author": ["Lacoste-Julien", "Simon", "Schmidt", "Mark W", "Bach", "Francis R"], "venue": "ArXiv e-prints,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Efficient accelerated coordinate descent methods and faster algorithms for solving linear systems", "author": ["Lee", "Yin Tat", "Sidford", "Aaron"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Analysis and design of optimization algorithms via integral quadratic constraints", "author": ["Lessard", "Laurent", "Recht", "Benjamin", "Packard", "Andrew"], "venue": "CoRR, abs/1408.3595,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "An Accelerated Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minimization", "author": ["Lin", "Qihang", "Lu", "Zhaosong", "Xiao"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning", "author": ["Mairal", "Julien"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Prox-Method with Rate of Convergence O(1/t) for Variational Inequalities with Lipschitz Continuous Monotone Operators and Smooth Convex-Concave Saddle Point Problems", "author": ["Nemirovski", "Arkadi"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k2)", "author": ["Nesterov", "Yurii"], "venue": "In Doklady AN SSSR (translated as Soviet Mathematics Doklady),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1983}, {"title": "Introductory Lectures on Convex Programming Volume: A Basic course, volume I", "author": ["Nesterov", "Yurii"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Smooth minimization of non-smooth functions", "author": ["Nesterov", "Yurii"], "venue": "Mathematical Programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Rakhlin", "Alexander", "Shamir", "Ohad", "Sridharan", "Karthik"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Schmidt", "Mark", "Le Roux", "Nicolas", "Bach", "Francis"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Online Learning and Online Convex Optimization", "author": ["Shalev-Shwartz", "Shai"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Proximal Stochastic Dual Coordinate Ascent", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "arXiv preprint arXiv:1211.2717,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "A Proximal Stochastic Gradient Method with Progressive Variance Reduction", "author": ["Xiao", "Lin", "Zhang", "Tong"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization", "author": ["Zhang", "Yuchen", "Xiao", "Lin"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "For instance, it is famously known that by applying (proximal) gradient descent starting from vector x0, one can obtain an \u03b5approximate minimizer of F (x) in T = O ( L \u03c3 log F (x0)\u2212F (x\u2217) \u03b5 ) iterations [17].", "startOffset": 203, "endOffset": 207}, {"referenceID": 15, "context": "This can be improved to T = O( \u221a L \u221a \u03c3 log F (x0)\u2212F (x \u2217) \u03b5 ) if the so-called (proximal) accelerated gradient descent is used, see [16] or Constant Step Scheme in [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "This can be improved to T = O( \u221a L \u221a \u03c3 log F (x0)\u2212F (x \u2217) \u03b5 ) if the so-called (proximal) accelerated gradient descent is used, see [16] or Constant Step Scheme in [17].", "startOffset": 164, "endOffset": 168}, {"referenceID": 16, "context": "For instance, the accelerated gradient descent method has its non-strongly convex variant (see General Scheme in [17]) and its non-smooth variant [18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "For instance, the accelerated gradient descent method has its non-strongly convex variant (see General Scheme in [17]) and its non-smooth variant [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 6, "context": "The second approach is to apply a reduction from non-strongly convex or non-smooth objectives into a strongly convex and smooth one, via the classical regularization or smoothing techniques: (see [7] Section 2.", "startOffset": 196, "endOffset": 199}, {"referenceID": 16, "context": "For instance, the optimal dependence on \u03b5 should be 1/ \u221a \u03b5 for first-order methods on smooth but non-strongly convex objectives (see General Scheme of [17]), as compared to log(1/\u03b5)/ \u221a \u03b5 obtained via regularization.", "startOffset": 151, "endOffset": 155}, {"referenceID": 2, "context": "The optimal dependence on \u03b5 should be 1/ \u221a \u03b5 for first-order methods on finite-sum, non-smooth but strongly convex objectives [3, 18], as compared to log(1/\u03b5)/ \u221a \u03b5 obtained via smoothing.", "startOffset": 126, "endOffset": 133}, {"referenceID": 17, "context": "The optimal dependence on \u03b5 should be 1/ \u221a \u03b5 for first-order methods on finite-sum, non-smooth but strongly convex objectives [3, 18], as compared to log(1/\u03b5)/ \u221a \u03b5 obtained via smoothing.", "startOffset": 126, "endOffset": 133}, {"referenceID": 19, "context": "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.", "startOffset": 29, "endOffset": 32}, {"referenceID": 13, "context": "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.", "startOffset": 43, "endOffset": 47}, {"referenceID": 7, "context": "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.", "startOffset": 98, "endOffset": 109}, {"referenceID": 9, "context": "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.", "startOffset": 98, "endOffset": 109}, {"referenceID": 18, "context": "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.", "startOffset": 98, "endOffset": 109}, {"referenceID": 2, "context": "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.", "startOffset": 195, "endOffset": 206}, {"referenceID": 14, "context": "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.", "startOffset": 195, "endOffset": 206}, {"referenceID": 17, "context": "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.", "startOffset": 195, "endOffset": 206}, {"referenceID": 16, "context": "We give a few example algorithms that satisfy HOOD: \u2022 The full gradient descent method solves Case 1 and satisfies HOOD with Time(L, \u03c3) = O(L/\u03c3) \u00b7 C where C is the time needed to compute a gradient \u2207f(x) and performing a proximal gradient update [17].", "startOffset": 246, "endOffset": 250}, {"referenceID": 15, "context": "\u2022 The accelerated gradient descent method of Nesterov [16, 17], when puts in its proximal form, solves Case 1 and satisfies HOOD with Time(L, \u03c3) = O( \u221a L/ \u221a \u03c3) \u00b7C.", "startOffset": 54, "endOffset": 62}, {"referenceID": 16, "context": "\u2022 The accelerated gradient descent method of Nesterov [16, 17], when puts in its proximal form, solves Case 1 and satisfies HOOD with Time(L, \u03c3) = O( \u221a L/ \u221a \u03c3) \u00b7C.", "startOffset": 54, "endOffset": 62}, {"referenceID": 1, "context": "Many subsequent works in this line of research also satisfy HOOD, including [2, 11, 12].", "startOffset": 76, "endOffset": 87}, {"referenceID": 10, "context": "Many subsequent works in this line of research also satisfy HOOD, including [2, 11, 12].", "startOffset": 76, "endOffset": 87}, {"referenceID": 11, "context": "Many subsequent works in this line of research also satisfy HOOD, including [2, 11, 12].", "startOffset": 76, "endOffset": 87}, {"referenceID": 8, "context": "\u2022 The SVRG [9] method and its proximal version [25] solve the finite-sum form of Case 1 and satisfy HOOD with Time(L, \u03c3) = O ( n + L\u03c3 ) \u00b7 C1 where C1 is the time need to compute a Smoothing reduction is typically applied to the finite sum form only because, for a general high dimensional function f(x), its smoothed variant f\u0302(x) may not be efficiently computable.", "startOffset": 11, "endOffset": 14}, {"referenceID": 24, "context": "\u2022 The SVRG [9] method and its proximal version [25] solve the finite-sum form of Case 1 and satisfy HOOD with Time(L, \u03c3) = O ( n + L\u03c3 ) \u00b7 C1 where C1 is the time need to compute a Smoothing reduction is typically applied to the finite sum form only because, for a general high dimensional function f(x), its smoothed variant f\u0302(x) may not be efficiently computable.", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "3 of [7], and the other is to define f\u0302i(\u03b1) = max\u03b2 { \u03b2 \u00b7 \u03b1 \u2212 f\u2217 i (\u03b2) \u2212 \u03b5 2\u03b12} using the Fenchel dual f\u2217 i (\u03b2) of fi(\u03b1), see for instance [18].", "startOffset": 5, "endOffset": 8}, {"referenceID": 17, "context": "3 of [7], and the other is to define f\u0302i(\u03b1) = max\u03b2 { \u03b2 \u00b7 \u03b1 \u2212 f\u2217 i (\u03b2) \u2212 \u03b5 2\u03b12} using the Fenchel dual f\u2217 i (\u03b2) of fi(\u03b1), see for instance [18].", "startOffset": 138, "endOffset": 142}, {"referenceID": 4, "context": "It can be verified that AdaGrad [5] also satisfies HOOD.", "startOffset": 32, "endOffset": 35}, {"referenceID": 1, "context": "Some accelerated gradient methods only apply to Case 1 and do not directly apply to Case 2 [2, 11, 12].", "startOffset": 91, "endOffset": 102}, {"referenceID": 10, "context": "Some accelerated gradient methods only apply to Case 1 and do not directly apply to Case 2 [2, 11, 12].", "startOffset": 91, "endOffset": 102}, {"referenceID": 11, "context": "Some accelerated gradient methods only apply to Case 1 and do not directly apply to Case 2 [2, 11, 12].", "startOffset": 91, "endOffset": 102}, {"referenceID": 11, "context": "4 of [12].", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "The SVRG [9] method does not provide any theoretical guarantee for the finite-sum form of Case 2.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "This improves on the best known theoretical running time obtained by non-accelerated methods, including O ( n log 1\u03b5 + L \u03b5 log 1 \u03b5 ) \u00b7C1 through the old reduction, as well as O ( n+L \u03b5 ) \u00b7 C1 through direct methods such as SAGA [4] and SAG [20].", "startOffset": 228, "endOffset": 231}, {"referenceID": 19, "context": "This improves on the best known theoretical running time obtained by non-accelerated methods, including O ( n log 1\u03b5 + L \u03b5 log 1 \u03b5 ) \u00b7C1 through the old reduction, as well as O ( n+L \u03b5 ) \u00b7 C1 through direct methods such as SAGA [4] and SAG [20].", "startOffset": 240, "endOffset": 244}, {"referenceID": 1, "context": "We can apply AdaptSmooth or JointAdaptRegSmooth to the aforementioned algorithms [2, 11, 12] in order to solve the finite-sum forms of Case 3 or 4.", "startOffset": 81, "endOffset": 92}, {"referenceID": 10, "context": "We can apply AdaptSmooth or JointAdaptRegSmooth to the aforementioned algorithms [2, 11, 12] in order to solve the finite-sum forms of Case 3 or 4.", "startOffset": 81, "endOffset": 92}, {"referenceID": 11, "context": "We can apply AdaptSmooth or JointAdaptRegSmooth to the aforementioned algorithms [2, 11, 12] in order to solve the finite-sum forms of Case 3 or 4.", "startOffset": 81, "endOffset": 92}, {"referenceID": 14, "context": "In contrast, to obtain such optimal methods, one usually needs very different primal-dual approaches such as [15] or [3].", "startOffset": 109, "endOffset": 113}, {"referenceID": 2, "context": "In contrast, to obtain such optimal methods, one usually needs very different primal-dual approaches such as [15] or [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 12, "context": "Some machine learning algorithms such as APCG [13], SPDC [26], AccSDCA [24] and SDCA [23] are not known to satisfy HOOD.", "startOffset": 46, "endOffset": 50}, {"referenceID": 25, "context": "Some machine learning algorithms such as APCG [13], SPDC [26], AccSDCA [24] and SDCA [23] are not known to satisfy HOOD.", "startOffset": 57, "endOffset": 61}, {"referenceID": 23, "context": "Some machine learning algorithms such as APCG [13], SPDC [26], AccSDCA [24] and SDCA [23] are not known to satisfy HOOD.", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "Some machine learning algorithms such as APCG [13], SPDC [26], AccSDCA [24] and SDCA [23] are not known to satisfy HOOD.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "When AdaptReg is applied to an accelerated gradient descent method designed for Case 1 such as [1, 2, 11, 12], we solve Case 2 with a total running time \u2211T\u22121 t=0 Time(L, \u03c30 \u00b72\u2212t) = O(Time(L, \u03c3T )) = O( \u221a L/\u03c3T ) \u00b7C = O( \u221a L\u0398/\u03b5) \u00b7C.", "startOffset": 95, "endOffset": 109}, {"referenceID": 1, "context": "When AdaptReg is applied to an accelerated gradient descent method designed for Case 1 such as [1, 2, 11, 12], we solve Case 2 with a total running time \u2211T\u22121 t=0 Time(L, \u03c30 \u00b72\u2212t) = O(Time(L, \u03c3T )) = O( \u221a L/\u03c3T ) \u00b7C = O( \u221a L\u0398/\u03b5) \u00b7C.", "startOffset": 95, "endOffset": 109}, {"referenceID": 10, "context": "When AdaptReg is applied to an accelerated gradient descent method designed for Case 1 such as [1, 2, 11, 12], we solve Case 2 with a total running time \u2211T\u22121 t=0 Time(L, \u03c30 \u00b72\u2212t) = O(Time(L, \u03c3T )) = O( \u221a L/\u03c3T ) \u00b7C = O( \u221a L\u0398/\u03b5) \u00b7C.", "startOffset": 95, "endOffset": 109}, {"referenceID": 11, "context": "When AdaptReg is applied to an accelerated gradient descent method designed for Case 1 such as [1, 2, 11, 12], we solve Case 2 with a total running time \u2211T\u22121 t=0 Time(L, \u03c30 \u00b72\u2212t) = O(Time(L, \u03c3T )) = O( \u221a L/\u03c3T ) \u00b7C = O( \u221a L\u0398/\u03b5) \u00b7C.", "startOffset": 95, "endOffset": 109}, {"referenceID": 3, "context": "This is faster than applying the old reduction which gives a running time O (( n+ L\u03b5 ) log L\u0398 \u03b5 ) \u00b7C1, and also faster than using direct methods such as SAGA [4] and SAG [20] which give a running time O ( n+L\u0398 \u03b5 ) \u00b7C1.", "startOffset": 158, "endOffset": 161}, {"referenceID": 19, "context": "This is faster than applying the old reduction which gives a running time O (( n+ L\u03b5 ) log L\u0398 \u03b5 ) \u00b7C1, and also faster than using direct methods such as SAGA [4] and SAG [20] which give a running time O ( n+L\u0398 \u03b5 ) \u00b7C1.", "startOffset": 170, "endOffset": 174}, {"referenceID": 20, "context": "From the property of Fenchel conjugate (see for instance the textbook [21]), we know that f (\u03bb) i (\u00b7) is a (1/\u03bb)-smooth function and therefore the objective F (\u03bb)(x) falls into the finite-sum form of Case 1 for problem (1.", "startOffset": 70, "endOffset": 74}, {"referenceID": 5, "context": "In particular, we work on empirical risk minimizations for the following three datasets that can be publicly downloaded from the LibSVM website [6]: \u2022 the covtype (binary.", "startOffset": 144, "endOffset": 147}, {"referenceID": 12, "context": "We apply AdaptReg to reduce it to Case 1 and apply either APCG [13], an accelerated method, or (Prox-)SDCA [22, 23], a non-accelerated method.", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "We apply AdaptReg to reduce it to Case 1 and apply either APCG [13], an accelerated method, or (Prox-)SDCA [22, 23], a non-accelerated method.", "startOffset": 107, "endOffset": 115}, {"referenceID": 22, "context": "We apply AdaptReg to reduce it to Case 1 and apply either APCG [13], an accelerated method, or (Prox-)SDCA [22, 23], a non-accelerated method.", "startOffset": 107, "endOffset": 115}, {"referenceID": 21, "context": ", automatic) choice which is Option I for SDCA (see [22]).", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "We apply AdaptSmooth to reduce it to Case 1 and apply SVRG [9].", "startOffset": 59, "endOffset": 62}, {"referenceID": 21, "context": "6 Note that some other methods, such as APCG or SDCA, although only providing theoretical guarantees for strongly convex and smooth objectives (Case 1), in practice work for Case 2 directly without smoothing (see for instance the discussion in [22]).", "startOffset": 244, "endOffset": 248}], "year": 2017, "abstractText": "The diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods, finely tuned to the smoothness, convexity, and other parameterizations of the objective. In this paper we attempt to simplify and reduce the complexity of algorithm design for machine learning by reductions: we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity found in practice. We show how these new reductions give rise to faster running times on training linear classifiers for certain families of loss functions, and that our reductions are optimal and cannot be improved in general. We conclude with experiments showing our reductions successfully transform methods between domains and achieve the desired performance predicted by theory.", "creator": "LaTeX with hyperref package"}}}