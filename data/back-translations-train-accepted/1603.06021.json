{"id": "1603.06021", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "A Fast Unified Model for Parsing and Sentence Understanding", "abstract": "Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they can only operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 25x over other tree-structured models, and its integrated parser allows it to operate on unparsed data with little loss of accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models.", "histories": [["v1", "Sat, 19 Mar 2016 00:22:20 GMT  (203kb,D)", "http://arxiv.org/abs/1603.06021v1", "Manuscript under review"], ["v2", "Mon, 13 Jun 2016 23:19:08 GMT  (352kb,D)", "http://arxiv.org/abs/1603.06021v2", "To appear at ACL 2016"], ["v3", "Fri, 29 Jul 2016 18:36:15 GMT  (349kb,D)", "http://arxiv.org/abs/1603.06021v3", "To appear at ACL 2016"]], "COMMENTS": "Manuscript under review", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["samuel r bowman", "jon gauthier", "abhinav rastogi", "raghav gupta", "christopher d manning", "christopher potts"], "accepted": true, "id": "1603.06021"}, "pdf": {"name": "1603.06021.pdf", "metadata": {"source": "CRF", "title": "A Fast Unified Model for Parsing and Sentence Understanding", "authors": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts"], "emails": ["sbowman@stanford.edu", "jgauthie@stanford.edu", "arastogi@stanford.edu", "rgupta93@stanford.edu", "manning@stanford.edu", "cgpotts@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, as never before, it will be able to retaliate, to retaliate."}, {"heading": "2 Related work", "text": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures of transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). Furthermore, there is recent work suggesting models that are primarily developed for generative language modeling tasks that also use this architecture (Zhang et al., 2016; Dyer et al., 2016). Ours is the first model that uses this architecture for sentence interpretation, rather than analyzing or generating it. Socher et al. (2011a, b) present versions of the TreeRNN model that are able to work via unparsed inputs. However, these test methods require a costly search time process."}, {"heading": "3 Our model: SPINN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Background: Shift-reduce parsing", "text": "SPINN is inspired by the Shift-Reduce-Parsing-Formalism (Aho and Ullman, 1972), which builds a tree structure over a sequence (e.g. a sentence in natural language) through a single left-right scan of its tokens. Formalism is often used in natural language analysis (e.g. Shieber, 1983; Nivre, 2003).A Shift-Reduce-Parser accepts a sequence of input tokens x = (x0,., xN \u2212 1) and consumes transitions t = (t0,., tT \u2212 1), with each tt-reduction parser indicating a step in the parsing process. Generally, a parser can also create these transitions on the fly while reading the tokens. It goes from left to right through a transition operation, combining the input tokens x incrementally into a tree structure."}, {"heading": "3.2 Composition and representation", "text": "SPINN is based on a shift-reduce parser, but it is designed to produce a vector representation of a set as output, rather than a tree as with standard shift-reduce parsing. It modifies the shift-reduction formalism by using ~ ~ ~ length vectors to represent each entry in the stack and buffer. Accordingly, the vector representations of two tree nodes are removed from the stack and fed into a composition function, which is a neural networking function that creates a representation for a new tree node that is the parent of the two nodes. This new node is pushed onto the stack."}, {"heading": "3.3 The tracking LSTM", "text": "In addition to the stack, buffer, and composition function, our complete model includes an additional component: the tracking LSTM. This is a simple low-dimensional sequence-based LSTM RNN that works in tandem with the model by taking input from the buffer and stack at each step. It is intended to maintain a low-resolution summary of the set, which is used for two purposes: It provides feature representations to the transit classifier, which allows the model to stand alone as a parser, and it also provides a secondary input of ~ e (see Equation 1) to the composition function, which incorporates context information into the construction of the sentence meaning, and forms what is effectively a tree-sequence hybrid model. Tracking LSTM's input (yellow in Figure 2) is the uppermost element of the buffer ~ h1b (which would be moved in a SHIFT operation ~ hybrid model) and the top two elements of the stack are the hybrids."}, {"heading": "3.4 Parsing: Predicting transitions", "text": "For SPINN to work with unparsed input, it must be able to generate its own transition sequence t, rather than relying on an external parser to provide it as part of the input. To do this, the tt model forecasts each step with a simple two-way softmax classifier whose input is the state of the tracking LSTM: (5) ~ pt = Softmax (Wtrans ~ htracking + ~ btrans) At test time, the model uses any transition (i.e. SHIFT or REDUCE) that is assigned a higher probability. The prediction function is trained to mimic the decisions of an external parser, and these decisions are used as inputs to the model during the training. For SNLI, we use the Stanford binary PCFG parser parses included in the corpus."}, {"heading": "3.5 Implementation issues", "text": "This year is the highest in the history of the country."}, {"heading": "3.6 TreeRNN-equivalence", "text": "Without the addition of the tracking LSTM, SPINN (in particular the SPINN-PI-NT variant for parsed input, not tracking) in the function it computes is exactly the same as a traditional tree-structured neural network model and therefore has the same learning dynamics. In both cases, the representation of each sentence consists of the recursive representation of the words, which are combined using a TreeRNN composition function (in our case the TreeLSTM function). SPINN, however, is dramatically faster and supports both integrated parsing and a novel approach to context by the tracking LSTM."}, {"heading": "3.7 Inference speed", "text": "In this section, we compare the test speed of our SPINN-PI-NT with a corresponding TreeRNN implemented conventionally and with a standard RNN sequence model. While the full models evaluated below are implemented and trained with Theano (Bergstra et al., 2010; Bastien et al., 2012), which are relatively efficient but not perfect for our model, we would like to compare well-optimized implementations of all three models. To do this, we implement the Feedford1 of SPINN-PI-NT and an LSTM RNN baseline in C + / CUDA, and compare this implementation with a CPU-based C + + / proprietary TreeRNN code from Irsoy and Cardie we have modified to exactly the same complications as SPINN-PI-NT.2 TreeRNNNNNs like this model."}, {"heading": "4 NLI Experiments", "text": "We evaluate SPINN on the basis of the task of drawing conclusions about natural language (NLI, also known as recognition of textual connections or RTE; Dagan et al., 2006). NLI is a task for classifying pairs of sentences, in which a model reads two sentences (a premise and a hypothesis) and makes a judgment about connections, contradictions, or neutrality that reflects the relationship between the meanings of the two sentences, as in this example from the SNLI corpus we use: premise: girls in red coats, blue headscarves, and jeans make a snow angel. Hypothesis: A girl out in the snow. Description: Context: Although NLI is framed as a simple triple classification task, it is nevertheless an effective method for assessing the ability of a model to extract a largely informative representation of the meaning of the sentence. In order for a model to function reliably on NLI, it must be able to quantify the nuclear phenomena we quantify 85S for most of the types, including 85S."}, {"heading": "4.1 Applying SPINN to SNLI", "text": "To classify an SNLI set pair, we run two copies of SPINN with common parameters: one on the premise and another on the hypotheses set. We then use their results (the ~ h states at the top of each stack at a time t = T) to construct a feature vector ~ xclassifier for the pair. This feature vector consists of linking these two set vectors, their difference and their elementary product (according to Mou et al., 2015): (6) ~ xclassifier = ~ hpremise ~ hhypothesis ~ hpremise \u2212 hhypothesis ~ hhypothesis ~ hhypothesis According to Bowman et al. (2015a) this feature vector is then passed to a series of 1024D ReLU neural network layers (i.e., an MLP; the number of layers is tuned as hyperparameters)."}, {"heading": "4.2 Models evaluated", "text": "We evaluate four models. All four use the set pair classifier architecture described in Section 4.1 and differ only in the function for calculating the set encodings. First, a single-layer LSTM-RNN (similar to that of Bowmanet al., 2015a) serves as the baseline encoder. Next, the minimum SPINN-PI-NT model (equivalent to a TreeLSTM) introduces the SPINN model design. SPINN-PI adds the tracking LSTM to this design. Finally, the full SPINN adds the built-in parser. We compare our models with multiple baselines, including the strongest published non-neural network-based result from Bowman et al. (2015a) and previous neural network models built around multiple types of set encoders."}, {"heading": "4.3 Results", "text": "Table 2 shows our results on SNLI sequence classification. For the full SPINN prediction, we also report a measure of match between the parses of this model and the parses captured with SNLI, calculated as classification accuracy over averaged transitions. We find that the mere SPINN-PI-NT model performs little better than the RNN baseline, but that SPINN-PI performs well with the added tracking LSTM. The success of the SPINN-PI, a hybrid tree sequence model, suggests that the tree and sequence-based coding method is at least partially complementary. The full SPINN model, with its relatively weak internal parser, performs slightly less well, but far outperforms the performance of the RNN base model. Both SPINN-PI and the full SPINN base model (SPINN base data) show significantly higher accuracy than all previous saturation codes."}, {"heading": "4.4 Discussion", "text": "We suspect that this improvement is largely due to more efficient learning of accurate generalizations as a whole, rather than a few specific phenomena. However, some patterns can be identified in the results. In particular, it seems that tree-structured models are better able to focus on the key actors and events mentioned in a sentence (possibly by learning an approximation of the linguistic concept of the head), and that they are thus better able to focus on the central actors and events described in a sentence. For example, this pair has been successfully classified by all three SPINN variants, but not by the RNN baseline (keywords highlighted): premise: A woman in red blouse stands with a small blond child in front of a small collapsible chalkboard. Hypothesis: A woman stands with her child at the center, but not able to parse the tree sequence hybrid model."}, {"heading": "5 Conclusions and future work", "text": "We are introducing a model architecture (SPINN-PINT) that corresponds to a TreeLSTM but is an order of magnitude faster in the test period. We are extending this architecture into a tree sequence hybrid model (SPINN-PI) and show that this has significant benefits for the SNLI task. Finally, we show that it is possible to leverage the strengths of this model without the need for an external parser by integrating a fast parser into the model (as in full SPINN), and that the lack of external parse information yields little loss of accuracy. As this paper aims to introduce a general purpose model for sentence coding, we do not pursue the use of soft attention (Bahdanau et al., 2015; Rockta-schel et al., 2015), despite its proven effectiveness on the SNLI task, we expect it should be productive."}, {"heading": "Acknowledgments", "text": "We acknowledge funding from a Google Faculty Research Award and the Stanford Data Science Initiative. In addition, this material is based on work supported by the National Science Foundation under grant number BCS 1456077. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. Some of the Tesla K40 used in this research have been donated by NVIDIA Corporation."}, {"heading": "A Hyperparameters", "text": "We use random searches to adjust the hyperparameters of the model, specify the ranges to search for each hyperparameter heuristically (and validate the appropriateness of the ranges on the development set), and then start eight copies of each experiment, each with newly sampled hyperparameters from those ranges. Table 3 (on the following page) shows the hyperparameters used in the best execution of each model."}], "references": [{"title": "The theory of parsing, translation, and compiling", "author": ["Alfred V. Aho", "Jeffrey D. Ullman."], "venue": "Prentice-Hall, Inc.", "citeRegEx": "Aho and Ullman.,? 1972", "shortCiteRegEx": "Aho and Ullman.", "year": 1972}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio."], "venue": "Deep Learning and Unsuper-", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer."], "venue": "Proc. NIPS.", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Theano: a CPU and GPU math expression", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Bowman et al\\.,? 2015a", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Tree-structured composition in neural networks without treestructured architectures", "author": ["Samuel R. Bowman", "Christopher D. Manning", "Christopher Potts."], "venue": "Proc. 2015 NIPS", "citeRegEx": "Bowman et al\\.,? 2015b", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Generative incremental dependency parsing with neural networks", "author": ["Jan Buys", "Phil Blunsom."], "venue": "Proc. ACL.", "citeRegEx": "Buys and Blunsom.,? 2015", "shortCiteRegEx": "Buys and Blunsom.", "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."], "venue": "arXiv:1601.06733.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."], "venue": "Machine learning challenges. Evaluating predictive uncertainty, visual object classification, and recognising tec-", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Compositionality as an empirical problem", "author": ["David Dowty."], "venue": "Direct Compositionality, Oxford Univ. Press.", "citeRegEx": "Dowty.,? 2007", "shortCiteRegEx": "Dowty.", "year": 2007}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proc. ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "arXiv:1602.07776.", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "A neural syntactic language model", "author": ["Ahmad Emami", "Frederick Jelinek."], "venue": "Machine learning 60(1-3).", "citeRegEx": "Emami and Jelinek.,? 2005", "shortCiteRegEx": "Emami and Jelinek.", "year": 2005}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas K\u00fcchler."], "venue": "Proc. IEEE International Conference on Neural Networks.", "citeRegEx": "Goller and K\u00fcchler.,? 1996", "shortCiteRegEx": "Goller and K\u00fcchler.", "year": 1996}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom."], "venue": "arXiv:1506.02516.", "citeRegEx": "Grefenstette et al\\.,? 2015", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv:1502.01852.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Discriminative training of a neural network statistical parser", "author": ["James Henderson."], "venue": "Proc. ACL.", "citeRegEx": "Henderson.,? 2004", "shortCiteRegEx": "Henderson.", "year": 2004}, {"title": "The zipper", "author": ["G\u00e9rard Huet."], "venue": "Journal of functional programming 7(05).", "citeRegEx": "Huet.,? 1997", "shortCiteRegEx": "Huet.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "arXiv:1502.03167.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Proc. NIPS.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov."], "venue": "Proc. NIPS.", "citeRegEx": "Joulin and Mikolov.,? 2015", "shortCiteRegEx": "Joulin and Mikolov.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proc. ACL.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Easy-first dependency parsing with hierarchical tree LSTMs", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "arXiv:1603.00375.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "When are tree structures necessary for deep learning of representations? In Proc", "author": ["Jiwei Minh-Thang Luong Li", "Dan Jurafsky", "Eudard Hovy."], "venue": "EMNLP.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Recognizing entailment and contradiction by tree-based convolution", "author": ["Lili Mou", "Men Rui", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin."], "venue": "arXiv:1512.08422.", "citeRegEx": "Mou et al\\.,? 2015", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre."], "venue": "Proc. IWPT .", "citeRegEx": "Nivre.,? 2003", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom."], "venue": "arXiv:1509.06664.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2015", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Sentence disambiguation by a shift-reduce parsing technique", "author": ["Stuart M. Shieber."], "venue": "Proc. ACL.", "citeRegEx": "Shieber.,? 1983", "shortCiteRegEx": "Shieber.", "year": 1983}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C. Lin", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proc. ICML.", "citeRegEx": "Socher et al\\.,? 2011a", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "JMLR 15.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proc. NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long shortterm memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. ACL.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Lecture 6.5 \u2013 RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning 4:2", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "A latent variable model for generative dependency parsing", "author": ["Ivan Titov", "James Henderson."], "venue": "Trends in Parsing Technology, Springer.", "citeRegEx": "Titov and Henderson.,? 2010", "shortCiteRegEx": "Titov and Henderson.", "year": 2010}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun."], "venue": "arXiv:1511.06361.", "citeRegEx": "Vendrov et al\\.,? 2015", "shortCiteRegEx": "Vendrov et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "arXiv:1509.01626.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Top-down tree long short-term memory networks", "author": ["Xingxing Zhang", "Liang Lu", "Mirella Lapata."], "venue": "arXiv:1511.00060.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 34, "context": "A wide range of current models in NLP are built around a neural network component that produces vector representations of sentence meaning (e.g., Sutskever et al., 2014; Tai et al., 2015).", "startOffset": 139, "endOffset": 187}, {"referenceID": 23, "context": "models (RNNs, see Figure 1a) with Long ShortTerm Memory (LSTM, Hochreiter and Schmidhuber, 1997), which accumulate information over the sentence sequentially; convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and K\u00fcchler, 1996; Socher et al.", "startOffset": 189, "endOffset": 236}, {"referenceID": 38, "context": "models (RNNs, see Figure 1a) with Long ShortTerm Memory (LSTM, Hochreiter and Schmidhuber, 1997), which accumulate information over the sentence sequentially; convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and K\u00fcchler, 1996; Socher et al.", "startOffset": 189, "endOffset": 236}, {"referenceID": 34, "context": "TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have largely been overlooked in favor of sequencebased RNNs because of their incompatibility with ar X iv :1 60 3.", "startOffset": 28, "endOffset": 85}, {"referenceID": 25, "context": "TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have largely been overlooked in favor of sequencebased RNNs because of their incompatibility with ar X iv :1 60 3.", "startOffset": 28, "endOffset": 85}, {"referenceID": 6, "context": "TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have largely been overlooked in favor of sequencebased RNNs because of their incompatibility with ar X iv :1 60 3.", "startOffset": 28, "endOffset": 85}, {"referenceID": 18, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 14, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 36, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 8, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 7, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 12, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 24, "context": "There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016).", "startOffset": 203, "endOffset": 371}, {"referenceID": 39, "context": "In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016).", "startOffset": 150, "endOffset": 189}, {"referenceID": 13, "context": "In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016).", "startOffset": 150, "endOffset": 189}, {"referenceID": 0, "context": "SPINN is inspired by the shift-reduce parsing formalism (Aho and Ullman, 1972), which builds a tree structure over a sequence (e.", "startOffset": 56, "endOffset": 78}, {"referenceID": 27, "context": "The formalism is widely used in natural language parsing (e.g., Shieber, 1983; Nivre, 2003).", "startOffset": 57, "endOffset": 91}, {"referenceID": 34, "context": "The TreeLSTM composition function (Tai et al., 2015) generalizes the LSTM neural network layer to tree- rather than sequence-based inputs, and it shares with the LSTM the idea of representing intermediate states as a pair of a fast-changing state representation ~h and a slower-changing memory representation ~c.", "startOffset": 34, "endOffset": 52}, {"referenceID": 28, "context": "Word representations We use word representations based on the standard 300D vector package provided with GloVe (Pennington et al., 2014).", "startOffset": 111, "endOffset": 136}, {"referenceID": 3, "context": "We did not find scheduled sampling (Bengio et al., 2015)\u2014allowing the model to use its own transition decisions in some instances at training time\u2014to help.", "startOffset": 35, "endOffset": 56}, {"referenceID": 19, "context": "We propose an alternative space-efficient stack representation inspired by the zipper technique (Huet, 1997), that we call thin stack.", "startOffset": 96, "endOffset": 108}, {"referenceID": 4, "context": "While the full models evaluated below are implemented and trained using Theano (Bergstra et al., 2010; Bastien et al., 2012), which is reasonably efficient but not perfect for our model, we wish to compare well-optimized implementations of all three models.", "startOffset": 79, "endOffset": 124}, {"referenceID": 2, "context": "While the full models evaluated below are implemented and trained using Theano (Bergstra et al., 2010; Bastien et al., 2012), which is reasonably efficient but not perfect for our model, we wish to compare well-optimized implementations of all three models.", "startOffset": 79, "endOffset": 124}, {"referenceID": 2, "context": ", 2010; Bastien et al., 2012), which is reasonably efficient but not perfect for our model, we wish to compare well-optimized implementations of all three models. To do this, we reimplement the feedforward1 of SPINN-PI-NT and an LSTM RNN baseline in C++/CUDA, and compare that implementation with a CPU-based C++/Eigen TreeRNN implementation from Irsoy and Cardie (2014), which we modified to perform exactly the same computations as SPINN-PI-NT.", "startOffset": 8, "endOffset": 371}, {"referenceID": 21, "context": "Batch size Fe ed fo rw ar d tim e (s ec ) Thin-stack GPU CPU (Irsoy and Cardie, 2014) RNN", "startOffset": 61, "endOffset": 85}, {"referenceID": 10, "context": "We evaluate SPINN on the task of natural language inference (NLI, a.k.a. recognizing textual entailment, or RTE; Dagan et al., 2006).", "startOffset": 60, "endOffset": 132}, {"referenceID": 5, "context": "Following Bowman et al. (2015a), this feature vector is then passed to a series of 1024D ReLU neural network layers (i.", "startOffset": 10, "endOffset": 32}, {"referenceID": 5, "context": "Previous non-NN results Lexicalized classifier (Bowman et al., 2015a) \u2014 \u2014 99.", "startOffset": 47, "endOffset": 69}, {"referenceID": 5, "context": "Previous sentence encoder-based NN results 100D LSTM encoders (Bowman et al., 2015a) 221k \u2014 84.", "startOffset": 62, "endOffset": 84}, {"referenceID": 37, "context": "6 1024D pretrained GRU encoders (Vendrov et al., 2015) 15m \u2014 98.", "startOffset": 32, "endOffset": 54}, {"referenceID": 26, "context": "4 300D Tree-based CNN encoders (Mou et al., 2015) 3.", "startOffset": 31, "endOffset": 49}, {"referenceID": 17, "context": "Initialization, optimization, and tuning We initialize the model parameters using the nonparametric strategy of He et al. (2015), with the exception of the softmax classifier parameters, which we initialize using random uniform samples from [\u22120.", "startOffset": 112, "endOffset": 129}, {"referenceID": 35, "context": "We use minibatch SGD with the RMSProp optimizer (Tieleman and Hinton, 2012) and a tuned starting learning rate that decays by a factor of 0.", "startOffset": 48, "endOffset": 75}, {"referenceID": 32, "context": "We apply both dropout (Srivastava et al., 2014) and batch normalization (Ioffe and Szegedy, 2015) to the output of the word embedding projection layer and to the feature vectors that serve as the inputs and outputs to the MLP that precedes the final entailment classifier.", "startOffset": 22, "endOffset": 47}, {"referenceID": 20, "context": ", 2014) and batch normalization (Ioffe and Szegedy, 2015) to the output of the word embedding projection layer and to the feature vectors that serve as the inputs and outputs to the MLP that precedes the final entailment classifier.", "startOffset": 32, "endOffset": 57}, {"referenceID": 5, "context": "We compare our models against several baselines, including the strongest published non-neural network-based result from Bowman et al. (2015a) and previous neural network models built around several types of sentence encoders.", "startOffset": 120, "endOffset": 142}, {"referenceID": 26, "context": "Most notably, these models outperform the tree-based CNN of Mou et al. (2015), which also uses tree-structured composition for local feature extraction, but uses simpler pooling techniques to build sentence features in the interest of efficiency.", "startOffset": 60, "endOffset": 78}, {"referenceID": 1, "context": "Because this paper aims to introduce a general purpose model for sentence encoding, we do not pursue the use of soft attention (Bahdanau et al., 2015; Rockt\u00e4schel et al., 2015), despite its demonstrated effectiveness on the SNLI task.", "startOffset": 127, "endOffset": 176}, {"referenceID": 29, "context": "Because this paper aims to introduce a general purpose model for sentence encoding, we do not pursue the use of soft attention (Bahdanau et al., 2015; Rockt\u00e4schel et al., 2015), despite its demonstrated effectiveness on the SNLI task.", "startOffset": 127, "endOffset": 176}, {"referenceID": 22, "context": "For a more ambitious goal, we expect that it should be possible to implement a variant of SPINN on top of a modified stack data structure with differentiable PUSH and POP operations (as in Grefenstette et al., 2015; Joulin and Mikolov, 2015).", "startOffset": 182, "endOffset": 241}, {"referenceID": 1, "context": "Because this paper aims to introduce a general purpose model for sentence encoding, we do not pursue the use of soft attention (Bahdanau et al., 2015; Rockt\u00e4schel et al., 2015), despite its demonstrated effectiveness on the SNLI task.3 However, we expect that it should be possible to productively combine our model with soft attention to reach state-of-the-art performance. Our tracking LSTM uses only simple, quickto-compute features drawn from the head of the buffer and the head of the stack. It is plausible that giving the tracking LSTM access to more information from the buffer and stack at each step would allow it to better represent the context at each tree node, yielding both better parsing and better sentence encoding. One promising way to pursue this goal would be to encode the full contents of the stack and buffer at each time step following the method used by Dyer et al. (2015). For a more ambitious goal, we expect that it should be possible to implement a variant of SPINN on top of a modified stack data structure with differentiable PUSH and POP operations (as in Grefenstette et al.", "startOffset": 128, "endOffset": 899}, {"referenceID": 28, "context": "Attention based models like Rockt\u00e4schel et al. (2015) and the unpublished Cheng et al.", "startOffset": 28, "endOffset": 54}, {"referenceID": 9, "context": "(2015) and the unpublished Cheng et al. (2016) have shown accuracies as high as 89.", "startOffset": 27, "endOffset": 47}], "year": 2016, "abstractText": "Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they can only operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stackaugmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single treesequence hybrid model by integrating treestructured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 25x over other tree-structured models, and its integrated parser allows it to operate on unparsed data with little loss of accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models.", "creator": "TeX"}}}