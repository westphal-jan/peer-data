{"id": "1603.06042", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Globally Normalized Transition-Based Neural Networks", "abstract": "We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.", "histories": [["v1", "Sat, 19 Mar 2016 03:56:03 GMT  (38kb)", "http://arxiv.org/abs/1603.06042v1", null], ["v2", "Wed, 8 Jun 2016 13:43:30 GMT  (39kb)", "http://arxiv.org/abs/1603.06042v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["daniel andor", "chris alberti", "david weiss", "aliaksei severyn", "alessandro presta", "kuzman ganchev", "slav petrov", "michael collins"], "accepted": true, "id": "1603.06042"}, "pdf": {"name": "1603.06042.pdf", "metadata": {"source": "CRF", "title": "Globally Normalized Transition-Based Neural Networks", "authors": ["Daniel Andor", "Chris Alberti", "David Weiss"], "emails": ["andor@google.com", "chrisalberti@google.com", "djweiss@google.com", "severyn@google.com", "apresta@google.com", "kuzman@google.com", "slav@google.com", "mjcollins@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.06 042v 1 [cs.C L] 19 Mar 201 6"}, {"heading": "1 Introduction", "text": "This year, we will be able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "2 Model", "text": "At its core, our model is an incremental transition-based parser (Nivre, 2006), which simply needs to adapt the transition system and input functions to perform different tasks."}, {"heading": "2.1 Transition System", "text": "Faced with an input x, most often a sentence, we define: \u2022 A set of states S. \u2022 A special initial state s = 1 = 1. \u2022 A set of permitted decisions A (s) for all S-S. \u2022 A transitional function t (s, d) returns a new state s \"for each decision d-A (s). We drop the dependence on x for brevity. We will use a function \u03c1 (s, d; \u03b8) to calculate the evaluation of the decision d in state s. The vector \u03b8 contains the model parameters and we assume that \u03c1 (s, d; \u03b8) is differentiable in relation to the dependence. In the course of this work, we will use transitional systems in which all complete structures for the same input x have the same number of decisions. (or n for brevity). In dependency saving, for example, this applies to both the arc standard and the arc-eager transition systems d.\""}, {"heading": "2.2 Global vs. Local Normalization", "text": "In the style of greedy neural network paralysis of Chen and Manning (2014), the conditional probability distribution of decisions dj is given context d1: j \u2212 1 asp (dj | d1: j \u2212 1; \u03b8) = exp \u03c1 (d1: j \u2212 1, dj; \u03b8) ZL (d1: j \u2212 1; \u03b8), (1) whereas ZL (d1: j \u2212 1; \u03b8) = \u2211 d \u2032 A (d1: j \u2212 1) exp \u03c1 (d1: j \u2212 1, d \u2032).Each ZL (d1: j \u2212 1; \u03b8) is a local normalization term. The probability of a decision sequence d1: n ispL (d1: j \u2212 j) = n ispL (d1: j \u2212 n) = n isp1 (dj | dj = n) = 1p (dj | d1: j \u2212 1; \u03b8).The probability of a decision sequence d1: n \u2212 n ispL (d1: j \u2212 j) = n (dj | dj: 1) can be search \u2212 dj \u2212 1."}, {"heading": "2.3 Training", "text": "Training data consist of inputs x paired with gold decision sequences d * 1: n. We use stochastic gradient lineage on the negative log probability of the data using the model. Under a locally normalized model, the negative log probability is Llocal (d * 1: n; \u03b8) = \u2212 ln pL (d * 1: n; \u03b8) = (4) \u2212 n \u00b2 j = 1\u03c1 (d * 1: j \u2212 1, d * j; \u03b8) + n \u00b2 j = 1lnZL (d * 1: j \u2212 1; \u03b8), whereas under a globally normalized model isLglobal (d * 1: n; g *) = \u2212 ln pG (d * 1: n; \u03b8) = \u2212 n \u00b2 j = 1\u03c1 (d * j * j * j *) + lnZG (n *) a significant practical advantage of locally normalized costs (4) is that they flow into independent terms, which we can each use on the basis of \"minimized\" gold stock inputs using the training data * 1 under negative load *."}, {"heading": "3 The Label Bias Problem", "text": "We would intuitively like the model to be able to reverse an earlier decision made during the search when later results become available that rule out the earlier decision as wrong. At first glance, it might appear that a locally standardized model used in conjunction with beam search or exact search is able to reverse earlier decisions. However, this section gives a more formal perspective on the label bias than in previous work, by proving that globally standardized model is strictly more meaningful than locally standardized model. Evidence is provided by the use of an example that provides an illustration of the label bias problem. Global models can be expressed as local models."}, {"heading": "4 Experiments", "text": "To demonstrate the flexibility and modeling capability of our approach, we provide experimental results on a variety of structured prediction tasks. We turn our attention first to POS tagging, then to syntactical dependency analysis, and finally to sentence compression. Although direct optimization of the global model (5) works well, we have found that training the model achieves the same precision much faster in two steps: we train the network based on the local target (4) and then perform additional training steps based on the global target (6). In this way, we pre-train all layers except the Softmax layer. We consciously forego complicated hand-engineering of input functions that could further improve performance (Durrett and Klein, 2015)."}, {"heading": "4.1 Part of Speech Tagging", "text": "Part of Language (POS) Tagging is a classic NLP task where modelling the structure of output is important to achieve state-of-the-art performance.Data & Evaluation We conducted experiments on a number of different data sets: (1) English Wall Street Journal (WSJ) part of Penn Treebank (Marcus et al., 1993) using standard POS tagging methods; (2) English \"Treebank Union\" multi-domain corpus, which includes data from OntoNotes Corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov and McDonald, 2012) and the updated and corrected Question Treebank model (Judge et al., 2006) with identical setup to White et al. (2015); and (3) CoNLL '09 Multilingual Shared Task (Hajic, et al., 2009).Model Configuration. Inspired by the Integrated Transition System (SHI), Parsing and Bohnet only works."}, {"heading": "4.2 Dependency Parsing", "text": "Depending on what we analyze, the goal is to create a directed tree that represents the syntactic structure of the input set. Data & Rating: We use the same corpora as in our POS tagging experiments, except that we use the standard parsing splits of the WSJ. We convert the English constituency trees into Stanford-style dependencies (De Marneffe et al., 2006) using version 3.3.0 of the converter. In English, we use the predicted POS tags (the same POS tags are used for all models) and exclude punctuation from the evaluation as standard. For the CoNLL '09 datasets, we follow standard practice and include all punctuations in the evaluation. We follow Alberti et al. (2015) and use our own predicted POS tags so that we can include a k-best tag function (see below), but use the provided morphological features."}, {"heading": "4.3 Sentence Compression", "text": "We follow Filippova et al. (2015), where a large collection of messages is used to generate heuristic compression cases. Our final corpus contains about 2.3 million compression cases: We use 2M examples for training, 130k for development, and 160k for the final test. We report per-token F1 score and per-sentence accuracy (A), i.e. percentage of instances that fully correspond to the golden compressions. Following Filippova et al. (2015), we also perform a human evaluation on 200 sentences, in which we ask the raters to use compressions for readability (read) and informativity (info) on a scale from 0 to 5.model configuration. The transition system for sentence compression is similar to POS tagging: we scan sentences from left to right and la-generated corpus human eval method."}, {"heading": "5 Discussion", "text": "We derived evidence of the label bias problem and the benefits of global models, and then emphatically confirmed this theoretical superiority by demonstrating the state of the art in three different tasks. Our experiments showed consistent improvements in the accuracy of globally normalized models compared to locally normalized models with beam search. In this section, we present our model and compare it with previous work and provide two examples of the label bias problem in practice."}, {"heading": "5.1 Related Neural CRF Work", "text": "Bottou et al. (1997) and Le Cun et al. (1998) describe the global tensile method UAS LASLocal (B = 1) 92.85 90.59 Local (B = 16) 93.32 91.09 Global (B = 16) {\u03b8 (d)} 93.45 91.21 Global (B = 16) {W2, \u03b8 (d)} 94.09 91.81 Global (B = 16) (full) 94.38 92.17Table 5: WSJ dev set scores for successively lower levels of backpropagation. The complete parameter set corresponds to the backpropagation up to the embedding. Wi: hidden layer i weighs insequence."}, {"heading": "5.2 Related Transition-Based Parsing Work", "text": "Our work is closest to the work of Wei\u00df et al. (2015), Zhou et al. (2015) and Watanabe and Sumita (2015); in these approaches, global normalization is added to the local model of Chen and Manning (2014). Empirically, Wei\u00df et al. (2015) achieves the best performance, although their model maintains the parameters of the locally normalized neural network and only forms a perceptron that uses the activations as characteristics. Therefore, their model is limited in its ability to revise the predictions of the locally normalized model. In Table 5, we show that complete backpropagation training down to the word embedding is very important and contributes significantly to the performance of our model. We also compared training under the CRF target with a perceptron-like loss of hinges between the gold and the best elements of the beam."}, {"heading": "5.3 Label Bias in Practice", "text": "Although the use of beam search with the local model exceeds greedy conclusions on average, beam search leads the local model to produce occasional empty compressions (Table 6). It is important to note that these are not search errors: however, empty compression is more likely under pL than predicting greedy conclusions. However, the more meaningful globally normalized model does not suffer from this limitation and correctly gives almost zero probability to empty compression. We also present some empirical evidence that the label bias problem is severe when analyzing. We trained models in which the scoring functions when analyzing position i in the set are limited to considering only tokens x1: i; therefore, unlike the full analysis model, there is no ability to look forward in the set when making a decision. 3 The result for a greedy model is a normal 76.96% UAS; for a locally normalized model with beam search is 835% globalized and a model."}, {"heading": "6 Conclusions", "text": "Our model combines the flexibility of transition-based algorithms with the modeling power of neural networks. Our results show that a relapse-free feed-forward network can outperform recurring models such as LSTMs when trained with global normalization. In addition, we reinforce our empirical findings by demonstrating that global normalization helps the model overcome the label distortion problem afflicting locally normalized models."}, {"heading": "Acknowledgements", "text": "We would like to thank Ling Wang for training his C2W Part-of-Speech Tagger in our setup and Emily Pitler, Ryan McDonald, Greg Coppola and Fernando Pereira for providing tremendously helpful discussions. Finally, we are grateful to all members of the Google Parsing Team."}], "references": [{"title": "Improved transition-based parsing and tagging with neural networks", "author": ["David Weiss", "Greg Coppola", "Slav Petrov"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Alberti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alberti et al\\.", "year": 2015}, {"title": "Improved transitionbased parsing by modeling characters instead of words with LSTMs", "author": ["Chris Dyer", "Noah A. Smith"], "venue": null, "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["Bohnet", "Nivre2012] Bernd Bohnet", "Joakim Nivre"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Bohnet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bohnet et al\\.", "year": 2012}, {"title": "Graph transformer networks for image recognition. Bulletin of the International Statistical Institute (ISI)", "author": ["Bottou", "LeCun2005] L\u00e9on Bottou", "Yann LeCun"], "venue": null, "citeRegEx": "Bottou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2005}, {"title": "Global training of document processing systems using graph transformer networks", "author": ["Bottou et al.1997] L\u00e9on Bottou", "Yann Le Cun", "Yoshua Bengio"], "venue": "In Proceedings of Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Bottou et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 1997}, {"title": "Une approche th\u00e9orique de lapprentissage connexionniste: Applications \u00e0 la reconnaissance de la parole", "author": ["L\u00e9on Bottou"], "venue": "Ph.D. thesis, Doctoral dissertation, Universite de Paris XI", "citeRegEx": "Bottou.,? \\Q1991\\E", "shortCiteRegEx": "Bottou.", "year": 1991}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Collins", "Roark2004] Michael Collins", "Brian Roark"], "venue": "In Proceedings of the 42nd Meeting of the Association for Computational Linguistics", "citeRegEx": "Collins et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2004}, {"title": "Search-based structured prediction. Machine Learning Journal (MLJ)", "author": ["John Langford", "Daniel Marcu"], "venue": null, "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Bill MacCartney", "Christopher D. Manning"], "venue": "In Proceedings of Fifth International Conference on Language Resources", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Neural conditional random fields", "author": ["Do", "Artires2010] Trinh Minh Tri Do", "Thierry Artires"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Do et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Do et al\\.", "year": 2010}, {"title": "Neural crf parsing", "author": ["Durrett", "Klein2015] Greg Durrett", "Dan Klein"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "Durrett et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Sentence compression by deletion with lstms", "author": ["Enrique Alfonseca", "Carlos A. Colmenares", "\u0141ukasz Kaiser", "Oriol Vinyals"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Training deterministic parsers with non-deterministic oracles. Transactions of the Association for Computational Linguistics, 1:403\u2013414", "author": ["Goldberg", "Nivre2013] Yoav Goldberg", "Joakim Nivre"], "venue": null, "citeRegEx": "Goldberg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2013}, {"title": "The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages", "author": ["Yi Zhang"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,", "citeRegEx": "Zhang.,? \\Q2009\\E", "shortCiteRegEx": "Zhang.", "year": 2009}, {"title": "Inducing history representations for broad coverage statistical parsing", "author": ["James Henderson"], "venue": "In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association", "citeRegEx": "Henderson.,? \\Q2003\\E", "shortCiteRegEx": "Henderson.", "year": 2003}, {"title": "Discriminative training of a neural network statistical parser", "author": ["James Henderson"], "venue": "In Proceedings of the 42nd Meeting of the Association for Computational Linguistics", "citeRegEx": "Henderson.,? \\Q2004\\E", "shortCiteRegEx": "Henderson.", "year": 2004}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Ontonotes: The 90% solution", "author": ["Hovy et al.2006] Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel"], "venue": "In Proceedings of the Human Language Technology Conference of the NAACL,", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Bidirectional LSTM-CRF models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Questionbank: Creating a corpus of parse-annotated questions", "author": ["Judge et al.2006] John Judge", "Aoife Cahill", "Josef van Genabith"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Associa-", "citeRegEx": "Judge et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Judge et al\\.", "year": 2006}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": "In Proceedings of the Eighteenth International Conference on Machine Learn-", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Gradient based learning applied to document recognition", "author": ["Le Cun et al.1998] Yann Le Cun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of IEEE,", "citeRegEx": "Cun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Cun et al\\.", "year": 1998}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Lei et al.2014] Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Lei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Structure compilation: Trading structure for features", "author": ["Liang et al.2008] Percy Liang", "III Hal Daum\u00e9", "Dan Klein"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Liang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2008}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Turning on the turbo: Fast third-order non-projective turbo parsers", "author": ["Miguel Almeida", "Noah A. Smith"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Martins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Inductive Dependency Parsing", "author": ["Joakim Nivre"], "venue": null, "citeRegEx": "Nivre.,? \\Q2006\\E", "shortCiteRegEx": "Nivre.", "year": 2006}, {"title": "Non-projective dependency parsing in expected linear time", "author": ["Joakim Nivre"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing", "citeRegEx": "Nivre.,? \\Q2009\\E", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "Conditional neural fields", "author": ["Peng et al.2009] Jian Peng", "Liefeng Bo", "Jinbo Xu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Peng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2009}, {"title": "Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL)", "author": ["Petrov", "McDonald2012] Slav Petrov", "Ryan McDonald"], "venue": null, "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "No-regret reductions for imitation learning and structured prediction. AISTATS", "author": ["Ross et al.2011] St\u00e9phane Ross", "Geoffrey J. Gordon", "J. Andrew Bagnell"], "venue": null, "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Efficient structured inference for transition-based parsing with neural networks and error states", "author": ["Vaswani", "Sagae2016] Ashish Vaswani", "Kenji Sagae"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Vaswani et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "Grammar as a foreign language", "author": ["\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Transition-based neural constituent parsing", "author": ["Watanabe", "Sumita2015] Taro Watanabe", "Eiichiro Sumita"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Watanabe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watanabe et al\\.", "year": 2015}, {"title": "Structured training for neural network transition-based parsing", "author": ["Weiss et al.2015] David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Recurrent conditional random field for language understanding", "author": ["Yao et al.2014] Kaisheng Yao", "Baolin Peng", "Geoffrey Zweig", "Dong Yu", "Xiaolong Li", "Feng Gao"], "venue": "In IEEE International Conference on Acoustics,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Incremental recurrent neural network dependency parser with searchbased discriminative training", "author": ["Yazdani", "Henderson2015] Majid Yazdani", "James Henderson"], "venue": "In Proceedings of the Nineteenth Conference on Computational Natural", "citeRegEx": "Yazdani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yazdani et al\\.", "year": 2015}, {"title": "Enforcing structural diversity in cube-pruned dependency parsing", "author": ["Zhang", "McDonald2014] Hao Zhang", "Ryan McDonald"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Conditional random fields", "author": ["Zheng et al.2015] Shuai Zheng", "Sadeep Jayasumana", "Bernardino Romera-Paredes", "Vibhav Vineet", "Zhizhong Su", "Dalong Du", "Chang Huang", "Philip H.S. Torr"], "venue": null, "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "Endto-end learning of semantic role labeling using recurrent neural networks", "author": ["Zhou", "Xu2015] Jie Zhou", "Wei Xu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "A neural probabilistic structuredprediction model for transition-based dependency parsing", "author": ["Zhou et al.2015] Hao Zhou", "Yue Zhang", "Jiajun Chen"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "In particular, variants of long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) have produced impressive results on some of the classic NLP tasks such as part-of-speech tagging (Ling et al., 2015), syntactic parsing (Vinyals et al.", "startOffset": 198, "endOffset": 217}, {"referenceID": 35, "context": ", 2015), syntactic parsing (Vinyals et al., 2015) and semantic role labeling (Zhou and Xu, 2015).", "startOffset": 27, "endOffset": 49}, {"referenceID": 29, "context": "Our model, described in detail in Section 2, uses a transition system (Nivre, 2006) and feature embeddings as introduced by Chen and Manning (2014).", "startOffset": 70, "endOffset": 83}, {"referenceID": 4, "context": "We do not use any recurrence, but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field (CRF) objective (Bottou et al., 1997; Le Cun et al., 1998; Lafferty et al., 2001) to overcome the label bias problem that locally normalized models suffer from.", "startOffset": 173, "endOffset": 238}, {"referenceID": 22, "context": "We do not use any recurrence, but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field (CRF) objective (Bottou et al., 1997; Le Cun et al., 1998; Lafferty et al., 2001) to overcome the label bias problem that locally normalized models suffer from.", "startOffset": 173, "endOffset": 238}, {"referenceID": 42, "context": "Since we use beam inference, we approximate the partition function by summing over the elements in the beam, and use early updates (Collins and Roark, 2004; Zhou et al., 2015).", "startOffset": 131, "endOffset": 175}, {"referenceID": 24, "context": "Our model, described in detail in Section 2, uses a transition system (Nivre, 2006) and feature embeddings as introduced by Chen and Manning (2014). We do not use any recurrence, but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field (CRF) objective (Bottou et al.", "startOffset": 71, "endOffset": 148}, {"referenceID": 36, "context": "Our ablation experiments show that we outperform Weiss et al. (2015) and Alberti et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 0, "context": "(2015) and Alberti et al. (2015) because we do global backpropagation training of all model parameters, while they fix the neural network parameters when training the global part of their model.", "startOffset": 11, "endOffset": 33}, {"referenceID": 0, "context": "(2015) and Alberti et al. (2015) because we do global backpropagation training of all model parameters, while they fix the neural network parameters when training the global part of their model. We also outperform Zhou et al. (2015) despite using a smaller beam.", "startOffset": 11, "endOffset": 233}, {"referenceID": 29, "context": "At its core, our model is an incremental transitionbased parser (Nivre, 2006).", "startOffset": 64, "endOffset": 77}, {"referenceID": 29, "context": "In dependency parsing for example, this is true for both the arc-standard and arc-eager transition systems (Nivre, 2006), where for a sentence x of length m, the number of decisions for any complete parse is n(x) = 2 \u00d7 m.", "startOffset": 107, "endOffset": 120}, {"referenceID": 29, "context": "Note that this is not true for the swap transition system defined in Nivre (2009). It is straightforward to extend the approach to make use of dynamic programming in the case where the same state can be reached by multiple decision sequences.", "startOffset": 69, "endOffset": 82}, {"referenceID": 29, "context": "Note that this is not true for the swap transition system defined in Nivre (2009). It is straightforward to extend the approach to make use of dynamic programming in the case where the same state can be reached by multiple decision sequences. define \u03c1(d1:j , d; \u03b8) to be equal to \u03c1(s, d; \u03b8) where s is the state reached by decisions d1:j . The scoring function \u03c1(s, d; \u03b8) can be defined in a number of ways. In this work, following Chen and Manning (2014), Weiss et al.", "startOffset": 69, "endOffset": 456}, {"referenceID": 29, "context": "Note that this is not true for the swap transition system defined in Nivre (2009). It is straightforward to extend the approach to make use of dynamic programming in the case where the same state can be reached by multiple decision sequences. define \u03c1(d1:j , d; \u03b8) to be equal to \u03c1(s, d; \u03b8) where s is the state reached by decisions d1:j . The scoring function \u03c1(s, d; \u03b8) can be defined in a number of ways. In this work, following Chen and Manning (2014), Weiss et al. (2015), and Zhou et al.", "startOffset": 69, "endOffset": 477}, {"referenceID": 29, "context": "Note that this is not true for the swap transition system defined in Nivre (2009). It is straightforward to extend the approach to make use of dynamic programming in the case where the same state can be reached by multiple decision sequences. define \u03c1(d1:j , d; \u03b8) to be equal to \u03c1(s, d; \u03b8) where s is the state reached by decisions d1:j . The scoring function \u03c1(s, d; \u03b8) can be defined in a number of ways. In this work, following Chen and Manning (2014), Weiss et al. (2015), and Zhou et al. (2015), we define it via a feedforward neural network as", "startOffset": 69, "endOffset": 501}, {"referenceID": 42, "context": "To make learning tractable with the globally normalized model, we use beam search and early updates (Collins and Roark, 2004; Zhou et al., 2015).", "startOffset": 100, "endOffset": 144}, {"referenceID": 21, "context": "However the label bias problem (see Lafferty et al. (2001), Bottou (1991), Bottou and LeCun (2005)) means that locally normalized models often have a very weak ability to revise earlier decisions.", "startOffset": 36, "endOffset": 59}, {"referenceID": 5, "context": "(2001), Bottou (1991), Bottou and LeCun (2005)) means that locally normalized models often have a very weak ability to revise earlier decisions.", "startOffset": 8, "endOffset": 22}, {"referenceID": 5, "context": "(2001), Bottou (1991), Bottou and LeCun (2005)) means that locally normalized models often have a very weak ability to revise earlier decisions.", "startOffset": 8, "endOffset": 47}, {"referenceID": 25, "context": "For a detailed analysis of the tradeoffs between structural features in CRFs and more powerful local classifiers without structural constraints, see Liang et al. (2008); in these experiments local classifiers are unable to reach the performance of CRFs on problems such as parsing and named entity recognition where structural constraints are important.", "startOffset": 149, "endOffset": 169}, {"referenceID": 26, "context": "17 Ling et al. (2015) 97.", "startOffset": 3, "endOffset": 22}, {"referenceID": 27, "context": "We conducted experiments on a number of different datasets: (1) English Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) with standard POS tagging splits; (2) English \u201cTreebank Union\u201d multi-domain corpus containing data from the OntoNotes corpus version 5 (Hovy et al.", "startOffset": 124, "endOffset": 145}, {"referenceID": 19, "context": ", 1993) with standard POS tagging splits; (2) English \u201cTreebank Union\u201d multi-domain corpus containing data from the OntoNotes corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov and McDonald, 2012), and the updated and corrected Question Treebank (Judge et al.", "startOffset": 143, "endOffset": 162}, {"referenceID": 21, "context": ", 2006), the English Web Treebank (Petrov and McDonald, 2012), and the updated and corrected Question Treebank (Judge et al., 2006) with identical setup to Weiss et al.", "startOffset": 111, "endOffset": 131}, {"referenceID": 19, "context": ", 1993) with standard POS tagging splits; (2) English \u201cTreebank Union\u201d multi-domain corpus containing data from the OntoNotes corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov and McDonald, 2012), and the updated and corrected Question Treebank (Judge et al., 2006) with identical setup to Weiss et al. (2015); and (3) CoNLL \u201909 multilingual shared task (Haji\u010d et al.", "startOffset": 144, "endOffset": 331}, {"referenceID": 29, "context": "Inspired by the integrated POS tagging and parsing transition system of Bohnet and Nivre (2012), we employ a simple transition system that uses only a SHIFT action and predicts the POS tag of the current word on the buffer as it gets shifted to the stack.", "startOffset": 83, "endOffset": 96}, {"referenceID": 26, "context": "In Table 1 we compare our model to a linear CRF and to the compositional characterto-word LSTM model of Ling et al. (2015). The CRF is a first-order linear model with exact inference and the same emission features as our model.", "startOffset": 104, "endOffset": 123}, {"referenceID": 26, "context": "In Table 1 we compare our model to a linear CRF and to the compositional characterto-word LSTM model of Ling et al. (2015). The CRF is a first-order linear model with exact inference and the same emission features as our model. It additionally also has transition features of the word, cluster and character n-gram up to length 3 on both endpoints of the transition. The results for Ling et al. (2015) were solicited from the authors.", "startOffset": 104, "endOffset": 402}, {"referenceID": 0, "context": "06 Alberti et al. (2015) 94.", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "60 Alberti et al. (2015) 92.", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "We follow Alberti et al. (2015) and use our own predicted POS tags so that we can include a k-best tag feature (see below) but use the supplied predicted morphological features.", "startOffset": 10, "endOffset": 32}, {"referenceID": 36, "context": "Our model configuration is basically the same as the one originally proposed by Chen and Manning (2014) and then refined by Weiss et al. (2015). In particular, we use the arc-standard transition system and extract the same set of features as prior work: words, part of speech tags, and dependency arcs and labels in the surrounding context of the state, as well as k-best tags as proposed by Alberti et al.", "startOffset": 124, "endOffset": 144}, {"referenceID": 0, "context": "In particular, we use the arc-standard transition system and extract the same set of features as prior work: words, part of speech tags, and dependency arcs and labels in the surrounding context of the state, as well as k-best tags as proposed by Alberti et al. (2015). We use two hidden layers of 1,024 dimensions each.", "startOffset": 247, "endOffset": 269}, {"referenceID": 34, "context": "41 reported by Weiss et al. (2015) with tri-training.", "startOffset": 15, "endOffset": 35}, {"referenceID": 34, "context": "41 reported by Weiss et al. (2015) with tri-training. As we show in Section 5, these gains can be attributed to the full backpropagation training that differentiates our approach from that of Weiss et al. (2015) and Alberti et al.", "startOffset": 15, "endOffset": 212}, {"referenceID": 0, "context": "(2015) and Alberti et al. (2015). Our results also significantly outperform the LSTM-based approaches of Dyer et al.", "startOffset": 11, "endOffset": 33}, {"referenceID": 0, "context": "(2015) and Alberti et al. (2015). Our results also significantly outperform the LSTM-based approaches of Dyer et al. (2015) and Ballesteros et al.", "startOffset": 11, "endOffset": 124}, {"referenceID": 0, "context": "(2015) and Alberti et al. (2015). Our results also significantly outperform the LSTM-based approaches of Dyer et al. (2015) and Ballesteros et al. (2015).", "startOffset": 11, "endOffset": 154}, {"referenceID": 13, "context": "We follow Filippova et al. (2015), where a large news collection is used to heuristically generate compression instances.", "startOffset": 10, "endOffset": 34}, {"referenceID": 13, "context": "We follow Filippova et al. (2015), where a large news collection is used to heuristically generate compression instances. Our final corpus contains about 2.3M compression instances: we use 2M examples for training, 130k for development and 160k for the final test. We report per-token F1 score and per-sentence accuracy (A), i.e. percentage of instances that fully match the golden compressions. Following Filippova et al. (2015) we also run a human evaluation on 200 sentences where we ask the raters to score compressions for readability (read) and informativeness (info) on a scale from 0 to 5.", "startOffset": 10, "endOffset": 430}, {"referenceID": 13, "context": "We also compare to the best sentence compression system from Filippova et al. (2015), a 3-layer stacked LSTM which uses dependency label information.", "startOffset": 61, "endOffset": 85}, {"referenceID": 3, "context": "Bottou et al. (1997) and Le Cun et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bottou et al. (1997) and Le Cun et al. (1998) describe global trainMethod UAS LAS", "startOffset": 0, "endOffset": 46}, {"referenceID": 30, "context": "Peng et al. (2009) add a non-linear neural network layer to a linearchain CRF and Do and Artires (2010) apply a similar approach to more general Markov network structures.", "startOffset": 0, "endOffset": 19}, {"referenceID": 30, "context": "Peng et al. (2009) add a non-linear neural network layer to a linearchain CRF and Do and Artires (2010) apply a similar approach to more general Markov network structures.", "startOffset": 0, "endOffset": 104}, {"referenceID": 30, "context": "Peng et al. (2009) add a non-linear neural network layer to a linearchain CRF and Do and Artires (2010) apply a similar approach to more general Markov network structures. Yao et al. (2014) and Zheng et al.", "startOffset": 0, "endOffset": 190}, {"referenceID": 30, "context": "Peng et al. (2009) add a non-linear neural network layer to a linearchain CRF and Do and Artires (2010) apply a similar approach to more general Markov network structures. Yao et al. (2014) and Zheng et al. (2015) introduce recurrence into the model and Huang et al.", "startOffset": 0, "endOffset": 214}, {"referenceID": 20, "context": "(2015) introduce recurrence into the model and Huang et al. (2015) finally combine CRFs and LSTMs.", "startOffset": 47, "endOffset": 67}, {"referenceID": 16, "context": "For early work on neural-networks for transition-based parsing, see Henderson (2003; 2004). Our work is closest to the work of Weiss et al. (2015), Zhou et al.", "startOffset": 68, "endOffset": 147}, {"referenceID": 16, "context": "For early work on neural-networks for transition-based parsing, see Henderson (2003; 2004). Our work is closest to the work of Weiss et al. (2015), Zhou et al. (2015) and Watanabe and Sumita (2015); in these approaches global normalization is added to the local model of Chen and Manning (2014).", "startOffset": 68, "endOffset": 167}, {"referenceID": 16, "context": "For early work on neural-networks for transition-based parsing, see Henderson (2003; 2004). Our work is closest to the work of Weiss et al. (2015), Zhou et al. (2015) and Watanabe and Sumita (2015); in these approaches global normalization is added to the local model of Chen and Manning (2014).", "startOffset": 68, "endOffset": 198}, {"referenceID": 16, "context": "For early work on neural-networks for transition-based parsing, see Henderson (2003; 2004). Our work is closest to the work of Weiss et al. (2015), Zhou et al. (2015) and Watanabe and Sumita (2015); in these approaches global normalization is added to the local model of Chen and Manning (2014). Empirically, Weiss et al.", "startOffset": 68, "endOffset": 295}, {"referenceID": 16, "context": "For early work on neural-networks for transition-based parsing, see Henderson (2003; 2004). Our work is closest to the work of Weiss et al. (2015), Zhou et al. (2015) and Watanabe and Sumita (2015); in these approaches global normalization is added to the local model of Chen and Manning (2014). Empirically, Weiss et al. (2015) achieves the best performance, even though their model keeps the parameters of the locally normalized neural network fixed and only trains a perceptron that uses the activations as features.", "startOffset": 68, "endOffset": 329}, {"referenceID": 33, "context": "Goldberg and Nivre (2013) describe improvements to a greedy parsing approach that makes use of methods from imitation learning (Ross et al., 2011) to augment the training set.", "startOffset": 127, "endOffset": 146}, {"referenceID": 8, "context": "Daum\u00e9 III et al. (2009) introduce Searn, an algorithm that allows a classifier making greedy decisions to become more robust to errors made in previous decisions.", "startOffset": 6, "endOffset": 24}, {"referenceID": 8, "context": "Daum\u00e9 III et al. (2009) introduce Searn, an algorithm that allows a classifier making greedy decisions to become more robust to errors made in previous decisions. Goldberg and Nivre (2013) describe improvements to a greedy parsing approach that makes use of methods from imitation learning (Ross et al.", "startOffset": 6, "endOffset": 189}], "year": 2016, "abstractText": "We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-ofspeech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.", "creator": "LaTeX with hyperref package"}}}