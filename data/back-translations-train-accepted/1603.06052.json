{"id": "1603.06052", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Fast DPP Sampling for Nystr\\\"om with Application to Kernel Methods", "abstract": "The Nystr\\\"om method has long been popular for scaling up kernel methods. However, successful use of Nystr\\\"om depends crucially on the selected landmarks. We consider landmark selection by using a Determinantal Point Process (DPP) to tractably select a diverse subset from the columns of an input kernel matrix. We prove that the landmarks selected using DPP sampling enjoy guaranteed error bounds; subsequently, we illustrate impact of DPP-sampled landmarks on kernel ridge regression. Moreover, we show how to efficiently sample from a DPP in linear time using a fast mixing (under certain constraints) Markov chain, which makes the overall procedure practical. Empirical results support our theoretical analysis: DPP-based landmark selection shows performance superior to existing approaches.", "histories": [["v1", "Sat, 19 Mar 2016 06:14:28 GMT  (914kb,D)", "https://arxiv.org/abs/1603.06052v1", null], ["v2", "Sat, 28 May 2016 04:04:55 GMT  (723kb,D)", "http://arxiv.org/abs/1603.06052v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chengtao li", "stefanie jegelka", "suvrit sra"], "accepted": true, "id": "1603.06052"}, "pdf": {"name": "1603.06052.pdf", "metadata": {"source": "CRF", "title": "Fast Dpp Sampling for Nystro\u0308m with Application to Kernel Methods", "authors": ["Chengtao Li", "Stefanie Jegelka", "Suvrit Sra"], "emails": ["ctli@mit.edu", "stefje@csail.mit.edu", "suvrit@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "In such cases, a low number of matrix approximations leads to a tolerable loss of knowledge in terms of selection methods to a tolerable loss of accuracy. A noteworthy instance is the Nystro-M method [32, 43], which takes a positive semidefinitive matrix K-N as a starting point, selects a small subset of C-3 from it, and constructs the approximation K-3, CKC. Matrix K-3 is then used instead of K, which can reduce runtimes from O (N3) to O (N | C | 3), a huge saving (since the introduction to machine learning)."}, {"heading": "2 Background and Notation", "text": "We approach a given positive semidifinite (PSD) matrix K-RN-N with self-decomposition K-U and eigenvalues \u03bb1 \u2265. We use Ki, \u00b7 for the i-th row and K \u00b7, j for the j-th column and, likewise, KC, \u00b7 for the rows of K and K \u00b7, C for the columns of K. Finally, KC, C is the submatrix of K with rows and columns indexed by C. In this notation, Kk = U \u00b7, [k], [k] U > \u00b7, [k] the best rank-k approximation to K in both Frobenius and spectral norms. We write r (\u00b7) for the rank and (\u00b7) \u2020 for the pseudoinverse, and denote a decomposition of K > B where B-Rr indicates."}, {"heading": "3 Dpp for the Nystro\u0308m Method", "text": "Next, we consider sampling the c-Dpp (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K (K) (K) (K) (K) (K (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K"}, {"heading": "4 Low-rank Kernel Ridge Regression", "text": "Our theoretical (Section 3) and empirical (Section 6.1) results indicate that Dpp-Nystro-m is well suited for scaling core methods (Section 6.1). \u2212 K (Section 6.1) The experiments in Section 6 confirm our results empirically. \u2212 K (Section 6.2) We have N samples {(xi, yi) Ni = 1, where yi = zi + ei are the observed effects on the zero increases with finite covariance. \u2212 K (Section 6.2) We minimize a regularized empirical loss min f-F 1 NN-F 1 NN-2 (yi, f (xi)) -K-2 via an RKHS F. Equally, we solve the problem in RN 1 NN-I = 1 '(yi, (K\u03b1) i) + 2 \u00b2 -K-K (K)."}, {"heading": "5 Fast Mixing Markov Chain Dpp", "text": "Despite excellent empirical performance and strong theoretical results, we have undertaken a general, rapid analysis of our observations, which is rarely used in applications because the calculation costs of O (N3) are so high that they proceed directly from a Dpp containing an intrinsic decomposition. Instead, we are following a different path: an MCMC sampler that offers a promising alternative if the chain mixes fast enough. Recent empirical results provide initial evidence [24], but without a theoretical analysis [21, 34] do not apply to our cardinality-limited setting. We offer a theoretical analysis that confirms rapid mixing (i.e., polynomial or even linear sampling) under certain conditions and combine it with our empirical results. The empirical results in Section 6 illustrate the favorable performance of Dpp-Nystro in the trade of time and error."}, {"heading": "6 Experiments", "text": "In our experiments, we evaluate the performance of Dpp-Nystro \ufffd m on both kernel selection and kernel learning tasks in terms of runtime and accuracy. We use 8 datasets: Abalone, Ailerons, Elevators, CompAct, CompAct (s), Bank32NH, Bank8FM and California Housing4. We examined 4,000 points from each dataset (3,000 training and 1,000 tests). During our experiments, we use an RBF kernel and select bandwidth and regulation parameters for each dataset by 10-fold cross-validation. We initialize the Gibbs samplers via Kmeans + + and run for 3,000 iterations. Results will be over 3 random subsets of data.6.1 Kernel We first explore Dpp-Nystro \ufffd m (kDPP in numbers) for approximating kernel matrices."}, {"heading": "7 Conclusion", "text": "In this paper, we have re-examined the use of k-determinantal point processes for the sample of good milestones of the Nystro \ufffd m Method. Theoretically and empirically, we observe their competitiveness, both for the approximation of the matrix and for the burr regression compared to the state of the art. To make this precise method scalable to large matrices, we consider an iterative approach and analyze it both theoretically and empirically. Our results indicate that the iterative approach, a Gibbs sampler, quickly achieves good limits; under certain conditions, even in a number of iteratons linearly in N, for a N-by-N matrix. Finally, our empirical results show that the iterative sampler achieves the best compromise between efficiency and accuracy among the most modern methods."}, {"heading": "Acknowledgements", "text": "This research was partially supported by a CAREER prize from NSF 1553284, an NSF grant IIS-1409802 and a Google Research Award. We also thank Xixian Chen for the discussions."}, {"heading": "A Bounds that hold with High Probability", "text": "To show high probability limits, we use concentration results on homogeneous strong Rayleigh measurements. Specifically, we use the following theory: Theorem 6 (Pemantle and Peres [33]). Let P make a k-homogeneous strong Rayleigh probability measurement on {0, 1} N and f an '-Lipschitz function on {0, 1} N, then P (f \u2212 E [f] a') \u2264 exp {\u2212 a2 / 8k}. It is known that a k-Dpp applies a homogeneous strong Rayleigh measurement on {0, 1} N [4 \u2212 10], i.e. Theorem 6 on results obtained with k-Dpp. Specifically, it applies to Theorem 1, which is held in expectation that we have the following limit, which holds with a high probability: c."}, {"heading": "B Supplementary Experiments", "text": "B.1 Kernel ApproximationFig. 7 shows the matrix norm of relative errors of various methods of kernel approximation on the remaining 7 datasets mentioned in the main text. B.2 Approximated Kernel Ridge RegressionFig. 8 shows the training and testing errors of various methods of kernel ridge regression on the remaining 7 datasets. B.3 The mixing of the Markov chain k-DppWe first shows the mixing of the Gibbs Dpp-Nystro \ufffd m landmarks with 50 landmarks of different performance scales: relative spectral standard errors, training errors and test errors of the kernel ridge regression in Fig. 9.We also show corresponding results for 100 and 200 landmarks in Fig. 10 and Fig. 11 to demonstrate that the chain for varying number of landmarks is actually a fast mixing and will yield relatively good results within a small number of iteration data."}], "references": [{"title": "Nystr\u00f6m approximation for large-scale determinantal processes", "author": ["R.H. Affandi", "A. Kulesza", "E. Fox", "B. Taskar"], "venue": "AISTATS, pages 85\u201398,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast randomized kernel methods with statistical guarantees", "author": ["A.E. Alaoui", "M.W. Mahoney"], "venue": "NIPS,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Some inequalities for reversible Markov chains", "author": ["D.J. Aldous"], "venue": "Journal of the London Mathematical Society, pages 564\u2013576,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1982}, {"title": "Monte Carlo Markov chain algorithms for sampling strongly Rayleigh distributions and determinantal point processes", "author": ["N. Anari", "S.O. Gharan", "A. Rezaei"], "venue": "COLT,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Sharp analysis of low-rank kernel matrix approximations", "author": ["F.R. Bach"], "venue": "COLT,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "JMLR, pages 1\u201348,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Predictive low-rank decomposition for kernel methods", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "ICML, pages 33\u201340,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "On landmark selection and sampling in high-dimensional data analysis", "author": ["M.-A. Belabbas", "P.J. Wolfe"], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, pages 4295\u20134312,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Spectral methods in machine learning and new strategies for very large datasets", "author": ["M.-A. Belabbas", "P.J. Wolfe"], "venue": "Proceedings of the National Academy of Sciences, pages 369\u2013374,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Negative dependence and the geometry of polynomials", "author": ["J. Borcea", "P. Br\u00e4nd\u00e9n", "T. Liggett"], "venue": "Journal of the American Mathematical Society, pages 521\u2013567,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Path coupling: A technique for proving rapid mixing in Markov chains", "author": ["R. Bubley", "M. Dyer"], "venue": "FOCS, pages 223\u2013231,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "On the impact of kernel approximation on learning accuracy", "author": ["C. Cortes", "M. Mohri", "A. Talwalkar"], "venue": "AISTATS, pages 113\u2013120,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["A. Deshpande", "L. Rademacher", "S. Vempala", "G. Wang"], "venue": "SODA, pages 1117\u20131126,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "JMLR, pages 2153\u20132175,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix", "author": ["P. Drineas", "R. Kannan", "M.W. Mahoney"], "venue": "SIAM Journal on Computing, pages 158\u2013183,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "A more rapidly mixing Markov chain for graph colorings", "author": ["M. Dyer", "C. Greenhill"], "venue": "Random Structures and Algorithms, pages 285\u2013317,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient SVM training using low-rank kernel representations", "author": ["S. Fine", "K. Scheinberg"], "venue": "JMLR, pages 243\u2013264,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "TPAMI, pages 214\u2013225,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Revisiting the Nystr\u00f6m method for improved large-scale machine learning", "author": ["A. Gittens", "M.W. Mahoney"], "venue": "ICML,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "JHU Press,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Sampling from probabilistic submodular models", "author": ["A. Gotovos", "H. Hassani", "A. Krause"], "venue": "NIPS, pages 1936\u20131944,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal column-based low-rank matrix reconstruction", "author": ["V. Guruswami", "A.K. Sinop"], "venue": "SODA, pages 1207\u20131214,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Determinantal processes and independence", "author": ["J.B. Hough", "M. Krishnapur", "Y. Peres", "B. Vir\u00e1g"], "venue": "Probability Surveys, pages 206\u2013229,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast determinantal point process sampling with application to clustering", "author": ["B. Kang"], "venue": "NIPS, pages 2319\u20132327,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "k-DPPs: Fixed-size determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "ICML, pages 1193\u20131200,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Determinantal point processes for machine learning", "author": ["A. Kulesza", "B. Taskar"], "venue": "arXiv preprint arXiv:1207.6083,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Ensemble Nystr\u00f6m method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "NIPS, pages 1060\u20131068,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Sampling methods for the nystr\u00f6m method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "The Journal of Machine Learning Research, pages 981\u20131006,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient sampling for k-determinantal point processes", "author": ["C. Li", "S. Jegelka", "S. Sra"], "venue": "AISTATS,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Gaussian quadrature for matrix inverse forms with applications", "author": ["C. Li", "S. Sra", "S. Jegelka"], "venue": "ICML,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "The coincidence approach to stochastic point processes", "author": ["O. Macchi"], "venue": "Advances in Applied Probability, pages 83\u2013122,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1975}, {"title": "\u00dcber die praktische Aufl\u00f6sung von Integralgleichungen mit Anwendungen auf Randwertaufgaben", "author": ["E.J. Nystr\u00f6m"], "venue": "Acta Mathematica, pages 185\u2013204,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1930}, {"title": "Concentration of Lipschitz functionals of determinantal and other strong Rayleigh measures", "author": ["R. Pemantle", "Y. Peres"], "venue": "Combinatorics, Probability and Computing, pages 140\u2013160,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast mixing for discrete point processes", "author": ["P. Rebeschini", "A. Karbasi"], "venue": "COLT,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Less is more: Nystr\u00f6m computational regularization", "author": ["A. Rudi", "R. Camoriano", "L. Rosasco"], "venue": "NIPS,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast kernel-based independent component analysis", "author": ["H. Shen", "S. Jegelka", "A. Gretton"], "venue": "IEEE Transactions on Signal Processing, pages 3498\u20133511,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse greedy matrix approximation for machine learning", "author": ["A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "ICML,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "A review of Nystr\u00f6m methods for large-scale machine learning", "author": ["S. Sun", "J. Zhao", "J. Zhu"], "venue": "Information Fusion, pages 36\u201348,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale manifold learning", "author": ["A. Talwalkar", "S. Kumar", "H. Rowley"], "venue": "CVPR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Large-scale SVD and manifold learning", "author": ["A. Talwalkar", "S. Kumar", "M. Mohri", "H. Rowley"], "venue": "JMLR, pages 3129\u20133152,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Symmetry and approximability of submodular maximization problems", "author": ["J. Vondr\u00e1k"], "venue": "SIAM Journal on Computing, 42(1):265\u2013304,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Using the matrix ridge approximation to speedup determinantal point processes sampling algorithms", "author": ["S. Wang", "C. Zhang", "H. Qian", "Z. Zhang"], "venue": "AAAI,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "NIPS, pages 682\u2013688,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2001}, {"title": "Improved Nystr\u00f6m low-rank approximation and error analysis", "author": ["K. Zhang", "I.W. Tsang", "J.T. Kwok"], "venue": "ICML, pages 1232\u20131239,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 31, "context": "A notable instance is the Nystr\u00f6m method [32, 43], which takes a positive semidefinite matrix K \u2208 RN\u00d7N as input, selects from it a small subset C of columns K\u00b7,C, and constructs the approximation K\u0303 = K\u00b7,CK C,CKC,\u00b7.", "startOffset": 41, "endOffset": 49}, {"referenceID": 42, "context": "A notable instance is the Nystr\u00f6m method [32, 43], which takes a positive semidefinite matrix K \u2208 RN\u00d7N as input, selects from it a small subset C of columns K\u00b7,C, and constructs the approximation K\u0303 = K\u00b7,CK C,CKC,\u00b7.", "startOffset": 41, "endOffset": 49}, {"referenceID": 5, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 135, "endOffset": 142}, {"referenceID": 35, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 135, "endOffset": 142}, {"referenceID": 7, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 191, "endOffset": 198}, {"referenceID": 17, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 191, "endOffset": 198}, {"referenceID": 38, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 218, "endOffset": 226}, {"referenceID": 39, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 218, "endOffset": 226}, {"referenceID": 34, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 243, "endOffset": 247}, {"referenceID": 0, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 284, "endOffset": 287}, {"referenceID": 1, "context": "Recent work [2, 5, 12] shows risk bounds for Nystr\u00f6m applied to various kernel methods.", "startOffset": 12, "endOffset": 22}, {"referenceID": 4, "context": "Recent work [2, 5, 12] shows risk bounds for Nystr\u00f6m applied to various kernel methods.", "startOffset": 12, "endOffset": 22}, {"referenceID": 11, "context": "Recent work [2, 5, 12] shows risk bounds for Nystr\u00f6m applied to various kernel methods.", "startOffset": 12, "endOffset": 22}, {"referenceID": 11, "context": "This choice governs the approximation error and subsequent performance of the approximated learning methods [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 42, "context": "The most basic strategy is to sample landmarks uniformly at random [43].", "startOffset": 67, "endOffset": 71}, {"referenceID": 36, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 129, "endOffset": 136}, {"referenceID": 16, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 129, "endOffset": 136}, {"referenceID": 13, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 198, "endOffset": 202}, {"referenceID": 14, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 222, "endOffset": 226}, {"referenceID": 18, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 262, "endOffset": 266}, {"referenceID": 43, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 280, "endOffset": 284}, {"referenceID": 8, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 318, "endOffset": 321}, {"referenceID": 25, "context": "We study landmark selection using Determinantal Point Processes (Dpp), discrete probability models that allow tractable sampling of diverse non-independent subsets [26, 31].", "startOffset": 164, "endOffset": 172}, {"referenceID": 30, "context": "We study landmark selection using Determinantal Point Processes (Dpp), discrete probability models that allow tractable sampling of diverse non-independent subsets [26, 31].", "startOffset": 164, "endOffset": 172}, {"referenceID": 8, "context": "Our work generalizes the determinant based scheme of Belabbas and Wolfe [9].", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "Previous analyses [9] fix a cardinality c = k; we allow the general case of selecting c \u2265 k columns.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "This task has been noted as the main application in [2, 5].", "startOffset": 52, "endOffset": 58}, {"referenceID": 4, "context": "This task has been noted as the main application in [2, 5].", "startOffset": 52, "endOffset": 58}, {"referenceID": 10, "context": "We consider a Gibbs sampler for k-Dpp, and analyze its mixing time using a path coupling [11] argument.", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 8, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 11, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 13, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 14, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 16, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 18, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 36, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 43, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 26, "context": "The ensemble Nystr\u00f6m method [27], for instance, uses a weighted combination of approximations.", "startOffset": 28, "endOffset": 32}, {"referenceID": 37, "context": "The modified Nystr\u00f6m method constructs an approximation K\u00b7,CK \u00b7,CKK \u2020 C,\u00b7KC,\u00b7 [38].", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "1) When conditioning on a fixed cardinality, one obtains a k-Dpp [25].", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": "Sampling from a (c-)Dpp can be done in polynomial time, but requires a full eigendecomposition of K [23], which is prohibitive for large N.", "startOffset": 100, "endOffset": 104}, {"referenceID": 0, "context": "A number of approaches have been proposed for more efficient sampling [1, 29, 42].", "startOffset": 70, "endOffset": 81}, {"referenceID": 28, "context": "A number of approaches have been proposed for more efficient sampling [1, 29, 42].", "startOffset": 70, "endOffset": 81}, {"referenceID": 41, "context": "A number of approaches have been proposed for more efficient sampling [1, 29, 42].", "startOffset": 70, "endOffset": 81}, {"referenceID": 8, "context": "It was essentially introduced in [9], but without making the explicit connection to Dpps.", "startOffset": 33, "endOffset": 36}, {"referenceID": 32, "context": "An additional argument based on [33] yields high probability bounds, too (Appendix A).", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "To show Theorem 1, we exploit a property of characteristic polynomials observed in [22].", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "Lemma 2 (Guruswami and Sinop [22]).", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": ", [19] on uniform and leverage score sampling), our bounds seem somewhat weaker asymptotically (since as c\u2192 N they do not converge to 1).", "startOffset": 2, "endOffset": 6}, {"referenceID": 32, "context": "Again, using [33], we obtain bounds that hold with high probability (Appendix A).", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "We build on [2, 5].", "startOffset": 12, "endOffset": 18}, {"referenceID": 4, "context": "We build on [2, 5].", "startOffset": 12, "endOffset": 18}, {"referenceID": 1, "context": "There exist works that consider Nystr\u00f6m methods in this scenario [2, 5].", "startOffset": 65, "endOffset": 71}, {"referenceID": 4, "context": "There exist works that consider Nystr\u00f6m methods in this scenario [2, 5].", "startOffset": 65, "endOffset": 71}, {"referenceID": 23, "context": "Recent empirical results provide initial evidence [24], but without a theoretical analysis3; other recent works [21, 34] do not apply to our cardinality-constrained setting.", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "Recent empirical results provide initial evidence [24], but without a theoretical analysis3; other recent works [21, 34] do not apply to our cardinality-constrained setting.", "startOffset": 112, "endOffset": 120}, {"referenceID": 33, "context": "Recent empirical results provide initial evidence [24], but without a theoretical analysis3; other recent works [21, 34] do not apply to our cardinality-constrained setting.", "startOffset": 112, "endOffset": 120}, {"referenceID": 3, "context": "[4] derived a different, general analysis of fast mixing that also confirms our observations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Given a Markov chain (Yt) on a state space \u03a9 with 3The analysis in [24] is not correct.", "startOffset": 67, "endOffset": 71}, {"referenceID": 2, "context": "If, in the new chain, Pr(Yt 6= Zt) \u2264 \u03b5 for some fixed t regardless of the starting state (Y0, Z0), then \u03c4(\u03b5) \u2264 t [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 10, "context": "Path coupling [11] relieves this burden by reducing the coupling to adjacent states in an appropriately constructed state graph.", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "[11, 16] Let \u03b4 be an integer-valued metric on \u03a9\u00d7\u03a9 where \u03b4(\u00b7, \u00b7) \u2264 D.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[11, 16] Let \u03b4 be an integer-valued metric on \u03a9\u00d7\u03a9 where \u03b4(\u00b7, \u00b7) \u2264 D.", "startOffset": 0, "endOffset": 8}, {"referenceID": 40, "context": "closely related to a symmetry that determines the complexity of submodular maximization [41] (indeed, F(S) = log det KS is a submodular function).", "startOffset": 88, "endOffset": 92}, {"referenceID": 33, "context": "It holds for kernels with sufficiently fast-decaying similarities, similar to the conditions in [34] for unconstrained sampling.", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "One iteration of the sampler can be implemented efficiently in O(c2) time using block inversion [20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 29, "context": "Additional speedups via quadrature are also possible [30].", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "We compare to uniform sampling (Unif) and leverage score sampling (Lev) [19] as baseline landmark selection methods.", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": "We also include AdapFull (AdapFull) [13] that performs quite well in practice but scales poorly, as O(N2), with the size of dataset.", "startOffset": 36, "endOffset": 40}, {"referenceID": 1, "context": "Although sampling with regularized leverage scores (RegLev) [2] is not originally designed for kernel approximations, we include its results to see how regularization affects leverage score sampling.", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "2 Kernel Ridge Regression Next, we apply Dpp-Nystr\u00f6m to kernel ridge regression, comparing against uniform sampling (Unif) [5] and regularized leverage score sampling (RegLev) [2] which have theoretical guarantees for this task.", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "2 Kernel Ridge Regression Next, we apply Dpp-Nystr\u00f6m to kernel ridge regression, comparing against uniform sampling (Unif) [5] and regularized leverage score sampling (RegLev) [2] which have theoretical guarantees for this task.", "startOffset": 176, "endOffset": 179}, {"referenceID": 1, "context": "For a fast, rougher approximation, we here compare to an approximation mentioned in [2].", "startOffset": 84, "endOffset": 87}, {"referenceID": 27, "context": "We also include AdapPartial (AdapPart) [28] that approximates AdapFull and is much more efficient, and Kmeans Nystr\u00f6m (Kmeans) [44] that empirically perform very well in kernel approximation.", "startOffset": 39, "endOffset": 43}, {"referenceID": 43, "context": "We also include AdapPartial (AdapPart) [28] that approximates AdapFull and is much more efficient, and Kmeans Nystr\u00f6m (Kmeans) [44] that empirically perform very well in kernel approximation.", "startOffset": 127, "endOffset": 131}, {"referenceID": 0, "context": "References [1] R.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] O.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[40] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[44] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Theorem 6 (Pemantle and Peres [33]).", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "It is known that a k-Dpp is a homogeneous strongly Rayleigh measure on {0, 1}N [4, 10], thus Theorem 6 applies to results obtained with k-Dpp.", "startOffset": 79, "endOffset": 86}, {"referenceID": 9, "context": "It is known that a k-Dpp is a homogeneous strongly Rayleigh measure on {0, 1}N [4, 10], thus Theorem 6 applies to results obtained with k-Dpp.", "startOffset": 79, "endOffset": 86}], "year": 2016, "abstractText": "The Nystr\u00f6m method has long been popular for scaling up kernel methods. Its theoretical guarantees and empirical performance rely critically on the quality of the landmarks selected. We study landmark selection for Nystr\u00f6m using Determinantal Point Processes (Dpps), discrete probability models that allow tractable generation of diverse samples. We prove that landmarks selected via Dpps guarantee bounds on approximation errors; subsequently, we analyze implications for kernel ridge regression. Contrary to prior reservations due to cubic complexity of Dpp sampling, we show that (under certain conditions) Markov chain Dpp sampling requires only linear time in the size of the data. We present several empirical results that support our theoretical analysis, and demonstrate the superior performance of Dpp-based landmark selection compared with existing approaches.", "creator": "LaTeX with hyperref package"}}}