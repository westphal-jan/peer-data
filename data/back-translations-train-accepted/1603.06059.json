{"id": "1603.06059", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Generating Natural Questions About an Image", "abstract": "There has been an explosion of work in the vision &amp; language community during the past few years from image captioning to video transcription, and answering questions about images. These tasks focus on literal descriptions of the image. To move beyond the literal, we choose to explore how questions about an image often address abstract events that the objects evoke. In this paper, we introduce the novel task of 'Visual Question Generation (VQG)', where the system is tasked with asking a natural and engaging question when shown an image. We provide three datasets which cover a variety of images from object-centric to event-centric, providing different and more abstract training data than the state-of-the-art captioning systems have used thus far. We train and test several generative and retrieval models to tackle the task of VQG. Evaluation results show that while such models ask reasonable questions given various images, there is still a wide gap with human performance. Our proposed task offers a new challenge to the community which we hope can spur further interest in exploring deeper connections between vision &amp; language.", "histories": [["v1", "Sat, 19 Mar 2016 07:27:15 GMT  (5296kb,D)", "https://arxiv.org/abs/1603.06059v1", null], ["v2", "Tue, 22 Mar 2016 06:54:58 GMT  (5298kb,D)", "http://arxiv.org/abs/1603.06059v2", null], ["v3", "Thu, 9 Jun 2016 01:20:49 GMT  (4546kb,D)", "http://arxiv.org/abs/1603.06059v3", "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["nasrin mostafazadeh", "ishan misra", "jacob devlin", "margaret mitchell", "xiaodong he", "lucy vanderwende"], "accepted": true, "id": "1603.06059"}, "pdf": {"name": "1603.06059.pdf", "metadata": {"source": "CRF", "title": "Generating Natural Questions About an Image", "authors": ["Nasrin Mostafazadeh", "Ishan Misra", "Jacob Devlin", "Margaret Mitchell", "Xiaodong He", "Lucy Vanderwende"], "emails": ["nasrinm@cs.rochester.edu,", "lucyv@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to trump themselves, and they are able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. \"(...) Most of them are not able to trump themselves.\" (...)"}, {"heading": "2 Related Work", "text": "For the task of captioning images, we have focused primarily on objects, e.g. Pascal VOC (QOC et al., 2010) and Microsoft Common Objects in Context (MS COCO) (Lin et al., 2014). MS COCO, for example, includes complex everyday scenes in 328k images, each paired with 5 captions. Event detection is the focus in video processing and action detection, but these are not include a textual description of the event (Yao et al., 2011b; Chao et al., 2015; Xiong et al., 2015). The number of actions in each of these datasets is still relatively small, ranging from 40 (Chao et al.) to 600 (Chao et al.) and all involve human-oriented activities (e.g. \"cooking,\" gardening \")."}, {"heading": "3 Data Collection Methodology", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "3.3 CaptionsBing\u22125000", "text": "The frequency of questions about the V QGBing \u2212 5000 dataset indicates that this dataset is very different from the MS COCO dataset. microsoft.com / en-us / downloads to get a link to a dynamic visualization and statistics of all n-gram sequences. To further investigate this difference, we crowdsourced 5 captions for each image in the V QGBing \u2212 5000 dataset with the same prompt as the source of the MS COCO labels. We call this new dataset CaptionsBing \u2212 5000. Table 3 shows the results of testing the state-of-the-art MSR dataset for CaptionsBing \u2212 5000 datasets used in comparison to the MS COCO datasets."}, {"heading": "4 Models", "text": "In this section, we present several generative and retrieval models for accomplishing the task of VQG. For all upcoming models, we use the VGGNet architecture (Simonyan and Zisserman, 2014) to calculate deep revolutionary image properties. We primarily use the 4096-dimensional output of the last fully connected layer (fc7) as input to the generative models."}, {"heading": "4.1 Generative Models", "text": "Figure 7 provides an overview of our three generative network models. The MELM model (Fang et al., 2014) is a pipeline based on a series of word probabilities of the candidates trained directly on images that then go through a language model of maximum entropy (ME). The MT model is a Sequence2Sequence translation model (Cho et al., 2014; Sutskever et al., 2014) that translates an image description directly into a question using MS-COCO labels and CaptionsBing \u2212 5000 as a translation source. These two models tended to produce less coherent sentences, the details of which can be found in the supplementary material. We obtained the best results by using an end-to-end-to-end neural model, GRNN, as a consequence. Gated Recurrent Neural Network (GRNN): This generation model is based on the state-of-the-of-the-art neural network used for imaging."}, {"heading": "4.2 Retrieval Methods", "text": "For the task of captioning, it was shown that up to 80% of the captions generated at test date by an almost state-of-the-art generational approach (Vinyals et al., 2015) were exactly identical to the captions of the captions, suggesting that the reuse of captions can achieve good results. In addition, basic approaches to captioning on MS COCO datasets were shown to outperform generation models by automatic metrics (Devlin et al., 2015). The performance of the retrievable models of the course depends on the diversity of datasets. We implemented several retriever models tailored to the task of VQG. As a first step, we calculate K-next adjacent images for each test image using the 7-dynamic results we obtain."}, {"heading": "5 Evaluation", "text": "While the number of possible questions in VQG is not limited, there is consensus among the natural questions (discussed in Section 3.1) that makes meaningful evaluation possible. Although human evaluation is the ideal form of evaluation, it is important to find an automatic metric that strongly correlates with human judgment to measure progress in the task."}, {"heading": "5.1 Human Evaluation", "text": "The quality of the evaluation is partly determined by the way the evaluation is presented. For example, 11Average-Word2Vec refers to the metric of textual similarity at the sentence level, where we calculate the cosinal similarity between two sentences by calculating their word-level vector representations Word2Vec (Mikolov et al., 2013). In this context, we use the GenSim software framework (R-ehu-r-jecek and Sojka, 2010).It is important that the human judges see different system hypotheses simultaneously to give a calibrated evaluation. We have crowdsourced our human evaluation on AMT and asked three crowd-workers to evaluate the quality of candidate questions on a three-level semantic scale."}, {"heading": "5.2 Automatic Evaluation", "text": "The goal of the automatic evaluation is to measure the similarity of the system-generated question hypotheses and the crowdsourced question references. To measure the overlap and textual similarities between hypotheses and references, we use standard metrics for machine translation, BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). We use BLEU with equal weights up to 4 grams and default setting of METEOR version 1.5. In addition, we use BLEU (Galley et al., 2015), which is specifically tailored to generational tasks with different references, such as conversations. The BLEU requires evaluation by reference, distinguishing between the quality of the references. To this end, we have crowdsourced three human ratings (on a scale of 1-3) per reference and used majority evaluation. The paired correlational analysis of human and automatic metrics is presented in Table 6, where we have the BLS-SpearS system and the very BLS-efficient one."}, {"heading": "5.3 Results", "text": "In this section, we present the human and automatic metric evaluation results of the previously introduced models. We randomly divided each VQG5000 data set into train sets (50%), valley sets (25%) and test sets (25%). To shed some light on the differences between our three data sets, we present the evaluation results separately on each data set in Table 5. Each model (Section 4.2) is trained once on all train sets and once only on its corresponding train set (shown as X in the results table). For quality control and further insight into the task, we include two human comments between our models: \"Humanconsensus\" (the same as a best), which represents the consensus human comment on the test image, and \"Humanrandom,\" which is a randomly selected comment among the five human comments."}, {"heading": "6 Discussion", "text": "We introduced the novel task of the \"Visual Question Generation,\" in which the system is assigned the task of asking a natural question based on an image. We provide three different sets of data, each covering a multitude of images. The biggest challenge is the Bing dataset, where systems must generate questions with event-centric concepts such as \"cause,\" \"event,\" \"happen,\" etc., from the visual input. In addition, we demonstrate that our Bing dataset represents challenging images for modern capturing systems. We encourage the community to report their system results on the Bing test dataset and according to the automatic measurement \"BLEU.\" All datasets are made available to the public. This work focuses on the development of the Capabil-12find Please Visual Question Generation at http: / / research.microsoft.com / en-us / downloads.ity to ask relevant and to-the-point questions, a central step we believe is a natural behavior of the system we should anticipate in order to develop a QI."}, {"heading": "Acknowledgment", "text": "We thank the anonymous reviewers for their valuable comments. We thank Larry Zitnick and Devi Parikh for their helpful discussions on this work, Rebecca Hanson for their great help in collecting data, Michel Galley for his evaluation guidelines, and Bill Dolan for his valuable feedback during this work."}], "references": [{"title": "2d human pose estimation: New benchmark and state of the art analysis", "author": ["Mykhaylo Andriluka", "Leonid Pishchulin", "Peter Gehler", "Bernt Schiele."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.", "citeRegEx": "Andriluka et al\\.,? 2014", "shortCiteRegEx": "Andriluka et al\\.", "year": 2014}, {"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "International Conference on Computer Vision (ICCV).", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Mind the gap: Learning to choose gaps for question generation", "author": ["Lee Becker", "Sumit Basu", "Lucy Vanderwende."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Becker et al\\.,? 2012", "shortCiteRegEx": "Becker et al\\.", "year": 2012}, {"title": "HICO: A benchmark for recognizing human-object interactions in images", "author": ["Yu-Wei Chao", "Zhan Wang", "Yugeng He", "Jiaxuan Wang", "Jia Deng."], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "Chao et al\\.,? 2015", "shortCiteRegEx": "Chao et al\\.", "year": 2015}, {"title": "D\u00e9j\u00e0 image-captions: A corpus of expressive descriptions in repetition", "author": ["Jianfu Chen", "Polina Kuznetsova", "David Warren", "Yejin Choi."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Language models for image captioning: The quirks and what works", "author": ["Jacob Devlin", "Hao Cheng", "Hao Fang", "Saurabh Gupta", "Li Deng", "Xiaodong He", "Geoffrey Zweig", "Margaret Mitchell."], "venue": "Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Devlin et al\\.,? 2015", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "CoRR, abs/1411.4389.", "citeRegEx": "Donahue et al\\.,? 2014", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "The pascal visual object classes (voc) challenge", "author": ["Mark Everingham", "Luc Gool", "Christopher K. Williams", "John Winn", "Andrew Zisserman."], "venue": "Int. J. Comput. Vision, 88(2):303\u2013338, June.", "citeRegEx": "Everingham et al\\.,? 2010", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest N. Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C. Platt", "C. Lawrence Zitnick", "Geoffrey Zweig."], "venue": "CoRR,", "citeRegEx": "Fang et al\\.,? 2014", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth."], "venue": "Proceedings of the 11th European Conference on", "citeRegEx": "Farhadi et al\\.,? 2010", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "A survey of current datasets for vision and language research", "author": ["Francis Ferraro", "Nasrin Mostafazadeh", "Ting-Hao Huang", "Lucy Vanderwende", "Jacob Devlin", "Michel Galley", "Margaret Mitchell."], "venue": "Proceedings of the 2015 Conference on Empiri-", "citeRegEx": "Ferraro et al\\.,? 2015", "shortCiteRegEx": "Ferraro et al\\.", "year": 2015}, {"title": "deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceed-", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu."], "venue": "CoRR, abs/1505.05612.", "citeRegEx": "Gao et al\\.,? 2015", "shortCiteRegEx": "Gao et al\\.", "year": 2015}, {"title": "Good question! statistical ranking for question generation", "author": ["Michael Heilman", "Noah A. Smith."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Heilman and Smith.,? 2010", "shortCiteRegEx": "Heilman and Smith.", "year": 2010}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "J. Artif. Int. Res., 47(1):853\u2013899, May.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Who Asked the First Question? The Origins of Human Choral Singing, Intelligence, Language and Speech", "author": ["Joseph Jordania."], "venue": "Logos.", "citeRegEx": "Jordania.,? 2006", "shortCiteRegEx": "Jordania.", "year": 2006}, {"title": "Deep questions without deep understanding", "author": ["Igor Labutov", "Sumit Basu", "Lucy Vanderwende."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan-", "citeRegEx": "Labutov et al\\.,? 2015", "shortCiteRegEx": "Labutov et al\\.", "year": 2015}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "author": ["Chin-Yew Lin", "Franz Josef Och."], "venue": "Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Lin and Och.,? 2004", "shortCiteRegEx": "Lin and Och.", "year": 2004}, {"title": "Microsoft COCO: common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollar", "C. Lawrence Zitnick."], "venue": "CoRR, abs/1405.0312.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Generating natural language questions to support learning on-line", "author": ["David Lindberg", "Fred Popowich", "John Nesbit", "Phil Winne."], "venue": "Proceedings of the 14th European Workshop on Natural Language Generation, pages 105\u2013114, Sofia, Bulgaria, Au-", "citeRegEx": "Lindberg et al\\.,? 2013", "shortCiteRegEx": "Lindberg et al\\.", "year": 2013}, {"title": "A multiworld approach to question answering about realworld scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz."], "venue": "Advances in Neural Information Processing Systems 27, pages 1682\u20131690.", "citeRegEx": "Malinowski and Fritz.,? 2014", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "Linguistic considerations in automatic question generation", "author": ["Karen Mazidi", "Rodney D. Nielsen."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 321\u2013326, Baltimore, Mary-", "citeRegEx": "Mazidi and Nielsen.,? 2014", "shortCiteRegEx": "Mazidi and Nielsen.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller."], "venue": "Commun. ACM, 38(11):39\u201341, November.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Computer-aided generation of multiple-choice tests", "author": ["Ruslan Mitkov", "Le An Ha."], "venue": "Jill Burstein and Claudia Leacock, editors, Proceedings of the HLT-NAACL 03 Workshop on Building Educational Applications Using Natural Language Processing,", "citeRegEx": "Mitkov and Ha.,? 2003", "shortCiteRegEx": "Mitkov and Ha.", "year": 2003}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L. Berg."], "venue": "Neural Information Processing Systems (NIPS).", "citeRegEx": "Ordonez et al\\.,? 2011", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318,", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "The TIMEBANK corpus", "author": ["J. Pustejovsky", "P. Hanks", "R. Sauri", "A. See", "R. Gaizauskas", "A. Setzer", "D. Radev", "B. Sundheim", "D. Day", "L. Ferro", "M. Lazo."], "venue": "Proceedings of Corpus Linguistics 2003, pages 647\u2013656, Lancaster, March.", "citeRegEx": "Pustejovsky et al\\.,? 2003", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2003}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u2013 50, Valletta, Malta, May. ELRA. http://is.", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "Question answering about images using visual semantic embeddings", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel."], "venue": "Deep Learning Workshop, ICML 2015.", "citeRegEx": "Ren et al\\.,? 2015", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "A database for fine grained activity detection of cooking activities", "author": ["Marcus Rohrbach", "Sikandar Amin", "Mykhaylo Andriluka", "Bernt Schiele."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, IEEE, June.", "citeRegEx": "Rohrbach et al\\.,? 2012", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman."], "venue": "CoRR, abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceed-", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Rich image captioning in the wild", "author": ["Kenneth Tran", "Xiaodong He", "Lei Zhang", "Jian Sun", "Cornelia Carapcea", "Chris Thrasher", "Chris Buehler", "Chris Sienkiewicz."], "venue": "Proceedings of Deep Vision Workshop at CVPR 2016. IEEE, June.", "citeRegEx": "Tran et al\\.,? 2016", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "An amr parser for english, french, german, spanish and japanese and a new amr-annotated corpus", "author": ["Lucy Vanderwende", "Arul Menezes", "Chris Quirk."], "venue": "Proceedings of NAACL 2015, June.", "citeRegEx": "Vanderwende et al\\.,? 2015", "shortCiteRegEx": "Vanderwende et al\\.", "year": 2015}, {"title": "The importance of being important: Question generation", "author": ["Lucy Vanderwende."], "venue": "In Workshop on the Question Generation Shared Task and Evaluation Challenge, Arlington, VA.", "citeRegEx": "Vanderwende.,? 2008", "shortCiteRegEx": "Vanderwende.", "year": 2008}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko."], "venue": "Proceedings the 2015 Conference of the North", "citeRegEx": "Venugopalan et al\\.,? 2015", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Computer Vision and Pattern Recognition.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Automatic question generation from text-an aid to independent study", "author": ["John H Wolfe."], "venue": "ACM SIGCUE Outlook, volume 10, pages 104\u2013112. ACM.", "citeRegEx": "Wolfe.,? 1976", "shortCiteRegEx": "Wolfe.", "year": 1976}, {"title": "Recognize complex events from static images by fusing deep channels", "author": ["Yuanjun Xiong", "Kai Zhu", "Dahua Lin", "Xiaoou Tang."], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.", "citeRegEx": "Xiong et al\\.,? 2015", "shortCiteRegEx": "Xiong et al\\.", "year": 2015}, {"title": "Action recognition by learning bases of action attributes and parts", "author": ["Bangpeng Yao", "Xiaoye Jiang", "Aditya Khosla", "Andy Lai Lin", "Leonidas J. Guibas", "Li Fei-Fei."], "venue": "International Conference on Computer Vision (ICCV), Barcelona, Spain, Novem-", "citeRegEx": "Yao et al\\.,? 2011a", "shortCiteRegEx": "Yao et al\\.", "year": 2011}, {"title": "Human action recognition by learning bases of action attributes and parts", "author": ["Bangpeng Yao", "Xiaoye Jiang", "Aditya Khosla", "Andy Lai Lin", "Leonidas J. Guibas", "Li Fei-Fei."], "venue": "International Conference on Computer Vision (ICCV), Barcelona, Spain, Novem-", "citeRegEx": "Yao et al\\.,? 2011b", "shortCiteRegEx": "Yao et al\\.", "year": 2011}, {"title": "Visual7w: Grounded question answering in images", "author": ["Yuke Zhu", "Oliver Groth", "Michael S. Bernstein", "Li Fei-Fei."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, IEEE.", "citeRegEx": "Zhu et al\\.,? 2016", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "tioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) and video Natural Questions: - Was anyone injured in the crash? - Is the motorcyclist alive? - What caused this accident?", "startOffset": 8, "endOffset": 87}, {"referenceID": 10, "context": "tioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) and video Natural Questions: - Was anyone injured in the crash? - Is the motorcyclist alive? - What caused this accident?", "startOffset": 8, "endOffset": 87}, {"referenceID": 8, "context": "tioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) and video Natural Questions: - Was anyone injured in the crash? - Is the motorcyclist alive? - What caused this accident?", "startOffset": 8, "endOffset": 87}, {"referenceID": 4, "context": "tioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) and video Natural Questions: - Was anyone injured in the crash? - Is the motorcyclist alive? - What caused this accident?", "startOffset": 8, "endOffset": 87}, {"referenceID": 33, "context": "transcription (Rohrbach et al., 2012; Venugopalan et al., 2015), to testing computer understanding of an image through question answering (Antol et al.", "startOffset": 14, "endOffset": 63}, {"referenceID": 40, "context": "transcription (Rohrbach et al., 2012; Venugopalan et al., 2015), to testing computer understanding of an image through question answering (Antol et al.", "startOffset": 14, "endOffset": 63}, {"referenceID": 1, "context": ", 2015), to testing computer understanding of an image through question answering (Antol et al., 2015; Malinowski and Fritz, 2014).", "startOffset": 82, "endOffset": 130}, {"referenceID": 23, "context": ", 2015), to testing computer understanding of an image through question answering (Antol et al., 2015; Malinowski and Fritz, 2014).", "startOffset": 82, "endOffset": 130}, {"referenceID": 7, "context": "It has been shown (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) that a reasonable language modeling paired with deep visual features trained on large enough datasets promise a good performance on image captioning, making it a less challenging task from language learning perspective.", "startOffset": 18, "endOffset": 80}, {"referenceID": 10, "context": "It has been shown (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) that a reasonable language modeling paired with deep visual features trained on large enough datasets promise a good performance on image captioning, making it a less challenging task from language learning perspective.", "startOffset": 18, "endOffset": 80}, {"referenceID": 8, "context": "It has been shown (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) that a reasonable language modeling paired with deep visual features trained on large enough datasets promise a good performance on image captioning, making it a less challenging task from language learning perspective.", "startOffset": 18, "endOffset": 80}, {"referenceID": 39, "context": "Learning to ask questions is an important task in NLP and is more than a syntactic transformation of a declarative sentence (Vanderwende, 2008).", "startOffset": 124, "endOffset": 143}, {"referenceID": 17, "context": "Furthermore, training a system to ask a good question (not only answer a question) may imbue the system with what appears to be a cognitive ability unique to humans among other primates (Jordania, 2006).", "startOffset": 186, "endOffset": 202}, {"referenceID": 10, "context": "Abstract terms refer to intangible things, such as feelings, concepts, and qualities Throughout this paper we use the state-of-the-art captioning system (Fang et al., 2014), henceforth MSR captioning system https://www.", "startOffset": 153, "endOffset": 172}, {"referenceID": 9, "context": "Pascal VOC (Everingham et al., 2010) and Microsoft Common Objects in Context (MS COCO) (Lin et al.", "startOffset": 11, "endOffset": 36}, {"referenceID": 21, "context": ", 2010) and Microsoft Common Objects in Context (MS COCO) (Lin et al., 2014).", "startOffset": 58, "endOffset": 76}, {"referenceID": 45, "context": "Event detection is the focus in video processing and action detection, but these do not include a textual description of the event (Yao et al., 2011b; Andriluka et al., 2014; Chao et al., 2015; Xiong et al., 2015).", "startOffset": 131, "endOffset": 213}, {"referenceID": 0, "context": "Event detection is the focus in video processing and action detection, but these do not include a textual description of the event (Yao et al., 2011b; Andriluka et al., 2014; Chao et al., 2015; Xiong et al., 2015).", "startOffset": 131, "endOffset": 213}, {"referenceID": 3, "context": "Event detection is the focus in video processing and action detection, but these do not include a textual description of the event (Yao et al., 2011b; Andriluka et al., 2014; Chao et al., 2015; Xiong et al., 2015).", "startOffset": 131, "endOffset": 213}, {"referenceID": 43, "context": "Event detection is the focus in video processing and action detection, but these do not include a textual description of the event (Yao et al., 2011b; Andriluka et al., 2014; Chao et al., 2015; Xiong et al., 2015).", "startOffset": 131, "endOffset": 213}, {"referenceID": 44, "context": "atively small, ranging from 40 (Yao et al., 2011a) to 600 (Chao et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 3, "context": ", 2011a) to 600 (Chao et al., 2015) and all involve humanoriented activity (e.", "startOffset": 16, "endOffset": 35}, {"referenceID": 1, "context": "The most notable, Visual Question Answering (VQA) (Antol et al., 2015), is an open-ended (free-form) dataset, in which both the questions and the answers are crowd-sourced, with workers prompted to ask a visually verifiable question which will \u2018stump a", "startOffset": 50, "endOffset": 70}, {"referenceID": 32, "context": "COCO-QA (CQA) (Ren et al., 2015), in contrast, does not use human-authored questions, but generates questions automatically from image captions of the MS COCO dataset by applying a set of transformation rules to generate", "startOffset": 14, "endOffset": 32}, {"referenceID": 14, "context": "Gao et al. (2015) used similar methodology to create a visual question answering dataset in Chinese.", "startOffset": 0, "endOffset": 18}, {"referenceID": 46, "context": "A recently published work on VQA, Visual7W (Zhu et al., 2016), establishes a grounding link on the object regions corresponding to the textual answer.", "startOffset": 43, "endOffset": 61}, {"referenceID": 42, "context": "Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.", "startOffset": 103, "endOffset": 162}, {"referenceID": 27, "context": "Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.", "startOffset": 103, "endOffset": 162}, {"referenceID": 15, "context": "Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.", "startOffset": 103, "endOffset": 162}, {"referenceID": 2, "context": "For the latter, several methods have been explored: (Becker et al., 2012) create fill-in-the-blank ques-", "startOffset": 52, "endOffset": 73}, {"referenceID": 24, "context": "tions, (Mazidi and Nielsen, 2014) and (Lindberg et al.", "startOffset": 7, "endOffset": 33}, {"referenceID": 22, "context": "tions, (Mazidi and Nielsen, 2014) and (Lindberg et al., 2013) use manually constructed question templates, while (Labutov et al.", "startOffset": 38, "endOffset": 61}, {"referenceID": 18, "context": ", 2013) use manually constructed question templates, while (Labutov et al., 2015) use crowdsourcing to collect a set of templates and then rank the potentially relevant templates for the selected content.", "startOffset": 59, "endOffset": 81}, {"referenceID": 32, "context": "dataset (Ren et al., 2015) and by VQA (Antol et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 1, "context": ", 2015) and by VQA (Antol et al., 2015).", "startOffset": 19, "endOffset": 39}, {"referenceID": 38, "context": "(2015), we use a list of most common abstract terms in English (Vanderwende et al., 2015), and count all the other words except a set of function words as concrete.", "startOffset": 63, "endOffset": 89}, {"referenceID": 12, "context": "Following Ferraro et al. (2015), we use a list of most common abstract terms in English (Vanderwende et al.", "startOffset": 10, "endOffset": 32}, {"referenceID": 29, "context": "ure 3(e) shows inter-annotation textual similarity according to the BLEU metric (Papineni et al., 2002).", "startOffset": 80, "endOffset": 103}, {"referenceID": 26, "context": "To obtain a more representative visualization of specific event types, we queried a search engine7 with 1,200 event-centric query terms which were obtained as follows: we aggregated all \u2018event\u2019 and \u2018process\u2019 hyponyms in WordNet (Miller, 1995),", "startOffset": 228, "endOffset": 242}, {"referenceID": 30, "context": "1,000 most frequent TimeBank events (Pustejovsky et al., 2003) and a set of manually curated 30 stereotypical events, from which we selected the top 1,200 queries based on Project Gutenberg word frequencies.", "startOffset": 36, "endOffset": 62}, {"referenceID": 37, "context": "Human evaluation results of a recent work (Tran et al., 2016) further confirms the significant image captioning quality degradation on out-of-domain data.", "startOffset": 42, "endOffset": 61}, {"referenceID": 29, "context": "Table 3 shows the results of testing the state-of-the-art MSR captioning system on the CaptionsBing\u22125000 dataset as compared to the MS COCO dataset, measured by the standard BLEU (Papineni et al., 2002) and ME-", "startOffset": 179, "endOffset": 202}, {"referenceID": 6, "context": "TEOR (Denkowski and Lavie, 2014) metrics.", "startOffset": 5, "endOffset": 32}, {"referenceID": 34, "context": "For all the forthcoming models we use the VGGNet (Simonyan and Zisserman, 2014) architecture for computing deep convolutional image features.", "startOffset": 49, "endOffset": 79}, {"referenceID": 10, "context": "The MELM model (Fang et al., 2014) is a pipeline starting from a set of candidate word probabilities which are directly trained", "startOffset": 15, "endOffset": 34}, {"referenceID": 5, "context": "The MT model is a Sequence2Sequence translation model (Cho et al., 2014; Sutskever et al., 2014) which directly translates a description of an image into a question, where we used the MS COCO captions and CaptionsBing\u22125000 as the source of translation.", "startOffset": 54, "endOffset": 96}, {"referenceID": 36, "context": "The MT model is a Sequence2Sequence translation model (Cho et al., 2014; Sutskever et al., 2014) which directly translates a description of an image into a question, where we used the MS COCO captions and CaptionsBing\u22125000 as the source of translation.", "startOffset": 54, "endOffset": 96}, {"referenceID": 7, "context": "Gated Recurrent Neural Network (GRNN): This generation model is based on the state-of-theart multimodal Recurrent Neural Network model used for image captioning (Devlin et al., 2015; Vinyals et al., 2015).", "startOffset": 161, "endOffset": 204}, {"referenceID": 41, "context": "Gated Recurrent Neural Network (GRNN): This generation model is based on the state-of-theart multimodal Recurrent Neural Network model used for image captioning (Devlin et al., 2015; Vinyals et al., 2015).", "startOffset": 161, "endOffset": 204}, {"referenceID": 41, "context": "For the task of image captioning, it has been shown that up to 80% of the captions generated at test time by a near state-of-the-art generation approach (Vinyals et al., 2015) were exactly identical to the training set captions, which suggests that reusing training annotations can achieve good results.", "startOffset": 153, "endOffset": 175}, {"referenceID": 7, "context": "basic nearest neighbor approaches to image captioning on the MS COCO dataset are shown to outperform generation models according to automatic metrics (Devlin et al., 2015).", "startOffset": 150, "endOffset": 171}, {"referenceID": 20, "context": "ing validation sets using the Smoothed-BLEU (Lin and Och, 2004) metric against the human reference questions.", "startOffset": 44, "endOffset": 63}, {"referenceID": 25, "context": "Average-Word2Vec refers to the sentence-level textual similarity metric where we compute the cosine similarity between two sentences by averaging their word-level Word2Vec (Mikolov et al., 2013) vector representations.", "startOffset": 172, "endOffset": 194}, {"referenceID": 31, "context": "Here we use the GenSim software framework (\u0158eh\u016f\u0159ek and Sojka, 2010).", "startOffset": 42, "endOffset": 67}, {"referenceID": 6, "context": ", 2002) and METEOR (Denkowski and Lavie, 2014).", "startOffset": 19, "endOffset": 46}, {"referenceID": 13, "context": "Additionally we use \u2206BLEU (Galley et al., 2015) which is specifically tailored towards generation tasks with diverse references, such as conversations.", "startOffset": 26, "endOffset": 47}, {"referenceID": 35, "context": "this work is to include question generation within a conversational system (Sordoni et al., 2015; Li et al., 2016), where the context and conversation history affect the types of questions being asked.", "startOffset": 75, "endOffset": 114}, {"referenceID": 19, "context": "this work is to include question generation within a conversational system (Sordoni et al., 2015; Li et al., 2016), where the context and conversation history affect the types of questions being asked.", "startOffset": 75, "endOffset": 114}], "year": 2016, "abstractText": "There has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription, and answering questions about images. These tasks have focused on literal descriptions of the image. To move beyond the literal, we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image. In this paper, we introduce the novel task of Visual Question Generation (VQG), where the system is tasked with asking a natural and engaging question when shown an image. We provide three datasets which cover a variety of images from object-centric to event-centric, with considerably more abstract training data than provided to state-of-the-art captioning systems thus far. We train and test several generative and retrieval models to tackle the task of VQG. Evaluation results show that while such models ask reasonable questions for a variety of images, there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics. Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language.", "creator": "LaTeX with hyperref package"}}}