{"id": "1603.06067", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings", "abstract": "We present a novel method for jointly learning compositional and non-compositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function. The scoring function is used to quantify the level of compositionality of each phrase, and the parameters of the function are jointly optimized with the objective for learning phrase embeddings. In experiments, we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases, and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality, substantially outperforming the previous state of the art. Moreover, our embeddings improve upon the previous best model on a transitive verb disambiguation task. We also show that a simple ensemble technique further improves the results for both tasks.", "histories": [["v1", "Sat, 19 Mar 2016 08:53:29 GMT  (156kb)", "http://arxiv.org/abs/1603.06067v1", null], ["v2", "Tue, 22 Mar 2016 07:24:51 GMT  (91kb,D)", "http://arxiv.org/abs/1603.06067v2", null], ["v3", "Wed, 8 Jun 2016 07:46:27 GMT  (93kb,D)", "http://arxiv.org/abs/1603.06067v3", "Accepted as a full paper at the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kazuma hashimoto", "yoshimasa tsuruoka"], "accepted": true, "id": "1603.06067"}, "pdf": {"name": "1603.06067.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["hassy@logos.t.u-tokyo.ac.jp", "tsuruoka@logos.t.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.06 067v 1 [cs.C L] 19 Mar 201 6"}, {"heading": "1 Introduction", "text": "In most of the previous work, phrase embedding is calculated from word embedding using different types of embedding functions, known as compositional embedding. An alternative way of embedding phrase embedding is to treat phrase embedding as a single entity and assign each candidate a unique embedding (Mikolov et al., 2013; Yazdani et al., 2015). Such embedding is called compositional embedding, which refers exclusively to non-compositional embedding and has the obvious problem of data embedding (i.e. rare or unknown phrase unit problems). At the same time, however, using compositional embedding is not the best option, as some phrases are inherently not composed."}, {"heading": "2 Method", "text": "In this section we describe our approach in the most general form, without specifying the function for calculating the compositional embedding or the target task for optimizing the embedding. Figure 1 shows the overview of our proposed method. For each iteration of the training (i.e. gradient calculation) of a specific target task (e.g. language modelling, machine translation or mood analysis), our method first calculates a compositivity score for each phrase. Afterwards, the score is used to weigh the compositional and non-compositional embedding of the phrase in order to calculate the expected embedding of the phrase to be used in the target task. Some examples of the compositivity values are also shown in the figure."}, {"heading": "2.1 Compositional Phrase Embeddings", "text": "The compositional embedding c (p) Rd \u00b7 1 of a phrase p = (w1, \u00b7 \u00b7, wL) becomes asc (p) = f (v (w1), \u00b7 \u00b7, v (wL)), (1) where d is the dimensionality, L is the phrase length, v (\u00b7) and Rd \u00b7 1 is a word embedding and f (\u00b7) is a compositional function. These may be simple functions such as elementary addition or multiplication (Mitchell and Lapata, 2008). More complex and recurring neural networks (Sutskever et al., 2014) are also commonly used. Word embedding and compositional functions are learned jointly on a specific target task. Since compositional embedding is word-based (i.e. unigram) information, they are less susceptible to the problem of data economy."}, {"heading": "2.2 Non-Compositional Phrase Embeddings", "text": "In contrast to the compositional embedding, the non-compositional embedding of a phrase n (p) and Rd \u00d7 1 is parameterised independently of each other, i.e. the phrase p is treated as a single word. Mikolov et al. (2013) show that non-compositional embedding is preferable when it comes to idiomatic phrases. However, some recent studies (Kartsaklis et al., 2014; Muraoka et al., 2014) have discussed the (dis) advantages of using compositional or non-compositional embedding, but in most cases a phrase is neither fully compositional nor fully compositional. To our knowledge, there is no method that allows us to learn compositional and non-compositional embedding together by including the levels of the compositivity of the phrases as real scores."}, {"heading": "2.3 Adaptive Joint Learning", "text": "In order to simultaneously consider both compositional and non-compositional aspects of each phrase, we calculate a phrase embedded v (p) by adaptive weighting of c (p) and n (p) as follows: v (p) = \u03b1 (p) + (1 \u2212 \u03b1 (p)) n (p), where \u03b1 (\u00b7) is a scoring function that quantifies the levels of compositivity and a real value in the range of 0 to 1. What we expect from the scoring function is that large values indicate a high degree of compositionality. In other words, when \u03b1 (p) is close to 1, the compositional embedding p is mainly considered, and vice versa. We expect \u03b1 (buy car) to be small, as shown in Figure 1. We parameterise the scoring function \u03b1 (p) as logistic regression."}, {"heading": "3 Learning Verb Phrase Embeddings", "text": "This section describes a particular instantiation of our approach, which was presented in the previous section, dealing with the task of learning the embedding of transient verb phrases."}, {"heading": "3.1 Word and Phrase Prediction in Predicate-Argument Relations", "text": "Acquisition of selective preferences using embedding has been widely studied, learning word and / or phrase embedding based on syntactic linkages (Bansal et al., 2014; Hashimoto and Tsuruoka, 2015; Levy and Goldberg, 2014; Van de Cruys, 2014). As with language modeling, these methods perform word (or phrase) predictions using (syntactic) contexts. In this paper, we focus on verb-object relationships and use a formulation presented in Hashimoto and Tsuruoka (2015), which is a plausibility assessment task for predicate argument tuples. They extracted subject-object noun (SVO preposition noun) tuples using a probable HPSG parser, Enju (Miyao and Tsujii, 2008), from the training companies."}, {"heading": "3.2 Applying the Adaptive Joint Learning", "text": "In this section, we apply our proposed method, adaptive collective learning, to the task described in Section 3.1. Here, we redefine the calculation of v (V O) by first counting v (V O) in equation (11) by c (V O) as, c (V O) = M (V) v (O), (15) and then V O to p in equation (2) and (3): v (V O) = \u03b1 (V O) c (V O) + (1 \u2212 \u03b1 (V O) n (V O)), (16) \u03b1 (V O) = properties count in equation. (17) The v (V O) in equation. (16) is used in equation. (12) and (13). We assume that the candidates of the phrases are given in advance. For the phrases not contained in the candidates, we set v (V O) = c (V O)."}, {"heading": "4 Experimental Settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Training Data", "text": "For training data, we used two sets of data, one small and one large: the British National Corpus (BNC) (Leech, 1992) and the English Wikipedia. Specifically, we used the publicly available Data2, which was pre-processed by Hashimoto and Tsuruoka (2015). BNC data consists of 1.38 million SVO tuples and 0.93 million SVOPN tuples. Wikipedia data consists of 23.6 million SVO tuples and 17.3 million SVOPN tuples. Following the provided Code3, we used exactly the same move / development / test split (0.8 / 0.1 / 0.1) for training the overall model. As a third training data, we also used the concatenation of the two data, hereinafter referred to as BNC-Wikipedia.We applied our adaptive common learning method to verb-object phrases observed more than K times in each corpus."}, {"heading": "4.2 Training Details", "text": "The model parameters consist of d-dimensional word embedding for nouns, non-compositional phrase embedding, d x d-dimensional matrices for verbs and prepositions, and a weight vector W for \u03b1 (V O). All model parameters are optimized together. We initialized the embedding and matrices with zero-mean-Gauss random values with a variance of 1d and 1 d2, respectively, and W with zeros. Initializing W with zeros forces the initial value of each \u03b1 (V O) to 0.5, since we use the logistic function to calculate \u03b1 (V O). Optimization was performed via Minibatch AdaGrad (Duchi et al., 2011). We set d to 25 and the minibatch size to 100. We set the candidate values for the learning rate to {0.01, 0.02, 0.03, 0.04, 0.05}. For the weight vectors, we used W for the training we selected the L2 for the strength and the regulation."}, {"heading": "5 Evaluation on the Compositionality Detection Function", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Evaluation Settings", "text": "First, we evaluated the compositivity detection function based on two sets of compositivity detection data, VJ '054 and MC' 075, provided by Venkatapathy and Joshi (2005) and McCarthy et al. (2007), respectively. VJ '05 consists of 765 verb-object pairs with human scores for compositivity. MC' 07 is a subset of VJ '05 and consists of 638 verb-object pairs. For example, the rating for \"Buy a car\" is 6, which is the highest score, indicating that the phrase is highly compositional. The rating for \"bear fruit\" is 1, with 4http: / / www.dianamccarthy.co.uk / downloads / SVAJ2005composiality _ rating.txt 5 http: / www.dianamccarthy.co.uk / downloads / emnlp2007data.txtis the lowest score, which indicates that the phrase is not highly compositional."}, {"heading": "5.2 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 Result Overview", "text": "Table 1 shows our results and the state of the art. Our method outperforms the previous state of the art in all areas. The result referred to as ensemble is the one that applies ensemble technique and achieves the strongest correlation with the human-annotated data sets. Even without ensemble technique, our method performs better than all previous methods. Kiela and Clark (2013) used window-based cocidence vectors and improved their score using WordNet hypernyms. In contrast, our method does not rely on such external resources and requires only analyzed corpora. We should note that Kiela and Clark (2013) reported that their score did not improve when using sliced corpora. Our method also outperforms DSPROTO +, which used a small portion of the specified data while our method is completely uncontrolled."}, {"heading": "5.2.2 Analysis of Compositionality Scores", "text": "Figure 2 shows how \u03b1 (V O) changes for the seven phrases during training on the BNC data. As shown in the figure, starting from 0.5, \u03b1 (V O) for6We will convert the Scipy 0.12.0 into Python.each phrase converges to its corresponding value. The differences in trends suggest that our method can not learn compositionality levels for the phrases. Table 2 shows the learned compositionality scores for the three groups of results along with the gold standard scores. The group (A) is considered consistent with the gold standard scores, the group (B) is not, and the group (C) shows examples for which the difference between the compositionality scores of our results is large.Characteristics of light verb \"take,\" \"make\" and \"have\" have. \""}, {"heading": "5.2.3 Effects of Ensemble", "text": "In order to verify the results on VJ '05, we calculated the correlation between the results from our results of the BNC and Wikipedia data. The correlation value is 0.674, which means that the two different corpora results are reasonably consistent, indicating the robustness of our method. However, the correlation value is still much lower than the perfect correlation; in other words, there are inconsistencies between the results learned with the corpora. Group (C) in Table 2 shows such two examples. In these cases, the ensemble technique is helpful in improving the results, as shown in the examples. Another interesting observation in our results is that the result of the ensemble technique exceeds the results of the BNC Wikipedia data, as shown in Table 1. This shows that the separate use of the training techniques and subsequently the implementation of the ensemble technique can provide better results. In contrast, many previous studies can be conducted on the basis of different corporations."}, {"heading": "6 Evaluation on the Phrase Embeddings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Evaluation Settings", "text": "Next, we evaluated the learned embeddings in the transitive verb disambiguation dataset GS '118 by Grefenstette and Sadrzadeh (2011). GS' 11 consists of 200 pairs of transitive verbs and each pair of verbs assumes the same subject and object. For8 http: / / www.cs.ox.ac.uk / activities / compdistmeaning / GS2011data.txtexample the transitive verb \"run\" is known as an ambiguous word, and this task requires identifying the meanings of \"run\" and \"operation\" as similar when taking \"people\" as subject and \"company\" as object. However, in the same setting the meanings of \"run\" and \"move\" are not similar to each other. Each pair has several human evaluations indicating how similar the phrases of the pair are. Rating metric and ensembles The evaluation was done by calculating the rank correlation between the human evaluations and the cosmic evaluations ensemble B."}, {"heading": "6.2 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1 Result Overview", "text": "Table 4 shows our results and the state of the art, and our method surpasses all previous methods in all areas. So far, the results in Hashimoto and Tsuruoka (2015) have been the best and correspond to those with \u03b1 (V O) = 1. We see that our method significantly improves the initial values by combining compositional and non-compositional embeddings adaptively. Together with the results in Table 1, these results show that our method allows us to improve the compositional function by jointly learning non-compositional embeddings and the scoring function to detect compositivity."}, {"heading": "6.2.2 Analysis of the Learned Embeddings", "text": "We investigated the effects of an adaptive weighting of compositional and non-compositional embeddings. Table 5 shows the five closest adjacent phrases with respect to the cosinal similarity of the three idiomatic phrases \"demanding tribute,\" \"catch eye\" and \"bearing fruit.\" Examples trained on the basis of Wikipedia data are shown for our method and baseline, i.e., \u03b1 (V O) = 1. As shown in Table 2, the compositivity levels of the three phrases are low and their non-compositional embeddings are mainly used to represent their meaning. An observation with \u03b1 (V O) = 1 is that headers (i.e. verbs) are highlighted in the examples shown, except \"demanding tribute.\" As with other embeddings methods, compositional embeddings are strongly influenced by their component words. As a result, phrases consisting of the same verb and similar objects often allow us to use the compositional method as our next compositional neighbors."}, {"heading": "7 Related Work", "text": "The embedding of words and phrases has been extensively studied in the field of natural language processing.The embedding of phrases has been shown to be effective in many linguistic processing tasks, such as machine translation (Cho et al., 2014; Sutskever et al., 2014), emotion analysis and semantic textual similarity (Tai et al., 2015).Most embedding of phrases is constructed by information at word level about different types of compositional functions such as long-term memory (Hochreiter and Schmidhuber, 1997) recurring neural networks.Such compositional functions should be powerful enough to efficiently encode information about all words into the phrase embeddings. By taking into account the compositivity of the phrases at the same time, our method would be helpful in making the compositional models so powerful that they could have the non-compositional purpose of coding perfectly from this first step to the first one."}, {"heading": "8 Conclusion and Future Work", "text": "We have presented a method for adaptive learning of compositional and non-compositional phrase embedding by jointly determining levels of compositivity of phrases. Our method reaches the state of the art in a compositivity recognition task of verb-object pairs and also improves the previous state-of-the-art method for a transitive verb-disambiguation task. In future work we will apply our method to other types of phrases and tasks."}, {"heading": "Acknowledgments", "text": "This work was supported by CREST, JST."}, {"heading": "2011 Conference on Empirical Methods in Natural", "text": "[Hashimoto and Tsuruoka2015] Kazuma Hashimoto and Yoshimasa Tsuruoka. 2014. Learning Embeddings for Transitive Verb Disambiguation by Implicit Tensor Factorization. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, pages 1-11. [Hashimoto et al. 2014] Kazuma Hashimoto, Pontus Stenetorp, and Yoshimasa Tsuruoka. 2014. Jointly Learning Word Representations and Composition Functions Using Predicate-Argument Structures. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1544-1555. [Hochreiter and Schmidhuber1997] Sepp Hochreiter and Ju \u00bc rgen Schmidhuber."}], "references": [{"title": "Tailoring Continuous Word Representations for Dependency Parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Transla", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Word Association Norms, Mutual Information and Lexicography", "author": ["Church", "Hanks1990] Kenneth Church", "Patrick Hanks"], "venue": "Computational Linguistics,", "citeRegEx": "Church et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Church et al\\.", "year": 1990}, {"title": "Verb Noun Construction MWE Token Classification", "author": ["Diab", "Bhutada2009] Mona Diab", "Pravin Bhutada"], "venue": "In Proceedings of the Workshop on Multiword Expressions: Identification,", "citeRegEx": "Diab et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Diab et al\\.", "year": 2009}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A Multiword Expression Data Set: Annotating NonCompositionality and Conventionalization for English Noun Compounds", "author": ["Aaron Smith", "Joakim Nivre"], "venue": null, "citeRegEx": "Farahmand et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2015}, {"title": "Experimental Support for a Categorical Compositional Distributional Model of Meaning", "author": ["Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": null, "citeRegEx": "Grefenstette et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2011}, {"title": "Learning Embeddings for Transitive Verb Disambiguation by Implicit Tensor Factorization", "author": ["Hashimoto", "Tsuruoka2015] Kazuma Hashimoto", "Yoshimasa Tsuruoka"], "venue": "In Proceedings of the 3rd Workshop on Continuous Vector Space Models", "citeRegEx": "Hashimoto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2015}, {"title": "Jointly Learning Word Representations and Composition Functions Using Predicate-Argument Structures", "author": ["Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka"], "venue": "In Proceedings of the 2014 Conference", "citeRegEx": "Hashimoto et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2014}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "A Unified Sentence Space for Categorical DistributionalCompositional Semantics: Theory and Experiments", "author": ["Mehrnoosh Sadrzadeh", "Stephen Pulman"], "venue": "In Proceedings of the 24th International Conference", "citeRegEx": "Kartsaklis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2012}, {"title": "Resolving Lexical Ambiguity in Tensor Regression Models of Meaning", "author": ["Nal Kalchbrenner", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Kartsaklis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2014}, {"title": "Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models", "author": ["Kiela", "Clark2013] Douwe Kiela", "Stephen Clark"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural", "citeRegEx": "Kiela et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2013}, {"title": "Dependency-Based Word Embeddings", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Automatic Identification of Non-compositional Phrases", "author": ["Dekang Lin"], "venue": "In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Lin.,? \\Q1999\\E", "shortCiteRegEx": "Lin.", "year": 1999}, {"title": "Detecting a Continuum of Compositionality in Phrasal Verbs", "author": ["Bill Keller", "John Carroll"], "venue": "In Proceedings of the ACL 2003 Workshop on Multiword Expressions:", "citeRegEx": "McCarthy et al\\.,? \\Q2003\\E", "shortCiteRegEx": "McCarthy et al\\.", "year": 2003}, {"title": "Detecting Compositionality of Verb-Object Combinations using Selectional Preferences", "author": ["Sriram Venkatapathy", "Aravind Joshi"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods", "citeRegEx": "McCarthy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McCarthy et al\\.", "year": 2007}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Evaluating Neural Word Representations in Tensor-Based Compositional Settings", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Matthew Purver"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods", "citeRegEx": "Milajevs et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Milajevs et al\\.", "year": 2014}, {"title": "Vector-based Models of Semantic Composition", "author": ["Mitchell", "Lapata2008] Jeff Mitchell", "Mirella Lapata"], "venue": "In Proceedings of 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Feature Forest Models for Probabilistic HPSG Parsing", "author": ["Miyao", "Tsujii2008] Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "Computational Linguistics,", "citeRegEx": "Miyao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Miyao et al\\.", "year": 2008}, {"title": "Finding The Best Model Among Representative Compositional Models", "author": ["Sonse Shimaoka", "Kazeto Yamamoto", "Yotaro Watanabe", "Naoaki Okazaki", "Kentaro Inui"], "venue": "In Proceedings of the 28th Pa-", "citeRegEx": "Muraoka et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Muraoka et al\\.", "year": 2014}, {"title": "Basic English Syntax with Exercises. B\u00f6lcs\u00e9sz Konzorcium", "author": ["Mark Newton"], "venue": null, "citeRegEx": "Newton.,? \\Q2006\\E", "shortCiteRegEx": "Newton.", "year": 2006}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model", "author": ["Pham et al.2015] Nghia The Pham", "Germ\u00e1n Kruszewski", "Angeliki Lazaridou", "Marco Baroni"], "venue": null, "citeRegEx": "Pham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "Using Sentence Plausibility to Learn the Semantics of Transitive Verbs", "author": ["Laura Rimell", "Stephen Clark"], "venue": null, "citeRegEx": "Polajnar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Polajnar et al\\.", "year": 2014}, {"title": "An Exploration of Discourse-Based Sentence Spaces for Compositional Distributional Semantics", "author": ["Laura Rimell", "Stephen Clark"], "venue": "In Proceedings of the First Workshop on Linking Computational Mod-", "citeRegEx": "Polajnar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Polajnar et al\\.", "year": 2015}, {"title": "An Empirical Study on Compositionality in Compound Nouns", "author": ["Reddy et al.2011] Siva Reddy", "Diana McCarthy", "Suresh Manandhar"], "venue": "In Proceedings of 5th International Joint Conference on Natural Language Processing,", "citeRegEx": "Reddy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2011}, {"title": "Semantic Compositionality through Recursive Matrix-Vector Spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved Semantic Representations From Tree-Structured Long ShortTerm Memory Networks", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Compu-", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Measuring the Relative Compositionality of Verb-Noun (V-N) Collocations by Integrating Features", "author": ["Venkatapathy", "Joshi2005] Sriram Venkatapathy", "Aravind Joshi"], "venue": "In Proceedings of Human Language Technology", "citeRegEx": "Venkatapathy et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Venkatapathy et al\\.", "year": 2005}, {"title": "Learning Semantic Composition to Detect Noncompositionality of Multiword Expressions", "author": ["Meghdad Farahmand", "James Henderson"], "venue": "In Proceedings of the 2015 Conference on Empiri-", "citeRegEx": "Yazdani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yazdani et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 24, "context": "tor space has proven effective in a variety of language processing tasks (Pham et al., 2015; Sutskever et al., 2014).", "startOffset": 73, "endOffset": 116}, {"referenceID": 29, "context": "tor space has proven effective in a variety of language processing tasks (Pham et al., 2015; Sutskever et al., 2014).", "startOffset": 73, "endOffset": 116}, {"referenceID": 17, "context": "An alternative way of computing phrase embeddings is to treat phrases as single units and assigning a unique embedding to each candidate phrase (Mikolov et al., 2013; Yazdani et al., 2015).", "startOffset": 144, "endOffset": 188}, {"referenceID": 32, "context": "An alternative way of computing phrase embeddings is to treat phrases as single units and assigning a unique embedding to each candidate phrase (Mikolov et al., 2013; Yazdani et al., 2015).", "startOffset": 144, "endOffset": 188}, {"referenceID": 24, "context": "One approach is to learn compositional embeddings by regarding all phrases as compositional (Pham et al., 2015; Socher et al., 2012).", "startOffset": 92, "endOffset": 132}, {"referenceID": 28, "context": "One approach is to learn compositional embeddings by regarding all phrases as compositional (Pham et al., 2015; Socher et al., 2012).", "startOffset": 92, "endOffset": 132}, {"referenceID": 11, "context": "and use the better ones (Kartsaklis et al., 2014; Muraoka et al., 2014).", "startOffset": 24, "endOffset": 71}, {"referenceID": 21, "context": "and use the better ones (Kartsaklis et al., 2014; Muraoka et al., 2014).", "startOffset": 24, "endOffset": 71}, {"referenceID": 10, "context": "and use the better ones (Kartsaklis et al., 2014; Muraoka et al., 2014). Kartsaklis et al. (2014) show that non-compositional embeddings are better suited for a phrase similarity task, whereas Muraoka et al.", "startOffset": 25, "endOffset": 98}, {"referenceID": 10, "context": "and use the better ones (Kartsaklis et al., 2014; Muraoka et al., 2014). Kartsaklis et al. (2014) show that non-compositional embeddings are better suited for a phrase similarity task, whereas Muraoka et al. (2014) report the opposite results on other tasks.", "startOffset": 25, "endOffset": 215}, {"referenceID": 29, "context": "More complex ones such as recurrent neural networks (Sutskever et al., 2014) are also commonly used.", "startOffset": 52, "endOffset": 76}, {"referenceID": 11, "context": "Some recent studies (Kartsaklis et al., 2014; Muraoka et al., 2014) have discussed the (dis)advantages of using compositional or non-compositional embeddings.", "startOffset": 20, "endOffset": 67}, {"referenceID": 21, "context": "Some recent studies (Kartsaklis et al., 2014; Muraoka et al., 2014) have discussed the (dis)advantages of using compositional or non-compositional embeddings.", "startOffset": 20, "endOffset": 67}, {"referenceID": 15, "context": "Mikolov et al. (2013) show that noncompositional embeddings are preferable when dealing with idiomatic phrases.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Acquisition of selectional preference using embeddings has been widely studied, where word and/or phrase embeddings are learned based on syntactic links (Bansal et al., 2014; Hashimoto and Tsuruoka, 2015; Levy and Goldberg, 2014; Van de Cruys, 2014).", "startOffset": 153, "endOffset": 249}, {"referenceID": 0, "context": "Acquisition of selectional preference using embeddings has been widely studied, where word and/or phrase embeddings are learned based on syntactic links (Bansal et al., 2014; Hashimoto and Tsuruoka, 2015; Levy and Goldberg, 2014; Van de Cruys, 2014). As with language modeling, these methods perform word (or phrase) prediction using (syntactic) contexts. In this work, we focus on verb-object relationships and employ a phrase embedding learning method presented in Hashimoto and Tsuruoka (2015). The task is a plausibility judgment task for predicate-argument tuples.", "startOffset": 154, "endOffset": 497}, {"referenceID": 4, "context": "The matrices and embeddings are learned by minimizing the cost function using AdaGrad (Duchi et al., 2011).", "startOffset": 86, "endOffset": 106}, {"referenceID": 10, "context": "as proposed by Kartsaklis et al. (2012). The operator \u2299 denotes element-wise multiplication.", "startOffset": 15, "endOffset": 40}, {"referenceID": 15, "context": "The second set of features, frequency and PMI (Church and Hanks, 1990) features, have proven effective in detecting the compositionality of transitive verbs in McCarthy et al. (2007) and Venkatapathy and Joshi (2005).", "startOffset": 160, "endOffset": 183}, {"referenceID": 15, "context": "The second set of features, frequency and PMI (Church and Hanks, 1990) features, have proven effective in detecting the compositionality of transitive verbs in McCarthy et al. (2007) and Venkatapathy and Joshi (2005). Given the training corpus, the frequency feature for a VO pair is computed as", "startOffset": 160, "endOffset": 217}, {"referenceID": 4, "context": "The optimization was performed via minibatch AdaGrad (Duchi et al., 2011).", "startOffset": 53, "endOffset": 73}, {"referenceID": 15, "context": "provided by Venkatapathy and Joshi (2005) and McCarthy et al. (2007), respectively.", "startOffset": 46, "endOffset": 69}, {"referenceID": 16, "context": "420 DSPROTO+ (McCarthy et al., 2007) 0.", "startOffset": 13, "endOffset": 36}, {"referenceID": 16, "context": "454 n/a DSPROTO (McCarthy et al., 2007) 0.", "startOffset": 16, "endOffset": 39}, {"referenceID": 16, "context": "398 n/a PMI (McCarthy et al., 2007) 0.", "startOffset": 12, "endOffset": 35}, {"referenceID": 16, "context": "274 n/a Frequency (McCarthy et al., 2007) 0.", "startOffset": 18, "endOffset": 41}, {"referenceID": 22, "context": "2 in Newton (2006), the term light verb is used to refer to verbs which can be used in combination with some other element where their contribution to the meaning of the whole construction is reduced in some way.", "startOffset": 5, "endOffset": 19}, {"referenceID": 21, "context": "By contrast, many of the previous studies on embedding-based methods combine different corpora into a single dataset, or use multiple corpora just separately and compare them (Hashimoto and Tsuruoka, 2015; Muraoka et al., 2014; Pennington et al., 2014).", "startOffset": 175, "endOffset": 252}, {"referenceID": 23, "context": "By contrast, many of the previous studies on embedding-based methods combine different corpora into a single dataset, or use multiple corpora just separately and compare them (Hashimoto and Tsuruoka, 2015; Muraoka et al., 2014; Pennington et al., 2014).", "startOffset": 175, "endOffset": 252}, {"referenceID": 16, "context": "574 n/a Milajevs et al. (2014) 0.", "startOffset": 8, "endOffset": 31}, {"referenceID": 16, "context": "574 n/a Milajevs et al. (2014) 0.456 n/a Polajnar et al. (2014) n/a 0.", "startOffset": 8, "endOffset": 64}, {"referenceID": 7, "context": "370 Hashimoto et al. (2014) 0.", "startOffset": 4, "endOffset": 28}, {"referenceID": 7, "context": "370 Hashimoto et al. (2014) 0.420 0.340 Polajnar et al. (2015) n/a 0.", "startOffset": 4, "endOffset": 63}, {"referenceID": 7, "context": "370 Hashimoto et al. (2014) 0.420 0.340 Polajnar et al. (2015) n/a 0.330 Grefenstette and Sadrzadeh (2011) n/a 0.", "startOffset": 4, "endOffset": 107}, {"referenceID": 1, "context": "The phrase embeddings have proven effective in many language processing tasks, such as machine translation (Cho et al., 2014; Sutskever et al., 2014), sentiment analysis and semantic textual similarity (Tai et al.", "startOffset": 107, "endOffset": 149}, {"referenceID": 29, "context": "The phrase embeddings have proven effective in many language processing tasks, such as machine translation (Cho et al., 2014; Sutskever et al., 2014), sentiment analysis and semantic textual similarity (Tai et al.", "startOffset": 107, "endOffset": 149}, {"referenceID": 30, "context": ", 2014), sentiment analysis and semantic textual similarity (Tai et al., 2015).", "startOffset": 60, "endOffset": 78}, {"referenceID": 14, "context": "Many studies have focused on detecting the compositionality of a variety of phrases (Lin, 1999), including the ones on verb phrases (Diab and Bhutada, 2009; McCarthy et al.", "startOffset": 84, "endOffset": 95}, {"referenceID": 15, "context": "Many studies have focused on detecting the compositionality of a variety of phrases (Lin, 1999), including the ones on verb phrases (Diab and Bhutada, 2009; McCarthy et al., 2003) and compound nouns (Farahmand et al.", "startOffset": 132, "endOffset": 179}, {"referenceID": 5, "context": ", 2003) and compound nouns (Farahmand et al., 2015; Reddy et al., 2011; Yazdani et al., 2015).", "startOffset": 27, "endOffset": 93}, {"referenceID": 27, "context": ", 2003) and compound nouns (Farahmand et al., 2015; Reddy et al., 2011; Yazdani et al., 2015).", "startOffset": 27, "endOffset": 93}, {"referenceID": 32, "context": ", 2003) and compound nouns (Farahmand et al., 2015; Reddy et al., 2011; Yazdani et al., 2015).", "startOffset": 27, "endOffset": 93}, {"referenceID": 16, "context": "Compared to statistical featurebased methods (McCarthy et al., 2007; Venkatapathy and Joshi, 2005), re-", "startOffset": 45, "endOffset": 98}, {"referenceID": 32, "context": "embeddings (Kiela and Clark, 2013; Yazdani et al., 2015).", "startOffset": 11, "endOffset": 56}, {"referenceID": 32, "context": "Yazdani et al. (2015)", "startOffset": 0, "endOffset": 22}], "year": 2017, "abstractText": "We present a novel method for jointly learning compositional and noncompositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function. The scoring function is used to quantify the level of compositionality of each phrase, and the parameters of the function are jointly optimized with the objective for learning phrase embeddings. In experiments, we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases, and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality, substantially outperforming the previous state of the art. Moreover, our embeddings improve upon the previous best model on a transitive verb disambiguation task. We also show that a simple ensemble technique further improves the results for both tasks.", "creator": "LaTeX with hyperref package"}}}