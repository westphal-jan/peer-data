{"id": "1603.06075", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Tree-to-Sequence Attentional Neural Machine Translation", "abstract": "Most of the existing neural machine translation (NMT) models focus on the conversion of sequential data and do not directly take syntax into consideration. We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.", "histories": [["v1", "Sat, 19 Mar 2016 10:08:40 GMT  (437kb)", "https://arxiv.org/abs/1603.06075v1", null], ["v2", "Tue, 22 Mar 2016 09:55:39 GMT  (340kb,D)", "http://arxiv.org/abs/1603.06075v2", null], ["v3", "Wed, 8 Jun 2016 08:39:11 GMT  (371kb,D)", "http://arxiv.org/abs/1603.06075v3", "Accepted as a full paper at the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["akiko eriguchi", "kazuma hashimoto", "yoshimasa tsuruoka"], "accepted": true, "id": "1603.06075"}, "pdf": {"name": "1603.06075.pdf", "metadata": {"source": "CRF", "title": "Tree-to-Sequence Attentional Neural Machine Translation", "authors": ["Akiko Eriguchi", "Kazuma Hashimoto"], "emails": ["tsuruoka}@logos.t.u-tokyo.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Translating words from English into Japanese has traditionally been one of the most complex language processing problems, but recent advances in Neural Machine Translation (NMT) have made it possible to perform translations with a simple end-to-end architecture. In the encoder decoder model (Cho et al., 2014b; Sutskever et al., 2014), the encoder reads the entire sequence of source words to generate a fixed length vector, and then another RNN generates the target words from the vector. The encoder decoder model has been enhanced with an attention mechanism (Bahdanau et al, 2015; Luong et al, 2015a) that allows the model to jointly learn the soft alignment between the source language and the target language."}, {"heading": "2 Neural Machine Translation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Encoder-Decoder Model", "text": "In other words, the NMT models are often referred to as encoder decoder models. In the encoder decoder models, a set is treated as a sequence of words. In the encoder process, a set is treated as a sequence of words. In the encoder decoder model, a set is treated as a sequence of words. In the encoder process, the encoder decoder set is treated as a sequence of words."}, {"heading": "2.2 Attentional Encoder-Decoder Model", "text": "The NMT models with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a) have been proposed to align each decoder state smoothly with the encoder states. The attention mechanism allows the NMT models to explicitly quantify how much each encoder state contributes to the word prediction at any time. In the attentive NMT model in Luong et al. (2015a), in the j-th step of the decoder process, the attention value \u03b1j (i) between the i-th hidden unit hi and the j-th hidden unit sj is calculated as follows: \u03b1j (i) = exp (hi \u00b7 sj)."}, {"heading": "2.3 Objective Function of NMT Models", "text": "The objective function for training NMT models is the sum of the log probabilities of the translation pairs in the training data: J (\u03b8) = 1 | D | \u2211 (x, y) \u0394Dlog p (y | x), (9) where D denotes a series of parallel sentence pairs. \u03b8 model parameters are learned by Stochastic Gradient Descent (SGD)."}, {"heading": "3 Attentional Tree-to-Sequence Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Tree-based Encoder + Sequential Encoder", "text": "The exsitienden NMT models are regarded as a succession of terms and neglects that are reflected in the structure of being. (D) D \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i. (D) i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i. (D) i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i."}, {"heading": "3.2 Initial Decoder Setting", "text": "As shown in Figure 3, we provide another tree LSTM unit that contains the final sequential encoder unit (hn) and the tree-based encoder unit (h (phr) root) as two child units, and set it as the initial s1 decoder as follows: s1 = gtree (hn, h (phr) root), (12) where gtree has the same function as ftree with another set of tree LSTM parameters. This initialization allows the decoder to collect information from both the sequential data and the phrase structures. Zoph and Knight (2016) proposed a similar method with a tree LSTM to initialize the decoder, allowing it to translate multiple source languages into a target language."}, {"heading": "3.3 Attention Mechanism in Our Model", "text": "This attention mechanism tells us which words or phrases are important in the source sentence when the model decipheres a target word. (13) The j-th context vector dj is composed of the sequential and phrase vectors weighted by the attention value \u03b1j (i). (13) Note that a binary tree has n \u2212 1 phrase nodes if the tree has n leaves. We set a definitive decoder s-j in the same way as Equation (7). Furthermore, we use the input feeding method (Luong et al., 2015a) in our model, which is a method for predicting the word yj \u2212 1, the current insertion unit (Luong et al., 2015a)."}, {"heading": "3.4 Sampling-Based Approximation to the NMT Models", "text": "The main computational bottleneck in the formation of NMT models is the calculation of the Softmax layer, which is described in Equation (8), as the computational costs increase linearly with the size of the vocabulary. GPU speedup has proven useful for sequence-based NMT models (Sutskever et al., 2014; Luong et al., 2015a), but it is not easily applicable in the handling of tree-structured data. To reduce the training costs of NMT models on the Softmax layer, we use BlackOut (Ji et al., 2016), a sampling-based approximation method. BlackOut has proven to be effective in RNN language models (RNLMs), allowing a model to run relatively quickly even with a million word vocabularies."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Training Data", "text": "We applied the proposed model to the English-Japanese translation data set of the ASPEC corpus specified in WAT '15.1. Following Zhu (2015), we extracted the first 1.5 million translation pairs from the training data. To obtain the phrase structures of the source sentences, i.e. English, we used the probabilistic HPSG parser Enju (Miyao and Tsujii, 2008). We used Enju only to obtain a binary phrase structure for each sentence, and then filtered out the translation pairs whose sentence lengths are longer than 50 and whose source sentences are not successfully analyzed. Table 1 shows the details of the data sets used in our experiments and performed the pre-processing steps recommended in WAT' 15.2."}, {"heading": "4.2 Training Details", "text": "The distortions, Softmax weights and BlackOut weights are initialized with zeros. BlackOut's hyperparameter \u03b2 is set to 0.4, as recommended by Ji et al. (2016). Following Jo \u0301 zefowicz et al. (2015), we initialize the forge gate distortions of LSTM and Tree-LSTM at 1.0. The remaining model parameters in the NMT models in our experiments are uniformly initialized at [\u2212 0.1, 0.1]. Model parameters are optimized by simple SGD with the mini-stack size of 128. Initial learning rate of SGD is 1.0. We halve the learning rate when the development loss worsens. Grade classification standards are shortened to 3.0 to avoid exploding gradient problems (Pascanu et al., 2012).Small training datasets We conduct experiments with our proposed model and the sequential word insertion model with the word insertion-MT-25th word-inserted model."}, {"heading": "4.3 Decoding process", "text": "We use the beam search to decipher a target sentence for an input sentence x and calculate the sum of the log probabilities of the target sentence y = (y1, \u00b7 \u00b7 \u00b7, ym) as the beam value: score (x, y) = m \u2211 j = 1 log p (yj | y < j, x). (15) Decryption in the NMT models is a generative process and depends on the target language model of a source sentence. Beam search using the length standardization in Cho et al. (2014a) does not work well when decrypting a long sentence (Cho et al., 2014a; Pouget-Abadie et al., 2014). In our preliminary experiments, the beam search using the length standardization in Cho et al. (2014a) was not effective in the English-Japanese translation (Cho et al., 2014a; Pouget-Abadie al, 2014)."}, {"heading": "4.4 Evaluation", "text": "We evaluated the models using two automatic evaluation metrics, RIBES (Isozaki et al., 2010) and BLEU (Papineni et al., 2002), following WAT '15. We used the KyTea-based evaluation script for the translation results. The RIBES value is a measure based on rank correlation coefficients with word accuracy, and the BLEU value is based on word accuracy in n-grams and a Brevity Penalty (BP) for results that are shorter than the references. RIBES is known to correlate more strongly with human judgments than BLEU when translating between English and Japanese, as discussed in Isozaki et al. (2010)."}, {"heading": "5 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Small Training Dataset", "text": "In fact, we are able to put ourselves at the top of the list, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \"\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \"\" We have to put ourselves at the top. \""}, {"heading": "5.2 Large Training Dataset", "text": "Table 6 shows the experimental results of RIBES and BLEU obtained by the trained models on the large dataset. We decipher the target rates by our proposed beam search with the beam size of 20.8 The results of the other systems are those obtained in Nakazawa et al. (2015).All of our proposed models show similar performance regardless of the value of the d. Our ensemble model consists of the three models with d = 512, 768 and 1024, and it shows the best RIBES score among all systems. 9As for the time required for training, our implementation takes about a day to perform an epoch on the large training dataset with d = 512. It would take about 11 days without using the BlackOut models."}, {"heading": "5.3 Qualitative Analysis", "text": "We illustrate the translations of the test data using our model with d = 512 and multiple attention relationships when decoding a sentence. Figures 4 and 5 translate an English sentence presented as a binary tree into Japanese, and several attention relationships between English words or phrases and Japanese words are shown with the highest attention scale. The additional attention relationships are also illustrated for comparison. We see the target words gently aligned with the source words and phrases. In Figure 4, the Japanese word means \"liquid crystal,\" and it has a high attention force score (\u03b1 = 0.41) with the English phrase \"liquid crystal for active matrix.\" This is due to the fact that the j-th hidden unit sj contains the contextual information about the previous words y < j, including \"liquid crystal,\" and it has a high attention force of 0.41 (= 0.41) for English crystal."}, {"heading": "6 Related Work", "text": "Kalchbrenner and Blunsom (2013) were the first to propose an end-to-end NMT model that uses Convolutional Neural Networks (CNNs) as the source encoder and RNNs as the target decoder. However, the encoder decoder model can be considered an extension of their model and replaces the CNNs with RNNNs using GRUs (Cho et al., 2014b) or LSTMs (Sutskever et al., 2014).Sutskever et al. (2014) have shown that reversing the input sequences in a Franco-English translation task is effective, and the technique has also proven effective in translation tasks between other European language pairs (Luong et al., 2015a).All of the above-mentioned NMT models are based on sequential encoders. In order to incorporate structural information into the NMT models, Cho et al. (2014a) has suggested learning dynamics based on them rather than on the structures in common."}, {"heading": "7 Conclusion", "text": "In this paper, we propose a novel syntactic approach that extends attention-based NMT models. We focus on the phrase structure of the input sentence and build a tree-based encoder that follows the analyzed tree. Our proposed tree-based encoder is a natural extension of the sequential encoder model, where the leaf units of the tree LSTM in the encoder can interact with the original LSTM sequential encoder. Furthermore, the attention mechanism allows the tree-based encoder to align not only the input words, but also the input phrases with the output words. Experimental results on the translation dataset WAT '15 between English and Japanese show that our proposed model achieves the best RIBES score and surpasses the sequential attention-based NMT model."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their constructive comments and suggestions. This work was supported by CREST, JST and JSPS KAKENHI Grant Number 15J12597."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation", "author": ["Cheng et al.2016] Yong Cheng", "Shiqi Shen", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proceedings of the 25th International Joint Con-", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "author": ["Cho et al.2014a] KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Proceedings of Eighth Workshop on Syntax,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Transla", "author": ["Cho et al.2014b] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model", "author": ["Cohn et al.2016] Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari"], "venue": null, "citeRegEx": "Cohn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association", "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "Learning to Forget: Continual Prediction with LSTM", "author": ["Gers et al.2000] Felix A. Gers", "J\u00fcrgen Schmidhuber", "Fred A. Cummins"], "venue": "Neural Computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gutmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2012}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Automatic Evaluation of Translation Quality", "author": ["Tsutomu Hirao", "Kevin Duh", "Katsuhito Sudoh", "Hajime Tsukada"], "venue": null, "citeRegEx": "Isozaki et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Isozaki et al\\.", "year": 2010}, {"title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies", "author": ["Ji et al.2016] Shihao Ji", "S.V.N. Vishwanathan", "Nadathur Satish", "Michael J. Anderson", "Pradeep Dubey"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "NAVER Machine Translation System for WAT 2015", "author": ["Lee et al.2015] Hyoung-Gyu Lee", "JaeSong Lee", "JunSeok Kim", "Chang-Ki Lee"], "venue": "In Proceedings of the 2nd Workshop on Asian Translation", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Tree-to-string alignment template for statistical machine translation", "author": ["Liu et al.2006] Yang Liu", "Qun Liu", "Shouxun Lin"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association", "citeRegEx": "Liu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Effective Approaches to Attention-based Neural Machine Translation", "author": ["Luong et al.2015a] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the Rare Word Problem in Neural Machine Translation", "author": ["Luong et al.2015b] Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computa-", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Feature Forest Models for Probabilistic HPSG Parsing", "author": ["Miyao", "Tsujii2008] Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "Computational Linguistics,", "citeRegEx": "Miyao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Miyao et al\\.", "year": 2008}, {"title": "On the elements of an accurate treeto-string machine translation system", "author": ["Neubig", "Duh2014] Graham Neubig", "Kevin Duh"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Neubig et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2014}, {"title": "Pointwise Prediction for Robust, Adaptable Japanese Morphological Analysis", "author": ["Neubig et al.2011] Graham Neubig", "Yosuke Nakata", "Shinsuke Mori"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational", "citeRegEx": "Neubig et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2011}, {"title": "Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015", "author": ["Neubig et al.2015] Graham Neubig", "Makoto Morishita", "Satoshi Nakamura"], "venue": "In Proceedings of the 2nd Workshop on Asian Translation", "citeRegEx": "Neubig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2015}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguis-", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Understanding the exploding gradient problem", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Overcoming the curse of sentence length for neural machine translation using automatic segmentation", "author": ["Dzmitry Bahdanau", "Bart van Merrienboer", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pouget.Abadie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pouget.Abadie et al\\.", "year": 2014}, {"title": "Syntactic Theory: A Formal Introduction. Center for the Study of Language and Information, Stanford, 2nd edition", "author": ["Sag et al.2003] Ivan A. Sag", "Thomas Wasow", "Emily Bender"], "venue": null, "citeRegEx": "Sag et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sag et al\\.", "year": 2003}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved Semantic Representations From Tree-Structured Long ShortTerm Memory Networks", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Compu-", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A syntax-based statistical translation model", "author": ["Yamada", "Knight2001] Kenji Yamada", "Kevin Knight"], "venue": "In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Yamada et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2001}, {"title": "Evaluating Neural Machine Translation in English-Japanese Task", "author": ["Zhongyuan Zhu"], "venue": "In Proceedings of the 2nd Workshop on Asian Translation", "citeRegEx": "Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Zhu.", "year": 2015}, {"title": "Multi-Source Neural Translation", "author": ["Zoph", "Knight2016] Barret Zoph", "Kevin Knight"], "venue": "In Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 26, "context": "In the Encoder-Decoder model (Cho et al., 2014b; Sutskever et al., 2014), a Recurrent Neural Network (RNN) called the encoder reads the whole sequence of source words to produce a fixedlength vector, and then another RNN called the decoder generates the target words from the vector.", "startOffset": 29, "endOffset": 72}, {"referenceID": 0, "context": "The Encoder-Decoder model has been extended with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a), which allows the model to jointly learn the soft alignment between the source language and the target language.", "startOffset": 72, "endOffset": 116}, {"referenceID": 14, "context": "In SMT, it is known that incorporating syntactic constituents of the source language into the models improves word alignment (Yamada and Knight, 2001) and translation accuracy (Liu et al., 2006; Neubig and Duh, 2014).", "startOffset": 176, "endOffset": 216}, {"referenceID": 26, "context": "NMT is an end-to-end approach to data-driven machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 65, "endOffset": 144}, {"referenceID": 0, "context": "NMT is an end-to-end approach to data-driven machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 65, "endOffset": 144}, {"referenceID": 6, "context": "We employ Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) in place of vanilla RNN units.", "startOffset": 46, "endOffset": 99}, {"referenceID": 0, "context": "The NMT models with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a) have been proposed to softly align each decoder state with the encoder states.", "startOffset": 43, "endOffset": 87}, {"referenceID": 0, "context": "The NMT models with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a) have been proposed to softly align each decoder state with the encoder states. The attention mechanism allows the NMT models to explicitly quantify how much each encoder state contributes to the word prediction at each time step. In the attentional NMT model in Luong et al. (2015a), at the j-th step of the decoder process, the attention score \u03b1j(i) between the i-th source hidden unit hi and the j-th target hidden unit sj is calculated as follows:", "startOffset": 44, "endOffset": 371}, {"referenceID": 25, "context": "In Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003), a sentence is composed of multiple phrase units and represented as a binary tree as shown in Figure 1.", "startOffset": 47, "endOffset": 65}, {"referenceID": 27, "context": "Each non-leaf node is also represented with an LSTM unit, and we employ Tree-LSTM (Tai et al., 2015) to calculate the LSTM unit of the parent node which has two child LSTM units.", "startOffset": 82, "endOffset": 100}, {"referenceID": 27, "context": "Our proposed tree-based encoder is a natural extension of the conventional sequential encoder, since Tree-LSTM is a generalization of chainstructured LSTM (Tai et al., 2015).", "startOffset": 155, "endOffset": 173}, {"referenceID": 15, "context": "Luong et al. (2015a) showed that the input-feeding approach improves BLEU scores.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "In order to reduce the training cost of the NMT models at the softmax layer, we employ BlackOut (Ji et al., 2016), a sampling-based approximation method.", "startOffset": 96, "endOffset": 113}, {"referenceID": 17, "context": "The negative samples are drawn from the unigram distribution raised to the power \u03b2 \u2208 [0, 1] (Mikolov et al., 2013).", "startOffset": 92, "endOffset": 114}, {"referenceID": 10, "context": "The advantages of Blackout over the other methods are discussed in Ji et al. (2016). Note that BlackOut can be used as the original softmax once the training is finished.", "startOffset": 67, "endOffset": 84}, {"referenceID": 20, "context": ", Japanese, we used KyTea (Neubig et al., 2011), a Japanese segmentation tool, and performed the pre-processing steps recommended in WAT\u201915.", "startOffset": 26, "endOffset": 47}, {"referenceID": 26, "context": "1 Following Zhu (2015), we extracted the first 1.", "startOffset": 12, "endOffset": 23}, {"referenceID": 23, "context": "0 to avoid exploding gradient problems (Pascanu et al., 2012).", "startOffset": 39, "endOffset": 61}, {"referenceID": 10, "context": "4 as recommended by Ji et al. (2016). Following J\u00f3zefowicz et al.", "startOffset": 20, "endOffset": 37}, {"referenceID": 10, "context": "4 as recommended by Ji et al. (2016). Following J\u00f3zefowicz et al. (2015), we initialize the forget gate biases of LSTM and Tree-LSTM with 1.", "startOffset": 20, "endOffset": 73}, {"referenceID": 24, "context": "The score becomes smaller as the target sentence becomes longer, and thus the simple beam search does not work well when decoding a long sentence (Cho et al., 2014a; Pouget-Abadie et al., 2014).", "startOffset": 146, "endOffset": 193}, {"referenceID": 2, "context": "The score becomes smaller as the target sentence becomes longer, and thus the simple beam search does not work well when decoding a long sentence (Cho et al., 2014a; Pouget-Abadie et al., 2014). In our preliminary experiments, the beam search with the length normalization in Cho et al. (2014a) was not effective in English-to-Japanese translation.", "startOffset": 147, "endOffset": 295}, {"referenceID": 2, "context": "The score becomes smaller as the target sentence becomes longer, and thus the simple beam search does not work well when decoding a long sentence (Cho et al., 2014a; Pouget-Abadie et al., 2014). In our preliminary experiments, the beam search with the length normalization in Cho et al. (2014a) was not effective in English-to-Japanese translation. The method in Pouget-Abadie et al. (2014) needs to estimate the conditional probability p(x|y) using another NMT model and thus is not suitable for our work.", "startOffset": 147, "endOffset": 391}, {"referenceID": 9, "context": "We evaluated the models by two automatic evaluation metrics, RIBES (Isozaki et al., 2010) and BLEU (Papineni et al.", "startOffset": 67, "endOffset": 89}, {"referenceID": 22, "context": ", 2010) and BLEU (Papineni et al., 2002) following WAT\u201915.", "startOffset": 17, "endOffset": 40}, {"referenceID": 9, "context": "We evaluated the models by two automatic evaluation metrics, RIBES (Isozaki et al., 2010) and BLEU (Papineni et al., 2002) following WAT\u201915. We used the KyTea-based evaluation script for the translation results.6 The RIBES score is a metric based on rank correlation coefficients with word precision, and the BLEU score is based on n-gram word precision and a Brevity Penalty (BP) for outputs shorter than the references. RIBES is known to have stronger correlation with human judgements than BLEU in translation between English and Japanese as discussed in Isozaki et al. (2010).", "startOffset": 68, "endOffset": 580}, {"referenceID": 26, "context": "As to the results of the ANMT model, reversing the word order in the input sentence decreases the scores in English-to-Japanese translation, which contrasts with the results of other language pairs reported in previous work (Sutskever et al., 2014; Luong et al., 2015a).", "startOffset": 224, "endOffset": 269}, {"referenceID": 0, "context": "Comparison with the NMT models The model of Zhu (2015) is an ANMT model (Bahdanau et al., 2015) with a bi-directional LSTM encoder, and uses 1024-dimensional hidden units and 1000-", "startOffset": 72, "endOffset": 95}, {"referenceID": 28, "context": "Comparison with the NMT models The model of Zhu (2015) is an ANMT model (Bahdanau et al.", "startOffset": 44, "endOffset": 55}, {"referenceID": 29, "context": "We found two sentences which ends without eos with d = 512, and then we decoded it again with the beam size of 1000 following Zhu (2015). Our ensemble model yields a METEOR (Denkowski and Lavie, 2014) score of 53.", "startOffset": 126, "endOffset": 137}, {"referenceID": 29, "context": "ANMT with LSTMs (Zhu, 2015) 79.", "startOffset": 16, "endOffset": 27}, {"referenceID": 13, "context": "21 3 pre-reordered ensembles ANMT with GRUs (Lee et al., 2015) 81.", "startOffset": 44, "endOffset": 62}, {"referenceID": 21, "context": "58 + ANMT Rerank (Neubig et al., 2015) 81.", "startOffset": 17, "endOffset": 38}, {"referenceID": 13, "context": "The model of Lee et al. (2015) is also an ANMT model with a bidirectional Gated Recurrent Unit (GRU) encoder, and uses 1000-dimensional hidden units and 200dimensional word embeddings.", "startOffset": 13, "endOffset": 31}, {"referenceID": 13, "context": "The model of Lee et al. (2015) is also an ANMT model with a bidirectional Gated Recurrent Unit (GRU) encoder, and uses 1000-dimensional hidden units and 200dimensional word embeddings. Both models are sequential ANMT models. Our single proposed model with d = 512 outperforms the best result of Zhu (2015)\u2019s end-to-end NMT model with ensemble and unknown replacement by +1.", "startOffset": 13, "endOffset": 306}, {"referenceID": 13, "context": "The model of Lee et al. (2015) is also an ANMT model with a bidirectional Gated Recurrent Unit (GRU) encoder, and uses 1000-dimensional hidden units and 200dimensional word embeddings. Both models are sequential ANMT models. Our single proposed model with d = 512 outperforms the best result of Zhu (2015)\u2019s end-to-end NMT model with ensemble and unknown replacement by +1.19 RIBES and by +0.17 BLEU scores. Our ensemble model shows better performance, in both RIBES and BLEU scores, than that of Zhu (2015)\u2019s best system which is a hybrid of the ANMT and SMT models by +1.", "startOffset": 13, "endOffset": 508}, {"referenceID": 13, "context": "The model of Lee et al. (2015) is also an ANMT model with a bidirectional Gated Recurrent Unit (GRU) encoder, and uses 1000-dimensional hidden units and 200dimensional word embeddings. Both models are sequential ANMT models. Our single proposed model with d = 512 outperforms the best result of Zhu (2015)\u2019s end-to-end NMT model with ensemble and unknown replacement by +1.19 RIBES and by +0.17 BLEU scores. Our ensemble model shows better performance, in both RIBES and BLEU scores, than that of Zhu (2015)\u2019s best system which is a hybrid of the ANMT and SMT models by +1.54 RIBES and by +0.74 BLEU scores and Lee et al. (2015)\u2019s ANMT system with special character-based decoding by +1.", "startOffset": 13, "endOffset": 629}, {"referenceID": 19, "context": "The best model in WAT\u201915 is Neubig et al. (2015)\u2019s treeto-string SMT model enhanced with reranking by ANMT using a bi-directional LSTM encoder.", "startOffset": 28, "endOffset": 49}, {"referenceID": 19, "context": "The best model in WAT\u201915 is Neubig et al. (2015)\u2019s treeto-string SMT model enhanced with reranking by ANMT using a bi-directional LSTM encoder. Our proposed end-to-end NMT model compares favorably with Neubig et al. (2015).", "startOffset": 28, "endOffset": 223}, {"referenceID": 26, "context": ", 2014b) or LSTMs (Sutskever et al., 2014).", "startOffset": 18, "endOffset": 42}, {"referenceID": 0, "context": "The attention mechanism (Bahdanau et al., 2015) has promoted NMT onto the next stage.", "startOffset": 24, "endOffset": 47}, {"referenceID": 1, "context": "Subsequently, several ANMT models have been proposed (Cheng et al., 2016; Cohn et al., 2016); however, each model is based on the existing sequential attentional models and does not focus on a syntactic structure of languages.", "startOffset": 53, "endOffset": 92}, {"referenceID": 4, "context": "Subsequently, several ANMT models have been proposed (Cheng et al., 2016; Cohn et al., 2016); however, each model is based on the existing sequential attentional models and does not focus on a syntactic structure of languages.", "startOffset": 53, "endOffset": 92}, {"referenceID": 0, "context": "The Encoder-Decoder model can be seen as an extension of their model, and it replaces the CNNs with RNNs using GRUs (Cho et al., 2014b) or LSTMs (Sutskever et al., 2014). Sutskever et al. (2014) have shown that making the input sequences reversed is effective in a French-to-English translation task, and the technique has also proven effective in translation tasks between other European language pairs (Luong et al.", "startOffset": 117, "endOffset": 195}, {"referenceID": 0, "context": "The Encoder-Decoder model can be seen as an extension of their model, and it replaces the CNNs with RNNs using GRUs (Cho et al., 2014b) or LSTMs (Sutskever et al., 2014). Sutskever et al. (2014) have shown that making the input sequences reversed is effective in a French-to-English translation task, and the technique has also proven effective in translation tasks between other European language pairs (Luong et al., 2015a). All of the NMT models mentioned above are based on sequential encoders. To incorporate structural information into the NMT models, Cho et al. (2014a) proposed to jointly learn structures inherent in source-side languages but did not report improvement of translation performance.", "startOffset": 117, "endOffset": 577}, {"referenceID": 0, "context": "The attention mechanism (Bahdanau et al., 2015) has promoted NMT onto the next stage. It enables the NMT models to translate while aligning the target with the source. Luong et al. (2015a) refined the attention model so that it can dynamically focus on local windows rather than the entire sentence.", "startOffset": 25, "endOffset": 189}], "year": 2016, "abstractText": "Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information. We propose a novel end-to-end syntactic NMT model, extending a sequenceto-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT\u201915 Englishto-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.", "creator": "LaTeX with hyperref package"}}}