{"id": "1603.06076", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Improving Hypernymy Detection with an Integrated Path-based and Distributional Method", "abstract": "Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complementary approaches. Distributional methods, whose supervised variants are the current best performers, and path-based methods who receive less research attention. We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural network, and achieve results comparable to distributional methods. We then extend the approach to integrate both path-based and distributional signals, significantly improving the state-of-the-art on this task.", "histories": [["v1", "Sat, 19 Mar 2016 10:09:53 GMT  (250kb,D)", "http://arxiv.org/abs/1603.06076v1", null], ["v2", "Tue, 24 May 2016 16:07:40 GMT  (200kb,D)", "http://arxiv.org/abs/1603.06076v2", "ACL 2016"], ["v3", "Tue, 7 Jun 2016 10:09:43 GMT  (200kb,D)", "http://arxiv.org/abs/1603.06076v3", "ACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vered shwartz", "yoav goldberg", "ido dagan"], "accepted": true, "id": "1603.06076"}, "pdf": {"name": "1603.06076.pdf", "metadata": {"source": "CRF", "title": "Improving Hypernymy Detection with an Integrated Path-based and Distributional Method", "authors": ["Vered Shwartz", "Yoav Goldberg", "Ido Dagan"], "emails": ["vered1986@gmail.com", "yoav.goldberg@gmail.com", "dagan@cs.biu.ac.il"], "sections": [{"heading": null, "text": "Detection of hypernymic relationships is a key task in the NLP, which is addressed in the literature with two complementary approaches: distribution methods, whose monitored variants currently perform best, and path-based methods, which receive less research attention. We propose an improved path-based algorithm, in which the dependency paths are coded using a recursive neural network, achieving results comparable to distribution methods. Subsequently, we expand the approach to integrate both path-based and distribution-based signals, significantly improving the state of the art in this task."}, {"heading": "1 Introduction", "text": "This year, the time has come for us to be able to find a solution, to be able to find a solution, to be able to find a solution, to be able to find a solution, to be able to find a solution, to be able to find a solution, to be able to find a solution, to be able to find a solution, to be able to find a solution, to be able to find a solution, to be able to find a solution, to be able to find a solution, to be able to find a solution, to be able to find a solution, to find a solution."}, {"heading": "2 Background", "text": "We present the two most important approaches to detecting hypernymia: distribution-based (Section 2.1) and path-based (Section 2.2), and then discuss the recent use of recurrent neural networks in the related task of classifying relationships (Section 2.3)."}, {"heading": "2.1 Distributional Methods", "text": "In these methods, the decision whether y is a hypernym for x is based on the distribution representations of the two terms, i.e. the contexts in which each term occurs in the body separately. Previous methods developed uncontrolled measures for hypernymia, starting with symmetrical similarity scales (Lin, 1998), followed by benchmarks based on the hypothesis of distributional inclusion (Weeds and Weir, 2003; Kotlerman et al., 2010). This hypothesis states that the connections of a hyponym are to a large extent to be included in the contexts of its hypernym. Recent work (Santus et al., 2014; Rimell, 2014) introduces new measures based on the assumption that the most typical linguistic contexts of a hypernym are less informative than those of its hypernymes. More recently, the focus of the distribution concept has been shifted to ~ ~ Baroni in order to represent a superordinate pair."}, {"heading": "2.2 Path-based Methods", "text": "Another approach to detecting hypernymia between a pair of terms (x, y) looks at lexicosyntactic paths linking the common occurrence of x and y in a large corpus. Automatic acquisition of hypernyms from free text, based on such paths, was first proposed by Hearst (1992), who identified a small set of lexico-syntactic paths indicating hypernymy relationships (e.g. Y such as X, X, and other Y. In a later paper, Snow et al. (2004) learned to recognize hypernymia. Instead of looking for specific paths indicating hypernymia, they represent each (x, y) term pair as the multiplicity of dependency paths linking x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x based on these paths."}, {"heading": "2.3 RNNs for Relation Classification", "text": "The classification of relationships is a related task, the aim of which is to classify the relationship expressed between two target concepts in a particular sentence, to one of the predefined relationship classes. To illustrate this, consider the following sentence from the data set on classification of relationships SemEval-2010 (Hendrickx et al., 2009): \"The [apples] e1 are in the [shopping cart] e2.\" Here is the relationship between the target units content containers (e1, e2). The shortest dependence paths between the target units proved to be informative for this task (Fundel et al., 2007). Recently, deep learning techniques have shown good performance in collecting indicative information in such paths. In particular, several papers show improved performance using recursive neural networks (RNN) that process a dependency path between my target units edge by edge."}, {"heading": "3 LSTM-based Hypernymy Detection", "text": "We present an LSTM-based method for detecting hypernymia. We first focus on improving the path representation (Section 3.1) and then integrate distribution signals into our network, creating a combined method (Section 3.2)."}, {"heading": "3.1 Path-based Network", "text": "Similarly, we represent each dependency path as a sequence of edges leading from x to y into the dependency network. Each edge contains the problem and part of the source node's cursor, the dependency etiquette, and the edge direction between two subsequent nodes. For readability, we label each edge as lemma / POS / dep by placing the directional marks between two subsequent edges. See Figure 1 for illustration, and not treating an entire dependency path as a single attribute, we encode the sequence of edges with a long-term short-term memory (LSTM) network. The vectors obtained for the different paths of a given (x, y) pair are pooled, and the resulting vector is used for classification. Figure 2 represents the entire network structure described below."}, {"heading": "3.2 Integrated Network", "text": "The network introduced in Section 3.1 classifies each (x-, y-) term pair based on the paths that connect x and y in the corpus. Our goal was to improve the previous path-based methods for hypernymy detection, and we show in Section 6 that our network actually exceeds them. However, since path-based and distribution-based methods are considered complementary, we present a simple way to integrate distribution features into the network, resulting in improved performance. We expanded the network to include distribution information for each term. Inspired by the supervised distribution method (Baroni et al., 2012), we simply associate x and y word embeddings with the (x-, y-) feature vector, redefining ~ vxy: ~ vxy = [~ vwx, ~ vpaths (x, y), ~ vpaths (x), the path (x), y, and the wy attributes respectively."}, {"heading": "4 Dataset", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Creating Instances", "text": "Neural networks typically require a large amount of training data, while existing hypernymy datasets, such as BLESS (Baroni and Lenci, 2011), are relatively small. Therefore, we followed the common methodology of creating a dataset using remote supervision from knowledge resources (Snow et al., 2004; Riedel et al., 2013). Following Snow et al. (2004), who constructed their dataset on the basis of WordNet hypernymia and aimed to create a larger dataset, we extract hypernymy relationships from multiple resources: WordNet (Fellbaum et al., 1998), DBPedia et al. (Auer et al., 2007), Wikidata (Vrandec prior ic, 2012), and Yago (Suchanek et al., 2007). All cases in our dataset, both positive and negative, are pairs of terms directly linked to at least one of the resources of which are associated with thousands of hypernyness resources."}, {"heading": "4.2 Random and Lexical Dataset Splits", "text": "As stated by Levy et al. (2015), we tend to do \"lexical memorization,\" i.e., instead of learning a relationship between the two terms, we usually learn an independent property of a single term in the pair: whether it is a \"prototypical hypernym\" or not. For example, if the training set contains pairs of terms such as (dog, animal) and (cow, animal), all of which are given as positive examples, the algorithm can learn that the animal classifies a new (x, animal) pair as positive, regardless of the relationship between x and animal."}, {"heading": "5 Baselines", "text": "We compare our method with several state-of-the-art methods for detecting hypernymia, as described in Section 2: path-based methods (Section 5.1) and distribution methods (Section 5.2). Due to different work using different data sets and corpora, we3The lexical split discards many pairs consisting of crossing terms, replicating the baselines instead of comparing them with the reported outcomes. We use the May 2015 Wikipedia dump as the underlying corpus of all methods and analyze it with spaCy4. We perform model selection using the validation set to adjust the hyperparameters of each method.5 The best hyperparameters are described in Table 3."}, {"heading": "5.1 Path-based Methods", "text": "Like Snow et al. (2004), we add paths with \"satellite margins,\" i.e. single words not already included in the dependency path that are connected to either X or Y, allowing paths like Y like X. The number of unique paths was 324,578. We use the \u03c72 function selection to maintain only the 100,000 most informative paths, and replace edges with their part-of-speech tags and wild cards. We generate the powerset of all possible generalizations, including the original paths. See Table 4 for examples. The number of attributes after generalization increased to 2,093,220. Similar to the first logic, we apply most generalizations."}, {"heading": "5.2 Distributional Methods", "text": "Unmonitored SLQS (Santus et al., 2014) is an entropy-based measure of hypernymia detection that reportedly exceeds previous unmonitored methods (Weeds and Weir, 2003; Kotlerman et al., 2010). Following the initial work, we set the number of most closely related contexts of each term to N = 50. We use the validation quantity to adjust the threshold for classifying a pair as positive. As our low results suggest, we have tried several state-of-the-art methods for unattended hypernymia detection: concatenation ~ x ~ y (Baroni et al., 2012), difference ~ y \u2212 ~ x (Roller et al., 2014; Fu et al., 2014; Weeds et al, this) selection (selection ~ x ~ y, selection et al.), difference ~ y \u2212 ~ x (emulsion) and emulsion (selection),"}, {"heading": "6 Results", "text": "Table 5 shows the performance values of our method and baselines. LSTM is our path-based recurrent neural network model (Section 3.1) and LSTM-Integrated is our combined method (Section 3.2). Comparison of path-based methods shows that generalization of paths improves memory while maintaining a similar level of precision, reevaluating the behavior of Nakashole et al. (2012). Our LSTM-based method outperforms both path-based baselines by significantly improving recall and with slightly less precision. The recall thrust is due to better path generalization as shown in Section 7.1. In terms of distribution methods, the unattended SLQS baseline has performed worse on our dataset. The low precision is due to its inability to differentiate between hypernyms and meryms widely used in our dataset."}, {"heading": "7 Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Qualitative Analysis of Learned Paths", "text": "We analyze the ability of our method to generalize about path structures by comparing prominent indicative paths that are clearly learned from each of the pathbased methods. We do this by finding high-scoring paths that have contributed to the classification of truly positive pairs in the dataset. In path-based baselines, these are the most highly weighted traits learned by the logistic regression classifier. In the LSTM-based method, it is less easy to identify the most indicative paths. We evaluate the contribution of a particular path to the classification by considering it to be the only path that appeared for the term pair, and calculate its TRUE label path based on the class distribution: softmax (W \u00b7 ~ vxy) [1], by defining a specific path to the classification by considering it as the only path that appeared for the term pair, and calculating it."}, {"heading": "7.2 Error Analysis", "text": "Incorrect Positive We categorized the false positive pairs on the random split according to the relationship that exists between the two pairs of terms in the re-sources used to construct the dataset. We grouped several semantic relationships between different resources and broad categories, e.g. synonyms also include aliases and Wikipedia redirection. Table 7 shows the distribution of semantic relationships between false positive pairs (y is a hyponym of x). Examination of a sample of these pairs suggests that they are usually synonyms, i.e. it is not clear whether one term is truly more general than the other. For example, my fiction is commented on in WordNet as a hypernym of a story, while our method classifies fiction as their hyponyms. One possible future research direction could be quite simply to expand our network pairs to include several simultaneously classifiable pairs of terms."}, {"heading": "8 Conclusion", "text": "We presented a neural network-based method for detecting hypernymia. First, we focused on improving path representation using LSTM and proposed a path-based model that is significantly better than previous path-based methods and consistent with previously superior distribution methods. We demonstrated that the increase in recall is the result of generalizing semantically similar paths, as opposed to earlier methods that either did not generalize or overgeneralize. Then, we expanded our network by integrating distribution signals, which resulted in an improvement of additional 14F1 points, and demonstrated that the path-based and distribution-based approaches are indeed complementary. Finally, our architecture seems straightforward to apply to multi-class classification, which could be used in future work to classify termpairs for multiple semantic relationships."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer et al.2007] S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "How we blessed distributional semantic evaluation", "author": ["Baroni", "Lenci2011] Marco Baroni", "Alessandro Lenci"], "venue": "In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,", "citeRegEx": "Baroni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2011}, {"title": "Entailment above the word level in distributional semantics", "author": ["Baroni et al.2012] Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan"], "venue": "In EACL,", "citeRegEx": "Baroni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "Evolutionary algorithms for definition extraction", "author": ["Borg et al.2009] Claudia Borg", "Mike Rosner", "Gordon Pace"], "venue": "In Proceedings of the 1st Workshop on Definition Extraction,", "citeRegEx": "Borg et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Borg et al\\.", "year": 2009}, {"title": "Toward an architecture for never-ending language learning", "author": ["Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R Hruschka Jr.", "Tom M Mitchell"], "venue": "In AAAI,", "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Learning semantic hierarchies via word embeddings", "author": ["Fu et al.2014] Ruiji Fu", "Jiang Guo", "Bing Qin", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "venue": "In ACL,", "citeRegEx": "Fu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2014}, {"title": "Relexrelation extraction using dependency parse", "author": ["Fundel et al.2007] Katrin Fundel", "Robert K\u00fcffner", "Ralf Zimmer"], "venue": "trees. Bioinformatics,", "citeRegEx": "Fundel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fundel et al\\.", "year": 2007}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["Marti A Hearst"], "venue": "In ACL,", "citeRegEx": "Hearst.,? \\Q1992\\E", "shortCiteRegEx": "Hearst.", "year": 1992}, {"title": "Semeval-2010 task 8: Multi-way classification", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Using hidden markov random fields to combine distributional and pattern-based word clustering", "author": ["Kaji", "Kitsuregawa2008] Nobuhiro Kaji", "Masaru Kitsuregawa"], "venue": "In COLING,", "citeRegEx": "Kaji et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kaji et al\\.", "year": 2008}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Directional distributional similarity for lexical inference", "author": ["Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet"], "venue": null, "citeRegEx": "Kotlerman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2010}, {"title": "A semi-supervised method to learn and construct taxonomies using the web", "author": ["Kozareva", "Hovy2010] Zornitsa Kozareva", "Eduard Hovy"], "venue": "In EMNLP,", "citeRegEx": "Kozareva et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kozareva et al\\.", "year": 2010}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Levy et al.2015] Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "An informationtheoretic definition of similarity", "author": ["Dekang Lin"], "venue": "In ICML,", "citeRegEx": "Lin.,? \\Q1998\\E", "shortCiteRegEx": "Lin.", "year": 1998}, {"title": "A dependency-based neural network for relation classification", "author": ["Liu et al.2015] Yang Liu", "Furu Wei", "Sujian Li", "Heng Ji", "Ming Zhou", "Houfeng Wang"], "venue": "arXiv preprint arXiv:1507.04646", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Gregory S Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Integrating pattern-based and distributional similarity methods for lexical entailment acquisition", "author": ["Ido Dagan", "Maayan Geffet"], "venue": "In COLING and ACL,", "citeRegEx": "Mirkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mirkin et al\\.", "year": 2006}, {"title": "Patty: a taxonomy of relational patterns with semantic types", "author": ["Gerhard Weikum", "Fabian Suchanek"], "venue": "In EMNLP and CoNLL,", "citeRegEx": "Nakashole et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2012}, {"title": "Learning word-class lattices for definition and hypernym extraction", "author": ["Navigli", "Velardi2010] Roberto Navigli", "Paola Velardi"], "venue": "In ACL,", "citeRegEx": "Navigli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2010}, {"title": "Combining neural networks and log-linear models to improve relation extraction", "author": ["Nguyen", "Grishman2015] Thien Huu Nguyen", "Ralph Grishman"], "venue": "arXiv preprint arXiv:1511.05926", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Adding semantics to data-driven paraphrasing", "author": ["Johan Bos", "Malvina Nissim", "Charley Beller", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": null, "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M Marlin"], "venue": null, "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Distributional lexical entailment by topic coherence", "author": ["Laura Rimell"], "venue": "In EACL,", "citeRegEx": "Rimell.,? \\Q2014\\E", "shortCiteRegEx": "Rimell.", "year": 2014}, {"title": "Inclusive yet selective: Supervised distributional hypernymy detection", "author": ["Katrin Erk", "Gemma Boleda"], "venue": "In COLING,", "citeRegEx": "Roller et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roller et al\\.", "year": 2014}, {"title": "Chasing hypernyms in vector spaces with entropy", "author": ["Santus et al.2014] Enrico Santus", "Alessandro Lenci", "Qin Lu", "Sabine Schulte Im Walde"], "venue": "In EACL,", "citeRegEx": "Santus et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Learning to exploit structured resources for lexical inference", "author": ["Omer Levy", "Ido Dagan", "Jacob Goldberger"], "venue": "In CoNLL,", "citeRegEx": "Shwartz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shwartz et al\\.", "year": 2015}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["Snow et al.2004] Rion Snow", "Daniel Jurafsky", "Andrew Y Ng"], "venue": null, "citeRegEx": "Snow et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2004}, {"title": "Semantic taxonomy induction from heterogenous evidence", "author": ["Snow et al.2006] Rion Snow", "Daniel Jurafsky", "Andrew Y Ng"], "venue": "In ACL,", "citeRegEx": "Snow et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2006}, {"title": "Yago: a core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In WWW,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "A general framework for distributional similarity", "author": ["Weeds", "Weir2003] Julie Weeds", "David Weir"], "venue": "In EMLP,", "citeRegEx": "Weeds et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2003}, {"title": "Learning to distinguish hypernyms and co-hyponyms", "author": ["Weeds et al.2014] Julie Weeds", "Daoud Clarke", "Jeremy Reffin", "David Weir", "Bill Keller"], "venue": "In COLING,", "citeRegEx": "Weeds et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Improved relation classification by deep recurrent neural networks with data augmentation", "author": ["Xu et al.2016] Yan Xu", "Ran Jia", "Lili Mou", "Ge Li", "Yunchuan Chen", "Yangyang Lu", "Zhi Jin"], "venue": "arXiv preprint arXiv:1601.03651", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "Lately, with the popularity of word embeddings (Mikolov et al., 2013), most focus has shifted towards supervised distributional methods, in which each (x, y) term-pair is represented using some combination of the terms\u2019 embedding vectors.", "startOffset": 47, "endOffset": 69}, {"referenceID": 7, "context": "Hearst (1992) identified a small set of frequent paths that indicate hypernymy, e.", "startOffset": 0, "endOffset": 14}, {"referenceID": 7, "context": "Hearst (1992) identified a small set of frequent paths that indicate hypernymy, e.g. Y such as X. Snow et al. (2004) represented each (x, y) term-pair as the multiset of dependency paths connecting their co-occurrences in a corpus, and trained a classifier to predict hypernymy, based on these features.", "startOffset": 0, "endOffset": 117}, {"referenceID": 19, "context": "tags or with wild cards, as done in PATTY (Nakashole et al., 2012).", "startOffset": 42, "endOffset": 66}, {"referenceID": 18, "context": "Though these approaches seem complementary, there has been rather little work on integrating them (Mirkin et al., 2006; Kaji and Kitsuregawa, 2008).", "startOffset": 98, "endOffset": 147}, {"referenceID": 15, "context": "Earlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al.", "startOffset": 107, "endOffset": 118}, {"referenceID": 12, "context": "Earlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010).", "startOffset": 206, "endOffset": 252}, {"referenceID": 27, "context": "More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the most typical linguistic contexts of a hypernym are less informative than those of its hyponyms.", "startOffset": 17, "endOffset": 52}, {"referenceID": 25, "context": "More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the most typical linguistic contexts of a hypernym are less informative than those of its hyponyms.", "startOffset": 17, "endOffset": 52}, {"referenceID": 2, "context": "Several methods are used to represent term-pairs as a combination of each term\u2019s embeddings vector: concatenation ~x\u2295~y (Baroni et al., 2012), difference ~y\u2212~x (Roller et al.", "startOffset": 120, "endOffset": 141}, {"referenceID": 26, "context": ", 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7 ~y.", "startOffset": 26, "endOffset": 84}, {"referenceID": 5, "context": ", 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7 ~y.", "startOffset": 26, "endOffset": 84}, {"referenceID": 33, "context": ", 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7 ~y.", "startOffset": 26, "endOffset": 84}, {"referenceID": 17, "context": "Based on neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al.", "startOffset": 32, "endOffset": 79}, {"referenceID": 23, "context": "Based on neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al.", "startOffset": 32, "endOffset": 79}, {"referenceID": 2, "context": ", 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014).", "startOffset": 64, "endOffset": 106}, {"referenceID": 26, "context": ", 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014).", "startOffset": 64, "endOffset": 106}, {"referenceID": 7, "context": "Automatic acquisition of hypernyms from free text, based on such paths, was first proposed by Hearst (1992), who identified a small set of lexico-syntactic paths that indicate hypernymy relations (e.", "startOffset": 94, "endOffset": 108}, {"referenceID": 29, "context": "In a later work, Snow et al. (2004) learned to detect hypernymy.", "startOffset": 17, "endOffset": 36}, {"referenceID": 30, "context": "\u2019s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al.", "startOffset": 72, "endOffset": 159}, {"referenceID": 4, "context": "\u2019s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al.", "startOffset": 72, "endOffset": 159}, {"referenceID": 24, "context": "\u2019s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al.", "startOffset": 72, "endOffset": 159}, {"referenceID": 3, "context": ", 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010).", "startOffset": 74, "endOffset": 120}, {"referenceID": 5, "context": "The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance.", "startOffset": 73, "endOffset": 87}, {"referenceID": 5, "context": "The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.\u2019s (2004) method were later used in tasks such as taxonomy construction (Snow et al.", "startOffset": 73, "endOffset": 154}, {"referenceID": 19, "context": "The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxonomy of term relations from free text.", "startOffset": 20, "endOffset": 44}, {"referenceID": 8, "context": "To illustrate, consider the following sentence, from the SemEval-2010 relation classification task dataset (Hendrickx et al., 2009): \u201cThe [apples]e1 are in the [basket]e2\u201d.", "startOffset": 107, "endOffset": 131}, {"referenceID": 6, "context": "The shortest dependency paths between the target entities were shown to be informative for this task (Fundel et al., 2007).", "startOffset": 101, "endOffset": 122}, {"referenceID": 16, "context": "Other papers suggest incorporating additional network architectures to further improve performance (Nguyen and Grishman, 2015; Liu et al., 2015).", "startOffset": 99, "endOffset": 144}, {"referenceID": 29, "context": "Like Snow et al. (2004), we added for each path, additional paths containing single daughters of x or y not already contained in the path, to include paths such as Such Y as X.", "startOffset": 5, "endOffset": 24}, {"referenceID": 23, "context": "We initialized the lemma embeddings with the 50-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014), trained on", "startOffset": 94, "endOffset": 119}, {"referenceID": 2, "context": "Inspired by the supervised distributional concatenation method (Baroni et al., 2012), we simply concatenate x and y word embeddings to the (x, y) feature vector, redefining ~ vxy:", "startOffset": 63, "endOffset": 84}, {"referenceID": 29, "context": "Therefore, we followed the common methodology of creating a dataset using distant supervision from knowledge resources (Snow et al., 2004; Riedel et al., 2013).", "startOffset": 119, "endOffset": 159}, {"referenceID": 24, "context": "Therefore, we followed the common methodology of creating a dataset using distant supervision from knowledge resources (Snow et al., 2004; Riedel et al., 2013).", "startOffset": 119, "endOffset": 159}, {"referenceID": 0, "context": "(2004), who constructed their dataset based on WordNet hypernymy, and aiming to create a larger dataset, we extract hypernymy relations from several resources: WordNet (Fellbaum, 1998), DBPedia (Auer et al., 2007), Wikidata (Vrande\u010di\u0107, 2012) and Yago (Suchanek et al.", "startOffset": 194, "endOffset": 213}, {"referenceID": 31, "context": ", 2007), Wikidata (Vrande\u010di\u0107, 2012) and Yago (Suchanek et al., 2007).", "startOffset": 45, "endOffset": 68}, {"referenceID": 23, "context": ", 2004; Riedel et al., 2013). Following Snow et al. (2004), who constructed their dataset based on WordNet hypernymy, and aiming to create a larger dataset, we extract hypernymy relations from several resources: WordNet (Fellbaum, 1998), DBPedia (Auer et al.", "startOffset": 8, "endOffset": 59}, {"referenceID": 28, "context": "To avoid including questionable relation types, we consider as denoting positive examples only indisputable hypernymy relations (Table 1), which we manually selected from the set of hypernymy indicating relations in Shwartz et al. (2015).", "startOffset": 216, "endOffset": 238}, {"referenceID": 29, "context": "Like Snow et al. (2004), we include only termpairs that have joint occurrences in the corpus, requiring at least two different dependency paths for each pair.", "startOffset": 5, "endOffset": 24}, {"referenceID": 14, "context": "As pointed out by Levy et al. (2015), supervised distributional lexical inference methods tend to perform \u201clexical memorization\u201d, i.", "startOffset": 18, "endOffset": 37}, {"referenceID": 14, "context": "As pointed out by Levy et al. (2015), supervised distributional lexical inference methods tend to perform \u201clexical memorization\u201d, i.e., instead of learning a relation between the two terms, they mostly learn an independent property of a single term in the pair: whether it is a \u201cprototypical hypernym\u201d or not. For instance, if the training set contains term-pairs such as (dog, animal), (cat, animal), and (cow, animal), all annotated as positive examples, the algorithm may learn that animal is a prototypical hypernym, classifying any new (x, animal) pair as positive, regardless of the relation between x and animal. Levy et al. (2015) suggested to split the train and test sets such that each will contain a distinct vocabulary (\u201clexical split\u201d), in order to prevent the model from overfitting by lexical memorization.", "startOffset": 18, "endOffset": 639}, {"referenceID": 14, "context": "We note that this differs from Levy et al. (2015), who split only the train and the test sets, and dedicated a subset of the train for validation.", "startOffset": 31, "endOffset": 50}, {"referenceID": 14, "context": "We note that this differs from Levy et al. (2015), who split only the train and the test sets, and dedicated a subset of the train for validation. We chose to deviate from Levy et al. (2015) because we noticed that when the validation set contains terms from the train set, the model is rewarded of lexical memorization when tuning the hyper-parameters, consequently yielding suboptimal performance on the lexically-distinct test set.", "startOffset": 31, "endOffset": 191}, {"referenceID": 27, "context": "5 Distributional SLQS (Santus et al., 2014) N = 50, threshold = 0.", "startOffset": 22, "endOffset": 43}, {"referenceID": 29, "context": "Like Snow et al. (2004), we add paths with \u201csatellite edges\u201d, i.", "startOffset": 5, "endOffset": 24}, {"referenceID": 19, "context": "Following PATTY\u2019s (Nakashole et al., 2012) approach to generalizing paths, we replace edges with their part-of-speech tags as well as with wild cards.", "startOffset": 18, "endOffset": 42}, {"referenceID": 27, "context": "Unsupervised SLQS (Santus et al., 2014) is an entropy-based measure for hypernymy detection, reported to outperform previous state-of-the-art unsupervised methods (Weeds and Weir, 2003; Kotlerman et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 12, "context": ", 2014) is an entropy-based measure for hypernymy detection, reported to outperform previous state-of-the-art unsupervised methods (Weeds and Weir, 2003; Kotlerman et al., 2010).", "startOffset": 131, "endOffset": 177}, {"referenceID": 2, "context": "Supervised To represent term-pairs with distributional features, we tried several state-of-the-art methods: concatenation ~x\u2295~y (Baroni et al., 2012), difference ~y\u2212~x (Roller et al.", "startOffset": 128, "endOffset": 149}, {"referenceID": 26, "context": ", 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7~y.", "startOffset": 26, "endOffset": 84}, {"referenceID": 5, "context": ", 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7~y.", "startOffset": 26, "endOffset": 84}, {"referenceID": 33, "context": ", 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7~y.", "startOffset": 26, "endOffset": 84}, {"referenceID": 17, "context": "We downloaded several pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014) of different sizes, and trained a number of classifiers: logistic regression, SVM, and SVM with RBF kernel, which was reported by Levy et al.", "startOffset": 45, "endOffset": 92}, {"referenceID": 23, "context": "We downloaded several pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014) of different sizes, and trained a number of classifiers: logistic regression, SVM, and SVM with RBF kernel, which was reported by Levy et al.", "startOffset": 45, "endOffset": 92}, {"referenceID": 1, "context": "Supervised To represent term-pairs with distributional features, we tried several state-of-the-art methods: concatenation ~x\u2295~y (Baroni et al., 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7~y. We downloaded several pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014) of different sizes, and trained a number of classifiers: logistic regression, SVM, and SVM with RBF kernel, which was reported by Levy et al. (2015) to perform best in this setting.", "startOffset": 129, "endOffset": 493}, {"referenceID": 19, "context": "Comparing the path-based methods shows that generalizing paths improves recall while maintaining similar levels of precision, reassessing the behavior found in Nakashole et al. (2012). Our LSTM-based method outperforms both path-based baselines by a significant improvement in recall and with slightly lower precision.", "startOffset": 160, "endOffset": 184}, {"referenceID": 27, "context": "Distributional SLQS (Santus et al., 2014) 0.", "startOffset": 20, "endOffset": 41}, {"referenceID": 14, "context": "We also reassess that indeed supervised distributional methods perform worse on a lexical split (Levy et al., 2015).", "startOffset": 96, "endOffset": 115}, {"referenceID": 22, "context": "A possible future research direction might be to quite simply extend our network to classify term-pairs simultaneously to multiple semantic relations, as in Pavlick et al. (2015). Such a multiclass model can hopefully better distinguish between these similar semantic relations.", "startOffset": 157, "endOffset": 179}], "year": 2016, "abstractText": "Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complementary approaches. Distributional methods, whose supervised variants are the current best performers, and path-based methods who receive less research attention. We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural network, and achieve results comparable to distributional methods. We then extend the approach to integrate both path-based and distributional signals, significantly improving the state-of-the-art on this task.", "creator": "LaTeX with hyperref package"}}}