{"id": "1603.06111", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "How Transferable are Neural Networks in NLP Applications?", "abstract": "Transfer learning is aimed to make use of valuable knowledge in a source domain to help the model performance in a target domain. It is particularly important to neural networks because neural models are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct a series of empirical studies and provide an illuminating picture on the transferability of neural networks in NLP.", "histories": [["v1", "Sat, 19 Mar 2016 16:38:31 GMT  (1061kb,D)", "http://arxiv.org/abs/1603.06111v1", null], ["v2", "Thu, 13 Oct 2016 07:45:31 GMT  (2337kb,D)", "http://arxiv.org/abs/1603.06111v2", "Accepted by EMNLP-16"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["lili mou", "zhao meng", "rui yan", "ge li", "yan xu", "lu zhang 0023", "zhi jin"], "accepted": true, "id": "1603.06111"}, "pdf": {"name": "1603.06111.pdf", "metadata": {"source": "CRF", "title": "How Transferable are Neural Networks in NLP Applications?", "authors": ["Lili Mou", "Zhao Meng", "Rui Yan", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "emails": ["doublepower.mou@gmail.com,", "zhaomeng.pku@outlook.com", "lige@sei.pku.edu.cn", "xuyan14@sei.pku.edu.cn", "zhanglu@sei.pku.edu.cn", "zhijin@sei.pku.edu.cn", "rui.yan.peking@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Datasets", "text": "In our study, we used three open datasets as follows. \u2022 Stanford Natural Language Inference (SNLI): a newly published large dataset containing more than 550,000 sentence pairs. \u2022 The task is to determine whether a sentence can be taken from another sentence (E), or whether the two sentences contradict each other (C). In addition, there is also a third target label that designates the two sentences as irrelevant, as neutral (N). \u2022 Sentences that include the compositional Knowl Edge (SICK): a small dataset with exactly the same classification goal as SNLI.6 \u2022 Microsoft Research Paraphrase Corpus (MSRP): a (small) dataset that includes the compositional Knowl Edge (SICK): a small dataset with exactly the same classification goal as SNLI.6 \u2022 Microsoft Research Paraphrase Corpus (MSRP): a (small) dataset that includes the compositional Knowl Edge (SICK)."}, {"heading": "3 The Neural Model and Settings", "text": "Since all of the above data sets can be considered a classification task via pairs of sentences, we can use a single neural model to solve the three problems in a uniform way. That is, the neural architecture is the same among different data sets, which makes it possible to study transfer learning, regardless of whether the tasks are semantically equivalent. Figure 1 shows the basic neural network in our study. We use the widely used Convolutionary Neural Network (CNN) to capture the semantics of a single sentence, but the evolution window size is 5. Then the vector representations of the two relationships are combined by concatenation and fed into a hidden layer before the softmax output."}, {"heading": "4 Transfer Methods", "text": "Since neural networks are usually trained incrementally with gradient parentage (or variant), it is easy to use gradient information in both source and target domains for optimization to achieve knowledge transfer. However, depending on how samples in source and target domains are planned, there are two basic approaches to neural network-based learning: \u2022 Parameter Initialization (INIT). INIT Proach first trains the network to S, and then directly uses the tuned parameters to initialize the network for T. In the INIT approach, we can fix the parameters in the target domain (Glorot et al), i.e., no training is performed on T. But if marked data is available in T, it would be better to finetune (1) the parameters such as Bowman (2015)."}, {"heading": "5 Results of Transferring by INIT", "text": "We first analyze how INIT behaves in NLP-based transfer learning. In addition to two different transfer scenarios for semantic kinship, as described in Section 2, we evaluated two other settings: (1) fine-tuning parameters 1 and (2) freezing parameters after transfer. Existing evidence shows that frozen parameters would affect performance in general (Peng et al., 2015), but they provide a more direct understanding of how transferable the features are (because the target domain optimization factor is excluded), so we included this setting in our experiments. Furthermore, we applied parameters layer by layer to answer our second research question. Note that the E \"H\" and E1H1O1 \"8 settings do not apply to SNLI because the output targets do not share the same meanings and numbers of the target classes."}, {"heading": "5.1 Overall Performance", "text": "Table 3 shows the main results of INIT. A quick observation is that the use of SNLI information significantly improves SICK performance, which is not surprising and is also reported in Bowman et al. (2015).For MSRP, however, there is almost no performance improvement, regardless of how we transfer the parameters. Although researchers in previous studies have drawn predominantly positive conclusions about transfer learning, if we carefully study Collobert and Weston (2008) we find a similar negative result as ours. In this paper, the authors report that transferring NER, POS, CHK and pre-trained word embeddings improves the SRL task by 1.91-3.90% (from 16.54-18.40% error rate), with the gain mainly due to word embeddings. In the settings that are pre-embedded, please refer to the heading of Table 3 for the 4, 2, 1 and \"symbols."}, {"heading": "E4 H2 O2 70.9 69.0", "text": "The above results are rather frustrating and indicate that NLP systems are not always QP systems. + b: in line with Bowman et al. (2015); + c: in line with Collobert and Weston (2008). Word embedding (common in NLP), NER, POS and CHK systems improve SRL accuracy by 0.04-0.21%. Also in the SNLI \u2192 MSRP experiment, the E1H1O2 setting leads to a deterioration of 0.2% (\u0445.5x std and not statistically significant)."}, {"heading": "5.2 Layer-by-Layer Analysis", "text": "The fact is that we are able to hide, and that we are able, we are going to be able, we are going to be able, we are going to be able, we are going to be able, we are going to be able, we are going to be able, we are going to be able, we are going to be able, we are going to be able, we are going to be able, we are going to be able, we are going to be able, we are going to be able, we are going to be able, we are going to be able, we are going to be able, we are going to be able. '"}, {"heading": "5.3 How does learning rate affect transfer?", "text": "Bowman et al. (2015) point out that after transmission, a high learning rate can damage the knowledge stored in the parameters; in their work, in addition to the parameters, they transmit information about the learning rate (AdaDelta) of S and T. Although the rule of thumb is to select all hyperparameters - including the learning rate - by validation, we are curious to know whether the above assumption is correct. Estimating a broad range of meaningful hyperparameters can ease the burden of model selection; it also provides evidence to better understand how transfer learning actually works. We map the learning curves of different learning rates \u03b1 in Figure 2 (SNLI \u2192 SICK, E1H1O2). (The figure does not apply a learning decline.) As we see, with a large learning rate such as \u03b1 = 0.3, accuracy increases quickly and reaches peaks in the 10th epoch. Training with a small learning rate (e.g. \u03b1 = 0.01) is a learning performance comparable to a large learning performance if we do not find a large learning rate when we find a large learning rate."}, {"heading": "5.4 When is it ready to transfer?", "text": "In the above experiments, we transfer the parameters when they have the highest validation performance to S. This is a simple and intuitive practice. However, we can imagine that the parameters that are well matched to the source data set may be too specific for it, i.e., the model exceeds S and therefore cannot fit T. Another advantage of early transfer lies in computational concerns. If we manage to transfer model parameters to S after one or a few epochs, we can save a lot of time, especially if S is large. We have therefore made efforts when the neural model is ready to be transferred. Figure 3a plots the learning curve of the SNLI source task. Accuracy increases significantly from epochs 1-5, it reaches a plateau, but still grows slowly until the 23rd epoch gives the highest validation accuracy. (Results related to the validation are not plotted."}, {"heading": "6 MULT, and its Combination with INIT", "text": "To answer RQ3, we will examine how multi-task learning works in transfer learning; we will also analyze the effect of the combination of MULT and INIT. In this section, we have applied the setting: Sharing embedding and hidden layers (referred to as E \u2665 H \u2665 O2), analogous to E1H1O2 in INIT. Note that sharing all parameters E \u2665 H \u2665 O \u2665 is not applicable to MSRP because of different output targets; therefore, we have not applied this setting. We have also tried the combination of MULT and INIT, i.e. we have used the pre-trained parameters on SNLI to initialize the multi-task training of SNLI and SICK / MSRP. This setting could be visually demonstrated by E1 \u2665 H1 \u2665 O2.In both MULT and MULT + INIT models, we have used the accuracy of SNLIT (0, 1)."}, {"heading": "7 Concluding Remarks", "text": "We conducted experiments with three sets of data showing that the transferability of neural NLP models depends on the semantic relationship between the source and target tasks. We analyzed the behavior of different neural layers: word embedding is transferable, but the results are similar (or worse than) word2vec; hidden layers are the most important transferable features that improve performance; and the initial layer is generally non-transferable. We also experimented with two transfer methods: Parameter Initialization (INIT) and Multi-Task Learning (MULT); they generally perform a similar performance in our experiment. Further results are bundled in Section 5 and not replicated.9 For aesthetic reasons, the most important results bundled in Section 1 are not uniform. Our study provides insights into the transferability of neural NLP models; the results also help to better understand neural traits."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Transfer learning is aimed to make use of valuable knowledge in a source domain to help the model performance in a target domain. It is particularly important to neural networks because neural models are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural networkbased transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct a series of empirical studies and provide an illuminating picture on the transferability of neural networks in NLP.1", "creator": "TeX"}}}