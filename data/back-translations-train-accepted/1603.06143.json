{"id": "1603.06143", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Neurally-Guided Procedural Models: Amortized Inference for Procedural Graphics Programs using Neural Networks", "abstract": "We present a deep learning approach for speeding up constrained procedural modeling. Probabilistic inference algorithms such as Sequential Monte Carlo (SMC) provide powerful tools for constraining procedural models, but they require many samples to produce desirable results. In this paper, we show how to create procedural models which learn how to satisfy constraints. We augment procedural models with neural networks: these networks control how the model makes random choices based on what output it has generated thus far. We call such a model a neurally-guided procedural model. As a pre-computation, we train these models on constraint-satisfying example outputs generated via SMC. They are then used as efficient importance samplers for SMC, generating high-quality results with very few samples. We evaluate our method on L-system-like models with image-based constraints. Given a desired quality threshold, neurally-guided models can generate satisfactory results up to 10x faster than unguided models.", "histories": [["v1", "Sat, 19 Mar 2016 20:58:47 GMT  (5959kb,D)", "http://arxiv.org/abs/1603.06143v1", null], ["v2", "Thu, 13 Oct 2016 20:10:09 GMT  (5956kb,AD)", "http://arxiv.org/abs/1603.06143v2", null]], "reviews": [], "SUBJECTS": "cs.GR cs.AI", "authors": ["daniel ritchie", "anna thomas", "pat hanrahan", "noah d goodman"], "accepted": true, "id": "1603.06143"}, "pdf": {"name": "1603.06143.pdf", "metadata": {"source": "META", "title": "Neurally-Guided Procedural Models:earning to Guide Procedural Models with Deep Neural Networks", "authors": ["Daniel Ritchie", "Anna Thomas", "Pat Hanrahan", "Noah D. Goodman"], "emails": ["ngoodman}@stanford.edu"], "sections": [{"heading": null, "text": "Probabilistic inference algorithms such as Sequential Monte Carlo (SMC) provide powerful tools for limiting procedural models, but they require a lot of sampling to achieve desirable outcomes. In this paper, we show how to build procedural models that learn how to meet limitations. We extend procedural models to include neural networks: these networks control how the model makes random decisions based on the performance it has performed to date. We call such a model a neural-driven procedural model. As a precalculation, we train these models on limitation-satisfying sample results generated via SMC. They are then used as efficient samplers of meaning for SMC and generate high-quality results with very few samples. We evaluate our method using L-system-like models with image-based limitations. Given a desired quality threshold, neural-driven models can generate satisfactory statistical outcomes to non-stabilizable 10x faster than non-stabilizable systems."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Work", "text": "In fact, we see ourselves as being able to create a new world, in which we see ourselves able to create a new world, in which we see ourselves able to create a new world, in which we are able to create a new world, in which we are able to create a new world, in which we are able to create a new world, in which we are able to create a new world, in which we are able to create a new world, in which we are able to create a new world, in which we are able to create a new world, in which we create a new world, in which we create a new world."}, {"heading": "3 Approach", "text": "In this section, we motivate and outline the process of creating, training and using neural process models. In this article, we present process models as probabilistic programs, i.e. programs that make random decisions and support conditional conclusions about their distribution of results [Goodman and Stuhlmu M\u00fcller 2014]."}, {"heading": "3.1 Motivation", "text": "We motivate our approach with a simple program chain that recursively generates a random sequence of linear segments that must necessarily match a target image. Figure 2a shows the text of this program together with samples generated from it (drawn black) against multiple target images (drawn gray). Chains generated by forward execution of the program do not match the targets, as forward samples ignore the limitations. Instead, we can generate limited samples using sequential Monte Carlo (SMC) [Ritchie et al. 2015b]. SMC generates multiple samples or particles in parallel by scanning them anew at each step of the program to favor restrictive partial results, resulting in definitive chains that correspond more closely to the targets. However, the algorithm requires many particles - and thus significant functional chains (pos, ang) {var newang = gang + aussian (0) (GTarsian, + 8);"}, {"heading": "SMC", "text": "In fact, most of us are able to play by the rules. (...) It's not like they play by the rules. (...) It's like they play by the rules. (...) It's like they play by the rules. (...) It's like they play by the rules. (...) It's like they play by the rules. (...) It's like they play by the rules. (...) It's like they play by the rules. (...)"}, {"heading": "3.2 System Overview", "text": "Figure 3 shows a high-level overview of our workflow for defining, training and using neural process models. It consists of the following steps: Transform The procedural model is first transformed by inserting a neural network into the program text for each random selection and transforming continuous random selections into mixture distributions (Figure 3a-b).The network receives the constraint as input (e.g. a target image) and all previously made random selections (shown in Figure 3a-b) and issues the parameters for the selection (e.g. Gaussian means, variances and mixing weights).We perform this transformation manually; it could be automated by source-throw resource compilation.The neural networks can capture several different constraints, but an appropriate architecture for them depends on the generative paradigm and the output domain of the procedural model (e.g. images, 3D models, we present some architecture, etc.)."}, {"heading": "4 Mathematical Foundations", "text": "Having outlined our approach, we now formally define neurallyx-led process models. For our purposes, one process model is a generative probability model of the following form: PM (x) = x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x x-x"}, {"heading": "Convolve + Downsample", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Convolve +", "text": "DownsampleCurrent Partial Output (50x50) Target Image FeaturesPartial Output FeaturesLocal State FeaturesFigure 4: Neural Network Architecture for Image-Matching Procedural Model. The network uses a multi-layer perceptron that uses a vector of attributes as input and prints the parameters for a random probability distribution. The input attributes come from three sources. Local State Features are the arguments for the function in which the random selection takes place. Target Image Features come from 3x3 pixel windows of the target image, which are extracted in multiple resolutions around the current position of the process model. Partial Output Features are analog windows that are extracted from the partial image that the model has generated. All of these attributes can be calculated from the target image and the sequence of random decisions made so far."}, {"heading": "5 Implementation", "text": "In this section we describe an implementation of neural accumulative procedural models: models that iteratively or recursively add new geometry to a structure. Most growth models, such as L-systems, are accumulative [Prusinkiewicz and Lindenmayer 1990], in contrast to other modelling paradigms: spatial subdivision, such as architectural split grammatics [Mu \u00bc ller et al. 2006]; object subdivision, such as fractal terrain [Lewis 1987]; or simulation, such as erosion-based terrain [S \u0430t'ava et al. 2008]. For our purposes, a procedural model is accumulative if it provides a \"current position\" during execution: a point p from which geometry generation is continued. We focus on 2D models (p. R2), although the techniques we present naturally extend to 3D."}, {"heading": "5.1 Network Architecture", "text": "In fact, most people who see themselves as being able to move are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance,"}, {"heading": "5.2 Training", "text": "We train neural-driven process models by stochastic gradient ascent using the gradient in Equation 2. Our system calculates this gradient by backpropagation from the log-p-i's to the neural network parameters \u03b8. We use the Adam algorithm for stochastic gradient optimization with \u03b1 = \u03b2 = 0.75 and an initial learning rate of 0.01 [Kingma and Ba 2015]. We found that a minichange size of one works best in our experiments: more frequent gradient updates led to faster convergence than less frequent but low-noise updates. We stop training after 20,000 gradient updates."}, {"heading": "5.3 Implementation Details", "text": "We implemented our prototype system in the Javascript-based probabilistic programming language WebPPL [Goodman and Stuhlmu M\u00fcller 2014], implementing neural networks with an open source Javascript library for neural computation.1 The source code for our system is available at [LINK ANONYMIZED]."}, {"heading": "6 Experiments", "text": "In this section, we evaluate qualitatively and quantitatively how well our neural-driven process models capture image-based constraints. First, we describe our target image databases before describing the details of several experiments. All time data reported in this section was collected on an Intel Core i7-3840QM machine with 16 GB of RAM running OSX 10.10.5."}, {"heading": "6.1 Image Datasets", "text": "As shown in Eq.1, each training sample from a procedural model must be paired with a constraint c drawn from a previous P (c) over possible constraints. During the training sample from a database of training images. In our experiments, we use the following image collections: \u2022 Scribbles: A set of 49 binary mask images drawn by hand using the brush tool in Photoshop. These were created according to 1https: / / github.com / dritchie / adnncover a set of possible shapes with thick and thin regions, high and low curvature, and different selfcuts. \u2022 Glyphs: A subset of 197 glyphs from the FF Tartine Script Bold font. Consists of all glyphs that have only one superficially connected component and at least 500 foreground pixels when rendered with 129x97. \u2022 PhyloPic: A set of 35 images from the Phyloic image, each of which we work with a database of two organisms, one for each of them."}, {"heading": "6.2 Shape Matching", "text": "In fact, it is so that it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way and a way in which it is about a way in which it is about a way in which it is about a way and a way and a way in which it is about a way in which it is about which it is about"}, {"heading": "6.3 Stylized \u201cCircuit\u201d Design", "text": "So far, we have focused on image matching constraints, but the architecture we have presented can learn other types of image-based constraints. In this section, we limit the tendril program to generating results that resemble stylized circuit designs. Dense packaging of long wire tracks is one of the most striking visual features of printed circuit boards. To achieve dense packaging, we encourage the program to fill a certain percentage of the image (\u03c4 = 0.5 in subsequent results). To mimic the appearance of tracks, we encourage the output image to have a dense, high-grade gradient field, as the tendril program can best achieve this result by generating many long rectilinear or diagonal edges. These constraints result in the following probability: \"circ (x) = N (edge (x)))) \u00b7 (1 \u2212 \u03b7 (abundance (I (x)), 1, 2), 3), 4) (I)."}, {"heading": "7 Discussion and Future Work", "text": "This paper presented neural-driven procedural models: restricted procedural models that use neural networks to detect constraint-induced dependencies. We developed a mathematical framework for defining and building such models. We also described a specific neural architecture for cumulative models that generate images. Finally, we examined the performance of neural-driven models and showed that they can produce high-quality results significantly faster than unguided models."}, {"heading": "7.1 Limitations", "text": "One limitation of our system is the need for training data to be generated via expensive inferences, which can lead to significant upfront costs, especially for computationally expensive models. Therefore, our method is not well suited for scenarios where the procedural model changes rapidly, such as accelerating the inner loop of a development and debugging cycle. Instead, it is best suited for scenarios where the model is set, such as using a finalized procedural model as part of a design tool. It can be particularly attractive for multi-user online deployment, where the system can collect sample results from the community, retrain regularly, and pass the updated procedural model on to users. Nor is our method good for capturing hard constraints that require some visual effects (e.g. symmetries), as it requires a continuous probability for each training test."}, {"heading": "7.2 Future Work", "text": "The architecture of the neural guidelines we present applies to image-generating programs, how could we extend it to other output areas? Architecture must represent the partial output of the model compactly at each point in the program. Of course, for 3D modeling, our architecture extends to 3D, for example, using voxel grids instead of images. For other output areas, it may also be possible to develop architectures that learn partial output state representation, as in the recent work on generating recurring sequences [Graves 2013]. Our architecture also focuses on accumulative process models, but applications of other generative paradigms could also benefit from neural guidance. An example is texture generation, which repeatedly generates content across its entire domain, often in a multi-scale manner. In such a setting, the guiding model cannot rely on a \"current position\" to extract decision-critical features. Learn which parts of the visual attention models are relevant to recent partial output, where they are evolving to look."}, {"heading": "BROOKS, S., GELMAN, A., JONES, G., AND MENG, X. 2011.", "text": "Handbook of Markov Chain Monte Carlo. CRC Press.CYBENKO, G. 1989. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems."}, {"heading": "DANG, M., LIENHARD, S., CEYLAN, D., NEUBERT, B.,", "text": "WONKA, P., AND PAULY, M. 2015. Interactive Design of Probability Density Functions for Shape Grammars.DOUCET, A., DE FREITAS, N., AND GORDON, N., Eds. 2001. Sequential Monte Carlo Methods in Practice. Springer.GOODMAN, N. D., AND STUHLMU \u00dcLLER, A., 2014. The Design and Implementation of Probabilistic Programming Languages. http: / / dippl.org. Access: 2015-12-23.GRAVES, A. 2013. Generating Sequences With Recurrent Neural Networks. CoRR abs / 138.0850.GU, S., GHAHRAMANI, Z., AND TURNER, R. E. 2015. Neural Adaptive Sequential Sequential Monte Carlo."}, {"heading": "KRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G. E. 2012.", "text": "ImageNet Classification with Deep Convolutional Neural Networks. In NIPS 2012.LEWIS, J. P. 1987. Generalized Stochastic Subdivision. ACM Trans. Graph. 6, 3."}, {"heading": "LIN, S., RITCHIE, D., FISHER, M., AND HANRAHAN, P. 2013.", "text": "Probabilistic Color-by-Numbers: Suggesting Pattern Colorizations using Factor Graphs, in: SIGGRAPH 2013.MACKAY, D. J. C. 2002, Information Theory, Inference & Learning Algorithms, Cambridge University Press."}, {"heading": "MARKS, J., ANDALMAN, B., BEARDSLEY, P. A., FREEMAN, W.,", "text": "GIBSON, S., HODGINS, J., KANG, T., MIRTICH, B., PFISTER, H., RUML, W., RYALL, K., SEIMS, J., AND SHIEBER, S. 1997. Design galleries: A general approach to setting parameters for computer graphics and animation. In SIGGRAPH 1997."}, {"heading": "MERRELL, P., SCHKUFZA, E., LI, Z., AGRAWALA, M., AND", "text": "KOLTUN, V. 2011. Interactive furniture layout using guidelines for interior design. In SIGGRAPH 2011.MNIH, A., AND GREGOR, K. 2014. Neural variation and learning in faith networks. In ICML 2014.MNIH, V., HEESS, N., GRAVES, A., AND KAVUKCUOGLU, K. 2014. Recurring models of visual attention. In NIPS 2014."}, {"heading": "MU\u0308LLER, P., WONKA, P., HAEGLER, S., ULMER, A., AND", "text": "VAN GOOL, L. 2006. Procedural Modeling of Buildings. In SIGGRAPH 2006.ME, CH, R., AND PRUSINKIEWICZ, S. 1996. Visual Modeling of Plants Interacting with Their Environment. In SIGGRAPH 1996.PRUSINKIEWICZ, P., AND LINDENMAYER, A. 1990. The Algorithmic Beauty of Plants. Springer-Verlag New York, Inc.PRUSINKIEWICZ, P., JAMES, M., AND ME, CH, R. 1994. Synthetic Topiary. In SIGGRAPH 1994."}, {"heading": "REZENDE, D. J., MOHAMED, S., AND WIERSTRA, D. 2014.", "text": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In ICML 2014."}, {"heading": "RITCHIE, D., LIN, S., GOODMAN, N. D., AND HANRAHAN, P.", "text": "2015. In Eurographics 2015.RITCHIE, D., MILDENHALL, B., GOODMAN, N. D., AND HANRAHAN, P. 2015. Controlling Procedural Modeling Programs with Stochastically-Ordered Sequential Monte Carlo. In SIGGRAPH 2015."}, {"heading": "RUMELHART, D. E., HINTON, G. E., AND WILLIAMS, R. J.", "text": "1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1. MIT Press, ch. Learning Internal Representations by Error Propagation.SCHWARZ, M., AND WONKA, P. 2014. Process design of exterior lighting of buildings with complex limitations. ACM Trans. Graph. 33, 5."}, {"heading": "STAVA, O., PIRK, S., KRATT, J., CHEN, B., MCH, R., DEUSSEN,", "text": "O., AND BENES, B. 2014. Inverse procedural modeling of trees. Computer Graphics Forum 33, 6."}, {"heading": "TALTON, J. O., LOU, Y., LESSER, S., DUKE, J., ME\u030cCH, R., AND", "text": "KOLTUN, V. 2011. Metropolis Procedural Modeling. ACM Trans. Graph. 30, 2."}, {"heading": "VANEGAS, C. A., GARCIA-DORADO, I., ALIAGA, D. G.,", "text": "BENES, B., AND WADDELL, P. 2012. Inverse design of urban process models. In SIGGRAPH Asia 2012.S, T'AVA, O., BENES, B., BRISBIN, M., AND KR, IVA \"NEK, J. 2008. Interactive terrain modeling using hydraulic erosion. In SCA 2008.WILLIAMS, R. J. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8.WINGATE, D., AND WEBER, T. 2012. Automated variation control in probabilistic programming. In NIPS 2012 Workshop on Probabilistic Programming."}, {"heading": "WONG, M. T., ZONGKER, D. E., AND SALESIN, D. H. 1998.", "text": "Computer generated Floral Ornament. In SIGGRAPH 1998."}, {"heading": "YEH, Y.-T., YANG, L., WATSON, M., GOODMAN, N. D., AND", "text": "HANRAHAN, S. 2012. Synthesizing open worlds with limitations using locally annealed Reversible Jump MCMC. In SIGGRAPH 2012."}, {"heading": "YU, L.-F., YEUNG, S.-K., TERZOPOULOS, D., AND CHAN, T. F.", "text": "2012. DressUp!: Outfit synthesis by automatic optimization. In SIGGRAPH Asia 2012."}, {"heading": "YUMER, M. E., ASENTE, P., MECH, R., AND KARA, L. B. 2015.", "text": "Procedural Modeling Using Autoencoder Networks. In UIST 2015."}, {"heading": "ZHU, L., XU, W., SNYDER, J., LIU, Y., WANG, G., AND GUO,", "text": "B. 2012. Motion-controlled mechanical toy modeling. In SIGGRAPH Asia 2012."}], "references": [{"title": "Guided Procedural Modeling", "author": ["B. BENE\u0160", "O. \u0160AVA", "R. M\u011aCH", "G. MILLER"], "venue": "Eurographics 2011.", "citeRegEx": "BENE\u0160 et al\\.,? 2011", "shortCiteRegEx": "BENE\u0160 et al\\.", "year": 2011}, {"title": "Handbook of Markov Chain Monte Carlo", "author": ["S. BROOKS", "A. GELMAN", "G. JONES", "X. MENG"], "venue": "CRC Press.", "citeRegEx": "BROOKS et al\\.,? 2011", "shortCiteRegEx": "BROOKS et al\\.", "year": 2011}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. CYBENKO"], "venue": "Mathematics of Control, Signals and Systems.", "citeRegEx": "CYBENKO,? 1989", "shortCiteRegEx": "CYBENKO", "year": 1989}, {"title": "Interactive Design of Probability Density Functions for Shape Grammars", "author": ["M. DANG", "S. LIENHARD", "D. CEYLAN", "B. NEUBERT", "P. WONKA", "M. PAULY"], "venue": null, "citeRegEx": "DANG et al\\.,? \\Q2015\\E", "shortCiteRegEx": "DANG et al\\.", "year": 2015}, {"title": "Sequential Monte Carlo Methods in Practice", "author": ["A. DOUCET", "N. DE FREITAS", "N. GORDON", "Eds."], "venue": "Springer.", "citeRegEx": "DOUCET et al\\.,? 2001", "shortCiteRegEx": "DOUCET et al\\.", "year": 2001}, {"title": "The Design and Implementation of Probabilistic Programming Languages. http://dippl.org", "author": ["N.D. GOODMAN", "A. STUHLM\u00dcLLER"], "venue": null, "citeRegEx": "GOODMAN and STUHLM\u00dcLLER,? \\Q2014\\E", "shortCiteRegEx": "GOODMAN and STUHLM\u00dcLLER", "year": 2014}, {"title": "Generating Sequences With Recurrent Neural Networks", "author": ["A. GRAVES"], "venue": "CoRR abs/1308.0850.", "citeRegEx": "GRAVES,? 2013", "shortCiteRegEx": "GRAVES", "year": 2013}, {"title": "Neural Adaptive Sequential Monte Carlo", "author": ["S. GU", "Z. GHAHRAMANI", "R.E. TURNER"], "venue": "NIPS 2015.", "citeRegEx": "GU et al\\.,? 2015", "shortCiteRegEx": "GU et al\\.", "year": 2015}, {"title": "Black Box Variational Inference", "author": ["J. MANNING", "K.N.R. RANGANATH", "D. BLEI"], "venue": "AISTATS 2014.", "citeRegEx": "MANNING et al\\.,? 2014", "shortCiteRegEx": "MANNING et al\\.", "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["KINGMA D.P.", "BA", "J."], "venue": "ICLR 2015.", "citeRegEx": "P. et al\\.,? 2015", "shortCiteRegEx": "P. et al\\.", "year": 2015}, {"title": "Auto-Encoding Variational Bayes", "author": ["D.P. KINGMA", "M. WELLING"], "venue": "ICLR 2014.", "citeRegEx": "KINGMA and WELLING,? 2014", "shortCiteRegEx": "KINGMA and WELLING", "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. KRIZHEVSKY", "I. SUTSKEVER", "G.E. HINTON"], "venue": "NIPS 2012.", "citeRegEx": "KRIZHEVSKY et al\\.,? 2012", "shortCiteRegEx": "KRIZHEVSKY et al\\.", "year": 2012}, {"title": "Generalized Stochastic Subdivision", "author": ["J.P. LEWIS"], "venue": "ACM Trans. Graph. 6, 3.", "citeRegEx": "LEWIS,? 1987", "shortCiteRegEx": "LEWIS", "year": 1987}, {"title": "Probabilistic Color-by-Numbers: Suggesting Pattern Colorizations Using Factor Graphs", "author": ["S. LIN", "D. RITCHIE", "M. FISHER", "P. HANRAHAN"], "venue": "SIGGRAPH 2013.", "citeRegEx": "LIN et al\\.,? 2013", "shortCiteRegEx": "LIN et al\\.", "year": 2013}, {"title": "Information Theory, Inference & Learning Algorithms", "author": ["D.J.C. MACKAY"], "venue": "Cambridge University Press.", "citeRegEx": "MACKAY,? 2002", "shortCiteRegEx": "MACKAY", "year": 2002}, {"title": "Design galleries: A general approach to setting parameters for computer graphics and animation", "author": ["J. MARKS", "B. ANDALMAN", "P.A. BEARDSLEY", "W. FREEMAN", "S. GIBSON", "J. HODGINS", "T. KANG", "B. MIRTICH", "H. PFISTER", "W. RUML", "K. RYALL", "J. SEIMS", "S. SHIEBER"], "venue": "SIGGRAPH 1997.", "citeRegEx": "MARKS et al\\.,? 1997", "shortCiteRegEx": "MARKS et al\\.", "year": 1997}, {"title": "Interactive Furniture Layout Using Interior Design Guidelines", "author": ["P. MERRELL", "E. SCHKUFZA", "Z. LI", "M. AGRAWALA", "V. KOLTUN"], "venue": "SIGGRAPH 2011.", "citeRegEx": "MERRELL et al\\.,? 2011", "shortCiteRegEx": "MERRELL et al\\.", "year": 2011}, {"title": "Neural Variational Inference and Learning in Belief Networks", "author": ["A. MNIH", "K. GREGOR"], "venue": "ICML 2014.", "citeRegEx": "MNIH and GREGOR,? 2014", "shortCiteRegEx": "MNIH and GREGOR", "year": 2014}, {"title": "Recurrent Models of Visual Attention", "author": ["V. MNIH", "N. HEESS", "A. GRAVES", "K. KAVUKCUOGLU"], "venue": "NIPS 2014.", "citeRegEx": "MNIH et al\\.,? 2014", "shortCiteRegEx": "MNIH et al\\.", "year": 2014}, {"title": "Procedural Modeling of Buildings", "author": ["P. M\u00dcLLER", "P. WONKA", "S. HAEGLER", "A. ULMER", "L. VAN GOOL"], "venue": "SIGGRAPH 2006.", "citeRegEx": "M\u00dcLLER et al\\.,? 2006", "shortCiteRegEx": "M\u00dcLLER et al\\.", "year": 2006}, {"title": "Visual Models of Plants Interacting with Their Environment", "author": ["R. M\u011aCH", "P. PRUSINKIEWICZ"], "venue": "SIGGRAPH 1996.", "citeRegEx": "M\u011aCH and PRUSINKIEWICZ,? 1996", "shortCiteRegEx": "M\u011aCH and PRUSINKIEWICZ", "year": 1996}, {"title": "The Algorithmic Beauty of Plants", "author": ["P. PRUSINKIEWICZ", "A. LINDENMAYER"], "venue": "Springer-Verlag New York, Inc.", "citeRegEx": "PRUSINKIEWICZ and LINDENMAYER,? 1990", "shortCiteRegEx": "PRUSINKIEWICZ and LINDENMAYER", "year": 1990}, {"title": "Synthetic Topiary", "author": ["P. PRUSINKIEWICZ", "M. JAMES", "R. M\u011aCH"], "venue": "SIGGRAPH 1994.", "citeRegEx": "PRUSINKIEWICZ et al\\.,? 1994", "shortCiteRegEx": "PRUSINKIEWICZ et al\\.", "year": 1994}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "author": ["D.J. REZENDE", "S. MOHAMED", "D. WIERSTRA"], "venue": "ICML 2014.", "citeRegEx": "REZENDE et al\\.,? 2014", "shortCiteRegEx": "REZENDE et al\\.", "year": 2014}, {"title": "Generating Design Suggestions under Tight Constraints with Gradient-based Probabilistic Programming", "author": ["D. RITCHIE", "S. LIN", "N.D. GOODMAN", "P. HANRAHAN"], "venue": "Eurographics 2015.", "citeRegEx": "RITCHIE et al\\.,? 2015", "shortCiteRegEx": "RITCHIE et al\\.", "year": 2015}, {"title": "Controlling Procedural Modeling Programs with Stochastically-Ordered Sequential Monte Carlo", "author": ["D. RITCHIE", "B. MILDENHALL", "N.D. GOODMAN", "P. HANRAHAN"], "venue": "SIGGRAPH 2015.", "citeRegEx": "RITCHIE et al\\.,? 2015", "shortCiteRegEx": "RITCHIE et al\\.", "year": 2015}, {"title": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol", "author": ["D.E. RUMELHART", "G.E. HINTON", "R.J. WILLIAMS"], "venue": "1. MIT Press, ch. Learning Internal Representations by Error Propagation.", "citeRegEx": "RUMELHART et al\\.,? 1986", "shortCiteRegEx": "RUMELHART et al\\.", "year": 1986}, {"title": "Procedural Design of Exterior Lighting for Buildings with Complex Constraints", "author": ["M. SCHWARZ", "P. WONKA"], "venue": "ACM Trans. Graph. 33, 5.", "citeRegEx": "SCHWARZ and WONKA,? 2014", "shortCiteRegEx": "SCHWARZ and WONKA", "year": 2014}, {"title": "Inverse Procedural Modelling of Trees", "author": ["O. STAVA", "S. PIRK", "J. KRATT", "B. CHEN", "R. MCH", "O. DEUSSEN", "B. BENES"], "venue": "Computer Graphics Forum 33, 6.", "citeRegEx": "STAVA et al\\.,? 2014", "shortCiteRegEx": "STAVA et al\\.", "year": 2014}, {"title": "Metropolis Procedural Modeling", "author": ["J.O. TALTON", "Y. LOU", "S. LESSER", "J. DUKE", "R. M\u011aCH", "V. KOLTUN"], "venue": "ACM Trans. Graph. 30, 2.", "citeRegEx": "TALTON et al\\.,? 2011", "shortCiteRegEx": "TALTON et al\\.", "year": 2011}, {"title": "Inverse Design of Urban Procedural Models", "author": ["C.A. VANEGAS", "I. GARCIA-DORADO", "D.G. ALIAGA", "B. BENES", "P. WADDELL"], "venue": "SIGGRAPH Asia 2012.", "citeRegEx": "VANEGAS et al\\.,? 2012", "shortCiteRegEx": "VANEGAS et al\\.", "year": 2012}, {"title": "Interactive Terrain Modeling Using Hydraulic Erosion", "author": ["O. \u0160T\u2019AVA", "B. BENE\u0160", "M. BRISBIN", "J. K\u0158IV\u00c1NEK"], "venue": "SCA", "citeRegEx": "\u0160T.AVA et al\\.,? \\Q2008\\E", "shortCiteRegEx": "\u0160T.AVA et al\\.", "year": 2008}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. WILLIAMS"], "venue": "Machine Learning 8.", "citeRegEx": "WILLIAMS,? 1992", "shortCiteRegEx": "WILLIAMS", "year": 1992}, {"title": "Automated Variational Inference in Probabilistic Programming", "author": ["D. WINGATE", "T. WEBER"], "venue": "NIPS 2012 Workshop on Probabilistic Programming.", "citeRegEx": "WINGATE and WEBER,? 2012", "shortCiteRegEx": "WINGATE and WEBER", "year": 2012}, {"title": "Computer-generated Floral Ornament", "author": ["M.T. WONG", "D.E. ZONGKER", "D.H. SALESIN"], "venue": "SIGGRAPH 1998.", "citeRegEx": "WONG et al\\.,? 1998", "shortCiteRegEx": "WONG et al\\.", "year": 1998}, {"title": "Synthesizing Open Worlds with Constraints Using Locally Annealed Reversible Jump MCMC", "author": ["YEH", "Y.-T.", "L. YANG", "M. WATSON", "N.D. GOODMAN", "P. HANRAHAN"], "venue": "SIGGRAPH 2012.", "citeRegEx": "YEH et al\\.,? 2012", "shortCiteRegEx": "YEH et al\\.", "year": 2012}, {"title": "DressUp!: Outfit Synthesis Through Automatic Optimization", "author": ["YU", "L.-F.", "YEUNG", "S.-K.", "D. TERZOPOULOS", "T.F. CHAN"], "venue": "SIGGRAPH Asia 2012.", "citeRegEx": "YU et al\\.,? 2012", "shortCiteRegEx": "YU et al\\.", "year": 2012}, {"title": "Procedural Modeling Using Autoencoder Networks", "author": ["M.E. YUMER", "P. ASENTE", "R. MECH", "L.B. KARA"], "venue": "UIST 2015.", "citeRegEx": "YUMER et al\\.,? 2015", "shortCiteRegEx": "YUMER et al\\.", "year": 2015}, {"title": "Motion-guided Mechanical Toy Modeling", "author": ["ZHU L.", "XU W.", "SNYDER J.", "LIU Y.", "WANG G.", "GUO", "B."], "venue": "SIGGRAPH Asia 2012.", "citeRegEx": "L. et al\\.,? 2012", "shortCiteRegEx": "L. et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 34, "context": "It can generate fine detail that would require painstaking effort to create by hand, such as decorative floral patterns [Wong et al. 1998].", "startOffset": 120, "endOffset": 138}, {"referenceID": 15, "context": "It can even generate surprising or unexpected results, helping users to explore large or unintuitive design spaces [Marks et al. 1997; Ritchie et al. 2015a].", "startOffset": 115, "endOffset": 156}, {"referenceID": 28, "context": "Many applications demand the ability to constrain or control procedural models: making their outputs resemble examples [Stava et al. 2014; Dang et al. 2015], fit a target shape [Prusinkiewicz et al.", "startOffset": 119, "endOffset": 156}, {"referenceID": 22, "context": "2015], fit a target shape [Prusinkiewicz et al. 1994; Talton et al. 2011; Ritchie et al. 2015b], or respect functional constraints such as physical stability [Ritchie et al.", "startOffset": 26, "endOffset": 95}, {"referenceID": 29, "context": "2015], fit a target shape [Prusinkiewicz et al. 1994; Talton et al. 2011; Ritchie et al. 2015b], or respect functional constraints such as physical stability [Ritchie et al.", "startOffset": 26, "endOffset": 95}, {"referenceID": 1, "context": "Samples from the posterior distribution can be drawn via approximate inference algorithms such as Markov Chain Monte Carlo (MCMC) or Sequential Monte Carlo (SMC) [Brooks et al. 2011; Doucet et al. 2001].", "startOffset": 162, "endOffset": 202}, {"referenceID": 4, "context": "Samples from the posterior distribution can be drawn via approximate inference algorithms such as Markov Chain Monte Carlo (MCMC) or Sequential Monte Carlo (SMC) [Brooks et al. 2011; Doucet et al. 2001].", "startOffset": 162, "endOffset": 202}, {"referenceID": 27, "context": "Performance can be improved through clever sampler design, but this requires time and expertise and is often problem-specific [Schwarz and Wonka 2014; Zhu et al. 2012].", "startOffset": 126, "endOffset": 167}, {"referenceID": 29, "context": "Probabilistic Inference for Procedural Modeling Many research projects have used Bayesian probabilistic inference to control procedural models: constraining the shape of a 3D object [Talton et al. 2011; Ritchie et al. 2015b], creating functionally-plausible and aesthetically-pleasing furniture arrangements [Merrell et al.", "startOffset": 182, "endOffset": 224}, {"referenceID": 16, "context": "2015b], creating functionally-plausible and aesthetically-pleasing furniture arrangements [Merrell et al. 2011; Yeh et al. 2012], coloring in patterns [Lin et al.", "startOffset": 90, "endOffset": 128}, {"referenceID": 35, "context": "2015b], creating functionally-plausible and aesthetically-pleasing furniture arrangements [Merrell et al. 2011; Yeh et al. 2012], coloring in patterns [Lin et al.", "startOffset": 90, "endOffset": 128}, {"referenceID": 13, "context": "2012], coloring in patterns [Lin et al. 2013], and dressing virtual characters [Yu et al.", "startOffset": 28, "endOffset": 45}, {"referenceID": 36, "context": "2013], and dressing virtual characters [Yu et al. 2012] are a few recent applications.", "startOffset": 39, "endOffset": 55}, {"referenceID": 22, "context": "The seminal work on open/environmentally-sensitive L-systems developed a formalism by which L-systems could query their spatial position and orientation [Prusinkiewicz et al. 1994; M\u011bch and Prusinkiewicz 1996].", "startOffset": 153, "endOffset": 209}, {"referenceID": 30, "context": "One recent project uses neural networks as computationally inexpensive proxies for costly scoring functions in an inverse urban procedural modeling setting [Vanegas et al. 2012].", "startOffset": 156, "endOffset": 177}, {"referenceID": 37, "context": "Another uses an autoencoder network to learn a low-dimensional representation space in which it is easy to explore the variability in a procedural model\u2019s output [Yumer et al. 2015].", "startOffset": 162, "endOffset": 181}, {"referenceID": 17, "context": "Neural Variational Inference Our method is also inspired by recent work in variational inference [Mnih and Gregor 2014; Rezende et al. 2014; Kingma and Welling 2014].", "startOffset": 97, "endOffset": 165}, {"referenceID": 23, "context": "Neural Variational Inference Our method is also inspired by recent work in variational inference [Mnih and Gregor 2014; Rezende et al. 2014; Kingma and Welling 2014].", "startOffset": 97, "endOffset": 165}, {"referenceID": 10, "context": "Neural Variational Inference Our method is also inspired by recent work in variational inference [Mnih and Gregor 2014; Rezende et al. 2014; Kingma and Welling 2014].", "startOffset": 97, "endOffset": 165}, {"referenceID": 7, "context": "The Neural Adaptive Sequential Monte Carlo algorithm is most similar to our method; it uses a similar learning objective and aims to train more efficient SMC importance samplers [Gu et al. 2015].", "startOffset": 178, "endOffset": 194}, {"referenceID": 24, "context": "Our system uses the version of SMC for probabilistic programs presented by Ritchie et al. [2015b], where particles are resampled after the program generates a new piece of geometry.", "startOffset": 75, "endOffset": 98}, {"referenceID": 33, "context": "It is the optimization objective used in many variational inference algorithms [Wingate and Weber 2012; J. Manning and Blei 2014; Mnih and Gregor 2014] as well the REINFORCE algorithm for reinforcement learning [Williams 1992].", "startOffset": 79, "endOffset": 151}, {"referenceID": 17, "context": "It is the optimization objective used in many variational inference algorithms [Wingate and Weber 2012; J. Manning and Blei 2014; Mnih and Gregor 2014] as well the REINFORCE algorithm for reinforcement learning [Williams 1992].", "startOffset": 79, "endOffset": 151}, {"referenceID": 32, "context": "Manning and Blei 2014; Mnih and Gregor 2014] as well the REINFORCE algorithm for reinforcement learning [Williams 1992].", "startOffset": 104, "endOffset": 119}, {"referenceID": 14, "context": "concentrating their probability mass in a smaller volume of the state space than the true distribution being approximated [MacKay 2002].", "startOffset": 122, "endOffset": 135}, {"referenceID": 21, "context": "Most growth models, such as L-systems, are accumulative [Prusinkiewicz and Lindenmayer 1990].", "startOffset": 56, "endOffset": 92}, {"referenceID": 12, "context": "2006]; object subdivision, such as fractal terrain [Lewis 1987]; or simulation, such as erosion-based terrain [\u0160t\u2019ava et al.", "startOffset": 51, "endOffset": 63}, {"referenceID": 26, "context": "We use a multilayer perceptron (MLP) architecture, because it is simple, easy to scale, and is a universal function approximator [Rumelhart et al. 1986; Cybenko 1989].", "startOffset": 129, "endOffset": 166}, {"referenceID": 2, "context": "We use a multilayer perceptron (MLP) architecture, because it is simple, easy to scale, and is a universal function approximator [Rumelhart et al. 1986; Cybenko 1989].", "startOffset": 129, "endOffset": 166}, {"referenceID": 11, "context": "Convolutional neural networks reduce an image to a fixed-width feature vector but are aimed at classification tasks: they detect features but are intentionally invariant to where those features occur [Krizhevsky et al. 2012].", "startOffset": 200, "endOffset": 224}, {"referenceID": 18, "context": "This architecture is similar to the foveated \u2018glimpses\u2019 used in recent work on neural models of visual attention [Mnih et al. 2014].", "startOffset": 113, "endOffset": 131}, {"referenceID": 22, "context": "This behavior is not easy to achieve with a purely generative space-filling approach such as environmentally-sensitive Lsystems [Prusinkiewicz et al. 1994], but it is simple to specify with constraints.", "startOffset": 128, "endOffset": 155}, {"referenceID": 33, "context": "This is also known as a partial mean field approximation [Wingate and Weber 2012].", "startOffset": 57, "endOffset": 81}, {"referenceID": 11, "context": "This suggests that \u223c1000 sample traces is sufficient, which may seem surprising, as many published deep learning systems require millions of training examples [Krizhevsky et al. 2012].", "startOffset": 159, "endOffset": 183}, {"referenceID": 6, "context": "For other output domains, it may be possible to develop architectures that learn a partial output state representation, as in recent work on recurrent sequence generation [Graves 2013].", "startOffset": 171, "endOffset": 184}, {"referenceID": 18, "context": "It might instead learn what parts of the current partial output are relevant, as recently-developed visual attention models learn where to look in an image to make classification decisions [Mnih et al. 2014].", "startOffset": 189, "endOffset": 207}], "year": 2017, "abstractText": "We present a deep learning approach for speeding up constrained procedural modeling. Probabilistic inference algorithms such as Sequential Monte Carlo (SMC) provide powerful tools for constraining procedural models, but they require many samples to produce desirable results. In this paper, we show how to create procedural models which learn how to satisfy constraints. We augment procedural models with neural networks: these networks control how the model makes random choices based on what output it has generated thus far. We call such a model a neurally-guided procedural model. As a pre-computation, we train these models on constraint-satisfying example outputs generated via SMC. They are then used as efficient importance samplers for SMC, generating high-quality results with very few samples. We evaluate our method on L-system-like models with image-based constraints. Given a desired quality threshold, neurally-guided models can generate satisfactory results up to 10x faster than unguided models. CR Categories: I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling\u2014Geometric algorithms, languages, and systems G.3 [Probability And Statistics]: Probabilistic algorithms (including Monte Carlo) I.2.6 [Artificial Intelligence]: Learning\u2014Connectionism and neural nets", "creator": "LaTeX acmsiggraph.cls (11/2015)"}}}