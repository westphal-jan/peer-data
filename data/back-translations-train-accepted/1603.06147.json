{"id": "1603.06147", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "A Character-level Decoder without Explicit Segmentation for Neural Machine Translation", "abstract": "The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.", "histories": [["v1", "Sat, 19 Mar 2016 21:35:04 GMT  (172kb,D)", "http://arxiv.org/abs/1603.06147v1", null], ["v2", "Wed, 23 Mar 2016 20:57:23 GMT  (172kb,D)", "http://arxiv.org/abs/1603.06147v2", null], ["v3", "Thu, 16 Jun 2016 04:06:01 GMT  (172kb,D)", "http://arxiv.org/abs/1603.06147v3", null], ["v4", "Tue, 21 Jun 2016 01:12:22 GMT  (174kb,D)", "http://arxiv.org/abs/1603.06147v4", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["junyoung chung", "kyunghyun cho", "yoshua bengio"], "accepted": true, "id": "1603.06147"}, "pdf": {"name": "1603.06147.pdf", "metadata": {"source": "CRF", "title": "A Character-level Decoder without Explicit Segmentation for Neural Machine Translation", "authors": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "emails": ["junyoung.chung@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "Existing machine translation systems have relied almost exclusively on word-level modeling with explicit segmentation, mainly due to the problem of data sparseness, which becomes much more difficult, especially at the n-Grammys, when a sentence is represented as a sequence of signs rather than words, as the length of the sequence increases significantly. In addition to data sparseness, we often believe that a word or its segmented lexeme is a fundamental unit of importance, which makes it natural to turn to translation as a mapping of source-language words to a sequence of target-language words. This continues with the recently proposed paradigm of neural machine translation, although neural networks do not suffer from character-level sequencing and are more likely to suffer from problems specific at the word level, such as the increased forced complexity of words at the target-level (Jean et al., 2015; Luong et al, 2015b)."}, {"heading": "2 Neural Machine Translation", "text": "Neural machine translation refers to a recently proposed approach to machine translation (Cho etar Xiv: 160 3.06 147v 1 [cs.C L] 19 Mar 201 6al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015). This approach aims to build an endpoint neural network that uses X = (x1,., xTx) as its starting set and outputs its translation Y = (y1,., yTy), where xt and yt \u2032 are both source and target symbols. This neural network is a composite of an encoder network and a decoder network.The encoder network encodes the input X into its continuous representation. In this paper, we closely follow the neural translation model proposed in Bahdanau et al. (2015) and use a bidirectional neural network consisting of two neural networks."}, {"heading": "3 Towards Character-Level Translation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Motivation", "text": "In fact, it is the case that most of them will be able to move into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "3.2 Why Word-Level Translation?", "text": "(1) The word as a fundamental unit of meaning A word can be understood in two different senses. (Booij, 2012) A word in the first sense becomes a fundamental unit of meaning (lexem) and can be understood in another sense as \"concrete word as it is used in a sentence.\" (Booij, 2012) Due to this view of words as basic units of meaning (either in the form of lexicon or derived form) from linguistics, these three processes will change the meaning of the lexicon, but often it remains close to the original meaning."}, {"heading": "3.3 Why Character-Level Translation?", "text": "This year, the time has come for us to be able to find a new home in the city, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home."}, {"heading": "3.4 Challenges and Questions", "text": "There are two overlapping challenges for the source page and the target page. On the source page, it is unclear how to build a neural network that learns a highly non-linear mapping from spelling to the meaning of a sentence. On the target page, there are two challenges: the first challenge is the same from the source page, since the neural network of the decoder has to summarize the translated. In addition, character-based modeling on the target page is more difficult, since the decoder network has to be able to generate a long, coherent string. This is a major challenge, as the size of the statespace is growing exponentially; the number of symbols, and in the case of characters, is often 300-1000 symbols long. All of these challenges should first be taken as questions; whether the current recurring neural networks that are already widespread in neural machine translation are able to address these challenges as they are."}, {"heading": "4 Character-level Translation", "text": "In this paper, we try to answer the questions raised above by testing two different types of recurrent neural networks on the target side (decoder). First, we test an existing recurrent neural network with gated recurrent units (GRU), which we call a base decoder. Second, we build a novel, two-tiered recurrent neural network inspired by the gated feedback network of Chung et al. (2015), which is called biscale recurrent neural networks. We design this network to facilitate the capture of two time scales, motivated by the fact that characters and words can function in two different time scales. We decide to test these two alternatives for the following purposes: experiments with the base decoder will clearly answer whether the existing neural network is sufficient to handle character-level decoding, which is not possible in the context of machine translation, if the first decoder is tested positively, or if the two-scaled alternative is tested."}, {"heading": "4.1 Bi-Scale Recurrent Neural Network", "text": "They contain the same number of units, i.e. they are faster than ever before, and h2 a slower timeline (hence a slower timeline); for each hidden unit, there is a faster output unit, which we point to with g1 and g2. For the description below, we use yt \"1 and ct\" for the previous target symbol and the context vector (see Eq. (2), respectively, the faster output layer. The faster layer prints two sets of activations, a normal output amount h1t \"and its dated version h.\" Activation of the faster layer is calculated."}, {"heading": "5 Experiment Settings", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the German Press Agency.\" We have never hesitated so long, \"he said.\" We have never hesitated so long until we have come to the point where we have seen ourselves able to retaliate, \"he said."}, {"heading": "6 Quantitative Analysis", "text": "Slower Layer for Alignment On En-De, we test which layer of the decoder should be used to calculate soft alignments. In the case of the decoder at subword level, we observed no difference between selecting one of the two layers of the decoder against using the concatenation of all layers (Table 1 (a-b)). On the other hand, with the decoder at character level, we noticed an improvement when only the slower layer (h2) was used for the soft alignment mechanism (Table 1 (c-g)). This suggests that the soft alignment mechanism has advantages by aligning a larger part of the target with a subword unit in the source, and we only use the lower layer for all other language pairs. Single Models In Table 1, we present a comprehensive report on the translation qualities of (1) subordinate decoder decoder, (2) at pointer level (3) decoder level."}, {"heading": "7 Qualitative Analysis", "text": "(1) Can the character-level decoder produce a long, coherent sentence? Translation into characters is drastically longer than into words, probably making it more difficult for a recursive neural network to generate a coherent sentence into characters. (2) Does the character-level decoder help with rare words? One advantage of character-level decoding is that it can model the composition of any string, allowing for better modeling of rare morphological variants. We confirm this empirically by observing the growing gap in average negative word probability between the character-level and the character-level so well that character-level decoders reduce the frequency of words. \u2212 This is shown in Figure 2 (right) and explains a possible cause for the success of the character-level decoding. \""}, {"heading": "8 Conclusion", "text": "In this paper, we addressed a fundamental question of whether a recently proposed neural machine translation system can handle translations directly at character level without word segmentation. We focused on the target page, where a decoder was asked to generate one character each while aligning smoothly between a target character and a source subword. Our extensive experiments on four language pairs - En-Cs, EnDe, En-Ru and En-Fi - strongly suggest that it is actually possible for neural machine translation to translate at character level, and that it actually benefits from it. Our result has a limitation that we have used subword symbols on the source side, but this has allowed for a finer analysis, but in the future we need to examine an environment in which the source page is also represented as a character sequence."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano (Bastien et al., 2012) for their support of the following research funding and computer support agencies: NSERC, Calcul Que'bec, Compute Canada, the Canada Research Chairs, CIFAR and Samsung. KC would like to thank Facebook and Google for their support (Google Faculty Award 2016)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1211.5590.", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "IEEE Transactions on Neural Networks, 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent."], "venue": "Advances in Neural Information Processing Systems, pages 932\u2013938.", "citeRegEx": "Bengio et al\\.,? 2001", "shortCiteRegEx": "Bengio et al\\.", "year": 2001}, {"title": "The grammar of words: An introduction to linguistic morphology", "author": ["Geert Booij."], "venue": "Oxford University Press.", "citeRegEx": "Booij.,? 2012", "shortCiteRegEx": "Booij.", "year": 2012}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A Botha", "Phil Blunsom."], "venue": "ICML 2014.", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "Variablelength word encodings for neural translation models", "author": ["Rohan Chitnis", "John DeNero."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2088\u20132093.", "citeRegEx": "Chitnis and DeNero.,? 2015", "shortCiteRegEx": "Chitnis and DeNero.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the Empiricial", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 32nd International Conference on Machine Learning.", "citeRegEx": "Chung et al\\.,? 2015", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Marta R Costa-Juss\u00e0", "Jos\u00e9 AR Fonollosa."], "venue": "arXiv preprint arXiv:1603.00810.", "citeRegEx": "Costa.Juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.Juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0", "author": ["Mathias Creutz", "Krista Lagus"], "venue": "Helsinki University of Technology", "citeRegEx": "Creutz and Lagus.,? \\Q2005\\E", "shortCiteRegEx": "Creutz and Lagus.", "year": 2005}, {"title": "Edinburgh\u2019s phrase-based machine translation systems for wmt-14", "author": ["Nadir Durrani", "Barry Haddow", "Philipp Koehn", "Kenneth Heafield."], "venue": "Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation, Baltimore, MD, USA,", "citeRegEx": "Durrani et al\\.,? 2014", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "Eu-bridge mt: Combined machine translation", "author": ["Markus Freitag", "Stephan Peitz", "Joern Wuebker", "Hermann Ney", "Matthias Huck", "Rico Sennrich", "Nadir Durrani", "Maria Nadejde", "Philip Williams", "Philipp Koehn"], "venue": null, "citeRegEx": "Freitag et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Freitag et al\\.", "year": 2014}, {"title": "The edinburgh/jhu phrase-based machine translation systems for wmt 2015", "author": ["Barry Haddow", "Matthias Huck", "Alexandra Birch", "Nikolay Bogoychev", "Philipp Koehn."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 126\u2013", "citeRegEx": "Haddow et al\\.,? 2015", "shortCiteRegEx": "Haddow et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Sepp Hochreiter."], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):107\u2013116.", "citeRegEx": "Hochreiter.,? 1998", "shortCiteRegEx": "Hochreiter.", "year": 1998}, {"title": "Chinese word segmentation: A decade review", "author": ["Changning Huang", "Hai Zhao."], "venue": "Journal of Chinese Information Processing, 21(3):8\u201320.", "citeRegEx": "Huang and Zhao.,? 2007", "shortCiteRegEx": "Huang and Zhao.", "year": 2007}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics: Short", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Naver machine translation system for wat 2015", "author": ["Hyoung-Gyu Lee", "JaeSong Lee", "Jun-Seok Kim", "Chang-Ki Lee."], "venue": "Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 69\u201373.", "citeRegEx": "Lee et al\\.,? 2015", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black."], "venue": "arXiv preprint arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015b", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL, pages 104\u2013113.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1508.04025.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "arXiv preprint arXiv:1410.8206.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Subword language modeling with neural networks", "author": ["Tomas Mikolov", "Ilya Sutskever", "Anoop Deoras", "HaiSon Le", "Stefan Kombrink", "J Cernocky."], "venue": "Preprint.", "citeRegEx": "Mikolov et al\\.,? 2012", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1312.6026.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Abu-matran at wmt 2015 translation task: Morphological segmentation and web crawling", "author": ["Raphael Rubino", "Tommi Pirinen", "Miquel Espla-Gomis", "N Ljube\u0161ic", "Sergio Ortiz Rojas", "Vassilis Papavassiliou", "Prokopis Prokopidis", "Antonio Toral."], "venue": "Proceed-", "citeRegEx": "Rubino et al\\.,? 2015", "shortCiteRegEx": "Rubino et al\\.", "year": 2015}, {"title": "Continuous space language models", "author": ["Holger Schwenk."], "venue": "Computer Speech & Language, 21(3):492\u2013 518.", "citeRegEx": "Schwenk.,? 2007", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["Rupesh K Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Advances in Neural Information Processing Systems, pages 2368\u20132376.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML\u201911), pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Can we translate letters? In Proceedings of the Second Workshop on Statistical Machine Translation, pages 33\u201339", "author": ["David Vilar", "Jan-T Peter", "Hermann Ney."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Vilar et al\\.,? 2007", "shortCiteRegEx": "Vilar et al\\.", "year": 2007}, {"title": "Edinburgh\u2019s syntax-based systems at wmt 2015", "author": ["Philip Williams", "Rico Sennrich", "Maria Nadejde", "Matthias Huck", "Philipp Koehn."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 199\u2013209.", "citeRegEx": "Williams et al\\.,? 2015", "shortCiteRegEx": "Williams et al\\.", "year": 2015}, {"title": "Efficient character-level document classification by combining convolution and recurrent layers", "author": ["Yijun Xiao", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1602.00367.", "citeRegEx": "Xiao and Cho.,? 2016", "shortCiteRegEx": "Xiao and Cho.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems, pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 17, "context": "This has continued with the more recently proposed paradigm of neural machine translation, although neural networks do not suffer from character-level modelling and rather suffer from the issues specific to word-level modelling, such as the increased computational complexity from a very large target vocabulary (Jean et al., 2015; Luong et al., 2015b).", "startOffset": 312, "endOffset": 352}, {"referenceID": 25, "context": "This has continued with the more recently proposed paradigm of neural machine translation, although neural networks do not suffer from character-level modelling and rather suffer from the issues specific to word-level modelling, such as the increased computational complexity from a very large target vocabulary (Jean et al., 2015; Luong et al., 2015b).", "startOffset": 312, "endOffset": 352}, {"referenceID": 31, "context": "We represent the source side as a sequence of subwords extracted using byte-pair encoding from Sennrich et al. (2015), and vary the target side to be either a sequence of subwords or characters.", "startOffset": 95, "endOffset": 118}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015). This approach aims at building an end-toend neural network that takes as input a source sentence X = (x1, . . . , xTx) and outputs its translation Y = (y1, . . . , yTy), where xt and yt\u2032 are respectively source and target symbols. This neural network is constructed as a composite of an encoder network and a decoder network. The encoder network encodes the input sentence X into its continuous representation. In this paper, we closely follow the neural translation model proposed in Bahdanau et al. (2015) and use a bidirectional recurrent neural network, which consists of two recurrent neural networks.", "startOffset": 8, "endOffset": 540}, {"referenceID": 24, "context": "For other possible implementations, see (Luong et al., 2015a).", "startOffset": 40, "endOffset": 61}, {"referenceID": 3, "context": "Neural networks have been shown to work well with word tokens (Bengio et al., 2001; Schwenk, 2007; Mikolov et al., 2010)", "startOffset": 62, "endOffset": 120}, {"referenceID": 30, "context": "Neural networks have been shown to work well with word tokens (Bengio et al., 2001; Schwenk, 2007; Mikolov et al., 2010)", "startOffset": 62, "endOffset": 120}, {"referenceID": 26, "context": "Neural networks have been shown to work well with word tokens (Bengio et al., 2001; Schwenk, 2007; Mikolov et al., 2010)", "startOffset": 62, "endOffset": 120}, {"referenceID": 31, "context": "but also with finer units, such as subwords (Sennrich et al., 2015; Botha and Blunsom, 2014; Luong et al., 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015).", "startOffset": 44, "endOffset": 112}, {"referenceID": 5, "context": "but also with finer units, such as subwords (Sennrich et al., 2015; Botha and Blunsom, 2014; Luong et al., 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015).", "startOffset": 44, "endOffset": 112}, {"referenceID": 23, "context": "but also with finer units, such as subwords (Sennrich et al., 2015; Botha and Blunsom, 2014; Luong et al., 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015).", "startOffset": 44, "endOffset": 112}, {"referenceID": 6, "context": ", 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015).", "startOffset": 63, "endOffset": 89}, {"referenceID": 5, "context": ", 2015; Botha and Blunsom, 2014; Luong et al., 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015). Although there have been a number of previous research reporting the use of neural networks with characters (see, e.g., Mikolov et al. (2012)), the dominant approach has been to preprocess the text into a sequence of symbols, each associated with a sequence of characters, after which the neural network is presented with those symbols rather than with characters.", "startOffset": 8, "endOffset": 278}, {"referenceID": 5, "context": ", 2015; Botha and Blunsom, 2014; Luong et al., 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015). Although there have been a number of previous research reporting the use of neural networks with characters (see, e.g., Mikolov et al. (2012)), the dominant approach has been to preprocess the text into a sequence of symbols, each associated with a sequence of characters, after which the neural network is presented with those symbols rather than with characters. More recently in the context of neural machine translation, two research groups have proposed to directly use characters. Kim et al. (2015) proposed to represent each word not as a single integer index as before, but as a sequence of characters, and use", "startOffset": 8, "endOffset": 641}, {"referenceID": 32, "context": "a convolutional network followed by a highway network (Srivastava et al., 2015) to extract a continuous representation of the word.", "startOffset": 54, "endOffset": 79}, {"referenceID": 9, "context": "This approach, which effectively replaces the embedding function ex, was adopted by Costa-Juss\u00e0 and Fonollosa (2016) for neural machine translation.", "startOffset": 84, "endOffset": 117}, {"referenceID": 9, "context": "This approach, which effectively replaces the embedding function ex, was adopted by Costa-Juss\u00e0 and Fonollosa (2016) for neural machine translation. Similarly, Ling et al. (2015b) use a bidirectional recurrent neural network to replace the embedding functions ex and ey to respectively encode a character sequence to and from the corresponding continuous word representation.", "startOffset": 84, "endOffset": 180}, {"referenceID": 10, "context": "This word segmentation procedure can be as simple as tokenization followed by some punctuation normalization, but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance (Creutz and Lagus, 2005; Huang and Zhao, 2007).", "startOffset": 218, "endOffset": 264}, {"referenceID": 16, "context": "This word segmentation procedure can be as simple as tokenization followed by some punctuation normalization, but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance (Creutz and Lagus, 2005; Huang and Zhao, 2007).", "startOffset": 218, "endOffset": 264}, {"referenceID": 18, "context": "ent approach was proposed by Lee et al. (2015), where they explicitly mark each character with its relative location in a word (e.", "startOffset": 29, "endOffset": 47}, {"referenceID": 4, "context": "\u201d (Booij, 2012).", "startOffset": 2, "endOffset": 15}, {"referenceID": 35, "context": "Indeed, Vilar et al. (2007) reported worse performance when the character sequence was directly used by a phrasebased machine translation system.", "startOffset": 8, "endOffset": 28}, {"referenceID": 2, "context": "wide adoption of word-level modelling is due to the difficulty in modelling long-term dependencies with recurrent neural networks (Bengio et al., 1994; Hochreiter, 1998).", "startOffset": 130, "endOffset": 169}, {"referenceID": 15, "context": "wide adoption of word-level modelling is due to the difficulty in modelling long-term dependencies with recurrent neural networks (Bengio et al., 1994; Hochreiter, 1998).", "startOffset": 130, "endOffset": 169}, {"referenceID": 10, "context": ", Creutz and Lagus (2005) for Finnish and other morphologically rich languages and Huang and Zhao (2007) for Chinese).", "startOffset": 2, "endOffset": 26}, {"referenceID": 10, "context": ", Creutz and Lagus (2005) for Finnish and other morphologically rich languages and Huang and Zhao (2007) for Chinese).", "startOffset": 2, "endOffset": 105}, {"referenceID": 5, "context": "Each of those variants will be then a composite of the lexeme vector (shared across these variants) and morpheme vectors (shared across words sharing the same suffix, for example) (Botha and Blunsom, 2014).", "startOffset": 180, "endOffset": 205}, {"referenceID": 14, "context": "Furthermore, in recent years, we have learned how to build and train a recurrent neural network that can well capture long-term dependencies by using more sophisticated activation functions, such as long short-term memory units (Hochreiter and Schmidhuber, 1997) and gated recurrent units (Cho et al.", "startOffset": 228, "endOffset": 262}, {"referenceID": 7, "context": "Furthermore, in recent years, we have learned how to build and train a recurrent neural network that can well capture long-term dependencies by using more sophisticated activation functions, such as long short-term memory units (Hochreiter and Schmidhuber, 1997) and gated recurrent units (Cho et al., 2014).", "startOffset": 289, "endOffset": 307}, {"referenceID": 21, "context": "Ling et al. (2015b) indeed states that \u201c[m]uch of the prior information regarding morphology, cognates and rare word translation among others, should be incorporated.", "startOffset": 0, "endOffset": 20}, {"referenceID": 26, "context": "For instance, Mikolov et al. (2012) and Sutskever et al.", "startOffset": 14, "endOffset": 36}, {"referenceID": 26, "context": "For instance, Mikolov et al. (2012) and Sutskever et al. (2011) trained a recurrent neural network language model (RNN-LM) on character sequences.", "startOffset": 14, "endOffset": 64}, {"referenceID": 26, "context": "For instance, Mikolov et al. (2012) and Sutskever et al. (2011) trained a recurrent neural network language model (RNN-LM) on character sequences. The latter showed that it is possible to generate sensible text sequences by simply sampling a character at a time from this model. More recently, Zhang et al. (2015) and Xiao and Cho (2016) successfully applied a convolutional net and a convolutionalrecurrent net respectively to character-level document classification without any explicit segmentation.", "startOffset": 14, "endOffset": 314}, {"referenceID": 26, "context": "For instance, Mikolov et al. (2012) and Sutskever et al. (2011) trained a recurrent neural network language model (RNN-LM) on character sequences. The latter showed that it is possible to generate sensible text sequences by simply sampling a character at a time from this model. More recently, Zhang et al. (2015) and Xiao and Cho (2016) successfully applied a convolutional net and a convolutionalrecurrent net respectively to character-level document classification without any explicit segmentation.", "startOffset": 14, "endOffset": 338}, {"referenceID": 8, "context": "neural network, inspired by the gated-feedback network from Chung et al. (2015), called a biscale recurrent neural network.", "startOffset": 60, "endOffset": 80}, {"referenceID": 31, "context": "For evaluation, we represent a source sentence as a sequence of subword symbols extracted by bytepair encoding (BPE, Sennrich et al. (2015)) and a", "startOffset": 117, "endOffset": 140}, {"referenceID": 12, "context": "(1) Freitag et al. (2014). (2, 6) Williams et al.", "startOffset": 4, "endOffset": 26}, {"referenceID": 12, "context": "(1) Freitag et al. (2014). (2, 6) Williams et al. (2015).", "startOffset": 4, "endOffset": 57}, {"referenceID": 11, "context": "(3, 5) Durrani et al. (2014). (4) Haddow et al.", "startOffset": 7, "endOffset": 29}, {"referenceID": 11, "context": "(3, 5) Durrani et al. (2014). (4) Haddow et al. (2015). (7) Rubino et al.", "startOffset": 7, "endOffset": 55}, {"referenceID": 11, "context": "(3, 5) Durrani et al. (2014). (4) Haddow et al. (2015). (7) Rubino et al. (2015).", "startOffset": 7, "endOffset": 81}, {"referenceID": 19, "context": "descent with Adam (Kingma and Ba, 2014).", "startOffset": 18, "endOffset": 39}, {"referenceID": 28, "context": "The norm of the gradient is clipped with a threshold 1 (Pascanu et al., 2013).", "startOffset": 55, "endOffset": 77}, {"referenceID": 1, "context": "The authors would like to thank the developers of Theano (Bastien et al., 2012).", "startOffset": 57, "endOffset": 79}], "year": 2016, "abstractText": "The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder\u2013 decoder with a subword-level encoder and a character-level decoder on four language pairs\u2013En-Cs, En-De, En-Ru and En-Fi\u2013 using the parallel corpora from WMT\u201915. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.", "creator": "TeX"}}}