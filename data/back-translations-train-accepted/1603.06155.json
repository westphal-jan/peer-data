{"id": "1603.06155", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "A Persona-Based Neural Conversation Model", "abstract": "We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gain in speaker consistency as measured by human judges.", "histories": [["v1", "Sat, 19 Mar 2016 23:15:18 GMT  (68kb,D)", "http://arxiv.org/abs/1603.06155v1", "10 pages"], ["v2", "Wed, 8 Jun 2016 17:19:58 GMT  (68kb,D)", "http://arxiv.org/abs/1603.06155v2", "Accepted for publication at ACL 2016"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "michel galley", "chris brockett", "georgios p spithourakis", "jianfeng gao", "william b dolan"], "accepted": true, "id": "1603.06155"}, "pdf": {"name": "1603.06155.pdf", "metadata": {"source": "CRF", "title": "A Persona-Based Neural Conversation Model", "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "emails": ["jiweil@stanford.edu", "mgalley@microsoft.com", "chrisbkt@microsoft.com", "jfgao@microsoft.com", "billdol@microsoft.com"], "sections": [{"heading": null, "text": "We present persona-based models for dealing with the question of speaker consistency in the generation of neural responses. A speaker model encodes personas in distributed embedding that capture individual characteristics such as background information and speech style. A dyadic speaker-addressee model captures the characteristics of interactions between two interlocutors. Our models lead to qualitative performance improvements in both helplessness and BLEU values compared to base sequence-to-sequence models, whereby the consistency of the speakers, measured by human judges, increases similarly."}, {"heading": "1 Introduction", "text": "In fact, we often discuss a consensus response from the people represented in the data we train (Li et al., 2015), and if they don't, they can be wildly inconsistent. In this paper, we will address the challenge of consistency and how they confer the coherent \"persona\" needed to model human behavior."}, {"heading": "2 Related Work", "text": "This work follows the line of investigation initiated by Ritter et al. (2011), which treats the generation of dialogue in conversation Q Schatzoni as a statistical machine translation (SMT) problem. Ritter et al. (2011) represents a break with previous and contemporary dialog work, which relies extensively on hand-coded rules that typically build statistical models based on heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning rules from a minimal set of authorized rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Chen et al., 2013; SMen et al., 2015)."}, {"heading": "3 Sequence-to-Sequence Models", "text": "Given a sequence of inputs X = {x1, x2,..., xnX}, an LSTM concatenates each time step with an input gate, a memory gate, and an output gate, each of which is designated as it, ft, and ot. We distinguish e and h, where et denotes the vector for a single text unit (for example, a word or a sentence) at the time t, while ht denotes the vector computed by the LSTM model at the time t by combining et and ht \u2212 1. ct, where et is the cell state vector at the time t, and \u03c3 denotes the sigmoid function. Then, the vector representation ht for each time concept is denoted by: it ft ot lt = circuit diagram tanh W \u00b7 [ht \u2212 1est] (1) ct \u2212 ct \u2212 1 + it \u00b7 lt (2) hst = ot \u00b7 tanh (ct) h (ct) (3), where the vector representation for each time graph is represented by t = circuit diagram."}, {"heading": "4 Personalized Response Generation", "text": "Our work presents two persona-based models: the speaker model, which models the personality of the respondent, and the speaker-address model, which models the way the respondent adapts his speech to a particular addressee - a linguistic phenomenon known as lexical extraction (Deutsch und Pechmann, 1982)."}, {"heading": "4.1 Notation", "text": "For the task of response generation, let M specify the input word sequence (message) M = {m1, m2,..., mI}. R denotes the word sequence as an answer to M, where R = {r1, r2,..., rJ, EOS} and J is the length of the answer (terminated by an EOS token). rt stands for a word symbol associated with a K-dimensional unique word embed et. V is the vocabulary size."}, {"heading": "4.2 Speaker Model", "text": "This model represents each loudspeaker as a vector or embedding that encodes the loudspeaker-specific information (e.g. dialect, register, age, gender, personal information) that influences the content and style of their responses. 2Figure 1 gives a brief representation of the loudspeaker model. Each loudspeaker is associated with a user-specific representation."}, {"heading": "4.3 Speaker-Addressee Model", "text": "A natural extension of the speaker model is a model that is sensitive to patterns of interaction between speaker and receiver within the conversation. In fact, speaking style, registration and content differ not only from the identity of the speaker, but also from that of the receiver. In scripts for the TV series Friends, which are used in some of our experiments, the speaker often speaks differently to his sister Monica than to Rachel, with whom he is in a repetitive relationship throughout the series.The proposed speaker-addressee model works as follows: We want to predict how the speaker would respond to a message produced by the speaker. Similar to the speaker model, we associate each speaker with a K-dimensional representation of the speaker, namely vi for the user i and vj for the user j. We get an interactive representation Vi, j RK \u00d7 1 by a linear combination of user vectors vi and vj in an attempt to model the interactive style of the user i towards the user."}, {"heading": "4.4 Decoding and Reranking", "text": "For decoding, the N leaderboards are created using the decoder with beam size B = 200. We set a maximum length of 20 for the created candidates. The decoding works as follows: At each step, we first check all possible B \u00b7 B candidates for the next word and add all hypotheses ending with an EOS token. Then, we preserve the unfinished Top B hypotheses and move on to the next word position. To solve the problem that SEQ2SEQ models tend to generate generic and everyday answers that I do not know, we follow Li et al. (2015) by recalculating the generated N leaderboard using a scoring function that associates a length penalty with the probability of the source of the given target: log p (R | M, v) + \u03bb log p (M | R) + \u03b3 | R | (11), where p (R | v) means the probability of SET-generated messages using SET-Q and SEv-Q-Q-generated messages (SET-Q-Q) and Q-Q-generated messages (SEv)."}, {"heading": "5 Datasets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Twitter Persona Dataset", "text": "Data Acquisition Training data for the Speaker Model was extracted from Twitter FireHose for the six-month period beginning January 1, 2012 \u2022 in other words, users who were relatively frequently involved in conversations, resulting in a number of 74,003 users participating in at least 60 (and no more than 300) calls (average: 92.24, median: 90). The data set extracted from the responses of these \"conversation leaders\" included 24,725,711 sliding windows with three spins (context message response) calls. In addition, we examined 12,000 conversations with three spins from the same user set from Twitter FireHose for the three-month period beginning July 1, 2012, and set them aside as development, validation, and test runs of two spins each."}, {"heading": "5.2 Twitter Sordoni Dataset", "text": "The Twitter Persona dataset was collected for this work for experiments with Speaker ID information. To get a comparison point with previous state-of-the-art work (Sordoni et al., 2015; Li et al., 2015), we measure our base model (non-persona) LSTM with previous work on the dataset of (Sordoni et al., 2015), which we call the Twitter Sordoni dataset. We only use its test set part, which contains responses for 2114 contexts and messages. It is important to note that the Sordoni dataset contains up to 10 references per message, while the Twitter Persona dataset has only one reference per message. BLEU values cannot therefore be compared across the two Twitter datasets (BLEU values for 10 references are generally much higher than with 1 reference). Details of this dataset are in (Sordoni et al., 2015)."}, {"heading": "5.3 Television Series Transcripts", "text": "Data Collection For the dyadic SpeakerAddressee Model, we used scripts from the American television comedies Friends3 and The Big Bang Theory, 4 available in the Internet Movie Script Database (IMSDb).5 We collected 13 main characters from the two series in a corpus of 69,565 turns. We divided the corpus into training, development and test sets, each with development and test sets of about 2,000 turns. Training Since the relatively small size of the data set does not allow for an open domain dialogue model, we chose a domain adaptation strategy in which we first used a standard SEQ2SEQ model using a much larger OpenSubtitle (OSDb) dataset (Tiedemann, 2009) and then adapted the pre-trained model to the TV series dataset.The OSDb dataset is large, noisy, open-domain dataset with approximately 60M-70M scripted movie characters."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Evaluation", "text": "In the following (Sordoni et al., 2015; Li et al., 2015), we used BLEU (Papineni et al., 2002) for setting and evaluating parameters. BLEU has been shown to correlate well with human judgment about the task of reaction generation, as shown in (Galley et al., 2015). In addition to the BLEU values, we also report helplessness, which is widely used as an indicator of modeling ability."}, {"heading": "6.2 Baseline", "text": "Since our main experiments are conducted with a new data set (the Twitter Persona Dataset), we first show that our LSTM baseline competes with the state of the art (Li et al., 2015) on an established data set, the Twitter Sordoni Dataset (Sordoni et al., 2015). Our starting point is simply our implementation of the LSTMMI of (Li et al., 2015), so the results should be relatively close to the reported results. Table 2 summarizes our results compared to previous work. We see that our system actually performs better than (Li et al., 2015), and we attribute the improvement to a larger training corpus, the use of dropouts during training, and possibly the \"conversationalist\" nature of our corpus."}, {"heading": "6.3 Results", "text": "In terms of BLEU values (Table 4), a significant power boost is observed for the speaker model compared to the standard SEQ2SEQ model, resulting in an increase of 21% in the maximum probability setting (MLE) and 11.7% in the mutual information setting (MMI). In line with the results in (Li et al., 2015), we observe a consistent power boost introduced by the MMI data function over a standard SEQ2SEQ model. It is worth noting that our Persona models are more advantageous for the MLE models than for the MMI models. This result is intuitive, as the Persona models contribute to the standard LSTM data models distorting the data outputs and the use of MMI models being less critical than the MMI models."}, {"heading": "6.4 Qualitative Analysis", "text": "We randomly selected 10 speakers (without cherry-picking) from the original Twitter data set. We gathered their user representations from a speaker look-up table and integrated them into the decoding models. However, since we cannot expect crowdsourced human judges to know or attempt to learn the basic truth of Twitter users who are not well-known public figures, we have conducted our experiment to evaluate the consistency of outputs from the speaker model by using a crowdsourcing service. Since we cannot expect crowdsourced human judges to know or attempt to learn the basic truth of Twitter users who are not well known, we have assessed the consistency of outputs associated with the speaker IDs. To this end, we have collected 24 pairs of questions for which we expect answers to be consistent when the persona model is coherent."}, {"heading": "7 Conclusions", "text": "There are many other aspects of speaking behavior, such as mood and emotion, that we have not examined here, but these are outside the scope of the current work and must be left to future work. Although the benefits presented by our new models are not spectacular, the systems exceed our SEQ2SEQ base systems in terms of BLEU, perplexity, and human assessments of speaker consistency. We have shown that we are able to capture certain personal characteristics, such as speaking style and background information, by encoding personas into distributed representations. In the speaker address model, the evidence also suggests that it is beneficial to capture dynamic interactions. Our ultimate goal is to be able to record the profile of any person whose identity is not known in advance, and to generate conversations that accurately mimic that person's persona in terms of speech response behavior and other prominent features."}, {"heading": "Acknowledgments", "text": "We thank Dan Jurafsky, Pushmeet Kohli, Chris Quirk, Alan Ritter and George Spithourakis for helpful discussions about this work."}], "references": [{"title": "Luke, I am your father: dealing with out-of-domain requests by using movies subtitles", "author": ["David Ameixa", "Luisa Coheur", "Pedro Fialho", "Paulo Quaresma."], "venue": "Intelligent Virtual Agents, pages 13\u201321. Springer.", "citeRegEx": "Ameixa et al\\.,? 2014", "shortCiteRegEx": "Ameixa et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "IRIS: a chatoriented dialogue system based on the vector space model", "author": ["Rafael E Banchs", "Haizhou Li."], "venue": "Proc. of the ACL 2012 System Demonstrations, pages 37\u201342.", "citeRegEx": "Banchs and Li.,? 2012", "shortCiteRegEx": "Banchs and Li.", "year": 2012}, {"title": "An empirical investigation of sparse log-linear models for improved dialogue act classification", "author": ["Yun-Nung Chen", "Wei Yu Wang", "Alexander Rudnicky."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8317\u2013", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Social interaction and the development of definite descriptions", "author": ["Werner Deutsch", "Thomas Pechmann."], "venue": "Cognition, 11:159\u2013184.", "citeRegEx": "Deutsch and Pechmann.,? 1982", "shortCiteRegEx": "Deutsch and Pechmann.", "year": 1982}, {"title": "\u2206BLEU: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of ACL-IJCNLP, pages", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng."], "venue": "Proc. of ACL, pages 699\u2013709, Baltimore, Maryland.", "citeRegEx": "Gao et al\\.,? 2014", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "User modeling in dialog systems: Potentials and hazards", "author": ["Alfred Kobsa."], "venue": "AI & society, 4(3):214\u2013231.", "citeRegEx": "Kobsa.,? 1990", "shortCiteRegEx": "Kobsa.", "year": 1990}, {"title": "A stochastic model of human-machine interaction for learning dialog strategies", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert."], "venue": "IEEE Transactions on Speech and Audio Processing, 8(1):11\u201323.", "citeRegEx": "Levin et al\\.,? 2000", "shortCiteRegEx": "Levin et al\\.", "year": 2000}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1510.03055.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "All the world\u2019s a stage: Learning character models from film", "author": ["Grace I Lin", "Marilyn A Walker."], "venue": "Proceedings of the Seventh AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE).", "citeRegEx": "Lin and Walker.,? 2011", "shortCiteRegEx": "Lin and Walker.", "year": 2011}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proc. of ACL, pages 11\u201319, Beijing, China, July.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Developing non-goal dialog system based on examples of drama television", "author": ["Lasguido Nio", "Sakriani Sakti", "Graham Neubig", "Tomoki Toda", "Mirna Adriani", "Satoshi Nakamura."], "venue": "Natural Interaction with Robots, Knowbots and Smartphones, pages 355\u2013361. Springer.", "citeRegEx": "Nio et al\\.,? 2014", "shortCiteRegEx": "Nio et al\\.", "year": 2014}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160\u2013167, Sapporo, Japan, July. Association for Computational Linguistics.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Stochastic language generation for spoken dialogue systems", "author": ["Alice H Oh", "Alexander I Rudnicky."], "venue": "Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems-Volume 3, pages 27\u201332.", "citeRegEx": "Oh and Rudnicky.,? 2000", "shortCiteRegEx": "Oh and Rudnicky.", "year": 2000}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proc. of ACL, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Are we there yet? research in commercial spoken dialog systems", "author": ["Roberto Pieraccini", "David Suendermann", "Krishna Dayanidhi", "Jackson Liscombe."], "venue": "Text, Speech and Dialogue, pages 3\u201313. Springer.", "citeRegEx": "Pieraccini et al\\.,? 2009", "shortCiteRegEx": "Pieraccini et al\\.", "year": 2009}, {"title": "Trainable approaches to surface natural language generation and their application to conversational dialog systems", "author": ["Adwait Ratnaparkhi."], "venue": "Computer Speech & Language, 16(3):435\u2013455.", "citeRegEx": "Ratnaparkhi.,? 2002", "shortCiteRegEx": "Ratnaparkhi.", "year": 2002}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 583\u2013593.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Effects of the user model on simulation-based learning of dialogue strategies", "author": ["Jost Schatztnann", "Matthew N Stuttle", "Karl Weilhammer", "Steve Young."], "venue": "Automatic Speech Recognition and Understanding, 2005 IEEE Workshop on, pages 220\u2013225.", "citeRegEx": "Schatztnann et al\\.,? 2005", "shortCiteRegEx": "Schatztnann et al\\.", "year": 2005}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proc. of AAAI.", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "ACL-IJCNLP, pages 1577\u20131586.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of NAACL-HLT.", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems (NIPS), pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "News from OPUS \u2013 a collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "Recent advances in natural language processing, volume 5, pages 237\u2013248.", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "Proc. of ICML Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "User models in dialog systems", "author": ["Wolfgang Wahlster", "Alfred Kobsa."], "venue": "Springer.", "citeRegEx": "Wahlster and Kobsa.,? 1989", "shortCiteRegEx": "Wahlster and Kobsa.", "year": 1989}, {"title": "A trainable generator for recommendations in multimodal dialog", "author": ["Marilyn A Walker", "Rashmi Prasad", "Amanda Stent."], "venue": "INTERSPEECH.", "citeRegEx": "Walker et al\\.,? 2003", "shortCiteRegEx": "Walker et al\\.", "year": 2003}, {"title": "Perceived or not perceived: Film character models for expressive nlg", "author": ["Marilyn A Walker", "Ricky Grant", "Jennifer Sawyer", "Grace I Lin", "Noah Wardrip-Fruin", "Michael Buell."], "venue": "Interactive Storytelling, pages 109\u2013 121. Springer.", "citeRegEx": "Walker et al\\.,? 2011", "shortCiteRegEx": "Walker et al\\.", "year": 2011}, {"title": "An annotated corpus of film dialogue for learning and characterizing character style", "author": ["Marilyn A Walker", "Grace I Lin", "Jennifer Sawyer."], "venue": "LREC, pages 1373\u20131378.", "citeRegEx": "Walker et al\\.,? 2012", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "Improving spoken dialogue understanding using phonetic mixture models", "author": ["William Yang Wang", "Ron Artstein", "Anton Leuski", "David Traum."], "venue": "FLAIRS Conference.", "citeRegEx": "Wang et al\\.,? 2011", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young."], "venue": "Proc. of EMNLP, pages 1711\u20131721, Lisbon, Portugal, September. Asso-", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "The hidden information state model: A practical framework for pomdp-based spoken dialogue management", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Simon Keizer", "Fran\u00e7ois Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu."], "venue": "Computer Speech & Language, 24(2):150\u2013", "citeRegEx": "Young et al\\.,? 2010", "shortCiteRegEx": "Young et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 19, "context": "As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2015).", "startOffset": 196, "endOffset": 278}, {"referenceID": 23, "context": "As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2015).", "startOffset": 196, "endOffset": 278}, {"referenceID": 26, "context": "As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2015).", "startOffset": 196, "endOffset": 278}, {"referenceID": 10, "context": "As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2015).", "startOffset": 196, "endOffset": 278}, {"referenceID": 10, "context": "Outputs are frequently vague or non-committal as discussed in (Li et al., 2015), and when not, they can be wildly inconsistent, as illustrated in Table 1.", "startOffset": 62, "endOffset": 79}, {"referenceID": 26, "context": "(Vinyals and Le, 2015) suggest that the lack of a coherent personality makes it impossible for current systems to pass the Turing test.", "startOffset": 0, "endOffset": 22}, {"referenceID": 26, "context": "Fortunately, sequence-to-sequence (SEQ2SEQ) models of conversation generation (Vinyals and Le, 2015; Li et al., 2015) provide a straightforward mechanism for incorporating personas as embeddings.", "startOffset": 78, "endOffset": 117}, {"referenceID": 10, "context": "Fortunately, sequence-to-sequence (SEQ2SEQ) models of conversation generation (Vinyals and Le, 2015; Li et al., 2015) provide a straightforward mechanism for incorporating personas as embeddings.", "startOffset": 78, "endOffset": 117}, {"referenceID": 9, "context": "(2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al.", "startOffset": 201, "endOffset": 306}, {"referenceID": 33, "context": "(2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al.", "startOffset": 201, "endOffset": 306}, {"referenceID": 28, "context": "(2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al.", "startOffset": 201, "endOffset": 306}, {"referenceID": 17, "context": "(2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al.", "startOffset": 201, "endOffset": 306}, {"referenceID": 31, "context": "(2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al.", "startOffset": 201, "endOffset": 306}, {"referenceID": 15, "context": ", 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).", "startOffset": 84, "endOffset": 205}, {"referenceID": 18, "context": ", 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).", "startOffset": 84, "endOffset": 205}, {"referenceID": 2, "context": ", 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).", "startOffset": 84, "endOffset": 205}, {"referenceID": 0, "context": ", 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).", "startOffset": 84, "endOffset": 205}, {"referenceID": 13, "context": ", 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).", "startOffset": 84, "endOffset": 205}, {"referenceID": 3, "context": ", 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).", "startOffset": 84, "endOffset": 205}, {"referenceID": 32, "context": "More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization.", "startOffset": 14, "endOffset": 32}, {"referenceID": 7, "context": ", 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization.", "startOffset": 50, "endOffset": 84}, {"referenceID": 9, "context": "This work follows the line of investigation initiated by Ritter et al. (2011) who treat generation of conversational dialog as a statistical machine translation (SMT) problem.", "startOffset": 57, "endOffset": 78}, {"referenceID": 9, "context": "This work follows the line of investigation initiated by Ritter et al. (2011) who treat generation of conversational dialog as a statistical machine translation (SMT) problem. Ritter et al. (2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al.", "startOffset": 57, "endOffset": 197}, {"referenceID": 24, "context": "Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation.", "startOffset": 64, "endOffset": 149}, {"referenceID": 6, "context": "Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation.", "startOffset": 64, "endOffset": 149}, {"referenceID": 1, "context": "Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation.", "startOffset": 64, "endOffset": 149}, {"referenceID": 12, "context": "Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation.", "startOffset": 64, "endOffset": 149}, {"referenceID": 21, "context": "Other researchers have recently used SEQ2SEQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015).", "startOffset": 138, "endOffset": 201}, {"referenceID": 22, "context": "Other researchers have recently used SEQ2SEQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015).", "startOffset": 138, "endOffset": 201}, {"referenceID": 26, "context": "Other researchers have recently used SEQ2SEQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015).", "startOffset": 138, "endOffset": 201}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation. Sordoni et al. (2015) augments Ritter et al.", "startOffset": 8, "endOffset": 177}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation. Sordoni et al. (2015) augments Ritter et al. (2011) by rescoring outputs using a SEQ2SEQ model conditioned on conversation history.", "startOffset": 8, "endOffset": 207}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation. Sordoni et al. (2015) augments Ritter et al. (2011) by rescoring outputs using a SEQ2SEQ model conditioned on conversation history. Other researchers have recently used SEQ2SEQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015). Serban et al. (2015) propose a hierarchical neural model aimed at capturing dependencies over an extended conversation history.", "startOffset": 8, "endOffset": 511}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation. Sordoni et al. (2015) augments Ritter et al. (2011) by rescoring outputs using a SEQ2SEQ model conditioned on conversation history. Other researchers have recently used SEQ2SEQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015). Serban et al. (2015) propose a hierarchical neural model aimed at capturing dependencies over an extended conversation history. Recent work by Li et al. (2015) measures mutual information between message and response in order to reduce the proportion of generic responses typical of SEQ2SEQ systems.", "startOffset": 8, "endOffset": 650}, {"referenceID": 27, "context": ", (Wahlster and Kobsa, 1989; Kobsa, 1990; Schatztnann et al., 2005; Lin and Walker, 2011)).", "startOffset": 2, "endOffset": 89}, {"referenceID": 8, "context": ", (Wahlster and Kobsa, 1989; Kobsa, 1990; Schatztnann et al., 2005; Lin and Walker, 2011)).", "startOffset": 2, "endOffset": 89}, {"referenceID": 20, "context": ", (Wahlster and Kobsa, 1989; Kobsa, 1990; Schatztnann et al., 2005; Lin and Walker, 2011)).", "startOffset": 2, "endOffset": 89}, {"referenceID": 11, "context": ", (Wahlster and Kobsa, 1989; Kobsa, 1990; Schatztnann et al., 2005; Lin and Walker, 2011)).", "startOffset": 2, "endOffset": 89}, {"referenceID": 30, "context": "Since generating meaningful responses in a open-domain scenario is intrinsically difficult in conventional dialog systems, existing models often focus on generalizing character style on the basis of qualitative statistical analysis (Walker et al., 2012; Walker et al., 2011).", "startOffset": 232, "endOffset": 274}, {"referenceID": 29, "context": "Since generating meaningful responses in a open-domain scenario is intrinsically difficult in conventional dialog systems, existing models often focus on generalizing character style on the basis of qualitative statistical analysis (Walker et al., 2012; Walker et al., 2011).", "startOffset": 232, "endOffset": 274}, {"referenceID": 8, "context": ", (Wahlster and Kobsa, 1989; Kobsa, 1990; Schatztnann et al., 2005; Lin and Walker, 2011)). Since generating meaningful responses in a open-domain scenario is intrinsically difficult in conventional dialog systems, existing models often focus on generalizing character style on the basis of qualitative statistical analysis (Walker et al., 2012; Walker et al., 2011). The present work, by contrast, is in the vein of the SEQ2SEQ models of Vinyals and Le (2015) and Li et al.", "startOffset": 16, "endOffset": 461}, {"referenceID": 8, "context": ", (Wahlster and Kobsa, 1989; Kobsa, 1990; Schatztnann et al., 2005; Lin and Walker, 2011)). Since generating meaningful responses in a open-domain scenario is intrinsically difficult in conventional dialog systems, existing models often focus on generalizing character style on the basis of qualitative statistical analysis (Walker et al., 2012; Walker et al., 2011). The present work, by contrast, is in the vein of the SEQ2SEQ models of Vinyals and Le (2015) and Li et al. (2015), enriching these models by training persona vectors directly from conversational data and relevant side-information, and incorporating these directly into the decoder.", "startOffset": 16, "endOffset": 482}, {"referenceID": 4, "context": "Our work introduces two persona-based models: the Speaker Model, which models the personality of the respondent, and the Speaker-Addressee Model which models the way the respondent adapts their speech to a given addressee \u2014 a linguistic phenomenon known as lexical entrainment (Deutsch and Pechmann, 1982).", "startOffset": 277, "endOffset": 305}, {"referenceID": 10, "context": "To deal with the issue that SEQ2SEQ models tend to generate generic and commonplace responses such as I don\u2019t know, we follow Li et al. (2015) by reranking the generated N-best list using a scoring function that linearly combines a length penalty and the log likelihood of source given target:", "startOffset": 126, "endOffset": 143}, {"referenceID": 14, "context": "We optimize \u03b3 and \u03bb on N-best lists of response candidates generated from the development set using MERT (Och, 2003) by optimizing BLEU.", "startOffset": 105, "endOffset": 116}, {"referenceID": 24, "context": "Training Protocols We trained four-layer SEQ2SEQ models on the Twitter corpus following the approach of (Sutskever et al., 2014).", "startOffset": 104, "endOffset": 128}, {"referenceID": 23, "context": "To obtain a point of comparison with prior state-ofthe-art work (Sordoni et al., 2015; Li et al., 2015), we measure our baseline (non-persona) LSTM model against prior work on the dataset of (Sordoni et al.", "startOffset": 64, "endOffset": 103}, {"referenceID": 10, "context": "To obtain a point of comparison with prior state-ofthe-art work (Sordoni et al., 2015; Li et al., 2015), we measure our baseline (non-persona) LSTM model against prior work on the dataset of (Sordoni et al.", "startOffset": 64, "endOffset": 103}, {"referenceID": 23, "context": ", 2015), we measure our baseline (non-persona) LSTM model against prior work on the dataset of (Sordoni et al., 2015), which we call the Twitter Sordoni Dataset.", "startOffset": 95, "endOffset": 117}, {"referenceID": 23, "context": "Details of this dataset are in (Sordoni et al., 2015).", "startOffset": 31, "endOffset": 53}, {"referenceID": 25, "context": "Training Since the relatively small size of the dataset does not allow for training an open domain dialog model, we adopted a domain adaption strategy where we first trained a standard SEQ2SEQ models using a much larger OpenSubtitles (OSDb) dataset (Tiedemann, 2009), and then adapting the pre-trained model to the TV series dataset.", "startOffset": 249, "endOffset": 266}, {"referenceID": 19, "context": "MT baseline (Ritter et al., 2011) 3.", "startOffset": 12, "endOffset": 33}, {"referenceID": 10, "context": "Standard LSTM MMI (Li et al., 2015) 5.", "startOffset": 18, "endOffset": 35}, {"referenceID": 19, "context": "We contrast our baseline against an SMT baseline (Ritter et al., 2011), and the best result (Li et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 10, "context": ", 2011), and the best result (Li et al., 2015) on the established dataset of (Sordoni et al.", "startOffset": 29, "endOffset": 46}, {"referenceID": 23, "context": ", 2015) on the established dataset of (Sordoni et al., 2015).", "startOffset": 38, "endOffset": 60}, {"referenceID": 23, "context": "Following (Sordoni et al., 2015; Li et al., 2015) we used BLEU (Papineni et al.", "startOffset": 10, "endOffset": 49}, {"referenceID": 10, "context": "Following (Sordoni et al., 2015; Li et al., 2015) we used BLEU (Papineni et al.", "startOffset": 10, "endOffset": 49}, {"referenceID": 16, "context": ", 2015) we used BLEU (Papineni et al., 2002) for parameter tuning and evaluation.", "startOffset": 21, "endOffset": 44}, {"referenceID": 5, "context": "BLEU has been shown to correlate well with human judgment on the response generation task, as demonstrated in (Galley et al., 2015).", "startOffset": 110, "endOffset": 131}, {"referenceID": 10, "context": "Since our main experiments are with a new dataset (the Twitter Persona Dataset), we first show that our LSTM baseline is competitive with the state-of-theart (Li et al., 2015) on an established dataset, the Twitter Sordoni Dataset (Sordoni et al.", "startOffset": 158, "endOffset": 175}, {"referenceID": 23, "context": ", 2015) on an established dataset, the Twitter Sordoni Dataset (Sordoni et al., 2015).", "startOffset": 63, "endOffset": 85}, {"referenceID": 10, "context": "Our baseline is simply our implementation of the LSTMMMI of (Li et al., 2015), so results should be relatively close to their reported results.", "startOffset": 60, "endOffset": 77}, {"referenceID": 10, "context": "We see that our system actually does better than (Li et al., 2015), and we attribute the improvement to a larger training corpus, the use of dropout during training, and possibly to the \u201cconversationalist\u201d nature of our corpus.", "startOffset": 49, "endOffset": 66}, {"referenceID": 10, "context": "In line with findings in (Li et al., 2015), we observe a consistent performance boost introduced by the MMI objective function over a standard SEQ2SEQ model based on the MLE objective function.", "startOffset": 25, "endOffset": 42}], "year": 2017, "abstractText": "We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gain in speaker consistency as measured by human judges.", "creator": "TeX"}}}