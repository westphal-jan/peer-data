{"id": "1603.06318", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Harnessing Deep Neural Networks with Logic Rules", "abstract": "Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.", "histories": [["v1", "Mon, 21 Mar 2016 03:33:20 GMT  (172kb,D)", "http://arxiv.org/abs/1603.06318v1", "18 pages, 3 figures. In submission"], ["v2", "Thu, 14 Apr 2016 05:28:21 GMT  (174kb,D)", "http://arxiv.org/abs/1603.06318v2", "Fix typos; add more references and disucssions (Sec.6 etc). 19 pages, 3 figures. In submission"], ["v3", "Tue, 19 Jul 2016 23:30:48 GMT  (174kb,D)", "http://arxiv.org/abs/1603.06318v3", "To appear in ACL2016"], ["v4", "Tue, 15 Nov 2016 21:41:21 GMT  (174kb,D)", "http://arxiv.org/abs/1603.06318v4", "Fix typos and experiment setting"]], "COMMENTS": "18 pages, 3 figures. In submission", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL stat.ML", "authors": ["zhiting hu", "xuezhe ma", "zhengzhong liu", "eduard h hovy", "eric p xing"], "accepted": true, "id": "1603.06318"}, "pdf": {"name": "1603.06318.pdf", "metadata": {"source": "CRF", "title": "Harnessing Deep Neural Networks with Logic Rules", "authors": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eric P. Xing"], "emails": ["zhitingh@cs.cmu.edu", "xuezhem@cs.cmu.edu", "liu@cs.cmu.edu", "hovy@cmu.edu", "epxing@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is a purely mental game, which is about putting people at the centre of attention, not about cornering them."}, {"heading": "2 Related Work", "text": "Given the intuitive value of combining logical rules and neural networks, various different forms of such a combination have been considered in different contexts. Neural symbolic systems (Garcez et al., 2012), such as KBANN (Towell et al., 1990) and CILP + + (Franc-a et al., 2014), construct network architectures from predetermined rules to conduct reasoning and knowledge acquisition. A related research direction, such as Markov Logics Networks (Richardson and Domingos, 2006), derives probabilistic graphic models (rather than neural networks) from the rulebook. With the recent success of deep neural networks in a variety of fields of application, it is increasingly desirable to integrate structured logical knowledge into general network types in order to utilize flexibility and reduce unpredictability. Recent work that trains on extraction of domain knowledge (Collobert et al., 2011), while producing improved results, \"does not apply the general image to a specific language\" (i.e., does not apply the \")."}, {"heading": "3 Method", "text": "In this section, we present our framework that summarizes the logically structured knowledge in a neural network, which is achieved by forcing the network to mimic the predictions of a rule-regulated teacher, and by iteratively developing both models during training (Section 3.2).The process is agnostic to the network architecture and therefore applicable to general types of neural models such as CNNs and RNNNs. We construct the teacher network in each iteration by adapting the principle of posterior regulation in our logical constraint setting (Section 3.3), where our formulation provides a coherent solution. Figure 1 shows an overview of the proposed framework."}, {"heading": "3.1 Learning Resources: Instances and Rules", "text": "Our approach allows neural networks to learn both from specific examples and from general rules. Here, we specify the settings of these \"learning resources.\" Suppose we have the input variable x-X and the target variable y-Y. For clarification, we focus on the K-path classification, where Y = \u2206 K is the K-dimensional probability simplex and y-dimensional probability (e.g. NER tagging, which is typically a sequence of classification decisions). Training data D = {(xn, yn)} Nn = 1 are a series of instances of (x, y). Let us also consider a series of first-order logic (FOL), which is typically a sequence of classification decisions. Training data D = {(xn, yn)} Nn = 1 are a series of instances of (x, y)."}, {"heading": "3.2 Rule Knowledge Distillation", "text": "A neural network defines a conditional probability p\u03b8 (y | x) by using a softmax q output layer that produces a K-dimensional soft prediction vector called \u03c3\u03b8 (x). The network is parameterized by weights \u03b8. Previous neural network training has been updated iteratively to produce the correct labels of educational instances. In order to integrate the information encoded in the rules, we propose to train the network to imitate the results of a rules-regulated projection of p\u03b8 (y | x) called q (y | x), explicitly including rule constraints as regulatory terms. In each iteration q is constructed by projecting p\u03b8 into a rules-restricted subspace and therefore has desirable properties. We present the construction in the next section. The prediction behavior of q discloses the information of the regulated subspace and rules."}, {"heading": "3.3 Teacher Network Construction", "text": "We now proceed to define the teacher network q (x) in each iteration of the problem. (D) The iteration index t is omitted for clarity. (D) The iteration index t (D) is used to comply with the rules. (D) The goal is to find the optimal solution for the rules while staying close to Pisa. (D) The first property we apply is a commonly used strategy that imposes the rule constraints on q by an expectation operator. That is, for each rule (G) indexed by l) to (X, Y) we expect Eq (X, Y) [rlg). (X, Y)"}, {"heading": "4 Applications", "text": "In this section, we demonstrate the versatility of our approach by applying it to two workhorse network architectures, i.e. the revolutionary network and the recursive network, to two representative applications, i.e. sentence-level sentiment analysis, which is a classification problem, and to the so-called entity recognition, which is a sequence learning problem. For each task, we first briefly describe the neural base network. Since we do not focus on tuning network architectures, we largely use the same or similar networks as previous successful neural models. We then design the linguistically motivated rules to be integrated."}, {"heading": "4.1 Sentiment Classification", "text": "The task is crucial for many opinion polls. A difficult point of the task is to capture the contrastive sense (e.g. by conjunction \"but\") within a sentence. Base network We use the single-channel Convolutionary Network proposed in (Kim, 2014). The simple model has achieved a convincing performance on various sentiment classification benchmarks.The network contains a revolutionary layer on word vectors of a particular sentence, followed by a maximum pooling layer over time and then a fully connected layer with soft-max output activation.A revolution is to apply a filter to word windows. Multiple filters with different window sizes are used to obtain multiple characteristics. Figure 2, left panel, shows the network architecture. Logic Rules One difficulty for the simple neural network is to identify the contrast sense in order to accurately capture the prevailing mood."}, {"heading": "4.2 Named Entity Recognition", "text": "The task assigns a named tag tag to each word in an \"X-Y\" format, where X is one of BIEOS (Beginning, Inside, End, Outside, and Singleton) and Y is the entity category. A valid tag sequence must follow certain constraints by defining the tagging scheme. Also, text with structures (e.g. lists) within or across sentences can usually show some consistency patterns (Base Network).The base network has a similar architecture to the bi-directional LSTM sequence (called BLSTM-CNN) contained in structures proposed in (Chiu and Nichols, 2015).The model uses a CNN and pre-formed word vectors to capture information at the character and word levels."}, {"heading": "5 Experiments", "text": "By integrating the simple but effective rules into the base networks, we get significant improvements in both tasks and achieve state-of-the-art or comparable results with previous high-performance systems. A comparison with a variety of other rule integration methods shows the unique effectiveness of our framework. Our approach also shows promising potentials in semi-supervised learning and sparse data context. During the experiments, we set the imitation parameter to \u03c0 = 0.1, the regulation parameter to C = 400. These values are selected based on the SST2 sensitivity data (see below). The confidence level of the rules is set to \u03bbl = 1, with the exception of hard constraints that are not relied upon. For the configuration of neural networks, we have largely adhered to the reference work set out in the following sections. All experiments were implemented on a Linux machine with eight 4.0 GHz CPU corneons, a Tesla 32GB PRAM and a popular GRAM network."}, {"heading": "5.1 Sentiment Classification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1 Setup", "text": "We test our method using a number of commonly used benchmarks, including 1) SST2, Stanford Sentiment Treebank (Socher et al., 2013), which contains 2 classes (negative and positive) and 6920 / 872 / 1821 sentences in the train / development / test kits, respectively. Following (Kim, 2014) we train models both on sentences and phrases, as all labels are available. 2) MR (Pang and Lee, 2005), a set of 10,662 one-sentence film reviews with negative or positive moods. 3) CR (Hu and Liu, 2004), customer reviews of various products containing 2 classes and 3,775 instances. For the neural base network, we use the \"non-static\" version in (Kim, 2014) with exactly the same configurations. In particular, word vectors are initialized with word2vec (Mikolov et al., 2013) and neural parameters are trained using the GSD rule (2012)."}, {"heading": "5.1.2 Results", "text": "In fact, most of them are able to survive by themselves if they do not put themselves in a position to survive by themselves; most of them are not able to survive by themselves; most of them are able to survive by themselves; most of them are unable to survive by themselves; most of them are unable to survive by themselves; most of them are unable to survive by themselves; most of them are unable to survive by themselves; most of them are unable to survive by themselves; most of them are unable to survive by themselves; most of them are unable to survive by themselves; most of them are unable to survive by themselves; and most of them are unable to survive by themselves."}, {"heading": "5.2 Named Entity Recognition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 Setup", "text": "We evaluate using the established NER benchmark CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003), which contains 14,987 / 3,466 / 3,684 sets and 204,567 / 51,578 / 46,666 tokens in Train / Dev / Test Sets, respectively. The dataset covers 4 categories, i.e. person, location, organization and miscellaneous. The BIOES tagging schema is used. We use largely the same configurations for the BLSTM network as in (Chiu and Nichols, 2015), except for the minor differences in architecture (Section 4.2), we use Adadelta for parameter updating."}, {"heading": "5.2.2 Results", "text": "Table 4 represents the performance of the NER task. By incorporating the bigchart transition rules (Series 2), we obtain an improvement in the F1 score that surpasses all previous neural methods (Series 4-8), including the BLSTM-CRF model (Lample et al., 2016), which applies a conditional random field (CRF) to a BLSTM model to capture transition patterns and promote valid sequences. In contrast, our method implements the desired constraints in a simpler way, using the declarative logical rule language while not introducing additional model parameters for learning. Further integration of the list rule (Series 3) provides a second power boost by achieving an F1 score that comes very close to the most powerful system Joint-NER-EL (Luo et al., 2015) (Series 9), which is a probable graphical model-based method for optimizing and connecting NER together an entity that uses large amounts of external resources."}, {"heading": "6 Discussion", "text": "We have developed a framework that combines deep neural networks with first-order logic rules to enable the integration of human knowledge and intentions into neural models. Specifically, we proposed an iterative distillation process that transfers the structured information of logical rules into the weights of neural networks, transmitted via a teacher network built on the principle of posterior regulation; our framework is general and applicable to different types of neural architectures; with a few intuitive rules, our framework significantly improves the basic networks based on emotion analysis and so-called entity recognition and demonstrates the practical importance of our approach; the encouraging results point to the strong potential of our approach to improve other NLP tasks and areas of application; we plan to explore more applications and integrate more structured knowledge into neural networks; we want to improve our framework to automatically learn the meaning of different rules and derive new rules from them."}, {"heading": "A Appendix", "text": "Solving the problem Eq. (3), Section 3.3We provide the detailed derivative for solving the problem in Eq. (3), Section 3.3, which we repeat here: min q \u2212 \u2212 \u2212 l pattern (q (Y | X) pattern (Y | X) pattern (Y | A).,., L, (A.1) The following derivative is largely off (Ganchev et al., 2010) for logical rule making, with some reformulations requiring a closed solution.,., L, (A.1) The following derivative is largely adapted to (Ganchev et al., 2010)."}], "references": [{"title": "Hinge-loss markov random fields and probabilistic soft logic", "author": ["S.H. Bach", "M. Broecheler", "B. Huang", "L. Getoor"], "venue": "arXiv preprint arXiv:1505.04406.", "citeRegEx": "Bach et al\\.,? 2015", "shortCiteRegEx": "Bach et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Model compression", "author": ["C. Bucilu", "R. Caruana", "A. Niculescu-Mizil"], "venue": "Proc. of KDD, pages 535\u2013541. ACM.", "citeRegEx": "Bucilu et al\\.,? 2006", "shortCiteRegEx": "Bucilu et al\\.", "year": 2006}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["J.P. Chiu", "E. Nichols"], "venue": "arXiv preprint arXiv:1511.08308.", "citeRegEx": "Chiu and Nichols,? 2015", "shortCiteRegEx": "Chiu and Nichols", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Fast relational learning using bottom clause propositionalization with artificial neural networks", "author": ["M.V. Fran\u00e7a", "G. Zaverucha", "Garcez", "A.S. d."], "venue": "Machine learning, 94(1):81\u2013104.", "citeRegEx": "Fran\u00e7a et al\\.,? 2014", "shortCiteRegEx": "Fran\u00e7a et al\\.", "year": 2014}, {"title": "Posterior regularization for structured latent variable models", "author": ["K. Ganchev", "J. Gra\u00e7a", "J. Gillenwater", "B. Taskar"], "venue": "JMLR, 11:2001\u20132049.", "citeRegEx": "Ganchev et al\\.,? 2010", "shortCiteRegEx": "Ganchev et al\\.", "year": 2010}, {"title": "Neural-symbolic learning systems: foundations and applications", "author": ["Garcez", "A.S. d.", "K. Broda", "D.M. Gabbay"], "venue": "Springer Science & Business Media.", "citeRegEx": "Garcez et al\\.,? 2012", "shortCiteRegEx": "Garcez et al\\.", "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "Mohamed", "A.-r", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "Sainath", "T. N"], "venue": "Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv preprint arXiv:1503.02531.", "citeRegEx": "Hinton et al\\.,? 2015", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "Proc. of KDD, pages 168\u2013177. ACM.", "citeRegEx": "Hu and Liu,? 2004", "shortCiteRegEx": "Hu and Liu", "year": 2004}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Z. Huang", "W. Xu", "K. Yu"], "venue": "arXiv preprint arXiv:1508.01991.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Proc. of EMNLP.", "citeRegEx": "Kim,? 2014", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proc. of NIPS, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Deep convolutional inverse graphics network", "author": ["T.D. Kulkarni", "W.F. Whitney", "P. Kohli", "J. Tenenbaum"], "venue": "Proc. of NIPS, pages 2530\u20132538.", "citeRegEx": "Kulkarni et al\\.,? 2015", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "Science, 350(6266):1332\u20131338. 14", "citeRegEx": "Lake et al\\.,? 2015", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Neural architectures for named entity recognition", "author": ["G. Lample", "M. Ballesteros", "S. Subramanian", "K. Kawakami", "C. Dyer"], "venue": "Proc. of NAACL.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "Proc. of ICML.", "citeRegEx": "Le and Mikolov,? 2014", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "Learning from measurements in exponential families", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Proc. of ICML, pages 641\u2013648. ACM.", "citeRegEx": "Liang et al\\.,? 2009", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "Unifying distillation and privileged information", "author": ["D. Lopez-Paz", "L. Bottou", "B. Sch\u00f6lkopf", "V. Vapnik"], "venue": "Prof. of ICLR.", "citeRegEx": "Lopez.Paz et al\\.,? 2016", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2016}, {"title": "Joint named entity recognition and disambiguation", "author": ["G. Luo", "X. Huang", "Lin", "C.-Y.", "Z. Nie"], "venue": "Proc. of EMNLP.", "citeRegEx": "Luo et al\\.,? 2015", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Proc. of NIPS, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning meaning", "author": ["M. Minksy"], "venue": "Technical Report AI Lab Memo. Project MAC. MIT.", "citeRegEx": "Minksy,? 1980", "shortCiteRegEx": "Minksy", "year": 1980}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "Proc. of CVPR, pages 427\u2013 436. IEEE.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "Proc. of ACL, pages 115\u2013124. Association for Computational Linguistics.", "citeRegEx": "Pang and Lee,? 2005", "shortCiteRegEx": "Pang and Lee", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proc. of EMNLP, volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine learning, 62(12):107\u2013136.", "citeRegEx": "Richardson and Domingos,? 2006", "shortCiteRegEx": "Richardson and Domingos", "year": 2006}, {"title": "Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "Proc. of EMNLP, volume 1631, page 1642. Citeseer.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "Proc. of ICLR.", "citeRegEx": "Szegedy et al\\.,? 2014", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["E.F. Tjong Kim Sang", "F. De Meulder"], "venue": "Proc. of CoNLL, pages 142\u2013 147. Association for Computational Linguistics. 15", "citeRegEx": "Sang and Meulder,? 2003", "shortCiteRegEx": "Sang and Meulder", "year": 2003}, {"title": "Refinement of approximate domain theories by knowledge-based neural networks", "author": ["G.G. Towell", "J.W. Shavlik", "M.O. Noordewier"], "venue": "Proceedings of the eighth National conference on Artificial intelligence, pages 861\u2013866. Boston, MA.", "citeRegEx": "Towell et al\\.,? 1990", "shortCiteRegEx": "Towell et al\\.", "year": 1990}, {"title": "Fast dropout training", "author": ["S. Wang", "C. Manning"], "venue": "Proc. of ICML, pages 118\u2013126.", "citeRegEx": "Wang and Manning,? 2013", "shortCiteRegEx": "Wang and Manning", "year": 2013}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["S. Wang", "C.D. Manning"], "venue": "Proc. of ACL, pages 90\u201394. Association for Computational Linguistics.", "citeRegEx": "Wang and Manning,? 2012", "shortCiteRegEx": "Wang and Manning", "year": 2012}, {"title": "Context-aware learning for sentence-level sentiment analysis with posterior regularization", "author": ["B. Yang", "C. Cardie"], "venue": "Proc. of ACL, pages 325\u2013335.", "citeRegEx": "Yang and Cardie,? 2014", "shortCiteRegEx": "Yang and Cardie", "year": 2014}, {"title": "Multichannel variable-size convolution for sentence classification", "author": ["W. Yin", "H. Schutze"], "venue": "Proc. of CONLL.", "citeRegEx": "Yin and Schutze,? 2015", "shortCiteRegEx": "Yin and Schutze", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler,? 2012", "shortCiteRegEx": "Zeiler", "year": 2012}, {"title": "Mgnc-cnn: A simple approach to exploiting multiple word embeddings for sentence classification", "author": ["Y. Zhang", "S. Roller", "B. Wallace"], "venue": "Proc. of NAACL.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Bayesian inference with posterior regularization and applications to infinite latent svms", "author": ["J. Zhu", "N. Chen", "E.P. Xing"], "venue": "JMLR, 15(1):1799\u20131847. 16", "citeRegEx": "Zhu et al\\.,? 2014", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Deep neural networks provide a powerful mechanism for learning patterns from massive data, achieving new levels of performance on image classification (Krizhevsky et al., 2012), speech recognition (Hinton et al.", "startOffset": 151, "endOffset": 176}, {"referenceID": 8, "context": ", 2012), speech recognition (Hinton et al., 2012), machine translation (Bahdanau et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 1, "context": ", 2012), machine translation (Bahdanau et al., 2014), playing strategic board games (Silver et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 27, "context": ", 2014), playing strategic board games (Silver et al., 2016), and so forth.", "startOffset": 39, "endOffset": 60}, {"referenceID": 29, "context": "The high predictive accuracy has heavily relied on large amounts of labeled data; and the purely data-driven learning can lead to uninterpretable and sometimes counter-intuitive results (Szegedy et al., 2014; Nguyen et al., 2015).", "startOffset": 186, "endOffset": 229}, {"referenceID": 23, "context": "The high predictive accuracy has heavily relied on large amounts of labeled data; and the purely data-driven learning can lead to uninterpretable and sometimes counter-intuitive results (Szegedy et al., 2014; Nguyen et al., 2015).", "startOffset": 186, "endOffset": 229}, {"referenceID": 22, "context": "On the other hand, the cognitive process of human beings have indicated that people learn not only from concrete examples (as DNNs do) but also from different forms of general knowledge and rich experiences (Minksy, 1980; Lake et al., 2015).", "startOffset": 207, "endOffset": 240}, {"referenceID": 15, "context": "On the other hand, the cognitive process of human beings have indicated that people learn not only from concrete examples (as DNNs do) but also from different forms of general knowledge and rich experiences (Minksy, 1980; Lake et al., 2015).", "startOffset": 207, "endOffset": 240}, {"referenceID": 7, "context": "Neural-symbolic systems (Garcez et al., 2012) construct a network from a given rule set to execute reasoning.", "startOffset": 24, "endOffset": 45}, {"referenceID": 4, "context": "To exploit a priori knowledge in general neural architectures, recent work augments each raw data instance with useful features (Collobert et al., 2011), while network training, however, is still limited to instance-label supervision and suffers from the same issues mentioned above.", "startOffset": 128, "endOffset": 152}, {"referenceID": 9, "context": "Methodologically, our approach can be seen as a combination of the knowledge distillation (Hinton et al., 2015; Bucilu et al., 2006) and the posterior regularization (PR) method (Ganchev et al.", "startOffset": 90, "endOffset": 132}, {"referenceID": 2, "context": "Methodologically, our approach can be seen as a combination of the knowledge distillation (Hinton et al., 2015; Bucilu et al., 2006) and the posterior regularization (PR) method (Ganchev et al.", "startOffset": 90, "endOffset": 132}, {"referenceID": 6, "context": ", 2006) and the posterior regularization (PR) method (Ganchev et al., 2010).", "startOffset": 53, "endOffset": 75}, {"referenceID": 7, "context": "Neural-symbolic systems (Garcez et al., 2012), such as KBANN (Towell et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 31, "context": ", 2012), such as KBANN (Towell et al., 1990) and CILP++ (Fran\u00e7a et al.", "startOffset": 23, "endOffset": 44}, {"referenceID": 5, "context": ", 1990) and CILP++ (Fran\u00e7a et al., 2014), construct network architectures from given rules to perform reasoning and knowledge acquisition.", "startOffset": 19, "endOffset": 40}, {"referenceID": 26, "context": "A related line of research, such as Markov logic networks (Richardson and Domingos, 2006), derives probabilistic graphical models (rather than neural networks) from the rule set.", "startOffset": 58, "endOffset": 89}, {"referenceID": 4, "context": "features from domain knowledge (Collobert et al., 2011), while producing improved results, does not go beyond the data-label paradigm.", "startOffset": 31, "endOffset": 55}, {"referenceID": 6, "context": "Though there do exist general frameworks that allow encoding structured constraints on latent variable models (Ganchev et al., 2010; Zhu et al., 2014; Liang et al., 2009), they either are not directly applicable to the NN case, or could yield inferior performance as in our empirical study.", "startOffset": 110, "endOffset": 170}, {"referenceID": 38, "context": "Though there do exist general frameworks that allow encoding structured constraints on latent variable models (Ganchev et al., 2010; Zhu et al., 2014; Liang et al., 2009), they either are not directly applicable to the NN case, or could yield inferior performance as in our empirical study.", "startOffset": 110, "endOffset": 170}, {"referenceID": 18, "context": "Though there do exist general frameworks that allow encoding structured constraints on latent variable models (Ganchev et al., 2010; Zhu et al., 2014; Liang et al., 2009), they either are not directly applicable to the NN case, or could yield inferior performance as in our empirical study.", "startOffset": 110, "endOffset": 170}, {"referenceID": 4, "context": "features from domain knowledge (Collobert et al., 2011), while producing improved results, does not go beyond the data-label paradigm. Kulkarni et al. (2015) uses a specialized training procedure with careful ordering of training instances to obtain an interpretable neural layer of an image network.", "startOffset": 32, "endOffset": 158}, {"referenceID": 0, "context": "We encode the FOL rules using soft logic (Bach et al., 2015) for flexible encoding and stable optimization.", "startOffset": 41, "endOffset": 60}, {"referenceID": 2, "context": "A similar imitation procedure has been used in other settings such as model compression (Bucilu et al., 2006; Hinton et al., 2015) where the process is termed distillation.", "startOffset": 88, "endOffset": 130}, {"referenceID": 9, "context": "A similar imitation procedure has been used in other settings such as model compression (Bucilu et al., 2006; Hinton et al., 2015) where the process is termed distillation.", "startOffset": 88, "endOffset": 130}, {"referenceID": 19, "context": ", the privileged information setting (Lopez-Paz et al., 2016)) while still enjoying the benefit of integration.", "startOffset": 37, "endOffset": 61}, {"referenceID": 6, "context": "Our framework is related to the posterior regularization (PR) method (Ganchev et al., 2010) which places constraints over model posterior in unsupervised setting.", "startOffset": 69, "endOffset": 91}, {"referenceID": 12, "context": "Base Network We use the single-channel convolutional network proposed in (Kim, 2014).", "startOffset": 73, "endOffset": 84}, {"referenceID": 3, "context": "Base Network The base network has a similar architecture with the bi-directional LSTM recurrent network (called BLSTM-CNN) proposed in (Chiu and Nichols, 2015) for NER which has outperformed most of previous neural models.", "startOffset": 135, "endOffset": 159}, {"referenceID": 3, "context": "Compared to (Chiu and Nichols, 2015) we omit the character type and capitalization features, as well as the additive transition matrix in the output layer.", "startOffset": 12, "endOffset": 36}, {"referenceID": 16, "context": "In contrast to recent work (Lample et al., 2016) which adds a conditional random field (CRF) to capture bi-gram dependencies between outputs, we instead apply logic rules which does not introduce extra parameters to learn.", "startOffset": 27, "endOffset": 48}, {"referenceID": 28, "context": "We test our method on a number of commonly used benchmarks, including 1) SST2, Stanford Sentiment Treebank (Socher et al., 2013) which contains 2 classes (negative and positive), and 6920/872/1821 sentences in the train/dev/test sets respectively.", "startOffset": 107, "endOffset": 128}, {"referenceID": 12, "context": "Following (Kim, 2014) we train models on both sentences and phrases since all labels are provided.", "startOffset": 10, "endOffset": 21}, {"referenceID": 24, "context": "2) MR (Pang and Lee, 2005), a set of 10,662 one-sentence movie reviews with negative or positive sentiment.", "startOffset": 6, "endOffset": 26}, {"referenceID": 10, "context": "3) CR (Hu and Liu, 2004), customer reviews of various products, containing 2 classes and 3,775 instances.", "startOffset": 6, "endOffset": 24}, {"referenceID": 12, "context": "For the base neural network we use the \u201cnon-static\u201d version in (Kim, 2014) with the exact same configurations.", "startOffset": 63, "endOffset": 74}, {"referenceID": 21, "context": "Specifically, word vectors are initialized using word2vec (Mikolov et al., 2013) and fine-tuned throughout training, and the neural parameters are trained using SGD with the Adadelta update rule (Zeiler, 2012).", "startOffset": 58, "endOffset": 80}, {"referenceID": 36, "context": ", 2013) and fine-tuned throughout training, and the neural parameters are trained using SGD with the Adadelta update rule (Zeiler, 2012).", "startOffset": 122, "endOffset": 136}, {"referenceID": 12, "context": "Model SST2 MR CR 1 CNN (Kim, 2014) 87.", "startOffset": 23, "endOffset": 34}, {"referenceID": 37, "context": "04 3 MGNC-CNN (Zhang et al., 2016) 88.", "startOffset": 14, "endOffset": 34}, {"referenceID": 35, "context": "4 \u2013 \u2013 4 MVCNN (Yin and Schutze, 2015) 89.", "startOffset": 14, "endOffset": 37}, {"referenceID": 12, "context": "4 \u2013 \u2013 5 CNN-multichannel (Kim, 2014) 88.", "startOffset": 25, "endOffset": 36}, {"referenceID": 17, "context": "0 6 Paragraph-Vec (Le and Mikolov, 2014) 87.", "startOffset": 18, "endOffset": 40}, {"referenceID": 34, "context": "8 \u2013 \u2013 7 CRF-PR (Yang and Cardie, 2014) \u2013 \u2013 82.", "startOffset": 15, "endOffset": 38}, {"referenceID": 28, "context": "7 8 RNTN (Socher et al., 2013) 85.", "startOffset": 9, "endOffset": 30}, {"referenceID": 32, "context": "4 \u2013 \u2013 9 G-Dropout (Wang and Manning, 2013) \u2013 79.", "startOffset": 18, "endOffset": 42}, {"referenceID": 33, "context": "1 10 NBSVM (Wang and Manning, 2012) \u2013 79.", "startOffset": 11, "endOffset": 35}, {"referenceID": 12, "context": "Row 1, CNN (Kim, 2014) corresponds to the \u201cCNN-non-static\u201d model in (Kim, 2014).", "startOffset": 11, "endOffset": 22}, {"referenceID": 12, "context": "Row 1, CNN (Kim, 2014) corresponds to the \u201cCNN-non-static\u201d model in (Kim, 2014).", "startOffset": 68, "endOffset": 79}, {"referenceID": 35, "context": "On SST2, MVCNN (Yin and Schutze, 2015) (Row 4) is the only system that shows a slightly better result than ours.", "startOffset": 15, "endOffset": 38}, {"referenceID": 6, "context": "4) \u201c-semi-PR\u201d is the posterior regularization (Ganchev et al., 2010) which imposes the rule constraint only through unlabeled data during training.", "startOffset": 46, "endOffset": 68}, {"referenceID": 3, "context": "We use the mostly same configurations for the BLSTM network as in (Chiu and Nichols, 2015), except that, besides the slight architecture difference (section 4.", "startOffset": 66, "endOffset": 90}, {"referenceID": 25, "context": "GloVe (Pennington et al., 2014) word vectors are used to initialize word features.", "startOffset": 6, "endOffset": 31}, {"referenceID": 16, "context": "56 improvement in F1 score that outperforms all previous neural based methods (Rows 4-8), including the BLSTM-CRF model (Lample et al., 2016) which applies a conditional random field (CRF) on top of a BLSTM model in order to capture the transition patterns and encourage valid sequences.", "startOffset": 120, "endOffset": 141}, {"referenceID": 16, "context": "18 4 BLSTM-CRF1 (Lample et al., 2016) 90.", "startOffset": 16, "endOffset": 37}, {"referenceID": 16, "context": "94 5 S-LSTM (Lample et al., 2016) 90.", "startOffset": 12, "endOffset": 33}, {"referenceID": 3, "context": "33 6 BLSTM-lex (Chiu and Nichols, 2015) 90.", "startOffset": 15, "endOffset": 39}, {"referenceID": 11, "context": "77 7 BLSTM-CRF2 (Huang et al., 2015) 90.", "startOffset": 16, "endOffset": 36}, {"referenceID": 4, "context": "10 8 NN-lex (Collobert et al., 2011) 89.", "startOffset": 12, "endOffset": 36}, {"referenceID": 20, "context": "59 9 Joint-NER-EL (Luo et al., 2015) 91.", "startOffset": 18, "endOffset": 36}, {"referenceID": 20, "context": "Further integration of the list rule (Row 3) provides a second boost in performance, achieving an F1 score very close to the best-performing system Joint-NER-EL (Luo et al., 2015) (Row 9) which is a probabilistic graphical model based method optimizing NER and entity linking jointly and using large amount of external resources.", "startOffset": 161, "endOffset": 179}], "year": 2016, "abstractText": "Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.", "creator": "LaTeX with hyperref package"}}}