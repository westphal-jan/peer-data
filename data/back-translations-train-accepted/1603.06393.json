{"id": "1603.06393", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning", "abstract": "We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.", "histories": [["v1", "Mon, 21 Mar 2016 11:35:08 GMT  (641kb,D)", "http://arxiv.org/abs/1603.06393v1", "12 pages, 6 figures"], ["v2", "Tue, 22 Mar 2016 03:33:58 GMT  (636kb,D)", "http://arxiv.org/abs/1603.06393v2", "11 pages, 8 figures"], ["v3", "Wed, 8 Jun 2016 13:53:21 GMT  (472kb,D)", "http://arxiv.org/abs/1603.06393v3", "10 pages, 5 figures, accepted by ACL2016"]], "COMMENTS": "12 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["jiatao gu", "zhengdong lu", "hang li", "victor o k li"], "accepted": true, "id": "1603.06393"}, "pdf": {"name": "1603.06393.pdf", "metadata": {"source": "CRF", "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning", "authors": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "emails": ["vli}@eee.hku.hk", "hangli.hl}@huawei.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them are able to abide by the rules that they have given themselves, and that they are able to abide by the rules that they have given themselves. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to break the rules. (...)"}, {"heading": "2 Background: Neural Models for Sequence-to-sequence Learning", "text": "Seq2Seq Learning can be expressed in a probabilistic perspective in such a way as to maximize the likelihood (or any other evaluation variable (Shen et al., 2015) of observing the output (target) sequence taking into account an input (source) sequence."}, {"heading": "2.1 RNN Encoder-Decoder", "text": "RNN-based encoder decoder is successfully applied to real Seq2Seq tasks, first by Cho et al. (2014) and Sutskever et al. (2014) and then by (Vinyals and Le, 2015; Vinyals et al., 2015a). In the encoder decoder framework, the source sequence X = [x1,..., xTS] is converted by the encoder RNN into a fixed length vector c, i.e. ht = f (xt, ht \u2212 1); c = \u03c6 ({h1,..., hTS}) (1), where {ht} are the RNN states, c is the so-called context vector, f is the dynamic function, and \u03c6 summarizes the hidden states, e.g. the choice of the last state hTS. In practice, it turns out that gated RNN alternatives such as L & lt. \u2212 fiction, & lt. \u2212 or GRU (Cho et al, the prediction in 2014 is much better than the prediction in the TS)."}, {"heading": "2.2 The Attention Mechanism", "text": "The attention mechanism was first introduced in Seq2Seq (Bahdanau et al., 2014) to release the load of summarizing the entire source into a vector of fixed length as context. Instead, attention uses a dynamically changing context in the decoding process. A natural option (or rather \"soft attention\") is to present ct as a weighted sum of the hidden states of the source, i.e., ct = TS = 1 \u03b1t\u03c4h\u03c4; \u03b1t\u03c4 = e\u043e (st \u2212 1, h\u043e) \u0432 (st \u2212 1, h\u043e \u2032) (3), where \u043e is the function that indicates the correspondence strength for attention normally approximated with a multilayered neural network (DNN). Note that in (Bahdanau et al., 2014) the source set is encoded with a bi-directional RNN, making any hidden state of contextual information aware of the two ends."}, {"heading": "3 COPYNET", "text": "From a cognitive point of view, the copying mechanism is linked to memorizing texts, which requires less understanding but ensures a high literal fidelity. From a modeling perspective, the copying processes are rigid and symbolic, making it more difficult than soft attention mechanisms to integrate into a fully differentiable neural model. In this section, we introduce COPYNET, a differentiated Seq2Seq model with a \"copying mechanism\" that can be trained end-to-end with only a steep descent."}, {"heading": "3.1 Model Overview", "text": "As shown in Figure 1, COPYNET is still an encoder decoder (in a slightly generalized sense), and the source sequence is transformed by encoder into a representation, which is then read by decoder to generate the target sequence.Encoder: As in (Bahdanau et al., 2014), a bidirectional RNN is used to transform the source sequence into a series of hidden states of equal length, with each hidden state corresponding to word text. This new representation of the source, {h1,..., hTS}, is considered a short-time memory (referred to as M in the rest of the paper), which is later accessed in multiple ways by generating the target sequence (decoding). Decoder: An RNN that reads M and predicts the target sequence (NENN decoder) is similar to the canonical RNN decoder mode of a specific word (YD), with only a difference between a YM (in the following word mode) and a Q."}, {"heading": "3.2 Prediction with Copying and Generation", "text": "Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary Vocabulary"}, {"heading": "3.3 State Update", "text": "COPYNET updates each decoding state st \u2212 1 with the previous state st \u2212 1, the previous symbol yt \u2212 1 and the context vector ct according to Equation (2) for the generic attention-based Seq2Seq model. However, there are some minor changes in the yt \u2212 1 \u2212 st path for the copy mechanism. Specifically, yt \u2212 1 is represented as [e (yt \u2212 1), where e (yt \u2212 1) is the word embedding associated with yt \u2212 1, whereas the weighted sum of the hidden states in M is the weighted sum of the hidden states in M, which corresponds to yt (yt \u2212 1) = 1 categorical sequence."}, {"heading": "3.4 Hybrid Addressing of M", "text": "In fact, the semantics of a word and its position in X in the hidden states are encoded in M. In fact, the attentive readers of COPYNET are driven more by semantics and the language model, enabling them to move freely on M. Indeed, the selective reading of COPYNET is driven by semantics and the language model."}, {"heading": "4 Learning", "text": "Although the copy mechanism uses the \"hard\" process of copying and pasting from the source or generating symbols from the vocabulary, COPYNET is completely distinguishable and can be optimized end-to-end by reverse propagation. Given the stacks of the source and target sequence {X} N and {Y} N, the goal is to minimize the negative log probability: L = \u2212 1 N \u2211 k = 1 T \u2211 t = 1 log [p (k) t | y (k) < t, X (k)], (10) where we index the instances by uppercase. Since the tribal model for observing any target is a mixture of generation mode and copy mode, there is no need for additional labels for modes. The network can learn to coordinate the two modes based on data."}, {"heading": "5 Experiments", "text": "We report on our empirical study of COPYNET on the following three tasks with different characteristics: 1. A synthetic dataset with simple patterns; 2. A real task for summarizing text; 3. A dataset for simple dialogues with only one turn."}, {"heading": "5.1 Synthetic Dataset", "text": "Dataset: We first randomly create transformation rules with 5 \u0445 20 symbols and variables x & y, e.g. a b x c d y e f \u2212 \u2192 g h x m, where {a b c d e f g h m} are regular symbols from a vocabulary of the size of 1,000. As shown in the table below, each rule can create a number of instances by replacing the variables with randomly generated subsequences (1 \u0445 15 symbols) from the same vocabulary. We create five types of rules, including \"x \u2192 \u2205.\" The task is to learn the Seq2Seq transformation from the training instances. This dataset is designed to examine the behavior of COPYNET when dealing with simple and rigid patterns. As the sequences to be repeated are random, they can also be seen as some extreme cases of red storage."}, {"heading": "5.2 Text Summarization", "text": "It was recently formulated as a Seq2Seq learning problem in (Rush et al., 2015; Hu et al., 2015), which essentially gives an abstract summary, since the summary is based on a representation of the document. In contrast, the extractive summary NEV lies between two categories, since part of the output summary is actually extracted from the document (via the copy mechanism), which along with the words may be generated from the-mode.Dataset: We evaluate our model on the recently published LCSTS dataset (Hu et al., 2015)."}, {"heading": "5.2.1 Case Study", "text": "We make the following interesting observations about the summary of textscCopyNet (Figure 4 and more in the supplementary material): 1) Most words come from copy mode, but the summary is usually still fluid; 2) COPYNET tends to cover consecutive words in the original document, but often assembles segments that are far from each other, indicating an ingenious coordination of content-based addressing and location-based addressing; 3) COPYNET treats OOV words really well: it can create an acceptable summary for documents with many OOVs, and even the summary itself often contains many OOV words. In contrast, canonical RNN-based approaches often fail in cases like this. It is quite complicated that COPYNET can often find important parts of the document, a behavior with the characteristics of an extractive summary, while it often generates words to \"connect\" those words, which shows its abstract aspect."}, {"heading": "5.3 Single-turn Dialogue", "text": "In this experiment, we follow the work on neural dialog model proposed in (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015), and test COPYNET on single-turn dialog. Basically, the neural model learns to generate a response tousers input from the given (input, response) pairs as training instances. Dataset: We build a simple dialog record based on the following three instructions: 1. Dialogue instances are collected by Baidu Tieba3 with some coverage of the conversations of real life, e.g. greeting and sports, etc. 2. Patterns with slots like hi, my name is x \u2192 hi, xare mined from the set, with possibly multiple response patterns on one input. 3. Similarly to the synthetic dataset, we enlarge the data sets we enlarge by filling the slots with suitable subsequences (e.g. name entities, data, etc.)."}, {"heading": "5.3.1 Case Study", "text": "As the examples in Figure 5 show, COPYNET accurately replicates the critical segments of the input with the copy mode and generates the rest of the responses smoothly via the generate mode. Note that in (2) and (3) the decoding sequence does not exactly match the standard sequence, but is still correct in meaning. In contrast, RNNSearch normally generates responses in the correct formats, but fails to capture the critical units in all three cases due to the difficulties the invisible words entail."}, {"heading": "6 Related Work", "text": "Our work is partly inspired by the recent work of Pointer Networks (Vinyals et al., 2015a), in which a pointer mechanism (very similar to the proposed copy mechanism) is used to predict the output sequence directly from input. In addition to the difference to our application (Vinyals et al., 2015a), a pointer mechanism cannot be predicted outside the input sequence, while COPYNET can of course combine generation and copying. COPYNET tackles the OOV problem more systemically with an end-to-end model. However, since COPYNET copies the exact source words as output, it cannot be applied directly to machine translations."}, {"heading": "7 Conclusion and Future Work", "text": "We suggested to COPYNET to integrate the copy mechanism into the Seq2Seq Learning Framework. For future work we will extend this idea to the task where source and target are in different languages, for example machine translation."}, {"heading": "A supplemental materials", "text": "A.1 Text Summarization We present more examples of summary results for the LCSTS test set and follow the same instructions described in Section 5.2 Comparing COPYNET and RNN encoders as you can see in Figure 6 and Figure 7.A.2 Dialog system with one turn. We present more examples of dialog results with one turn to the test set of the dialog set, following the same instructions described in Section 5.3, and comparing COPYNET and RNN encoders as you can see in Figure 8."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."], "venue": "arXiv preprint arXiv:1410.5401.", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1512.03385.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Lcsts: a large scale chinese short text summarization dataset", "author": ["Baotian Hu", "Qingcai Chen", "Fangze Zhu."], "venue": "arXiv preprint arXiv:1506.05865.", "citeRegEx": "Hu et al\\.,? 2015", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "Neural random-access machines", "author": ["Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1511.06392.", "citeRegEx": "Kurach et al\\.,? 2015", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74\u201381, Barcelona, Spain, July. Associa-", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Mixture models", "author": ["Geoffrey J McLachlan", "Kaye E Basford."], "venue": "inference and applications to clustering. Statistics: Textbooks and Monographs, New York: Dekker, 1988, 1.", "citeRegEx": "McLachlan and Basford.,? 1988", "shortCiteRegEx": "McLachlan and Basford.", "year": 1988}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1509.00685.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "arXiv preprint arXiv:1503.02364.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "CoRR, abs/1512.02433.", "citeRegEx": "Shen et al\\.,? 2015", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Highway networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "arXiv preprint arXiv:1505.00387.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Advances in Neural Information Processing Systems, pages 2674\u20132682.", "citeRegEx": "Vinyals et al\\.,? 2015a", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems, pages 2755\u20132763.", "citeRegEx": "Vinyals et al\\.,? 2015b", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Recently, neural network-based sequence-tosequence learning (Seq2Seq) has achieved remarkable success in various natural language processing (NLP) tasks, including but not limited to Machine Translation (Cho et al., 2014; Bahdanau et al., 2014), Syntactic Parsing (Vinyals et al.", "startOffset": 203, "endOffset": 244}, {"referenceID": 0, "context": "Recently, neural network-based sequence-tosequence learning (Seq2Seq) has achieved remarkable success in various natural language processing (NLP) tasks, including but not limited to Machine Translation (Cho et al., 2014; Bahdanau et al., 2014), Syntactic Parsing (Vinyals et al.", "startOffset": 203, "endOffset": 244}, {"referenceID": 18, "context": ", 2014), Syntactic Parsing (Vinyals et al., 2015b), Text Summarization (Rush et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 10, "context": ", 2015b), Text Summarization (Rush et al., 2015) and Dialogue Systems (Vinyals and Le, 2015).", "startOffset": 29, "endOffset": 48}, {"referenceID": 16, "context": ", 2015) and Dialogue Systems (Vinyals and Le, 2015).", "startOffset": 29, "endOffset": 51}, {"referenceID": 0, "context": "Adding the attention mechanism (Bahdanau et al., 2014) to Seq2Seq, first proposed for automatic alignment in machine translation, has led to significant improvement on the performance of various tasks (Shang et al.", "startOffset": 31, "endOffset": 54}, {"referenceID": 11, "context": ", 2014) to Seq2Seq, first proposed for automatic alignment in machine translation, has led to significant improvement on the performance of various tasks (Shang et al., 2015; Rush et al., 2015).", "startOffset": 154, "endOffset": 193}, {"referenceID": 10, "context": ", 2014) to Seq2Seq, first proposed for automatic alignment in machine translation, has led to significant improvement on the performance of various tasks (Shang et al., 2015; Rush et al., 2015).", "startOffset": 154, "endOffset": 193}, {"referenceID": 12, "context": "Seq2Seq Learning can be expressed in a probabilistic view as maximizing the likelihood (or some other evaluation metrics (Shen et al., 2015)) of observing the output (target) sequence given an input (source) sequence.", "startOffset": 121, "endOffset": 140}, {"referenceID": 16, "context": "(2014), and then by (Vinyals and Le, 2015; Vinyals et al., 2015a).", "startOffset": 20, "endOffset": 65}, {"referenceID": 17, "context": "(2014), and then by (Vinyals and Le, 2015; Vinyals et al., 2015a).", "startOffset": 20, "endOffset": 65}, {"referenceID": 1, "context": "RNN-based Encoder-Decoder is successfully applied to real world Seq2Seq tasks, first by Cho et al. (2014) and Sutskever et al.", "startOffset": 88, "endOffset": 106}, {"referenceID": 1, "context": "RNN-based Encoder-Decoder is successfully applied to real world Seq2Seq tasks, first by Cho et al. (2014) and Sutskever et al. (2014), and then by (Vinyals and Le, 2015; Vinyals et al.", "startOffset": 88, "endOffset": 134}, {"referenceID": 4, "context": "In practice it is found that gated RNN alternatives such as LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al.", "startOffset": 65, "endOffset": 99}, {"referenceID": 1, "context": "In practice it is found that gated RNN alternatives such as LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) often perform much better than vanilla ones.", "startOffset": 107, "endOffset": 125}, {"referenceID": 0, "context": "The attention mechanism was first introduced to Seq2Seq (Bahdanau et al., 2014) to release the burden of summarizing the entire source into a fixed-length vector as context.", "startOffset": 56, "endOffset": 79}, {"referenceID": 0, "context": "Note that in (Bahdanau et al., 2014) the source sentence is encoded with a Bi-directional RNN, making each hidden state h\u03c4 aware of the contextual information from both ends.", "startOffset": 13, "endOffset": 36}, {"referenceID": 0, "context": "Encoder: Same as in (Bahdanau et al., 2014), a bi-directional RNN is used to transform the source sequence into a series of hidden states with equal length, with each hidden state ht corresponding to word xt.", "startOffset": 20, "endOffset": 43}, {"referenceID": 0, "context": "It is similar with the canonical RNN-decoder in (Bahdanau et al., 2014), with however the following important differences", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "(4) deviated from the canonical definition of mixture model (McLachlan and Basford, 1988).", "startOffset": 60, "endOffset": 89}, {"referenceID": 2, "context": "Unlike the explicit design for hybrid addressing in Neural Turing Machine (Graves et al., 2014; Kurach et al., 2015), COPYNET is more subtle: it provides the architecture that can facilitate some particular locationbased addressing and lets the model figure out the details from the training data for specific tasks.", "startOffset": 74, "endOffset": 116}, {"referenceID": 6, "context": "Unlike the explicit design for hybrid addressing in Neural Turing Machine (Graves et al., 2014; Kurach et al., 2015), COPYNET is more subtle: it provides the architecture that can facilitate some particular locationbased addressing and lets the model figure out the details from the training data for specific tasks.", "startOffset": 74, "endOffset": 116}, {"referenceID": 10, "context": "It has been recently formulated as a Seq2Seq learning problem in (Rush et al., 2015; Hu et al., 2015), which essentially gives abstractive summarization since the summary is generated based on a representation of the document.", "startOffset": 65, "endOffset": 101}, {"referenceID": 5, "context": "It has been recently formulated as a Seq2Seq learning problem in (Rush et al., 2015; Hu et al., 2015), which essentially gives abstractive summarization since the summary is generated based on a representation of the document.", "startOffset": 65, "endOffset": 101}, {"referenceID": 5, "context": "Dataset: We evaluate our model on the recently published LCSTS dataset (Hu et al., 2015), a large scale dataset for short text summarization.", "startOffset": 71, "endOffset": 88}, {"referenceID": 5, "context": "Following the setting of (Hu et al., 2015) we use Part I as the training set and and the subset of Part III scored from 3 to 5 as testing set.", "startOffset": 25, "endOffset": 42}, {"referenceID": 5, "context": "We set the vocabulary size to 3,000 (+C) and 10,000 (+W) respectively, which are much smaller than those for models in (Hu et al., 2015).", "startOffset": 119, "endOffset": 136}, {"referenceID": 5, "context": "Following (Hu et al., 2015), we evaluate the test performance with the commonly used ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004), and compare it against the two models in (Hu et al.", "startOffset": 10, "endOffset": 27}, {"referenceID": 7, "context": ", 2015), we evaluate the test performance with the commonly used ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004), and compare it against the two models in (Hu et al.", "startOffset": 94, "endOffset": 105}, {"referenceID": 5, "context": ", 2015), we evaluate the test performance with the commonly used ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004), and compare it against the two models in (Hu et al., 2015), which are essentially canonical Encoder-Decoder and its variant with attention.", "startOffset": 148, "endOffset": 165}, {"referenceID": 5, "context": "6 (Hu et al., 2015) +W 17.", "startOffset": 2, "endOffset": 19}, {"referenceID": 5, "context": "2 (Hu et al., 2015) +W 26.", "startOffset": 2, "endOffset": 19}, {"referenceID": 5, "context": "Hu et al. (2015) reports that the performance of a word-based model is inferior to a character-based one.", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "Hu et al. (2015) reports that the performance of a word-based model is inferior to a character-based one. One possible explanation is that a wordbased model, even with a much larger vocabulary (50,000 words in Hu et al. (2015)), still has a large proportion of OOVs due to the large number of entity names in the summary data and the mistakes in word segmentation.", "startOffset": 0, "endOffset": 227}, {"referenceID": 11, "context": "In this experiment we follow the work on neural dialogue model proposed in (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015), and test COPYNET on single-turn dialogue.", "startOffset": 75, "endOffset": 139}, {"referenceID": 16, "context": "In this experiment we follow the work on neural dialogue model proposed in (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015), and test COPYNET on single-turn dialogue.", "startOffset": 75, "endOffset": 139}, {"referenceID": 13, "context": "In this experiment we follow the work on neural dialogue model proposed in (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015), and test COPYNET on single-turn dialogue.", "startOffset": 75, "endOffset": 139}, {"referenceID": 17, "context": "Our work is partially inspired by the recent work of Pointer Networks (Vinyals et al., 2015a), in which a pointer mechanism (quite similar with the proposed copying mechanism) is used to predict the output sequence directly from the input.", "startOffset": 70, "endOffset": 93}, {"referenceID": 17, "context": "In addition to the difference with ours in application, (Vinyals et al., 2015a) cannot predict outside of the set of input sequence, while COPYNET can naturally combine generating and copying.", "startOffset": 56, "endOffset": 79}, {"referenceID": 14, "context": "Similar ideas are proposed for training very deep neural networks in (Srivastava et al., 2015; He et al., 2015) for classification tasks, where shortcuts are built between layers for the direct carrying of information.", "startOffset": 69, "endOffset": 111}, {"referenceID": 3, "context": "Similar ideas are proposed for training very deep neural networks in (Srivastava et al., 2015; He et al., 2015) for classification tasks, where shortcuts are built between layers for the direct carrying of information.", "startOffset": 69, "endOffset": 111}, {"referenceID": 6, "context": "Luong et al. (2015) introduced a heuristics to postprocess the translated sentence using annotations on the source sentence.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural networkbased Seq2Seq learning and propose a new model called COPYNET with encoderdecoder structure. COPYNET can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose subsequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of COPYNET. For example, COPYNET can outperform regular RNN-based model with remarkable margins on text summarization tasks.", "creator": "TeX"}}}