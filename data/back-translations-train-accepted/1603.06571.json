{"id": "1603.06571", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Bayesian Neural Word Embedding", "abstract": "Recently, several works in the domain of natural language processing presented successful methods for word embedding. Among them, the Skip-gram (SG) with negative sampling, known also as Word2Vec, advanced the state-of-the-art of various linguistics tasks. In this paper, we propose a scalable Bayesian neural word embedding algorithm that can be beneficial to general item similarity tasks as well. The algorithm relies on a Variational Bayes solution for the SG objective and a detailed step by step description of the algorithm is provided. We present experimental results that demonstrate the performance of the proposed algorithm and show it is competitive with the original SG method.", "histories": [["v1", "Mon, 21 Mar 2016 16:32:06 GMT  (145kb)", "http://arxiv.org/abs/1603.06571v1", null], ["v2", "Sun, 5 Jun 2016 16:49:11 GMT  (156kb)", "http://arxiv.org/abs/1603.06571v2", null], ["v3", "Mon, 20 Feb 2017 20:45:33 GMT  (181kb)", "http://arxiv.org/abs/1603.06571v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["oren barkan"], "accepted": true, "id": "1603.06571"}, "pdf": {"name": "1603.06571.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "In this paper, we propose a scalable description of the algorithm that demonstrates the performance of the proposed algorithm and shows that it is a comparable task derived from the original SG method.: The algorithm relies on a variation solution for the SG objectively and a detailed step description of the algorithm is provided. We present experimental results that demonstrate the performance of the proposed algorithm and show that it can compete and show the proposed algorithms with the original SG method.: The algorithm relies on a variation solution for the SG objectively and a detailed description of the algorithm is provided. We present experimental results that demonstrate the performance of the proposed algorithm and show that it is comparable to the proposed algorithms with the original SG method."}], "references": [{"title": "Devise: A deep visual-semantic embedding model", "author": ["A Frome", "GS Corrado", "J Shlens", "S Bengio", "J Dean", "T. Mikolov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "author": ["J Pennington", "R Socher", "CD. Manning"], "venue": "In EMNLP 2014 Oct (Vol", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Combining Distributed Vector Representations for Words", "author": ["J Garten", "K Sagae", "V Ustun", "M. Dehghani"], "venue": "In Proceedings of NAACL-HLT 2015 Jun", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["C Chelba", "T Mikolov", "M Schuster", "Q Ge", "T Brants", "P Koehn", "T. Robinson"], "venue": "arXiv preprint arXiv:1312.3005", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R Collobert", "J. Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning 2008 Jul", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "A scalable hierarchical distributed language model. InAdvances in neural information processing systems", "author": ["Mnih A", "Hinton GE"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems", "author": ["T Mikolov", "I Sutskever", "K Chen", "GS Corrado", "J. Dean"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["T Mikolov", "K Chen", "G Corrado", "J. Dean"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "One-class collaborative filtering with random graphs", "author": ["U. Paquet", "Koenigstein", "May"], "venue": "In Proceedings of the 22nd international conference on World Wide Web (pp. 999-1008)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["A Lazaridou", "NT Pham", "M. Baroni"], "venue": "arXiv preprint arXiv:1501.02598", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Item2Vec: Neural item embedding for collaborative filtering", "author": ["N. Barkan O. Koenigstein"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "A variational approach to Bayesian logistic regression problems and their extensions", "author": ["T. Jaakkola", "M. Jordan"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "A stochastic approximation method", "author": ["H Robbins", "S. Monro"], "venue": "The annals of mathematical statistics", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1951}, {"title": "The evidence framework applied to classification networks", "author": ["MacKay DJ"], "venue": "Neural computation", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Placing search in context: The concept revisited", "author": ["L Finkelstein", "E Gabrilovich", "Y Matias", "E Rivlin", "Z Solan", "G Wolfman", "E. Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web 2001 Apr", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "The proof and measurement of association between two things", "author": ["C. Spearman"], "venue": "The American journal of psychology", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1904}, {"title": "A new measure of rank correlation. Biometrika", "author": ["Kendall MG"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1938}], "referenceMentions": [{"referenceID": 1, "context": "Introduction Recent progress in neural word embedding methods have advanced the state-of-the-art in the domain of natural language processing [2, 6, 7, 8, 9].", "startOffset": 142, "endOffset": 157}, {"referenceID": 4, "context": "Introduction Recent progress in neural word embedding methods have advanced the state-of-the-art in the domain of natural language processing [2, 6, 7, 8, 9].", "startOffset": 142, "endOffset": 157}, {"referenceID": 5, "context": "Introduction Recent progress in neural word embedding methods have advanced the state-of-the-art in the domain of natural language processing [2, 6, 7, 8, 9].", "startOffset": 142, "endOffset": 157}, {"referenceID": 6, "context": "Introduction Recent progress in neural word embedding methods have advanced the state-of-the-art in the domain of natural language processing [2, 6, 7, 8, 9].", "startOffset": 142, "endOffset": 157}, {"referenceID": 7, "context": "Introduction Recent progress in neural word embedding methods have advanced the state-of-the-art in the domain of natural language processing [2, 6, 7, 8, 9].", "startOffset": 142, "endOffset": 157}, {"referenceID": 6, "context": "Specifically, Skipgram (SG) with negative sampling , known also as Word2Vec [8], set new records in various linguistic tasks and its applications have been extended to other domains beyond NLP such as computer vision [1, 11], Collaborative Filtering (CF) and item similarity [12].", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "Specifically, Skipgram (SG) with negative sampling , known also as Word2Vec [8], set new records in various linguistic tasks and its applications have been extended to other domains beyond NLP such as computer vision [1, 11], Collaborative Filtering (CF) and item similarity [12].", "startOffset": 217, "endOffset": 224}, {"referenceID": 9, "context": "Specifically, Skipgram (SG) with negative sampling , known also as Word2Vec [8], set new records in various linguistic tasks and its applications have been extended to other domains beyond NLP such as computer vision [1, 11], Collaborative Filtering (CF) and item similarity [12].", "startOffset": 217, "endOffset": 224}, {"referenceID": 10, "context": "Specifically, Skipgram (SG) with negative sampling , known also as Word2Vec [8], set new records in various linguistic tasks and its applications have been extended to other domains beyond NLP such as computer vision [1, 11], Collaborative Filtering (CF) and item similarity [12].", "startOffset": 275, "endOffset": 279}, {"referenceID": 6, "context": "al in [8].", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "1 Negative sampling Negative sampling [8] is introduced in order to overcome the above computational problem by the replacement of the softmax function from Eq.", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "This distribution was found to significantly outperform both the uniform and unigram distributions, empirically [2, 8, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 6, "context": "This distribution was found to significantly outperform both the uniform and unigram distributions, empirically [2, 8, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 7, "context": "This distribution was found to significantly outperform both the uniform and unigram distributions, empirically [2, 8, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 1, "context": "These biases often explain properties such as frequency of a word in the text [2, 7] or popularity of an item in the dataset [10].", "startOffset": 78, "endOffset": 84}, {"referenceID": 5, "context": "These biases often explain properties such as frequency of a word in the text [2, 7] or popularity of an item in the dataset [10].", "startOffset": 78, "endOffset": 84}, {"referenceID": 8, "context": "These biases often explain properties such as frequency of a word in the text [2, 7] or popularity of an item in the dataset [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 6, "context": "2 Data subsampling In order to overcome the imbalance between rare and frequent words the following subsampling procedure is suggested [8]: Given the input words sequence, we discard each word w with a probability", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "This procedure is reported to accelerate the learning process and to improve the representation of rare words [8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "The last two options are reported to produce superior representation [3].", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "Inspired by a recent CF work [10], we suggest a VB approximation for the true posteriors of U and V .", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "Furthermore, these hyperparameters can be treated as random variables and be learnt from the data [10].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "We start by lower bounding ( | , ) p D U V using the following logistic bound [13]:", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "Furthermore, it was shown [13] that the bound in Eq.", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "A common practice is to apply updates in the spirit of the Robbins-Monro method [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "This results in a Bayesian version of the Item2Vec method [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "(24) and [15], respectively.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "Note that this approach is taken in [10] as well.", "startOffset": 36, "endOffset": 40}, {"referenceID": 14, "context": "We evaluate the algorithms on two different tasks: the word similarity task [16] and the word analogy task [2, 8].", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "We evaluate the algorithms on two different tasks: the word similarity task [16] and the word analogy task [2, 8].", "startOffset": 107, "endOffset": 113}, {"referenceID": 6, "context": "We evaluate the algorithms on two different tasks: the word similarity task [16] and the word analogy task [2, 8].", "startOffset": 107, "endOffset": 113}, {"referenceID": 14, "context": "The word similarity task [16] requires to score pairs of word according to their relatedness.", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "The scores are in the range of [0, 10], where 0 stands for totally unrelated words and 10 stands for very much related or identical words.", "startOffset": 31, "endOffset": 38}, {"referenceID": 15, "context": "8 In order to compare between the BSG and SG models, we compute for each model the Spearman [17] and Kendall [18] rank correlation coefficients with respect to the ground truth.", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "8 In order to compare between the BSG and SG models, we compute for each model the Spearman [17] and Kendall [18] rank correlation coefficients with respect to the ground truth.", "startOffset": 109, "endOffset": 113}, {"referenceID": 6, "context": "The word analogy task [8] is essentially a completion task: a bank of questions in the form of \u2018 a w is to b w as", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "1 Datasets We trained both models on the corpus from [5].", "startOffset": 53, "endOffset": 56}, {"referenceID": 14, "context": "The word similarity evaluation dataset [16] contains 353 pairs of words that were scored by 13 \u2013 16 subjects according to their relatedness.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "The word analogy evaluation dataset [2, 8] consists of 14 distinct groups of analogy questions, where each group contains a different number of questions.", "startOffset": 36, "endOffset": 42}, {"referenceID": 6, "context": "The word analogy evaluation dataset [2, 8] consists of 14 distinct groups of analogy questions, where each group contains a different number of questions.", "startOffset": 36, "endOffset": 42}, {"referenceID": 14, "context": "3 Results Table 1 presents the Spearman and Kendall rank correlation coefficients for BSG and SG on the word similarity task [16].", "startOffset": 125, "endOffset": 129}, {"referenceID": 1, "context": "Comparing our results with the literature [2, 8], we see that the accuracies that were obtained by both models are relatively low.", "startOffset": 42, "endOffset": 48}, {"referenceID": 6, "context": "Comparing our results with the literature [2, 8], we see that the accuracies that were obtained by both models are relatively low.", "startOffset": 42, "endOffset": 48}, {"referenceID": 1, "context": "6 billions in [2, 8].", "startOffset": 14, "endOffset": 20}, {"referenceID": 6, "context": "6 billions in [2, 8].", "startOffset": 14, "endOffset": 20}, {"referenceID": 1, "context": "100 \u2013 600 in [2, 8].", "startOffset": 13, "endOffset": 19}, {"referenceID": 6, "context": "100 \u2013 600 in [2, 8].", "startOffset": 13, "endOffset": 19}, {"referenceID": 1, "context": "Third, our models were trained for 10 iterations, where the models in [2] ran for 50 \u2013 100 iterations.", "startOffset": 70, "endOffset": 73}, {"referenceID": 14, "context": "TABLE 1 A COMPARISON BETWEEN BSG AND SG ON THE WORD SIMILARITY TASK [16]", "startOffset": 68, "endOffset": 72}, {"referenceID": 6, "context": "405 TABLE 2 A COMPARISON BETWEEN BSG AND SG ON THE WORD ANALOGY TASK [8]", "startOffset": 69, "endOffset": 72}], "year": 2016, "abstractText": "Recently, several works in the domain of natural language processing presented successful methods for word embedding. Among them, the Skip-gram (SG) with negative sampling, known also as Word2Vec, advanced the state-of-the-art of various linguistics tasks. In this paper, we propose a scalable Bayesian neural word embedding algorithm that can be beneficial to general item similarity tasks as well. The algorithm relies on a Variational Bayes solution for the SG objective and a detailed step by step description of the algorithm is provided. We present experimental results that demonstrate the performance of the proposed algorithm and show it is competitive with the original SG method.", "creator": "PScript5.dll Version 5.2.2"}}}