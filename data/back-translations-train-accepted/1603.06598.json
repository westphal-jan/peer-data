{"id": "1603.06598", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Stack-propagation: Improved Representation Learning for Syntax", "abstract": "Traditional syntax models typically leverage part-of-speech (POS) information by constructing features from hand-tuned templates. We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations. We propose a simple method for learning a stacked pipeline of models which we call \"stack-propagation\". We apply this to dependency parsing and tagging, where we use the hidden layer of the tagger network as a representation of the input tokens for the parser. At test time, our parser does not require predicted POS tags. On 19 languages from the Universal Dependencies, our method is 1.3% (absolute) more accurate than a state-of-the-art graph-based approach and 2.7% more accurate than the most comparable greedy model.", "histories": [["v1", "Mon, 21 Mar 2016 20:12:44 GMT  (709kb,D)", "http://arxiv.org/abs/1603.06598v1", null], ["v2", "Wed, 8 Jun 2016 01:39:25 GMT  (729kb,D)", "http://arxiv.org/abs/1603.06598v2", "10 pages"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuan zhang", "david weiss"], "accepted": true, "id": "1603.06598"}, "pdf": {"name": "1603.06598.pdf", "metadata": {"source": "CRF", "title": "Stack-propagation: Improved Representation Learning for Syntax", "authors": ["Yuan Zhang", "David Weiss"], "emails": ["yuanzh@csail.mit.edu", "djweiss@google.com"], "sections": [{"heading": "1 Introduction", "text": "This year, the time has come for us to be able to go to a place where we can go to a place where we can go to a place where we can go to a place where we can go to a place where we can stay."}, {"heading": "2 Continuous Stacking Model", "text": "In this year, it has come to the point where we see ourselves in a position to go to another world, in which we go to another world, in which we go to another world, in which we go to another world, in which we go to a world, in which we go to a world, in which we go to a world, in which we in which we live, in which we are in which we live, in which we in which we live, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we live, in which we, in which we, in which we live, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we..."}, {"heading": "2.1 The Tagger Network", "text": "As described above, our POS tagger follows the basic structure of previous work with embedding, W ords feature extractionSuffixesC lustersEmbeddingHidden (Relu) Softmaxhidden and Softmax layers. Like the \"Windowapproach\" network by Collobert et al. (2011), the tagger is evaluated by token, with features extracted from a window of tokens surrounding the target. Input consists of a rich set of features for POS tagging that are deterministically extracted from the training data. As in previous work, the features are divided into groups of different sizes that share an embedding matrix E. Features for each group g are presented as a sparse matrix Xg with the dimension F g \u00d7 V g, where F g is the number of feature templates in the group, and V g is the number of feature templates in the group, and g is the vocabulary size of the template."}, {"heading": "2.2 The Parser Network", "text": "The parser component follows the same design as the POS tagger except for the features and output space. Instead of a window-based classifier, the features are extracted from an arcStandard parser configuration: stack s, buffer b, and the dependencies constructed to date. (Nivre, 2004) Implementation of this model is separated by four discrete features: words, labels (from previous decisions), POS tags, and morphological attributes (Chen and Manning)."}, {"heading": "3 Learning with Stack-propagation", "text": "In this area, we are able to set out in search of new paths that will lead us into the future."}, {"heading": "3.1 Implementation details", "text": "Following Weiss et al. (2015), we use minibatched averaged stochastic gradient descent (ASGD) (Bottou, 2010) with momentum (Hinton, 2012) to learn the parameters \u044b of the network. We use a separate learning rate, moving average and speed for the tagger network and the parser; the PARSER updates all averages, speeds and learning rates, while the TAGGER only updates the tagging factors. We adjusted the hyperparameters of the pulse rate \u00b5, the initial learning rate \u03b70 and the learning rate decay \u03b3 using pre-set data. Training data for parsing and tagging can be extracted either from the same corpus or from different corpus; in our experiments they were always the same. In order to weigh the two objectives against each other, we used a random sampling scheme to perform 10 epochs of PARSER updates and 5 epochs of TGER updates with our AGER updates before we found that we delivered better results in our experiments."}, {"heading": "4 Experiments", "text": "In this section, we evaluate our approach to various tasks for analyzing dependencies in a variety of languages."}, {"heading": "4.1 Experimental Setup", "text": "We first examined our model for 19 languages from Universal Dependencies Treebanks v1.2.2 We selected the 19 largest languages currently spoken for which full data was freely available. We used the rough universal tagset in our experiments without explicit morphological annotations. To measure the accuracy of the analysis, we used the arc-standard (Nivre, 2004) transition system with greedy decoding. Since this transition system only produces projective trees, we first apply a projection step to all tree banks before unrolling the gold derivatives during the training. We make an exception for Dutch, where we observed a significant gain in development data by introducing the SWAP action (Nivre, 2http: / universaldependencies.org2009)."}, {"heading": "4.2 Results", "text": "We present our main results to the Universal Treebanks in Table 2. We directly compare our approach to other baselines in two ways. Firstly, we compare the effectiveness of our learned continuous representations with those of Alberti et al. (2015), which link the predicted distribution OverPOS tags with word embedding as input to the parser. Since they also include beam search in education, we implement a greedy version of their method to allow direct comparison of token representations. We point out that this is the \"Pipeline (Ptag)\" baseline baseline. Secondly, we compare our architecture without POS tags as a regulation, which we call \"Ours (window-based) -based.\" This model has the same architecture as our complete model, but without POS monitoring and updates."}, {"heading": "5 Discussion", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "6 Conclusions", "text": "Through a simple learning method we call \"stack propagation,\" our model learns effective intermediate representations for parsing by using POS tags as a regularization of implicit representations. Our model outperforms all modern parsers when evaluated in 19 languages of the Universal Dependencies Treebank, and outperforms other greedy models of the Wall Street Journal. We observe that the ideas presented in this paper can also serve as a principal way to optimize upstream NLP components for downstream applications. In future work, we will expand this idea beyond sequence modeling to improve models in NLP that use trees as traits. The basic idea of stack propagation is that the hidden layers of neural models that are used to generate annotations themselves can be used."}, {"heading": "Acknowledgments", "text": "We would like to thank Ryan McDonald, Emily Pitler, Chris Alberti, Michael Collins and Slav Petrov for their repeated discussions, suggestions and feedback, as well as all members of the Google NLP Parsing Team. We would also like to thank Miguel Ballesteros for his support in running the character-based LSTM."}], "references": [{"title": "Improved transition-based parsing and tagging with neural networks", "author": ["Chris Alberti", "David Weiss", "Greg Coppola", "Slav Petrov."], "venue": "Proceedings of EMNLP 2015.", "citeRegEx": "Alberti et al\\.,? 2015", "shortCiteRegEx": "Alberti et al\\.", "year": 2015}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proceddings of EMNLP.", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["Bernd Bohnet", "Joakim Nivre."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and", "citeRegEx": "Bohnet and Nivre.,? 2012", "shortCiteRegEx": "Bohnet and Nivre.", "year": 2012}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L\u00e9on Bottou."], "venue": "Proceedings of COMPSTAT\u20192010, pages 177\u2013186. Springer.", "citeRegEx": "Bottou.,? 2010", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), volume 1, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa."], "venue": "JMLR.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine De Marneffe", "Bill MacCartney", "Christopher D. Manning."], "venue": "In", "citeRegEx": "Marneffe et al\\.,? 2006", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Neural crf parsing", "author": ["Greg Durrett", "Dan Klein."], "venue": "Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.", "citeRegEx": "Durrett and Klein.,? 2015", "shortCiteRegEx": "Durrett and Klein.", "year": 2015}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith."], "venue": "ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Semantic role labeling with neural network factors", "author": ["Nicholas FitzGerald", "Oscar Tckstrm", "Kuzman Ganchev", "Dipanjan Das."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP \u201915).", "citeRegEx": "FitzGerald et al\\.,? 2015", "shortCiteRegEx": "FitzGerald et al\\.", "year": 2015}, {"title": "Incremental joint pos tagging and dependency parsing in chinese", "author": ["Jun Hatori", "Takuya Matsuzaki", "Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "In IJCNLP,", "citeRegEx": "Hatori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hatori et al\\.", "year": 2011}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["Geoffrey E Hinton."], "venue": "Neural Networks: Tricks of the Trade, pages 599\u2013619. Springer.", "citeRegEx": "Hinton.,? 2012", "shortCiteRegEx": "Hinton.", "year": 2012}, {"title": "Simple semi-supervised dependency parsing", "author": ["Terry Koo", "Xavier Carreras P\u00e9rez", "Michael Collins."], "venue": "46th Annual Meeting of the Association for Computational Linguistics, pages 595\u2013603.", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1381\u20131391.", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Joint models for chinese pos tagging and dependency parsing", "author": ["Zhenghua Li", "Min Zhang", "Wanxiang Che", "Ting Liu", "Wenliang Chen", "Haizhou Li."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Li et al\\.,? 2011", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Joint optimization for chinese POS tagging and dependency parsing", "author": ["Zhenghua Li", "Min Zhang", "Wanxiang Che", "Ting Liu", "Wenliang Chen."], "venue": "IEEE/ACM Transactions on Audio, Speech & Language Processing, pages 274\u2013286.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre."], "venue": "Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, IncrementParsing \u201904, pages 50\u201357, Stroudsburg, PA, USA. Association", "citeRegEx": "Nivre.,? 2004", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Non-projective dependency parsing in expected linear time", "author": ["Joakim Nivre."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages", "citeRegEx": "Nivre.,? 2009", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "arXiv preprint arXiv:1104.2086.", "citeRegEx": "Petrov et al\\.,? 2011", "shortCiteRegEx": "Petrov et al\\.", "year": 2011}, {"title": "Joint chinese word segmentation, pos tagging and parsing", "author": ["Xian Qian", "Yang Liu."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 501\u2013", "citeRegEx": "Qian and Liu.,? 2012", "shortCiteRegEx": "Qian and Liu.", "year": 2012}, {"title": "Joint pos tagging and transition-based constituent parsing in chinese with non-local features", "author": ["Zhiguo Wang", "Nianwen Xue."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 733\u2013742.", "citeRegEx": "Wang and Xue.,? 2014", "shortCiteRegEx": "Wang and Xue.", "year": 2014}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of ACL 2015, pages 323\u2013333.", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Stacked generalization", "author": ["David H Wolpert."], "venue": "Neural networks.", "citeRegEx": "Wolpert.,? 1992", "shortCiteRegEx": "Wolpert.", "year": 1992}, {"title": "Randomized greedy inference for joint segmentation, POS tagging and dependency parsing", "author": ["Yuan Zhang", "Chengtao Li", "Regina Barzilay", "Kareem Darwish."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "In recent years, transition-based dependency parsers powered by neural network scoring functions have dramatically increased the state-of-theart in terms of both speed and accuracy (Chen and Manning, 2014; Alberti et al., 2015; Weiss et al., 2015).", "startOffset": 181, "endOffset": 247}, {"referenceID": 0, "context": "In recent years, transition-based dependency parsers powered by neural network scoring functions have dramatically increased the state-of-theart in terms of both speed and accuracy (Chen and Manning, 2014; Alberti et al., 2015; Weiss et al., 2015).", "startOffset": 181, "endOffset": 247}, {"referenceID": 22, "context": "In recent years, transition-based dependency parsers powered by neural network scoring functions have dramatically increased the state-of-theart in terms of both speed and accuracy (Chen and Manning, 2014; Alberti et al., 2015; Weiss et al., 2015).", "startOffset": 181, "endOffset": 247}, {"referenceID": 7, "context": "Similar approaches also achieve state-ofthe-art in other NLP tasks, such as constituency parsing (Durrett and Klein, 2015) or semantic role labeling (FitzGerald et al.", "startOffset": 97, "endOffset": 122}, {"referenceID": 9, "context": "Similar approaches also achieve state-ofthe-art in other NLP tasks, such as constituency parsing (Durrett and Klein, 2015) or semantic role labeling (FitzGerald et al., 2015).", "startOffset": 149, "endOffset": 174}, {"referenceID": 23, "context": "In the pipeline or stacking (Wolpert, 1992) method, these are predicted from an independently trained tagger and used as features in the parser.", "startOffset": 28, "endOffset": 43}, {"referenceID": 19, "context": "The POS tags may also contain only coarse information, such as when using the universal tagset of Petrov et al. (2011).", "startOffset": 98, "endOffset": 119}, {"referenceID": 12, "context": "either using semi-supervised clustering instead of POS tags (Koo et al., 2008) or building recurrent representations of words using neural networks (Dyer et al.", "startOffset": 60, "endOffset": 78}, {"referenceID": 8, "context": ", 2008) or building recurrent representations of words using neural networks (Dyer et al., 2015; Ballesteros et al., 2015).", "startOffset": 77, "endOffset": 122}, {"referenceID": 1, "context": ", 2008) or building recurrent representations of words using neural networks (Dyer et al., 2015; Ballesteros et al., 2015).", "startOffset": 77, "endOffset": 122}, {"referenceID": 14, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 10, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 2, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 20, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 21, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 15, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 24, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 0, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 4, "context": "Both networks are implemented with a refined version of the feed-forward network (Figure 3) from Chen and Manning (2014), as described in Weiss et al.", "startOffset": 97, "endOffset": 121}, {"referenceID": 4, "context": "Both networks are implemented with a refined version of the feed-forward network (Figure 3) from Chen and Manning (2014), as described in Weiss et al. (2015). We link the tagger network to the parser by translating traditional feature templates for parsing into feed-forward connections from the tagger to the parser (Figure 2).", "startOffset": 97, "endOffset": 158}, {"referenceID": 1, "context": "We observe a >2% absolute gain in labeled accuracy compared to state-of-the-art, LSTM-based greedy parsers (Ballesteros et al., 2015) and a >1% gain compared to a state-of-the-art, graphbased method (Lei et al.", "startOffset": 107, "endOffset": 133}, {"referenceID": 13, "context": ", 2015) and a >1% gain compared to a state-of-the-art, graphbased method (Lei et al., 2014).", "startOffset": 73, "endOffset": 91}, {"referenceID": 1, "context": "One important finding of this work is that, even without POS tags, our architecture outperforms recurrent approaches that build custom word representations using character-based LSTMs (Ballesteros et al., 2015).", "startOffset": 184, "endOffset": 210}, {"referenceID": 4, "context": "The basic unit of our model (Figure 3) is a simple, feed-forward network that has been shown to work very well for parsing tasks (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 129, "endOffset": 173}, {"referenceID": 22, "context": "The basic unit of our model (Figure 3) is a simple, feed-forward network that has been shown to work very well for parsing tasks (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 129, "endOffset": 173}, {"referenceID": 4, "context": "In a traditional stacking (pipeline) approach, we would use the discrete predicted POS tags from the tagger as features in the parser (Chen and Manning, 2014).", "startOffset": 134, "endOffset": 158}, {"referenceID": 4, "context": "To implement this, we show how we can reuse feature templates from Chen and Manning (2014) to specify the feed-forward connections from the tagger network to the parser network.", "startOffset": 67, "endOffset": 91}, {"referenceID": 5, "context": "Like the \u201cwindowapproach\u201d network of Collobert et al. (2011), the tagger is evaluated per-token, with features extracted from a window of tokens surrounding the target.", "startOffset": 37, "endOffset": 61}, {"referenceID": 17, "context": "(Nivre, 2004).", "startOffset": 0, "endOffset": 13}, {"referenceID": 4, "context": "Prior implementations of this model used up to four groups of discrete features: words, labels (from previous decisions), POS tags, and morphological attributes (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015).", "startOffset": 161, "endOffset": 227}, {"referenceID": 22, "context": "Prior implementations of this model used up to four groups of discrete features: words, labels (from previous decisions), POS tags, and morphological attributes (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015).", "startOffset": 161, "endOffset": 227}, {"referenceID": 0, "context": "Prior implementations of this model used up to four groups of discrete features: words, labels (from previous decisions), POS tags, and morphological attributes (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015).", "startOffset": 161, "endOffset": 227}, {"referenceID": 4, "context": "Thus, the overall training procedure is similar to that introduced in Chen and Manning (2014). To incorporate the POS tags as a regularization during learning, we take a fairly standard approach from multi-task learning.", "startOffset": 70, "endOffset": 94}, {"referenceID": 3, "context": "(2015), we use minibatched averaged stochastic gradient descent (ASGD) (Bottou, 2010) with momentum (Hinton, 2012) to learn the parameters \u0398 of the network.", "startOffset": 71, "endOffset": 85}, {"referenceID": 11, "context": "(2015), we use minibatched averaged stochastic gradient descent (ASGD) (Bottou, 2010) with momentum (Hinton, 2012) to learn the parameters \u0398 of the network.", "startOffset": 100, "endOffset": 114}, {"referenceID": 20, "context": "Following Weiss et al. (2015), we use minibatched averaged stochastic gradient descent (ASGD) (Bottou, 2010) with momentum (Hinton, 2012) to learn the parameters \u0398 of the network.", "startOffset": 10, "endOffset": 30}, {"referenceID": 1, "context": "\u201cB\u201915 LSTM\u201d is the character-based LSTM model (Ballesteros et al., 2015), while \u201cOurs (window)\u201d is our window-based architecture variant without stackprop.", "startOffset": 46, "endOffset": 72}, {"referenceID": 17, "context": "For simplicity, we use the arc-standard (Nivre, 2004) transition system with greedy decoding.", "startOffset": 40, "endOffset": 53}, {"referenceID": 1, "context": "Note that following Ballesteros et al. (2015), we did not use any auxiliary data beyond that in the treebanks, such as pre-trained word embeddings.", "startOffset": 20, "endOffset": 46}, {"referenceID": 16, "context": "For a final set of experiments, we evaluated on the standard Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993)), dependencies generated from version 3.", "startOffset": 113, "endOffset": 134}, {"referenceID": 6, "context": "0 of the Stanford converter (De Marneffe et al., 2006). We followed standard practice and used sections 2-21 for training, section 22 for development, and section 23 for testing. Following Weiss et al. (2015), we used section 24 to tune any hyperparameters of the model to avoid overfitting to the development set.", "startOffset": 32, "endOffset": 209}, {"referenceID": 0, "context": "First, we compare the effectiveness of our learned continuous representations with those of Alberti et al. (2015), who use the predicted distribution over", "startOffset": 92, "endOffset": 114}, {"referenceID": 8, "context": "NO TAGS Dyer et al. (2015) 92.", "startOffset": 8, "endOffset": 27}, {"referenceID": 8, "context": "60 Dyer et al. (2015) 93.", "startOffset": 3, "endOffset": 22}, {"referenceID": 0, "context": "05 Alberti et al. (2015) 94.", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "For reference, we show the most accurate models from Alberti et al. (2015) and Weiss et al.", "startOffset": 53, "endOffset": 75}, {"referenceID": 0, "context": "For reference, we show the most accurate models from Alberti et al. (2015) and Weiss et al. (2015), which use a deeper model and beam search for inference.", "startOffset": 53, "endOffset": 99}, {"referenceID": 1, "context": "Since this model never observes POS tags in any way, we compare against a recurrent character-based parser (Ballesteros et al., 2015) which is state-of-the-art when no POS tags are provided.", "startOffset": 107, "endOffset": 133}, {"referenceID": 13, "context": "3 Finally, we compare to RGBParser (Lei et al., 2014), a state-of-the art graph-based (non-greedy) approach.", "startOffset": 35, "endOffset": 53}, {"referenceID": 1, "context": "We thank Ballesteros et al. (2015) for their assistance running their code on the treebanks.", "startOffset": 9, "endOffset": 35}, {"referenceID": 8, "context": "And while stackprop doesn\u2019t achieve the highest reported accuracies on the WSJ, it does achieve competitive accuracies and outperforms prior state-of-the-art for greedy methods (Dyer et al., 2015).", "startOffset": 177, "endOffset": 196}, {"referenceID": 2, "context": "transition system of Bohnet and Nivre (2012), which augments the SHIFT transition to predict POS tags.", "startOffset": 21, "endOffset": 45}, {"referenceID": 0, "context": "We compare these variants along with our reimplementation of the pipelined model of Alberti et al. (2015) in Table 4.", "startOffset": 84, "endOffset": 106}, {"referenceID": 4, "context": "Previous neural parsers that use POS tags require learning embeddings for words and other features on top of the parameters used in the POS tagger (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 147, "endOffset": 191}, {"referenceID": 22, "context": "Previous neural parsers that use POS tags require learning embeddings for words and other features on top of the parameters used in the POS tagger (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 147, "endOffset": 191}], "year": 2017, "abstractText": "Traditional syntax models typically leverage part-of-speech (POS) information by constructing features from hand-tuned templates. We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations. We propose a simple method for learning a stacked pipeline of models which we call \u201cstack-propagation.\u201d We apply this to dependency parsing and tagging, where we use the hidden layer of the tagger network as a representation of the input tokens for the parser. At test time, our parser does not require predicted POS tags. On 19 languages from the Universal Dependencies, our method is 1.3% (absolute) more accurate than a state-of-the-art graph-based approach and 2.7% more accurate than the most comparable greedy model.", "creator": "TeX"}}}