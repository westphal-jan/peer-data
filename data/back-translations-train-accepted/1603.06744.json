{"id": "1603.06744", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2016", "title": "Latent Predictor Networks for Code Generation", "abstract": "Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.", "histories": [["v1", "Tue, 22 Mar 2016 11:41:51 GMT  (1701kb,D)", "http://arxiv.org/abs/1603.06744v1", null], ["v2", "Wed, 8 Jun 2016 14:46:00 GMT  (2543kb,D)", "http://arxiv.org/abs/1603.06744v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["wang ling", "edward grefenstette", "karl moritz hermann", "tom\\'a\\v{s} ko\\v{c}isk\\'y", "rew senior", "fumin wang", "phil blunsom"], "accepted": true, "id": "1603.06744"}, "pdf": {"name": "1603.06744.pdf", "metadata": {"source": "CRF", "title": "Latent Predictor Networks for Code Generation", "authors": ["Wang Ling", "Edward Grefenstette", "Karl Moritz Hermann", "Tomas Kocisky", "Andrew Senior", "Fumin Wang", "Phil Blunsom"], "emails": ["lingwang@google.com", "etg@google.com", "kmh@google.com", "tkocisky@google.com", "andrewsenior@google.com", "awaw@google.com", "pblunsom@google.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2 Dataset Extraction", "text": "We draw data from open source implementations of two distinct TCGs, MTG in Java1 and HS in Python.2 The statistics of the corpora are in Table 1. In both corpora, each map is implemented in a separate class file, which we free from imports and comments. We categorize the contents of each map into two different groups: single fields containing only one value; and text fields containing multiple words representing different units of meaning. In MTG, there are six singular fields (attack, defense, rarity, sentence, ID and health) and four text fields (cost, type, name and description), whereas HS cards have eight singular fields (attack, health, cost and durability, rarity, type, race and class) and two text fields (name and description). Text fields are symbolized by splitting them into whitespace and punctuation, taking domain-specific artifacts into account (e.g., the Manze and MTG are described as \"two green and two green fields\")."}, {"heading": "3 Problem Definition", "text": "Given the description of a map x, our decoding problem is to find the Y code in such a way that: y = argmax ylogP (y | x) (1) Here logP (y | x) is estimated by a particular model. We define y = y1.. y | y | as the string of code with the length | y |. We index each input field with k = 1.. | x |, where | x | quantifies the number of input fields. | xk | denotes the number of characters in xk and xki selects the i-th character."}, {"heading": "4 Structured Attention", "text": "The reason why we draw so much attention to ourselves is the fact that we pay so much attention to ourselves. \"The reason why the attention model of [7] ki draws so much attention to us is another:\" By the chain rule, \"logP (y | x) = [1] = [2], logP (yt | yt \u2212 1), logP (1), logP (1), logP (1), logP (1), logP (yt \u2212 1), logP (1), logP (1), logP (1), logP (1), logP (1), logP (1), logP (1), logP (1),\" P (1), \"P (1),\" P, \"P (1),\" P, \"P (1),\" P, \"P (1),\" P, \"P (1),\" P, \"P (1),\", \"P (1),\" P (1), \"P,\" P (1), \"P,\" P (1), \"P (1),\" P, \"P (1),\" P (1), \"P (1),\" P, \"P (1),\" P (1), \"), P (1), P (1,\"), P (1), P (1, \"), P (1), P (1), P (1), P (1, P (1,\"), P (1), P (1), P (1, P (1), P (1), P (1, P (1, P (1), P (1), P, P (1), P (1, P (1), P (1, P (1), P (1, P (1), P (1), P (1), P (1, P (1, P (1), P (1), P (1, P (1, P (1), P (1), P (1), P (1), P (1, P (1), P (1, P (1), P (1, P (1), P (1, P (1,"}, {"heading": "5 Latent Predictor Networks", "text": "\"We assume that the probability of a distribution of units that can be copied is very high."}, {"heading": "5.1 Inference", "text": "In the training period, we use backpropagation to determine the probability of the observed code according to Equation 7. Gradient calculation must be performed with respect to each calculated probability P (rt | y1.. yt \u2212 1, x) and P (st | y1.. yt \u2212 1, x, rt).The derivative log P (y | x) \u2202 P (rt | y1.. yt \u2212 1, x) yields: \u2202 \u03b1tP (rt | y1.. yt \u2212 1, x) \u03b2t, rt + \u043frt P (y | x) \u2202 P (rt | y1.. yt \u2212 1, x) = \u03b1t\u03b2t, rt \u03b1 | y | + 1 (9) Here \u03b1t denotes the cumulative probability of all values up to the time stamp t and \u03b2t, rt denotes the cumulative probability from the predictor rt t, with the exception of P \u2212 rt the cumulative probability of all values up to the time stamp t and \u00df, rt denotes the cumulative probability from the predictor."}, {"heading": "5.2 Decoding", "text": "Each state S corresponds to a selection of predictor rt and segment st to a given timestamp t. This state is evaluated as V (S) = logP (st | y1.. yt \u2212 1, x, rt) + logP (rt | y1.. yt \u2212 1, x) + V (prev (S))), where prev (S) denotes the preceding state of S. For each timestamp, the n states with the highest values V are extended, where n is the size of the bar. For each predictor rt, each output st generates a new state. Finally, for each timestamp t, all states that up to that time produce the same output are merged by summing their probabilities."}, {"heading": "6 Code Compression", "text": "Since the attention-based model traverses all input devices at each generation step, the generation becomes quite expensive for datasets such as v len (v len), where the average card code contains 1,080 characters. Although this is not the main contribution in our work, we propose a simple method to compress the code while maintaining the structure of the code, which allows us to train on datasets with longer code (e.g. MTG). The idea behind this method is that many keywords can be learned in the programming language (e.g. public and return) as well as frequently used functions and classes (e.g. card) without character information. We use these by mapping such strings to additional symbols Xi (e.g. public class copy () \u2192 \"X1 X2 X3 X3\"). Formally, we search for the string v among all strings V (max) up to the length max."}, {"heading": "7 Experiments", "text": "In fact, it is not the case that the two models presented in this publication are a model that requires the ability to generalize in other areas, but rather a model in which the different types of generalization and generalization are interwoven. (...) It is not that it is a model of generalization, but a model of generalization. \"(...) It is as if it is a line of Python codes that works with multiple input fields by linking them together, while the latter designates our proposed\" sequence \"and.\" Attention. \"We can also address our problem within the framework of semantic parsing [2, 14, 5]. Unfortunately, these approaches define strong assumptions regarding grammar and the structure of production, which makes it difficult to make it.\""}, {"heading": "7.1 Results", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is not a country, but a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which"}, {"heading": "8 Related Work", "text": "While we focus on widely used programming languages, namely Java and Python, our work relates to studies on generating arbitrary executable code, including regular expression generation [24] and input document parsing code [4]. Much research has also been invested in generating formal languages, such as database queries [25, 26], agent-specific language [27] or instructions for smartphones [28]. Finally, a considerable effort in this task focuses on semantic parsing [2, 3, 4, 5, 6]. Recently proposed models focus on combinatory categorical grammars [24, 5], Bajesian tree-walkers [3, 4] and probabilistic context-free grammars [16]. Work in natural language programming [30, 31] where users write sequential lines of code from natural language is also related to our work."}, {"heading": "9 Conclusion", "text": "We introduced an architecture of neural networks called the Latent Prediction Network, which enables efficient marginalization across multiple predictors. Within this architecture, we propose a generative model for code generation that combines a Softmax character layer to generate language-specific tokens and multiple pointer networks to copy keywords from input. Along with other enhancements, namely structured attention and code compression, our model is applied to both existing data sets and a newly created model with implementation of TCG playing cards. Our experiments show that our model exceeds several benchmarks showing the importance of combining different types of predictors."}], "references": [{"title": "Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst. Moses"], "venue": "In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Learning for semantic parsing with statistical machine translation", "author": ["Yuk Wah Wong", "Raymond J. Mooney"], "venue": "In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Semantic parsing with bayesian tree transducers", "author": ["Bevan Keeley Jones", "Mark Johnson", "Sharon Goldwater"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "From natural language specifications to program input parsers", "author": ["Tao Lei", "Fan Long", "Regina Barzilay", "Martin Rinard"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Broad-coverage ccg semantic parsing with amr", "author": ["Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Language to code: Learning semantic parsers for if-this-then-that recipes", "author": ["Chris Quirk", "Raymond Mooney", "Michel Galley"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "R\u00e1mon Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Learning to generate pseudo-code from source code using statistical machine translation", "author": ["Yusuke Oda", "Hiroyuki Fudaba", "Graham Neubig", "Hideaki Hata", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura"], "venue": "In 30th IEEE/ACM International Conference on Automated Software Engineering", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Semi-markov conditional random fields for information extraction", "author": ["Sunita Sarawagi", "William W. Cohen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "A generative model for parsing natural language to meaning representations", "author": ["Wei Lu", "Hwee Tou Ng", "Wee Sun Lee", "Luke S. Zettlemoyer"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Inducing probabilistic ccg grammars from logical form with higher-order unification", "author": ["Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Semantic parsing as machine translation", "author": ["Jacob Andreas", "Andreas Vlachos", "Stephen Clark"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang"], "venue": "Comput. Linguist.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz Josef Och", "Hermann Ney"], "venue": "Comput. Linguist.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Minimum Error Rate Semi-Ring", "author": ["Artem Sokolov", "Fran\u00e7ois Yvon"], "venue": "Proceedings of the European Conference on Machine Translation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn"], "venue": "In Proceedings of the 51th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "CoRR, abs/1212.5701,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Exploring the limits of language modeling", "author": ["Rafal J\u00f3zefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Using semantic unification to generate regular expressions from natural language", "author": ["Nate Kushman", "Regina Barzilay"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["John M. Zelle", "Raymond J. Mooney"], "venue": "In AAAI/IAAI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1996}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Learning to transform natural to formal languages", "author": ["Rohit J. Kate", "Yuk Wah Wong", "Raymond J. Mooney"], "venue": "In Proceedings of the Twentieth National Conference on Artificial Intelligence", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Smartsynth: Synthesizing smartphone automation scripts from natural language", "author": ["Vu Le", "Sumit Gulwani", "Zhendong Su"], "venue": "In Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["S.R.K. Branavan", "Harr Chen", "Luke S. Zettlemoyer", "Regina Barzilay"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Programming with unrestricted natural language", "author": ["David Vadas", "James R. Curran"], "venue": "In Proceedings of the Australasian Language Technology Workshop", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "Integrating programming by example and natural language programming", "author": ["Mehdi Hafezi Manshadi", "Daniel Gildea", "James F. Allen"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "On end-to-end program generation from user intention by deep neural networks", "author": ["Lili Mou", "Rui Men", "Ge Li", "Lu Zhang", "Zhi Jin"], "venue": "CoRR, abs/1510.07211,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "A Convolutional Attention Network for Extreme Summarization of Source Code", "author": ["M. Allamanis", "H. Peng", "C. Sutton"], "venue": "ArXiv e-prints,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Neural Architectures for Named Entity Recognition", "author": ["G. Lample", "M. Ballesteros", "S. Subramanian", "K. Kawakami", "C. Dyer"], "venue": "ArXiv e-prints, March 2016", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "The generation of both natural and formal languages often requires models conditioned on diverse predictors [1, 2].", "startOffset": 108, "endOffset": 114}, {"referenceID": 1, "context": "The generation of both natural and formal languages often requires models conditioned on diverse predictors [1, 2].", "startOffset": 108, "endOffset": 114}, {"referenceID": 1, "context": "This dataset presents additional challenges to prior work in code generation [2, 3, 4, 5, 6], including the handling of structured input\u2014i.", "startOffset": 77, "endOffset": 92}, {"referenceID": 2, "context": "This dataset presents additional challenges to prior work in code generation [2, 3, 4, 5, 6], including the handling of structured input\u2014i.", "startOffset": 77, "endOffset": 92}, {"referenceID": 3, "context": "This dataset presents additional challenges to prior work in code generation [2, 3, 4, 5, 6], including the handling of structured input\u2014i.", "startOffset": 77, "endOffset": 92}, {"referenceID": 4, "context": "This dataset presents additional challenges to prior work in code generation [2, 3, 4, 5, 6], including the handling of structured input\u2014i.", "startOffset": 77, "endOffset": 92}, {"referenceID": 5, "context": "This dataset presents additional challenges to prior work in code generation [2, 3, 4, 5, 6], including the handling of structured input\u2014i.", "startOffset": 77, "endOffset": 92}, {"referenceID": 6, "context": "Thus, we propose an extension to attention-based neural models [7] to attend over structured inputs.", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "Background When |x| = 1, the attention model of [7] applies.", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "We implement g using a Long Short-Term Memory (LSTM) RNNs [8].", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "We first encode each token xki using the C2W model described in [9], which is a replacement for lookup tables where word representations are learned at the character level (cf.", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "As explained earlier, this problem is not specific to our task: for instance, in the dataset of [10], a model must learn to map from timeout = int ( timeout ) to \u201cconvert timeout into an integer.", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "\u03b1 and \u03b2 can be computed efficiently using the forward-backward algorithm for Semi-Markov models [12], where we associate P (rt | y1.", "startOffset": 96, "endOffset": 100}, {"referenceID": 9, "context": "Additionally, to test the model\u2019s ability of generalize to other domains, we report results in the Django dataset [10], comprising of 16000 training, 1000 development and 1805 test annotations.", "startOffset": 114, "endOffset": 118}, {"referenceID": 11, "context": "Neural Benchmarks We implement two standard neural networks, namely a sequence-tosequence model [13] and an attention-based model [7].", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "Neural Benchmarks We implement two standard neural networks, namely a sequence-tosequence model [13] and an attention-based model [7].", "startOffset": 130, "endOffset": 133}, {"referenceID": 1, "context": "Machine Translation Baselines Our problem can also be viewed in the framework of semantic parsing [2, 14, 3, 5].", "startOffset": 98, "endOffset": 111}, {"referenceID": 12, "context": "Machine Translation Baselines Our problem can also be viewed in the framework of semantic parsing [2, 14, 3, 5].", "startOffset": 98, "endOffset": 111}, {"referenceID": 2, "context": "Machine Translation Baselines Our problem can also be viewed in the framework of semantic parsing [2, 14, 3, 5].", "startOffset": 98, "endOffset": 111}, {"referenceID": 4, "context": "Machine Translation Baselines Our problem can also be viewed in the framework of semantic parsing [2, 14, 3, 5].", "startOffset": 98, "endOffset": 111}, {"referenceID": 13, "context": "Unfortunately, these approaches define strong assumptions regarding the grammar and structure of the output, which makes it difficult to generalize for other domains [15].", "startOffset": 166, "endOffset": 170}, {"referenceID": 14, "context": "However, the work in [16] provides evidence that using machine translation systems without committing", "startOffset": 21, "endOffset": 25}, {"referenceID": 0, "context": "We follow the same approach and create a phrase-based [1] model and a hierarchical model (or PCFG) [17] as benchmarks for the work presented here.", "startOffset": 54, "endOffset": 57}, {"referenceID": 15, "context": "We follow the same approach and create a phrase-based [1] model and a hierarchical model (or PCFG) [17] as benchmarks for the work presented here.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "We used the models implemented in Moses to generate these baselines using standard parameters, using IBM Alignment Model 4 for word alignments [18], MERT for tuning [19] and a 4-gram Kenser-Ney Smoothed language model [20].", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "We used the models implemented in Moses to generate these baselines using standard parameters, using IBM Alignment Model 4 for word alignments [18], MERT for tuning [19] and a 4-gram Kenser-Ney Smoothed language model [20].", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "We used the models implemented in Moses to generate these baselines using standard parameters, using IBM Alignment Model 4 for word alignments [18], MERT for tuning [19] and a 4-gram Kenser-Ney Smoothed language model [20].", "startOffset": 218, "endOffset": 222}, {"referenceID": 5, "context": "Retrieval Baseline It was reported in [6] that a simple retrieval method that outputs the most similar input for each sample, measured using Levenshtein Distance, leads to good results.", "startOffset": 38, "endOffset": 41}, {"referenceID": 19, "context": "Thus, we also test using BLEU-4 [21] at the token level.", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "Training is performed using mini-batches of 20 samples using AdaDelta [22] and we report results using the iteration with the highest BLEU score on the validation set (tested at intervals of 5000 mini-batches).", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "While the reason for this is uncertain, it is similar to the finding that language models that output characters tend to under-perform those that output words [23].", "startOffset": 159, "endOffset": 163}, {"referenceID": 22, "context": "These include generating regular expressions [24], and the code for parsing input documents [4].", "startOffset": 45, "endOffset": 49}, {"referenceID": 3, "context": "These include generating regular expressions [24], and the code for parsing input documents [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 23, "context": "Much research has also been invested in generating formal languages, such as database queries [25, 26], agent specific language [27] or smart phone instructions [28].", "startOffset": 94, "endOffset": 102}, {"referenceID": 24, "context": "Much research has also been invested in generating formal languages, such as database queries [25, 26], agent specific language [27] or smart phone instructions [28].", "startOffset": 94, "endOffset": 102}, {"referenceID": 25, "context": "Much research has also been invested in generating formal languages, such as database queries [25, 26], agent specific language [27] or smart phone instructions [28].", "startOffset": 128, "endOffset": 132}, {"referenceID": 26, "context": "Much research has also been invested in generating formal languages, such as database queries [25, 26], agent specific language [27] or smart phone instructions [28].", "startOffset": 161, "endOffset": 165}, {"referenceID": 27, "context": "Finally, mapping natural language into a sequence of actions for the generation of executable code [29].", "startOffset": 99, "endOffset": 103}, {"referenceID": 1, "context": "Finally, a considerable effort in this task has focused on semantic parsing [2, 3, 4, 5, 6].", "startOffset": 76, "endOffset": 91}, {"referenceID": 2, "context": "Finally, a considerable effort in this task has focused on semantic parsing [2, 3, 4, 5, 6].", "startOffset": 76, "endOffset": 91}, {"referenceID": 3, "context": "Finally, a considerable effort in this task has focused on semantic parsing [2, 3, 4, 5, 6].", "startOffset": 76, "endOffset": 91}, {"referenceID": 4, "context": "Finally, a considerable effort in this task has focused on semantic parsing [2, 3, 4, 5, 6].", "startOffset": 76, "endOffset": 91}, {"referenceID": 5, "context": "Finally, a considerable effort in this task has focused on semantic parsing [2, 3, 4, 5, 6].", "startOffset": 76, "endOffset": 91}, {"referenceID": 22, "context": "Recently proposed models focus on Combinatory Categorical Grammars [24, 5], Bayesian Tree Transducers [3, 4] and Probabilistic Context Free Grammars [16].", "startOffset": 67, "endOffset": 74}, {"referenceID": 4, "context": "Recently proposed models focus on Combinatory Categorical Grammars [24, 5], Bayesian Tree Transducers [3, 4] and Probabilistic Context Free Grammars [16].", "startOffset": 67, "endOffset": 74}, {"referenceID": 2, "context": "Recently proposed models focus on Combinatory Categorical Grammars [24, 5], Bayesian Tree Transducers [3, 4] and Probabilistic Context Free Grammars [16].", "startOffset": 102, "endOffset": 108}, {"referenceID": 3, "context": "Recently proposed models focus on Combinatory Categorical Grammars [24, 5], Bayesian Tree Transducers [3, 4] and Probabilistic Context Free Grammars [16].", "startOffset": 102, "endOffset": 108}, {"referenceID": 14, "context": "Recently proposed models focus on Combinatory Categorical Grammars [24, 5], Bayesian Tree Transducers [3, 4] and Probabilistic Context Free Grammars [16].", "startOffset": 149, "endOffset": 153}, {"referenceID": 28, "context": "The work in natural language programming [30, 31], where users write lines of code from natural language, is also related to our work.", "startOffset": 41, "endOffset": 49}, {"referenceID": 29, "context": "The work in natural language programming [30, 31], where users write lines of code from natural language, is also related to our work.", "startOffset": 41, "endOffset": 49}, {"referenceID": 9, "context": "Finally, the reverse mapping from code into natural language is explored in [10].", "startOffset": 76, "endOffset": 80}, {"referenceID": 30, "context": "Character-based sequence-to-sequence models have previously been used to generate code from natural language in [32].", "startOffset": 112, "endOffset": 116}, {"referenceID": 6, "context": "Inspired by these works, LPNs provide a richer framework by employing attention models [7], pointer networks [11] and character-based embeddings [9].", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "Inspired by these works, LPNs provide a richer framework by employing attention models [7], pointer networks [11] and character-based embeddings [9].", "startOffset": 145, "endOffset": 148}, {"referenceID": 31, "context": "Our formulation can also be seen as a generalization of [33], who implement a special case where two predictors have the same granularity (a sub-token softmax and a pointer network).", "startOffset": 56, "endOffset": 60}, {"referenceID": 32, "context": "Finally, HMMs have been employed in neural models to marginalize over label sequences in [34, 35] by modeling transitions between labels.", "startOffset": 89, "endOffset": 97}, {"referenceID": 33, "context": "Finally, HMMs have been employed in neural models to marginalize over label sequences in [34, 35] by modeling transitions between labels.", "startOffset": 89, "endOffset": 97}], "year": 2016, "abstractText": "Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.", "creator": "LaTeX with hyperref package"}}}