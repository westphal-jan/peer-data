{"id": "1603.06807", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2016", "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus", "abstract": "Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question-answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question-answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear to be indistinguishable from real human-generated questions.", "histories": [["v1", "Tue, 22 Mar 2016 14:25:16 GMT  (452kb,D)", "http://arxiv.org/abs/1603.06807v1", "13 pages, 1 figure, 7 tables"], ["v2", "Sun, 29 May 2016 20:00:20 GMT  (452kb,D)", "http://arxiv.org/abs/1603.06807v2", "13 pages, 1 figure, 7 tables"]], "COMMENTS": "13 pages, 1 figure, 7 tables", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["iulian vlad serban", "alberto garc\u00eda-dur\u00e1n", "\u00e7aglar g\u00fcl\u00e7ehre", "sungjin ahn", "sarath chandar", "aaron c courville", "yoshua bengio"], "accepted": true, "id": "1603.06807"}, "pdf": {"name": "1603.06807.pdf", "metadata": {"source": "CRF", "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus", "authors": ["Iulian Vlad Serban", "Alberto Garc\u0131\u0301a-Dur\u00e1n", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio"], "emails": ["sarath.chandar@umontreal.ca", "aaron.courville@umontreal.ca", "yoshua.bengio@umontreal.ca", "alberto.garcia-duran@utc.fr"], "sections": [{"heading": "1 Introduction", "text": "(Lopez et al., 2011; * First authors).The question of the manner in which the question is posed, the question, the question, the question, the question, the question, the question, the question, the question, the question, the question, the question, the question, the question, the question, the question, the question, the question, the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the"}, {"heading": "2 Related Work", "text": "In recent years, the issue of Generation has aroused interest with remarkable work by Rus et al. (2010), followed by increasing interest from the Community Natural Language Generation (NLG). A simple rules-based approach has been suggested in various studies as wh-fronting or wh-inversion (Kalady et al., 2010; Ali et al., 2010), with the disadvantage that the semantic content of words is not used apart from their syntactical role. The problem of determining the question type (e.g. that a where-question should be triggered for places) that requires knowledge of the category type of the elements involved in the sentence has been addressed in two different ways: through the use of called entity recognizers (Mannem et al., 2010; Yao and Zhang, 2010) or semantic role markers (Chen et al al al, 2009). In Curto et al al al al al al al, questions are divided into classes according to their syntactical structure."}, {"heading": "3 Task Definition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Knowledge Bases", "text": "In general, a KB can be considered a multirelational graph consisting of a series of nodes (units) and a series of edges (relationships) that connect nodes to each other. In Freebase (Bollacker et al., 2008), these relationships are directed and always connect exactly two units. In Freebase, for example, the two units Fire Creek and Nantahala National Forest are connected to each other by the relationship that is contained by the relationship. Since the Triple {fire Creek, contained by, nantahala National Forest} represents complete and self-contained information, it is also referred to as a fact in which Fire Creek is the subject (head of the edge), contained by the relationship, and nantahala National Forest is the object (tail of the edge)."}, {"heading": "3.2 Transducing Facts to Questions", "text": "We aim to translate a fact into a question in such a way: 1. The question concerns the subject and the relation of the fact, and 2. The object of the fact represents a valid answer to the generated question. We model this in a probable framework as a directed graphic model: P (Q | F) = N \u0441n = 1P (wn | w < n, F), (1) where F = (subject, relationship, object) represents the fact, Q = (w1,.., wN) represents the question as a sequence of characters w1,..., wN and w < n represents all characters generated before tokens wn. Specifically, wN stands for the question mark symbol '?'."}, {"heading": "3.3 Dataset", "text": "We use the SimpleQuestions dataset (Bordes et al., 2015) to train our models, which is by far the largest dataset of question-and-answer pairs created by humans based on a KB. It contains over 100K question-and-answer pairs created by users on Amazon Mechanical Turk3 in English based on the Freebase KB. 4 In order to create the questions, human participants were each shown an entire freebase fact and asked to formulate a question in such a way that the object of the presented fact responds to the question.4 Consequently, both the subject and the relationship are explicitly stated in each question.4 But indirectly, characteristics of the object can also be given, since people also have access to it. Frequently, when commentators formulate a question, they tend to be more informative about the target object by giving specific information about it in the generated question.4 For example, the question of which city the American actress was from, the only one of the objects in the American city was necessarily informed, or the only one of the objects in the American city X was necessarily required in the object."}, {"heading": "4 Model", "text": "We propose to address the problem with models inspired by the recent success of neural machine translation models (Sutskever et al., 2014; Bahdanau et al., 2015). Intuitively, one can imagine the translation task as a \"lossy translation\" from structured knowledge (facts) into human language (questions in natural language), in which certain aspects of structured knowledge are deliberately omitted (e.g. the name of the object). Typically, these models consist of two components: an encoder encoding the source phrase into one or more fixed vectors, and a decoder decoding the target phrase based on the results of the encoder."}, {"heading": "4.1 Encoder", "text": "In contrast to the neural machine translation frame, our source language is not a language of its own, but rather a sequence of three variables that constitute a fact. We propose an encoder submodel that encodes each atom of fact into an embedding. Each atom {s, r, o} can stand for subject, relationship, or object of a fact F = (s, r, o) is represented as a 1-of-K vector Xatom whose embedding is obtained as an eatom = an atom, where an RDEnc \u00b7 K is the embedding matrix of the input word atom and K is the size of that vocabulary. The encoder transforms this embedding into an enc (F) atom, RHDec as an enc (F) atom = WEnceatom, where WEnceatom, RHDec \u00d7 DEnc. This embedding matric, an encoder matrix, could be an encoding (Wwo), an encoding (Wwo)."}, {"heading": "4.2 Decoder", "text": "For the decoder, we use a GRU reurrent neural network (RNN) (Cho et al., 2014) with an attention mechanism (Bahdanau et al., 2015) on the encoder representation to generate the associated question Q., It has been shown that the GRU RNN in each time step n as: grn = 1 + Crc (F, hn \u2212 1) + CN (2) gun = 1 + Cuc (F, hn \u2212 1) + Uuhn \u2212 1) (3) h = tanh (WrEoutwn \u2212 1 + Cc (F, hn \u2212 1)."}, {"heading": "4.3 Modeling the Source Language", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in"}, {"heading": "4.4 Generating Questions", "text": "To solve the problem of data sparseness and the hitherto invisible words on the landing page, we draw inspiration from the placeholders suggested for dealing with rare words in neural machine translation by Luong et al. (2015). For each question and answer pair, we are therefore looking for words in Question5Extracted from one of the most recent freebase dumps (downloaded until mid-August 2015) https: / / developopers. google.com / freebase / datawhich overlap with words in the subject name of the fact chain of facts.6 We heuristically estimate the sequence of the most likely words in the question corresponding to the subject chain. These words will then be replaced by the placeholder model < Placeholder-Date >. For example, given the fact {fires creek, contained by, nantahala national forest} the original question Which forest is Fires Creek in the question that the question is turned into the question of the < what is the subtext < the model <"}, {"heading": "4.5 Template-based Baseline", "text": "To compare our neural network models, we propose a (non-parametric) template-based baseline model that uses the entire training set when generating a question. The baseline works with questions modified with the placeholder as in the previous section. If you use a fact F as input, the baseline in the training set selects an equally random fact Fc, with Fc having the same relationship to F. Then, the baseline looks at the questions that correspond to Fc, and as in the SP model, the placeholder token in the question is replaced by the subject line of the fact F in the last step."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Training Procedure", "text": "To train the neural network models, we optimized the log probability using the grade-based first order optimization algorithm Adam (Kingma and Ba, 2015). To decide when to stop training, we used the early pausing with patience (Bengio, 2012) of the METEOR value obtained for the validation set. In all experiments, we used the standard division of the SimpleQuestions dataset into training, validation, and test sets. We trained TransE embedding dimensions 200 for each subject, relationship, and object. Based on preliminary experiments, we set the learning rate for all neural network models to 0.0025 and cut parameter gradations with standards greater than 0.1. We also set the embedding dimensionality of words to 200, and the hidden state of the RNN decoder to dimensionality 600."}, {"heading": "5.2 Evaluation", "text": "In order to examine the performance of our models, we use both automatic evaluation metrics and human evaluators."}, {"heading": "5.2.1 Automatic Evaluation Metrics", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to make them a reality."}, {"heading": "5.2.2 Human Evaluation Study", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution that meets the needs of the people."}, {"heading": "6 Conclusion", "text": "The neural networks combine ideas from newer neural network architectures for statistical machine translation as well as multirelational knowledge data embedding to overcome scarcity problems and placeholder techniques for dealing with rare words. The generated question and answer pairs are evaluated using automated evaluation metrics such as BLEU, METEOR and sentence similarity, surpassing a template-based baseline model. When evaluated by untrained human subjects, the question and answer pairs generated by our most powerful neural network seem indistinguishable from real human-generated questions. Finally, we use our most powerful neural network model to generate a corpus of 30M question and answer pairs that we hope future researchers will be able to improve their question-answer systems."}, {"heading": "Acknowledgments", "text": "The authors thank IBM Research, NSERC, Canada Research Chairs and CIFAR for funding, Yang Yu, Bing Xiang, Bowen Zhou and Gerald Tesauro for constructive feedback, and Antoine Bordes, Nicolas Usunier, Sumit Chopra and Jason Weston for providing the SimpleQuestions dataset. This research was made possible in part by the support of Calcul Qubec (www.calculquebec.ca) and Compute Canada (www.computecanada.ca)."}, {"heading": "A Supplemental Material: Generated Questions", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Automation of question generation from sentences", "author": ["Ali et al.2010] Husam Ali", "Yllias Chali", "Sadid A Hasan"], "venue": "In Proceedings of QG2010: The Third Workshop on Question Generation,", "citeRegEx": "Ali et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ali et al\\.", "year": 2010}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures", "author": ["Banerjee", "Lavie2005] Satanjeev Banerjee", "Alon Lavie"], "venue": null, "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Yoshua Bengio"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "Semantic parsing via paraphrasing", "author": ["Berant", "Liang2014] Jonathan Berant", "Percy Liang"], "venue": "In Proceedings of ACL,", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Open question answering with weakly supervised embedding models. In Machine Learning and Knowledge Discovery in Databases - European Conference, (ECML PKDD)", "author": ["Jason Weston", "Nicolas Usunier"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Largescale simple question answering with memory", "author": ["Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Generating questions automatically from informational text", "author": ["Chen et al.2009] Wei Chen", "Gregory Aist", "Jack Mostow"], "venue": "In Proceedings of the 2nd Workshop on Question Generation (AIED", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Microsoft COCO captions: Data collection and evaluation", "author": ["Chen et al.2015] Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollar", "C. Lawrence Zitnick"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Question generation based on lexicosyntactic patterns learned from the web", "author": ["Curto et al.2012] Sergio Curto", "A Mendes", "Luisa Coheur"], "venue": "Dialogue and Discourse,", "citeRegEx": "Curto et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Curto et al\\.", "year": 2012}, {"title": "Generating natural language from linked data: Unsupervised template extraction", "author": ["Duma", "Klein2013] Daniel Duma", "Ewan Klein"], "venue": null, "citeRegEx": "Duma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duma et al\\.", "year": 2013}, {"title": "Web question answering: Is more always better", "author": ["Dumais et al.2002] Susan Dumais", "Michele Banko", "Eric Brill", "Jimmy Lin", "Andrew Ng"], "venue": "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in in-", "citeRegEx": "Dumais et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Dumais et al\\.", "year": 2002}, {"title": "QUEST: A model of question answering", "author": ["Sallie E Gordon", "Lawrence E Brainerd"], "venue": "Computers and Mathematics with Applications,", "citeRegEx": "Graesser et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Graesser et al\\.", "year": 1992}, {"title": "LSTM: A search space odyssey", "author": ["Greff et al.2015] Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Natural language question generation using syntax and keywords", "author": ["Ajeesh Elikkottil", "Rajarshi Das"], "venue": "In Proceedings of QG2010: The Third Workshop on Question Generation,", "citeRegEx": "Kalady et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kalady et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "In The International Conference on Learning Representations", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Building large knowledge-based systems; representation and inference in the Cyc project", "author": ["Lenat", "Guha1989] Douglas B. Lenat", "Ramanathan V. Guha"], "venue": null, "citeRegEx": "Lenat et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Lenat et al\\.", "year": 1989}, {"title": "Is question answering fit for the semantic web? a survey", "author": ["Lopez et al.2011] Vanessa Lopez", "Victoria Uren", "Marta Sabou", "Enrico Motta"], "venue": "Semantic Web,", "citeRegEx": "Lopez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lopez et al\\.", "year": 2011}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proceedings of ACL,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Question generation from paragraphs at upenn: Qgstec system description", "author": ["Rashmi Prasad", "Aravind Joshi"], "venue": "In Proceedings of QG2010: The Third Workshop on Question Generation,", "citeRegEx": "Mannem et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mannem et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Question generation from concept maps", "author": ["Olney et al.2012] Andrew M Olney", "Arthur C Graesser", "Natalie K Person"], "venue": "Dialogue and Discourse,", "citeRegEx": "Olney et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Olney et al\\.", "year": 2012}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A comparison of greedy and optimal assessment of natural language student input using wordto-word similarity metrics", "author": ["Rus", "Lintean2012] Vasile Rus", "Mihai Lintean"], "venue": "In Proceedings of the Seventh Workshop on Building Educational Appli-", "citeRegEx": "Rus et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rus et al\\.", "year": 2012}, {"title": "The first question generation shared task evaluation challenge", "author": ["Rus et al.2010] Vasile Rus", "Brendan Wyse", "Paul Piwek", "Mihai Lintean", "Svetlana Stoyanchev", "Cristian Moldovan"], "venue": "In Proceedings of the 6th International Natural Language Generation", "citeRegEx": "Rus et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rus et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Overview of the trec-9 question answering track", "author": ["Voorhees", "Tice2000] Ellen M Voorhees", "DM Tice"], "venue": null, "citeRegEx": "Voorhees et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Voorhees et al\\.", "year": 2000}, {"title": "Wikidata: a free collaborative knowledgebase", "author": ["Vrande\u010di\u0107", "Kr\u00f6tzsch2014] Denny Vrande\u010di\u0107", "Markus Kr\u00f6tzsch"], "venue": "Communications of the ACM,", "citeRegEx": "Vrande\u010di\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vrande\u010di\u0107 et al\\.", "year": 2014}, {"title": "Question generation with minimal recursion semantics", "author": ["Yao", "Zhang2010] Xuchen Yao", "Yi Zhang"], "venue": "In Proceedings of QG2010: The Third Workshop on Question Generation,", "citeRegEx": "Yao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2010}, {"title": "Semantics-based question generation and implementation", "author": ["Yao et al.2012] Xuchen Yao", "Gosse Bouma", "Yi Zhang"], "venue": "Dialogue and Discourse,", "citeRegEx": "Yao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "More recently, researchers have started to utilize large-scale knowledge bases (KBs) (Lopez et al., 2011), such as Freebase (Bollacker et al.", "startOffset": 85, "endOffset": 105}, {"referenceID": 5, "context": ", 2011), such as Freebase (Bollacker et al., 2008), WikiData (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) and Cyc (Lenat and Guha, 1989).", "startOffset": 26, "endOffset": 50}, {"referenceID": 8, "context": "hand-crafted rules and heuristics to synthesize artificial QA corpora (Bordes et al., 2014; Bordes et al., 2015).", "startOffset": 70, "endOffset": 112}, {"referenceID": 9, "context": "hand-crafted rules and heuristics to synthesize artificial QA corpora (Bordes et al., 2014; Bordes et al., 2015).", "startOffset": 70, "endOffset": 112}, {"referenceID": 9, "context": "We frame question generation as a transduction problem starting from a Freebase fact, represented by a triple consisting of a subject, a relationship and an object, which is transduced into a question about the subject, where the object is the correct answer (Bordes et al., 2015).", "startOffset": 259, "endOffset": 280}, {"referenceID": 12, "context": "els (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), and we use an approach similar to Luong et al.", "startOffset": 4, "endOffset": 69}, {"referenceID": 29, "context": "els (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), and we use an approach similar to Luong et al.", "startOffset": 4, "endOffset": 69}, {"referenceID": 1, "context": "els (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), and we use an approach similar to Luong et al.", "startOffset": 4, "endOffset": 69}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015), and we use an approach similar to Luong et al. (2015) for dealing with the problem of rare-words.", "startOffset": 8, "endOffset": 86}, {"referenceID": 27, "context": "Question generation has attracted interest in recent years with notable work by Rus et al. (2010), fol-", "startOffset": 80, "endOffset": 98}, {"referenceID": 18, "context": "A simple rule-based approach was proposed in different studies as wh-fronting or wh-inversion (Kalady et al., 2010; Ali et al., 2010).", "startOffset": 94, "endOffset": 133}, {"referenceID": 0, "context": "A simple rule-based approach was proposed in different studies as wh-fronting or wh-inversion (Kalady et al., 2010; Ali et al., 2010).", "startOffset": 94, "endOffset": 133}, {"referenceID": 23, "context": "been addressed in two different ways: by using named entity recognizers (Mannem et al., 2010; Yao and Zhang, 2010) or semantic role labelers (Chen et al.", "startOffset": 72, "endOffset": 114}, {"referenceID": 10, "context": ", 2010; Yao and Zhang, 2010) or semantic role labelers (Chen et al., 2009).", "startOffset": 55, "endOffset": 74}, {"referenceID": 10, "context": ", 2010; Yao and Zhang, 2010) or semantic role labelers (Chen et al., 2009). In Curto et al. (2012) questions are split into classes according to their syn-", "startOffset": 56, "endOffset": 99}, {"referenceID": 10, "context": "After the identification of key points, Chen et al. (2009) apply handcrafted-templates to generate questions framed in the right target expression by following the analysis of Graesser et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 10, "context": "After the identification of key points, Chen et al. (2009) apply handcrafted-templates to generate questions framed in the right target expression by following the analysis of Graesser et al. (1992), who classify questions according to a taxonomy consisting of 18 categories.", "startOffset": 40, "endOffset": 199}, {"referenceID": 33, "context": "a syntactic representation of the sentence), and second, transform the symbolic representation of the text into the question (Yao et al., 2012).", "startOffset": 125, "endOffset": 143}, {"referenceID": 14, "context": "In the same spirit, Duma et al. (2013) generate short descriptions from triples by using templates defined by the rela-", "startOffset": 20, "endOffset": 39}, {"referenceID": 25, "context": "Our baseline is similar to that of Olney et al. (2012), where a set of relationship-specific templates are defined.", "startOffset": 35, "endOffset": 55}, {"referenceID": 5, "context": "In Freebase (Bollacker et al., 2008) these relationships are directed and always connect exactly two entities.", "startOffset": 12, "endOffset": 36}, {"referenceID": 9, "context": "We use the SimpleQuestions dataset (Bordes et al., 2015) in order to train our models.", "startOffset": 35, "endOffset": 56}, {"referenceID": 29, "context": "translation models (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 19, "endOffset": 66}, {"referenceID": 1, "context": "translation models (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 19, "endOffset": 66}, {"referenceID": 7, "context": "3), we have learned it separately and beforehand with TransE (Bordes et al., 2013), a model aimed at modeling this kind of multi-relational data.", "startOffset": 61, "endOffset": 82}, {"referenceID": 12, "context": "For the decoder, we use a GRU recurrent neural network (RNN) (Cho et al., 2014) with an", "startOffset": 61, "endOffset": 79}, {"referenceID": 1, "context": "attention-mechanism (Bahdanau et al., 2015) on the encoder representation to generate the associated question Q to that fact F .", "startOffset": 20, "endOffset": 43}, {"referenceID": 17, "context": "RNN architectures, such as the LSTM RNN (Greff et al., 2015).", "startOffset": 40, "endOffset": 60}, {"referenceID": 6, "context": "Multi-relational embeddingbased models (Bordes et al., 2011) have recently become popular to learn distributed vector embed-", "startOffset": 39, "endOffset": 60}, {"referenceID": 7, "context": "Due to its simplicity and good performance, we choose to use TransE (Bordes et al., 2013) to learn such embeddings.", "startOffset": 68, "endOffset": 89}, {"referenceID": 6, "context": "Further details are given by Bordes et al. (2013).", "startOffset": 29, "endOffset": 50}, {"referenceID": 22, "context": "dling rare words in neural machine translation by Luong et al. (2015). For every question and answer pair, we search for words in the question", "startOffset": 50, "endOffset": 70}, {"referenceID": 22, "context": "The main difference with respect to that of Luong et al. (2015) is that we do not use placeholder tokens in the input language, because then the entities and relationships in the in-", "startOffset": 44, "endOffset": 64}, {"referenceID": 3, "context": "To decide when to stop training we used early stopping with patience (Bengio, 2012) on the METEOR score obtained for the validation set.", "startOffset": 69, "endOffset": 83}, {"referenceID": 26, "context": "BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) are two", "startOffset": 5, "endOffset": 28}, {"referenceID": 11, "context": "widely used evaluation metrics in statistical machine translation and automatic image-caption generation (Chen et al., 2015).", "startOffset": 105, "endOffset": 124}, {"referenceID": 24, "context": "The metric makes use of a word similarity score, which in our experiments is the cosine similarity between two Word2Vec word embeddings (Mikolov et al., 2013).", "startOffset": 136, "endOffset": 158}], "year": 2016, "abstractText": "Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale questionanswer corpora available. In this paper we present the 30M Factoid QuestionAnswer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the questiongeneration model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear to be indistinguishable from real human-generated questions.", "creator": "LaTeX with hyperref package"}}}