{"id": "1603.07252", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2016", "title": "Neural Summarization by Extracting Sentences and Words", "abstract": "Traditional approaches to extractive summarization rely heavily on human-engineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.", "histories": [["v1", "Wed, 23 Mar 2016 16:05:46 GMT  (936kb,D)", "http://arxiv.org/abs/1603.07252v1", null], ["v2", "Tue, 7 Jun 2016 13:41:50 GMT  (936kb,D)", "http://arxiv.org/abs/1603.07252v2", "Published at ACL2016"], ["v3", "Fri, 1 Jul 2016 03:16:03 GMT  (937kb,D)", "http://arxiv.org/abs/1603.07252v3", "ACL2016 conference paper with appendix"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jianpeng cheng 0001", "mirella lapata"], "accepted": true, "id": "1603.07252"}, "pdf": {"name": "1603.07252.pdf", "metadata": {"source": "CRF", "title": "Neural Summarization by Extracting Sentences and Words", "authors": ["Jianpeng Cheng", "Mirella Lapata"], "emails": ["jianpeng.cheng@ed.ac.uk", "mlap@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "In this paper, we propose a data-driven approach based on neural networks and continuous sentence characteristics. We develop a general framework for summarizing a single document, consisting of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summation models that can extract sentences or words. We train our models on large corpora with hundreds of thousands of document summary pairs. Experimental results on two summation data sets show that our models achieve results that are comparable to the state of the art without access to linguistic annotations."}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they are able to live, in which they are able"}, {"heading": "2 Problem Formulation", "text": "In this section, we formally define the summary tasks considered in this thesis. In view of a document D consisting of a sequence of sentences {s1, \u00b7 \u00b7 \u00b7, sm} and a set of words {w1, \u00b7 \u00b7 \u00b7, wn}, we are interested in obtaining summaries at two levels of granularity, namely sentences and words. Sentence extraction aims to create a set of words from D by selecting a subset of j sentences (where j < m). We do this by evaluating each sentence within D and predicting a designation yL \u00b2 {0.1} that indicates whether the sentence should be included in the summary. Since we use verified training, the goal is to determine the probability of all sentence descriptions yL = (y1L, \u00b7 \u00b7, ymL) using the input document D and the model parameter dsy: Log p (yL | D; p) = 1 Log (i."}, {"heading": "3 Training Data for Summarization", "text": "In the meantime, we have developed a methodology similar to that of Hermann et al. (2015) and have created two large data sets, one for sentence extraction and another for word extraction. In a nutshell, we have extracted three hundred thousand news articles and their corresponding highlights from the mail (see Figure 1 for an example). The highlights (created by news editors) are truly abstract summaries and therefore not suitable for training."}, {"heading": "4 Summarization Model", "text": "Key components of our summary model include a neural network-based document reader and an attention-based content extractor. We first describe the document reader and then present the details of our sentence and word extractors."}, {"heading": "4.1 Document Reader", "text": "The role of the reader is to deduce the meaning of the document from its constituent sentences, each of which is treated as a sequence of words. First, we obtain representation vectors at the sentence level that form a single-layer Convolutionary Neural Network (CNN) with a max-overtime Pooling Operations (Kalchburner and Blunsom, 2013; Zhang and Lapata, 2014; Kim et al., 2016). Next, we build representations for documents that use a standard recurrent neural network (RNN) that combines sentences recursively. CNN operates at the word level that leads to the4We use the Python Gensim Library and the 300-dimensional GoogleNews vectors.acquisition of sentence representations that are then used as inputs for the RNN that acquires documentary representations at a hierarchical level. We describe these two sub-components of the text reader. Convolutional Scoctivities have decided to construct a model of the documentary representation."}, {"heading": "4.2 Sentence Extractor", "text": "In the standard neural sequence-to-sequence modeling paradigm (Bahdanau et al., 2015), an attention mechanism is used as an intermediate step to decide on which input region to focus on in order to generate the next output. In contrast, our sentence extractor applies attention to extracting distinctive sentences directly after reading the sentences. The extractor is another recurring neural network that sequentially labels sentences, taking into account not only whether they are individually relevant but also mutually redundant. The complete architecture for the document encoder and sentence extractor is shown in Figure 2. You can see that the next labeling decision is made with both the coded document and the previously labeled sentences in the back of your mind. Given the encoders of hidden states (h1, \u00b7, hm) and the extractor of hidden states (h-1, h-1) we become at a specific time (h)."}, {"heading": "4.3 Word Extractor", "text": "Compared to sentence extraction, which is a pure sequence marking task, word extraction is closer to a generation task, where relevant content must be selected and then reproduced fluently and grammatically. A small change in the structure of the sequence marking model makes it suitable for the generation: instead of predicting a label for the next sentence at each step, the model directly prints the next word in the summary. It uses a hierarchical attention architecture: at step t, the softmax softly5 decoder records each document set and then pays attention to each word in the document and calculates the probability of the next word to be included in the summary p (w \u2032 t = wi | d, w \u2032 1, \u00b7, w \u2032 t \u2212 1) with a softmax classifier: h \u00b7 t = LSTM (w \u2212 1, h \u2212 1)."}, {"heading": "4.4 Pre-training the Convolutional Sentence Encoder", "text": "In direct, supervised training with given examples, the parameters of the system, including all words and phrases, would require hard attention to first select a sentence and then a few words from it as a summary, but this would make the system indistinguishable for the training. Although hard attention can be trained with the REINFORCE algorithm (Williams, 1992), it requires the capture of individual actions and could lead to very large variances.6We have found empirically that feeding the previous Sentencelevel attention vector as an additional input to the LSTM could lead to a small improvement in the system, which is not shown in the equation. Embedding can converge into a local optimum that minimizes the classification error but poorly generalizes it. To improve the model, we propose to train the Convolutionary sentence coder to capture the sentence semantics."}, {"heading": "5 Experimental Setup", "text": "This year, it is more than ever before in the history of the city, where it has gone down in history as never before."}, {"heading": "6 Results", "text": "The results of this study show that the results of the LEAD and LREG baselines are associated with significant marginalisation, while we are grateful to Kristian Woodsend for giving us access to the results of his system. Unfortunately, we do not have access to the results of TGRAPH or URANK for inclusion in the human evaluation system. This is an encouraging result, as our model only provides access to the raw text."}, {"heading": "7 Conclusions", "text": "In this paper, we presented a data-driven summary framework based on an encoder-extractor architecture. We developed two classes of models based on sentence and word extraction. We demonstrated that our models can be trained on large data sets and learn computerization features based on continuous representation without having to resort to linguistic annotations. Instructions for the future are diverse. One way to improve the word-based model would be to combine a diagram-based decoding algorithm (Cohn and Lapata, 2009) with the neural word extractor. It would also be interesting to apply the neural models presented here in a phrase-based setting similar to Lebret et al. (2015)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Headline generation based on statistical translation", "author": ["Banko et al.2000] Michele Banko", "Vibhu O. Mittal", "Michael J. Witbrock"], "venue": "In Proceedings of the 38th ACL,", "citeRegEx": "Banko et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2000}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "arXiv preprint arXiv:1312.3005", "citeRegEx": "Chelba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2014}, {"title": "Sentence compression as tree transduction", "author": ["Cohn", "Lapata2009] Trevor Anthony Cohn", "Mirella Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cohn et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2009}, {"title": "Text summarization via hidden Markov models", "author": ["Conroy", "O\u2019Leary2001] Conroy", "O\u2019Leary"], "venue": "In Proceedings of the 34th Annual ACL SIGIR,", "citeRegEx": "Conroy et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Conroy et al\\.", "year": 2001}, {"title": "Lexpagerank: Prestige in multidocument text summarization", "author": ["Erkan", "Radev2004] G\u00fcne\u015f Erkan", "Dragomir R. Radev"], "venue": "In Proceedings of the 2004 EMNLP,", "citeRegEx": "Erkan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Erkan et al\\.", "year": 2004}, {"title": "Event-based extractive summarization", "author": ["Filatova", "Vasileios Hatzivassiloglou"], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04", "citeRegEx": "Filatova et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Filatova et al\\.", "year": 2004}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "HueiChi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Gulcehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent convolutional neural networks for discourse compositionality", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Character-aware neural language models", "author": ["Kim et al.2016] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "In Proceedings of the 30th AAAI,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 EMNLP,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Summarization based on embedding distributions", "author": ["Masaki Noguchi", "Taichi Yatsuka"], "venue": "In Proceedings of the 2015 EMNLP,", "citeRegEx": "Kobayashi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2015}, {"title": "A trainable document summarizer", "author": ["Kupiec et al.1995] Julian Kupiec", "Jan O. Pedersen", "Francine Chen"], "venue": "In Proceedings of the 18th Annual International ACM SIGIR,", "citeRegEx": "Kupiec et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kupiec et al\\.", "year": 1995}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Phrase-based image captioning", "author": ["Lebret et al.2015] R\u00e9mi Lebret", "Pedro O Pinheiro", "Ronan Collobert"], "venue": "In Proceedings of the 32nd ICML,", "citeRegEx": "Lebret et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebret et al\\.", "year": 2015}, {"title": "Automatic evaluation of summaries using n-gram co-occurrence statistics", "author": ["Lin", "Hovy2003] Chin-Yew Lin", "Eduard H. Hovy"], "venue": "In Proceedings of HLT NAACL,", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Language independent extractive summarization", "author": ["Rada Mihalcea"], "venue": "In Proceedings of the ACL Interactive Poster and Demonstration Sessions,", "citeRegEx": "Mihalcea.,? \\Q2005\\E", "shortCiteRegEx": "Mihalcea.", "year": 2005}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization", "author": ["Nenkova et al.2006] Ani Nenkova", "Lucy Vanderwende", "Kathleen McKeown"], "venue": "In Proceedings of the 29th Annual", "citeRegEx": "Nenkova et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nenkova et al\\.", "year": 2006}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st ACL,", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Topical coherence for graph-based extractive summarization", "author": ["Hans-Martin Ramsl", "Michael Strube"], "venue": "In Proceedings of the 2015 EMNLP,", "citeRegEx": "Parveen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parveen et al\\.", "year": 2015}, {"title": "Mead-a platform for multidocument multilingual text summarization", "author": ["Radev et al.2004] Dragomir Radev", "Timothy Allison", "Sasha Blair-Goldensohn", "John Blitzer", "Arda Celebi", "Stanko Dimitrov", "Elliott Drabek", "Ali Hakim", "Wai Lam", "Danyu Liu"], "venue": null, "citeRegEx": "Radev et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Radev et al\\.", "year": 2004}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the 2015 EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Enhancing singledocument summarization by combining RankNet and third-party sources", "author": ["Svore et al.2007] Krysta Svore", "Lucy Vanderwende", "Christopher Burges"], "venue": "In Proceedings of the 2007 EMNLP-CoNLL,", "citeRegEx": "Svore et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Svore et al\\.", "year": 2007}, {"title": "Towards a unified approach to simultaneous single-document and multidocument summarizations", "author": ["Xiaojun Wan"], "venue": "In Proceedings of the 23rd COLING,", "citeRegEx": "Wan.,? \\Q2010\\E", "shortCiteRegEx": "Wan.", "year": 2010}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Automatic generation of story highlights", "author": ["Woodsend", "Lapata2010] Kristian Woodsend", "Mirella Lapata"], "venue": "In Proceedings of the 48th ACL,", "citeRegEx": "Woodsend et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2010}, {"title": "Extractive summarization by maximizing semantic volume", "author": ["Fei Liu", "Noah A. Smith"], "venue": "In Proceedings of the 2015 EMNLP,", "citeRegEx": "Yogatama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2015}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Zhang", "Lapata2014] Xingxing Zhang", "Mirella Lapata"], "venue": "In Proceedings of 2014 EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "These include surface features such as sentence position and length (Radev et al., 2004), the words in the title, the presence of proper nouns, content features such as word frequency (Nenkova et al.", "startOffset": 68, "endOffset": 88}, {"referenceID": 22, "context": ", 2004), the words in the title, the presence of proper nouns, content features such as word frequency (Nenkova et al., 2006), and event features such as action nouns (Filatova and Hatzivassiloglou, 2004).", "startOffset": 103, "endOffset": 125}, {"referenceID": 16, "context": "Several methods have been used in order to select the summary sentences ranging from binary classifiers (Kupiec et al., 1995), to hidden Markov models (Conroy and O\u2019Leary, 2001), graph-based algorithms (Erkan and Radev, 2004; Mihalcea, 2005), and integer linear programming (Woodsend and Lapata, 2010).", "startOffset": 104, "endOffset": 125}, {"referenceID": 20, "context": ", 1995), to hidden Markov models (Conroy and O\u2019Leary, 2001), graph-based algorithms (Erkan and Radev, 2004; Mihalcea, 2005), and integer linear programming (Woodsend and Lapata, 2010).", "startOffset": 84, "endOffset": 123}, {"referenceID": 27, "context": "There has been a surge of interest recently in repurposing sequence transduction neural network architectures for NLP tasks such as machine translation (Sutskever et al., 2014), question answering (Hermann et al.", "startOffset": 152, "endOffset": 176}, {"referenceID": 9, "context": ", 2014), question answering (Hermann et al., 2015), and sentence compression (Rush et al.", "startOffset": 28, "endOffset": 50}, {"referenceID": 26, "context": ", 2015), and sentence compression (Rush et al., 2015).", "startOffset": 34, "endOffset": 53}, {"referenceID": 0, "context": "An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding.", "startOffset": 23, "endOffset": 46}, {"referenceID": 28, "context": "Inspired by previous work on summarization (Woodsend and Lapata, 2010; Svore et al., 2007) and reading comprehension (Hermann et al.", "startOffset": 43, "endOffset": 90}, {"referenceID": 9, "context": ", 2007) and reading comprehension (Hermann et al., 2015) we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website.", "startOffset": 34, "endOffset": 56}, {"referenceID": 15, "context": "A few recent studies (Kobayashi et al., 2015; Yogatama et al., 2015) perform sentence extraction based on pre-trained sentence embeddings following an unsupervised optimization paradigm.", "startOffset": 21, "endOffset": 68}, {"referenceID": 32, "context": "A few recent studies (Kobayashi et al., 2015; Yogatama et al., 2015) perform sentence extraction based on pre-trained sentence embeddings following an unsupervised optimization paradigm.", "startOffset": 21, "endOffset": 68}, {"referenceID": 1, "context": "The idea of creating a summary by extracting words from the source document was pioneered in Banko et al. (2000) who view summarization as a problem analogous to statistical machine translation and generate headlines using statistical models for selecting and ordering the summary words.", "startOffset": 93, "endOffset": 113}, {"referenceID": 1, "context": "The idea of creating a summary by extracting words from the source document was pioneered in Banko et al. (2000) who view summarization as a problem analogous to statistical machine translation and generate headlines using statistical models for selecting and ordering the summary words. Our word-based model is similar in spirit, however, it operates over continuous representations, produces multi-sentence output, and jointly selects summary words and organizes them into sentences. A few recent studies (Kobayashi et al., 2015; Yogatama et al., 2015) perform sentence extraction based on pre-trained sentence embeddings following an unsupervised optimization paradigm. Our work also uses continuous representations to express the meaning of sentences and documents, but importantly employs neural networks more directly to perform the actual summarization task. Rush et al. (2015) propose a neural attention model for abstractive sentence compression which is trained on pairs of headlines and first sentences in an article.", "startOffset": 93, "endOffset": 885}, {"referenceID": 9, "context": "To overcome the paucity of annotated data for training, we adopt a methodology similar to Hermann et al. (2015) and create two large-scale datasets, one for sentence extraction and another one for word extraction.", "startOffset": 90, "endOffset": 112}, {"referenceID": 9, "context": "To overcome the paucity of annotated data for training, we adopt a methodology similar to Hermann et al. (2015) and create two large-scale datasets, one for sentence extraction and another one for word extraction. In a nutshell, we retrieved3 hundreds of thousands of news articles and their corresponding highlights from DailyMail (see Figure 1 for an example). The highlights (created by news editors) are genuinely abstractive summaries and therefore not readily suited to supervised training. To create the training data for sentence extraction, we reverse approximated the gold standard label of each document sentence given the summary based on their semantic correspondence (Woodsend and Lapata, 2010). Specifically, we designed a rulebased system that determines whether a document sentence matches a highlight and should be labeled with 1 (must be in the summary), and 0 otherwise. The rules take into account the position of the sentence in the document, the unigram and bigram overlap between document sentences and highlights, the number of entities appearing in the highlight and in the document sentence. We adjusted the weights of the rules on 9,000 documents with manual sentence labels created by Woodsend and Lapata (2010), and evaluated the preprocessing rules on a held-out set of 216 documents coming from the same dataset.", "startOffset": 90, "endOffset": 1241}, {"referenceID": 9, "context": "3The script for constructing our datasets is modified from the one released in Hermann et al. (2015). subsequently used to label 200K documents (with approximately 30% of the sentences in each document being deemed summary-worthy).", "startOffset": 79, "endOffset": 101}, {"referenceID": 12, "context": "We first obtain representation vectors at the sentence level using a single-layer convolutional neural network (CNN) with a max-overtime pooling operation (Kalchbrenner and Blunsom, 2013; Zhang and Lapata, 2014; Kim et al., 2016).", "startOffset": 155, "endOffset": 229}, {"referenceID": 13, "context": "Firstly, CNNs can be trained effectively (without any long-term dependencies in the model) and secondly, they have been successfully used for sentence-level classification tasks such as sentiment analysis (Kim, 2014).", "startOffset": 205, "endOffset": 216}, {"referenceID": 0, "context": "In the standard neural sequence-to-sequence modeling paradigm (Bahdanau et al., 2015), an attention mechanism is used as an intermediate step to decide which input region to focus on in order to generate the next output.", "startOffset": 62, "endOffset": 85}, {"referenceID": 2, "context": "To mitigate this, we adopt a curriculum learning strategy (Bengio et al., 2015): at the beginning of training when pt\u22121 cannot be predicted accurately, we set it to the true label of the previous sentence; as training goes on, we gradually shift its value to the predicted label p(yL(t\u22121) = 1|d).", "startOffset": 58, "endOffset": 79}, {"referenceID": 8, "context": "A possible enhancement would be to pair the extractor with a neural language model, which can be pretrained on a large amount of unlabeled documents and then jointly tuned with the extractor during decoding (Gulcehre et al., 2015).", "startOffset": 207, "endOffset": 230}, {"referenceID": 23, "context": "We incorporate the features in a log-linear reranker whose feature weights are optimized with minimum error rate training (Och, 2003).", "startOffset": 122, "endOffset": 133}, {"referenceID": 30, "context": "Although hard attention can be trained with the REINFORCE algorithm (Williams, 1992), it requires the sampling of discrete actions and could lead to very high variance.", "startOffset": 68, "endOffset": 84}, {"referenceID": 12, "context": "For the convolutional sentence model, we followed Kim et al. (2016) and used a list of kernel sizes {1, 2, 3, 4, 5, 6, 7}.", "startOffset": 50, "endOffset": 68}, {"referenceID": 9, "context": "A similar data augmentation approach has been used for reading comprehension (Hermann et al., 2015).", "startOffset": 77, "endOffset": 99}, {"referenceID": 25, "context": "Rush et al. (2015) address this issue by adding a new set of features and a log-linear model component to their system.", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "Specifically, we perform named entity recognition with the package provided by Hermann et al. (2015) and maintain a set of randomly initialized entity embeddings.", "startOffset": 79, "endOffset": 101}, {"referenceID": 21, "context": "We sidestep this during training by performing negative sampling (Mikolov et al., 2013) which trims the vocabulary of different documents to the same length.", "startOffset": 65, "endOffset": 87}, {"referenceID": 21, "context": "7We used the word2vec (Mikolov et al., 2013) skip-gram model with context window size 6, negative sampling size 10 and hierarchical softmax 1.", "startOffset": 22, "endOffset": 44}, {"referenceID": 3, "context": "The model was trained on the Google 1-billion word benchmark (Chelba et al., 2014).", "startOffset": 61, "endOffset": 82}, {"referenceID": 26, "context": "It can also be viewed as a hierarchical documentlevel extension of the abstractive sentence summarizer proposed by Rush et al. (2015). We trained this model with negative sampling to avoid the excessive computation of the normalization constant.", "startOffset": 115, "endOffset": 134}, {"referenceID": 24, "context": "The other two systems, TGRAPH (Parveen et al., 2015) and URANK (Wan, 2010), produce more typical summaries and represent the state of the art.", "startOffset": 30, "endOffset": 52}, {"referenceID": 29, "context": ", 2015) and URANK (Wan, 2010), produce more typical summaries and represent the state of the art.", "startOffset": 18, "endOffset": 29}, {"referenceID": 18, "context": "It would also be interesting to apply the neural models presented here in a phrase-based setting similar to Lebret et al. (2015).", "startOffset": 108, "endOffset": 129}], "year": 2016, "abstractText": "Traditional approaches to extractive summarization rely heavily on humanengineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.", "creator": "LaTeX with hyperref package"}}}