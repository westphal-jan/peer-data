{"id": "1603.07323", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2016", "title": "Learning Mixtures of Plackett-Luce Models", "abstract": "In this paper we address the identifiability and efficient learning problems of finite mixtures of Plackett-Luce models for rank data. We prove that for any $k\\geq 2$, the mixture of $k$ Plackett-Luce models for no more than $2k-1$ alternatives is non-identifiable and this bound is tight for $k=2$. For generic identifiability, we prove that the mixture of $k$ Plackett-Luce models over $m$ alternatives is generically identifiable if $k\\leq\\lfloor\\frac {m-2} 2\\rfloor!$.", "histories": [["v1", "Wed, 23 Mar 2016 19:58:37 GMT  (91kb,D)", "https://arxiv.org/abs/1603.07323v1", "26 pages, 2 figures"], ["v2", "Mon, 30 May 2016 15:34:42 GMT  (103kb,D)", "http://arxiv.org/abs/1603.07323v2", "26 pages, 2 figures"], ["v3", "Tue, 7 Jun 2016 15:46:54 GMT  (93kb,D)", "http://arxiv.org/abs/1603.07323v3", "26 pages, 2 figures; remove (incorrectly) generated date; add summary to section 6"]], "COMMENTS": "26 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhibing zhao", "peter piech", "lirong xia"], "accepted": true, "id": "1603.07323"}, "pdf": {"name": "1603.07323.pdf", "metadata": {"source": "CRF", "title": "Learning Mixtures of Plackett-Luce Models", "authors": ["Zhibing Zhao", "Peter Piech"], "emails": ["zhaoz6@rpi.edu", "piechp@rpi.edu", "xial@cs.rpi.edu"], "sections": [{"heading": null, "text": "Our experiments show that our GMM algorithm is significantly faster than the Gormley and Murphy EMM algorithm (2008), while achieving competitive statistical efficiency."}, {"heading": "1 Introduction", "text": "In many machine learning problems, the data consists of rankings on a limited number of basic alternatives Marden (1995). For example, meta-search engines aggregate rankings on websites of individual search engines Dwork et al. (2001); rankings on documents are combined to find the most relevant document in informational retrieval Liu (2011); noisy responses from online workers are aggregated to produce a more accurate answer in crowdsourcing models Mao et al. (2013). Ranking data is also very common in economics and political science. Consumers often give discreet selection data to McFadden (1974) and voters often give rankings on presidential candidates Gormley and Murphy (2008). Perhaps the most commonly used statistical model for ranking data is the Plackett-Luce model Plackett-Plackett (1975); Luce (1959). The Plackett-Luce model is a natural generalization of multinomistic regression."}, {"heading": "1.1 Our Contributions", "text": "We answer Q1 with the following theorems. The answer depends on the number of components k in the mixture model and the number of alternatives m.theorem 1 and 2. For each m \u2265 2 and each k \u2265 m + 12, the k-mixture Plackett-Luce model (referred to by k-PL) is not identifiable. This lower limit for k as function m is narrow for k = 2 (m = 4).The second half of the theorem is positive: the mixture of two Plackett-Luce models is identifiable for four or more alternatives. We suspect that the limit is narrow for all k > 2.The k-PL is generically identifiable formalities alternatives if the Lebesgue measurement of unidentifiable parameters is 0. We prove the following positive results for k-PL.Theorem 3. For each m \u2265 6 and each k \u2264 m \u2212 M algorithm, the algorithm is strict."}, {"heading": "1.2 Related Work and Discussions", "text": "Most previous work in mixed models (especially Gaussian mixing models) focuses on cardinal data Teicher (1961, 1963); McAfee and Peel (2004); Kalai et al. (2012); Dasgupta (1999). Little is known about the identifiability of mixtures of models for rank data. For rank data, Iannario (2010) has demonstrated the identifiability of the mix of shifted binomial models and the uniform models. (2014) The identifiability of mixtures of two Mallows models has also been studied by Lu and Boutilier (2014) and Chierichetti et al. (2015) Our work focuses on mixtures of Plackett-Luce models. Technically, part of our (non) identification models is motivated by the work of Teicher (1963), who has obtained sufficient conditions for finite mixing models."}, {"heading": "2 Preliminaries", "text": "Let's specify the number of linear orders (rankings) that have transitive, antisymmetric and overall binary relationships, above A. A ranking is often referred to as ai1 ai2 \u00b7 \u00b7 \u00b7 target, which means that ai1 is the most preferred alternative, ai2 is the second preferred target that is least preferred, etc. Let P = (V1, V2, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Vn) denotes the data (also referred to as the preference profile), with for all j \u2264 n, Vj \u2264 n (A).Definition 1 (Plackett Luce model) being the least preferred space."}, {"heading": "3 Identifiability of Plackett-Luce Mixture Models", "text": "We first point out a general problem to show a relationship between the rank of Fkm and the identifiability of Plackett-Luce mixture models. We remember that a set of vectors is not degenerated if its elements are in pairs. Lemma 1 If the rank of Fkm 2k is for all non-degenerated elements, the rank of Fkm 2k is for all non-degenerated elements (1),............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "5 Experiments", "text": "The performance of our GMM algorithm (algorithm 1) is compared with the Gormley and Murphy EMM algorithm (2008) for 2-PL in terms of runtime and statistical efficiency for synthetic data. Synthetic data sets are generated as follows. \u2022 Generating the basic truth: for k = 2 mixtures and m = 4 alternatives, the mixing coefficient \u03b1 3 is generated uniformly randomly and the Plackett-Luce components are generated from the Dir (~ 1) Dirichlet distribution. \u2022 Generating data: Given a basic truth ~ feasible, we generate each ranking with probabilities parameterized from the PL model."}, {"heading": "6 Summary and Future Work", "text": "In this paper, we address the problem of identifiability and efficient learning for Plackett-Luce mixture models. We show that the mixture of k Plackett-Luce models is not identifiable for more than 2k-1 alternatives, and this limit is narrow for k = 2. For generic identifiability, we demonstrate that the mixture of k Plackett-Luce models is generically identifiable for no more than 2k-1 alternatives if k \u2264 bm \u2212 22 c!. We also propose a GMM algorithm for learning 2-PL with four or more alternatives. Our experiments show that our GMM algorithm is significantly faster than the EMM algorithm in Gormley and Murphy (2008), while achieving competitive statistical efficiency. There are many directions for future research. An obvious open question is whether k-PL is identifiable for 2k alternatives, which we suspect is efficient for k-3."}, {"heading": "Acknowledgements", "text": "This work is supported by the National Science Foundation with the IIS-1453542 Fellowship and a SimonsBerkeley Research Fellowship. We thank all reviewers for their helpful comments and suggestions."}], "references": [{"title": "Identifiability of parameters in latent structure models with many observed variables", "author": ["Elizabeth S. Allman", "Catherine Matias", "John A. Rhodes"], "venue": "The Annals of Statistics,", "citeRegEx": "Allman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Allman et al\\.", "year": 2009}, {"title": "Learning Mixtures of Ranking Models", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet", "Aravindan Vijayaraghavan"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "Awasthi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2014}, {"title": "Generalized method-ofmoments for rank aggregation", "author": ["Hossein Azari Soufiani", "William Chen", "David C. Parkes", "Lirong Xia"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Soufiani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Soufiani et al\\.", "year": 2013}, {"title": "Label ranking methods based on the plackett-luce model", "author": ["Weiwei Cheng", "Krzysztof J. Dembczynski", "Eyke H\u00fcllermeier"], "venue": "Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Cheng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2010}, {"title": "On learning mixture models for permutations", "author": ["Flavio Chierichetti", "Anirban Dasgupta", "Ravi Kumar", "Silvio Lattanzi"], "venue": "Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science,", "citeRegEx": "Chierichetti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chierichetti et al\\.", "year": 2015}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "Foundations of Computer Science,", "citeRegEx": "Dasgupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta.", "year": 1999}, {"title": "Rank aggregation methods for the web", "author": ["Cynthia Dwork", "Ravi Kumar", "Moni Naor", "D. Sivakumar"], "venue": "In Proceedings of the 10th World Wide Web Conference,", "citeRegEx": "Dwork et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2001}, {"title": "Exploring voting blocs within the Irish electorate: A mixture modeling approach", "author": ["Isobel Claire Gormley", "Thomas Brendan Murphy"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gormley and Murphy.,? \\Q2008\\E", "shortCiteRegEx": "Gormley and Murphy.", "year": 2008}, {"title": "Generalized Method of Moments", "author": ["Alastair R. Hall"], "venue": null, "citeRegEx": "Hall.,? \\Q2005\\E", "shortCiteRegEx": "Hall.", "year": 2005}, {"title": "Large Sample Properties of Generalized Method of Moments", "author": ["Lars Peter Hansen"], "venue": "Estimators. Econometrica,", "citeRegEx": "Hansen.,? \\Q1982\\E", "shortCiteRegEx": "Hansen.", "year": 1982}, {"title": "MM algorithms for generalized Bradley-Terry models", "author": ["David R. Hunter"], "venue": "In The Annals of Statistics,", "citeRegEx": "Hunter.,? \\Q2004\\E", "shortCiteRegEx": "Hunter.", "year": 2004}, {"title": "On the identifiability of a mixture model for ordinal data", "author": ["Maria Iannario"], "venue": "Metron, 68(1):87\u201394,", "citeRegEx": "Iannario.,? \\Q2010\\E", "shortCiteRegEx": "Iannario.", "year": 2010}, {"title": "Disentangling gaussians", "author": ["Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant"], "venue": "In Communications of the ACM,", "citeRegEx": "Kalai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2012}, {"title": "Learning to Rank for Information", "author": ["Tie-Yan Liu"], "venue": null, "citeRegEx": "Liu.,? \\Q2011\\E", "shortCiteRegEx": "Liu.", "year": 2011}, {"title": "Effective sampling and learning for mallows models with pairwisepreference data", "author": ["Tyler Lu", "Craig Boutilier"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lu and Boutilier.,? \\Q2014\\E", "shortCiteRegEx": "Lu and Boutilier.", "year": 2014}, {"title": "Individual Choice Behavior: A Theoretical Analysis", "author": ["Robert Duncan Luce"], "venue": null, "citeRegEx": "Luce.,? \\Q1959\\E", "shortCiteRegEx": "Luce.", "year": 1959}, {"title": "Better human computation through principled voting", "author": ["Andrew Mao", "Ariel D. Procaccia", "Yiling Chen"], "venue": "In Proceedings of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Mao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2013}, {"title": "Analyzing and Modeling Rank Data", "author": ["John I. Marden"], "venue": null, "citeRegEx": "Marden.,? \\Q1995\\E", "shortCiteRegEx": "Marden.", "year": 1995}, {"title": "Conditional logit analysis of qualitative choice behavior", "author": ["Daniel McFadden"], "venue": "In Frontiers of Econometrics,", "citeRegEx": "McFadden.,? \\Q1974\\E", "shortCiteRegEx": "McFadden.", "year": 1974}, {"title": "Finite Mixture Models", "author": ["Geoffrey McLachlan", "David Peel"], "venue": null, "citeRegEx": "McLachlan and Peel.,? \\Q2004\\E", "shortCiteRegEx": "McLachlan and Peel.", "year": 2004}, {"title": "Mixture Models: Inference and Applications to Clustering", "author": ["Geoffrey J. McLachlan", "Kaye E. Basford"], "venue": null, "citeRegEx": "McLachlan and Basford.,? \\Q1988\\E", "shortCiteRegEx": "McLachlan and Basford.", "year": 1988}, {"title": "The analysis of permutations", "author": ["Robin L. Plackett"], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics),", "citeRegEx": "Plackett.,? \\Q1975\\E", "shortCiteRegEx": "Plackett.", "year": 1975}, {"title": "Dealing with label switching in mixture models", "author": ["Matthew Stephens"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Stephens.,? \\Q2000\\E", "shortCiteRegEx": "Stephens.", "year": 2000}, {"title": "Identifiability of mixtures", "author": ["Henry Teicher"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Teicher.,? \\Q1961\\E", "shortCiteRegEx": "Teicher.", "year": 1961}, {"title": "Identifiability of finite mixtures", "author": ["Henry Teicher"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Teicher.,? \\Q1963\\E", "shortCiteRegEx": "Teicher.", "year": 1963}, {"title": "Thurstone. A law of comparative judgement", "author": ["Louis Leon"], "venue": "Psychological Review,", "citeRegEx": "Leon,? \\Q1927\\E", "shortCiteRegEx": "Leon", "year": 1927}], "referenceMentions": [{"referenceID": 7, "context": "Our experiments show that our GMM algorithm is significantly faster than the EMM algorithm by Gormley and Murphy (2008), while achieving competitive statistical efficiency.", "startOffset": 94, "endOffset": 120}, {"referenceID": 12, "context": "In many machine learning problems the data are composed of rankings over a finite number of alternatives Marden (1995). For example, meta-search engines aggregate rankings over webpages from individual search engines Dwork et al.", "startOffset": 105, "endOffset": 119}, {"referenceID": 6, "context": "For example, meta-search engines aggregate rankings over webpages from individual search engines Dwork et al. (2001); rankings over documents are combined to find the most relevant document in information retrieval Liu (2011); noisy answers from online workers are aggregated to produce a more accurate answer in crowdsourcing Mao et al.", "startOffset": 97, "endOffset": 117}, {"referenceID": 6, "context": "For example, meta-search engines aggregate rankings over webpages from individual search engines Dwork et al. (2001); rankings over documents are combined to find the most relevant document in information retrieval Liu (2011); noisy answers from online workers are aggregated to produce a more accurate answer in crowdsourcing Mao et al.", "startOffset": 97, "endOffset": 226}, {"referenceID": 6, "context": "For example, meta-search engines aggregate rankings over webpages from individual search engines Dwork et al. (2001); rankings over documents are combined to find the most relevant document in information retrieval Liu (2011); noisy answers from online workers are aggregated to produce a more accurate answer in crowdsourcing Mao et al. (2013). Rank data are also very common in economics and political science.", "startOffset": 97, "endOffset": 345}, {"referenceID": 6, "context": "For example, meta-search engines aggregate rankings over webpages from individual search engines Dwork et al. (2001); rankings over documents are combined to find the most relevant document in information retrieval Liu (2011); noisy answers from online workers are aggregated to produce a more accurate answer in crowdsourcing Mao et al. (2013). Rank data are also very common in economics and political science. For example, consumers often give discrete choices data McFadden (1974) and voters often give rankings over presidential candidates Gormley and Murphy (2008).", "startOffset": 97, "endOffset": 485}, {"referenceID": 6, "context": "For example, meta-search engines aggregate rankings over webpages from individual search engines Dwork et al. (2001); rankings over documents are combined to find the most relevant document in information retrieval Liu (2011); noisy answers from online workers are aggregated to produce a more accurate answer in crowdsourcing Mao et al. (2013). Rank data are also very common in economics and political science. For example, consumers often give discrete choices data McFadden (1974) and voters often give rankings over presidential candidates Gormley and Murphy (2008). Perhaps the most commonly-used statistical model for rank data is the Plackett-Luce model Plackett (1975); Luce (1959).", "startOffset": 97, "endOffset": 571}, {"referenceID": 6, "context": "For example, meta-search engines aggregate rankings over webpages from individual search engines Dwork et al. (2001); rankings over documents are combined to find the most relevant document in information retrieval Liu (2011); noisy answers from online workers are aggregated to produce a more accurate answer in crowdsourcing Mao et al. (2013). Rank data are also very common in economics and political science. For example, consumers often give discrete choices data McFadden (1974) and voters often give rankings over presidential candidates Gormley and Murphy (2008). Perhaps the most commonly-used statistical model for rank data is the Plackett-Luce model Plackett (1975); Luce (1959).", "startOffset": 97, "endOffset": 678}, {"referenceID": 6, "context": "For example, meta-search engines aggregate rankings over webpages from individual search engines Dwork et al. (2001); rankings over documents are combined to find the most relevant document in information retrieval Liu (2011); noisy answers from online workers are aggregated to produce a more accurate answer in crowdsourcing Mao et al. (2013). Rank data are also very common in economics and political science. For example, consumers often give discrete choices data McFadden (1974) and voters often give rankings over presidential candidates Gormley and Murphy (2008). Perhaps the most commonly-used statistical model for rank data is the Plackett-Luce model Plackett (1975); Luce (1959). The Plackett-Luce model is a natural generalization of multinomial logistic regression.", "startOffset": 97, "endOffset": 691}, {"referenceID": 18, "context": "be used for clustering McLachlan and Basford (1988). The k-mixture of Plackett-Luce combines k individual Plackett-Luce models via a linear vector of mixing coefficients.", "startOffset": 23, "endOffset": 52}, {"referenceID": 7, "context": "For example, Gormley and Murphy (2008) propose an Expectation Minorization Maximization (EMM) algorithm to compute the MLE of Plackett-Luce mixture models.", "startOffset": 13, "endOffset": 39}, {"referenceID": 7, "context": "For example, Gormley and Murphy (2008) propose an Expectation Minorization Maximization (EMM) algorithm to compute the MLE of Plackett-Luce mixture models. The EMM was applied to an Irish election dataset with 5 alternatives and the four components in the mixture model are interpreted as voting blocs. Surprisingly, the identifiability of Plackett-Luce mixture models is still unknown. Identifiability is an important property for statistical models, which requires that different parameters of the model have different distributions over samples. Identifiability is crucial because if the model is not identifiable, then there are cases where it is impossible to estimate the parameter from the data, and in such cases conclusions drawn from the learned parameter can be wrong. In particular, if PlackettLuce mixture models are not identifiable, then the voting bloc produced by the EMM algorithm of Gormley and Murphy (2008) can be dramatically different from the ground truth.", "startOffset": 13, "endOffset": 928}, {"referenceID": 7, "context": "For example, Gormley and Murphy (2008) propose an Expectation Minorization Maximization (EMM) algorithm to compute the MLE of Plackett-Luce mixture models. The EMM was applied to an Irish election dataset with 5 alternatives and the four components in the mixture model are interpreted as voting blocs. Surprisingly, the identifiability of Plackett-Luce mixture models is still unknown. Identifiability is an important property for statistical models, which requires that different parameters of the model have different distributions over samples. Identifiability is crucial because if the model is not identifiable, then there are cases where it is impossible to estimate the parameter from the data, and in such cases conclusions drawn from the learned parameter can be wrong. In particular, if PlackettLuce mixture models are not identifiable, then the voting bloc produced by the EMM algorithm of Gormley and Murphy (2008) can be dramatically different from the ground truth. In this paper, we address the following two important questions about the theory and practice of Plackett-Luce mixture models for rank data. Q1. Are Plackett-Luce mixture models identifiable? Q2. How can we efficiently learn Plackett-Luce mixture models? Q1 can be more complicated than one may think because the non-identifiability of a mixture model usually comes from two sources. The first is label switching, which means that if we label the components of a mixture model differently, the distribution over samples does not change Stephens (2000). This can be avoided by ordering the components and merging the same components in the mixture model.", "startOffset": 13, "endOffset": 1533}, {"referenceID": 7, "context": "For example, Gormley and Murphy (2008) propose an Expectation Minorization Maximization (EMM) algorithm to compute the MLE of Plackett-Luce mixture models. The EMM was applied to an Irish election dataset with 5 alternatives and the four components in the mixture model are interpreted as voting blocs. Surprisingly, the identifiability of Plackett-Luce mixture models is still unknown. Identifiability is an important property for statistical models, which requires that different parameters of the model have different distributions over samples. Identifiability is crucial because if the model is not identifiable, then there are cases where it is impossible to estimate the parameter from the data, and in such cases conclusions drawn from the learned parameter can be wrong. In particular, if PlackettLuce mixture models are not identifiable, then the voting bloc produced by the EMM algorithm of Gormley and Murphy (2008) can be dramatically different from the ground truth. In this paper, we address the following two important questions about the theory and practice of Plackett-Luce mixture models for rank data. Q1. Are Plackett-Luce mixture models identifiable? Q2. How can we efficiently learn Plackett-Luce mixture models? Q1 can be more complicated than one may think because the non-identifiability of a mixture model usually comes from two sources. The first is label switching, which means that if we label the components of a mixture model differently, the distribution over samples does not change Stephens (2000). This can be avoided by ordering the components and merging the same components in the mixture model. The second is more fundamental, which states that the mixture model is nonidentifiable even after ordering and merging duplicate components. Q1 is about the second type of non-identifiability. The EMM algorithm by Gormley and Murphy (2008) converges to the MLE, but as we will see in the experiments, it can be very slow when the data size is not too small.", "startOffset": 13, "endOffset": 1875}, {"referenceID": 9, "context": "For Q2, we propose a generalized method of moments (GMM)1 algorithm (Hansen, 1982) to 1This should not be confused with Gaussian mixture models.", "startOffset": 68, "endOffset": 82}, {"referenceID": 7, "context": "We then compare our GMM algorithm and the EMM algorithm Gormley and Murphy (2008) w.", "startOffset": 56, "endOffset": 82}, {"referenceID": 7, "context": "2 Related Work and Discussions Most previous work in mixture models (especially Gaussian mixture models) focuses on cardinal data Teicher (1961, 1963); McLachlan and Peel (2004); Kalai et al.", "startOffset": 152, "endOffset": 178}, {"referenceID": 3, "context": "2 Related Work and Discussions Most previous work in mixture models (especially Gaussian mixture models) focuses on cardinal data Teicher (1961, 1963); McLachlan and Peel (2004); Kalai et al. (2012); Dasgupta (1999).", "startOffset": 179, "endOffset": 199}, {"referenceID": 0, "context": "(2012); Dasgupta (1999). Little is known about the identifiability of mixtures of models for rank data.", "startOffset": 8, "endOffset": 24}, {"referenceID": 0, "context": "(2012); Dasgupta (1999). Little is known about the identifiability of mixtures of models for rank data. For rank data, Iannario (2010) proved the identifiability of the mixture of shifted binomial model and the uniform models.", "startOffset": 8, "endOffset": 135}, {"referenceID": 0, "context": "Awasthi et al. (2014) proved the identifiability of mixtures of two Mallows\u2019 models.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Awasthi et al. (2014) proved the identifiability of mixtures of two Mallows\u2019 models. Mallows mixture models were also studied by Lu and Boutilier (2014) and Chierichetti et al.", "startOffset": 0, "endOffset": 153}, {"referenceID": 0, "context": "Awasthi et al. (2014) proved the identifiability of mixtures of two Mallows\u2019 models. Mallows mixture models were also studied by Lu and Boutilier (2014) and Chierichetti et al. (2015). Our paper, on the other hand, focuses on mixtures of Plackett-Luce models.", "startOffset": 0, "endOffset": 184}, {"referenceID": 0, "context": "Awasthi et al. (2014) proved the identifiability of mixtures of two Mallows\u2019 models. Mallows mixture models were also studied by Lu and Boutilier (2014) and Chierichetti et al. (2015). Our paper, on the other hand, focuses on mixtures of Plackett-Luce models. Technically, part of our (non-)identifiability proofs is motivated by the work of Teicher (1963), who obtained sufficient conditions for the identifiability of finite mixture models.", "startOffset": 0, "endOffset": 357}, {"referenceID": 0, "context": "Awasthi et al. (2014) proved the identifiability of mixtures of two Mallows\u2019 models. Mallows mixture models were also studied by Lu and Boutilier (2014) and Chierichetti et al. (2015). Our paper, on the other hand, focuses on mixtures of Plackett-Luce models. Technically, part of our (non-)identifiability proofs is motivated by the work of Teicher (1963), who obtained sufficient conditions for the identifiability of finite mixture models. However, technically these conditions cannot be directly applied to k-PL because they work either for finite families (Theorem 1 in Teicher (1963)) or for cardinal data (Theorem 2 in Teicher (1963)).", "startOffset": 0, "endOffset": 590}, {"referenceID": 0, "context": "Awasthi et al. (2014) proved the identifiability of mixtures of two Mallows\u2019 models. Mallows mixture models were also studied by Lu and Boutilier (2014) and Chierichetti et al. (2015). Our paper, on the other hand, focuses on mixtures of Plackett-Luce models. Technically, part of our (non-)identifiability proofs is motivated by the work of Teicher (1963), who obtained sufficient conditions for the identifiability of finite mixture models. However, technically these conditions cannot be directly applied to k-PL because they work either for finite families (Theorem 1 in Teicher (1963)) or for cardinal data (Theorem 2 in Teicher (1963)).", "startOffset": 0, "endOffset": 641}, {"referenceID": 0, "context": "Our proof for generic identifiability is based on a novel application of the tensor-decomposition approach that analyzes the generic Kruskal\u2019s rank of matrices advocated by Allman et al. (2009). In addition to being important in their own right, our (non)-identifiability theorems also carry a clear message that has been overlooked in the literature: when using Plackett-Luce mixture models to fit rank data, one must be very careful about the interpretation of the learned parameter.", "startOffset": 173, "endOffset": 194}, {"referenceID": 0, "context": "Our proof for generic identifiability is based on a novel application of the tensor-decomposition approach that analyzes the generic Kruskal\u2019s rank of matrices advocated by Allman et al. (2009). In addition to being important in their own right, our (non)-identifiability theorems also carry a clear message that has been overlooked in the literature: when using Plackett-Luce mixture models to fit rank data, one must be very careful about the interpretation of the learned parameter. Specifically, whenm \u2264 2k\u22121, it is necessary to double-check whether the learned parameter is identifiable (Theorem 1), which can be computationally hard. On the positive side, identifiability may not be a big concern in practice under a much milder condition (k \u2264 bm\u22122 2 c!, Theorem 3). Gormley and Murphy (2008) used 4-PL to fit an Irish election dataset with 5 alternatives.", "startOffset": 173, "endOffset": 799}, {"referenceID": 0, "context": "Our proof for generic identifiability is based on a novel application of the tensor-decomposition approach that analyzes the generic Kruskal\u2019s rank of matrices advocated by Allman et al. (2009). In addition to being important in their own right, our (non)-identifiability theorems also carry a clear message that has been overlooked in the literature: when using Plackett-Luce mixture models to fit rank data, one must be very careful about the interpretation of the learned parameter. Specifically, whenm \u2264 2k\u22121, it is necessary to double-check whether the learned parameter is identifiable (Theorem 1), which can be computationally hard. On the positive side, identifiability may not be a big concern in practice under a much milder condition (k \u2264 bm\u22122 2 c!, Theorem 3). Gormley and Murphy (2008) used 4-PL to fit an Irish election dataset with 5 alternatives. According to our Theorem 1, 4-PL for 5 alternatives is non-identifiable. Moreover, our generic identifiability theorem (Theorem 3) does not apply because m = 5 < 6. Therefore, it is possible that there exists another set of voting blocs and mixing coefficients with the same likelihood as the output of the EMM algorithm. Whether it is true or not, we believe that it is important to add discussions and justifications of the uniqueness of the voting blocs obtained by Gormley and Murphy (2008). Parameter inference for single Plackett-Luce models is studied in Cheng et al.", "startOffset": 173, "endOffset": 1358}, {"referenceID": 0, "context": "Our proof for generic identifiability is based on a novel application of the tensor-decomposition approach that analyzes the generic Kruskal\u2019s rank of matrices advocated by Allman et al. (2009). In addition to being important in their own right, our (non)-identifiability theorems also carry a clear message that has been overlooked in the literature: when using Plackett-Luce mixture models to fit rank data, one must be very careful about the interpretation of the learned parameter. Specifically, whenm \u2264 2k\u22121, it is necessary to double-check whether the learned parameter is identifiable (Theorem 1), which can be computationally hard. On the positive side, identifiability may not be a big concern in practice under a much milder condition (k \u2264 bm\u22122 2 c!, Theorem 3). Gormley and Murphy (2008) used 4-PL to fit an Irish election dataset with 5 alternatives. According to our Theorem 1, 4-PL for 5 alternatives is non-identifiable. Moreover, our generic identifiability theorem (Theorem 3) does not apply because m = 5 < 6. Therefore, it is possible that there exists another set of voting blocs and mixing coefficients with the same likelihood as the output of the EMM algorithm. Whether it is true or not, we believe that it is important to add discussions and justifications of the uniqueness of the voting blocs obtained by Gormley and Murphy (2008). Parameter inference for single Plackett-Luce models is studied in Cheng et al. (2010) and Azari Soufiani et al.", "startOffset": 173, "endOffset": 1445}, {"referenceID": 0, "context": "Our proof for generic identifiability is based on a novel application of the tensor-decomposition approach that analyzes the generic Kruskal\u2019s rank of matrices advocated by Allman et al. (2009). In addition to being important in their own right, our (non)-identifiability theorems also carry a clear message that has been overlooked in the literature: when using Plackett-Luce mixture models to fit rank data, one must be very careful about the interpretation of the learned parameter. Specifically, whenm \u2264 2k\u22121, it is necessary to double-check whether the learned parameter is identifiable (Theorem 1), which can be computationally hard. On the positive side, identifiability may not be a big concern in practice under a much milder condition (k \u2264 bm\u22122 2 c!, Theorem 3). Gormley and Murphy (2008) used 4-PL to fit an Irish election dataset with 5 alternatives. According to our Theorem 1, 4-PL for 5 alternatives is non-identifiable. Moreover, our generic identifiability theorem (Theorem 3) does not apply because m = 5 < 6. Therefore, it is possible that there exists another set of voting blocs and mixing coefficients with the same likelihood as the output of the EMM algorithm. Whether it is true or not, we believe that it is important to add discussions and justifications of the uniqueness of the voting blocs obtained by Gormley and Murphy (2008). Parameter inference for single Plackett-Luce models is studied in Cheng et al. (2010) and Azari Soufiani et al. (2013). Azari Soufiani et al.", "startOffset": 173, "endOffset": 1478}, {"referenceID": 0, "context": "Our proof for generic identifiability is based on a novel application of the tensor-decomposition approach that analyzes the generic Kruskal\u2019s rank of matrices advocated by Allman et al. (2009). In addition to being important in their own right, our (non)-identifiability theorems also carry a clear message that has been overlooked in the literature: when using Plackett-Luce mixture models to fit rank data, one must be very careful about the interpretation of the learned parameter. Specifically, whenm \u2264 2k\u22121, it is necessary to double-check whether the learned parameter is identifiable (Theorem 1), which can be computationally hard. On the positive side, identifiability may not be a big concern in practice under a much milder condition (k \u2264 bm\u22122 2 c!, Theorem 3). Gormley and Murphy (2008) used 4-PL to fit an Irish election dataset with 5 alternatives. According to our Theorem 1, 4-PL for 5 alternatives is non-identifiable. Moreover, our generic identifiability theorem (Theorem 3) does not apply because m = 5 < 6. Therefore, it is possible that there exists another set of voting blocs and mixing coefficients with the same likelihood as the output of the EMM algorithm. Whether it is true or not, we believe that it is important to add discussions and justifications of the uniqueness of the voting blocs obtained by Gormley and Murphy (2008). Parameter inference for single Plackett-Luce models is studied in Cheng et al. (2010) and Azari Soufiani et al. (2013). Azari Soufiani et al. (2013) proposed a GMM, which is quite different from our method, and cannot be applied to Plackett-Luce mixture models.", "startOffset": 173, "endOffset": 1508}, {"referenceID": 0, "context": "Our proof for generic identifiability is based on a novel application of the tensor-decomposition approach that analyzes the generic Kruskal\u2019s rank of matrices advocated by Allman et al. (2009). In addition to being important in their own right, our (non)-identifiability theorems also carry a clear message that has been overlooked in the literature: when using Plackett-Luce mixture models to fit rank data, one must be very careful about the interpretation of the learned parameter. Specifically, whenm \u2264 2k\u22121, it is necessary to double-check whether the learned parameter is identifiable (Theorem 1), which can be computationally hard. On the positive side, identifiability may not be a big concern in practice under a much milder condition (k \u2264 bm\u22122 2 c!, Theorem 3). Gormley and Murphy (2008) used 4-PL to fit an Irish election dataset with 5 alternatives. According to our Theorem 1, 4-PL for 5 alternatives is non-identifiable. Moreover, our generic identifiability theorem (Theorem 3) does not apply because m = 5 < 6. Therefore, it is possible that there exists another set of voting blocs and mixing coefficients with the same likelihood as the output of the EMM algorithm. Whether it is true or not, we believe that it is important to add discussions and justifications of the uniqueness of the voting blocs obtained by Gormley and Murphy (2008). Parameter inference for single Plackett-Luce models is studied in Cheng et al. (2010) and Azari Soufiani et al. (2013). Azari Soufiani et al. (2013) proposed a GMM, which is quite different from our method, and cannot be applied to Plackett-Luce mixture models. The MM algorithm by Hunter (2004), which is compared in Azari Soufiani et al.", "startOffset": 173, "endOffset": 1655}, {"referenceID": 0, "context": "Our proof for generic identifiability is based on a novel application of the tensor-decomposition approach that analyzes the generic Kruskal\u2019s rank of matrices advocated by Allman et al. (2009). In addition to being important in their own right, our (non)-identifiability theorems also carry a clear message that has been overlooked in the literature: when using Plackett-Luce mixture models to fit rank data, one must be very careful about the interpretation of the learned parameter. Specifically, whenm \u2264 2k\u22121, it is necessary to double-check whether the learned parameter is identifiable (Theorem 1), which can be computationally hard. On the positive side, identifiability may not be a big concern in practice under a much milder condition (k \u2264 bm\u22122 2 c!, Theorem 3). Gormley and Murphy (2008) used 4-PL to fit an Irish election dataset with 5 alternatives. According to our Theorem 1, 4-PL for 5 alternatives is non-identifiable. Moreover, our generic identifiability theorem (Theorem 3) does not apply because m = 5 < 6. Therefore, it is possible that there exists another set of voting blocs and mixing coefficients with the same likelihood as the output of the EMM algorithm. Whether it is true or not, we believe that it is important to add discussions and justifications of the uniqueness of the voting blocs obtained by Gormley and Murphy (2008). Parameter inference for single Plackett-Luce models is studied in Cheng et al. (2010) and Azari Soufiani et al. (2013). Azari Soufiani et al. (2013) proposed a GMM, which is quite different from our method, and cannot be applied to Plackett-Luce mixture models. The MM algorithm by Hunter (2004), which is compared in Azari Soufiani et al. (2013), is also very different from the EMM that is being compared in this paper.", "startOffset": 173, "endOffset": 1706}, {"referenceID": 15, "context": "We will prove this result for a more general class of models called random utility models (RUM), of which the Plackett-Luce model is a special case Thurstone (1927).", "startOffset": 119, "endOffset": 165}, {"referenceID": 0, "context": "By Corollary 3 in Allman et al. (2009), k-PL is generically identifiable.", "startOffset": 18, "endOffset": 39}, {"referenceID": 8, "context": "1 in Hall (2005). Assumption 3.", "startOffset": 5, "endOffset": 17}, {"referenceID": 8, "context": "1 in Hall (2005). The main hardness is the identifiability of 2-PL w.", "startOffset": 5, "endOffset": 17}, {"referenceID": 7, "context": "The performance of our GMM algorithm (Algorithm 1) is compared to the EMM algorithm Gormley and Murphy (2008) for 2-PL with respect to running time and statistical efficiency for synthetic data.", "startOffset": 84, "endOffset": 110}, {"referenceID": 7, "context": "Our experiments show that our GMM algorithm is significantly faster than the EMM algorithm in Gormley and Murphy (2008), while achieving competitive statistical efficiency.", "startOffset": 94, "endOffset": 120}], "year": 2016, "abstractText": "In this paper we address the identifiability and efficient learning problems of finite mixtures of Plackett-Luce models for rank data. We prove that for any k \u2265 2, the mixture of k PlackettLuce models for no more than 2k \u2212 1 alternatives is non-identifiable and this bound is tight for k = 2. For generic identifiability, we prove that the mixture of k Plackett-Luce models over m alternatives is generically identifiable if k \u2264 bm\u22122 2 c!. We also propose an efficient generalized method of moments (GMM) algorithm to learn the mixture of two Plackett-Luce models and show that the algorithm is consistent. Our experiments show that our GMM algorithm is significantly faster than the EMM algorithm by Gormley and Murphy (2008), while achieving competitive statistical efficiency.", "creator": "LaTeX with hyperref package"}}}