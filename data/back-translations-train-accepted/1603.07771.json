{"id": "1603.07771", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Neural Text Generation from Structured Data with Application to the Biography Domain", "abstract": "This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magni- tude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocab- ulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text genera- tion. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that trans- fer sample-specific words from the in- put database to the generated output sen- tence. Our neural model significantly out- performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.", "histories": [["v1", "Thu, 24 Mar 2016 22:40:00 GMT  (282kb,D)", "http://arxiv.org/abs/1603.07771v1", null], ["v2", "Thu, 22 Sep 2016 14:47:44 GMT  (341kb,D)", "http://arxiv.org/abs/1603.07771v2", "Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016"], ["v3", "Fri, 23 Sep 2016 15:16:46 GMT  (341kb,D)", "http://arxiv.org/abs/1603.07771v3", "Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["r\u00e9mi lebret", "david grangier", "michael auli"], "accepted": true, "id": "1603.07771"}, "pdf": {"name": "1603.07771.pdf", "metadata": {"source": "CRF", "title": "Generating Text from Structured Data with Application to the Biography Domain", "authors": ["Remi Lebret", "David Grangier", "Michael Auli"], "emails": ["remi@lebret.ch", "grangier@fb.com", "michaelauli@fb.com"], "sections": [{"heading": null, "text": "A typical application is the generation of a weather forecast based on a series of structured records of meteorological measurements. Unlike previous work, we scale to the large and very diverse problem of generating biographies of personalities based on Wikipedia information boxes. An infobox is a fact table that describes a person who resembles a person in a knowledge base (Bollacker et al., 2008; Ferrucci, 2012). Similar generation applications include generating product descriptions based on a catalog of millions of elements with dozens of attributes. Previous work experimented with data sets containing only a few tens of thousands of data sets, such as Weathergov or the Robocup datasets, while our datasets contain over 700 k biographies from Wikipedia."}, {"heading": "2 Related Work", "text": "Traditionally, generational systems have been based on rules and handmade specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010). The generation is divided into modular but highly interdependent decisions: (1) Content planning defines which parts of the input fields or representations of meaning should be selected; (2) Sentencing determines which selected fields should be treated in each output set; and (3) Surface realization generates these sequences. Data-driven approaches were also proposed to automatically learn the individual modules. An approach first aligns records and sentences, and then learns a content selection model (Duboue and McKeown, 2002; Barzilay and Lapata, 2005). Hierarchical hidden semi-Markov generative models were also used to first determine which facts to discuss and then words to emerge from the selected facts (al Liet, 2009)."}, {"heading": "3 Language Modeling for Constrained Sentence generation", "text": "Conditional language models are a popular choice for generating sentences. We introduce a tabulated language model to limit sentence generation to elements from fact tables."}, {"heading": "3.1 Language Model", "text": "In view of a sentence s = w1,..., wT consisting of T-words from a vocabulary W estimates a language model: P (s) = T-1 = 1P (wt | w1,..., wt \u2212 1). (1) Let ct = wt \u2212 (n \u2212 1),.., wt \u2212 1 be the sequence of n \u2212 1 context words preceding wt-1. In a grammatical model, Equation 1 is approximate asP (s) \u2248 T-t = 1P (wt | ct), (2) assuming an order n Markov property."}, {"heading": "3.2 Language Model Conditioned on Tables", "text": "For the first time in a long time, I have been able to say, \"I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader, I am the leader.\""}, {"heading": "3.3 Copy actions", "text": "We now turn to the use of table information when evaluating output words. In particular, knowing that a word appears in the table is valuable when we think about generating that word as output. In addition, sentences expressing facts from a given table often copy words from the table. We therefore expand our language model to predict special field marks such as Name 1 or Name 2, which are then replaced by the corresponding words from the table; we only include field marks when the field mark is actually in the table. Our model reads a table and defines an output range W-Q of words W and all marks in Table Q. This allows us to generate words that are not included in the vocabulary. For example, it is unlikely that Park-Rhodes in Figure 1 belongs to W. However, Park-Rhodes is included in the output range as Name 2 (since it is the second symbol of the name field), which allows our model to generate this frequency range."}, {"heading": "4 A Neural Language Model Approach", "text": "A forward-looking neural speech model (NLM) estimates P (wt | ct) in Equation 1 with a parametric function \u03c6\u03b8. This function is a composition of simple differentiable functions or layers, \u03b8 referring to the learnable parameters of the network. Considering a context input value, it gives a score for each next word wt-W, \u03c6\u03b8 (ct) and p-R-W. The probability distribution is then determined by applying the Softmax activation function: P (wt = w | ct) = exp (\u03c6\u03b8 (ct, w)) \u2211 | W | i = 1 exp (\u03c6\u03b8 (ct, wi)) (9)"}, {"heading": "4.1 Embeddings as inputs", "text": "This year the number of mentioned, mentioned and mentioned hsci-eaJnlhsrcsrc\u00fceSrlhsrteeaeFnln in the eeisrrcnh-eaJnlhsrcnlhsrteeaeFnlrsrteeaeoiuiuuugnnlrgVnlrrrsrsrrsrsrteeoioiuiuiuiuiuiuuuuuuuugnnlrrrrrrgn in the rf\u00fc of rf\u00fc-eaeaeaeaeaos-eaeaos-frso-eaeaeaos-rros-rrrros-rrrrros-f\u00fc in the rf\u00fc of rf\u00fc-eaeaeaeaeaeeeeeeaos-rteeaos-eaeaeaeaeaos-rso-eaeaeaeaeaeaeaeaeaeos-ros-ros-rrrros-rrrrrros-rrrfos-rrrrrrrrfos-fos-f\u00fc."}, {"heading": "4.2 In-vocabulary outputs", "text": "The representation of the context hct is then multiplied by a matrix with one line per word, giving a real value for each word in the vocabulary, \u03c6Wct = W outhct + b out \u0441 R | W |, (15) where W 1-Rnhu \u00b7 d1, W-R | W | \u00d7 nhu, b1-Rnhu and bout-R | W | are learnable weights and distortions, and Tanh denotes the component-wise hyperbolic tangent."}, {"heading": "4.3 Mixing outputs for better copying", "text": "Section 3.3 explains that each word w is also associated with zw, the set of fields in which it occurs, along with the position in that field. Similar to local conditioning, we represent each field and position pair (j, i) with an embedded Fj, i. These embedding are then projected into the same space as the latent representation of the context input hct. Finally, with the maximum operation via the embedding dimension, each word is embedded in a unique vector: qw = max {tanh (W 2Fj, i + b2), Fj, i \u0439zw} (16), where W 2, Rnhu \u00b7 d and b2, Rnhu are learnable weights and distortions, and qw, Rnhu. A point product with the context vector hct creates a real value for each word w in the table, Qct (w) = hct \u00b7 wt \u00b2 (then, Qw \u00b2) gets the final value (Q), Qw \u00b2 (Q)."}, {"heading": "4.4 Training", "text": "The neural language model is designed to minimize the negative log probability of a training set s with stochastic gradient descent (SGD; LeCun et al. 2012): L\u03b8 (s) = \u2212 T \u2211 t = 1logP (wt | ct, zct, gf, gw), (19) with \u03b8 = {W; Z; Gf; Gw; F; W 1; b1; W 2; b2; W out; bout}."}, {"heading": "5 Experiments", "text": "Our neural network model (Section 4) is designed to generate sentences from tables for major problems where a variety of sentence types need to be generated. Biographies are therefore a good framework to evaluate our model, with Wikipedia offering a large and diverse set of data."}, {"heading": "5.1 Biography dataset", "text": "The corpus consists of 728,321 biographical articles extracted from Wikipedia (September 2015 dump), and these biographies have been identified using \"WikiProject Biography.\" 1 For each biographical article, only the introductory section and the infobox are retained; we retain only articles for which there is an infobox. The resulting data set has a vocabulary of 403k words. Introductions are broken down into sentences and provided with the Stanford CoreNLP toolkit (Manning et al., 2014). All numbers are assigned to a special token \"0,\" with the exception of years assigned to another special token \"XXXX.\" Introductions are also tokenized, templates for birth dates and death dates have been formatted in natural language.2 All tokens in introductions and infoboxes have been packaged lower. The final corpus has been divided into three subsections to provide training (80%, 10%) validation (10%) and (10%)."}, {"heading": "5.2 Baseline", "text": "Our basic model is an interpolated language model of the Kneser-Ney (KN) and we use the KenLM toolkit to train 5 gram models without circumcision (Heafield et al., 2013). We equip the baseline with word copying actions from tables into sentences (Section 3.3) by pre-processing words that occur in both as follows: Each copied word w is replaced by a special symbol reflecting its table descriptor zw (Equation 3). The introductory section of the table in Figure 1 can look like this: \"Name 1 Name 2 (Date of Birth 1 Date of Birth 2 Date of Birth 3 - Date of Death 1 Date 2 Date of Death 3) was an English linguist, Fields 3 Pathologist, Fields 10 Scientists, Mathematicians, Mystics and 1See https: / / en.wikipedia.org / wiki / Wikipedia: WikiProject _ Biography2See https: / / dekipediada.org / Template"}, {"heading": "5.3 Training setup", "text": "For our neural models, we train 11 gram language models (n = 11) with a learning rate of 0.0025. Table 2 describes the other hyperparameters. We include all fields that occur at least 100 times in the training data of F, the set of fields. We include the 20,000 most common words in the vocabulary. The other hyperparameters are set by validation, maximizing BLEU over a validation subset of 1000 sentences. Similarly, early quitting is applied: the training ends when BLEU stops improving on the same validation subset. It should be noted that the maximum number of tokens in a field l = 10 means that we only encode 10 positions: for longer field values, the final tokens are not dropped, but their position is limited to 10. We initialize the word embedding W from Hellinger PCA, which is calculated using the set of training biographies. This representation has proved helpful for various applications (Live, 2014 and Collobert)."}, {"heading": "5.4 Evaluation metrics", "text": "We use two different parameters to evaluate our models. The performance is initially evaluated in terms of perplexity, the standard parameter for language modeling. We also measure the quality of the sentences produced with BLEU (Papineni et al., 2002) and specify the mean and standard deviation for five models initialized with different seeds."}, {"heading": "6 Results", "text": "This section describes our results and discusses the effects of the various conditioning variables."}, {"heading": "6.1 The more, the better", "text": "The results (Table 1) show that more conditioning information helps improve the performance of our models. However, we first discuss models without copy actions (the first three results) and then discuss models with copy actions (the remaining results). Note that the factorization of our models results in three different output ranges, which makes comparing helplessness less easy: First, models without copy actions use a fixed output vocabulary of size | W |. Second, the Table-KN model (characterized by?) uses a fixed vocabulary of approximate size | F | oil + | W |. Third, all other table NLM models (marked by \u2020) have the usual fixed output vocabulary plus a variable quantity Q, which changes with each input table, as in Section 3.3.3.Without copy actions. In terms of perplexity, the (i) neural language model (NLM) is slightly better than an interpolated language model, KN, and the initial output of NLM."}, {"heading": "6.2 Attention mechanism", "text": "Our model implements attention via input table fields. For each word w in the table is equation (18) na mebi rthda tebi rthpl acena tiona lityoc cupa tion1 2 1 2 3 1 1 2 < s > nelliewong (bornseptember12.1934) isanamericanpoetandactivst.Figure 3: Visualizing attention values for Nellie Wong's Wikipedia Infobox. Lines represent the probability distribution over (field, position) pairs from the table after generating each word. The columns represent the context of conditioning, e.g. the model takes n \u2212 1 words as context. The darker the color, the higher the probability.the language model takes score throuWct and adds a distortion. The distortion is the point product between a representation of the table field in which w occurs, and a representation of the context that summarizes the equation 17 (the previously generated word)."}, {"heading": "6.3 Sentence decoding", "text": "We use a standard beam search to explore a larger number of sets compared to a simple greedy search. This allows us to explore K times more paths, resulting in a linear increase in the number of forward computation steps for our language model. We compare different beam settings for the base table KN and our table NLM (Figure 2). The best validation BLEU can be achieved with a beam size of K = 5. Our model is also many times faster than the baseline and requires only about 200 ms per set with K = 5. Beam search generates many n-gram lookups for Kneser-Ney, requiring many random memory accesses; while neural models perform scoring through matrix matrix products, an operation that is more local and can be performed in parallel in blocks where modern graphics processors shine (Kindratenko, 2014)."}, {"heading": "6.4 Qualitative analysis", "text": "Table 3 shows generations for different variants of our model based on the Wikipedia table in Figure 1. First, the comparison with the Fact Table shows that our training dates are not perfect. The month of birth mentioned in the Fact Table and the first sentence of the Wikipedia article are different; this may have been introduced by an employee who edited the article and the information is not consistent. All three versions of our model correctly generate the beginning of the sentence by copying the name, date of birth and date of death from the table. The model correctly uses the past tense since the date of death in the table indicates that the person has deceased. Frederick Parker-Rhodes was a scientist, but this profession is not mentioned directly in the table. Therefore, the model without global conditioning cannot predict the right profession and it continues the generation with the most frequent occupation (in Wikipedia) for a deceased person. In contrast, global conditioning through the fields helps the model to understand that this person was actually a scientist."}, {"heading": "7 Conclusions", "text": "Local and global conditioning significantly improves our model, and we outperform a Kneser-Ney language model by almost 15 BLEU. Our task uses an order of magnitude more data than previous work and has a vocabulary three orders of magnitude larger. In this essay, we have focused only on generating the first sentence and will deal with generating longer biographies in future work. Furthermore, the current educational loss function does not explicitly punish the model for generating false facts, such as predicting the wrong nationality or profession, is currently not considered worse than choosing the wrong determinator. A loss function that could assess the actual accuracy would certainly improve penalty generation by avoiding such errors."}], "references": [{"title": "A simple domain-independent probabilistic approach to generation", "author": ["References G. Angeli", "P. Liang", "D. Klein."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Angeli et al\\.,? 2010", "shortCiteRegEx": "Angeli et al\\.", "year": 2010}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Collective content selection for concept-to-text generation", "author": ["R. Barzilay", "M. Lapata."], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 331\u2013338. Association for", "citeRegEx": "Barzilay and Lapata.,? 2005", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2005}, {"title": "Aggregation via set partitioning for natural language generation", "author": ["R. Barzilay", "M. Lapata."], "venue": "Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational", "citeRegEx": "Barzilay and Lapata.,? 2006", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2006}, {"title": "Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models", "author": ["A. Belz."], "venue": "Natural Language Engineering, 14(04):431\u2013455.", "citeRegEx": "Belz.,? 2008", "shortCiteRegEx": "Belz.", "year": 2008}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin."], "venue": "Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "International Conference on Management of Data, pages 1247\u20131250. ACM.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Coral: Using natural language generation for navigational assistance", "author": ["R. Dale", "S. Geldof", "J.-P. Prost."], "venue": "Proceedings of the 26th Australasian computer science conference-Volume 16, pages 35\u201344. Australian Computer Society, Inc.", "citeRegEx": "Dale et al\\.,? 2003", "shortCiteRegEx": "Dale et al\\.", "year": 2003}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, vol-", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Content planner construction via evolutionary algorithms and a corpus-based fitness function", "author": ["P.A. Duboue", "K.R. McKeown."], "venue": "Proceedings of INLG 2002, pages 89\u201396.", "citeRegEx": "Duboue and McKeown.,? 2002", "shortCiteRegEx": "Duboue and McKeown.", "year": 2002}, {"title": "From captions to visual concepts and back", "author": ["Platt", "L.C. Zitnick", "G. Zweig."], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.", "citeRegEx": "Platt et al\\.,? 2015", "shortCiteRegEx": "Platt et al\\.", "year": 2015}, {"title": "Introduction to this is watson", "author": ["David A Ferrucci."], "venue": "IBM Journal of Research and Development, 56(3.4):1\u20131.", "citeRegEx": "Ferrucci.,? 2012", "shortCiteRegEx": "Ferrucci.", "year": 2012}, {"title": "Generating multilingual descriptions from linguistically annotated owl ontologies: the naturalowl system", "author": ["D. Galanis", "I. Androutsopoulos."], "venue": "Proceedings of the Eleventh European Workshop on Natural Language Generation, pages 143\u2013146. As-", "citeRegEx": "Galanis and Androutsopoulos.,? 2007", "shortCiteRegEx": "Galanis and Androutsopoulos.", "year": 2007}, {"title": "Generation of biomedical arguments for lay readers", "author": ["N. Green."], "venue": "Proceedings of the Fourth International Natural Language Generation Conference, pages 114\u2013121. Association for Computational Linguistics.", "citeRegEx": "Green.,? 2006", "shortCiteRegEx": "Green.", "year": 2006}, {"title": "Surface realisation from knowledge-bases", "author": ["B. Gyawali", "C. Gardent."], "venue": "Proc. of ACL.", "citeRegEx": "Gyawali and Gardent.,? 2014", "shortCiteRegEx": "Gyawali and Gardent.", "year": 2014}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690\u2013696, Sofia, Bulgaria, August.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei."], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Generative alignment and semantic parsing for learning from ambiguous supervision", "author": ["J. Kim", "R.J. Mooney."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 543\u2013551. Association for Com-", "citeRegEx": "Kim and Mooney.,? 2010", "shortCiteRegEx": "Kim and Mooney.", "year": 2010}, {"title": "Numerical Computations with GPUs", "author": ["Volodymyr Kindratenko."], "venue": "Springer.", "citeRegEx": "Kindratenko.,? 2014", "shortCiteRegEx": "Kindratenko.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel."], "venue": "arXiv preprint arXiv:1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "A global model for concept-to-text generation", "author": ["I. Konstas", "M. Lapata."], "venue": "J. Artif. Int. Res., 48(1):305\u2013346, October.", "citeRegEx": "Konstas and Lapata.,? 2013", "shortCiteRegEx": "Konstas and Lapata.", "year": 2013}, {"title": "Word embeddings through hellinger pca", "author": ["R. Lebret", "R. Collobert."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 482\u2013490, Gothenburg, Sweden, April. Association for Com-", "citeRegEx": "Lebret and Collobert.,? 2014", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "Efficient backprop", "author": ["Y. A LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller."], "venue": "Neural networks: Tricks of the trade, pages 9\u201348. Springer.", "citeRegEx": "LeCun et al\\.,? 2012", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "Learning semantic correspondences with less supervision", "author": ["P. Liang", "M.I. Jordan", "D. Klein."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing", "citeRegEx": "Liang et al\\.,? 2009", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "A probabilistic forestto-string model for language generation from typed lambda calculus expressions", "author": ["W. Lu", "H.T. Ng."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1611\u20131622. Association", "citeRegEx": "Lu and Ng.,? 2011", "shortCiteRegEx": "Lu and Ng.", "year": 2011}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["M.-T. Luong", "I. Sutskever", "Q. V Le", "O. Vinyals", "W. Zaremba."], "venue": "Proc. ACL, pages 11\u201319.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky."], "venue": "Association for Computational Linguistics (ACL) System Demonstrations, pages 55\u201360.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "What to talk about and how? selective generation using lstms with coarse-to-fine alignment", "author": ["H. Mei", "M. Bansal", "M.R. Walter."], "venue": "Proceedings of Human Language Technologies: The 2016 Annual Conference of the North American Chapter of", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Trainable approaches to surface natural language generation and their application to conversational dialog systems", "author": ["A. Ratnaparkhi."], "venue": "Computer Speech & Language, 16(3):435\u2013455.", "citeRegEx": "Ratnaparkhi.,? 2002", "shortCiteRegEx": "Ratnaparkhi.", "year": 2002}, {"title": "Building natural language generation systems, volume 33", "author": ["E. Reiter", "R. Dale", "Z. Feng."], "venue": "MIT Press.", "citeRegEx": "Reiter et al\\.,? 2000", "shortCiteRegEx": "Reiter et al\\.", "year": 2000}, {"title": "Choosing words in computergenerated weather forecasts", "author": ["E. Reiter", "S. Sripada", "J. Hunter", "J. Yu", "I. Davy."], "venue": "Artificial Intelligence, 167(1):137\u2013169.", "citeRegEx": "Reiter et al\\.,? 2005", "shortCiteRegEx": "Reiter et al\\.", "year": 2005}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li."], "venue": "arXiv preprint arXiv:1503.02364.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Generating approximate geographic descriptions", "author": ["R. Turner", "S. Sripada", "E. Reiter."], "venue": "Empirical methods in natural language generation, pages 121\u2013 140. Springer.", "citeRegEx": "Turner et al\\.,? 2010", "shortCiteRegEx": "Turner et al\\.", "year": 2010}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan."], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrk\u0161i\u0107", "PeiHao Su", "David Vandyke", "Steve Young."], "venue": "Proceedings of the 2015 Conference on Empirical", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Generation by inverting a semantic parser that uses statistical machine translation", "author": ["Y.W. Wong", "R.J. Mooney."], "venue": "HLT-NAACL, pages 172\u2013179.", "citeRegEx": "Wong and Mooney.,? 2007", "shortCiteRegEx": "Wong and Mooney.", "year": 2007}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio."], "venue": "Proceedings of The 32nd International Conference on Machine Learning, volume 37, July.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Attention with intention for a neural network conversation model", "author": ["K. Yao", "G. Zweig", "B. Peng."], "venue": "arXiv preprint arXiv:1510.08565.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "Concept-to-text generation addresses the problem of rendering structured records into natural language (Reiter et al., 2000).", "startOffset": 103, "endOffset": 124}, {"referenceID": 6, "context": "An infobox is a fact table describing a person, similar to a person subgraph in a knowledge base (Bollacker et al., 2008; Ferrucci, 2012).", "startOffset": 97, "endOffset": 137}, {"referenceID": 11, "context": "An infobox is a fact table describing a person, similar to a person subgraph in a knowledge base (Bollacker et al., 2008; Ferrucci, 2012).", "startOffset": 97, "endOffset": 137}, {"referenceID": 5, "context": "We address this issue by parameterizing words and fields as vectors (embeddings), along with a neural language model operating on these embeddings (Bengio et al., 2003).", "startOffset": 147, "endOffset": 168}, {"referenceID": 17, "context": "This contrasts with less flexible strategies that assume the generation to follow either a hybrid alignment tree (Kim and Mooney, 2010), a probabilistic context-free grammar (Konstas and Lapata, 2013), or a tree adjoining grammar (Gyawali and Gardent, 2014).", "startOffset": 113, "endOffset": 135}, {"referenceID": 20, "context": "This contrasts with less flexible strategies that assume the generation to follow either a hybrid alignment tree (Kim and Mooney, 2010), a probabilistic context-free grammar (Konstas and Lapata, 2013), or a tree adjoining grammar (Gyawali and Gardent, 2014).", "startOffset": 174, "endOffset": 200}, {"referenceID": 14, "context": "This contrasts with less flexible strategies that assume the generation to follow either a hybrid alignment tree (Kim and Mooney, 2010), a probabilistic context-free grammar (Konstas and Lapata, 2013), or a tree adjoining grammar (Gyawali and Gardent, 2014).", "startOffset": 230, "endOffset": 257}, {"referenceID": 5, "context": "We address this issue by parameterizing words and fields as vectors (embeddings), along with a neural language model operating on these embeddings (Bengio et al., 2003). This factorization allows us to scale to a large number of words and fields compared to Liang et al. (2009) and Kim and Mooney (2010) where the number of parameters grows as the product of the number of words and fields.", "startOffset": 148, "endOffset": 278}, {"referenceID": 5, "context": "We address this issue by parameterizing words and fields as vectors (embeddings), along with a neural language model operating on these embeddings (Bengio et al., 2003). This factorization allows us to scale to a large number of words and fields compared to Liang et al. (2009) and Kim and Mooney (2010) where the number of parameters grows as the product of the number of words and fields.", "startOffset": 148, "endOffset": 304}, {"referenceID": 7, "context": "Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010).", "startOffset": 82, "endOffset": 191}, {"referenceID": 31, "context": "Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010).", "startOffset": 82, "endOffset": 191}, {"referenceID": 13, "context": "Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010).", "startOffset": 82, "endOffset": 191}, {"referenceID": 12, "context": "Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010).", "startOffset": 82, "endOffset": 191}, {"referenceID": 33, "context": "Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010).", "startOffset": 82, "endOffset": 191}, {"referenceID": 9, "context": "One approach first aligns records and sentences and then learns a content selection model (Duboue and McKeown, 2002; Barzilay and Lapata, 2005).", "startOffset": 90, "endOffset": 143}, {"referenceID": 2, "context": "One approach first aligns records and sentences and then learns a content selection model (Duboue and McKeown, 2002; Barzilay and Lapata, 2005).", "startOffset": 90, "endOffset": 143}, {"referenceID": 23, "context": "Hierarchical hidden semi-Markov generative models have also been used to first determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts (Liang et al., 2009).", "startOffset": 193, "endOffset": 213}, {"referenceID": 3, "context": "Sentence planning has been formulated as a supervised set partitioning problem over facts where each partition corresponds to a sentence (Barzilay and Lapata, 2006).", "startOffset": 137, "endOffset": 164}, {"referenceID": 29, "context": "End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011).", "startOffset": 146, "endOffset": 217}, {"referenceID": 36, "context": "End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011).", "startOffset": 146, "endOffset": 217}, {"referenceID": 4, "context": "End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011).", "startOffset": 146, "endOffset": 217}, {"referenceID": 24, "context": "End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011).", "startOffset": 146, "endOffset": 217}, {"referenceID": 0, "context": "More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013).", "startOffset": 76, "endOffset": 145}, {"referenceID": 17, "context": "More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013).", "startOffset": 76, "endOffset": 145}, {"referenceID": 20, "context": "More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013).", "startOffset": 76, "endOffset": 145}, {"referenceID": 19, "context": "Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al.", "startOffset": 94, "endOffset": 200}, {"referenceID": 16, "context": "Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al.", "startOffset": 94, "endOffset": 200}, {"referenceID": 34, "context": "Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al.", "startOffset": 94, "endOffset": 200}, {"referenceID": 37, "context": "Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al.", "startOffset": 94, "endOffset": 200}, {"referenceID": 8, "context": ", 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al.", "startOffset": 29, "endOffset": 93}, {"referenceID": 1, "context": ", 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al.", "startOffset": 29, "endOffset": 93}, {"referenceID": 25, "context": ", 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al.", "startOffset": 29, "endOffset": 93}, {"referenceID": 32, "context": ", 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015).", "startOffset": 50, "endOffset": 106}, {"referenceID": 35, "context": ", 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015).", "startOffset": 50, "endOffset": 106}, {"referenceID": 38, "context": ", 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015).", "startOffset": 50, "endOffset": 106}, {"referenceID": 27, "context": "Our model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tackle the Weathergov and Robocup tasks.", "startOffset": 29, "endOffset": 47}, {"referenceID": 5, "context": "Because the probability estimates are smooth functions of the continuous word embeddings, a small change in the features results in a small change in the probability estimates (Bengio et al., 2003).", "startOffset": 176, "endOffset": 197}, {"referenceID": 22, "context": "The neural language model is trained to minimize the negative log-likelihood of a training sentence s with stochastic gradient descent (SGD; LeCun et al. 2012) :", "startOffset": 135, "endOffset": 159}, {"referenceID": 26, "context": "Introductions are split into sentences and tokenized with the Stanford CoreNLP toolkit (Manning et al., 2014).", "startOffset": 87, "endOffset": 109}, {"referenceID": 15, "context": "Our baseline is an interpolated Kneser-Ney (KN) language model and we use the KenLM toolkit to train 5-gram models without pruning (Heafield et al., 2013).", "startOffset": 131, "endOffset": 154}, {"referenceID": 21, "context": "This representation has shown to be helpful for various applications (Lebret and Collobert, 2014).", "startOffset": 69, "endOffset": 97}, {"referenceID": 28, "context": "We also measure the quality of the generated sentences with BLEU (Papineni et al., 2002) and report the mean and standard deviation for five models initialized with different seeds.", "startOffset": 65, "endOffset": 88}, {"referenceID": 18, "context": "Beam search generates many n-gram lookups for Kneser-Ney which requires many random memory accesses; while neural models perform scoring through matrix-matrix products, an operation which is more local and can be performed in a block parallel manner where modern graphic processors shine (Kindratenko, 2014).", "startOffset": 288, "endOffset": 307}], "year": 2017, "abstractText": "This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly outperforms a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.", "creator": "LaTeX with hyperref package"}}}