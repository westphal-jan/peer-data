{"id": "1603.07772", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Co-occurrence Feature Learning for Skeleton based Action Recognition using Regularized Deep LSTM Networks", "abstract": "Skeleton based action recognition distinguishes human actions using the trajectories of skeleton joints, which provide a very good representation for describing actions. Considering that recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) can learn feature representations and model long-term temporal dependencies automatically, we propose an end-to-end fully connected deep LSTM network for skeleton based action recognition. Inspired by the observation that the co-occurrences of the joints intrinsically characterize human actions, we take the skeleton as the input at each time slot and introduce a novel regularization scheme to learn the co-occurrence features of skeleton joints. To train the deep LSTM network effectively, we propose a new dropout algorithm which simultaneously operates on the gates, cells, and output responses of the LSTM neurons. Experimental results on three human action recognition datasets consistently demonstrate the effectiveness of the proposed model.", "histories": [["v1", "Thu, 24 Mar 2016 22:43:55 GMT  (874kb,D)", "http://arxiv.org/abs/1603.07772v1", "AAAI 2016 conference"]], "COMMENTS": "AAAI 2016 conference", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["wentao zhu", "cuiling lan", "junliang xing", "wenjun zeng", "yanghao li", "li shen", "xiaohui xie"], "accepted": true, "id": "1603.07772"}, "pdf": {"name": "1603.07772.pdf", "metadata": {"source": "META", "title": "Co-occurrence Feature Learning for Skeleton based Action Recognition using Regularized Deep LSTM Networks", "authors": ["Wentao Zhu", "Cuiling Lan", "Junliang Xing", "Wenjun Zeng", "Yanghao Li", "Li Shen", "Xiaohui Xie"], "emails": ["wentaoz1@uci.edu,", "culan@microsoft.com,", "wezeng@microsoft.com,", "jlxing@nlpr.ia.ac.cn,", "lyttonhao@pku.edu.cn,", "li.shen@vipl.ict.ac.cn,", "xhx@ics.uci.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive themselves if they don't see themselves able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Activity Recognition with Neural Networks", "text": "In contrast to the handmade features, there is a growing trend to learn robust feature representations from raw data with deep neural networks, and excellent performance has been reported in image classification (Krizhevsky, Sutskever, and Hinton 2012) and speech recognition (Graves, Mohamed, and Hinton 2013). However, there is little work that uses neural networks for skeleton-based action detection. A multi-layered perception network is trained to classify each image (Cho and Chen 2014); however, such a network is not particularly good at exploring temporal dependencies. In contrast, a gesture recognition system (Lefebvre et al. 2013) uses a flat bidirectional LSTM with only one forward-hidden layer and one backward-hidden layer to explore extensive temporal dependencies. A deep-recurring neural network architecture with handmade subnets is linked to the handheld networks with only one forward-hidden layer and one backward-hidden layer to explore extensive temporal dependencies."}, {"heading": "2.2 Co-occurrence Exploration", "text": "An action is usually associated and characterized only with the interactions and combinations of a subset of the skeletal joints, for example, the joints \"hand,\" \"arm,\" and \"head\" are associated with the action \"telephoning.\" An actionlet ensemble model exploits this feature by degrading certain conjunctions of the features corresponding to some subsets of the joints (Wang, Liu, and Junsong 2012). Similarly, actions involving two people can be characterized by the interaction of a subset of the joints of the two persons (Yun et al. 2012; Ji, Ye, and Cheng 2014). Inspired by the actionlet ensemble model, we introduce a new exploration mechanism in the deep LSTM architecture to achieve automatic co-occurrence mining, rather than pre-determining which joints should be grouped."}, {"heading": "2.3 Dropout for Recurrent Neural Networks", "text": "In order to maintain the ability of RNs to model sequences, a dropout has been proposed only for the feedforward (along layers) connections, but not for the recurring (along layers) connections (Pham et al. 2014), to prevent all information from being deleted from the units (due to a dropout). Note that the previous work only takes the dropout into account in the output response for an LSTM neuron (Zaremba, Sutskever and Vinyals 2014). However, given that an LSTM neuron consists of internal cell and gate units, we believe that one should consider not only the output of the neuron, but also its internal structure in order to design effective dropout programs. In this paper, we draft an in-depth dropout for LSTM to address this problem."}, {"heading": "3 Deep LSTM with Co-occurrence Exploration and In-depth Dropout", "text": "Figure 1 shows the architecture of the proposed network, which has three bidirectional LSTM layers, two feedback layers, and a Softmax layer that provides the predictions, and the proposed complete connectivity architecture allows you to fully exploit the inherent correlations between the skeletal joints. In the network, co-occurence exploration is applied to the connections before the second LSTM layer to learn the co-occurrences of joints / features. LSTM dropout is applied to the last LSTM layer for more effective learning. Note that each LSTM layer uses bidirectional LSTM and we do not explicitly distinguish the forward and backward LSTM neurons in Figure 1."}, {"heading": "3.1 Overview of LSTM", "text": "The RNN is a successful model for sequential learning (Graves 2012). For recursive \u03b2 neurons on one level, the output responses ht are calculated on the basis of the xt input to that level and the ht \u2212 1 response from the previous time slotht = \u03b8 (Wxhxt + Whhht \u03b2 1 + bh), (1) where \u03b8 (\u00b7) denotes the activation function, bh denotes the bias vector, Wxh is the matrix of the weights between the input and the hidden layer, and Whh is the matrix of the recursive weights from the hidden layer to itself in adjacent time steps used for the exploration of the temporal dependence. \u2212 LSTM is an advanced RNN architecture that can learn dependencies between the input and the hidden layer, and Whh is the matrix."}, {"heading": "3.2 Co-occurrence Exploration", "text": "However, it is difficult to find an answer to the question of how such a situation can arise. (...) It is very likely that such a situation will arise. (...) It is very likely that such a situation will arise. (...) It is very likely that such a situation will arise. \"(...) We expect that the network will automatically investigate the connections between the different types of actions. (...) We expect that the connections between the different types of actions will arise.\" (...) We expect that the network will investigate the connections of discrimination from discriminatory connections. \"(...) We expect that the connections of discriminatory connections between the different types of actions will automatically investigate.\" (...) We expect that the network will automatically investigate the connections of discrimination from different kinds of actions. (...) We expect that the network will automatically investigate the connections of discrimination from different kinds of actions."}, {"heading": "3.3 In-depth Dropout for LSTM", "text": "It is not the first time that we have found the errors in the errors of the LSTM errors in Fig. 5 (a) in the unfiltered form in which the units are explicitly associated with each other, in the reurrent neural networks, the triggers of all errors from a unit are not expected, especially if the unit has made many mistakes in the past (Pham et al. 2014)."}, {"heading": "3.4 Action Recognition using the Learned Model", "text": "With the learned deep LSTM network, the probability that a sequence X belongs to the class Ck isp (Ck | X) = eok \u2211 C i = 1 e oi, k = 1, \u00b7 \u00b7 \u00b7, C, o = T \u2211 t = 1 (W \u2212 \u2192 h o \u2212 \u2192 h t + W \u2190 \u2212 h o \u2190 \u2212 h t + bo), (10) where C denotes the number of classes, T denotes the length of the test sequence, o = [o1, o2, \u00b7 \u00b7, oC], \u2212 \u2192 h t and \u2190 \u2212 h t the output answers of the last bidirectional LSTM layer. Subsequently, the class with the highest probability is selected as an action class."}, {"heading": "4 Experiments", "text": "We validate the proposed model on the basis of the SBU-Kinect interaction dataset (Yun et al. 2012), the HDM05 dataset (Mu \ufffd ller et al. 2007) and the CMU dataset (CMU 2003), the basic truth of which was described by ourselves. We also tested our model on the basis of the Berkeley MHAD action dataset (Ofli et al. 2013) and achieved 100% accuracy. In order to investigate the effects of each component in our model, we conduct experiments under different configurations, which are presented as follows: \u2022 Deep LSTM is our basic unregulated schema; \u2022 Deep LSTM + Co occurrence is the schema with our pro-established co-occurrence regulation; \u2022 Deep LSTM + Simple Dropout is the schema with the dropout algorithm proposed by Zaremba et al; Deep LM et al al al-Scan."}, {"heading": "4.1 SBU Kinect Interaction Dataset", "text": "The SBU-Kinect interaction dataset is a Kinect human activity detection dataset that represents a two-person interaction and contains 230 sequences of 8 classes (6,614 frames) with subject-independent 5-fold cross-validation; the smoothed positions of joints are used as input to the deep LSTM network for detection; the number of neurons is set to 100 x 2, 100, 110 x 2, 110, 200 x 2 for the first to fifth level, with 2 pointing to bidirectional LSTM, thus doubling the number of neurons. We have compared our schemas with other skeletal-based methods (Yun et al. 2012; Ji, Ye and Cheng 2014; Du, Wang and Wang 2015); We add an additional layer to merge the two subnets that correspond to the two people when they extend the hierarchical RNM interaction scheme for use in the Wang and Wang interactions (2015)."}, {"heading": "4.2 HDM05 Dataset", "text": "The HDM05 dataset contains 2,337 skeletal sequences executed by 5 actors (184,046 images after sampling).For fair comparison, we use the same protocol (65 classes, 10-fold cross-validation) as Cho and Chen (Cho and Chen 2014).The pre-processing is the same as in the hierarchical RNN scheme (Du, Wang and Wang 2015) (centralization of joint positions to the human center for each image and smoothing of positions).The number of neurons is 100 x 2, 110 x 120 x 2, 120 x 2 and 200 x 2 for the five layers, respectively. Table 2 shows the results in terms of average accuracy. Our basic deep LSTM achieves better results than the multi-layer perception model, which suggests that LSTM has better modeling capability than the MLP. With the proposed co-occurrency learning and a profound dropout hierarchy, our NSTM manually performs better than the N model."}, {"heading": "4.3 CMU Dataset", "text": "The categorized dataset contains 2,235 sequences (987,341 frames after down-sampling) and is the largest skeleton-based human action dataset to date. This dataset is much more challenging because: (i) the length of the sequences varies greatly; (ii) the diversity within the class is great, e.g. when \"walking,\" different people walk at different speeds and in different ways; (iii) the dataset contains complex actions such as dance, yoga. We evaluated the performance on both the whole dataset (CMU) and on a subset of the dataset (CMU subset). For this subset, we selected eight representative action categories containing 664 sequences (125,667 frames after down-sampling), with actions of jump, walk back, run, getting, pickup, basketball and the same dynamics."}, {"heading": "4.4 Discussions", "text": "To further understand our deep LSTM network, we visualize the weights we learned in the first LSTM layer on the SBU-Kinect interaction dataset in Fig. 6 (a). Each element represents the absolute value of the weight between the corresponding skeletal joint and the entrance gate of that LSTM neuron. It is observed that the weights in the diagonal positions marked by the red ellipse have high values, meaning that coexistence regulation helps to automatically learn the human parts. In contrast to the part-based subnet fusion model (Du, Wang and Wang 2015), the learned coexistence of joints does not restrict the connections to the same part, as there are many large weights outside the diagonal regions, e.g. in the regions marked by white circles, making the network more powerful for action detection. This also means the importance of the proposed connecting architecture of each of the different weights of the interconnecting groups."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a fully networked, deep LSTM network for skeletal-based action detection. The proposed model facilitates automatic learning of trait co-occurrences from the skeletal joints through our designed regulation. To ensure effective learning of the deep model, we design a deep dropout algorithm for the LSTM neurons that performs dropouts for the internal gates, the cell and the initial reaction of the LSTM neuron. Experimental results demonstrate the state-of-the-art performance of our model on multiple datasets."}, {"heading": "Acknowledgment", "text": "We would like to thank David Wipf of Microsoft Research Asia for the valuable discussions, and Yong Du of the Institute of Automation of the Chinese Academy of Sciences for providing the hierarchical RNN code for comparison."}], "references": [{"title": "Classifying and visualizing motion capture sequences using deep neural networks", "author": ["K. Cho", "X. Chen"], "venue": "International Conference on Computer Vision Theory and Applications, 122\u2013130.", "citeRegEx": "Cho and Chen,? 2014", "shortCiteRegEx": "Cho and Chen", "year": 2014}, {"title": "CMU graphics lab motion capture database", "author": ["CMU."], "venue": "http://mocap.cs.cmu.edu/.", "citeRegEx": "CMU.,? 2003", "shortCiteRegEx": "CMU.", "year": 2003}, {"title": "Sparse solutions to linear inverse problems with multiple measurement vectors", "author": ["S.F. Cotter", "B.D. Rao", "K. Engan", "K. Kreutz-Delgado"], "venue": "IEEE Transactions on Signal Processing 53(7):2477\u20132488.", "citeRegEx": "Cotter et al\\.,? 2005", "shortCiteRegEx": "Cotter et al\\.", "year": 2005}, {"title": "Hierarchical recurrent neural network for skeleton based action recognition", "author": ["Y. Du", "W. Wang", "L. Wang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 1110\u20131118.", "citeRegEx": "Du et al\\.,? 2015", "shortCiteRegEx": "Du et al\\.", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber,? 2005", "shortCiteRegEx": "Graves and Schmidhuber", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, 6645\u2013 6649.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["A. Graves"], "venue": "Springer.", "citeRegEx": "Graves,? 2012", "shortCiteRegEx": "Graves", "year": 2012}, {"title": "Interactive body part contrast mining for human interaction recognition", "author": ["Y. Ji", "G. Ye", "H. Cheng"], "venue": "IEEE International Conference on Multimedia and Expo Workshops, 1\u20136.", "citeRegEx": "Ji et al\\.,? 2014", "shortCiteRegEx": "Ji et al\\.", "year": 2014}, {"title": "Visual perception of biological motion and a model for it is analysis", "author": ["G. Johansson"], "venue": "Perception and Psychophysics 14(2):201\u2013 211.", "citeRegEx": "Johansson,? 1973", "shortCiteRegEx": "Johansson", "year": 1973}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "BLSTM-RNN based 3D gesture classification", "author": ["G. Lefebvre", "S. Berlemont", "F. Mamalet", "C. Garcia"], "venue": "Proceedings of the International Conference on Artificial Neural Networks and Machine Learning, 381\u2013388.", "citeRegEx": "Lefebvre et al\\.,? 2013", "shortCiteRegEx": "Lefebvre et al\\.", "year": 2013}, {"title": "Documentation mocap database HDM05", "author": ["M. M\u00fcller", "T. R\u00f6der", "M. Clausen", "B. Eberhardt", "B. Kr\u00fcger", "A. Weber"], "venue": "Technical Report CG-2007-2, Universit\u00e4t Bonn. M\u00fcller, M.; R\u00f6der, T.; and Clausen, M. 2005. Efficient contentbased retrieval of motion capture data. ACM Transactions on", "citeRegEx": "M\u00fcller et al\\.,? 2007", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2007}, {"title": "Berkeley MHAD: A comprehensive multimodal human action database", "author": ["F. Ofli", "R. Chaudhry", "G. Kurillo", "R. Vidal", "R. Bajcsy"], "venue": "Proceedings of the IEEE Workshop on Applications on Computer Vision, 53\u201360.", "citeRegEx": "Ofli et al\\.,? 2013", "shortCiteRegEx": "Ofli et al\\.", "year": 2013}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["V. Pham", "T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": "International Conference on Frontiers in Handwriting Recognition, 285\u2013290.", "citeRegEx": "Pham et al\\.,? 2014", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "A survey on vision-based human action recognition", "author": ["R. Poppe"], "venue": "Image and Vision Computing 28(6):976\u2013990.", "citeRegEx": "Poppe,? 2010", "shortCiteRegEx": "Poppe", "year": 2010}, {"title": "Smoothing and differentiation of data by simplified least squares procedures", "author": ["A. Savitzky", "M.J. Golay"], "venue": "Analytical chemistry 36(8):1627\u20131639.", "citeRegEx": "Savitzky and Golay,? 1964", "shortCiteRegEx": "Savitzky and Golay", "year": 1964}, {"title": "Conditional models for contextual human motion recognition", "author": ["C. Sminchisescu", "A. Kanaujia", "Z. Li", "D. Metaxas"], "venue": "IEEE Conference on Computer Vision, volume 2, 1808\u20131815.", "citeRegEx": "Sminchisescu et al\\.,? 2005", "shortCiteRegEx": "Sminchisescu et al\\.", "year": 2005}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15:1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Human activity detection from RGBD images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "AAAI workshop on Pattern, Activity and Intent Recognition, 47\u201355.", "citeRegEx": "Sung et al\\.,? 2011", "shortCiteRegEx": "Sung et al\\.", "year": 2011}, {"title": "Unstructured human activity detection from RGBD images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "IEEE International Conference on Robotics and Automation, 842\u2013849.", "citeRegEx": "Sung et al\\.,? 2012", "shortCiteRegEx": "Sung et al\\.", "year": 2012}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 1\u20139.", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Mining actionlet ensemble for action recognition with depth cameras", "author": ["J. Wang", "Z. Liu", "Y. Junsong"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 1290\u20131297.", "citeRegEx": "Wang et al\\.,? 2012", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "A survey of vision-based methods for action representation, segmentation and recognition", "author": ["D. Weinland", "R. Ronfard", "E. Boyerc"], "venue": "Computer Vision and Image Understanding 115(2):224\u2013241.", "citeRegEx": "Weinland et al\\.,? 2011", "shortCiteRegEx": "Weinland et al\\.", "year": 2011}, {"title": "View invariant human action recognition using histograms of 3D joints", "author": ["L. Xia", "C.-C. Chen", "J.K. Aggarwal"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops, 20\u201327.", "citeRegEx": "Xia et al\\.,? 2012", "shortCiteRegEx": "Xia et al\\.", "year": 2012}, {"title": "Effective 3D action recognition using EigenJoints", "author": ["X. Yang", "Y. Tian"], "venue": "Journal of Visual Communication and Image Representation 25(1):2\u201311.", "citeRegEx": "Yang and Tian,? 2014", "shortCiteRegEx": "Yang and Tian", "year": 2014}, {"title": "Two-person interaction detection using body pose features and multiple instance learning", "author": ["K. Yun", "J. Honorio", "D. Chattopadhyay", "T.L. Berg", "D. Samaras"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops, 28\u201335.", "citeRegEx": "Yun et al\\.,? 2012", "shortCiteRegEx": "Yun et al\\.", "year": 2012}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "It facilitates a wide range of applications such as intelligent video surveillance, human-computer interaction, and video understanding (Poppe 2010; Weinland, Ronfard, and Boyerc 2011).", "startOffset": 136, "endOffset": 184}, {"referenceID": 8, "context": "Biological observations suggest that humans can recognize actions from just the motion of a few light displays attached to the human body (Johansson 1973).", "startOffset": 138, "endOffset": 154}, {"referenceID": 19, "context": "One is to design robust and discriminative features from the skeleton (and the corresponding RGBD images) for intra-frame content representation (M\u00fcller, R\u00f6der, and Clausen 2005; Wang, Liu, and Junsong 2012; Sung et al. 2012; Yang and Tian 2014; Ji, Ye, and Cheng 2014).", "startOffset": 145, "endOffset": 269}, {"referenceID": 24, "context": "One is to design robust and discriminative features from the skeleton (and the corresponding RGBD images) for intra-frame content representation (M\u00fcller, R\u00f6der, and Clausen 2005; Wang, Liu, and Junsong 2012; Sung et al. 2012; Yang and Tian 2014; Ji, Ye, and Cheng 2014).", "startOffset": 145, "endOffset": 269}, {"referenceID": 18, "context": "The other is to explore temporal dependencies of the interframe content for action dynamics modeling, using hierarchical maximum entropy Markov model (Sung et al. 2011), hidden Markov model (Xia, Chen, and Aggarwal 2012) or Conditional Random Fields (Sminchisescu et al.", "startOffset": 150, "endOffset": 168}, {"referenceID": 16, "context": "2011), hidden Markov model (Xia, Chen, and Aggarwal 2012) or Conditional Random Fields (Sminchisescu et al. 2005).", "startOffset": 87, "endOffset": 113}, {"referenceID": 4, "context": "Inspired by the success of deep recurrent neural networks (RNNs) using the Long Short-Term Memory (LSTM) architecture for speech feature learning and time series modeling (Graves, Mohamed, and Hinton 2013; Graves and Schmidhuber 2005), we intend to build an effective action recognition model based on deep LSTM network.", "startOffset": 171, "endOffset": 234}, {"referenceID": 6, "context": "The proposed network is constructed by inheriting many insights from recent successful networks (Graves 2012; Krizhevsky, Sutskever, and Hinton 2012; Szegedy et al. 2015; Du, Wang, and Wang 2015) and is designed to robustly model complex relationships among different joints.", "startOffset": 96, "endOffset": 195}, {"referenceID": 20, "context": "The proposed network is constructed by inheriting many insights from recent successful networks (Graves 2012; Krizhevsky, Sutskever, and Hinton 2012; Szegedy et al. 2015; Du, Wang, and Wang 2015) and is designed to robustly model complex relationships among different joints.", "startOffset": 96, "endOffset": 195}, {"referenceID": 0, "context": "A multi-layer perceptron network is trained to classify each frame (Cho and Chen 2014); however, such a network cannot explore temporal dependencies very well.", "startOffset": 67, "endOffset": 86}, {"referenceID": 10, "context": "In contrast, a gesture recognition system (Lefebvre et al. 2013) employs a shallow bidirectional LSTM with only one forward hidden layer and one backward hidden layer to explore long-range temporal dependencies.", "startOffset": 42, "endOffset": 64}, {"referenceID": 25, "context": "Similarly, actions involving two people can be characterized by the interactions of a subset of the two persons\u2019 joints (Yun et al. 2012; Ji, Ye, and Cheng 2014).", "startOffset": 120, "endOffset": 161}, {"referenceID": 13, "context": "In order to preserve the ability of RNNs to model sequences, dropout applied only to the feedforward (along layers) connections but not to the recurrent (along time) connections is proposed (Pham et al. 2014).", "startOffset": 190, "endOffset": 208}, {"referenceID": 6, "context": "1 Overview of LSTM The RNN is a successful model for sequential learning (Graves 2012).", "startOffset": 73, "endOffset": 86}, {"referenceID": 6, "context": "To allow the information from both the future and the past to determine the output, bidirectional LSTM can be utilized (Graves 2012).", "startOffset": 119, "endOffset": 132}, {"referenceID": 6, "context": "whereL is the maximum likelihood loss function of the deep LSTM network (Graves 2012).", "startOffset": 72, "endOffset": 85}, {"referenceID": 2, "context": "i \u221a\u2211 jw 2 i,j (Cotter et al. 2005), is used to drive the units to select a conjunction of descriptive inputs (joints/features) since the `21 norm can encourage the matrix Wx\u03b2,k to be column sparse.", "startOffset": 14, "endOffset": 34}, {"referenceID": 13, "context": "For recurrent neural networks, the erasing of all the information from a unit is not expected, especially when the unit remembers events that occurred many timesteps back in the past (Pham et al. 2014).", "startOffset": 183, "endOffset": 201}, {"referenceID": 25, "context": "We validate the proposed model on the SBU kinect interaction dataset (Yun et al. 2012), HDM05 dataset (M\u00fcller et al.", "startOffset": 69, "endOffset": 86}, {"referenceID": 11, "context": "2012), HDM05 dataset (M\u00fcller et al. 2007), and CMU dataset (CMU 2003) whose groundtruth was labeled by ourselves.", "startOffset": 21, "endOffset": 41}, {"referenceID": 12, "context": "We have also tested our model on the Berkeley MHAD action recognition dataset (Ofli et al. 2013) and achieved 100% accuracy.", "startOffset": 78, "endOffset": 96}, {"referenceID": 15, "context": "To reduce the influence of noise in the captured skeleton data, we smooth each joint\u2019s position of the raw skeleton using the filter (\u22123, 12, 17, 12,\u22123) /35 in the temporal domain (Savitzky and Golay 1964; Du, Wang, and Wang 2015).", "startOffset": 180, "endOffset": 230}, {"referenceID": 17, "context": "Note that when dropout is applied, the number of neurons in the corresponding layer is doubled as suggested by previous work (Srivastava et al. 2014).", "startOffset": 125, "endOffset": 149}, {"referenceID": 25, "context": "We have compared our schemes with other skeleton based methods (Yun et al. 2012; Ji, Ye, and Cheng 2014; Du, Wang, and Wang 2015).", "startOffset": 63, "endOffset": 129}, {"referenceID": 25, "context": "(%) Raw skeleton (Yun et al. 2012) 49.", "startOffset": 17, "endOffset": 34}, {"referenceID": 25, "context": "7 Joint feature (Yun et al. 2012) 80.", "startOffset": 16, "endOffset": 33}, {"referenceID": 0, "context": "For fair comparison, we use the same protocol (65 classes, 10-fold cross validation) as used by Cho and Chen (Cho and Chen 2014).", "startOffset": 109, "endOffset": 128}, {"referenceID": 0, "context": "(%) Multi-layer Perceptron (Cho and Chen 2014) 95.", "startOffset": 27, "endOffset": 46}], "year": 2016, "abstractText": "Skeleton based action recognition distinguishes human actions using the trajectories of skeleton joints, which provide a very good representation for describing actions. Considering that recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) can learn feature representations and model long-term temporal dependencies automatically, we propose an endto-end fully connected deep LSTM network for skeleton based action recognition. Inspired by the observation that the co-occurrences of the joints intrinsically characterize human actions, we take the skeleton as the input at each time slot and introduce a novel regularization scheme to learn the co-occurrence features of skeleton joints. To train the deep LSTM network effectively, we propose a new dropout algorithm which simultaneously operates on the gates, cells, and output responses of the LSTM neurons. Experimental results on three human action recognition datasets consistently demonstrate the effectiveness of the proposed model.", "creator": "TeX"}}}