{"id": "1603.08023", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "abstract": "We investigate evaluation metrics for end-to-end dialogue systems where supervised labels, such as task completion, are not available. Recent works in end-to-end dialogue systems have adopted metrics from machine translation and text summarization to compare a model's generated response to a single target response. We show that these metrics correlate very weakly or not at all with human judgements of the response quality in both technical and non-technical domains. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.", "histories": [["v1", "Fri, 25 Mar 2016 20:32:21 GMT  (787kb,D)", "http://arxiv.org/abs/1603.08023v1", "First 4 authors had equal contribution. 13 pages, 5 tables, 6 figures. Submitted to ACL 2016"], ["v2", "Tue, 3 Jan 2017 18:28:32 GMT  (723kb,D)", "http://arxiv.org/abs/1603.08023v2", "First 4 authors had equal contribution. 13 pages, 5 tables, 6 figures. EMNLP 2016"]], "COMMENTS": "First 4 authors had equal contribution. 13 pages, 5 tables, 6 figures. Submitted to ACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["chia-wei liu", "ryan lowe", "iulian serban", "michael noseworthy", "laurent charlin", "joelle pineau"], "accepted": true, "id": "1603.08023"}, "pdf": {"name": "1603.08023.pdf", "metadata": {"source": "CRF", "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "authors": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V. Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "emails": ["chia-wei.liu@mail.mcgill.ca", "ryan.lowe@mail.mcgill.ca", "michael.noseworthy@mail.mcgill.ca", "jpineau}@cs.mcgill.ca", "iulian.vlad.serban@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "2 Related Work", "text": "Evaluation methods for monitored dialog systems include the PARADISE framework (Walker et al., 1997), which simultaneously optimizes for tasks and alternative costs, such as the number of utterances and the delay in response of agents. Similarly, MeMo (Mo \ufffd ller et al., 2006) evaluates dialog systems through interactions with simulated users. A comprehensive overview of such metrics can be found in (Jokinen and McTear, 2009). We focus on metrics that are model-independent, i.e., the model that generates the response does not also evaluate its quality; therefore, we do not consider word perplexity, although it has been used to evaluate uncontrolled dialog models (Serban et al., 2015)."}, {"heading": "3 Evaluation Metrics", "text": "Given the context of a conversation and a proposed response, our goal is to automatically assess how appropriate and relevant the proposed response to the conversation is. We focus on metrics that compare it to the conversation's basic truth response. In particular, we examine two approaches: word-based similarity metrics and wordembedding-based similarity metrics."}, {"heading": "3.1 Word Overlap-based Metrics", "text": "In fact, it is the case that the EU Commission is in a position to orient itself in the question of whether the EU Commission will be able to comply with the rules of the EU. (...) In the question of whether the EU Commission is in a position to comply with the rules of the EU, the EU Commission has oriented itself in the question of whether the EU Commission will be in a position to comply with the rules of the EU. (...) The EU Commission has complied with the EU Commission. (...) The EU Commission has complied with the obligation of the EU Commission. (...) The EU Commission has complied with the obligation of the EU Commission. (...) The EU Commission has complied with the obligation of the EU Commission. (...) The EU Commission has complied with the obligation of the EU Commission. (...) The EU Commission has complied with the obligation of the EU Commission. (...) The EU Commission has complied with the obligation of the EU Commission. (...) The EU Commission has complied with the obligation of the EU Commission."}, {"heading": "3.2 Embedding-based Metrics", "text": "It is important to consider the meaning of each word defined by a word defined by another word, which assigns a vector for each word. (It is a vector for each word used by another word). (It is a vector, which approximates the meaning of a word, considering how often it is related to other words.) It is a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a, a vector, a vector, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a"}, {"heading": "4 End-to-End Dialogue Models", "text": "We now describe a variety of models that can be used to generate a response in the context of a conversation, which can be divided into two categories: on-demand models and generative models. Although we do not consider all the models available, the models selected cover a wide range of end-to-end models that appear in recent literature and provide a good selection of models to illustrate evaluation with existing metrics."}, {"heading": "4.1 Retrieval Models", "text": "The ranking or retrieval of models for dialog systems is typically based on whether they can retrieve the correct response from a corpus of predefined answers (Schatzmann et al., 2005). Such systems can be evaluated by recall or precision measurements. However, when used in a real environment, these models will not have access to the correct response because there is an invisible conversation. Thus, in the results presented below, we will remove the correct response from the corpus and ask the model to retrieve the most appropriate response from the remaining utterances. We then evaluate each model by comparing the retrieved response to the bottom-up response of the conversation. This closely mimics the use of these models in real life, as it tests the model's ability to generalize invisible contexts."}, {"heading": "4.2 Generative Models", "text": "In this context, we refer to a model as generative when it is able to generate completely new sentences that are not visible in the language model of the Lernset.LSTM. The base model is a language model of the LSTM (Hochreiter and Schmidhuber, 1997), which is trained to predict the next word in the (context, response) pair. During the test period, the model receives context, encodes it with the LSTM, and generates an answer using a greedy beam search procedure (Graves, 2013).During the test period, the model receives context, encodes with the LSTM, and generates an answer using a greedy beam search procedure (Graves, 2013).HRED. Finally, we consider the hierarchically recurring encoder decoder (HRED) (Serban et al., 2015).In the traditional encoder decoder framework, all statements are hierarchically summarized in the context prior to encoding."}, {"heading": "4.3 Conclusions from an Incomplete Analysis", "text": "To illustrate this point, we compare the performance of selected models by embedding metrics in two different domains: the Ubuntu Dialogue Corpus (Lowe et al., 2015), which contains technical vocabulary and in which conversations are often focused on solving a particular problem or obtaining specific information, and a non-technical Twitter corpus gathered using the method of Ritter et al. (2010), with dialogues covering a variety of topics, often without a specific goal. We consider these two datasets to be contrasting dialog domains, i.e. technical assistance against occasional chat, and because they are among the largest publicly available corporations, making them good candidates for building data-driven dialogue systems."}, {"heading": "5 Human Correlation Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data collection", "text": "We conducted a human survey to determine the correlation between human judgment on the quality of the answers and the score assigned by each metric, with the aim of following the BLEU evaluation process (Papineni et al., 2002a). 25 volunteers from the Computer Science Department at the author's institution received a context and a proposed answer and were asked to rate the quality of the answers on a scale from 1 to 5 4; in this case, a 1 indicates that the answer is either inappropriate or makes sense given the context, and a 5 indicates that the answer is very reasonable. Each volunteer received 100 questions for each of the Ubuntu and Twitter datasets. These questions correspond to 20 unique contexts with 5 different answers: an expression randomly drawn from other parts of the test group, the answer selected from each of the TF-IDF, DE and HRED models, and an answer written by a human commentator. These questions were selected because they covered a similar range of quality."}, {"heading": "5.2 Survey Results", "text": "We present the correlation between human judgments and each metaphor in Table 3. We calculate the absence of people who are asked to evaluate the text, often separately, such as \"appropriateness\" and \"informativism\" of the text (Hovy, 1999; Papineni et al.). Our evaluation focuses on appropriateness. We do not consider vulnerability because 4 out of 5 suggested responses to each context were generated by a human being. We do not consider informativism because it is not necessarily important (in Twitter), or because it is highly correlated with appropriateness (in Ubuntu)."}, {"heading": "5.3 Qualitative Analysis", "text": "To determine exactly why the metrics fail, we examine some qualitative samples where there is a discrepancy between the metrics and human evaluation. In Figure 2, we show two examples where all of the embedded metrics and BLEU-1 rate the proposed answer significantly differently from humans. From the context, it is clear that the proposed response is reasonable - in fact, both responses want to express gratitude. However, the proposed response has a different formulation from the actual response, and therefore the metrics are unable to separate the distinctive words from the rest, suggesting that the embedded metrics would benefit from a weighting of the word characteristics. The right-hand side of the image shows the reverse scenario: the embedded metrics rate the proposed response highly, while people most likely do not."}, {"heading": "6 Discussion", "text": "We have shown that many of the metrics commonly used in literature to evaluate unattended dialog systems are not strongly correlated with human judgment. Here, we are working on important issues arising from our analyses. Limited Tasks. Our analysis focuses on relatively unlimited areas. Other work that divides the dialog system into a dialogue planner and a component for generating natural language can find stronger correlations with the BLEU metric. Wen et al. (2015) provide an example when proposing a model that maps dialog files to natural language sentences and uses BLEU to evaluate the quality of the sentences generated. Since mapping of dialogue files to natural language sentences is more limited and more similar to the machine translation task, it seems likely that BLEU correlates better with human judgments. However, empirical research is necessary to justify this."}, {"heading": "Appendix: Full scatter plots", "text": "We present the scatter diagrams for all the metrics considered and their correlation with human judgment in Figures 3-7. As noted above, there is very little correlation for each of the metrics, and the BLEU-3 and BLEU-4 values are often close to zero."}], "references": [{"title": "METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures", "author": ["Banerjee", "Lavie2005] S. Banerjee", "A. Lavie"], "venue": null, "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit", "author": ["Jacob Cohen"], "venue": "Psychological bulletin,", "citeRegEx": "Cohen.,? \\Q1968\\E", "shortCiteRegEx": "Cohen.", "year": 1968}, {"title": "The measurement of textual coherence with latent semantic analysis", "author": ["Foltz et al.1998] P.W. Foltz", "W. Kintsch", "T.K. Landauer"], "venue": "Discourse processes,", "citeRegEx": "Foltz et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Foltz et al\\.", "year": 1998}, {"title": "Bootstrapping dialog systems with word embeddings", "author": ["Forgues et al.2014] G. Forgues", "J. Pineau", "J.-M. Larcheveque", "R. Tremblay"], "venue": null, "citeRegEx": "Forgues et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Forgues et al\\.", "year": 2014}, {"title": "2015a. deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse", "author": ["Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret l", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Galley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "2015b. deltableu: A discriminative metric for generation tasks with intrinsically diverse tar", "author": ["Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Galley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["A. Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Toward finely differentiated evaluation metrics for machine translation", "author": ["Eduard Hovy"], "venue": "In Proceedings of the Eagles Workshop on Standards and Evaluation", "citeRegEx": "Hovy.,? \\Q1999\\E", "shortCiteRegEx": "Hovy.", "year": 1999}, {"title": "Spoken Dialogue Systems", "author": ["Jokinen", "McTear2009] K. Jokinen", "M. McTear"], "venue": null, "citeRegEx": "Jokinen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jokinen et al\\.", "year": 2009}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Landauer", "Dumais1997] Thomas K Landauer", "Susan T Dumais"], "venue": "Psychological review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "2015a. A diversity-promoting objective function for neural conversation models. CoRR, abs/1510.03055", "author": ["Li et al.2015a] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "2015b. A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055", "author": ["Li et al.2015b] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A personabased neural conversation model", "author": ["Li et al.2016] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1603.06155", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In Text summarization branches out: Proceedings of the ACL-04 workshop,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Lowe et al.2015] Ryan Lowe", "Nissan Pow", "Iulian V. Serban", "Joelle Pineau"], "venue": "In SIGDIAL", "citeRegEx": "Lowe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al.2013] T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Memo: towards automatic usability evaluation of spoken dialogue services by user error simulations", "author": ["M\u00f6ller et al.2006] S. M\u00f6ller", "R. Englert", "K.P. Engelbrecht", "V.V. Hafner", "A. Jameson", "A. Oulasvirta", "A. Raake", "N. Reithinger"], "venue": null, "citeRegEx": "M\u00f6ller et al\\.,? \\Q2006\\E", "shortCiteRegEx": "M\u00f6ller et al\\.", "year": 2006}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["S. Roukos", "T Ward", "W Zhu"], "venue": "In Proceedings of the 40th annual meeting on Association for Computational Linguistics (ACL)", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish results", "author": ["Salim Roukos", "Todd Ward", "John Henderson", "Florence Reeder"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Unsupervised modeling of twitter conversations. In North American Chapter of the Association for Computational Linguistics (NAACL)", "author": ["Ritter et al.2010] A. Ritter", "C. Cherry", "B. Dolan"], "venue": null, "citeRegEx": "Ritter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Data-driven response generation in social media", "author": ["Ritter et al.2011a] Alan Ritter", "Colin Cherry", "William B. Dolan"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Data-driven response generation in social media", "author": ["Ritter et al.2011b] Alan Ritter", "Colin Cherry", "William B Dolan"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics", "author": ["Rus", "Lintean2012] V. Rus", "M. Lintean"], "venue": "In Proceedings of the Seventh Workshop on Building Educational Applications Us-", "citeRegEx": "Rus et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rus et al\\.", "year": 2012}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["K. Georgila", "S. Young"], "venue": "In 6th Special Interest Group on Discourse and Dialogue (SIGDIAL)", "citeRegEx": "Schatzmann et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "Building EndTo-End Dialogue Systems Using Generative Hierarchical Neural Networks", "author": ["Serban et al.2015] I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau"], "venue": "In AAAI Conference on Artificial Intelligence", "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Sordoni et al.2015] A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J. Nie", "J. Gao", "B. Dolan"], "venue": "In Conference of the North American", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Paradise: A framework for evaluating spoken dialogue agents", "author": ["Walker et al.1997] M.A. Walker", "D.J. Litman", "C.A. Kamm", "A. Abella"], "venue": "In Proceedings of the eighth conference on European chapter of the Association for Computational Lin-", "citeRegEx": "Walker et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Walker et al\\.", "year": 1997}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisit toy tasks. arXiv preprint arXiv:1502.05698", "author": ["Weston et al.2015] J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Towards universal paraphrastic sentence embeddings. CoRR, abs/1511.08198", "author": ["Wieting et al.2015] J. Wieting", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": null, "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "end-to-end systems directly from large amounts of text data for a variety of natural language tasks, such as question answering (Weston et al., 2015), machine translation (Cho et al.", "startOffset": 128, "endOffset": 149}, {"referenceID": 1, "context": ", 2015), machine translation (Cho et al., 2014), and dialogue response generation systems (Sordoni et al.", "startOffset": 29, "endOffset": 47}, {"referenceID": 27, "context": ", 2014), and dialogue response generation systems (Sordoni et al., 2015), in particular through the use of neural network models.", "startOffset": 50, "endOffset": 72}, {"referenceID": 28, "context": "Typically, evaluation is done using human-generated supervised signals, such as a task completion test or a user satisfaction score (Walker et al., 1997; M\u00f6ller et al., 2006).", "startOffset": 132, "endOffset": 174}, {"referenceID": 18, "context": "Typically, evaluation is done using human-generated supervised signals, such as a task completion test or a user satisfaction score (Walker et al., 1997; M\u00f6ller et al., 2006).", "startOffset": 132, "endOffset": 174}, {"referenceID": 26, "context": "trained (end-to-end) to predict the next utterance of a conversation, given several context utterances (Serban et al., 2015).", "startOffset": 103, "endOffset": 124}, {"referenceID": 15, "context": ", 2002a) and METEOR (Banerjee and Lavie, 2005) are now standard for evaluating machine translation models, and ROUGE (Lin, 2004) is often used for automatic summarization.", "startOffset": 117, "endOffset": 128}, {"referenceID": 17, "context": "rics such as BLEU, METEOR, and ROUGE, and word-embedding based similarity metrics derived from word embedding models such as Word2Vec (Mikolov et al., 2013).", "startOffset": 134, "endOffset": 156}, {"referenceID": 16, "context": "We study the applicability of these metrics by using them to evaluate a variety of end-to-end dialogue models, including both retrieval models such as the Dual Encoder (Lowe et al., 2015)", "startOffset": 168, "endOffset": 187}, {"referenceID": 26, "context": "and generative models that incorporate some form of recurrent decoder (Serban et al., 2015).", "startOffset": 70, "endOffset": 91}, {"referenceID": 27, "context": "This is despite the fact that metrics such as BLEU have seen significant recent use in evaluating unsupervised dialogue systems (Ritter et al., 2011a; Sordoni et al., 2015; Li et al., 2015b; Li et al., 2016).", "startOffset": 128, "endOffset": 207}, {"referenceID": 14, "context": "This is despite the fact that metrics such as BLEU have seen significant recent use in evaluating unsupervised dialogue systems (Ritter et al., 2011a; Sordoni et al., 2015; Li et al., 2015b; Li et al., 2016).", "startOffset": 128, "endOffset": 207}, {"referenceID": 18, "context": "Similarly, MeMo (M\u00f6ller et al., 2006) evaluates dialogue systems through interactions with simulated users.", "startOffset": 16, "endOffset": 37}, {"referenceID": 26, "context": "it has been used to evaluate unsupervised dialogue models (Serban et al., 2015).", "startOffset": 58, "endOffset": 79}, {"referenceID": 25, "context": "Further, we only consider metrics that can be used to evaluate proposed responses against ground-truth responses, so we do not consider retrieval-based metrics such as recall, which has been used to evaluate dialogue models (Schatzmann et al., 2005; Lowe et al., 2015).", "startOffset": 224, "endOffset": 268}, {"referenceID": 16, "context": "Further, we only consider metrics that can be used to evaluate proposed responses against ground-truth responses, so we do not consider retrieval-based metrics such as recall, which has been used to evaluate dialogue models (Schatzmann et al., 2005; Lowe et al., 2015).", "startOffset": 224, "endOffset": 268}, {"referenceID": 21, "context": "Ritter et al. (2011b) formulate the unsupervised", "startOffset": 0, "endOffset": 22}, {"referenceID": 27, "context": "Sordoni et al. (2015) extend this idea using a recurrent language model to generate responses in a context-sensitive manner.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Li et al. (2015b) evaluate their proposed diversity-promoting objective function for neural network models using BLEU score with only a single ground truth response.", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "A modified version of BLEU, deltaBLEU (Galley et al., 2015b), which takes into account several humanevaluated ground truth responses, is shown to have a weak to moderate correlation to human judgement using Twitter dialogues. However, such human annotation is often infeasible to obtain in practice. Galley et al (2015b) also show that, even with several ground truth responses available, the standard BLEU metric correlates at best weakly with human judgements.", "startOffset": 39, "endOffset": 321}, {"referenceID": 15, "context": "1 While these metrics have been shown to correlate with human judgement in their target domains (Papineni et al., 2002a; Lin, 2004), they have not been evaluated for dialogue systems.", "startOffset": 96, "endOffset": 131}, {"referenceID": 5, "context": "To the best of our knowledge, only BLEU has been evaluated in the dialogue system setting quantitatively by Galley et al. (2015a) on the Twitter domain.", "startOffset": 108, "endOffset": 130}, {"referenceID": 15, "context": "ROUGE (Lin, 2004) is a set of evaluation metrics used for automatic summarization.", "startOffset": 6, "endOffset": 17}, {"referenceID": 17, "context": "Methods such as Word2Vec (Mikolov et al., 2013) calculate these", "startOffset": 25, "endOffset": 47}, {"referenceID": 3, "context": "meanings of phrases by averaging the vector representations of their constituent words (Foltz et al., 1998; Landauer and Dumais, 1997; Mitchell and Lapata, 2008).", "startOffset": 87, "endOffset": 161}, {"referenceID": 31, "context": "This method has been widely used in other domains, for example in textual similarity tasks (Wieting et al., 2015).", "startOffset": 91, "endOffset": 113}, {"referenceID": 4, "context": "Another way to calculate sentence-level embeddings is using vector extrema (Forgues et al., 2014).", "startOffset": 75, "endOffset": 97}, {"referenceID": 25, "context": "are typically evaluated based on whether they can retrieve the correct response from a corpus of predefined responses, which includes the ground truth response to the conversation (Schatzmann et al., 2005).", "startOffset": 180, "endOffset": 205}, {"referenceID": 16, "context": "- Inverse Document Frequency (TF-IDF) retrieval model (Lowe et al., 2015).", "startOffset": 54, "endOffset": 73}, {"referenceID": 16, "context": "neural network (RNN) based architecture called the Dual Encoder (DE) model (Lowe et al., 2015).", "startOffset": 75, "endOffset": 94}, {"referenceID": 7, "context": "During test time, the model is given a context, encodes it with the LSTM and generates a response using a greedy beam search procedure (Graves, 2013).", "startOffset": 135, "endOffset": 149}, {"referenceID": 7, "context": "test time, the model is given a context, encodes it with the LSTM and generates a response using a greedy beam search procedure (Graves, 2013).", "startOffset": 128, "endOffset": 142}, {"referenceID": 26, "context": "Recurrent Encoder-Decoder (HRED) (Serban et al., 2015).", "startOffset": 33, "endOffset": 54}, {"referenceID": 16, "context": "To illustrate this point, we compare the performance of selected models according to the embedding metrics on two different domains: the Ubuntu Dialogue Corpus (Lowe et al., 2015), which con-", "startOffset": 160, "endOffset": 179}, {"referenceID": 21, "context": "ing the procedure of Ritter et al. (2010), where the dialogues cover a diverse set of topics often without any particular goal.", "startOffset": 21, "endOffset": 42}, {"referenceID": 14, "context": "Although this may not come as a surprise to some researchers in the dialogue system community, the fact remains that BLEU has been frequently used to evaluate unsupervised dialogue systems (Li et al., 2015a; Li et al., 2016; Galley et al., 2015a; Ritter et al., 2011a).", "startOffset": 189, "endOffset": 268}, {"referenceID": 2, "context": "the other respondents, which is a standard measure for inter-rater agreement (Cohen, 1968).", "startOffset": 77, "endOffset": 90}, {"referenceID": 9, "context": "Studies asking humans to evaluate text often rate different aspects separately, such as \u2018adequacy\u2019, \u2018fluency\u2019 and \u2018informativeness\u2019 of the text (Hovy, 1999; Papineni et al., 2002b).", "startOffset": 144, "endOffset": 180}, {"referenceID": 29, "context": "Wen et al. (2015) provide an example of this, when they propose a model to map", "startOffset": 0, "endOffset": 18}, {"referenceID": 27, "context": "There has been some work on using a larger set of automatically retrieved plausible responses when evaluating with BLEU (Sordoni et al., 2015).", "startOffset": 120, "endOffset": 142}], "year": 2016, "abstractText": "We investigate evaluation metrics for endto-end dialogue systems where supervised labels, such as task completion, are not available. Recent works in end-to-end dialogue systems have adopted metrics from machine translation and text summarization to compare a model\u2019s generated response to a single target response. We show that these metrics correlate very weakly or not at all with human judgements of the response quality in both technical and non-technical domains. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.", "creator": "LaTeX with hyperref package"}}}