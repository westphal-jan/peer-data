{"id": "1603.08148", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2016", "title": "Pointing the Unknown Words", "abstract": "The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems, including both traditional count based models and deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models with attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one of the softmax layers predicts the location of a word in the source sentence, and the other softmax layer predicts a word in the shortlist vocabulary. The decision of which softmax layer to use at each timestep is adaptively made by an MLP which is conditioned on the context. We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. Using our proposed model, we observe improvements in two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset.", "histories": [["v1", "Sat, 26 Mar 2016 22:31:57 GMT  (50kb,D)", "https://arxiv.org/abs/1603.08148v1", null], ["v2", "Sun, 3 Apr 2016 21:12:57 GMT  (50kb,D)", "http://arxiv.org/abs/1603.08148v2", null], ["v3", "Sun, 21 Aug 2016 20:03:39 GMT  (156kb,D)", "http://arxiv.org/abs/1603.08148v3", "ACL 2016 Oral Paper"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["\u00e7aglar g\u00fcl\u00e7ehre", "sungjin ahn", "ramesh nallapati", "bowen zhou", "yoshua bengio"], "accepted": true, "id": "1603.08148"}, "pdf": {"name": "1603.08148.pdf", "metadata": {"source": "CRF", "title": "Pointing the Unknown Words", "authors": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is in such a way that most people who are able to place themselves in the world, feel themselves squeezed into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into itself, into the world, into itself, into the world, into the world, into itself, into the world, into the world, into itself, into the world, into the world, into itself, into the world, into the world, into itself, into the world, into the world, into itself, into the world, into the world, into itself, into the world, into the world, into the world, into itself, into the world, into the world, into itself, into the world, into the world, into itself, into the world, into the world, into itself, into the world, into itself, into the world, into itself, into the world, into itself, into the world, into itself, into the world, into itself, into itself, into the world, into itself, into the world, into itself, into the world, into itself, into itself, into the world, the world, into itself, into the world, into itself, the world, into itself, into itself, the world, into itself, into itself, the world, into itself, the world, into itself, into the world, in itself, the world, the world, in itself, the world, in itself, the world, in itself, the world, in itself, in itself, the world, in itself, the world, in itself, the world, the world, in itself, the world, in itself, the world, the world, in itself, the world, the world, in itself, in itself, the world, in itself, the world, the world, the world, in itself, the world, the world, in itself, the world, the world, in itself, the world, the world,"}, {"heading": "2 Related Work", "text": "The attention-based pointer mechanism is first introduced in the pointer networks (Vinyals et al., 2015). In pointer networks, the output space of the target sequence is limited to be the observations in the input sequence (not the input space). However, instead of having a fixed dimension, the applicability of the Softmax output level is limited, Softmax output quantity of different dimensions is dynamically computed for each input sequence so that we maximize the attention span of the target input. However, its applicability is limited because, unlike our model, there is no way to choose whether it shows or not; it always indicates that the pointer networks are considered a special case of our model in which we always choose to refer to a context Output Quantity Output. Several approaches have been suggested to resolve the rare words / unknown words problem, which can generally be divided into three categories. The first category of the max. set of the output speed"}, {"heading": "3 Neural Machine Translation Model with Attention", "text": "As a basis for neural machine translation, we use the model proposed by (Bahdanau et al., 2014), which learns to (soft) align and translate together. We refer to this model as NMT. The encoder of the NMT is a bidirectional RNN (Schuster and Paliwal, 1997). The forward RNN reads the input sequence x = (x1,., xT) in left-to-right direction, which results in a sequence of hidden states (\u2212 h 1,., \u2212 h T). The backward RNN reads x in reverse direction and outputs (\u2190 \u2212 h,. \u2212 h T). We then link the hidden states of forward and backward directed RNNNNs at any point in time and obtain a sequence of annotation vectors (h1,."}, {"heading": "4 The Pointer Softmax", "text": "In this section, we present our method, which is called the softmax (PS) pointer, to handle the rare and unknown words. < p > p > p > p > p > p > p > p > p > p > p > p > p < p > p > p > p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\""}, {"heading": "4.1 Basic Components of the Pointer Softmax", "text": "In this section we discuss practical details of the three basic components of the pointer softmax. The interactions between these components and the model are illustrated in Figure 3.Location Softmax lt: The position of the word to be copied from the source code to the target is predicted by the location softmax lt. The location softmax prints the conditional probability distribution p (lt | zt = 0, (y, z) < t, x). For models using the attention mechanism such as NMT, we can reuse the probability distributions across the source words to predict the position of the word to be scored. Otherwise, we can simply use a pointer mesh of the model to predict the position. Shortlist Softmax wt: The subset of words in the vocabulary V is determined by the shortlist softmax wt.Switching network dt: The switching network dt is an MLP with sigmoid output function that switches a scalar probability between and."}, {"heading": "5 Experiments", "text": "In this section, we provide our main experimental results with the softmax pointer for machine translation and summary tasks. In our experiments, we used the same basic model and have just replaced the softmax level with the softmax level of the language model. We use the Adadelta learning rule (Zeiler, 2012) for the formation of NMT models. The code for the softmax pointer model is available at https: / / github.com / caglar / pointer _ softmax."}, {"heading": "5.1 The Rarest Word Detection", "text": "We construct a synthetic task and perform some preliminary experiments to compare the results with the pointer softmax and the regular softmax performance for the rare words. The vocabulary size of our synthetic task is | V | = 600 using sequences of length 7. The words in the sequences are randomly generated according to their universal distribution, which has the shape of a geometric distribution. The task is to predict the rarest word in the sequence according to the universal distribution of the words. During the training, the sequences are randomly generated. Prior to the training, validation and test sets are seed.We use a GRU layer over the input sequence and take the last hidden state to obtain the summary of the input sequence.The weighting, according to are conditioned only on ct, and the MLP prediction of the dt characters is conditioned on the latent representation of the weight and LP."}, {"heading": "5.2 Summarization", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "5.3 Neural Machine Translation", "text": "This year, it has come to the point where it will be able to put itself at the top, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "6 Conclusion", "text": "In this paper, we propose a simple extension of the traditional soft, attention-based shortlist softmax by using pointers via the input sequence. We show that the entire model can be trained together with a single objective function. We observe noticeable improvements over the baseline in machine translation and summary tasks using Pointer-Softmax. Thanks to a very simple modification compared to NMT, our model is able to generalize the invisible words and deal with rare words more efficiently. For the summary task on Gigaword datasets, the Pointer softmax was able to improve the results, even when used together with the big vocabulary trick. In the case of neural machine translation, we observed that training with the Pointer softmax also improves the convergence speed of the model. In the French to English machine translation on Europarl corpora, we find that using the Pointer can also improve the convergence of the model softmax."}, {"heading": "Acknowledgments", "text": "We would like to thank the developers of Theano 5 for developing such a powerful tool for scientific computing (Theano Development Team, 2016) and the support of the following research funding and computing organisations: http: / / deeplearning.net / software / theano / support: NSERC, Samsung, Calcul Que \ufffd bec, Compute Canada, the Canada Research Chairs and CIFAR. C. G. would like to thank IBM T.J. Watson Research for funding this research during his internship between October 2015 and January 2016."}], "references": [{"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Bengio", "Sen\u00e9cal2008] Yoshua Bengio", "JeanS\u00e9bastien Sen\u00e9cal"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "Bengio et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2008}, {"title": "Sumit Chopra", "author": ["Antoine Bordes", "Nicolas Usunier"], "venue": "and Jason Weston.", "citeRegEx": "Bordes et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural summarization by extracting sentences and words. arXiv preprint arXiv:1603.07252", "author": ["Cheng", "Lapata2016] Jianpeng Cheng", "Mirella Lapata"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "KyungHyun Cho", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre"], "venue": "and Yoshua Bengio.", "citeRegEx": "Chung et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Oriol Vinyals", "author": ["Dan Gillick", "Cliff Brunk"], "venue": "and Amarnag Subramanya.", "citeRegEx": "Gillick et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Hang Li", "author": ["Jiatao Gu", "Zhengdong Lu"], "venue": "and Victor OK Li.", "citeRegEx": "Gu et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Misha Denil", "author": ["Caglar Gulcehre", "Marcin Moczulski"], "venue": "and Yoshua Bengio.", "citeRegEx": "Gulcehre et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann", "Hyv\u00e4rinen2012] Michael U Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gutmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2012}, {"title": "Shaoqing Ren", "author": ["Kaiming He", "Xiangyu Zhang"], "venue": "and Jian Sun.", "citeRegEx": "He et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Mustafa Suleyman", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay"], "venue": "and Phil Blunsom.", "citeRegEx": "Hermann et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Roland Memisevic", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho"], "venue": "and Yoshua Bengio.", "citeRegEx": "Jean et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A method for stochastic optimization", "author": ["Kingma", "Adam2015] Diederik P Kingma", "Jimmy Ba Adam"], "venue": "In International Conference on Learning Representation", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Oriol Vinyals", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le"], "venue": "and Wojciech Zaremba.", "citeRegEx": "Luong et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Elena Lieven", "author": ["Danielle Matthews", "Tanya Behne"], "venue": "and Michael Tomasello.", "citeRegEx": "Matthews et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Mnih", "Kavukcuoglu2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Tomas Mikolov", "author": ["Razvan Pascanu"], "venue": "and Yoshua Bengio.", "citeRegEx": "Pascanu et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Kyunghyun Cho", "author": ["Razvan Pascanu", "Caglar Gulcehre"], "venue": "and Yoshua Bengio.", "citeRegEx": "Pascanu et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Sumit Chopra", "author": ["Alexander M. Rush"], "venue": "and Jason Weston.", "citeRegEx": "Rush et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Barry Haddow", "author": ["Rico Sennrich"], "venue": "and Alexandra Birch.", "citeRegEx": "Sennrich et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Malinda Carpenter", "author": ["Michael Tomasello"], "venue": "and Ulf Liszkowski.", "citeRegEx": "Tomasello et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Meire Fortunato", "author": ["Oriol Vinyals"], "venue": "and Navdeep Jaitly.", "citeRegEx": "Vinyals et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems, including both the traditional countbased and the deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models using attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sentence, and the other predicts a word in the shortlist vocabulary. At each time-step, the decision of which softmax layer to use choose adaptively made by an MLP which is conditioned on the context. We motivate our work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. We observe improvements on two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset using our proposed model.", "creator": "LaTeX with hyperref package"}}}