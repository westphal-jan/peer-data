{"id": "1603.08482", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2016", "title": "Estimating Mixture Models via Mixtures of Polynomials", "abstract": "Mixture modeling is a general technique for making any simple model more expressive through weighted combination. This generality and simplicity in part explains the success of the Expectation Maximization (EM) algorithm, in which updates are easy to derive for a wide class of mixture models. However, the likelihood of a mixture model is non-convex, so EM has no known global convergence guarantees. Recently, method of moments approaches offer global guarantees for some mixture models, but they do not extend easily to the range of mixture models that exist. In this work, we present Polymom, an unifying framework based on method of moments in which estimation procedures are easily derivable, just as in EM. Polymom is applicable when the moments of a single mixture component are polynomials of the parameters. Our key observation is that the moments of the mixture model are a mixture of these polynomials, which allows us to cast estimation as a Generalized Moment Problem. We solve its relaxations using semidefinite optimization, and then extract parameters using ideas from computer algebra. This framework allows us to draw insights and apply tools from convex optimization, computer algebra and the theory of moments to study problems in statistical estimation.", "histories": [["v1", "Mon, 28 Mar 2016 18:55:02 GMT  (271kb,D)", "http://arxiv.org/abs/1603.08482v1", "NIPS 2015"]], "COMMENTS": "NIPS 2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["sida i wang", "arun tejasvi chaganty", "percy liang"], "accepted": true, "id": "1603.08482"}, "pdf": {"name": "1603.08482.pdf", "metadata": {"source": "CRF", "title": "Estimating Mixture Models via Mixtures of Polynomials", "authors": ["Sida I. Wang", "Arun Tejasvi Chaganty", "Percy Liang"], "emails": ["sidaw@cs.stanford.edu", "chaganty@cs.stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "The idea of mixture modeling is to explain data by a weighted combination of simple parameterized distributions [52, 38]. In practice, the maximum probability of an estimation using expectation maximization (EM) is the workhorse for these models, since the parameter updates are often easily derivable. In practice, however, it is known that the method of moments, which dates back to Pearson [47] from 1894, is experiencing a recent revival [3, 2, 5, 27, 26, 11, 28, 24, 22, 8] due to its strong global theoretical guarantees. Current methods, however, are heavily dependent on specific distributions and are not easily extendable to new online components. In this paper, we present a method of the moment approach, which we call polymoma, for estimating a broader class of mixing models in which the torque equations are polynomicular."}, {"heading": "2 Problem formulation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The method of moments estimator", "text": "In one mixing model, each data point x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "2.2 Solving the moment conditions", "text": "Our goal is to restore the complexity of the general polynomial equation, which is limited by the number of solutions and the individual components of the K system. \"We want to restore various K components of the mixing model that generated the data and their respective mixing ratios.\" \"We want to start by ignoring the problem of sampling interference and identifiability and assuming that we obtain exact moment conditions as defined in (4). Any condition fn.\" \"K\" variables is a polynomial of parameters, \"for n = 1.\" N. Equation 4 is a polynomial system of N equations in the K + K \u00b7 P variables [2).,., \"K\" variables \"and.\" It is natural to ask whether the standard polynomial solution methods can solve (4), in which case each fn solution is simple. \"Unfortunately, the complexity of the individual polynomial systems is limited by the number of the polynomial solutions.\""}, {"heading": "3 Moment completion", "text": "The first step is to reformulate problem 6 as an example of the Generalized Moment Problem (GMP) introduced by [33]. A reference to the GMP, the algorithms for solving GMP and its various extensions is [34]. We start with the observation that problem 6 depends only on the integrals of the monomials under the measurement \u00b5: for example, if fn (\u03b8) = 2\u03b8 3 1-2, then we only need to know the integralsovers of the constituent monomials (y3,0: =) and y2,1: =. To evaluate the integral dimension over fn, we need to optimize the integral dimension over the constituent monomials (y3,0: =)."}, {"heading": "4 Solution extraction", "text": "Having completed the (parameter) Moment Matrix Mr (y) (Section 3), we now turn to the problem of extracting the model parameters. (3) The solution method we present is based on ideas from solving multivariate polynomial systems where the solutions are eigenvalues of certain multiplication matrices. (4) The main advantage of solution extraction is that the moments of higher order and structure in the parameters are within the framework of no model-specific effects.Recall that the true moment matrix isMr (y) = K \u00b2 k = 1\u03c0kv (5). (4)."}, {"heading": "5 Applications", "text": "The moment polynomial can then be derived by calculating the expectations under the model, a calculation comparable to deriving updates for EM.2 h\u03b1 (, c) = \u2211 b\u03b1 / 2c i = 0 a\u03b1, \u03b1 \u2212 2i2 \u00b2 -2ici and a\u03b1. If the coefficient of degree i is term of the new (univariate) hermite polynomial, the first few are h1 (, c) = Appendix II \u2212 2ici and a\u03b1, i the absolute value of the degree i-term of the new (univariate) hermite polynomial. The first few are h1 (, c) = Appendix II (, c) = 2 + c, h3 (, c) = 3 + 3 (we), h4 (, c) = 4 + 6\u04452c + 3c2.We implemented polymoma for several blending models in python and the code can be found."}, {"heading": "6 Conclusion", "text": "We presented a uniform framework for learning many types of mixing models by means of the Method of Moments. For example, for mixing Gaussian elements we can apply the same algorithm to both mixtures in 1D that require moments of higher order [47, 24] and mixtures in high dimensions where moments of lower order suffice [5]. It is the Generalized Moment Problem [33, 34] and its semi-defined relaxation hierarchies that gives us the universality, although we rely heavily on the ability of nuclear norm minimization to regain the underlying rank. Consequently, although we always get parameters that satisfy the conditions of the moment, we have no formal guarantees for consistent assessment in general, although we have guarantees for several model families. The second main instrument is solution extraction, which characterizes a more general structure of mixing models that could be used to link the technical structure, which is observed by many [5] literature."}, {"heading": "A Examples", "text": "In this section, we first describe how incomplete tensor factorization can be considered a special case of the solution framework (\u03b22) and how the mixture of the Gaussians, the mixture of linear regressions and multiview mixtures (5), and the symmetric, incomplete decomposition of tensors can be considered a solution problem (\u03b21). Many latent variable models have been questioned as a solution problem using tensor decomposition [5] and symmetric, incomplete tensor decomposition."}, {"heading": "A.2.1 Mixture of Linear Regressions", "text": "In Example 2.2 we described the mixture of linear regressions model in a q dimension with the parameters \u043d k = (wk, \u03c3 2 k). Let us now consider the D-dimensional expansion: we observe x = [x, \u0432] 3, where x: [x1,.., xD] is drawn from an unspecified distribution, and we select observation functions \u03c6\u03b1, b (x) = x + with \u0445 N (0, \u03c32) for a known solution. The parameters are \u03b8 k = (wk) for 1 \u2264 k. Next we select observation functions \u03c6\u03b1, b (x) = x\u03b1\u03c5b for \u03b1: 0 \u2264 | \u03b1 | 3 and 0 \u2264 b \u2264 3, with the corresponding moments polynomials: f\u03b1, b (\u03b8, x) = x \u03b1E can be a moment of observation [0, \u03c32) [(w \u00b7 x +) b]. These polynomials can be expressed in closed form using hermite polynomials (see section 2.A.2)."}, {"heading": "A.2.2 Mixture of Gaussians", "text": "Let us look at the data of Gaussians with diagonal covariance, x | z \u0445 N (\u0441z, diag (cz)). The parameters of this model are: 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001 2001, 2001, 2001, 2001 2001, 2001, 2001 2001, 2001, 2001, 2001 2001, 2001 2001, 2001, 2001 2001, 2001, 2001 2001, 2001 2001, 2001 2001, 2001 2001, 2001, 2001 2001, 2001 2001, 2001, 2001 2001, 2001, 2001 2001 2001, 2001, 2001 2001, 2001 2001, 2001, 2001 2001, 2001, 2001, 2001 2001, 2001 2001, 2001, 2001 2001, 2001 2001, 2001 2001, 2001 2001, 2001, 2001, 2001 2001, 2001, 2001, 2001, 2001 2001 2001 2001, 2001, 2001, 2001, 2001 2001, 2001, 2001, 2001 2001, 2001, 2001, 2001 2001, 2001 2001 2001, 2001 2001, 2001, 2001, 2001 2001, 2001, 2001, 2001 2001, 2001, 2001 2001, 2001, 2001, 2001 2001, 2001, 2001, 2001 2001 2001, 2001 2001, 2001, 2001, 2001, 2001 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001 2001, 2001, 2001, 2001, 2001, 2001 2001, 2001, 2001, 2001, 2001 2001 2001, 2001, 2001, 2001 2001, 2001, 2001, 2001, 2001 2001"}, {"heading": "A.2.3 Mixture of Binomials", "text": "To illustrate how polymoma can be applied to a discrete model, we add a quick example of mixing binomials in one dimension. In this model, x-N and 0 \u2264 x \u2264 m and each component is a binomial distribution for m experiments, each with a probability of success p. The probability mass function for the entire mixing model is p (x) = \u2211 K k = 1 \u03c0k (m x) pxk (1 \u2212 pk) m \u2212 x. There are only K scalar parameters p1,..., pK and the observation function is only the empirical probabilities \u03c6i (x) = 1x = i for x, i-N, 0 \u2264 x, i \u2264 n, with corresponding polynomials fi (p) = (m i) pi (1 \u2212 p) m \u2212 i, which can be extended to linear constraints in problem 8."}, {"heading": "A.2.4 Multiview Mixtures", "text": "x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}, {"heading": "B Separability", "text": "In conditional models, the coefficients of the moment polynomial = can depend on the data, but such dependence can sometimes interrupt the process of converting component-moment constraints into mixing moment constraints. In this section, we define separability, which is a sufficient condition for which dependence is permissible under polymoma, and then we give some counter-examples. Let us consider a mixture of linear regressions [54, 11] where the parameters phenomenon = (wk, \u03c3 2 k) are the slope and noise variance for each component k. Then, each data point x = [x, y] from component k will be valid by scanning x of an unknown distribution x and setting y = wkx."}, {"heading": "C Theory of the moment completion problem", "text": "In solution extraction, we assumed that moments of all monomials are observed, but in many models only polynomials of parameters can be estimated from the data. For example, in a Gaussian mixture, the observable function of the 2nd moment \u03c6 (x) = \u04412 + c is a polynomial, but solution extraction requires moments of monomials such as \u043f2 and c. Furthermore, in Section 4, we assumed that underlying true parameters exist [\u03d1 k] K = 1, while an arbitrary moment sequence of parameters y and the associated moment matrix M (y) may not agree with any parameters (i.e. no representative measure). In Section 5, we showed how completion of the moment can be accomplished using only linear algebra for multiview models, and now focus on the most difficult case of solving the SDP problem. While we do not have a complete answer, since we cannot provide sufficient literature to solve the most relevant problem 7, we can point to some of the literature."}, {"heading": "C.1 Conditions for solution extraction", "text": "In Section 4, we have shown that simple conditions based only on column space are sufficient to succeed, but in order to further investigate consistency and noise, we need to address a few more important questions. First, let's consider the silent situation where we may not have enough moment contracts to provide a unique solution (identifiability). Even if we assume that we have enough constraints to identify a K mix, we do not yet know whether solving the relaxed problem 8, which relaxes the rank = K constraint, can restore the true parameters. Second, there can be no K base ranking under noise, and even if a K base ranking exists, it cannot correspond to any applicable parameters."}, {"heading": "D Extensions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1 Constraints on parameters", "text": "While constraints can often be dealt with in maximum probability or maximum of a prioterior learning method using EM (29, see split parameters), it is less clear how to address constraints using the tensor decomposition approach. Some parameters are known: Gaussian with sparse covariance matrix, where we already know that some dimensions are uncorrelated; to solve a substitution using an HMM model, the transition matrix is a language model bound to given.parameters: transitions in an HMM could only depend on the relative difference between states when the transition matrix is arranged."}, {"heading": "D.2 Noise and statistical efficiency", "text": "It is also argued that moments of higher order are too noisy to be useful, but there are also more of them, and they contain more information about the model parameters as long as we can calculate how loud they are. We look at the problem of sloppiness and a weighting matrix W 0 \u2212 N modeling how much noise is present in each constraint function. This effect is relatively well known, and here is a very simple example that shows that many more noisy measurements can improve efficiency. Example D.3 (efficient estimation). Suppose that X \u2212 N value is present in each constraint function."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Mixture modeling is a general technique for making any simple model more expressive<lb>through weighted combination. This generality and simplicity in part explains the<lb>success of the Expectation Maximization (EM) algorithm, in which updates are easy<lb>to derive for a wide class of mixture models. However, the likelihood of a mixture<lb>model is non-convex, so EM has no known global convergence guarantees. Recently,<lb>method of moments approaches offer global guarantees for some mixture models, but<lb>they do not extend easily to the range of mixture models that exist. In this work, we<lb>present Polymom, an unifying framework based on method of moments in which es-<lb>timation procedures are easily derivable, just as in EM. Polymom is applicable when<lb>the moments of a single mixture component are polynomials of the parameters. Our<lb>key observation is that the moments of the mixture model are a mixture of these<lb>polynomials, which allows us to cast estimation as a Generalized Moment Problem.<lb>We solve its relaxations using semidefinite optimization, and then extract parame-<lb>ters using ideas from computer algebra. This framework allows us to draw insights<lb>and apply tools from convex optimization, computer algebra and the theory of mo-<lb>ments to study problems in statistical estimation. Simulations show good empirical<lb>performance on several models.", "creator": "LaTeX with hyperref package"}}}