{"id": "1603.08861", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "Revisiting Semi-Supervised Learning with Graph Embeddings", "abstract": "We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.", "histories": [["v1", "Tue, 29 Mar 2016 17:46:16 GMT  (534kb,D)", "http://arxiv.org/abs/1603.08861v1", null], ["v2", "Thu, 26 May 2016 23:57:09 GMT  (535kb,D)", "http://arxiv.org/abs/1603.08861v2", "ICML 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhilin yang", "william w cohen", "ruslan salakhutdinov"], "accepted": true, "id": "1603.08861"}, "pdf": {"name": "1603.08861.pdf", "metadata": {"source": "META", "title": "Revisiting Semi-Supervised Learning with Graph Embeddings", "authors": ["Zhilin Yang", "William Cohen", "Ruslan Salakhutdinov"], "emails": ["ZHILINY@CS.CMU.EDU", "WCOHEN@CS.CMU.EDU", "RSALAKHU@CS.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "2. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Semi-Supervised Learning", "text": "The main objective of semi-supervised learning is to use unmarked data to improve prediction performance. Let L and U be the number of marked and unmarked instances. Let x1: L and xL + 1: L + U denote the characteristic vectors of the marked and unmarked instances, respectively. The terms y1: L. The problem of semi-supervised learning is based on both marked and unmarked instances and is defined as learning a classifier f: x \u2192 y. There are two learning paradigms, transductive learning and inductive learning. Transductive learning (Zhu et al., 2003; Zhou et al., 2004) only aims to apply the classifier f to the unmarked instances observed during the training, and the classifier does not generalize to unobserved instances. In other words, transductive learning does not institute the labels of the marked instances + L."}, {"heading": "2.2. Graph-Based Semi-Supervised Learning", "text": "In fact, it is not as if it is a system in which it is a system. (...) It is not as if it is a system in which it is a system. (...) It is not as if it is a system in which it is a system. (...) It is not as if it is a system in which it is a system. (...) It is as if it is a system in which it is a system. (...) It is as if it is a system in which it is a system. (...) It is as if it is a system in which it is a system. (...) It is as if it is a system in which it is a system in which it is a system. (...) It is as if it is a system in which it is a system in which it is a system."}, {"heading": "2.3. Learning Embeddings", "text": "A cluster method (Handcock et al., 2007) has been proposed to learn latent social states in a social network in order to predict social times. More recently, a number of embedding methods are based on the Skipgram model, which is a variant of the Softmax model. In view of an instance and its context, Skipgram's goal is usually formulated in such a way that the protocol loss of predicting the context is minimized by embedding an instance as input characteristics. Formally, Skipgram's goal can usually be formulated in such a way that the protocol loss of predicting the context is used as input characteristics by embedding an instance. Formally, {(i, c)} is considered a set of instance i and context c, the loss function can be written as \u2212 determination (i, c) protocol of the protocol of the model's protocol."}, {"heading": "2.4. Comparison", "text": "We compare our approach in this paper with other methods of semi-supervised learning and embedding learning in Table 1. In contrast to our approach, conventional chart methods based on diagrams, including label propagation (Zhu et al., 2003), manifold regularization (Belkin et al., 2006) and MAD (Talukdar & Crammer, 2009), impose a regularization on the labels but do not learn embedding. Semi-supervised embedding methods (Weston et al., 2012) learn embedding in a neural network, but our approach differs from this method in that we do not impose embedding but predict the context in the diagram. Graph embedding methods (Perozzi et al., 2014; Tian et al., 2014) encode the graph structure in embedding, but unlike our approach, these embedding methods are not able to directly transmit this information for unspecified task."}, {"heading": "3. Semi-Supervised Learning with Graph Embeddings", "text": "According to the notations in the previous section, the input to our method contains labeled instances x1: L, y1: L, unlabeled instances xL + 1: L + U, and a graph called a matrix A. Each instance i has an embedding that is called ei.We formulate our framework based on forward-facing neural networks. Given the input feature vector x, the k-th hidden layer of the network is called hk, which is a nonlinear function of the previous hidden layer hk \u2212 1 defined as: hk (x) = ReLU (Wkhk \u2212 1 (x) + bk), where Wk and bk are parameters of the k-th layer, and h0 (x) = x. We take the reflected linear unit ReLU (x) = max (0, x) as a nonlinear function in this work. In the following sections, we first introduce how we formulate our learning context from the graph and then semiframe."}, {"heading": "3.1. Sampling Context", "text": "It is usually intractable to optimize Eq. (2) due to normalization over the entire context space. < (1) Negative sampling has been suggested to address this problem (Mikolovet al., 2013), with samples being negative examples to approximate the normalization term. However, in our case we will scan the sample (i, c, q) from a distribution in which i and c denote the instance and context respectively. (i, c) is a positive pair and g = \u2212 1 means negative. Given (i, c,) we minimize the loss of the classification of the pair (i, c) leading to a binary label. (I, c) Log value (wTc ei) \u2212 I (g = \u2212 1) Log value (\u2212 wTc ei), where we define the sigmoid function, which we define as a unimoid function (x), and I (\u00b7 e \u2212 x) is an indicator function that is true when the argument is 1."}, {"heading": "3.2. Transductive Formulation", "text": "In this section, we present a method that suggests labeling blank instances yL + 1: L + U without generalizing to unobserved instances. Transductive learning is generally more powerful than inductive learning, because transductive learning can use the unlabeled test data in model training (Joachims, 1999). We apply k layers to the input attribute vector x to apply hk (x) and l layers to embed e to obtain hl (e), as shown in Figure 2 (a). The two hidden layers are concatenated and fed to a softmax layer to predict the class name of the instance. Specifically, the probability of predicting the label y as: p (y | x, e) = exp [h) k (e) T] layer is written in which the first layer loss, hl (e) model T layer and the first layer of the transformation layer."}, {"heading": "3.3. Inductive Formulation", "text": "In many cases, it is desirable to learn a classifier that can generalize to unobserved cases, so we also consider an inductive learning paradigm. To make the method inductive, the prediction of the label y should depend only on the input note vector x. Therefore, we define embedding e as a parameterized function of feature x, as shown in Figure 2 (b). Similar to the transductive formulation, we apply k layers to the input note vector x to obtain hk (x). However, instead of using a \"free\" embedding, we apply l1 layers to the input note vector x and define it as embedding e = hl1 (x))). Then, additional l2 layers are applied to the embedding hl2 (e) = hl2 (x))) ED (hl1 (x)))), which can be regarded as a function (x), where this is only 1 (l)."}, {"heading": "3.4. Training", "text": "We use stochastic gradient descent (SGD) (Bottou, 2010) to train our model in mini-batch mode. For each iteration, we first try a batch of labeled instances and take a gradient step to optimize the loss function of the class labeling forecast. Then, we try a batch of context (i, c, \u03b3) and take another gradient step to optimize the loss function of the context prediction. Algorithm 2 illustrates the SGD-based training algorithm for transductive formulation. Similarly, we can replace p (yi | xi, ei) with p (yi | xi) in L1 to get the training algorithm for inductive formulation. Let's list all the model parameters."}, {"heading": "4. Experiments", "text": "We compare our approach with label propagation (LP) (Zhu et al., 2003), semi-supervised embedding (SemiEmb) (Weston et al., 2012), manifold regularization (ManiReg) (Belkin et al., 2006), TSVM (Joachims, 1999), and graph embeddings (GraphEmb et al., 2014). Another baseline method called a feature is a linear softmax model that takes only the feature vectors x as input. We also derive a variant of Planetoid-G that learns embedding to jointly predict class names and graph context without using feature vectors. Planetoid-G's architecture is similar to Figure 2 (a)."}, {"heading": "4.1. Text Classification", "text": "We have three text classification datasets4, including Citeseer, Cora and Pubmed (Sen et al., 2008).Each2https: / / github.com / phanein / deepwalk 3http: / / svmlight.joachims.org / 4http: / / linqs.umiacs.umd.edu / projects / lbc / dataset is that we put all documents and quotations into a class. We treat the bagof words as characteristics. We construct the diagrams based on the citation links; when we invent a document, we use aij = aJhar. The goal is to put each document into a class."}, {"heading": "4.2. Distantly-Supervised Entity Extraction", "text": "Next, we looked at the DIEL (Distant Information Extraction using coordinate-term Lists) datasets 5. The DIEL dataset contains pre-extracted characteristics for each entity mentioned in the text and a graph linking mentions of the entity to the coordinate lists (Bing et al., 2015). The goal is to extract medical entities from text-given feature vectors and the graph. We treat the top-k entities specified by a model as positive instances and calculate Recall @ k for evaluation (Bing et al., 2015), including data splitting of different runs, pre-processing of entity mentions and coordinate lists and the evaluation. We report on the average result of 10 runs in Table 4, where Feat refers to a result obtained by SVM (referred to as DS baseline in the DIEL paper).The result of LP was also taken from the paper SVEL-10 (where the SVM refers to the SVM result)."}, {"heading": "4.3. Entity Classification", "text": "We have linked to an entity classification of datasets from the knowledge base of Never Ending Language Learning (NELL) 6 (Carlson et al., 2010) and a hierarchical entity classification dataset7, the NELL entities to texts in ClueWeb09 (Dalvi & Cohen, 2016). We have the En-5http: / / www.cs.cmu.edu / \"lbing / data / emnlp-15-diel.tar.gz6http: / rtw.ml.l.cnlhsE entities / rsE-entitities-iiiiiiiiiiiic-niiihc-niiihc-niiihc-niihc-niihc-niihc-nihc-niihc-niihc-nihc-niihc-niiihc-nihc-niihc-nihc-niihc-nihc-niihc-niiihc-nihc-nihe-niihc-nihc-nihc-nihihc-niihc-nihc-nihihc-nihihe-nihc-nihc-nihihc-nihc-nihihihe-nihc-nihc-nihihc-nihihc-nihe-nihihe-nihihc-nihihc-nihc-nihc-nihu-nihihc-nihu-nihihu-nihc-nihihihc"}, {"heading": "5. Conclusion", "text": "In the transductive variant, we learn an embedding for each instance in order to jointly predict the class name and graph context. In the inductive variant, we learn a hidden layer as an embedding for predicting the graph context. Our experimental results on five benchmark data sets show that a) joint training brings an improvement over unattended learning; b) predicting the graph context is more effective than laplac regulation; c) the performance of the inductive variant depends on the informativeness of trait vectors. Planetoid significantly outperforms other methods considered so far in all data sets considered. One direction of future work would be to apply our framework to more complex networks, including recursive networks. It would also be interesting to experiment with data sets in which a graph is created on the basis of distances between feature vectors."}], "references": [{"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["Belkin", "Mikhail", "Niyogi", "Partha", "Sindhwani", "Vikas"], "venue": null, "citeRegEx": "Belkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2006}, {"title": "Improving distant supervision for information extraction using label propagation through lists", "author": ["Bing", "Lidong", "Chaudhari", "Sneha", "Wang", "Richard C", "Cohen", "William W"], "venue": "In EMNLP,", "citeRegEx": "Bing et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bing et al\\.", "year": 2015}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Garcia-Duran", "Alberto", "Weston", "Jason", "Yakhnenko", "Oksana"], "venue": "In NIPS,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["Bottou", "L\u00e9on"], "venue": "In COMPSTAT,", "citeRegEx": "Bottou and L\u00e9on.,? \\Q2010\\E", "shortCiteRegEx": "Bottou and L\u00e9on.", "year": 2010}, {"title": "Toward an architecture for never-ending language learning", "author": ["Carlson", "Andrew", "Betteridge", "Justin", "Kisiel", "Bryan", "Settles", "Burr", "Hruschka Jr.", "Estevam R", "Mitchell", "Tom M"], "venue": "In AAAI,", "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Hierarchical semisupervised classification with incomplete class hierarchies", "author": ["Dalvi", "Bhavana", "Cohen", "William W"], "venue": "In WSDM,", "citeRegEx": "Dalvi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dalvi et al\\.", "year": 2016}, {"title": "Model-based clustering for social networks", "author": ["Handcock", "Mark S", "Raftery", "Adrian E", "Tantrum", "Jeremy M"], "venue": "Journal of the Royal Statistical Society: Series A (Statistics in Society),", "citeRegEx": "Handcock et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Handcock et al\\.", "year": 2007}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang", "Zhiheng", "Xu", "Wei", "Yu", "Kai"], "venue": "arXiv preprint arXiv:1508.01991,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Graph regularized transductive classification on heterogeneous information networks", "author": ["Ji", "Ming", "Sun", "Yizhou", "Danilevsky", "Marina", "Han", "Jiawei", "Gao", "Jing"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Ji et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2010}, {"title": "Transductive inference for text classification using support vector machines", "author": ["Joachims", "Thorsten"], "venue": "In ICML,", "citeRegEx": "Joachims and Thorsten.,? \\Q1999\\E", "shortCiteRegEx": "Joachims and Thorsten.", "year": 1999}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": null, "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Deepwalk: Online learning of social representations", "author": ["Perozzi", "Bryan", "Al-Rfou", "Rami", "Skiena", "Steven"], "venue": "In KDD,", "citeRegEx": "Perozzi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Perozzi et al\\.", "year": 2014}, {"title": "Collective classification in network data", "author": ["Sen", "Prithviraj", "Namata", "Galileo", "Bilgic", "Mustafa", "Getoor", "Lise", "Galligher", "Brian", "Eliassi-Rad", "Tina"], "venue": "AI magazine,", "citeRegEx": "Sen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sen et al\\.", "year": 2008}, {"title": "Estimation and prediction for stochastic blockmodels for graphs with latent block structure", "author": ["Snijders", "Tom AB", "Nowicki", "Krzysztof"], "venue": "Journal of classification,", "citeRegEx": "Snijders et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Snijders et al\\.", "year": 1997}, {"title": "New regularized algorithms for transductive learning", "author": ["Talukdar", "Partha Pratim", "Crammer", "Koby"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Talukdar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Talukdar et al\\.", "year": 2009}, {"title": "Line: Large-scale information network embedding", "author": ["Tang", "Jian", "Qu", "Meng", "Wang", "Mingzhe", "Zhang", "Ming", "Yan", "Jun", "Mei", "Qiaozhu"], "venue": "In WWW, pp. 1067\u20131077. International World Wide Web Conferences Steering Committee,", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Learning deep representations for graph clustering", "author": ["Tian", "Fei", "Gao", "Bin", "Cui", "Qing", "Chen", "Enhong", "Liu", "Tie-Yan"], "venue": "In AAAI,", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "JMLR, 9(2579-2605):85,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Deep learning via semi-supervised embedding", "author": ["Weston", "Jason", "Ratle", "Fr\u00e9d\u00e9ric", "Mobahi", "Hossein", "Collobert", "Ronan"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Weston et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2012}, {"title": "Pidgin: ontology alignment using web text as interlingua", "author": ["Wijaya", "Derry", "Talukdar", "Partha Pratim", "Mitchell", "Tom"], "venue": "In CIKM,", "citeRegEx": "Wijaya et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wijaya et al\\.", "year": 2013}, {"title": "Learning with local and global consistency", "author": ["Zhou", "Dengyong", "Bousquet", "Olivier", "Lal", "Thomas Navin", "Weston", "Jason", "Sch\u00f6lkopf", "Bernhard"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2004}, {"title": "Revisiting Semi-Supervised Learning with Graph", "author": ["Zhu", "Xiaojin", "Ghahramani", "Zoubin", "Lafferty", "John"], "venue": "In ICML,", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 23, "context": "Graph-based semi-supervised learning defines the loss function as a weighted sum of the supervised loss over labeled instances and a graph Laplacian regularization term (Zhu et al., 2003; Zhou et al., 2004; Belkin et al., 2006; Weston et al., 2012).", "startOffset": 169, "endOffset": 248}, {"referenceID": 22, "context": "Graph-based semi-supervised learning defines the loss function as a weighted sum of the supervised loss over labeled instances and a graph Laplacian regularization term (Zhu et al., 2003; Zhou et al., 2004; Belkin et al., 2006; Weston et al., 2012).", "startOffset": 169, "endOffset": 248}, {"referenceID": 0, "context": "Graph-based semi-supervised learning defines the loss function as a weighted sum of the supervised loss over labeled instances and a graph Laplacian regularization term (Zhu et al., 2003; Zhou et al., 2004; Belkin et al., 2006; Weston et al., 2012).", "startOffset": 169, "endOffset": 248}, {"referenceID": 20, "context": "Graph-based semi-supervised learning defines the loss function as a weighted sum of the supervised loss over labeled instances and a graph Laplacian regularization term (Zhu et al., 2003; Zhou et al., 2004; Belkin et al., 2006; Weston et al., 2012).", "startOffset": 169, "endOffset": 248}, {"referenceID": 11, "context": "a word embedding might predict nearby context words (Mikolov et al., 2013; Pennington et al., 2014), or a node embedding might predict nearby nodes in a graph (Perozzi et al.", "startOffset": 52, "endOffset": 99}, {"referenceID": 12, "context": "a word embedding might predict nearby context words (Mikolov et al., 2013; Pennington et al., 2014), or a node embedding might predict nearby nodes in a graph (Perozzi et al.", "startOffset": 52, "endOffset": 99}, {"referenceID": 13, "context": ", 2014), or a node embedding might predict nearby nodes in a graph (Perozzi et al., 2014; Tang et al., 2015).", "startOffset": 67, "endOffset": 108}, {"referenceID": 17, "context": ", 2014), or a node embedding might predict nearby nodes in a graph (Perozzi et al., 2014; Tang et al., 2015).", "startOffset": 67, "endOffset": 108}, {"referenceID": 5, "context": "For example, word embeddings trained from a language model can be applied to partof-speech tagging, chunking and named entity recognition (Collobert et al., 2011; Huang et al., 2015).", "startOffset": 138, "endOffset": 182}, {"referenceID": 8, "context": "For example, word embeddings trained from a language model can be applied to partof-speech tagging, chunking and named entity recognition (Collobert et al., 2011; Huang et al., 2015).", "startOffset": 138, "endOffset": 182}, {"referenceID": 13, "context": "Existing results show that graph embeddings are effective at classifying the nodes in a graph, such as user behavior prediction in a social network (Perozzi et al., 2014; Tang et al., 2015).", "startOffset": 148, "endOffset": 189}, {"referenceID": 17, "context": "Existing results show that graph embeddings are effective at classifying the nodes in a graph, such as user behavior prediction in a social network (Perozzi et al., 2014; Tang et al., 2015).", "startOffset": 148, "endOffset": 189}, {"referenceID": 23, "context": "Transductive learning (Zhu et al., 2003; Zhou et al., 2004) only aims to apply the classifier f on the unlabeled instances observed at training time, and the classifier does not generalize to unobserved instances.", "startOffset": 22, "endOffset": 59}, {"referenceID": 22, "context": "Transductive learning (Zhu et al., 2003; Zhou et al., 2004) only aims to apply the classifier f on the unlabeled instances observed at training time, and the classifier does not generalize to unobserved instances.", "startOffset": 22, "endOffset": 59}, {"referenceID": 0, "context": "Inductive learning (Belkin et al., 2006; Weston et al., 2012), on the other hand, aims to learn a parameterized classifier f that is generalizable to unobserved instances.", "startOffset": 19, "endOffset": 61}, {"referenceID": 20, "context": "Inductive learning (Belkin et al., 2006; Weston et al., 2012), on the other hand, aims to learn a parameterized classifier f that is generalizable to unobserved instances.", "startOffset": 19, "endOffset": 61}, {"referenceID": 23, "context": "The graph A can either be derived from distances between instances (Zhu et al., 2003), or be explicitly derived from external data, such as a knowledge graph (Wijaya et al.", "startOffset": 67, "endOffset": 85}, {"referenceID": 21, "context": ", 2003), or be explicitly derived from external data, such as a knowledge graph (Wijaya et al., 2013) or a citation network between documents (Ji et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 9, "context": ", 2013) or a citation network between documents (Ji et al., 2010).", "startOffset": 48, "endOffset": 65}, {"referenceID": 23, "context": "Label propagation (Zhu et al., 2003) forces f to agree with labeled instances y1:L; f is a label lookup table for unlabeled instances in the graph, and can be obtained with a closedform solution.", "startOffset": 18, "endOffset": 36}, {"referenceID": 22, "context": "Learning with local and global consistency (Zhou et al., 2004) defines l as squared loss and f as a label lookup table; it does not force f to agree with labeled instances.", "startOffset": 43, "endOffset": 62}, {"referenceID": 0, "context": "Manifold regularization (Belkin et al., 2006) parameterizes f in the Reproducing Kernel Hilbert Space (RKHS) with l being squared loss or hinge loss.", "startOffset": 24, "endOffset": 45}, {"referenceID": 23, "context": "TSVM (Joachims, 1999) \u221a \u221a Transductive \u00d7 \u00d7 Label propagation (Zhu et al., 2003) \u00d7 \u221a Transductive \u00d7 Regularization Manifold Reg (Belkin et al.", "startOffset": 61, "endOffset": 79}, {"referenceID": 0, "context": ", 2003) \u00d7 \u221a Transductive \u00d7 Regularization Manifold Reg (Belkin et al., 2006) \u221a \u221a Inductive \u00d7 Regularization ICA (Sen et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 14, "context": ", 2006) \u221a \u221a Inductive \u00d7 Regularization ICA (Sen et al., 2008) \u00d7 \u221a Transductive \u00d7 Features MAD (Talukdar & Crammer, 2009) \u00d7 \u221a Transductive \u00d7 Regularization Semi Emb (Weston et al.", "startOffset": 43, "endOffset": 61}, {"referenceID": 20, "context": ", 2008) \u00d7 \u221a Transductive \u00d7 Features MAD (Talukdar & Crammer, 2009) \u00d7 \u221a Transductive \u00d7 Regularization Semi Emb (Weston et al., 2012) \u221a \u221a Inductive \u221a Regularization Graph Emb (Perozzi et al.", "startOffset": 110, "endOffset": 131}, {"referenceID": 13, "context": ", 2012) \u221a \u221a Inductive \u221a Regularization Graph Emb (Perozzi et al., 2014) \u00d7 \u00d7 Transductive \u221a Context Planetoid (this paper) \u221a \u221a Both \u221a Context", "startOffset": 49, "endOffset": 71}, {"referenceID": 20, "context": "Semi-supervised embedding (Weston et al., 2012) extends the regularization term in Eq.", "startOffset": 26, "endOffset": 47}, {"referenceID": 14, "context": "Iterative classification algorithm (ICA) (Sen et al., 2008) uses a local classifier that takes the labels of neighbor nodes as input; since not all neighbor labels are available at the beginning, ICA uses an iterative process between estimating the local classifier and assigning new labels.", "startOffset": 41, "endOffset": 59}, {"referenceID": 7, "context": "A clustering method (Handcock et al., 2007) was proposed to learn latent social states in a social network to predict social ties.", "startOffset": 20, "endOffset": 43}, {"referenceID": 11, "context": "Skipgram was first introduced to learn representations of words, known as word2vec (Mikolov et al., 2013).", "startOffset": 83, "endOffset": 105}, {"referenceID": 13, "context": "Deepwalk (Perozzi et al., 2014) uses the embedding of a node to predict the context in the graph, where the context is generated by random walk.", "startOffset": 9, "endOffset": 31}, {"referenceID": 17, "context": "LINE (Tang et al., 2015) extends the model to have multiple context spaces C for modeling both first and second order proximity.", "startOffset": 5, "endOffset": 24}, {"referenceID": 2, "context": "TransE (Bordes et al., 2013) learns the embeddings of entities in a knowledge graph jointly with their relations.", "startOffset": 7, "endOffset": 28}, {"referenceID": 18, "context": "Autoencoders were used to learn graph embeddings for clustering on graphs (Tian et al., 2014).", "startOffset": 74, "endOffset": 93}, {"referenceID": 23, "context": "Unlike our approach, conventional graph Laplacian based methods, including label propagation (Zhu et al., 2003), manifold regularization (Belkin et al.", "startOffset": 93, "endOffset": 111}, {"referenceID": 0, "context": ", 2003), manifold regularization (Belkin et al., 2006) and MAD (Talukdar & Crammer, 2009), impose regularization on the labels but do not learn embeddings.", "startOffset": 33, "endOffset": 54}, {"referenceID": 20, "context": "Semi-supervised embedding method (Weston et al., 2012) learns embeddings in a neural network, but our approach is different from this method in that instead of imposing regularization, we use the embeddings to predict the context in the graph.", "startOffset": 33, "endOffset": 54}, {"referenceID": 13, "context": "Graph embedding methods (Perozzi et al., 2014; Tian et al., 2014) encode the graph structure into embeddings; however, different from our approach, these meth-", "startOffset": 24, "endOffset": 65}, {"referenceID": 18, "context": "Graph embedding methods (Perozzi et al., 2014; Tian et al., 2014) encode the graph structure into embeddings; however, different from our approach, these meth-", "startOffset": 24, "endOffset": 65}, {"referenceID": 11, "context": "Negative sampling was proposed to address this issue (Mikolov et al., 2013), which samples negative examples to approximate the normalization term.", "startOffset": 53, "endOffset": 75}, {"referenceID": 13, "context": "Our random walk based sampling method is similar to Deepwalk (Perozzi et al., 2014).", "startOffset": 61, "endOffset": 83}, {"referenceID": 23, "context": "We compare our approach with label propagation (LP) (Zhu et al., 2003), semi-supervised embedding (SemiEmb) (Weston et al.", "startOffset": 52, "endOffset": 70}, {"referenceID": 20, "context": ", 2003), semi-supervised embedding (SemiEmb) (Weston et al., 2012), manifold regularization (ManiReg) (Belkin et al.", "startOffset": 45, "endOffset": 66}, {"referenceID": 0, "context": ", 2012), manifold regularization (ManiReg) (Belkin et al., 2006), TSVM (Joachims, 1999), and graph embeddings (GraphEmb) (Perozzi et al.", "startOffset": 43, "endOffset": 64}, {"referenceID": 13, "context": ", 2006), TSVM (Joachims, 1999), and graph embeddings (GraphEmb) (Perozzi et al., 2014).", "startOffset": 64, "endOffset": 86}, {"referenceID": 1, "context": "Results marked with \u2217 are taken from the original DIEL paper (Bing et al., 2015) with the same data splits.", "startOffset": 61, "endOffset": 80}, {"referenceID": 14, "context": "We first considered three text classification datasets4, including Citeseer, Cora and Pubmed (Sen et al., 2008).", "startOffset": 93, "endOffset": 111}, {"referenceID": 1, "context": "The DIEL dataset contains pre-extracted features for each entity mention in text, and a graph that connects entity mentions to coordinate lists (Bing et al., 2015).", "startOffset": 144, "endOffset": 163}, {"referenceID": 1, "context": "We follow the exact experimental setup as in the original DIEL paper (Bing et al., 2015), including data splits of different runs, preprocessing of entity mentions and coordinate lists, and evaluation.", "startOffset": 69, "endOffset": 88}, {"referenceID": 1, "context": "The result of LP was also taken from (Bing et al., 2015).", "startOffset": 37, "endOffset": 56}, {"referenceID": 4, "context": "We sorted out an entity classification dataset from the knowledge base of Never Ending Language Learning (NELL)6 (Carlson et al., 2010) and a hierarchical entity classification dataset7 that links NELL entities to text in ClueWeb09 (Dalvi & Cohen, 2016).", "startOffset": 113, "endOffset": 135}], "year": 2016, "abstractText": "We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.", "creator": "LaTeX with hyperref package"}}}