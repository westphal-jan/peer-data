{"id": "1603.08884", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "abstract": "Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging {\\it MCTest} benchmark. Partly because of its limited size, prior work on {\\it MCTest} has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for {\\it MCTest}, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15\\% absolute).", "histories": [["v1", "Tue, 29 Mar 2016 18:52:46 GMT  (58kb,D)", "http://arxiv.org/abs/1603.08884v1", "9 pages, submitted to ACL"]], "COMMENTS": "9 pages, submitted to ACL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["adam trischler", "zheng ye", "xingdi yuan", "jing he", "philip bachman"], "accepted": true, "id": "1603.08884"}, "pdf": {"name": "1603.08884.pdf", "metadata": {"source": "CRF", "title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "authors": ["Adam Trischler", "Zheng Ye jeff.ye", "Xingdi Yuan eric.yuan", "Jing He", "Phillip Bachman"], "emails": ["k.suleman@maluuba.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "2 The Problem", "text": "In this section, we borrow from Sachan et al. (2015), who beautifully outlined the MC problem. Machine understanding requires machines to answer questions based on unstructured text, which can be considered a selection of the best answer from a range of candidates. In multiple choice cases, candidate answers are predefined, but candidate answers can also be undefined and yet limited (e.g. to yes, no, or any noun phrase in the text) (Sachan et al. (2015). Let T for each question be the unstructured text and A = {ai} the set of candidate answers to q. The task of machine understanding is reduced to selecting the answer for which there is the most evidence T. As in Sachan et al. (2015), we combine an answer and a question into a hypothesis, hi = f (q, ai). To facilitate comparisons of the text with the hypotheses, we subdivide the passage into sentences tj, T = {tj} of our setting, we place one point each, and one in each of the characters q."}, {"heading": "3 Related Work", "text": "In this section, we will focus on the most powerful models that are applied specifically to MCTest, since they are somewhat unique among MC datasets (see Section 5). Generally, models can be divided into two categories: those that use fixed, constructed features, and neural models. Most of the work on MCTest falls into the former category. Manually constructed features often require considerable effort on the part of a designer and / or various auxiliary tools to extract them, and they cannot be modified by training. On the other hand, neural models can be trained consistently and typically use only one feature: vector representations of words. Text embedding is fed into a complex and possibly deep neural network where text is processed and compared for questions and answers. In deep models, mechanisms of attention and working memory are common, as in Weston et al. (2014) and Hermann et al. (2015)."}, {"heading": "3.1 Feature-engineering models", "text": "Sachan et al. (2015) treated MCTest as a structured prediction problem, looking for a latent answer that combines a structure that combines question, answer, and text, which corresponds to the best latent alignment of a hypothesis with corresponding text snippets. The process of (latent) selection of text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation (Bahdanau et al., 2014; Weston et al., 2014; Hill et al., 2015; Hermann et al., 2015). The model uses event and entity correlation linkages across sentences along with a variety of other features, including specifically trained word vectors for synonymy; antonymy and class integration relationships from external database sources; dependencies and semantic role designations. The model is expanded using a latent SVM model, so that top-classified questions are first pre-formed."}, {"heading": "3.2 Neural models", "text": "This is because the dataset is sparse and complex. Yin et al. (2016), at the same time as this work, examined in-depth, end-to-end recurring models with attention mechanisms; they also measured the performance of the Attentive Reader (Hermann et al., 2015) and the Neural Reasoner (Peng et al., 2015); and they developed an attention-based revolutionary network, the HABCNN. Their network operates on a hierarchy similar to our own and provides further evidence of the promise of hierarchical perspectives. Specifically, the HABCNN processes text at the sentence level and at the snippet level, where it combines adjacent sentences (as we do with an n-gram input); the embedding of vectors for the question and the answer candidates are combined and encoded by a Convolutionary Network. This encoding attracts attention via sentences and snippet encodings."}, {"heading": "4 The Parallel-Hierarchical Model", "text": "Let us now fully define our machine understanding model: We first describe each of the perspectives individually, and then describe how they are combined. In the following, we will use subscriptions to index sequence elements such as word vectors and up- or upscript to indicate whether elements are from the text, question, or answer. Specifically, we will use the subscriptions k, m, n, p to index sequences from text, question, answer, or hypothesis, or upscript t, q, a, h. We will show the model schematically in Figure 1."}, {"heading": "4.1 Semantic Perspective", "text": "The semantic perspective is similar to Memory Networks \"approach to embedding input in memory space (Weston et al., 2014). Each sentence of the text is a sequence of d-dimensional word vectors: tj = {tk}, tk = Rd. The semantic vector st is calculated by embedding the word vectors in a d-dimensional space using a two-layer network that implements a weighted sum, followed by an affine transformation and a nonlinearity; i.e. st = f (At k + btA). (1) The matrix At RD \u00b7 d, the bias vector btA RD, and for f we use the leaky ReLU function. The scalar vector is a feasible weight assigned to each word in the vocabulary."}, {"heading": "4.2 Word-by-Word Perspective", "text": "The first step in building the word-by-word perspective is to transform word vectors from a sentence, a question, and an answer by corresponding neural functions. For the text, we transform the question and answer to q-m (Bttk + btB), with Bt-RD-d, btB-RD, and f being the leaky ReLU again. We transform the question and answer to q-m and a conjunction analogously using different matrices and bias vectors. Unlike the semantic perspective, we separate the question and answer candidates in the word-by-word perspective. This is because word-by-word matches are inherently more important than word matches, and we want our model to learn to use this property."}, {"heading": "4.2.1 Sentential", "text": "Inspired by the work of Wang and Jiang (2015) on paraphrase recognition, we calculate similarities between hypotheses and sentences at the word level. This calculation uses the cosinal similarity as before: cqkm = cos (determining the text word that best matches each question word), (3) cakn = cos (determining the average agreement across the whole question). (4) The word-by-word match between a sentence and a question is determined by taking the maximum over k (determining the text word that best matches each question word) and then taking a weighted mean over m (determining the average agreement over the whole question): M q = 1Z \u00b2 m specmmax k cqkm. (5) In this case \u03c9m is the word weight for the question word and Z normalizes these weights to one over the question. We define the similarity between a sentence and the answer candidate's M3qm + the answer candidate's M3qm + finally we combine the correspondence according to the M3qm + the answer candidate's M1q."}, {"heading": "4.2.2 Sequential Sliding Window", "text": "The sequential sliding window is related to the original MCTest baseline by Richardson et al. (2013). Our sliding window decays from its focus word according to a Gaussian distribution, which we extend by assigning a traceable weight to each location in the window. This modification allows the window to use information about the distance between word matches; the original baseline uses distance information through a predefined function. The sliding window scans the words of the text as a continuous sequence, without sentence interruptions. Each window is treated like a sentence in the previous subsection, but we include a localized weight \u03bb (k), which is based on the position of a word in the window, which, when a window is given, is k from its global position. The cosmic similarity is assqkm = \u03bb (k) cos (getrtk, qm) cos (\u0441tk, qm), (7) for the question and analogous for the answer, which is initialized by its global position."}, {"heading": "4.2.3 Dependency Sliding Window", "text": "The dependency slider window works identically to the linear slider window, but from a different point of view of the text passage. The output of this component is M swd and is formed analogously to M swd. The dependency perspective uses the Mrs. Dependency Parser (Chen and Manning, 2014) as an auxiliary tool. Thus, the dependency graph can be regarded as a fixed feature. Furthermore, we do not distinguish the linearisation of the dependency graph because it is based on a property decomposition. However, we treat the linearisation in data preprocessing in such a way that the model only sees newly ordered word vector inputs. In particular, we run the Stanford Dependency Parser on each text set to build a dependency graph. This graph has nw vertices, one for each word in the sentence. From the dependency graph we form the Lapland matrix (Rnw \u00d7 nw) and determine its eigenvectors."}, {"heading": "4.3 Combining Distributed Evidence", "text": "MCTest is explicitly designed to ensure that it cannot be solved by lexical techniques alone, but requires some form of conclusion or limited thinking instead (Richardson et al., 2013). It therefore includes questions where the evidence for an answer encompasses several sentences. To perform a synthesis, our model also takes ngrams of sentences, i.e. pairs of sentences and triplets strung together, and treats them exactly like individual sentences and applies all the functions detailed above. A subsequent pooling operation combines values across all n-grams (including the input of a single sentence), which is described in the next subparagraph. 1We experimented with assigning unique marginal weights to unique relationship types in the dependency graph. However, this had a negligible effect. We assume that this is due to the fact that dependency graphs are trees without cycles."}, {"heading": "4.4 Combining Perspectives", "text": "We use a multi-layered perceptron to combine M sem, Mword, M swd and M sws as the final, matching value Mi for each respondent. This network also bundles and combines the individual n-gram values and uses a linear activation function. Our general training goal is to minimize the ranking losses L (T, q, A) = max (0, \u00b5 + max i Mi 6 = i * \u2212 Mi *), (10) where \u00b5 is a constant margin, i \u0445 indicates the right answer, and we take the maximum above i, so that we rank the correct answer over the best placed wrong answer (of which there are three) This approach worked better than comparing the correct answer to the wrong answers one at a time, as in Wang and McAllester (2015).Our implementation of the parallel hierarchical model using the Keras framework is available on Github.2."}, {"heading": "4.5 Training Wheels", "text": "Prior to the training, we initialized the neural network components of our model to perform meaningful heuristic functions, but the training did not approach the small MCTest without this crucial approach. Empirically, we found that we could achieve an accuracy of over 50% with the MCTest by using a simple sum of word vectors followed by a point product between the question sum and the hypothesis sum.2http: / / www.hiddenwebsite.com Therefore, we initialized the semantic perspective network to perform this sum by initializing Ax as identity matrix and bxA as zero vector, x, h. Remember that the activation function is aReLU, so positive results are invariable. We also found basic word matching values helpful, so that we initialized the word-for-word networks just as the network was initialized for perspective resistance combinations to perform a sum of single values using a zero-weight vector and a zero-vector."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 The Dataset", "text": "MCTest is a collection of 660 children's stories at elementary school level and the related questions written by human subjects. The stories are fictitious and ensure that the answer must be found in the text itself and is carefully limited to what a small child can understand (Richardson et al., 2013).The more sophisticated variant consists of 500 stories, each with four multiple choice questions. Despite the elementary level, stories and questions are more natural and complex than those found in synthetic MC datasets such as bAbI (Weston et al., 2014) and CNN (Hermann et al., 2015). MCTest is challenging because it is both complicated and small. According to Hill et al. (2015), \"it is very difficult to train statistical models solely on MCTest.\" Its size limits the number of parameters to be trained and prevents complex language modeling from being learned simultaneously with the ability to answer questions."}, {"heading": "5.2 Training and Model Details", "text": "In this section, we describe important details of the training process and model setup. For a complete list of hyperparameter settings, our stop word list, and other trifles, we refer interested readers to our Github repository. Wang vectors use Google's publicly available embedded word2vec on the 100 billion word message corpus (Mikolov et al., 2013). These vectors are fixed throughout the training, as we found that the training was unhelpful (probably because of the size of MCTest). The vectors are 300-dimensional (d = 300).We do not use a stop word list for the text passage, but rely on traceable word weights to assign global meaning ratings to words. These weights are initialized with the inverse document frequency (IDF)."}, {"heading": "5.3 Results", "text": "The first three lines represent feature-engineered methods. Richardson et al. (2013) + RTE is the most powerful variant of the original baseline published with MCTest. It uses a lexical sliding window and distance-based measurement, supplemented by rules for detecting textual entanglements. We described the methods of Sachan et al. (2015) and Wang and McAllester (2015) in Section 3. On the MCTest-500, the parallel hierarchical model clearly outperforms these methods in individual questions (> 2%) and easily outperforms the latter two methods in multiple questions. Wang and McAllester's method (2015) clearly outperforms the parallel hierarchical model in individual questions."}, {"heading": "6 Analysis and Discussion", "text": "We measure the contribution of each component of the model by mitigating it. Results are given in Table 2. Unsurprisingly, the n-gram functionality is important and contributes almost 5% to improving accuracy; without it, the model has almost nomeans for the synthesis of distributed proofs. However, the Top-N function contributes very little to the overall performance, suggesting that most multiple questions have distributed their proofs over contiguous sentences. On the other hand, the removal of the sentence component made the most significant difference by reducing the performance by more than 5%. Simple word-by-word matching is obviously useful for MCTest. The sequential sliding window contributes by highlighting the meaning of word-distance measurements. On the other hand, the dependency-based sliding window makes only a small contribution. It may be that the linearisation of the dependency curve is too far removed from its information."}, {"heading": "7 Conclusion", "text": "We have presented the novel parallel-hierarchical model for machine understanding and evaluated it with the small but complex MCTest. Our model achieves state-of-the-art results and surpasses several function-oriented and neural approaches. Working with our model has brought to our attention the following (not necessarily novel) concepts, which we record here to promote further empirical validation. \u2022 A good understanding of language is supported by hierarchical levels of understanding (cf. Hill et al. (2015)). \u2022 Exogenous attention (the learnable word weights) can be largely helpful for NLP. \u2022 The approach of the training wheels, i.e. the initialization of neural networks to perform meaningful heuristics, appears helpful for small datasets. \u2022 Thinking about language is challenging, but in some cases easy to simulate."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Factoid question answering over unstructured and structured web content", "author": ["Cucerzan", "Agichtein2005] Silviu Cucerzan", "Eugene Agichtein"], "venue": "In TREC,", "citeRegEx": "Cucerzan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cucerzan et al\\.", "year": 2005}, {"title": "Deep residual learning for image recognition", "author": ["He et al.2015] Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations. arXiv preprint arXiv:1511.02301", "author": ["Hill et al.2015] Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar et al.2015] Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "A simple way to initialize recurrent networks of rectified linear units. arXiv preprint arXiv:1504.00941", "author": ["Le et al.2015] Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Neural networks underlying endogenous and exogenous visual\u2013spatial orienting", "author": ["Mayer et al.2004] Andrew R Mayer", "Jill M Dorflinger", "Stephen M Rao", "Michael Seidenberg"], "venue": null, "citeRegEx": "Mayer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mayer et al\\.", "year": 2004}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Machine comprehension with discourse relations", "author": ["Narasimhan", "Barzilay2015] Karthik Narasimhan", "Regina Barzilay"], "venue": "In 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Towards neural network-based reasoning", "author": ["Peng et al.2015] Baolin Peng", "Zhengdong Lu", "Hang Li", "Kam-Fai Wong"], "venue": "arXiv preprint arXiv:1508.05508", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Christopher JC Burges", "Erin Renshaw"], "venue": "In EMNLP,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Learning answerentailing structures for machine comprehension", "author": ["Avinava Dubey", "Eric P Xing", "Matthew Richardson"], "venue": "In Proceedings of ACL", "citeRegEx": "Sachan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sachan et al\\.", "year": 2015}, {"title": "A strong lexical matching method for the machine comprehension test", "author": ["Smith et al.2015] Ellery Smith", "Nicola Greco", "Matko Bosnjak", "Andreas Vlachos"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Smith et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Learning natural language inference with lstm", "author": ["Wang", "Jiang2015] Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1512.08849", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Machine comprehension with syntax, frames, and semantics", "author": ["Wang", "McAllester2015] Hai Wang"], "venue": "Volume 2: Short Papers,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Attention-based convolutional neural network for machine comprehension", "author": ["Yin et al.2016] Wenpeng Yin", "Sebastian Ebert", "Hinrich Sch\u00fctze"], "venue": "arXiv preprint arXiv:1602.04341", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Such tests are objectively gradable and can be used to assess a range of abilities, from basic understanding to causal reasoning to inference (Richardson et al., 2013).", "startOffset": 142, "endOffset": 167}, {"referenceID": 14, "context": "Given a text passage and a question about its content, a system is tested on its ability to determine the correct answer (Sachan et al., 2015).", "startOffset": 121, "endOffset": 142}, {"referenceID": 13, "context": "In this work, we focus on MCTest, a complex but data-limited comprehension benchmark, whose multiple-choice questions require not only extraction but also inference and limited reasoning (Richardson et al., 2013).", "startOffset": 187, "endOffset": 212}, {"referenceID": 10, "context": "Words are represented throughout by embedding vectors (Mikolov et al., 2013).", "startOffset": 54, "endOffset": 76}, {"referenceID": 5, "context": "We also use a sliding window acting on a subsentential scale (inspired by the work of Hill et al. (2015)), which implicitly considers the linear distance between matched words.", "startOffset": 86, "endOffset": 105}, {"referenceID": 17, "context": "Computational models that comprehend (insofar as they perform well on MC datasets) have developed contemporaneously in several research groups (Weston et al., 2014; Sukhbaatar et al., 2015; Hill et al., 2015; Hermann et al., 2015; Kumar et al., 2015).", "startOffset": 143, "endOffset": 250}, {"referenceID": 5, "context": "Computational models that comprehend (insofar as they perform well on MC datasets) have developed contemporaneously in several research groups (Weston et al., 2014; Sukhbaatar et al., 2015; Hill et al., 2015; Hermann et al., 2015; Kumar et al., 2015).", "startOffset": 143, "endOffset": 250}, {"referenceID": 4, "context": "Computational models that comprehend (insofar as they perform well on MC datasets) have developed contemporaneously in several research groups (Weston et al., 2014; Sukhbaatar et al., 2015; Hill et al., 2015; Hermann et al., 2015; Kumar et al., 2015).", "startOffset": 143, "endOffset": 250}, {"referenceID": 7, "context": "Computational models that comprehend (insofar as they perform well on MC datasets) have developed contemporaneously in several research groups (Weston et al., 2014; Sukhbaatar et al., 2015; Hill et al., 2015; Hermann et al., 2015; Kumar et al., 2015).", "startOffset": 143, "endOffset": 250}, {"referenceID": 4, "context": ", 2015; Hermann et al., 2015; Kumar et al., 2015). Models designed specifically for MCTest include those of Richardson et al. (2013), and more recently Sachan et al.", "startOffset": 8, "endOffset": 133}, {"referenceID": 4, "context": ", 2015; Hermann et al., 2015; Kumar et al., 2015). Models designed specifically for MCTest include those of Richardson et al. (2013), and more recently Sachan et al. (2015), Wang and McAllester (2015), and Yin et al.", "startOffset": 8, "endOffset": 173}, {"referenceID": 4, "context": ", 2015; Hermann et al., 2015; Kumar et al., 2015). Models designed specifically for MCTest include those of Richardson et al. (2013), and more recently Sachan et al. (2015), Wang and McAllester (2015), and Yin et al.", "startOffset": 8, "endOffset": 201}, {"referenceID": 4, "context": ", 2015; Hermann et al., 2015; Kumar et al., 2015). Models designed specifically for MCTest include those of Richardson et al. (2013), and more recently Sachan et al. (2015), Wang and McAllester (2015), and Yin et al. (2016). In experiments, our Parallel-Hierarchical model achieves state-of-the-art accuracy on MCTest, outperforming these existing methods.", "startOffset": 8, "endOffset": 224}, {"referenceID": 14, "context": ", to yes, no, or any noun phrase in the text) (Sachan et al., 2015).", "startOffset": 46, "endOffset": 67}, {"referenceID": 14, "context": "In this section we borrow from Sachan et al. (2015), who laid out the MC problem nicely.", "startOffset": 31, "endOffset": 52}, {"referenceID": 14, "context": "As in Sachan et al. (2015), we combine an answer and a question into a hypothesis, hi =", "startOffset": 6, "endOffset": 27}, {"referenceID": 4, "context": "(2014) and Hermann et al. (2015).", "startOffset": 11, "endOffset": 33}, {"referenceID": 0, "context": "The process of (latently) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation (Bahdanau et al., 2014; Weston et al., 2014; Hill et al., 2015; Hermann et al., 2015).", "startOffset": 161, "endOffset": 246}, {"referenceID": 5, "context": "The process of (latently) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation (Bahdanau et al., 2014; Weston et al., 2014; Hill et al., 2015; Hermann et al., 2015).", "startOffset": 161, "endOffset": 246}, {"referenceID": 4, "context": "The process of (latently) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation (Bahdanau et al., 2014; Weston et al., 2014; Hill et al., 2015; Hermann et al., 2015).", "startOffset": 161, "endOffset": 246}, {"referenceID": 0, "context": "The process of (latently) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation (Bahdanau et al., 2014; Weston et al., 2014; Hill et al., 2015; Hermann et al., 2015). The model uses event and entity coreference links across sentences along with a host of other features. These include specifically trained word vectors for synonymy; antonymy and class-inclusion relations from external database sources; dependencies and semantic role labels. The model is trained using a latent structural SVM extended to a multitask setting, so that questions are first classified using a pretrained top-level classifier. This enables the system to use different processing strategies for different question categories. The model also combines question and answer into a well-formed statement using the rules of Cucerzan and Agichtein (2005).", "startOffset": 162, "endOffset": 908}, {"referenceID": 14, "context": "Our model is simpler than that of Sachan et al. (2015) in terms of the features it takes in, the training procedure (stochastic gradient descent vs.", "startOffset": 34, "endOffset": 55}, {"referenceID": 13, "context": "Wang and McAllester (2015) augmented the baseline feature set from Richardson et al. (2013) with features for syntax, frame semantics, coreference chains, and word embeddings.", "startOffset": 67, "endOffset": 92}, {"referenceID": 13, "context": "Wang and McAllester (2015) augmented the baseline feature set from Richardson et al. (2013) with features for syntax, frame semantics, coreference chains, and word embeddings. They combined features using a linear latent-variable classifier trained to minimize a max-margin loss function. As in Sachan et al. (2015), questions and answers are combined using a set of manually written rules.", "startOffset": 67, "endOffset": 316}, {"referenceID": 13, "context": "Wang and McAllester (2015) augmented the baseline feature set from Richardson et al. (2013) with features for syntax, frame semantics, coreference chains, and word embeddings. They combined features using a linear latent-variable classifier trained to minimize a max-margin loss function. As in Sachan et al. (2015), questions and answers are combined using a set of manually written rules. The method of Wang and McAllester (2015) achieved the previous state of the art, but has significant complexity in terms of the feature set.", "startOffset": 67, "endOffset": 432}, {"referenceID": 15, "context": "Space does not permit a full description of all models in this category, but see also Smith et al. (2015) and Narasimhan and Barzilay (2015).", "startOffset": 86, "endOffset": 106}, {"referenceID": 15, "context": "Space does not permit a full description of all models in this category, but see also Smith et al. (2015) and Narasimhan and Barzilay (2015).", "startOffset": 86, "endOffset": 141}, {"referenceID": 4, "context": "They measured the performance of the Attentive Reader (Hermann et al., 2015) and the Neural Reasoner (Peng et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 12, "context": ", 2015) and the Neural Reasoner (Peng et al., 2015), both deep, end-to-end recurrent models with attention mechanisms, and also developed an attention-based convolutional network, the HABCNN.", "startOffset": 32, "endOffset": 51}, {"referenceID": 20, "context": "Other neural models tested in Yin et al. (2016) fare even worse.", "startOffset": 30, "endOffset": 48}, {"referenceID": 9, "context": "These scalar weights implement a kind of exogenous or bottomup attention that depends only on the input stimulus (Mayer et al., 2004).", "startOffset": 113, "endOffset": 133}, {"referenceID": 13, "context": "The sequential sliding window is related to the original MCTest baseline by Richardson et al. (2013). Our sliding window decays from its focus word according to a Gaussian distribution, which we extend by assigning a trainable weight to each location in the window.", "startOffset": 76, "endOffset": 101}, {"referenceID": 13, "context": "MCTest was explicitly designed to ensure that it could not be solved by lexical techniques alone, but would instead require some form of inference or limited reasoning (Richardson et al., 2013).", "startOffset": 168, "endOffset": 193}, {"referenceID": 3, "context": "In residual networks (He et al., 2015), shortcut connections bypass certain layers in the network so that a simpler function can be trained in conjunction with the full model.", "startOffset": 21, "endOffset": 38}, {"referenceID": 7, "context": "For instance, Le et al. (2015) proposed the identity-matrix initialization in the context of recurrent neural networks in order to preserve the error signal through backpropagation.", "startOffset": 14, "endOffset": 31}, {"referenceID": 13, "context": "The stories are fictional, ensuring that the answer must be found in the text itself, and carefully limited to what a young child can understand (Richardson et al., 2013).", "startOffset": 145, "endOffset": 170}, {"referenceID": 4, "context": ", 2014) and CNN (Hermann et al., 2015).", "startOffset": 16, "endOffset": 38}, {"referenceID": 5, "context": "As per Hill et al. (2015), \u201cit is very difficult to train statistical models only on MCTest.", "startOffset": 7, "endOffset": 26}, {"referenceID": 10, "context": "For word vectors we use Google\u2019s publicly available embeddings, trained with word2vec on the 100-billion-word News corpus (Mikolov et al., 2013).", "startOffset": 122, "endOffset": 144}, {"referenceID": 14, "context": "Following earlier methods, we use a heuristic to improve performance on negation questions (Sachan et al., 2015; Wang and McAllester, 2015).", "startOffset": 91, "endOffset": 139}, {"referenceID": 16, "context": "We found dropout (Srivastava et al., 2014) to be particularly effective at improving generalization from the training to the test set, and used 0.", "startOffset": 17, "endOffset": 42}, {"referenceID": 13, "context": "Richardson et al. (2013) + RTE is the best-performing variant of the original baseline published along with MCTest.", "startOffset": 0, "endOffset": 25}, {"referenceID": 13, "context": "Richardson et al. (2013) + RTE is the best-performing variant of the original baseline published along with MCTest. It uses a lexical sliding window and distance-based measure, augmented with rules for recognizing textual entailment. We described the methods of Sachan et al. (2015) and Wang and McAllester (2015) in Section 3.", "startOffset": 0, "endOffset": 283}, {"referenceID": 13, "context": "Richardson et al. (2013) + RTE is the best-performing variant of the original baseline published along with MCTest. It uses a lexical sliding window and distance-based measure, augmented with rules for recognizing textual entailment. We described the methods of Sachan et al. (2015) and Wang and McAllester (2015) in Section 3.", "startOffset": 0, "endOffset": 314}, {"referenceID": 13, "context": "Richardson et al. (2013) + RTE is the best-performing variant of the original baseline published along with MCTest. It uses a lexical sliding window and distance-based measure, augmented with rules for recognizing textual entailment. We described the methods of Sachan et al. (2015) and Wang and McAllester (2015) in Section 3. On MCTest-500, the Parallel Hierarchical model significantly outperforms these methods on single questions (> 2%) and slightly outperforms the latter two on multi questions (\u2248 0.3%) and overall (\u2248 1%). The method of Wang and McAllester (2015) achieves the best overall result on MCTest-160.", "startOffset": 0, "endOffset": 571}, {"referenceID": 20, "context": "Performance measures are taken from Yin et al. (2016). Here we see our model outperforming the alternatives by a large margin across the board (> 15%).", "startOffset": 36, "endOffset": 54}, {"referenceID": 5, "context": "Hill et al. (2015)).", "startOffset": 0, "endOffset": 19}], "year": 2016, "abstractText": "Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging MCTest benchmark. Partly because of its limited size, prior work on MCTest has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for MCTest, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15% absolute).", "creator": "LaTeX with hyperref package"}}}