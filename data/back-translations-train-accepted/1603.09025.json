{"id": "1603.09025", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Recurrent Batch Normalization", "abstract": "We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps.", "histories": [["v1", "Wed, 30 Mar 2016 02:57:20 GMT  (223kb,D)", "http://arxiv.org/abs/1603.09025v1", null], ["v2", "Thu, 31 Mar 2016 20:45:03 GMT  (222kb,D)", "http://arxiv.org/abs/1603.09025v2", null], ["v3", "Mon, 4 Apr 2016 17:16:08 GMT  (222kb,D)", "http://arxiv.org/abs/1603.09025v3", null], ["v4", "Tue, 12 Apr 2016 19:23:44 GMT  (222kb,D)", "http://arxiv.org/abs/1603.09025v4", null], ["v5", "Tue, 28 Feb 2017 00:59:42 GMT  (292kb,D)", "http://arxiv.org/abs/1603.09025v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tim cooijmans", "nicolas ballas", "c\\'esar laurent", "\\c{c}a\\u{g}lar g\\\"ul\\c{c}ehre", "aaron courville"], "accepted": true, "id": "1603.09025"}, "pdf": {"name": "1603.09025.pdf", "metadata": {"source": "CRF", "title": "Recurrent Batch Normalization", "authors": ["Tim Cooijmans", "Nicolas Ballas", "C\u00e9sar Laurent", "Aaron Courville"], "emails": ["firstname.lastname@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "Recently, it has been shown that a large number of complex, successive problems, including speech recognition, are indeed of very high importance. However, it is known that these are very powerful networks, which are very capable of impairing the efficiency of training and training."}, {"heading": "2 Prerequisites", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 LSTM", "text": "In the eaeLnn used in the sequence modeling, Wx-Rdx-dh, b-Rdh and the initial state h0-Rdh are model parameters. (Whht \u2212 1 + Wxxt + b) A popular choice for the activation function is tanh.RNs In the sequence modeling it is the natural ability to process variable length sequences. (Wx-Rdh) b-Rdh and the initial state h0-Rdh are model parameters. (Wht \u2212 1 + Wxxxt + b) The nlrdhsceaeS-Rdh is the first order stochastic gradients (SGD)."}, {"heading": "2.2 Batch Normalization", "text": "Covariate shift [23] is a phenomenon in machine learning where the characteristics presented to a model change in distribution during training have to be adapted in order to be successful in the presence of a covariance shift. In deep neural networks, this problem manifests itself as an internal covariance shift [11], where changing the parameters of a layer affects the distribution of inputs to all layers above it. Batch normalization [11] is a recently proposed network repair that aims to reduce internal covariance shift by standardizing activations using statistical estimates of their means and standard deviations. However, it does not decorrelate activations because the involved matrix inversion would be too expensive. Batch normalization of transformation is as follows: BN (h; g; \u03b2) = \u03b2 \u2212 E (h)."}, {"heading": "3 Batch-Normalized LSTM", "text": "In fact, it is that we are able to hide, and that we are able, we will be able, we will be able, we will be able."}, {"heading": "4 Initializing \u03b3 for Gradient Flow", "text": "Although batch normalization allows easy control of the preactivation variance by the \u03b3 parameters, the usual practice is to normalize the variance of the units. We suspect that the difficulties encountered so far with the relapse batch normalization reported in the literature [14, 1] are largely due to improper initialization of the batch normalization parameters, especially \u03b3. In this section, we show the effects of \u03b3 on the gradient flow. In Figure 2 (a), we show how the pre-activation variance affects the gradient propagation in a simple RNN on the sequential MNIST task described in Section 5.1. Since the backpropagation is done in the opposite direction, the gradient propagation is best read from right to left. The quantity plotted is the norm for the gradient of the loss in relation to the hidden state in various time steps."}, {"heading": "5 Experiments", "text": "This section presents an empirical evaluation of the proposed batch-normalized LSTM for four different tasks. The results show that batch-normalized LSTM leads to a faster convergence for all tasks than a baseline-LSTM and can also lead to a better generalization. Note that in all experiments we initialize the batch normalization scale and shift the parameters \u03b3 and \u03b2 to 0.1 and 0 respectively."}, {"heading": "5.1 Sequential MNIST", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "5.2 Character-level Penn Treebank", "text": "We evaluate our model based on the task of character-level speech modeling based on the Penn Treebank corpus [17] according to the traction / validity / test partition of [19]. For training, we segment the training sequence into examples of length 100. The training sequence does not divide cleanly by 100, so for each epoch we randomly cut a sub-sequence that does this and segments it. Our starting point is a 1000-unit LSTM, trained to predict the next character using a Softmax classifier on the hidden state ht. We use stochastic gradient lineage on size 64 minibatches, with gradient clipping at 1.0 and a step rule determined by Adam [12] with a learning rate of 0.002. We use orthogonal initialization for all weight matrices. The construction for the batch-normalized LSTM model is identical in all respects, except for the introduction of batch normalization, as shown in Figure 4 (a)."}, {"heading": "5.3 Text8", "text": "We evaluate our model based on a second task for modeling character-level language based on the Text8 dataset [16]. This dataset is derived from Wikipedia and consists of a sequence of 100M characters, including alphabetic characters and spaces only. We follow previous authors [19, 28] and use the first 90M characters for training, the next 5M for validation, and the last 5M characters for testing. We train on non-overlapping sequences of length 180. Our model is a BN-LSTM with 2000 units, trained to predict the next character with a Softmax classifier on the hidden state. We use stochastic gradient descent on size 64 minibatches, with a gradient section of 1.0 and a step rule determined by Adam [12] with a learning rate of 0.01. All weight matrices have been initialized to be orthogonal. We provide the validation performance of the modelling report early on the results of the 3."}, {"heading": "5.4 Teaching Machines to Read and Comprehend", "text": "To demonstrate the universality and practicality of our re-parameterization, we are exploring the use of batch-normalized LSTM in a unidirectional Attentive Reader Model [8].2 We are evaluating two variants. The first variant, referred to as BN-LSTM, consists of the vanilla Attentive Reader Model with the LSTM simply replaced by our BN-LSTM re-parameterization. We are not comparing them to [7] how they evaluate in another environment. 2We are using the existing implementation at https: / github.com / caglar _ reader.batch Normalization in the attention calculations, how they look, how they are able to use the existing implementations."}, {"heading": "6 Conclusion", "text": "Contrary to previous findings [14, 1], we have shown that batch normalization of the hidden states of recurrent neural networks significantly improves optimization. In fact, this brings similar benefits to batch normalization in feedback-forward neural networks: Our proposed BN-LSTM trains faster and generalizes better on a variety of tasks, including speech modeling and questioning. We have argued that proper initialization of batch normalization parameters is critical and suggest that previous difficulties [14, 1] are largely due to improper initialization."}, {"heading": "Acknowledgements", "text": "The authors would like to acknowledge the support of the following research funding and computer support agencies: NSERC, Calcul Que'bec, Compute Canada, the Canada Research Chairs and CIFAR. We would also like to thank the developers of Theano [4] and the libraries Blocks and Fuel [25] for developing such powerful tools for the scientific calculator. We would like to thank C-ag-lar Gu'lc-hehr for sharing its implementation of Attentive Reader and for helping with experiments, as well as David Krueger, Saizheng Zhang, Ishmael Belghazi and Yoshua Bengio for discussions and suggestions."}], "references": [{"title": "Deep speech 2: Endto-end speech recognition in english and mandarin", "author": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos"], "venue": "arXiv preprint arXiv:1512.02595,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Unitary evolution recurrent neural networks", "author": ["Martin Arjovsky", "Amar Shah", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1511.06464,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["Sepp Hochreiter"], "venue": "Master\u2019s thesis, Institut fur Informatik, Technische Universitat,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "shift. CoRR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Regularizing rnns by stabilizing activations", "author": ["David Krueger", "Roland Memisevic"], "venue": "arXiv preprint arXiv:1511.08400,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Batch normalized recurrent neural networks", "author": ["C\u00e9sar Laurent", "Gabriel Pereyra", "Phil\u00e9mon Brakel", "Ying Zhang", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1510.01378,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Large text compression benchmark", "author": ["Matt Mahoney"], "venue": "URL: http://www. mattmahoney. net/text/text. html,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Comput. Linguist.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1993}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML-", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Subword language modeling with neural networks. preprint (http://www", "author": ["Tom\u00e1\u0161 Mikolov", "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink", "J Cernocky"], "venue": "fit. vutbr. cz/imikolov/rnnlm/char. pdf),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Persistent contextual neural networks for learning symbolic data sequences", "author": ["Yann Ollivier"], "venue": "CoRR, abs/1306.0514,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Regularization and nonlinearities for neural language models: when are they needed", "author": ["Marius Pachitariu", "Maneesh Sahani"], "venue": "arXiv preprint arXiv:1301.5650,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["Hidetoshi Shimodaira"], "venue": "Journal of statistical planning and inference,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Architectural complexity measures of recurrent neural networks", "author": ["Saizheng Zhang", "Yuhuai Wu", "Tong Che", "Zhouhan Lin", "Roland Memisevic", "Ruslan Salakhutdinov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.08210,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Recurrent neural network architectures such as LSTM [10] and GRU [6] have recently exhibited state-of-the-art performance on a wide range of complex sequential problems including speech recognition [1], machine translation [3] and image and video captioning [26, 27].", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "Recurrent neural network architectures such as LSTM [10] and GRU [6] have recently exhibited state-of-the-art performance on a wide range of complex sequential problems including speech recognition [1], machine translation [3] and image and video captioning [26, 27].", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "Recurrent neural network architectures such as LSTM [10] and GRU [6] have recently exhibited state-of-the-art performance on a wide range of complex sequential problems including speech recognition [1], machine translation [3] and image and video captioning [26, 27].", "startOffset": 198, "endOffset": 201}, {"referenceID": 2, "context": "Recurrent neural network architectures such as LSTM [10] and GRU [6] have recently exhibited state-of-the-art performance on a wide range of complex sequential problems including speech recognition [1], machine translation [3] and image and video captioning [26, 27].", "startOffset": 223, "endOffset": 226}, {"referenceID": 24, "context": "Recurrent neural network architectures such as LSTM [10] and GRU [6] have recently exhibited state-of-the-art performance on a wide range of complex sequential problems including speech recognition [1], machine translation [3] and image and video captioning [26, 27].", "startOffset": 258, "endOffset": 266}, {"referenceID": 25, "context": "Recurrent neural network architectures such as LSTM [10] and GRU [6] have recently exhibited state-of-the-art performance on a wide range of complex sequential problems including speech recognition [1], machine translation [3] and image and video captioning [26, 27].", "startOffset": 258, "endOffset": 266}, {"referenceID": 21, "context": "Effective optimization of recurrent neural networks is an active area of study [22, 18, 20].", "startOffset": 79, "endOffset": 91}, {"referenceID": 17, "context": "Effective optimization of recurrent neural networks is an active area of study [22, 18, 20].", "startOffset": 79, "endOffset": 91}, {"referenceID": 19, "context": "Effective optimization of recurrent neural networks is an active area of study [22, 18, 20].", "startOffset": 79, "endOffset": 91}, {"referenceID": 22, "context": "It is known that for deep feed-forward neural networks, covariate shift [23, 11] degrades the efficiency of training.", "startOffset": 72, "endOffset": 80}, {"referenceID": 10, "context": "It is known that for deep feed-forward neural networks, covariate shift [23, 11] degrades the efficiency of training.", "startOffset": 72, "endOffset": 80}, {"referenceID": 10, "context": "This internal covariate shift [11] may play an especially important role in recurrent neural networks, which resemble very deep feed-forward networks.", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "Batch normalization [11] is a recently proposed technique for controlling the distributions of feedforward neural network activations, thereby reducing internal covariate shift.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "Although batch normalization has demonstrated significant training speed-ups and generalization benefits in feedforward networks, it has proven difficult to apply in recurrent architectures [14, 1].", "startOffset": 190, "endOffset": 197}, {"referenceID": 0, "context": "Although batch normalization has demonstrated significant training speed-ups and generalization benefits in feedforward networks, it has proven difficult to apply in recurrent architectures [14, 1].", "startOffset": 190, "endOffset": 197}, {"referenceID": 13, "context": "However, it has been hypothesized [14] that applying batch normalization in this way hurts training because of exploding gradients due to repeated rescaling.", "startOffset": 34, "endOffset": 38}, {"referenceID": 14, "context": "In particular, we achieve state-of-the-art performance on Sequential MNIST [15] and character-level language modeling [19] on Penn Treebank [17] and text8 [16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "In particular, we achieve state-of-the-art performance on Sequential MNIST [15] and character-level language modeling [19] on Penn Treebank [17] and text8 [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 16, "context": "In particular, we achieve state-of-the-art performance on Sequential MNIST [15] and character-level language modeling [19] on Penn Treebank [17] and text8 [16].", "startOffset": 140, "endOffset": 144}, {"referenceID": 15, "context": "In particular, we achieve state-of-the-art performance on Sequential MNIST [15] and character-level language modeling [19] on Penn Treebank [17] and text8 [16].", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "Training RNNs using first-order stochastic gradient descent (SGD) however is notoriously difficult due to the well-known problem of exploding/vanishing gradients [5, 9, 22].", "startOffset": 162, "endOffset": 172}, {"referenceID": 8, "context": "Training RNNs using first-order stochastic gradient descent (SGD) however is notoriously difficult due to the well-known problem of exploding/vanishing gradients [5, 9, 22].", "startOffset": 162, "endOffset": 172}, {"referenceID": 21, "context": "Training RNNs using first-order stochastic gradient descent (SGD) however is notoriously difficult due to the well-known problem of exploding/vanishing gradients [5, 9, 22].", "startOffset": 162, "endOffset": 172}, {"referenceID": 4, "context": "While the long-term dependencies problem is unsolvable in absolute [5], it has been demonstrated that different RNN re-parameterizations, such as the LSTM [10], GRU [6] or i/u-RNN [15, 2] can help mitigate the vanishing gradient problem.", "startOffset": 67, "endOffset": 70}, {"referenceID": 9, "context": "While the long-term dependencies problem is unsolvable in absolute [5], it has been demonstrated that different RNN re-parameterizations, such as the LSTM [10], GRU [6] or i/u-RNN [15, 2] can help mitigate the vanishing gradient problem.", "startOffset": 155, "endOffset": 159}, {"referenceID": 5, "context": "While the long-term dependencies problem is unsolvable in absolute [5], it has been demonstrated that different RNN re-parameterizations, such as the LSTM [10], GRU [6] or i/u-RNN [15, 2] can help mitigate the vanishing gradient problem.", "startOffset": 165, "endOffset": 168}, {"referenceID": 14, "context": "While the long-term dependencies problem is unsolvable in absolute [5], it has been demonstrated that different RNN re-parameterizations, such as the LSTM [10], GRU [6] or i/u-RNN [15, 2] can help mitigate the vanishing gradient problem.", "startOffset": 180, "endOffset": 187}, {"referenceID": 1, "context": "While the long-term dependencies problem is unsolvable in absolute [5], it has been demonstrated that different RNN re-parameterizations, such as the LSTM [10], GRU [6] or i/u-RNN [15, 2] can help mitigate the vanishing gradient problem.", "startOffset": 180, "endOffset": 187}, {"referenceID": 9, "context": "In what follows, we focus on the LSTM architecture [10] with recurrent transition given by \uf8ec\uf8ec\uf8ed f\u0303t \u0129t \u00f5t g\u0303t \uf8f7\uf8f7\uf8f8 = Whht\u22121 +Wxxt + b (2)", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "Covariate shift [23] is a phenomenon in machine learning where the features presented to a model change in distribution during the course of training.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "In deep neural networks, this problem manifests as internal covariance shift [11], where changing the parameters of a layer affects the distribution of the inputs to all layers above it.", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "Batch Normalization [11] is a recently proposed network re-parameterization that aims to reduce internal covariate shift.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "Contrary to previous work [14, 1], we leverage batch normalization in both the input-to-hidden and", "startOffset": 26, "endOffset": 33}, {"referenceID": 0, "context": "Contrary to previous work [14, 1], we leverage batch normalization in both the input-to-hidden and", "startOffset": 26, "endOffset": 33}, {"referenceID": 13, "context": "We suspect that the previous difficulties with recurrent batch normalization reported in the literature [14, 1] are largely due to improper initialization of the batch normalization parameters, and \u03b3 in particular.", "startOffset": 104, "endOffset": 111}, {"referenceID": 0, "context": "We suspect that the previous difficulties with recurrent batch normalization reported in the literature [14, 1] are largely due to improper initialization of the batch normalization parameters, and \u03b3 in particular.", "startOffset": 104, "endOffset": 111}, {"referenceID": 14, "context": "We evaluate our batch-normalized LSTM on a sequential version of the MNIST classification task [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "TANH-RNN [15] 35.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "0 iRNN [15] 97.", "startOffset": 7, "endOffset": 11}, {"referenceID": 1, "context": "0 uRNN [2] 95.", "startOffset": 7, "endOffset": 10}, {"referenceID": 26, "context": "4 sTANH-RNN [28] 98.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "HF-MRNN [19] 1.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "41 Norm-stabilized LSTM [13] 1.", "startOffset": 24, "endOffset": 28}, {"referenceID": 18, "context": "39 ME n-gram [19] 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "td-LSTM [28] 1.", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": "63 HF-MRNN [19] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "54 skipping RNN [21] 1.", "startOffset": 16, "endOffset": 20}, {"referenceID": 23, "context": "The model is trained using RMSProp [24] with learning rate of 10\u22123 and 0.", "startOffset": 35, "endOffset": 39}, {"referenceID": 1, "context": "It has been highlighted in [2] that pMNIST contains many longer term dependencies across pixels than in the original pixel ordering, where a lot of structure is local.", "startOffset": 27, "endOffset": 30}, {"referenceID": 16, "context": "We evaluate our model on the task of character-level language modeling on the Penn Treebank corpus [17] according to the train/valid/test partition of [19].", "startOffset": 99, "endOffset": 103}, {"referenceID": 18, "context": "We evaluate our model on the task of character-level language modeling on the Penn Treebank corpus [17] according to the train/valid/test partition of [19].", "startOffset": 151, "endOffset": 155}, {"referenceID": 11, "context": "0 and step rule determined by Adam [12] with learning rate 0.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": "We evaluate our model on a second character-level language modeling task on the text8 dataset [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "We follow previous authors [19, 28] and use the first 90M characters for training, the next 5M for validation and the final 5M characters for testing.", "startOffset": 27, "endOffset": 35}, {"referenceID": 26, "context": "We follow previous authors [19, 28] and use the first 90M characters for training, the next 5M for validation and the final 5M characters for testing.", "startOffset": 27, "endOffset": 35}, {"referenceID": 11, "context": "0 and step rule determined by Adam [12] with learning rate 0.", "startOffset": 35, "endOffset": 39}, {"referenceID": 7, "context": "To demonstrate the generality and practical applicability of our re-parameterization, we explore the use of batch-normalized LSTM in a unidirectional Attentive Reader Model [8].", "startOffset": 173, "endOffset": 176}, {"referenceID": 6, "context": "We do not compare with [7] as they evaluate in a different setting.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "Figure 5: Error rate on the validation set for the Attentive Reader models on a variant of the CNN QA task [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 11, "context": "We use stochastic gradient descent on minibatches of size 64, with gradient clipping at 10 and step rule determined by Adam [12] with learning rate 8\u00d7 10\u22125.", "startOffset": 124, "endOffset": 128}, {"referenceID": 7, "context": "We follow a similar preprocessing pipeline to [8].", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "However contrary to [8], we limit the number of sentences in each passage to 4 in order to save computation.", "startOffset": 20, "endOffset": 23}, {"referenceID": 13, "context": "Contrary to previous findings [14, 1], we have demonstrated that batch-normalizing the hidden states of recurrent neural networks greatly improves optimization.", "startOffset": 30, "endOffset": 37}, {"referenceID": 0, "context": "Contrary to previous findings [14, 1], we have demonstrated that batch-normalizing the hidden states of recurrent neural networks greatly improves optimization.", "startOffset": 30, "endOffset": 37}, {"referenceID": 13, "context": "We have argued that proper initialization of the batch normalization parameters is crucial, and suggest that previous difficulties [14, 1] were due in large part to improper initialization.", "startOffset": 131, "endOffset": 138}, {"referenceID": 0, "context": "We have argued that proper initialization of the batch normalization parameters is crucial, and suggest that previous difficulties [14, 1] were due in large part to improper initialization.", "startOffset": 131, "endOffset": 138}, {"referenceID": 3, "context": "We would also like to thank the developers of Theano [4] and the Blocks and Fuel [25] libraries, for developing such powerful tools for scientific computing.", "startOffset": 53, "endOffset": 56}], "year": 2016, "abstractText": "We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization.", "creator": "LaTeX with hyperref package"}}}