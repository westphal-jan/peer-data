{"id": "1603.09054", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Unsupervised Measure of Word Similarity: How to Outperform Co-occurrence and Vector Cosine in VSMs", "abstract": "In this paper, we claim that vector cosine, which is generally considered among the most efficient unsupervised measures for identifying word similarity in Vector Space Models, can be outperformed by an unsupervised measure that calculates the extent of the intersection among the most mutually dependent contexts of the target words. To prove it, we describe and evaluate APSyn, a variant of the Average Precision that, without any optimization, outperforms the vector cosine and the co-occurrence on the standard ESL test set, with an improvement ranging between +9.00% and +17.98%, depending on the number of chosen top contexts.", "histories": [["v1", "Wed, 30 Mar 2016 07:05:45 GMT  (790kb)", "http://arxiv.org/abs/1603.09054v1", "in AAAI 2016. arXiv admin note: substantial text overlap witharXiv:1603.08701"]], "COMMENTS": "in AAAI 2016. arXiv admin note: substantial text overlap witharXiv:1603.08701", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["enrico santus", "tin-shing chiu", "qin lu", "alessandro lenci", "chu-ren huang"], "accepted": true, "id": "1603.09054"}, "pdf": {"name": "1603.09054.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Measure of Word Similarity: How to Outperform Co-occurrence and Vector Cosine in VSMs", "authors": ["Enrico Santus", "Tin-Shing Chiu", "Qin Lu", "Alessandro Lenci", "Chu-Ren Huang"], "emails": ["esantus@gmail.com,", "cstschiu@comp.polyu.edu.hk,{qin.lu,", "churen.huang}@polyu.edu.hk", "alessandro.lenci@ling.unipi.it"], "sections": [{"heading": "Introduction and Related Work", "text": "Word similarity detection plays an important role in Natural Language Processing (NLP), as it is the backbone of several applications, as paraphrasing, query expansion, word sense disambiguation, automatic thesauri creation, and so on (Terra and Clarke, 2003) Several approaches have been proposed to measure word similarity (Jarmasz and Szpakowicz, 2003; Levy et al., 2015), some of which are lexicon-based while others are corpora-based. The latter generally rely on the distribution hypothesis that words that occur in similar contexts also have similar meanings (Harris, 1954). Although they all extract statistics from large corporations, they define what must be considered context (i.e. lexeme, syntax, etc.) and how such context is used (Santus et al al, 2014a Hearst, 1992)."}, {"heading": "Method and Evaluation", "text": "The vector cosine described in the following equation (where! \"is the i-th character in the vector x) calculates word similarity by comparing the normalized correlation between the context distribution of two words,! and!: cos!,! =!!!!!!!!!!!! Our hypothesis is that similar words share more interdependent contexts than less similar ones. One way to test it is: i) measuring the intersection between the most commonly used N contexts of two target words and ii) weighting such an intersection according to the order of shared contexts in the dependency ranking. For each target word, we actually evaluate all contexts according to the values of Local Mutual Information (LMI; Evert, 2005) and select the uppermost N: (,) and the note ()."}, {"heading": "Evaluation", "text": "VSM. We use a window-based VSM that records word coincidences within the 5 closest content words to the left and right of each target and weights them with LMI. TEST SET. For evaluation, we use the ESL dataset introduced in Turney (2001) as a method of evaluating algorithms for measuring the degree of word similarity. The test set consists of 50 multiple-choice synonym questions with 4 choices. Each question has been converted into four pairs, using the problem word as the first word and one of the possible choices as the second word. Some words have been lemmatized to have a corresponding form in VSM. TASK. We assigned APSyn to the results of all pairs, and then sorted them in decreasing order."}, {"heading": "Discussion and Conclusions", "text": "In this paper, we have described APSyn, a measurement based on the calculation of the common first characteristics and their performance in ESL questions. Although performance needs to be improved by optimizing the model (i.e. learning the value of N from a training set), our experiments show that the intersection between the N-related contexts of the target words is actually an index of similarity. It is also relevant to mention the role of N at this point: the greater the number of contexts considered, the lower the ability to identify similarities. This also confirms our hypothesis that similar words share a significant number of top mutually dependent contexts, but such an intersection becomes less significant if not only the top contexts are taken into account. Since ESL is small, the reported results should be further investigated on a larger dataset (Santus et al, 2015).2"}], "references": [{"title": "The Statistics of Word Cooccurrences: Word Pairs and Collocations", "author": ["S. Evert"], "venue": "Dissertation, University of Stuttgart.", "citeRegEx": "Evert,? 2005", "shortCiteRegEx": "Evert", "year": 2005}, {"title": "Distributional structure", "author": ["Z. Harris"], "venue": "Word, Vol. 10 (23). 146162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "Automatic Acquisition of Hyponyms from Large Text Corpora", "author": ["M.A. Hearst"], "venue": "Proceedings of the 14th International Conference on Computational Linguistics. 539-545.", "citeRegEx": "Hearst,? 1992", "shortCiteRegEx": "Hearst", "year": 1992}, {"title": "Roget\u2019s thesaurus and semantic similarity, Proceedings of RANLP 2003", "author": ["M. Jarmasz", "S. Szpakowicz"], "venue": "212-219.", "citeRegEx": "Jarmasz and Szpakowicz,? 2003", "shortCiteRegEx": "Jarmasz and Szpakowicz", "year": 2003}, {"title": "Improving Distributional Similarity with Lessons Learned from Word Embeddings", "author": ["O. Levy", "Y. Goldberg", "Dagan I."], "venue": "TACL 2015.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Chasing Hypernyms in Vector Spaces with Entropy", "author": ["E. Santus", "A. Lenci", "Q. Lu", "Schulte im Walde", "S.."], "venue": "Proceedings of EACL 2014, 2:38\u201342.", "citeRegEx": "Santus et al\\.,? 2014a", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Unsupervised Antonym-Synonym Discrimination in Vector Space", "author": ["E. Santus", "Q. Lu", "A. Lenci", "Huang", "C-R."], "venue": "Proceedings of CLIC-IT 2014.", "citeRegEx": "Santus et al\\.,? 2014b", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Taking Antonymy Mask off in Vector Space", "author": ["E. Santus", "Q. Lu", "A. Lenci", "Huang", "C-R."], "venue": "Proceedings of PACLIC 2014.", "citeRegEx": "Santus et al\\.,? 2014c", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models", "author": ["E. Santus", "F. Yung", "A. Lenci", "Huang", "C-R"], "venue": "Proceedings of ACLIJCNLP", "citeRegEx": "Santus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2015}, {"title": "Frequency estimates for statistical word similarity measures", "author": ["E. Terra", "C.L.A. Clarke"], "venue": "Proceedings of HLT/NAACL 2003. 244\u2013251.", "citeRegEx": "Terra and Clarke,? 2003", "shortCiteRegEx": "Terra and Clarke", "year": 2003}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Articial Intelligence Research, Vol. 37. 141-188.", "citeRegEx": "Turney and Pantel,? 2010", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Mining the Web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["P.D. Turney"], "venue": "Proceedings of ECML-2001. 491-502.", "citeRegEx": "Turney,? 2001", "shortCiteRegEx": "Turney", "year": 2001}], "referenceMentions": [{"referenceID": 9, "context": "Word similarity detection plays an important role in Natural Language Processing (NLP), as it is the backbone of several applications, such as paraphrasing, query expansion, word sense disambiguation, automatic thesauri creation, and so on (Terra and Clarke, 2003).", "startOffset": 240, "endOffset": 264}, {"referenceID": 3, "context": "Several approaches have been proposed to measure word similarity (Jarmasz and Szpakowicz, 2003; Levy et al., 2015).", "startOffset": 65, "endOffset": 114}, {"referenceID": 4, "context": "Several approaches have been proposed to measure word similarity (Jarmasz and Szpakowicz, 2003; Levy et al., 2015).", "startOffset": 65, "endOffset": 114}, {"referenceID": 1, "context": "The latter generally rely on the distributional hypothesis, according to which words that occur in similar contexts also have similar meanings (Harris, 1954).", "startOffset": 143, "endOffset": 157}, {"referenceID": 5, "context": ") and in which way such context is used (Santus et al., 2014a; Hearst, 1992).", "startOffset": 40, "endOffset": 76}, {"referenceID": 2, "context": ") and in which way such context is used (Santus et al., 2014a; Hearst, 1992).", "startOffset": 40, "endOffset": 76}, {"referenceID": 10, "context": "ly used to calculate word similarity by measuring the distance between these vectors (Turney and Pantel, 2010).", "startOffset": 85, "endOffset": 110}, {"referenceID": 8, "context": "For this reason, several datasets have been proposed by the NLP community to test distributional similarity measures (Santus et al., 2015).", "startOffset": 117, "endOffset": 138}, {"referenceID": 5, "context": "Indeed, according to the distributional hypothesis, similarity does not only include synonyms, but also other semantic relations, such as hypernymy, co-hyponymy and even antonymy (Santus et al., 2014b-c). For this reason, several datasets have been proposed by the NLP community to test distributional similarity measures (Santus et al., 2015). One of the most used is the English as a Second Language dataset (ESL), introduced in Turney (2001). It consists of 50 multiple-choice synonym questions, with 4 choices each.", "startOffset": 180, "endOffset": 445}, {"referenceID": 0, "context": "For every target word, in fact, we rank all contexts according to the Local Mutual Information values (LMI; Evert, 2005) and pick the top N:", "startOffset": 102, "endOffset": 120}, {"referenceID": 11, "context": "For evaluation, we use the ESL dataset, introduced in Turney (2001) as a way of evaluating algorithms for measuring the degree of word similarity.", "startOffset": 54, "endOffset": 68}, {"referenceID": 8, "context": "Since ESL is small, the reported results should be further investigated on a larger dataset (Santus et al., 2015).", "startOffset": 92, "endOffset": 113}], "year": 2015, "abstractText": "In this paper, we claim that vector cosine \u2013 which is generally considered among the most efficient unsupervised measures for identifying word similarity in Vector Space Models \u2013 can be outperformed by an unsupervised measure that calculates the extent of the intersection among the most mutually dependent contexts of the target words. To prove it, we describe and evaluate APSyn, a variant of the Average Precision that, without any optimization, outperforms the vector cosine and the co-occurrence on the standard ESL test set, with an improvement ranging between +9.00% and +17.98%, depending on the number of chosen top contexts. Introduction and Related Work Word similarity detection plays an important role in Natural Language Processing (NLP), as it is the backbone of several applications, such as paraphrasing, query expansion, word sense disambiguation, automatic thesauri creation, and so on (Terra and Clarke, 2003). Several approaches have been proposed to measure word similarity (Jarmasz and Szpakowicz, 2003; Levy et al., 2015). Some of them are lexicon-based, while others are corpora-based. The latter generally rely on the distributional hypothesis, according to which words that occur in similar contexts also have similar meanings (Harris, 1954). Although all of them extract statistics from large corpora, they vary in the way they define what has to be considered context (i.e. lexemes, syntax, etc.) and in which way such context is used (Santus et al., 2014a; Hearst, 1992). A common way to represent word meaning in NLP is by using vectors to encode the Strength of Association (SoA) between the target word and its contexts. In the resulting Vector Space Model (VSM), vector cosine is then generalCopyright \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. ly used to calculate word similarity by measuring the distance between these vectors (Turney and Pantel, 2010). A well-known problem with the statistical approaches is that they rely on a very loose definition of similarity. Indeed, according to the distributional hypothesis, similarity does not only include synonyms, but also other semantic relations, such as hypernymy, co-hyponymy and even antonymy (Santus et al., 2014b-c). For this reason, several datasets have been proposed by the NLP community to test distributional similarity measures (Santus et al., 2015). One of the most used is the English as a Second Language dataset (ESL), introduced in Turney (2001). It consists of 50 multiple-choice synonym questions, with 4 choices each. In this paper, we describe and evaluate APSyn, a completely unsupervised measure that calculates the extent of the intersection among the N most related contexts of two target words, weighting such intersection according to the rank of the contexts in a mutual dependency ranked list. In our experiments, APSyn outperforms the cosine, with an improvement ranging between +9.00% and +17.98% in the ESL test set, depending on the chosen N. Method and Evaluation The vector cosine, described in the following equation (where f!\" is the i-th feature in the vector x), calculates word similarity by looking at the normalized correlation between the contexts distribution of two words, w! and w!: cos w!,w! = f!!\u00d7 !!! f!! f!! !\u00d7 f!! ! Our hypothesis is that similar words share more mutually dependent contexts than less similar ones. A way to test it is: i) measuring the intersection among the N most related contexts of two target words, and ii) weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. For every target word, in fact, we rank all contexts according to the Local Mutual Information values (LMI; Evert, 2005) and pick the top N: APSyn(w!,w!) = 1 (rank! f! + rank! f! )/2 !\u2208!(!!)\u2229!(!!) That is, for every feature f included in the intersection between the top N features of w!, N(F!), and w!, N(F!), APSyn will add 1 divided by the average rank of the feature, among the top LMI ranked features of w!, rank! f , and w!, rank! f .", "creator": "Word"}}}