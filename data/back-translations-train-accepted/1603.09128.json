{"id": "1603.09128", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders", "abstract": "We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information. Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense for a given word, and a decoder which predicts context words based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time.", "histories": [["v1", "Wed, 30 Mar 2016 11:09:01 GMT  (55kb,D)", "http://arxiv.org/abs/1603.09128v1", "11 pages, to appear at NAACL 2016"]], "COMMENTS": "11 pages, to appear at NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["simon suster", "ivan titov", "gertjan van noord"], "accepted": true, "id": "1603.09128"}, "pdf": {"name": "1603.09128.pdf", "metadata": {"source": "CRF", "title": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders", "authors": ["Simon \u0160uster", "Ivan Titov", "Gertjan van Noord"], "emails": ["s.suster@rug.nl", "titov@uva.nl", "g.j.m.van.noord@rug.nl"], "sections": [{"heading": "1 Introduction", "text": "In recent years we have drawn a lot of attention to ourselves, and the representations associated with it have shown that they can grasp the syntactic and semantic properties of words. They have been evaluated intrinsically, although they are able to deal with the generalization of words and improve their generalization (themselves and themselves). However, it is also possible that most of them focus on the development of models that represent a word with a single vector, some researchers have tried to grasp polysemic explicit properties of each word with multiple vectors (Huang et al, 2012; Tian et al al al.)."}, {"heading": "2 Word Embeddings with Discrete Autoencoders", "text": "Our method borrows its general structure from neural autocoders (Rumelhart et al., 1986; Bengio etal., 2013). Autocoders are trained to reproduce their input by first mapping their input to a (low-dimensional) hidden layer and then predicting an approximation of input to that hidden layer. In our case, the hidden layer is not a real-valued vector, but a categorical variable encoding the meaning of a word. Discrete state autocoders have been successful in several natural language processing applications, including POS tagging and word alignment (Ammar et al., 2014), semantic role induction (Titov and Khoddam, 2015), and relational discovery of a word (Marcheggiani and Titov, 2016). Formally, our model consists of two components: a coding that assigns a meaning to a pivot word, a context and a reconstruction stive."}, {"heading": "2.1 Learning and regularization", "text": "Since sensory assignments are not observed during training, the learning goal includes marginalization compared to the word senses and can therefore be written as follows: \"index i\" goes over all pivot words in the first language, \"j\" over all context words predicted with each i, and \"s marginalized over all possible senses of the word xi.\" In practice, we avoid the costly calculation of the normalization factor in the Softmax calculation of equation (2) and use negative sampling (Mikolov et al., 2013b) instead of the protocol p (xj | xi, s): \"objective calculation of the normalization factor in the Softmax calculation of equation (2) and we use the negative sampling calculation of H (Mikolov et al., 2013b) instead of the protocol p (xi, s):\" objective objective objective calculation, \"objective calculation,\" objective calculation of the normalization factor in the Softmax calculation of equation (objective) and we use the negative sampling calculation of H (Mikolov et al., al., 2013b): \"objective objective objective objective objective calculation,\" objective calculation of p (xi, s), objective calculation of the normalization factor in the Softmax calculation of equation (2) and we use the objective sampling calculation of H (Mikolov et al, al al., 2013b)."}, {"heading": "2.2 Obtaining word representations", "text": "In the test period, we construct word representations by averaging all sensual embeddings for a word xi and weighting them against sensory expectations (Li and Jurafsky, 2015) 3: \u03c9i = \u0445 S p (s | xi, Ci) i, s. (5) 2Id.This means that we are only updating the embeddings i, s *, for the s * = argmaxs p (s | xi, Ci, C \u2032 i, \u03b8).3Although our training goal has few properties, the latecomers are not completely peak during the test period, which makes the weighting useful. Unlike in training, the sensory prediction step does not use the translinguistic context C \u00b2 i here, as it is not available in the evaluation tasks. In this work, we do not marginalidate the non-observable translingual context, but simply ignore it in the calculation. Sometimes, even the first-language context is missing, as in the situation in many word similarities that we use on average."}, {"heading": "3 Word affiliation from alignments", "text": "In defining the crossword puzzle, we refer to heuristics inspired by Devlin et al. (2014). The bilingual context words are understood as the plurality of words around and including the pivot point associated with xi: C \u2032 i = {x \u2032 ai \u2212 m,..., x \u2032 ai,..., x \u2032 ai + m}, (6) where x \u2032 ai is the word associated with xi and the parameter term regulates the context window size. By choosing m = 0, only the associated word is used as the l \u2032 context, and by choosing m = \u221e, the l \u2032 context is the entire sentence. To get the index ai, we use the following: 1) If xi is aimed at exactly one bilingual word, ai is the index of the word to which it is aligned. 2) If xi is aligned with multiple words, ai is the index of the aligned word in the middle (and rounded down if necessary)."}, {"heading": "4 Parameters and Set-up", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Learning parameters", "text": "We use the AdaGrad Optimizer (Duchi et al., 2011) with an initial learning rate of 0.1. We set the minibatch size to 1000, the number of negative samples to 1, the sample factor to 0.001, and the window parameter m to 5. All embeddings are 50-dimensional (unless otherwise specified) and are initialized by samples from the even distribution between [\u2212 0.05, 0.05]. We include all words that occur at least 20 times in the corpus in the vocabulary. We set the number of senses per word to 3 (see further discussion in \u00a7 6.4 and \u00a7 7). All other parameters with their default values can be checked in the source code available online."}, {"heading": "4.2 Bilingual data", "text": "In a large number of papers on multilingual word representations, Europarl (Koehn, 2005) is the preferred source of parallel data. However, the scope of Europarl is rather limited, while we would like to receive word representations of more general language, also in order to carry out an effective evaluation of semantic similarity data sets, where the domains are usually broader. Therefore, we use the following parallel corpora: News commentaries (Bojar et al., 2013) (NC), Yandex-1M4 (RU-EN), CzEng 1.0 (Bojar et al., 2012) (CZ-EN), from which we exclude the EU legislative texts, and GigaFrEn (Callison-Burch et al., 2009) (FR-EN). The size of the corpora is evaluated in Table 1. Word representations trained on the NC corpora are assessed only intrinsically due to their small size."}, {"heading": "5 Evaluation Tasks", "text": "We evaluate the quality of our word representations in a number of tasks, both intrinsic and extrinsic."}, {"heading": "5.1 Word similarity", "text": "We are interested here in how well the semantic similarity ratings achieved by embedding comparisons correlate with human ratings. To this end, we use a variety of benchmarks for English and report on the Spearman correlation between human ratings and the cosinal ratings derived from our word representations. The SCWS benchmark (Huang et al., 2012) is probably the most appropriate 4https: / / translate.yandex.ru / corpussimilarity dataset for evaluating multi-sense embedding, as it allows us to perform the Sense prediction step based on the sentential context for each word in painting. The other benchmarks we use provide the ratings for the word pairs without context. WS-353 contains 353 human-rated word pairs (Finkelstein et al., 2001), while Agiral et Mbenchmarks contain the comparability for 2009 and 2009 S-Wedness (S)."}, {"heading": "5.2 Supersense similarity", "text": "We also evaluate a task that measures the similarity between the embedding - in our case consistently averaged in the case of multi-sense embedding - and a matrix of super-sense characteristics extracted from SemCor using the Qvec tool (Tsvetkov et al., 2015). We choose this method because it provides results that correlate well with extrinsic tasks such as text classification and mood analysis. We believe that this, in combination with word similarity tasks from the previous section, can give a reliable picture of the generic quality of the word embedding examined in this work."}, {"heading": "5.3 POS tagging", "text": "As a downstream evaluation task, we use the learned word representations to initialize the embedding layer of a neural network tagging model. We use the same revolutionary architecture as Li and Juraf-sky (2015): an input layer that uses a chain of adjacent embedding as input, three hidden layers with a fluted linear unit activation function, and a Softmax output layer. We train for 10 epochs using a sentence as a stack. Other hyperparameters can be examined in the source code. Multi-meaning word embedding is derived from the sentential context (weighted average) as for evaluation on the SCWS dataset. We use the standard splits of the Penn Treebank part of the Wall Street Journal: 0-18 for training, 19-21 for development, and 22-24 for testing."}, {"heading": "6 Results", "text": "This year it is so far that it will only take one year to reach an agreement."}, {"heading": "6.1 The amount of (parallel) data", "text": "Fig. 2a shows how the semantic similarity measured on SCWS develops as a function of the increase in larger partial samples from FR-EN, our largest parallel corpus. BIMU embedding shows relatively stable improvements over MU and in particular over DSG embedding. MU and BIMU used to achieve the same performance as DSG at 100%, with only about 40 / 50% of the corpus being used."}, {"heading": "6.2 The dimensionality and frequent words", "text": "Li and Jurafsky (2015) argue that increasing the dimensionality of the SG model is often enough to achieve better results than that of their multi-sense model. We consider the effect of dimensionality on the semantic similarity in Fig. 2b and see that merely increasing the dimensionality of the SG model (to one of 100, 200 or 300 dimensions) is not enough to exceed the MU or BIMU models. If the vocabulary is limited to 6,000 most common words, the representations get a higher quality. We see that the models, especially SG, benefit somewhat more from the increased dimensionality when looking at these most common words, which is in line with expectations - frequent words require more representation capacity due to their complex semantic and syntactic behavior (Atkins and Rundell, 2008)."}, {"heading": "6.3 The role of bilingual signal", "text": "The degree of contribution of the second language l \u2032 during learning is influenced by two parameters, \u03bb for the trade-off between the meaning of the first and second language in the sense of the predictive part (encoder) and the value m for the size of the window around the bilingual word associated with the pivot point. Fig. 3a suggests that the context from the second language is useful in the sense of the prediction and that it should be relatively heavily weighted (about 0.7 and 0.8, depending on the language). Regarding the role of context window size in the sense of ambiguity, the WSD literature indicates both smaller (local) and larger (more up-to-to-date) monolingual contexts as useful, see e.g. Ide and Ve'ronis (1998) for an overview. In Fig. 3b we find that looking at a very narrow context in the second language - the associated word or an m = 1 window around it - is best, and that there is little gain in a wider window."}, {"heading": "6.4 The number of senses", "text": "In our work, the number of senses k is a model parameter that we set to 3 throughout the empirical study. We briefly comment on other possibilities of k [2, 4, 5]. We have found that k = 2 is a good choice for corpora RU-EN and FR-EN (but not for CZ-EN), with an improvement of about 0.2 points over k = 3 for SCWS and for POS marking. For the larger values of k, performance tends to deteriorate. For example, for RU-EN, the k = 5 value for SCWS is about 0.6 points below our default setting."}, {"heading": "7 Additional Related Work", "text": "There are a number of studies that deal with the meaningfulness of people, who deal with the question of how they should behave in the world, how they behave in the world, how they behave in the world, how they behave in the world, how they live in the world, how they live in the world, how they live in the world, how they live, how they live in the world, how they live in the world, how they live in the world, how they live in the world, how they live in the world, how they live in the world, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they live, how they, how they live, how they, how they live, how they, how they live, how they, how they live, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, they, how they, how they, how they, how they, how they, how they, how they, how they, how they, they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, how they, they, how they, how they, how they, how they, how they, how they, how they, how they"}, {"heading": "8 Conclusion", "text": "We have presented a method for learning multi-sense embedding that performs sensory assessments and context predictions together. Monolingual and bilingual information are used in the sensory embedding during the training. We have examined the model performance on a variety of tasks and have shown that the bilingual signal improves the sensory embedding factor, although the cross-lingual information is not available at test date. In this way, we are able to obtain word representations that are of better quality than the monolingual trained multi-sense representations and that exceed the skip-gram embedding for intrinsic tasks. We have analyzed the model performance under several conditions, namely varying dimensionality, vocabulary, data volume, and size of the bilingual context. For the latter parameter, we find that bilingual information is useful even when used as context, which indicates that a mere sentence alignment may be sufficient in certain situations."}, {"heading": "Acknowledgments", "text": "We thank Jiwei Li for providing his tagger implementation as well as Robert Grimm, Diego Marcheggiani and the anonymous reviewers for useful comments. Calculation work was done at the Peregrine HPC Cluster of the University of Groningen. The second author was supported by the NWO Vidi Scholarship 016.153.327."}], "references": [{"title": "A study on similarity and relatedness using distributional and WordNet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa."], "venue": "NAACL-HLT.", "citeRegEx": "Agirre et al\\.,? 2009", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Conditional random field autoencoders for unsupervised structured prediction", "author": ["Waleed Ammar", "Chris Dyer", "Noah A. Smith."], "venue": "NIPS.", "citeRegEx": "Ammar et al\\.,? 2014", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "The Oxford guide to practical lexicography", "author": ["Sue B.T. Atkins", "Michael Rundell."], "venue": "Oxford University Press.", "citeRegEx": "Atkins and Rundell.,? 2008", "shortCiteRegEx": "Atkins and Rundell.", "year": 2008}, {"title": "An unsupervised model for instance level subcategorization acquisition", "author": ["Simon Baker", "Roi Reichart", "Anna Korhonen."], "venue": "EMNLP.", "citeRegEx": "Baker et al\\.,? 2014", "shortCiteRegEx": "Baker et al\\.", "year": 2014}, {"title": "Unsupervised translation sense clustering", "author": ["Mohit Bansal", "John Denero", "Dekang Lin."], "venue": "NAACL-HLT.", "citeRegEx": "Bansal et al\\.,? 2012", "shortCiteRegEx": "Bansal et al\\.", "year": 2012}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "ACL.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In ACL.", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Breaking sticks and ambiguities with adaptive skip-gram", "author": ["Sergey Bartunov", "Dmitry Kondrashkin", "Anton Osokin", "Dmitry Vetrov."], "venue": "arXiv preprint arXiv:1502.07257.", "citeRegEx": "Bartunov et al\\.,? 2015", "shortCiteRegEx": "Bartunov et al\\.", "year": 2015}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pierre Vincent."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "The Joy of Parallelism with CzEng", "author": ["Ond\u0159ej Bojar", "Zden\u011bk \u017dabokrtsk\u00fd", "Ond\u0159ej Du\u0161ek", "Petra Galu\u0161\u010d\u00e1kov\u00e1", "Martin Majli\u0161", "David Mare\u010dek", "Ji\u0159\u0131\u0301 Mar\u0161\u0131\u0301k", "Michal Nov\u00e1k", "Martin Popel", "Ale\u0161 Tamchyna"], "venue": null, "citeRegEx": "Bojar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2012}, {"title": "Findings of the 2013 Workshop on Statistical Machine Translation", "author": ["Specia."], "venue": "WMT.", "citeRegEx": "Specia.,? 2013", "shortCiteRegEx": "Specia.", "year": 2013}, {"title": "Word-sense disambiguation using statistical methods", "author": ["Peter F Brown", "Stephen A Della Pietra", "Vincent J Della Pietra", "Robert L Mercer."], "venue": "ACL.", "citeRegEx": "Brown et al\\.,? 1991", "shortCiteRegEx": "Brown et al\\.", "year": 1991}, {"title": "Distributional semantics in technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "NamKhanh Tran."], "venue": "ACL.", "citeRegEx": "Bruni et al\\.,? 2012", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Findings of the 2009 Workshop on Statistical Machine Translation", "author": ["Chris Callison-Burch", "Philipp Koehn", "Christof Monz", "Josh Schroeder."], "venue": "WMT.", "citeRegEx": "Callison.Burch et al\\.,? 2009", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2009}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Sarath Chandar A P", "Stanislas Lauly", "Hugo Larochelle", "Mitesh M. Khapra", "Balaraman Ravindran", "Vikas C. Raykar", "Amrita Saha."], "venue": "NIPS.", "citeRegEx": "P et al\\.,? 2014", "shortCiteRegEx": "P et al\\.", "year": 2014}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun."], "venue": "EMNLP.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Word sense disambiguation using a second language monolingual corpus", "author": ["Ido Dagan", "Alon Itai."], "venue": "Computational Linguistics, 20(4):563\u2013596.", "citeRegEx": "Dagan and Itai.,? 1994", "shortCiteRegEx": "Dagan and Itai.", "year": 1994}, {"title": "The CMU-Avenue French-English Translation System", "author": ["Michael Denkowski", "Greg Hanneman", "Alon Lavie."], "venue": "WMT.", "citeRegEx": "Denkowski et al\\.,? 2012", "shortCiteRegEx": "Denkowski et al\\.", "year": 2012}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul."], "venue": "ACL.", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "An unsupervised method for word sense tagging using parallel corpora", "author": ["Mona Diab", "Philip Resnik."], "venue": "ACL.", "citeRegEx": "Diab and Resnik.,? 2002", "shortCiteRegEx": "Diab and Resnik.", "year": 2002}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models", "author": ["Chris Dyer", "Adam Lopez", "Juri Ganitkevitch", "Johnathan Weese", "Ferhan Ture", "Phil Blunsom", "Hendra Setiawan", "Vladimir Eidelman", "Philip Resnik."], "venue": "ACL.", "citeRegEx": "Dyer et al\\.,? 2010", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "An information theoretic approach to bilingual word clustering", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "ACL.", "citeRegEx": "Faruqui and Dyer.,? 2013", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2013}, {"title": "Community evaluation and exchange of word vectors at wordvectors.org", "author": ["Manaal Faruqui", "Chris Dyer"], "venue": "In ACL System Demonstrations", "citeRegEx": "Faruqui and Dyer.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "EACL.", "citeRegEx": "Faruqui and Dyer.,? 2014b", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "WWW.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Posterior regularization for structured latent variable models", "author": ["Kuzman Ganchev", "Jo\u00e3o Gra\u00e7a", "Jennifer Gillenwater", "Ben Taskar."], "venue": "The Journal of Machine Learning Research, 11:2001\u20132049.", "citeRegEx": "Ganchev et al\\.,? 2010", "shortCiteRegEx": "Ganchev et al\\.", "year": 2010}, {"title": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments", "author": ["Stephan Gouws", "Yoshua Bengio", "Greg Corrado."], "venue": "arXiv preprint arXiv:1410.2455.", "citeRegEx": "Gouws et al\\.,? 2014", "shortCiteRegEx": "Gouws et al\\.", "year": 2014}, {"title": "Improving unsupervised vector-space thematic fit evaluation via role-filler prototype clustering", "author": ["Clayton Greenberg", "Asad Sayeed", "Vera Demberg."], "venue": "NAACL.", "citeRegEx": "Greenberg et al\\.,? 2015", "shortCiteRegEx": "Greenberg et al\\.", "year": 2015}, {"title": "Learning sense-specific word embeddings by exploiting bilingual resources", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu."], "venue": "COLING.", "citeRegEx": "Guo et al\\.,? 2014", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Large-scale learning of word relatedness with constraints", "author": ["Guy Halawi", "Gideon Dror", "Evgeniy Gabrilovich", "Yehuda Koren."], "venue": "KDD.", "citeRegEx": "Halawi et al\\.,? 2012", "shortCiteRegEx": "Halawi et al\\.", "year": 2012}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "ACL.", "citeRegEx": "Hermann and Blunsom.,? 2014", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "Embedding word similarity with neural machine translation", "author": ["Felix Hill", "Kyunghyun Cho", "S\u00e9bastien Jean", "Coline Devin", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.6448.", "citeRegEx": "Hill et al\\.,? 2014a", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "arXiv preprint arXiv:1408.3456.", "citeRegEx": "Hill et al\\.,? 2014b", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "ACL.", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Introduction to the special issue on word sense disambiguation: the state of the art", "author": ["Nancy Ide", "Jean V\u00e9ronis."], "venue": "Computational linguistics, 24(1):2\u201340.", "citeRegEx": "Ide and V\u00e9ronis.,? 1998", "shortCiteRegEx": "Ide and V\u00e9ronis.", "year": 1998}, {"title": "Cross-lingual sense determination: Can it work", "author": ["Nancy Ide"], "venue": "Computers and the Humanities,", "citeRegEx": "Ide.,? \\Q2000\\E", "shortCiteRegEx": "Ide.", "year": 2000}, {"title": "Word sense acquisition from bilingual comparable corpora", "author": ["Hiroyuki Kaji."], "venue": "NAACL-HLT.", "citeRegEx": "Kaji.,? 2003", "shortCiteRegEx": "Kaji.", "year": 2003}, {"title": "Learning to represent words in context with multilingual supervision", "author": ["Kazuya Kawakami", "Chris Dyer."], "venue": "arXiv preprint arXiv:1511.04623.", "citeRegEx": "Kawakami and Dyer.,? 2015", "shortCiteRegEx": "Kawakami and Dyer.", "year": 2015}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai."], "venue": "COLING.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "MT summit, volume 5.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "CoNLL.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Do multi-sense embeddings improve natural language understanding", "author": ["Jiwei Li", "Dan Jurafsky"], "venue": "In EMNLP", "citeRegEx": "Li and Jurafsky.,? \\Q2015\\E", "shortCiteRegEx": "Li and Jurafsky.", "year": 2015}, {"title": "Phrase clustering for discriminative learning", "author": ["Dekang Lin", "Xiaoyun Wu."], "venue": "ACL-IJCNLP of AFNLP.", "citeRegEx": "Lin and Wu.,? 2009", "shortCiteRegEx": "Lin and Wu.", "year": 2009}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "NAACL.", "citeRegEx": "Lu et al\\.,? 2015", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Discrete-state variational autoencoders for joint discovery and factorization of relations", "author": ["Diego Marcheggiani", "Ivan Titov."], "venue": "Transactions of the Association for Computational Linguistics, 4.", "citeRegEx": "Marcheggiani and Titov.,? 2016", "shortCiteRegEx": "Marcheggiani and Titov.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "ICLR Workshop Papers.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "NIPS.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["George A Miller", "Walter G Charles."], "venue": "Language and cognitive processes, 6(1):1\u201328.", "citeRegEx": "Miller and Charles.,? 1991", "shortCiteRegEx": "Miller and Charles.", "year": 1991}, {"title": "Infinite dimensional word embeddings", "author": ["Eric Nalisnick", "Sachin Ravi."], "venue": "arXiv preprint arXiv:1511.05392.", "citeRegEx": "Nalisnick and Ravi.,? 2015", "shortCiteRegEx": "Nalisnick and Ravi.", "year": 2015}, {"title": "Multilingual part-of-speech tagging: Two unsupervised approaches", "author": ["Tahira Naseem", "Benjamin Snyder", "Jacob Eisenstein", "Regina Barzilay."], "venue": "Journal of Artificial Intelligence Research, 36:1\u201345.", "citeRegEx": "Naseem et al\\.,? 2009", "shortCiteRegEx": "Naseem et al\\.", "year": 2009}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum."], "venue": "EMNLP.", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Exploiting parallel texts for word sense disambiguation: An empirical study", "author": ["Hwee Tou Ng", "Bin Wang", "Yee Seng Chan."], "venue": "ACL.", "citeRegEx": "Ng et al\\.,? 2003", "shortCiteRegEx": "Ng et al\\.", "year": 2003}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Alexandre Passos", "Vineet Kumar", "Andrew McCallum."], "venue": "CoNLL.", "citeRegEx": "Passos et al\\.,? 2014", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "A word at a time: computing word relatedness using temporal semantic analysis", "author": ["Kira Radinsky", "Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch."], "venue": "WWW.", "citeRegEx": "Radinsky et al\\.,? 2011", "shortCiteRegEx": "Radinsky et al\\.", "year": 2011}, {"title": "Multiprototype vector-space models of word meaning", "author": ["Joseph Reisinger", "J. Raymond Mooney."], "venue": "NAACL-HLT.", "citeRegEx": "Reisinger and Mooney.,? 2010", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B Goodenough."], "venue": "Communications of the ACM, 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Learning internal representations by error propagation", "author": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams."], "venue": "David E. Rumelhart, James L. McClelland, and PDP Research Group, editors, Parallel Distributed Processing: Explorations in the Mi-", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Climbing the Tower of Babel: Unsupervised Multilingual Learning", "author": ["Benjamin Snyder", "Regina Barzilay."], "venue": "ICML.", "citeRegEx": "Snyder and Barzilay.,? 2010", "shortCiteRegEx": "Snyder and Barzilay.", "year": 2010}, {"title": "Leveraging monolingual data for crosslingual compositional word representations", "author": ["Hubert Soyer", "Pontus Stenetorp", "Akiko Aizawa."], "venue": "CoRR, abs/1412.6334.", "citeRegEx": "Soyer et al\\.,? 2014", "shortCiteRegEx": "Soyer et al\\.", "year": 2014}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Oscar T\u00e4ckstr\u00f6m", "Ryan McDonald", "Jakob Uszkoreit."], "venue": "NAACL-HLT.", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2012", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu."], "venue": "COLING.", "citeRegEx": "Tian et al\\.,? 2014", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Unsupervised induction of semantic roles within a reconstructionerror minimization framework", "author": ["Ivan Titov", "Ehsan Khoddam."], "venue": "NAACL.", "citeRegEx": "Titov and Khoddam.,? 2015", "shortCiteRegEx": "Titov and Khoddam.", "year": 2015}, {"title": "Crosslingual induction of semantic roles", "author": ["Ivan Titov", "Alexandre Klementiev."], "venue": "ACL.", "citeRegEx": "Titov and Klementiev.,? 2012", "shortCiteRegEx": "Titov and Klementiev.", "year": 2012}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer."], "venue": "EMNLP.", "citeRegEx": "Tsvetkov et al\\.,? 2015", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "ACL.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Verb similarity on the taxonomy of wordnet", "author": ["Dongqiang Yang", "David M.W. Powers."], "venue": "GWC.", "citeRegEx": "Yang and Powers.,? 2006", "shortCiteRegEx": "Yang and Powers.", "year": 2006}, {"title": "Bilingually-constrained phrase embeddings for machine translation", "author": ["Jiajun Zhang", "Shujie Liu", "Mu Li", "Ming Zhou", "Chengqing Zong."], "venue": "ACL.", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Zou et al\\.,? 2013", "shortCiteRegEx": "Zou et al\\.", "year": 2013}, {"title": "Toward using confidence intervals to compare correlations", "author": ["Guang Yong Zou."], "venue": "Psychological methods, 12(4).", "citeRegEx": "Zou.,? 2007", "shortCiteRegEx": "Zou.", "year": 2007}], "referenceMentions": [{"referenceID": 48, "context": "They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al.", "startOffset": 39, "endOffset": 108}, {"referenceID": 6, "context": "They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al.", "startOffset": 39, "endOffset": 108}, {"referenceID": 42, "context": "They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al.", "startOffset": 39, "endOffset": 108}, {"referenceID": 67, "context": ", 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014).", "startOffset": 140, "endOffset": 227}, {"referenceID": 16, "context": ", 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014).", "startOffset": 140, "endOffset": 227}, {"referenceID": 5, "context": ", 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014).", "startOffset": 140, "endOffset": 227}, {"referenceID": 55, "context": ", 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014).", "startOffset": 140, "endOffset": 227}, {"referenceID": 35, "context": "While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015).", "startOffset": 238, "endOffset": 345}, {"referenceID": 63, "context": "While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015).", "startOffset": 238, "endOffset": 345}, {"referenceID": 53, "context": "While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015).", "startOffset": 238, "endOffset": 345}, {"referenceID": 15, "context": "While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015).", "startOffset": 238, "endOffset": 345}, {"referenceID": 43, "context": "While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015).", "startOffset": 238, "endOffset": 345}, {"referenceID": 40, "context": ", a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014).", "startOffset": 117, "endOffset": 215}, {"referenceID": 32, "context": ", a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014).", "startOffset": 117, "endOffset": 215}, {"referenceID": 28, "context": ", a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014).", "startOffset": 117, "endOffset": 215}, {"referenceID": 30, "context": "Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014).", "startOffset": 107, "endOffset": 125}, {"referenceID": 25, "context": "We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009).", "startOffset": 80, "endOffset": 199}, {"referenceID": 70, "context": "We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009).", "startOffset": 80, "endOffset": 199}, {"referenceID": 65, "context": "We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009).", "startOffset": 80, "endOffset": 199}, {"referenceID": 60, "context": "We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009).", "startOffset": 80, "endOffset": 199}, {"referenceID": 52, "context": "We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009).", "startOffset": 80, "endOffset": 199}, {"referenceID": 38, "context": "Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991).", "startOffset": 151, "endOffset": 256}, {"referenceID": 54, "context": "Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991).", "startOffset": 151, "endOffset": 256}, {"referenceID": 20, "context": "Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991).", "startOffset": 151, "endOffset": 256}, {"referenceID": 37, "context": "Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991).", "startOffset": 151, "endOffset": 256}, {"referenceID": 17, "context": "Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991).", "startOffset": 151, "endOffset": 256}, {"referenceID": 11, "context": "Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991).", "startOffset": 151, "endOffset": 256}, {"referenceID": 59, "context": "Our method borrows its general structure from neural autoencoders (Rumelhart et al., 1986; Bengio et al., 2013).", "startOffset": 66, "endOffset": 111}, {"referenceID": 8, "context": "Our method borrows its general structure from neural autoencoders (Rumelhart et al., 1986; Bengio et al., 2013).", "startOffset": 66, "endOffset": 111}, {"referenceID": 1, "context": "Discrete-state autoencoders have been successful in several natural language processing applications, including POS tagging and word alignment (Ammar et al., 2014), semantic role induction (Titov and Khoddam, 2015) and relation discovery (Marcheggiani and Titov, 2016).", "startOffset": 143, "endOffset": 163}, {"referenceID": 64, "context": ", 2014), semantic role induction (Titov and Khoddam, 2015) and relation discovery (Marcheggiani and Titov, 2016).", "startOffset": 33, "endOffset": 58}, {"referenceID": 47, "context": ", 2014), semantic role induction (Titov and Khoddam, 2015) and relation discovery (Marcheggiani and Titov, 2016).", "startOffset": 82, "endOffset": 112}, {"referenceID": 48, "context": "This is effectively a Skip-Gram model (Mikolov et al., 2013a) extended to rely on senses.", "startOffset": 38, "endOffset": 61}, {"referenceID": 49, "context": "(2) and use negative sampling (Mikolov et al., 2013b) instead of log p(xj |xi, s, \u03b8):", "startOffset": 30, "endOffset": 53}, {"referenceID": 53, "context": "Optimizing the autoencoding objective is broadly similar to the learning algorithm defined for multi-sense embedding induction in some of the previous work (Neelakantan et al., 2014; Li and Jurafsky, 2015).", "startOffset": 156, "endOffset": 205}, {"referenceID": 43, "context": "Optimizing the autoencoding objective is broadly similar to the learning algorithm defined for multi-sense embedding induction in some of the previous work (Neelakantan et al., 2014; Li and Jurafsky, 2015).", "startOffset": 156, "endOffset": 205}, {"referenceID": 27, "context": "We therefore use a form of posterior regularization (Ganchev et al., 2010) where we can encode our prior expectations that the posteriors should be sharp.", "startOffset": 52, "endOffset": 74}, {"referenceID": 47, "context": "This modified objective can also be motivated from a variational approximation perspective, see Marcheggiani and Titov (2016) for details.", "startOffset": 96, "endOffset": 126}, {"referenceID": 43, "context": "At test time, we construct the word representations by averaging all sense embeddings for a word xi and weighting them with the sense expectations (Li and Jurafsky, 2015)3:", "startOffset": 147, "endOffset": 170}, {"referenceID": 19, "context": "In defining the crosslingual signal we draw on a heuristic inspired by Devlin et al. (2014). The secondlanguage context words are taken to be the multiset of words around and including the pivot affiliated to xi: C \u2032 i = {xai\u2212m, .", "startOffset": 71, "endOffset": 92}, {"referenceID": 22, "context": "We use the cdec aligner (Dyer et al., 2010) to wordalign the parallel corpora.", "startOffset": 24, "endOffset": 43}, {"referenceID": 21, "context": "We use the AdaGrad optimizer (Duchi et al., 2011) with initial learning rate set to 0.", "startOffset": 29, "endOffset": 49}, {"referenceID": 41, "context": "In a large body of work on multilingual word representations, Europarl (Koehn, 2005) is the preferred source of parallel data.", "startOffset": 71, "endOffset": 84}, {"referenceID": 9, "context": "0 (Bojar et al., 2012) (CZ-EN) from which we exclude the EU legislation texts, and GigaFrEn (Callison-Burch et al.", "startOffset": 2, "endOffset": 22}, {"referenceID": 13, "context": ", 2012) (CZ-EN) from which we exclude the EU legislation texts, and GigaFrEn (Callison-Burch et al., 2009) (FR-EN).", "startOffset": 77, "endOffset": 106}, {"referenceID": 35, "context": "The SCWS benchmark (Huang et al., 2012) is probably the most suitable", "startOffset": 19, "endOffset": 39}, {"referenceID": 26, "context": "WS-353 contains 353 human-rated word pairs (Finkelstein et al., 2001), while Agirre et al.", "startOffset": 43, "endOffset": 69}, {"referenceID": 58, "context": "The RG-65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) benchmarks contain nouns only.", "startOffset": 10, "endOffset": 43}, {"referenceID": 50, "context": "The RG-65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) benchmarks contain nouns only.", "startOffset": 58, "endOffset": 84}, {"referenceID": 56, "context": "The MTurk-287 (Radinsky et al., 2011) and MTurk-771 (Halawi et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 31, "context": ", 2011) and MTurk-771 (Halawi et al., 2012) include word pairs whose similarity was crowdsourced from AMT.", "startOffset": 22, "endOffset": 43}, {"referenceID": 12, "context": "Similarly, MEN (Bruni et al., 2012) is an AMT-annotated dataset of 3000 word pairs.", "startOffset": 15, "endOffset": 35}, {"referenceID": 68, "context": "The YP130 (Yang and Powers, 2006) and Verb-143 (Baker et al.", "startOffset": 10, "endOffset": 33}, {"referenceID": 3, "context": "The YP130 (Yang and Powers, 2006) and Verb-143 (Baker et al., 2014) measure verb similarity.", "startOffset": 47, "endOffset": 67}, {"referenceID": 46, "context": "Rare-Word (Luong et al., 2013) contains 2034 rare-word pairs.", "startOffset": 10, "endOffset": 30}, {"referenceID": 34, "context": "Finally, SimLex-999 (Hill et al., 2014b) is intended to measure pure similarity as opposed to relatedness.", "startOffset": 20, "endOffset": 40}, {"referenceID": 0, "context": ", 2001), while Agirre et al. (2009) separate this benchmark for similarity (WS-SIM) and relatedness (WS-REL).", "startOffset": 15, "endOffset": 36}, {"referenceID": 0, "context": ", 2001), while Agirre et al. (2009) separate this benchmark for similarity (WS-SIM) and relatedness (WS-REL). The RG-65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) benchmarks contain nouns only. The MTurk-287 (Radinsky et al., 2011) and MTurk-771 (Halawi et al., 2012) include word pairs whose similarity was crowdsourced from AMT. Similarly, MEN (Bruni et al., 2012) is an AMT-annotated dataset of 3000 word pairs. The YP130 (Yang and Powers, 2006) and Verb-143 (Baker et al., 2014) measure verb similarity. Rare-Word (Luong et al., 2013) contains 2034 rare-word pairs. Finally, SimLex-999 (Hill et al., 2014b) is intended to measure pure similarity as opposed to relatedness. For these benchmarks, we prepare the word representations by taking a uniform average of all sense embeddings per word. The evaluation is carried out using the tool described in Faruqui and Dyer (2014a). Due to space constraints, we report the results by averaging over all benchmarks (Similarity), and include the individual results in the online repository.", "startOffset": 15, "endOffset": 912}, {"referenceID": 66, "context": "We also evaluate on a task measuring the similarity between the embeddings\u2014in our case uniformly averaged in the case of multi-sense embeddings\u2014and a matrix of supersense features extracted from the English SemCor, using the Qvec tool (Tsvetkov et al., 2015).", "startOffset": 235, "endOffset": 258}, {"referenceID": 43, "context": "We use the same convolutional architecture as Li and Jurafsky (2015): an input layer taking a concatenation of neighboring embeddings as input, three hidden layers with a rectified linear unit activation function and a softmax output layer.", "startOffset": 46, "endOffset": 69}, {"referenceID": 71, "context": "We have also measured the 95% confidence intervals of the difference between the correlation coefficients of BIMU and SG, following the method described in Zou (2007). According to these values, BIMU significantly outperforms SG on RU-EN, and on French, Russian and Spanish NC corpora.", "startOffset": 156, "endOffset": 167}, {"referenceID": 18, "context": "Also, FR-EN is known to be noisy, containing webcrawled sentences that are not parallel or not natural language (Denkowski et al., 2012).", "startOffset": 112, "endOffset": 136}, {"referenceID": 43, "context": "For example, Li and Jurafsky (2015) use the concatenation of Gigaword and Wikipedia with more than 5B words.", "startOffset": 13, "endOffset": 36}, {"referenceID": 15, "context": "0 Chen et al. (2014) 68.", "startOffset": 2, "endOffset": 21}, {"referenceID": 15, "context": "0 Chen et al. (2014) 68.4 Neelakantan et al. (2014) 69.", "startOffset": 2, "endOffset": 52}, {"referenceID": 15, "context": "0 Chen et al. (2014) 68.4 Neelakantan et al. (2014) 69.3 Li and Jurafsky (2015) 69.", "startOffset": 2, "endOffset": 80}, {"referenceID": 29, "context": "Similar observations have been made in the context of selectional preference modeling of polysemous verbs (Greenberg et al., 2015).", "startOffset": 106, "endOffset": 130}, {"referenceID": 43, "context": "The neural network tagger may be able to implicitly perform disambiguation on top of single-sense SG embeddings, similarly to what has been argued in Li and Jurafsky (2015). The tagging accuracies obtained with MU on CZ-EN and FR-EN are similar to the one obtained by Li and Jurafsky with their multi-sense model (93.", "startOffset": 150, "endOffset": 173}, {"referenceID": 2, "context": "This is according to expectations\u2014frequent words need more representational capacity due to their complex semantic and syntactic behavior (Atkins and Rundell, 2008).", "startOffset": 138, "endOffset": 164}, {"referenceID": 41, "context": "It is argued in Li and Jurafsky (2015) that often just increasing the dimensionality of the SG model suffices to obtain better results than that of their multi-sense model.", "startOffset": 16, "endOffset": 39}, {"referenceID": 36, "context": "Ide and V\u00e9ronis (1998) for an overview.", "startOffset": 0, "endOffset": 23}, {"referenceID": 35, "context": "One line of research has dealt with sense induction as a separate, clustering problem that is followed by an embedding learning component (Huang et al., 2012; Reisinger and Mooney, 2010).", "startOffset": 138, "endOffset": 186}, {"referenceID": 57, "context": "One line of research has dealt with sense induction as a separate, clustering problem that is followed by an embedding learning component (Huang et al., 2012; Reisinger and Mooney, 2010).", "startOffset": 138, "endOffset": 186}, {"referenceID": 53, "context": "In another, the sense assignment and the embeddings are trained jointly (Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Bartunov et al., 2015).", "startOffset": 72, "endOffset": 163}, {"referenceID": 63, "context": "In another, the sense assignment and the embeddings are trained jointly (Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Bartunov et al., 2015).", "startOffset": 72, "endOffset": 163}, {"referenceID": 43, "context": "In another, the sense assignment and the embeddings are trained jointly (Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Bartunov et al., 2015).", "startOffset": 72, "endOffset": 163}, {"referenceID": 7, "context": "In another, the sense assignment and the embeddings are trained jointly (Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Bartunov et al., 2015).", "startOffset": 72, "endOffset": 163}, {"referenceID": 48, "context": "(2014) propose an extension of Skip-Gram (Mikolov et al., 2013a) by introducing sense-specific parameters together with the k-means-inspired \u2018centroid\u2019 vectors that keep track of the contexts in which word senses have occurred.", "startOffset": 41, "endOffset": 64}, {"referenceID": 7, "context": ", 2014; Li and Jurafsky, 2015; Bartunov et al., 2015). Neelakantan et al. (2014) propose an extension of Skip-Gram (Mikolov et al.", "startOffset": 31, "endOffset": 81}, {"referenceID": 7, "context": ", 2014; Li and Jurafsky, 2015; Bartunov et al., 2015). Neelakantan et al. (2014) propose an extension of Skip-Gram (Mikolov et al., 2013a) by introducing sense-specific parameters together with the k-means-inspired \u2018centroid\u2019 vectors that keep track of the contexts in which word senses have occurred. They explore two model variants, one in which the number of senses is the same for all words, and another in which a threshold value determines the number of senses for each word. The results comparing the two variants are inconclusive, with the advantage of the dynamic variant being virtually nonexistent. In our work, we use the static approach. Whenever there is evidence for less senses than the number of available sense vectors, this is unlikely to be a serious issue as the learning would concentrate on some of the senses, and these would then be the preferred predictions also at test time. Li and Jurafsky (2015) build upon the work of Neelakantan et al.", "startOffset": 31, "endOffset": 926}, {"referenceID": 28, "context": "Guo et al. (2014) perform a sense induction step based on clustering translations prior to learning word embeddings.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "In a similar vein, Bansal et al. (2012) use bilingual corpora to perform soft word clustering, extending the previous work on the monolingual case of", "startOffset": 19, "endOffset": 40}, {"referenceID": 45, "context": "Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space.", "startOffset": 92, "endOffset": 216}, {"referenceID": 25, "context": "Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space.", "startOffset": 92, "endOffset": 216}, {"referenceID": 33, "context": "Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space.", "startOffset": 92, "endOffset": 216}, {"referenceID": 69, "context": "Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space.", "startOffset": 92, "endOffset": 216}, {"referenceID": 23, "context": "Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space.", "startOffset": 92, "endOffset": 216}, {"referenceID": 70, "context": "Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space.", "startOffset": 92, "endOffset": 216}, {"referenceID": 32, "context": "A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T\u00e4ckstr\u00f6m et al., 2012).", "startOffset": 171, "endOffset": 313}, {"referenceID": 28, "context": "A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T\u00e4ckstr\u00f6m et al., 2012).", "startOffset": 171, "endOffset": 313}, {"referenceID": 61, "context": "A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T\u00e4ckstr\u00f6m et al., 2012).", "startOffset": 171, "endOffset": 313}, {"referenceID": 40, "context": "A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T\u00e4ckstr\u00f6m et al., 2012).", "startOffset": 171, "endOffset": 313}, {"referenceID": 62, "context": "A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T\u00e4ckstr\u00f6m et al., 2012).", "startOffset": 171, "endOffset": 313}, {"referenceID": 38, "context": "The recent works of Kawakami and Dyer (2015) and Nalisnick and Ravi (2015) are also of interest.", "startOffset": 20, "endOffset": 45}, {"referenceID": 38, "context": "The recent works of Kawakami and Dyer (2015) and Nalisnick and Ravi (2015) are also of interest.", "startOffset": 20, "endOffset": 75}, {"referenceID": 38, "context": "The recent works of Kawakami and Dyer (2015) and Nalisnick and Ravi (2015) are also of interest. The latter work on the infinite Skip-Gram model in which the embedding dimensionality is stochastic is relevant since it demonstrates that their embeddings exploit different dimensions to encode different word meanings. Just like us, Kawakami and Dyer (2015) use bilingual supervision, but in a more complex LSTM network that is trained to predict word translations.", "startOffset": 20, "endOffset": 356}], "year": 2016, "abstractText": "We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information. Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense for a given word, and a decoder which predicts context words based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time.", "creator": "LaTeX with hyperref package"}}}