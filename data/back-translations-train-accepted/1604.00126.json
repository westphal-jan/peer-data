{"id": "1604.00126", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "Nonparametric Spherical Topic Modeling with Word Embeddings", "abstract": "Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference.", "histories": [["v1", "Fri, 1 Apr 2016 04:36:58 GMT  (529kb,D)", "http://arxiv.org/abs/1604.00126v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG stat.ML", "authors": ["kayhan batmanghelich", "ardavan saeedi", "karthik narasimhan", "samuel gershman"], "accepted": true, "id": "1604.00126"}, "pdf": {"name": "1604.00126.pdf", "metadata": {"source": "CRF", "title": "Nonparametric Spherical Topic Modeling with Word Embeddings", "authors": ["Kayhan Batmanghelich", "Ardavan Saeedi", "Karthik Narasimhan", "Sam Gershman"], "emails": ["kayhan@mit.edu", "ardavans@mit.edu", "karthikn@mit.edu", "gershman@fas.harvard.edu"], "sections": [{"heading": "1 Introduction", "text": "Prior to working on topic modeling, the main focus was on the use of categorical probabilities (Lead et al., 2003; Lead and Lafferty, 2006; Rosen-Zvi et al., 2004). Applications of topic models in the textual field treat words as discrete observations, ignoring the semantics of language. Recent developments in the representation of words distribution (Mikolov et al., 2013; Pennington et al., 2014) have managed to capture certain semantic regularities, but have not been studied more extensively than authors who have contributed equally and are listed alphabetically in the context of topic modeling."}, {"heading": "2 Related Work", "text": "Topic modeling and word embeddings Das et al. (2015) proposed a theme model that uses a Gaussian distribution over word embeddings. By inferring the vector representations of the words, their model is encouraged to group words that are semantically similar, leading to more coherent themes. In contrast, we propose to use Mises-Fisher (vMF) distributions based on cosmic similarity between word vectors rather than Euclidean distance. vMF in theme models. vMF distribution was used to model directional data by placing points on a unity sphere (Dhillon and Sra, 2003). Reisinger et al. (2010) propose an admixture model that uses vMF to model documents that are represented as a vector of normalized word frequencies. This does not take into account semantic similarities at the word level. In contrast to their method, we use vMF for word embeddings."}, {"heading": "3 Model", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "4 Experiments", "text": "Setup We conduct experiments on two different text corpora: 11266 documents from 20 NEWSGROUPS1 and 1566 documents from the NIPS corpus2. We use 50-dimensional word embeddings trained on text from Wikipedia using word2vec3. The vectors are post-processed to have unit '2 standard. We evaluate our model using a coherence benchmark (Newman et al., 2010) that effectively correlates with human judgment (Lau et al., 2014). To do this, we calculate the punch line mutual information (PMI) using a reference corpus of 300k documents from Wikipedia. The PMI is calculated using coincidence statistics on word pairs (ui, uj) in 20-word sliding windows: PMI (ui, uj) = log p (ui, uj) \u00b7 uj. We compare our model with two basic DDA-points from two individual models."}, {"heading": "5 Conclusion", "text": "Recently, distributional representations of words that exhibit semantic consistency over directional metrics such as cosmic similarity have emerged. Neither categorical nor Gaussian observational distributions used in existing theme models are suitable for using such correlations. In this paper, we demonstrate the use of the von Mises-Fisher distribution to model words as points over a unity sphere. We use HDP as a baseline theme model and propose an efficient algorithm based on stochastic variation conclusions. Of course, our model utilizes the semantic structures of word embedding while flexibly inating the number of topics. We show that our method outperforms three competing approaches to topic coherence on two different datasets. 4Our sHDP implementation is done in Python and the G-LDA code in Java."}], "references": [{"title": "Joydeep Ghosh", "author": ["Arindam Banerjee", "Inderjit S Dhillon"], "venue": "and Suvrit Sra.", "citeRegEx": "Banerjee et al.2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Dynamic topic models", "author": ["Blei", "Lafferty2006] David M Blei", "John D Lafferty"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "2003", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Latent dirichlet allocation. the Journal of machine Learning research, 3:993\u2013", "citeRegEx": "Blei et al.2003", "shortCiteRegEx": null, "year": 1022}, {"title": "Probabilistic topic models", "author": ["David M Blei"], "venue": "Communications of the ACM,", "citeRegEx": "Blei.,? \\Q2012\\E", "shortCiteRegEx": "Blei.", "year": 2012}, {"title": "Truly nonparametric online variational inference for hierarchical dirichlet processes", "author": ["Bryant", "Sudderth2012] Michael Bryant", "Erik B Sudderth"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bryant et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bryant et al\\.", "year": 2012}, {"title": "Manzil Zaheer", "author": ["Rajarshi Das"], "venue": "and Chris Dyer.", "citeRegEx": "Das et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling data using directional distributions. Technical report, Technical Report TR03-06, Department of Computer Sciences, The University of Texas at Austin", "author": ["Dhillon", "Sra2003] Inderjit S Dhillon", "Suvrit Sra"], "venue": "URL ftp://ftp. cs. utexas", "citeRegEx": "Dhillon et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2003}, {"title": "Von mises-fisher clustering models", "author": ["Gopal", "Yang2014] Siddarth Gopal", "Yiming Yang"], "venue": null, "citeRegEx": "Gopal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gopal et al\\.", "year": 2014}, {"title": "Wei Gao", "author": ["Yulan He", "Chenghua Lin"], "venue": "and Kam-Fai Wong.", "citeRegEx": "He et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Chong Wang", "author": ["Matthew D Hoffman", "David M Blei"], "venue": "and John Paisley.", "citeRegEx": "Hoffman et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "David Newman", "author": ["Jey Han Lau"], "venue": "and Timothy Baldwin.", "citeRegEx": "Lau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Greg S Corrado", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen"], "venue": "and Jeff Dean.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Karl Grieser", "author": ["David Newman", "Jey Han Lau"], "venue": "and Timothy Baldwin.", "citeRegEx": "Newman et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "David M Blei", "author": ["John Paisley", "Chingyue Wang"], "venue": "and Michael I Jordan.", "citeRegEx": "Paisley et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Christopher D", "author": ["Jeffrey Pennington", "Richard Socher"], "venue": "Manning.", "citeRegEx": "Pennington et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Bryan Silverthorn", "author": ["Joseph Reisinger", "Austin Waters"], "venue": "and Raymond J Mooney.", "citeRegEx": "Reisinger et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Mark Steyvers", "author": ["Michal Rosen-Zvi", "Thomas Griffiths"], "venue": "and Padhraic Smyth.", "citeRegEx": "Rosen.Zvi et al.2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Matthew J Beal", "author": ["Yee Whye Teh", "Michael I Jordan"], "venue": "and David M Blei.", "citeRegEx": "Teh et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Matthew J Beal", "author": ["Yee Whye Teh", "Michael I Jordan"], "venue": "and David M Blei.", "citeRegEx": "Teh et al.2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference.", "creator": "LaTeX with hyperref package"}}}