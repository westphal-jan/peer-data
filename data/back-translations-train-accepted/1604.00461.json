{"id": "1604.00461", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2016", "title": "Embedding Lexical Features via Low-Rank Tensors", "abstract": "Modern NLP models rely heavily on engineered features, which often combine word and contextual information into complex lexical features. Such combination results in large numbers of features, which can lead to over-fitting. We present a new model that represents complex lexical features---comprised of parts for words, contextual information and labels---in a tensor that captures conjunction information among these parts. We apply low-rank tensor approximations to the corresponding parameter tensors to reduce the parameter space and improve prediction speed. Furthermore, we investigate two methods for handling features that include $n$-grams of mixed lengths. Our model achieves state-of-the-art results on tasks in relation extraction, PP-attachment, and preposition disambiguation.", "histories": [["v1", "Sat, 2 Apr 2016 04:59:21 GMT  (1859kb,D)", "http://arxiv.org/abs/1604.00461v1", "Accepted by NAACL 2016"]], "COMMENTS": "Accepted by NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["mo yu", "mark dredze", "raman arora", "matthew r gormley"], "accepted": true, "id": "1604.00461"}, "pdf": {"name": "1604.00461.pdf", "metadata": {"source": "CRF", "title": "Embedding Lexical Features via Low-Rank Tensors", "authors": ["Mo Yu", "Mark Dredze", "Matthew R. Gormley"], "emails": ["yum@us.ibm.com", "mdredze@cs.jhu.edu", "arora@cs.jhu.edu", "mgormley@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that in the real world, it's just a question of to what extent it's about a way in which people in the real world identify with the real world, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is what it's about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about which is about, which is about, which is about which is about, which is about, which is about which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about which is about, which is about, which is about which is about, which is about which is about, which is about, which is about, which is about which is about, which is about, which is about which is about, which is about which is about, which is about, which is about, which is about which is about, which is about which is about, which is about, which is about which is about, which is about which is about, which is about, which is about which is about, which is about, which is about, which is about which is about, which is about which is about, which is about, which is about, which is about which is about, which is about which is about, which is about, which is about, which is about, which is about, which is about, which is about, which is about which is about, which is about,"}, {"heading": "2 Notation and Definitions", "text": "Let T-Rd1 \u00b7 \u00b7 dK be a K-way tensor (i.e., a tensor with K views). In this paper we consider the tensor k-mode product, i.e. the multiplication of a tensor T-Rd1 \u00b7 \u00b7 dK by a matrix x x \u00b7 cker \u00b7 J (or a vector if J = 1) rank k. The product is denoted by T-k x and is of size d1 \u00b7 \u00b7 \u00b7 dK \u2212 1 \u00b7 \u00b7 dK + 1 \u00b7 dK. Element wise we have (T-k x) i1... ik \u2212 1 j ik + 1... iK = dk."}, {"heading": "3 Factorization of Lexical Features", "text": "Suppose we have a characteristic that contains information from a label y, several lexical elements w1,., wn and non-lexical property, etc. This characteristic can be factored in as a conjunction of each part. (...) The characteristics when all (n + 2) parts in the instance are fired (which is reflected in Figure 1d, illustrates this case with two lexical parts. Faced with an input instance x and its associated designation y, we can extract a set of characteristics S (x, y). In 2u, y denote one-hot vectors instead of symbols.a traditional log-linear model, we consider the instance x and its associated designation y."}, {"heading": "4 Feature Representations via Low-rank Tensor Approximations", "text": "In this section, we will show how to calculate the score in (4) without constructing the complete functional tensor using two tensor approximation methods (\u00a7 4.1 and \u00a7 4.2). We will start with some intuition. To evaluate the original (complete) tensor representation of \u03c6, we need a parameter tensor T of size d1 \u00b7 d2 \u00b7. \u00b7 dn + 2, where d3 = \u00b7 \u00b7 = dn + 2 = | V | is the vocabulary size, n is the number of lexical parts in the characteristic, and d1 = L | and d2 = | F | are the number of different terms or non-lexical properties. (\u00a7 5 will handle n varying properties.) Our methods reduce the tensor size by embedding each part of the formulas in a low-dimensional space in which we represent each label, not lexical properties and words with lesser properties."}, {"heading": "4.1 Tucker Form", "text": "For our first approximation, we assume that tensor T has a low Tucker decomposition: T = g \u00b7 l Wl \u00b7 f Wf \u00b7 w1 W1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 wn Wn. We can then express the scoring function (4) for a attribute \u03c6 = (y, u, w1,... wn) with n-lexical parts, such as: s (y, u, w1, \u00b7, wn; g, Wl, Wf, {Wi} ni = 1) = g \u00b7 l h (l) y \u00b7 f (f) u \u00b7 w1 h (1) w1 \u00b7 \u00b7 \u00b7 wn h (n) wn, (6), which amounts to first weighting u, y and wi (for all i) to low-dimensional vectors h (f) u (f) u, h (l) y, h (i) wi, and then projecting these hidden representations using the flat core grades g. The lower dimensional representations of a K and the corresponding criteria (the representations) are based on the FRs."}, {"heading": "4.2 CP Form", "text": "For the Tucker approximation, the number of parameters in (6) is scaled exponentially with the number of lexical parts. If, for example, each h (i) wi has a dimensionality r, then the scalability and further control of the complexity of our tensor model is achieved by approximating the parameter tensor by means of CP decomposition as in (2), which results in the following scoring function: s (y, u, w1, \u00b7 \u00b7, wn; Wl, Wf, {Wi} ni = 1) = r \u2211 j = 1 (h (l) y \u0445 h (f) u \u0432h (1) w1 \u0445 \u00b7 \u00b7 \u00b7 \u0445h (n) wn) j. (7) We call this model Low-Rank Feature Representation with CP form (LRFRn-CP)."}, {"heading": "4.3 Pre-trained Word Embeddings", "text": "It is one of the few that we are able to improve the prefabricated word embeddings, as we use them in practice. (We) We are the only one that is able to facilitate the prefabricated word embeddings through ew, so that the transformation matrices use Wi for the lexical parts of the size ri / M. (We point out that if they are sufficiently large, our model improves the prefabricated word embeddings to improve the expressiveness of the model, as is often the case with deep network models.Remarks Our LRFRs introduce the beds for non-lexical properties and make them better. (We) We have the ability to use the prefabricated word embeddings to improve the expressiveness of the model, as we do with deep network models.Remarks Our LRFRs introduce the beds for non-lexical properties, so that they better adapt to common settings."}, {"heading": "6 Parameter Estimation", "text": "The goal of learning is to find a Tensor T that solves the problem (5). Note that this is a non-convex goal, so compared to the convex goal in a traditional log-linear model, we trade better feature representations with the cost of a harder optimization problem. While stochastic gradient descent (SGD) is a natural choice for learning representations in large data environments, problem (5) includes ranking constraints that require a costly closer operation to enforce the constraints on each repetition of the SGD. We strive for a more efficient learning algorithm. Note that we have specified the size of each transformation matrix Wi-Rri-di so that the smaller dimension (ri < di) corresponds to the upper limit of the rank. Therefore, the ranking constants are always fulfilled by a course of the SGD, and we essentially have an unlimited optimization problem. Note that we do not guarantee this way of fulfilling the orthosis."}, {"heading": "7 Experimental Settings", "text": "We evaluate LRFR et al. (2015), we use both gold entity spans and types representing the model based on information about the relationship between us and the others. (2015) We use the same word embedding in Belinkov et al. (2014) on PP attachment for a fair comparison. (2015) We use the same word embedding in Belinkov et al. (2014) on PP attachment for a fair comparison. (2014) on PP attachment for a fair comparison. For the other experiments we use the same 200-d word embedding in Yu et al. (2015).Relation We use the English part of ACE 2005 relationship Dataset al. (2006) Following Yu et al. (2015), we use both gold entity spans and types representing the model of communication via communication."}, {"heading": "8 Results", "text": "This year it is more than ever before in the history of the city."}, {"heading": "9 Related Work", "text": "These methods treat traits as atomic elements and ignore the inner structure of traits so that they can learn separate embeddings for each trait without common parameters. As a result, they still suffer from large parameter spaces when the trait space is very huge.5Another line of research studies includes the inner structures of lexical traits: e.g. Koo et al. (2008), Turian et al. (2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), which use pre-trained word embeddings to replace lexical traits."}, {"heading": "10 Conclusion", "text": "We have introduced LRFR, a feature representation model that exploits the inner structure of complex lexical features and uses a low tensor to efficiently score with this representation. LRFR reaches the state of the art in several tasks, including relation extraction, PP binding and disambiguation of prepositions. We make our implementation available for general use.6"}, {"heading": "Acknowledgements", "text": "Much of this work was done during my visit to MD and RA at JHU. This research was supported in part by NSF funding IIS-1546482.6https: / / github.com / Gorov / LowRankFCM."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Exploring compositional architectures and word vector representations for prepositional phrase attachment", "author": ["Tao Lei", "Regina Barzilay", "Amir Globerson"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Belinkov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Belinkov et al\\.", "year": 2014}, {"title": "Neural probabilistic language models", "author": ["Bengio et al.2006] Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain"], "venue": "In Innovations in Machine Learning", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "A semantic matching energy function for learning with multirelational data", "author": ["Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "Machine Learning", "citeRegEx": "Bordes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Online learning in tensor space. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Cao", "Khudanpur2014] Yuan Cao", "Sanjeev Khudanpur"], "venue": null, "citeRegEx": "Cao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2014}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Improved relation extraction with feature-rich compositional embedding models", "author": ["Mo Yu", "Mark Dredze"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Gormley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gormley et al\\.", "year": 2015}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["Hermann", "Blunsom2013] Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Association for Computational Linguistics", "citeRegEx": "Hermann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2013}, {"title": "Semantic frame identification with distributed word representations", "author": ["Dipanjan Das", "Jason Weston", "Kuzman Ganchev"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Tensor decompositions and applications", "author": ["Kolda", "Bader2009] Tamara G Kolda", "Brett W Bader"], "venue": "SIAM review,", "citeRegEx": "Kolda et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolda et al\\.", "year": 2009}, {"title": "Simple semi-supervised dependency parsing", "author": ["Koo et al.2008] Terry Koo", "Xavier Carreras", "Michael Collins"], "venue": "In Proceedings of ACL", "citeRegEx": "Koo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Low-rank tensors for scoring dependency structures. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Lei et al.2014] Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola"], "venue": null, "citeRegEx": "Lei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "High-order low-rank tensors for semantic role labeling", "author": ["Lei et al.2015] Tao Lei", "Yuan Zhang", "Llu\u0131\u0301s M\u00e0rquez", "Alessandro Moschitti", "Regina Barzilay"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "A dependency-based neural network for relation classification", "author": ["Liu et al.2015] Yang Liu", "Furu Wei", "Sujian Li", "Heng Ji", "Ming Zhou", "Houfeng WANG"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Ma et al.2015] Mingbo Ma", "Liang Huang", "Bowen Zhou", "Bing Xiang"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Effective self-training for parsing", "author": ["Eugene Charniak", "Mark Johnson"], "venue": "In Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computa-", "citeRegEx": "McClosky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Name tagging with word clusters and discriminative training", "author": ["Miller et al.2004] Scott Miller", "Jethran Guinness", "Alex Zamanian"], "venue": "In Proceedings of HLT-NAACL", "citeRegEx": "Miller et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2004}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Lapata2010] Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "Employing word representations and regularization for domain adaptation of relation extraction", "author": ["Nguyen", "Grishman2014] Thien Huu Nguyen", "Ralph Grishman"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Leveraging preposition ambiguity to assess representation of semantic interaction in cdsm", "author": ["Ritter et al.2014] Samuel Ritter", "Cotie Long", "Denis Paperno", "Marco Baroni", "Matthew Botvinick", "Adele Goldberg"], "venue": "In NIPS Workshop on Learning Semantics", "citeRegEx": "Ritter et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2014}, {"title": "Composition of word representations improves semantic role labelling", "author": ["Roth", "Woodsend2014] Michael Roth", "Kristian Woodsend"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Roth et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2014}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of EMNLP-CoNLL", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of ACL", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts"], "venue": "Proceedings of EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Learning distributed representations for structured output prediction", "author": ["Srikumar", "Manning2014] Vivek Srikumar", "Christopher D Manning"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Srikumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srikumar et al\\.", "year": 2014}, {"title": "Semi-supervised relation extraction with large-scale word clustering", "author": ["Sun et al.2011] Ang Sun", "Ralph Grishman", "Satoshi Sekine"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Template kernels for dependency parsing", "author": ["Yoav Goldberg", "Amir Globerson"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Taub.Tabib et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Taub.Tabib et al\\.", "year": 2015}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Association for Computational Linguistics", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Unsupervised multi-domain adaptation with feature embeddings", "author": ["Yang", "Eisenstein2015] Yi Yang", "Jacob Eisenstein"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Learning composition models for phrase embeddings", "author": ["Yu", "Dredze2015] Mo Yu", "Mark Dredze"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Combining word embeddings and feature embeddings for fine-grained relation extraction. In North American Chapter of the Association for Computational Linguistics (NAACL)", "author": ["Yu et al.2015] Mo Yu", "Matthew R. Gormley", "Mark Dredze"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Enforcing structural diversity in cubepruned dependency parsing", "author": ["Zhang", "McDonald2014] Hao Zhang", "Ryan McDonald"], "venue": "In Proceedings of ACL", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou et al.2005] GuoDong Zhou", "Jian Su", "Jie Zhang", "Min Zhang"], "venue": "In Proceedings of ACL", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al.", "startOffset": 231, "endOffset": 252}, {"referenceID": 19, "context": ", 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014).", "startOffset": 72, "endOffset": 197}, {"referenceID": 12, "context": ", 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014).", "startOffset": 72, "endOffset": 197}, {"referenceID": 30, "context": ", 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014).", "startOffset": 72, "endOffset": 197}, {"referenceID": 28, "context": ", 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014).", "startOffset": 72, "endOffset": 197}, {"referenceID": 10, "context": ", 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014).", "startOffset": 72, "endOffset": 197}, {"referenceID": 33, "context": "eters by approximating the parameter tensor with a low-rank tensor: the Tucker approximation of Yu et al. (2015) but applied to each embedding type (view), or the Canonical/Parallel-Factors Decomposition (CP).", "startOffset": 96, "endOffset": 113}, {"referenceID": 8, "context": "Compared to our previous work (Gormley et al., 2015; Yu et al., 2015), this work allows for higherorder interactions, mixed-length n-gram features, lower-rank representations.", "startOffset": 30, "endOffset": 69}, {"referenceID": 33, "context": "Compared to our previous work (Gormley et al., 2015; Yu et al., 2015), this work allows for higherorder interactions, mixed-length n-gram features, lower-rank representations.", "startOffset": 30, "endOffset": 69}, {"referenceID": 13, "context": "(Lei et al., 2014; Lei et al., 2015), the proposed factorization has the following advantages:", "startOffset": 0, "endOffset": 36}, {"referenceID": 14, "context": "(Lei et al., 2014; Lei et al., 2015), the proposed factorization has the following advantages:", "startOffset": 0, "endOffset": 36}, {"referenceID": 13, "context": "Note that Lei et al. (2014) uses embeddings by concatenating them to specific views, which increases dimensionality, but the improvement is limited.", "startOffset": 10, "endOffset": 28}, {"referenceID": 18, "context": "To alleviate this problem, we use pre-trained continuous word embeddings (Mikolov et al., 2013) as input embeddings rather than the one-hot word encodings.", "startOffset": 73, "endOffset": 95}, {"referenceID": 29, "context": "Taub-Tabib et al. (2015) have different kernel functions for different order of dependency features.", "startOffset": 0, "endOffset": 25}, {"referenceID": 7, "context": "We use AdaGrad (Duchi et al., 2011) and apply L2 regularization on all Wis and g, except for the case of ri=di, where we will start with Wi = I and regularize with \u2016Wi I\u20162.", "startOffset": 15, "endOffset": 35}, {"referenceID": 1, "context": "We use the same word embeddings in Belinkov et al. (2014) on PP-attachment for a fair comparison.", "startOffset": 35, "endOffset": 58}, {"referenceID": 1, "context": "We use the same word embeddings in Belinkov et al. (2014) on PP-attachment for a fair comparison. For the other experiments, we use the same 200-d word embeddings in Yu et al. (2015). Relation Extraction We use the English portion of the ACE 2005 relation extraction dataset (Walker et al.", "startOffset": 35, "endOffset": 183}, {"referenceID": 1, "context": "We use the same word embeddings in Belinkov et al. (2014) on PP-attachment for a fair comparison. For the other experiments, we use the same 200-d word embeddings in Yu et al. (2015). Relation Extraction We use the English portion of the ACE 2005 relation extraction dataset (Walker et al., 2006). Following Yu et al. (2015), we use both gold entity spans and types, train the model on the news domain and test on the broadcast conversation domain.", "startOffset": 35, "endOffset": 325}, {"referenceID": 33, "context": "We use the same feature templates and evaluate on finegrained relations (sub-types, 32 labels) (Yu et al., 2015).", "startOffset": 95, "endOffset": 112}, {"referenceID": 27, "context": "We compare to two baseline methods: 1) a loglinear model with a rich binary feature set from Sun et al. (2011) and Zhou et al.", "startOffset": 93, "endOffset": 111}, {"referenceID": 27, "context": "We compare to two baseline methods: 1) a loglinear model with a rich binary feature set from Sun et al. (2011) and Zhou et al. (2005) as described in Yu et al.", "startOffset": 93, "endOffset": 134}, {"referenceID": 27, "context": "We compare to two baseline methods: 1) a loglinear model with a rich binary feature set from Sun et al. (2011) and Zhou et al. (2005) as described in Yu et al. (2015) (BASELINE); 2) the embedding model (FCM) of Gormley et al.", "startOffset": 93, "endOffset": 167}, {"referenceID": 8, "context": "(2015) (BASELINE); 2) the embedding model (FCM) of Gormley et al. (2015), which uses rich linguistic features for relation extraction.", "startOffset": 51, "endOffset": 73}, {"referenceID": 1, "context": "PP-attachment We consider the prepositional phrase (PP) attachment task of Belinkov et al. (2014),3 where for each PP the correct head (verbs or nouns) must be selected from content words before the PP (within a 10-word window).", "startOffset": 75, "endOffset": 98}, {"referenceID": 1, "context": "We extract a dev set from section 22 of the PTB following the description in Belinkov et al. (2014).", "startOffset": 77, "endOffset": 100}, {"referenceID": 22, "context": "Preposition Disambiguation We consider the preposition disambiguation task proposed by Ritter et al. (2014). The task is to determine the spatial relationship a preposition indicates based on the two objects connected by the preposition.", "startOffset": 87, "endOffset": 108}, {"referenceID": 31, "context": "Task Benchmark Dataset Numbers on Each View #Labels (d1) #Non-lexical Features (d2) Relation Extraction Yu et al. (2015) ACE 2005 32 264 PP-attachment Belinkov et al.", "startOffset": 104, "endOffset": 121}, {"referenceID": 1, "context": "(2015) ACE 2005 32 264 PP-attachment Belinkov et al. (2014) WSJ 1,213 / 607 Preposition Disambiguation Ritter et al.", "startOffset": 37, "endOffset": 60}, {"referenceID": 1, "context": "(2015) ACE 2005 32 264 PP-attachment Belinkov et al. (2014) WSJ 1,213 / 607 Preposition Disambiguation Ritter et al. (2014) Ritter et al.", "startOffset": 37, "endOffset": 124}, {"referenceID": 1, "context": "(2015) ACE 2005 32 264 PP-attachment Belinkov et al. (2014) WSJ 1,213 / 607 Preposition Disambiguation Ritter et al. (2014) Ritter et al. (2014) 6 9/3", "startOffset": 37, "endOffset": 145}, {"referenceID": 33, "context": "Table 2: Up-left: Unigram lexical features (only showing non-lexical parts) for relation extraction (from Yu et al. (2014)).", "startOffset": 106, "endOffset": 123}, {"referenceID": 22, "context": "We include three baselines: point-wise addition (SUM) (Mitchell and Lapata, 2010), concatenation (Ritter et al., 2014), and an SVM based on handcrafted features in Table 2.", "startOffset": 97, "endOffset": 118}, {"referenceID": 1, "context": "Belinkov et al. (2014) also reported results of parsers and parser re-rankers, which can access to additional resources (complete parses for training and complete sentences as inputs) so it is unfair to compare them with the standalone systems like HPCD and our LRFR.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "System Resources Used Acc SVM (Belinkov et al., 2014) distance, word, embedding, clusters, POS, WordNet, VerbNet 86.", "startOffset": 30, "endOffset": 53}, {"referenceID": 1, "context": "0 HPCD (Belinkov et al., 2014) distance, embedding, POS, WordNet, VerbNet 88.", "startOffset": 7, "endOffset": 30}, {"referenceID": 13, "context": "RBG (Lei et al., 2014) dependency parser 88.", "startOffset": 4, "endOffset": 22}, {"referenceID": 17, "context": "4 Charniak-RS (McClosky et al., 2006) dependency parser + re-ranker 88.", "startOffset": 14, "endOffset": 37}, {"referenceID": 1, "context": "The baseline results are from Belinkov et al. (2014).", "startOffset": 30, "endOffset": 53}, {"referenceID": 22, "context": "Preposition Disambiguation LRFR improves (Table 5) over the best methods (SUM and Concatenation) in Ritter et al. (2014) as well as the SVM", "startOffset": 100, "endOffset": 121}, {"referenceID": 12, "context": "This is equivalent to an SVM with the compound cluster features as in Koo et al. (2008). It performs much worse than LRFR1-BROWN, showing the advantage of using word embeddings and low-rank tensors.", "startOffset": 70, "endOffset": 88}, {"referenceID": 31, "context": "Dimensionality Reduction for Complex Features is a standard technique to address high-dimensional features, including PCA, alternating structural optimization (Ando and Zhang, 2005), denoising autoencoders (Vincent et al., 2008), and feature embeddings (Yang and Eisenstein, 2015).", "startOffset": 206, "endOffset": 228}, {"referenceID": 9, "context": "Koo et al. (2008), Turian et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Koo et al. (2008), Turian et al. (2010), Sun et al.", "startOffset": 0, "endOffset": 40}, {"referenceID": 9, "context": "Koo et al. (2008), Turian et al. (2010), Sun et al. (2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al.", "startOffset": 0, "endOffset": 59}, {"referenceID": 9, "context": "Koo et al. (2008), Turian et al. (2010), Sun et al. (2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al.", "startOffset": 0, "endOffset": 87}, {"referenceID": 9, "context": "Koo et al. (2008), Turian et al. (2010), Sun et al. (2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al.", "startOffset": 0, "endOffset": 113}, {"referenceID": 8, "context": "(2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al. (2014) used pre-trained word embeddings to replace the lexical parts of features ; Srikumar and Manning (2014), Gormley et al.", "startOffset": 66, "endOffset": 88}, {"referenceID": 8, "context": "(2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al. (2014) used pre-trained word embeddings to replace the lexical parts of features ; Srikumar and Manning (2014), Gormley et al.", "startOffset": 66, "endOffset": 192}, {"referenceID": 8, "context": "(2014) used pre-trained word embeddings to replace the lexical parts of features ; Srikumar and Manning (2014), Gormley et al. (2015) and Yu et al.", "startOffset": 112, "endOffset": 134}, {"referenceID": 8, "context": "(2014) used pre-trained word embeddings to replace the lexical parts of features ; Srikumar and Manning (2014), Gormley et al. (2015) and Yu et al. (2015) propose splitting lexical features into different parts and employing tensors to perform classification.", "startOffset": 112, "endOffset": 155}, {"referenceID": 6, "context": "Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b).", "startOffset": 113, "endOffset": 201}, {"referenceID": 3, "context": "Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b).", "startOffset": 113, "endOffset": 201}, {"referenceID": 24, "context": "Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b).", "startOffset": 113, "endOffset": 201}, {"referenceID": 16, "context": "dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguistic features.", "startOffset": 17, "endOffset": 52}, {"referenceID": 15, "context": "dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguistic features.", "startOffset": 17, "endOffset": 52}, {"referenceID": 1, "context": "For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 1, "context": "For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.", "startOffset": 13, "endOffset": 147}, {"referenceID": 1, "context": "For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.", "startOffset": 13, "endOffset": 178}, {"referenceID": 13, "context": "Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014).", "startOffset": 95, "endOffset": 162}, {"referenceID": 13, "context": "Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014). Yu and Dredze (2015) proposed a model to compose phrase embeddings from words, which has an equivalent form of our CPbased method under certain restrictions.", "startOffset": 121, "endOffset": 185}], "year": 2016, "abstractText": "Modern NLP models rely heavily on engineered features, which often combine word and contextual information into complex lexical features. Such combination results in large numbers of features, which can lead to overfitting. We present a new model that represents complex lexical features\u2014comprised of parts for words, contextual information and labels\u2014in a tensor that captures conjunction information among these parts. We apply lowrank tensor approximations to the corresponding parameter tensors to reduce the parameter space and improve prediction speed. Furthermore, we investigate two methods for handling features that include n-grams of mixed lengths. Our model achieves state-of-the-art results on tasks in relation extraction, PPattachment, and preposition disambiguation.", "creator": "LaTeX with hyperref package"}}}