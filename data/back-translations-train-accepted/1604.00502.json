{"id": "1604.00502", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2016", "title": "Online Updating of Word Representations for Part-of-Speech Tagging", "abstract": "We propose online unsupervised domain adaptation (DA), which is performed incrementally as data comes in and is applicable when batch DA is not possible. In a part-of-speech (POS) tagging evaluation, we find that online unsupervised DA performs as well as batch DA.", "histories": [["v1", "Sat, 2 Apr 2016 13:52:23 GMT  (758kb,D)", "http://arxiv.org/abs/1604.00502v1", "EMNLP'2015. Released POS tagger \"FLORS\" for online domain adaptation"]], "COMMENTS": "EMNLP'2015. Released POS tagger \"FLORS\" for online domain adaptation", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin 0001", "tobias schnabel", "hinrich sch\u00fctze"], "accepted": true, "id": "1604.00502"}, "pdf": {"name": "1604.00502.pdf", "metadata": {"source": "CRF", "title": "Online Updating of Word Representations for Part-of-Speech Tagging", "authors": ["Wenpeng Yin", "Tobias Schnabel", "Hinrich Sch\u00fctze"], "emails": ["wenpeng@cis.lmu.de", "tbs49@cornell.edu", "inquiries@cislmu.org"], "sections": [{"heading": "1 Introduction", "text": "Unattended domain customization is a scenario that practitioners often face when they need to build robust NLP systems. They have flagged data in the source domain, but want to improve performance in the target domain by using unlabeled data alone. However, most work on unattended domain customization in NLP uses batch learning: it assumes that much of the unlabeled data is available in the target domain before testing. Batch learning is not possible in many real-world scenarios where incoming data from a new target domain needs to be processed immediately. More importantly, in many real-world scenarios, the data does not come with proper domain items, and it is not immediately obvious that an input stream is delivering data from a new domain. Consider an NLP system that analyzes e-mail in an organization."}, {"heading": "2 Experimental setup", "text": "Indeed, there are a number of reasons why it has to come this far to save the world. (...) There are many reasons why it has to come this far. (...) There are many reasons why it has to come this far. (...) There are many reasons why it has to come this far. (...) There are many reasons why it has to come this far. (...) There are many reasons why it has to come this far. (...) There are many reasons why it has to come this far. (...) There are many reasons why it has to come this far. (...)"}, {"heading": "3 Experimental results", "text": "In fact, most of us are able to play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...) We must play by the rules. (...)"}, {"heading": "3.1 Time course of tagging accuracy", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "4 Related work", "text": "Many supervised learning algorithms are online or have online versions. Active learning (Lewis and Gale, 1994; Tong and Koller, 2001; Laws et al., 2011) is another supervised learning framework that processes training examples - which are normally obtained interactively - in small batches (Bordes et al., 2005). All this work on supervised online learning is not directly relevant to this work as we address the issue of unsupervised DA. Unlike online supervised learners, we keep the statistical model unchanged during DA and adopt a representative learning approach: any unlabeled context of a word is used to update its representation. There is much work on the unsupervised DA for POS tagging, including work with restriction-based methods (Subramanya et al., 2010; Rush et al., 2012), such as weighting (Choi and Palmer, 2012), self-training (Huang et al, 2011 and Huang, 2006, and not for this Baumanya)."}, {"heading": "5 Conclusion", "text": "We have introduced the online update of word representations, a new method of domain adjustment for cases where target domain data is read from a stream and BATCH processing is not possible. We have shown that online unattended DA works just as well as batch learning and the error rate drops significantly compared to STATIC (i.e. no domain adjustment). Our implementation of FLORS is available at cistern.cis.lmu.de / florsAckledgments. This work was supported by a Baidu grant to Wenpeng Yin and by the German Research Foundation (DFG SCHU 2246 / 10-1 FADeBaC)."}], "references": [{"title": "Domain adaptation with structural correspondence learning", "author": ["Blitzer et al.2006] John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": "In EMNLP,", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Fast kernel classifiers with online and active learning", "author": ["Seyda Ertekin", "Jason Weston", "L\u00e9on Bottou"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bordes et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2005}, {"title": "Fast and robust part-of-speech tagging using dynamic model selection", "author": ["Choi", "Palmer2012] Jinho D. Choi", "Martha Palmer"], "venue": "In ACL: Short Papers,", "citeRegEx": "Choi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2012}, {"title": "Domain adaptation for statistical classifiers", "author": ["III Daum\u00e9", "III Marcu2006] Hal Daum\u00e9", "Marcu. Daniel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2006}, {"title": "Exploring representation-learning approaches to domain adaptation", "author": ["Huang", "Yates2010] Fei Huang", "Alexander Yates"], "venue": "In DANLP,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Improving a simple bigram HMM part-of-speech tagger by latent annotation and self-training", "author": ["Vladimir Eidelman", "Mary Harper"], "venue": "In NAACL-HLT: Short Papers,", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Fast domain adaptation for part of speech tagging for dialogues", "author": ["K\u00fcbler", "Baucom2011] Sandra K\u00fcbler", "Eric Baucom"], "venue": "In RANLP,", "citeRegEx": "K\u00fcbler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "K\u00fcbler et al\\.", "year": 2011}, {"title": "Active learning with Amazon Mechanical Turk", "author": ["Laws et al.2011] Florian Laws", "Christian Scheible", "Hinrich Sch\u00fctze"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Laws et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Laws et al\\.", "year": 2011}, {"title": "A sequential algorithm for training text classifiers", "author": ["Lewis", "Gale1994] David D. Lewis", "William A. Gale"], "venue": "In ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Lewis et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 1994}, {"title": "English gigaword fourth edition", "author": ["Robert Parker"], "venue": "Linguistic Data Consortium", "citeRegEx": "Parker.,? \\Q2009\\E", "shortCiteRegEx": "Parker.", "year": 2009}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Petrov", "McDonald2012] Slav Petrov", "Ryan McDonald"], "venue": "In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Improved parsing and POS tagging using intersentence consistency constraints", "author": ["Roi Reichart", "Michael Collins", "Amir Globerson"], "venue": "In EMNLPCoNLL,", "citeRegEx": "Rush et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2012}, {"title": "FLORS: Fast and simple domain adaptation for part-of-speech tagging", "author": ["Schnabel", "Sch\u00fctze2014] Tobias Schnabel", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Schnabel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2014}, {"title": "Efficient graph-based semi-supervised learning of structured tagging models", "author": ["Slav Petrov", "Fernando Pereira"], "venue": "In EMNLP,", "citeRegEx": "Subramanya et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Subramanya et al\\.", "year": 2010}, {"title": "Support vector machine active learning with applications to text classification", "author": ["Tong", "Koller2001] Simon Tong", "Daphne Koller"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Tong et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Tong et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "This is in contrast to other work, notably Blitzer et al. (2006), that carefully selects \u201cstable\u201d distributional features and discards \u201cunstable\u201d dis-", "startOffset": 43, "endOffset": 65}, {"referenceID": 9, "context": "We also include 100,000 WSJ sentences from 1988 and 500,000 sentences from Gigaword (Parker, 2009).", "startOffset": 84, "endOffset": 98}, {"referenceID": 7, "context": "Active learning (Lewis and Gale, 1994; Tong and Koller, 2001; Laws et al., 2011) is another supervised learning framework that processes training examples \u2013 usually obtained interactively \u2013 in small batches (Bordes et al.", "startOffset": 16, "endOffset": 80}, {"referenceID": 1, "context": ", 2011) is another supervised learning framework that processes training examples \u2013 usually obtained interactively \u2013 in small batches (Bordes et al., 2005).", "startOffset": 134, "endOffset": 155}, {"referenceID": 13, "context": "POS tagging, including work using constraintbased methods (Subramanya et al., 2010; Rush et al., 2012), instance weighting (Choi and Palmer, 2012), self-training (Huang et al.", "startOffset": 58, "endOffset": 102}, {"referenceID": 11, "context": "POS tagging, including work using constraintbased methods (Subramanya et al., 2010; Rush et al., 2012), instance weighting (Choi and Palmer, 2012), self-training (Huang et al.", "startOffset": 58, "endOffset": 102}, {"referenceID": 5, "context": ", 2012), instance weighting (Choi and Palmer, 2012), self-training (Huang et al., 2009; Huang and Yates, 2010), and co-training (K\u00fcbler and Baucom, 2011).", "startOffset": 67, "endOffset": 110}, {"referenceID": 4, "context": ", 2012), instance weighting (Choi and Palmer, 2012), self-training (Huang et al., 2009; Huang and Yates, 2010), and co-training (K\u00fcbler and Baucom, 2011). All of this work uses batch learning. For space reasons, we do not discuss supervised DA (e.g., Daum\u00e9 III and Marcu (2006)).", "startOffset": 68, "endOffset": 278}], "year": 2016, "abstractText": "We propose online unsupervised domain adaptation (DA), which is performed incrementally as data comes in and is applicable when batch DA is not possible. In a part-of-speech (POS) tagging evaluation, we find that online unsupervised DA performs as well as batch DA.", "creator": "LaTeX with hyperref package"}}}