{"id": "1604.00503", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2016", "title": "Discriminative Phrase Embedding for Paraphrase Identification", "abstract": "This work, concerning paraphrase identification task, on one hand contributes to expanding deep learning embeddings to include continuous and discontinuous linguistic phrases. On the other hand, it comes up with a new scheme TF-KLD-KNN to learn the discriminative weights of words and phrases specific to paraphrase task, so that a weighted sum of embeddings can represent sentences more effectively. Based on these two innovations we get competitive state-of-the-art performance on paraphrase identification.", "histories": [["v1", "Sat, 2 Apr 2016 13:57:02 GMT  (29kb)", "http://arxiv.org/abs/1604.00503v1", "NAACL'2015"]], "COMMENTS": "NAACL'2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "hinrich sch\\\"utze"], "accepted": true, "id": "1604.00503"}, "pdf": {"name": "1604.00503.pdf", "metadata": {"source": "CRF", "title": "Discriminative Phrase Embedding for Paraphrase Identification", "authors": ["Wenpeng Yin"], "emails": ["wenpeng@cis.lmu.de"], "sections": [{"heading": null, "text": "ar Xiv: 160 4.00 503v 1 [cs.C L] 2A pr"}, {"heading": "1 Introduction", "text": "In fact, it is that we are able to hide, and that we are able to hide, \"he said."}, {"heading": "2 Related work", "text": "We are now discussing relevant previous work based on the linguistic granularity of feature learning. The first line is compositional semantics, which learns representations for words and then composes them into sentence representations. Blacoe and Lapata (2012) conducted a comparative study of three word representation methods (simple distribution semantics (Mitchell and Lapata, 2010), distribution memory (Baroni and Lenci, 2010) and word embedding (Collobert and Weston, 2008), along with three compositional methods (addition, point multiplication and recursive autoencoders (Socher et al., 2011). They showed that word embedding addition is competitive, despite its simplicity and inaccuracy. The second category searches directly for elements at the sentence level. Ji and Eisenstein (2013) investigated metaphrams, metrams and dependencies."}, {"heading": "3 Embedding learning for units", "text": "As explained in Section 1, \"units\" in this work include individual words, continuous phrases, and discontinuous phrases. Phrases have a greater linguistic granularity than words, and therefore generally have more meanings for a sentence. For example, the successful detection of continuous phrases \"side effects\" and discontinuous phrases \"pick \u00b7 \u00b7 off\" is helpful in understanding the sentence meaning correctly. This section focuses on how to recognize phrases and how to represent them."}, {"heading": "3.1 Phrase collection", "text": "Phrases defined by a lexicon have not previously been extensively studied in depth learning. To collect canonical phrase sets, we extract two-word phrases defined in Wiktionary1 and Words1 (Miller and Fellbaum, 1998) to form a collection of 95,218. This collection contains consecutive phrases - phrases whose parts always occur side by side (e.g. \"side effects\") - and discontinuous phrases - phrases whose parts occur more often separately (e.g. \"pick... off\")."}, {"heading": "3.2 Identification of phrase continuity", "text": "Wiktionary and WordNet do not categorize phrases as continuous or discontinuous, so we need heuristics to automatically determine this. For each phrase \"A B,\" we calculate [c1, c2, c3, c4, c5], where ci, 1 \u2264 i \u2264 5, indicates that there are ci occurrences of A and B in this order with a space1http: / / en.wiktionary.org gof i. We calculate these statistics for a corpus consisting of the English gigaword (Graff et al., 2003) and Wikipedia. We set the maximum distance to 5, because discontinuous phrases rarely exceed 5 tokens. If c1 is 10 times higher than (c2 + c3 + c4 + c5) / 4, we classify \"A B\" as continuous, otherwise as discontinuous."}, {"heading": "3.3 Sentence reformatting", "text": "Sentence.. \"A.. B..\" is \u2022 reformatted as... \"A B..\" when A and B form a contiguous phrase and no word between them and \u2022 reformatted as... \"A B... A B.\" when A and B form a discontinuous phrase and are separated by 1 to 4 words. We replace each of the two contiguous words with A B to provide the context of both components of the phrase when learning. This method of phrase recognition will generate some false positives, e.g. when \"Pick\" and \"Off\" occur in a context such as \"she chose an island off the coast of Maine.\" However, our experimental results suggest that it is robust enough for our purposes. We run word2vec (Mikolov et al., 2013) on the reformatted Wikipedia corpus to learn embedding for all units. The embedding size is fixed at 200."}, {"heading": "4 Measure of unit discriminativity", "text": "We will present a set as the sum of the embeddings of its units. Building on Ji and Eisenstein (2013)'s TF-KLD, we want units according to their ability to distinguish two sets specific to the paraphrase task.TF-KLD assumes a training set of sentence pairs in the form < ui, vi, ti >, where ui and vi are the occurrence of units in the paraphrase task.TF-KLD is the gold mark. Then we define pk and qk as follows. \u2022 pk = P (uik | vik = 1, ti = 1) This is the probability that the unit wk occurs in the sentence ui, because wk occurs in its counterpart vi and they are paraphrases. \u2022 qk = P (uik | vik = 1, ti = 0).KP"}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data and baselines", "text": "Dre rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rf the rfu the rfu the rfu the rf the rfu the rfu the rfu the rfu the rf the rfu the rf the rf the rf the rf the rf the ru the r"}, {"heading": "5.2 Experimental results", "text": "Table 1 shows the performance for both the six methods and the majority basis. Interestingly, the method of Ji and Eisenstein (2013) performs best in terms of accuracy, surpassing (Ji and Eisenstein, 2013) um.009 (resp. 052). This is a deficiency in its approach. WORD + GOOGLE performs slightly worse than WORD + PHRASE; this suggests that linguistic phrases may be more effective in identifying paraphrases than statistical phrases. Cases as a whole and subsets suggest that phrase embeddings improve sentence representation. Overall, the accuracy of WORD + PHRASE is lower than subsentences, as WORD + PHRASE has no advantage over WORD without sentences."}, {"heading": "5.3 Effectiveness of TF-KLD-KNN", "text": "The main contribution of TF-KLD-KNN is that it achieves full coverage of feature weights in the face of the scarcity of data. We now compare four weighting methods on the overall corpus and with the combi-4https: / / code.google.com / p / word2vec / nation of MT features: NOWEIGHT, TF-IDF, TFKLD, TF-KLDTable 2 suggests that task-specific reweighting approaches (including TF-KLD and TF-KLD-KNN) are superior to non-specific systems (NOWEIGHT and TF-IDF)."}, {"heading": "5.4 Reweighting schemes for unseen units", "text": "We compare our reweighting scheme KNN (i.e. TFKLD-KNN) with three other reweighting schemes. Zero: Zero: Zero weight, i.e. ignore invisible units; Type average: take the average of the weights of all known unit types in the test set; Context average: average of the weights of the adjacent known units of the unknown unit (two, one or by default zero, depending on how many there are). Figure 1 shows that KNN performs best."}, {"heading": "6 Conclusion", "text": "In this thesis, TF-KLD-KNN was introduced, a new reweighting scheme that effectively learns the distinctive power of known and unknown units. We have further improved per-performance paraphrase identification by using continuous and discontinuous phrase embedding. In the future, we plan to conduct experiments in a cross-domain setup and improve our domain matching algorithm for paraphrase identification."}, {"heading": "Acknowledgments", "text": "We thank members of the CIS for comments on earlier versions of this paper. This work was supported by Baidu (through a Baidu scholarship to Wenpeng Yin) and the German Research Foundation (DFG SCHU 2246 / 8-2, SPP 1335)."}], "references": [{"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Computational Linguistics, 36(4):673\u2013 721.", "citeRegEx": "Baroni and Lenci.,? 2010", "shortCiteRegEx": "Baroni and Lenci.", "year": 2010}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Paraphrase identification as probabilistic quasi-synchronous recognition", "author": ["Dipanjan Das", "Noah A Smith."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Pro-", "citeRegEx": "Das and Smith.,? 2009", "shortCiteRegEx": "Das and Smith.", "year": 2009}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["Bill Dolan", "Chris Quirk", "Chris Brockett."], "venue": "Proceedings of the 20th international conference on Computational Linguistics, pages 350\u2013356. Association for", "citeRegEx": "Dolan et al\\.,? 2004", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin."], "venue": "The Journal of Machine Learning Research, 9:1871\u20131874.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Using machine translation evaluation techniques to determine sentence-level semantic equivalence", "author": ["Andrew Finch", "Young-Sook Hwang", "Eiichiro Sumita."], "venue": "Proceedings of the Third International Workshop on Paraphrasing (IWP2005), pages 17\u201324.", "citeRegEx": "Finch et al\\.,? 2005", "shortCiteRegEx": "Finch et al\\.", "year": 2005}, {"title": "English gigaword", "author": ["David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda."], "venue": "Linguistic Data Consortium, Philadelphia.", "citeRegEx": "Graff et al\\.,? 2003", "shortCiteRegEx": "Graff et al\\.", "year": 2003}, {"title": "Discriminative improvements to distributional sentence similarity", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Ji and Eisenstein.,? 2013", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2013}, {"title": "Paraphrase identification on the basis of supervised machine learning techniques", "author": ["Zornitsa Kozareva", "Andr\u00e9s Montoyo."], "venue": "Advances in natural language processing, pages 524\u2013533. Springer.", "citeRegEx": "Kozareva and Montoyo.,? 2006", "shortCiteRegEx": "Kozareva and Montoyo.", "year": 2006}, {"title": "Re-examining machine translation metrics for paraphrase identification", "author": ["Nitin Madnani", "Joel Tetreault", "Martin Chodorow."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Madnani et al\\.,? 2012", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: An electronic lexical database", "author": ["George Miller", "Christiane Fellbaum"], "venue": null, "citeRegEx": "Miller and Fellbaum.,? \\Q1998\\E", "shortCiteRegEx": "Miller and Fellbaum.", "year": 1998}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive science, 34(8):1388\u20131429.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "User\u2019s guide to sigf: Significance testing by approximate randomisation", "author": ["Sebastian Pad\u00f3"], "venue": null, "citeRegEx": "Pad\u00f3,? \\Q2006\\E", "shortCiteRegEx": "Pad\u00f3", "year": 2006}, {"title": "Paraphrase recognition via dissimilarity significance classification", "author": ["Long Qiu", "Min-Yen Kan", "Tat-Seng Chua."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 18\u201326. Association for Computational Lin-", "citeRegEx": "Qiu et al\\.,? 2006", "shortCiteRegEx": "Qiu et al\\.", "year": 2006}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennington", "Andrew Y Ng", "Christopher D Manning."], "venue": "Advances in Neural Information Processing Systems, volume 24, pages 801\u2013", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Paraphrase identification using semantic heuristic features", "author": ["Zia Ul-Qayyum", "Wasif Altaf."], "venue": "Research", "citeRegEx": "Ul.Qayyum and Altaf.,? 2012", "shortCiteRegEx": "Ul.Qayyum and Altaf.", "year": 2012}, {"title": "Using dependency-based features to take the para-farce out of paraphrase", "author": ["Stephen Wan", "Mark Dras", "Robert Dale", "C\u00e9cile Paris."], "venue": "Proceedings of the Australasian Language Technology Workshop, volume 2006, pages 131\u2013138.", "citeRegEx": "Wan et al\\.,? 2006", "shortCiteRegEx": "Wan et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 1, "context": ", Blacoe and Lapata (2012)), we compute the vector of a sentence as the sum of the vectors of its components.", "startOffset": 2, "endOffset": 27}, {"referenceID": 10, "context": "We use (Madnani et al., 2012) as our baseline", "startOffset": 7, "endOffset": 29}, {"referenceID": 4, "context": "(2012) gets state-of-the-art performance on the Microsoft Research Paraphrase (MSRP) corpus (Dolan et al., 2004).", "startOffset": 92, "endOffset": 112}, {"referenceID": 9, "context": "resentations, we compute three kinds of features to describe a pair of sentences \u2013 cosine similarity, element-wise sum and absolute element-wise difference \u2013 and show that combining them with the features from Madnani et al. (2012) gets state-of-the-art performance on the Microsoft Research Paraphrase (MSRP) corpus (Dolan et al.", "startOffset": 210, "endOffset": 232}, {"referenceID": 13, "context": "Blacoe and Lapata (2012) carried out a comparative study of three word representation methods (the simple distributional semantic space (Mitchell and Lapata, 2010), distributional memory tensor (Baroni and Lenci, 2010) and word embedding (Collobert and Weston, 2008)), along with three composition methods (addition, point-wise multiplication, and recursive autoencoder (Socher et al.", "startOffset": 136, "endOffset": 163}, {"referenceID": 0, "context": "Blacoe and Lapata (2012) carried out a comparative study of three word representation methods (the simple distributional semantic space (Mitchell and Lapata, 2010), distributional memory tensor (Baroni and Lenci, 2010) and word embedding (Collobert and Weston, 2008)), along with three composition methods (addition, point-wise multiplication, and recursive autoencoder (Socher et al.", "startOffset": 194, "endOffset": 218}, {"referenceID": 2, "context": "Blacoe and Lapata (2012) carried out a comparative study of three word representation methods (the simple distributional semantic space (Mitchell and Lapata, 2010), distributional memory tensor (Baroni and Lenci, 2010) and word embedding (Collobert and Weston, 2008)), along with three composition methods (addition, point-wise multiplication, and recursive autoencoder (Socher et al.", "startOffset": 238, "endOffset": 266}, {"referenceID": 16, "context": "Blacoe and Lapata (2012) carried out a comparative study of three word representation methods (the simple distributional semantic space (Mitchell and Lapata, 2010), distributional memory tensor (Baroni and Lenci, 2010) and word embedding (Collobert and Weston, 2008)), along with three composition methods (addition, point-wise multiplication, and recursive autoencoder (Socher et al., 2011)).", "startOffset": 370, "endOffset": 391}, {"referenceID": 0, "context": "Blacoe and Lapata (2012) carried out a comparative study of three word representation methods (the simple distributional semantic space (Mitchell and Lapata, 2010), distributional memory tensor (Baroni and Lenci, 2010) and word embedding (Collobert and Weston, 2008)), along with three composition methods (addition, point-wise multiplication, and recursive autoencoder (Socher et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 8, "context": "Ji and Eisenstein (2013) explored unigrams, bigrams and dependency pairs as sentence features.", "startOffset": 0, "endOffset": 25}, {"referenceID": 18, "context": "Wan et al. (2006) used N-gram overlap,", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "Finch et al. (2005) and Madnani et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Finch et al. (2005) and Madnani et al. (2012) combined several machine translation metrics.", "startOffset": 0, "endOffset": 46}, {"referenceID": 3, "context": "Das and Smith (2009) presented a generative model over two sentences\u2019 dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given sentence.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Das and Smith (2009) presented a generative model over two sentences\u2019 dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given sentence. Socher et al. (2011) used recursive autoencoders to learn representations for words and word sequences on each layer of the sentence parsing tree, and then proposed dynamic pooling layer to form a fixed-size matrix as the representation of the two sentences.", "startOffset": 0, "endOffset": 250}, {"referenceID": 3, "context": "Das and Smith (2009) presented a generative model over two sentences\u2019 dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given sentence. Socher et al. (2011) used recursive autoencoders to learn representations for words and word sequences on each layer of the sentence parsing tree, and then proposed dynamic pooling layer to form a fixed-size matrix as the representation of the two sentences. Other work representative of this line is by Kozareva and Montoyo (2006), Qiu et al.", "startOffset": 0, "endOffset": 561}, {"referenceID": 3, "context": "Das and Smith (2009) presented a generative model over two sentences\u2019 dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given sentence. Socher et al. (2011) used recursive autoencoders to learn representations for words and word sequences on each layer of the sentence parsing tree, and then proposed dynamic pooling layer to form a fixed-size matrix as the representation of the two sentences. Other work representative of this line is by Kozareva and Montoyo (2006), Qiu et al. (2006), Ul-Qayyum and Altaf (2012).", "startOffset": 0, "endOffset": 580}, {"referenceID": 3, "context": "Das and Smith (2009) presented a generative model over two sentences\u2019 dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given sentence. Socher et al. (2011) used recursive autoencoders to learn representations for words and word sequences on each layer of the sentence parsing tree, and then proposed dynamic pooling layer to form a fixed-size matrix as the representation of the two sentences. Other work representative of this line is by Kozareva and Montoyo (2006), Qiu et al. (2006), Ul-Qayyum and Altaf (2012).", "startOffset": 0, "endOffset": 608}, {"referenceID": 12, "context": "To collect canonical phrase set, we extract two-word phrases defined in Wiktionary1 and Wordnet (Miller and Fellbaum, 1998) to form a collection of size 95,218.", "startOffset": 96, "endOffset": 123}, {"referenceID": 7, "context": "We compute these statistics for a corpus consisting of English Gigaword (Graff et al., 2003) and Wikipedia.", "startOffset": 72, "endOffset": 92}, {"referenceID": 11, "context": "We run word2vec (Mikolov et al., 2013) on the reformatted Wikipedia corpus to learn embeddings for all units.", "startOffset": 16, "endOffset": 38}, {"referenceID": 8, "context": "Building on Ji and Eisenstein (2013)\u2019s TF-KLD, we want to weight units according to their ability to discriminate two sentences specific to the paraphrase task.", "startOffset": 12, "endOffset": 37}, {"referenceID": 4, "context": "We use the MSRP corpus (Dolan et al., 2004) for evaluation.", "startOffset": 23, "endOffset": 43}, {"referenceID": 1, "context": "Following Blacoe and Lapata (2012), we simply represent a sentence as the unweighted sum of the embeddings of all its units.", "startOffset": 10, "endOffset": 35}, {"referenceID": 10, "context": "\u2022 MT is the method proposed by Madnani et al. (2012): the sentence pair is represented as a vector of eight different machine translation metrics.", "startOffset": 31, "endOffset": 53}, {"referenceID": 8, "context": "\u2022 Ji and Eisenstein (2013). We reimplemented their \u201cinductive\u201d setup which is based on matrix factorization and is the top-performing system in paraphrasing task.", "startOffset": 2, "endOffset": 27}, {"referenceID": 11, "context": "Mikolov et al. (2013) use a data-driven method to detect statistical phrases which are mostly continuous bigrams.", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "We use LIBLINEAR (Fan et al., 2008) as our linear SVM implementation.", "startOffset": 17, "endOffset": 35}, {"referenceID": 8, "context": "subset) setup, WORD+PHRASE performs best and outperforms (Ji and Eisenstein, 2013) by .", "startOffset": 57, "endOffset": 82}, {"referenceID": 8, "context": "subset) setup, WORD+PHRASE performs best and outperforms (Ji and Eisenstein, 2013) by .009 (resp. .052) on accuracy. Interestingly, Ji and Eisenstein (2013)\u2019s method obtains worse performance on subset.", "startOffset": 58, "endOffset": 157}, {"referenceID": 8, "context": "839 Ji and Eisenstein (2013) .", "startOffset": 4, "endOffset": 29}, {"referenceID": 14, "context": "Significant improvements over MT are marked with \u2217 (approximate randomization test, Pad\u00f3 (2006), p < .", "startOffset": 84, "endOffset": 96}], "year": 2016, "abstractText": "This work, concerning paraphrase identification task, on one hand contributes to expanding deep learning embeddings to include continuous and discontinuous linguistic phrases. On the other hand, it comes up with a new scheme TF-KLD-KNN to learn the discriminative weights of words and phrases specific to paraphrase task, so that a weighted sum of embeddings can represent sentences more effectively. Based on these two innovations we get competitive state-of-the-art performance on paraphrase identification.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}