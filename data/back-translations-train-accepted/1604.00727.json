{"id": "1604.00727", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Character-Level Question Answering with Attention", "abstract": "We show that an encoder-decoder framework can be successfully be applied to question-answering with a structured knowledge base. In addition, we propose a new character-level modeling approach for this task, which we use to make our model robust to unseen entities and predicates. We use our model for single-relation question answering, and demonstrate the effectiveness of our novel approach on the SimpleQuestions dataset, where we improve state-of-the-art accuracy by 2% for both Freebase2M and Freebase5M subsets proposed. Importantly, we achieve these results even though our character-level model has 16x less parameters than an equivalent word-embedding model, uses significantly less training data than previous work which relies on data augmentation, and encounters only 1.18% of the entities seen during training when testing.", "histories": [["v1", "Mon, 4 Apr 2016 02:43:23 GMT  (646kb,D)", "http://arxiv.org/abs/1604.00727v1", null], ["v2", "Tue, 5 Apr 2016 23:09:31 GMT  (647kb,D)", "http://arxiv.org/abs/1604.00727v2", null], ["v3", "Fri, 8 Apr 2016 21:12:47 GMT  (647kb,D)", "http://arxiv.org/abs/1604.00727v3", null], ["v4", "Sun, 5 Jun 2016 02:02:10 GMT  (9011kb,D)", "http://arxiv.org/abs/1604.00727v4", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["xiaodong he", "david golub"], "accepted": true, "id": "1604.00727"}, "pdf": {"name": "1604.00727.pdf", "metadata": {"source": "CRF", "title": "Character-Level Question Answering with Attention", "authors": ["David Golub", "Xiaodong He"], "emails": ["golubd@cs.washington.edu", "xiaohe@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we are able to hide, in the way we do it and in the way we do it."}, {"heading": "2 Related Work", "text": "Our work builds on three important threads of machine learning research: Semantic-parsing for open-domain question answering, character-level modeling, and sequence to sequence learning.Semantic parsing for open-domain question answering, which translated a question into a strucbed-base query, is a key component in question answer with a knowledge base as Freebase. While early approaches are based on building high-quality lexicon for domain-specific databases such as GeoQuery (Tang and Mooney, 2001), recent work focuses on building semantic parsing frameworks for general knowledge bases such as Freebase, (Yih et al., 2014), (Bordes et al.), (Bordes et al., 2015), (Berant and Liang, 2014), (Fader et al, al al al al."}, {"heading": "3 Model", "text": "In other words, our model assigns a conditional probability of predicting the next predicate or entity yt if an input query q and all previously predicted entries are given to the knowledge base y0, \u00b7 \u00b7 \u00b7, yt \u2212 1. It is therefore important to note that since our model focuses on answering individual relation questions in this work, each question is decoded into exactly 2 issues - the source unit and the predicate. Consequently, the goal of our model is to generate a single topic unit and a predicate from the freebase, such as (Barack Obama, / People / Person / Place of Birth), which can then be used to determine the facts.Our end-to-end framework consists of two components. As (Bordes et al., 2015) we have an approximate entity linking the entity that provides the list of possible predicates and entities."}, {"heading": "3.1 Representing Predicates and Entities", "text": "In a knowledge like Freebase, entities are typically commented with rich information such as descriptions, aliases in different languages and types. However, in order to keep the experimental setup consistent with previous work such as (Bordes et al., 2015) and (Yih et al., 2014), we only generate their English aliases for each predicate and entity in Freebase. For example, to create an embedding for the entity Freebase / m / 02mjmr, we would take the characters of their English alias Barack Obama and feed them into our system. However, we incorporate additional information from Freebase, such as entity descriptions for our linkage system, to improve overall precision, such as previous work in this area (Ling et al., 2015), (Lin et al., 2012), (Liang et al., 2013)."}, {"heading": "3.2 Prediction System", "text": "Our end-to-end prediction system is a function f (q, e1.. n, p1.. m) that dictates as input a question q, a list of candidates e1.. n, a list of candidates p1.. m and generates a probability value p (ei, pk | q) for seeing unit i and predicate j for all i-1.. n, j-1.. m. It consists of four components: 1. An encoder for the question that generates an embedding vector for each character (Figure 1a).2. An encoder for the predicates / units in a knowledge base that generates a single embedding vector of a fixed size N (Figure 2).3. An LSTM with attention mechanisms whose output (Figure 1e) is also a single vector of size N (Figure 1d).4. A semantic relevance layer on the top of the decoder STM, the step 1e to the predictor in relation to the first time and its effect."}, {"heading": "3.3 Embeddings", "text": "If we leave the embedding matrix E and the characters in a string y1.. n, the output of the embedding for the ith character is E \u00b7 yi, where yi is a uniform encoding for the respective character. We learn separate embedding matrices Ee, Ep, Eq for entity, predicate, and question mark. This helps our model to grasp the fact that different characters can have different meanings depending on whether they originate from an entity, predicate, or question."}, {"heading": "3.4 Question representation", "text": "We feed the embedding of the question into a two-layer gated feedback LSTM as described in (Chung et al., 2015) (Figure 1a) and take the outputs in each step as final embedding for the question. These are the vectors h1... hn in Figure 1 and are the context for our attention LSTM."}, {"heading": "3.5 Predicate/Entity representation", "text": "To generate the latent semantic representation of the entity / predicates, we use a two-layer Convolutionary Neural Network (CNN) via character-level embedding followed by a max pooling layer, similar to (Yih et al., 2014).The general meaning of a predicate or entity is often determined by a few keywords in its English alias, and the construction of the CNN + max pooling layer allows us to extract the most prominent local characteristics while simultaneously forming a global feature vector of constant length. As shown in Figure 2, we feed character-level embedding for all candidate units / predicates through a temporary revolutionary neural network: f (x1... n) = tanh (W3 \u0445 max (tanh (W2 \u0445 conv (tanhW1 \u0445 conv (x1.. n)))))))) Where the number of characters is, the maximum network layer is (xf), the size is N (n)."}, {"heading": "3.6 Attention-based LSTM", "text": "For our attention-based LSTM, we use the same architecture as described in (Bahdanau et al., 2014) (Figure 1d) (Figure 1d). We note that the inclusion of an attention mechanism slightly increases performance over the mean pooling and allows us to see that the soft alignments generated by our model are reasonable. At each step, we feed a context vector (Figure 1c) and an input into the LSTM (Figure 1g). To generate the context vector, an attention mechanism is used via the question embeddings (Figure 1b). Let si \u2212 1 recalculate the hidden state of the LSTM at the time i \u2212 1, and hj embed the Jth question character. Then, the context vector ci, which represents the entire question, is recalculated by the alignment model at each step, as follows: Tci = Texx = Vextanp."}, {"heading": "3.7 Prediction", "text": "Figure 2 illustrates how our model generates predictions. If we allow the new hidden states of attention LSTM at times of 1, 2, o1 or o2 (Figure 1e), we want to maximize the following values: exp (\u03bb \u043a cos (o1, yc)) \u2211 n i = 1 exp (\u043c cos (o1, yi))) * exp (\u03bb \u0445 cos (o2, zc)) \u2211 n i = 1 exp (\u043c cos (o2, zi))), where \u03bb is a constant and c is the index of the correct predicate / entity. A similar optimization goal has been used to train the semantic similarity modules proposed in (Yih et al., 2014) and (Yih et al., 2015). During the prediction time, yi and zi are all units and predicates returned by our entity linking system during the amplification time."}, {"heading": "3.8 Entity Linking", "text": "For linking entities, we are experimenting with the official Freebase API as well as with an approximate linkage system, which we build on the Freebase5M / Freebase2M subsets (Bordes et al., 2015). For the FreebaseAPI, we split the question into 1-5 ngrams and call up all facts where the theme unit matches one of the questionnaires. For the Freebase5M and Freebase2M subsets, we store all English entity names and descriptions in a full-text search system and call up a subset of triples whose English name or description matches a query. The triples are ordered by tf-idf valuations.To generate our queries for the Freebase5M / 2M subsets, we just feed a single fragment into our entity linkage system."}, {"heading": "4 Dataset and Experimental Settings", "text": "We evaluate our models based on the SimpleQuestions dataset (Bordes et al., 2015), which consists of 108,442 individual questions and the associated source, predicate, target and triple questions from Freebase. The dataset is divided into 21,687 tests, 10,845 validation and 75,910 move questions, respectively. Only 216 of the 18,363 unique units observed during the test were seen during the training, and 886 of 1034 predicates."}, {"heading": "4.1 Preprocessing and setup", "text": "We do not use data magnification and only use the 76k questions for training and 21k questions for testing. We use RMS Prop with a learning rate of 1e \u2212 4, embedding at the character level of 200, and all of our LSTMs have hidden / memory states of size 100. We do not use regularization and train our models on a standard Telsa GPU for 4-7 days and 19-30 epochs. During decoding, we generate the predicate in the first step and the unit in the second step."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 End-to-end results on SimpleQuestions", "text": "In experiments where we use either the proposed FB2M or the proposed FB5M subsets, we obtain state-of-the-art results on SimpleQuestions with accuracies of 66.2% and 65.7%, respectively. In addition, our drawing model is compact and has Results Raw Question Reranked1.2M parameters as opposed to the 19.9M parameters of our word embedding model, a 16-fold reduction in size. We also find that using our model to filter out candidate ngrams to be sent to our entity linking system significantly increases precision, with an increase of 12% for top-1 results and 6% for top-50 results. Our results are summarized in Table 2, Table 3, and we have exemplary generated queries in Table 1. Our experimental results are slightly worse when we use FB5M as a knowledge base because all test queries are sampled from F2M."}, {"heading": "5.2 Ablation and word-embedding experiments", "text": "For our experiments in sections 5.3 and 5.4, we sampled 200 entities and predicates from the test set. We then insert the correct entity, dictate to these noise sets, and determine the accuracy of our model in predicting the gold predicate / entity from this combined set."}, {"heading": "5.3 Performance of word-embedding models", "text": "Our results with models showing different attention mechanisms and character-level embedding are summarized in Table 5 and Figure 3. We note that word-embedding models have considerable difficulty generalizing to invisible entities, and after 11 epochs are only able to accurately predict 40% of entities based on noise, with a final common accuracy of 34.01%. This shows the difficulty of generalizing word-embedding models to invisible entities, since many entities are represented by words that are rarely seen during training.In contrast, character-embedding models have no such problems and achieve an accuracy of 96.25% in predicting the correct entity during negative sample experiments. This empirically supports the hypothesis that character-embedding types capture the semantic similarity between entities in a knowledge base and in their natural language."}, {"heading": "5.4 Depth ablation study", "text": "In this experiment, we are conducting an ablation study to determine whether deeper models need to be used to create high-level embedding at the character level. Our results are summarized in Figure 4 and Table 6. In our ablation experiments, we are comparing the effectiveness of using a single-layer LSTM versus a multi-layer gated feedback LSTM to encode the question and a single-layer versus two-layer winding neural network to encode the predicate / entity. We find that a two-layer LSTM increases collective accuracy by more than 6%. Most of the accuracy gains are based on improved predicate prediction, possibly because they require a more complicated alignment mechanism than entities."}, {"heading": "5.5 Attention Mechanisms", "text": "We conduct experiments with four different attention mechanisms - pooling, pooling, pooling, pooling with fine grains, pooling with dropouts, and pooling with coarse grains. Our results are summarized in Figure 3, Tables 4, and 5. During our negative sample experiments, we found that coarse-grained attention models easily outperform those with medium-grain attention that outperform those with coarse-grained pooling and dropout. We postulate this because fine-grained attention is harder to optimize, despite the fact that it gives an LSTM more control over which parts of the question are to be addressed. We also visualize our attention via question marks for coarse-grained attention models. As shown in Table 4, we find that coarse-grained attention is able to learn where the predicates and entities are in question."}, {"heading": "6 Summary", "text": "Finally, we proposed a new attention-based encoder decoder model for answering questions with a structured knowledge base and demonstrated the promise of our approach to inclusion questions, where we achieved state-of-the-art results with the SimpleQuestions dataset. Furthermore, our compact model generalized well to invisible units, despite the fact that we used significantly less training data than previous work. In the future, we would like to expand our model to address multi-relationship questions and use our character coding to naturally deal with spelling mistakes that often occur in practice."}], "references": [{"title": "Deep speech 2: Endto-end speech recognition in english and mandarin", "author": ["Zhenyao Zhu"], "venue": null, "citeRegEx": "Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Zhu.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Semantic parsing via paraphrasing", "author": ["Jonathan Berant", "Percy Liang"], "venue": null, "citeRegEx": "Berant and Liang.,? \\Q2014\\E", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "Question answering with subgraph embeddings", "author": ["Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "CoRR, abs/1406.3676.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "CoRR, abs/1506.02075.", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1502.02367.", "citeRegEx": "Chung et al\\.,? 2015", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Anthony Fader", "Luke S Zettlemoyer", "Oren Etzioni."], "venue": "Citeseer.", "citeRegEx": "Fader et al\\.,? 2013", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Named entity recognition with character-level models", "author": ["Dan Klein", "Joseph Smarr", "Huy Nguyen", "Christopher D Manning."], "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 180\u2013183. Asso-", "citeRegEx": "Klein et al\\.,? 2003", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Learning dependency-based compositional semantics", "author": ["Percy Liang", "Michael I Jordan", "Dan Klein."], "venue": "Computational Linguistics, 39(2):389\u2013446.", "citeRegEx": "Liang et al\\.,? 2013", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "Entity linking at web scale. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pages 84\u201388", "author": ["Thomas Lin", "Oren Etzioni"], "venue": null, "citeRegEx": "Lin and Etzioni,? \\Q2012\\E", "shortCiteRegEx": "Lin and Etzioni", "year": 2012}, {"title": "Design challenges for entity linking", "author": ["Xiao Ling", "Sameer Singh", "Daniel S Weld."], "venue": "Transactions of the Association for Computational Linguistics, 3:315\u2013328.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Using multiple clause constructors in inductive logic programming for semantic parsing", "author": ["Lappoon R Tang", "Raymond J Mooney."], "venue": "Machine Learning: ECML 2001, pages 466\u2013477. Springer.", "citeRegEx": "Tang and Mooney.,? 2001", "shortCiteRegEx": "Tang and Mooney.", "year": 2001}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko."], "venue": "arXiv preprint arXiv:1412.4729.", "citeRegEx": "Venugopalan et al\\.,? 2014", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio."], "venue": "CoRR, abs/1502.03044.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Semantic parsing for single-relation question answering", "author": ["Wen-tau Yih", "Xiaodong He", "Christopher Meek."], "venue": "Citeseer.", "citeRegEx": "Yih et al\\.,? 2014", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Wen Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao."], "venue": "ACL.", "citeRegEx": "Yih et al\\.,? 2015", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1410.4615.", "citeRegEx": "Zaremba and Sutskever.,? 2014", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2014}, {"title": "Text understanding from scratch", "author": ["Xiang Zhang", "Yann LeCun."], "venue": "CoRR, abs/1502.01710.", "citeRegEx": "Zhang and LeCun.,? 2015", "shortCiteRegEx": "Zhang and LeCun.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "We use our model for singlerelation question answering, and demonstrate the effectiveness of our novel approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy by 2% for both Freebase2M and Freebase5M subsets proposed.", "startOffset": 143, "endOffset": 164}, {"referenceID": 15, "context": "In this paper we focus on single-relation factoid questions, which are one of the most common forms of questions found in search query logs and community question answering websites (Yih et al., 2014), (Fader et al.", "startOffset": 182, "endOffset": 200}, {"referenceID": 6, "context": ", 2014), (Fader et al., 2013).", "startOffset": 9, "endOffset": 29}, {"referenceID": 1, "context": "To address these challenges, we propose the use of an encoder-decoder framework with attention, which has already been successfully applied to machine translation, speech recognition, and image captioning (Bahdanau et al., 2014), (Amodei et al.", "startOffset": 205, "endOffset": 228}, {"referenceID": 14, "context": ", 2015), (Xu et al., 2015).", "startOffset": 9, "endOffset": 26}, {"referenceID": 12, "context": "While early approaches relied on building high-quality lexicons for domain-specific databases such as GeoQuery (Tang and Mooney, 2001), recent work has focused on building semantic parsing frameworks for general knowledge bases such as Freebase, (Yih et al.", "startOffset": 111, "endOffset": 134}, {"referenceID": 15, "context": "While early approaches relied on building high-quality lexicons for domain-specific databases such as GeoQuery (Tang and Mooney, 2001), recent work has focused on building semantic parsing frameworks for general knowledge bases such as Freebase, (Yih et al., 2014), (Bordes et al.", "startOffset": 246, "endOffset": 264}, {"referenceID": 3, "context": ", 2014), (Bordes et al., 2014), (Bordes et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 4, "context": ", 2014), (Bordes et al., 2015), (Berant and Liang, 2014), (Fader et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 2, "context": ", 2015), (Berant and Liang, 2014), (Fader et al.", "startOffset": 9, "endOffset": 33}, {"referenceID": 6, "context": ", 2015), (Berant and Liang, 2014), (Fader et al., 2013).", "startOffset": 35, "endOffset": 55}, {"referenceID": 3, "context": "To address this issue, recent work relies on producing high-level embeddings for predicates and entities in a knowledge base based off of their textual descriptions, (Bordes et al., 2014), (Bordes et al.", "startOffset": 166, "endOffset": 187}, {"referenceID": 4, "context": ", 2014), (Bordes et al., 2015), (Yih et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 15, "context": ", 2015), (Yih et al., 2014), (Yih et al.", "startOffset": 9, "endOffset": 27}, {"referenceID": 16, "context": ", 2014), (Yih et al., 2015).", "startOffset": 9, "endOffset": 27}, {"referenceID": 6, "context": "Consequently, they often rely on significant data augmentation from sources such as Paralex (Fader et al., 2013), which contains 18 million question-paraphrase pairs scraped from WikiAnswers, to have sufficient examples for each word they encounter.", "startOffset": 92, "endOffset": 112}, {"referenceID": 18, "context": "named entity recognition (Zhang and LeCun, 2015), (Chung et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 5, "context": "named entity recognition (Zhang and LeCun, 2015), (Chung et al., 2015), (Klein et al.", "startOffset": 50, "endOffset": 70}, {"referenceID": 7, "context": ", 2015), (Klein et al., 2003).", "startOffset": 9, "endOffset": 29}, {"referenceID": 5, "context": "Moreover, in (Chung et al., 2015) the authors show that gated-feedback LSTMs on top of on character-level embeddings can capture long-term dependencies in language modeling.", "startOffset": 13, "endOffset": 33}, {"referenceID": 11, "context": "First introduced in (Sutskever et al., 2014), in an encoder-decoder network, a source sequence is first encoded into a fixed-length vector which intuitively captures \u201cmeaning\u201d, and then decoded into a desired target sequence.", "startOffset": 20, "endOffset": 44}, {"referenceID": 13, "context": "This approach has been used successfully in diverse domains such as machine translation, image captioning, and executing programs (Venugopalan et al., 2014), (Bahdanau et al.", "startOffset": 130, "endOffset": 156}, {"referenceID": 1, "context": ", 2014), (Bahdanau et al., 2014), (Xu et al.", "startOffset": 9, "endOffset": 32}, {"referenceID": 14, "context": ", 2014), (Xu et al., 2015), (Zaremba and Sutskever, 2014).", "startOffset": 9, "endOffset": 26}, {"referenceID": 17, "context": ", 2015), (Zaremba and Sutskever, 2014).", "startOffset": 9, "endOffset": 38}, {"referenceID": 4, "context": "Like (Bordes et al., 2015), we have an approximate entity linking system that prunes the list of possible predicates and entities our model has to evaluate, and a prediction system which determines the most likely (entity, predicate) pair from the filtered list.", "startOffset": 5, "endOffset": 26}, {"referenceID": 4, "context": "However, to keep the experimental setup consistent with previous work such as (Bordes et al., 2015) and (Yih et al.", "startOffset": 78, "endOffset": 99}, {"referenceID": 15, "context": ", 2015) and (Yih et al., 2014), we generate highlevel embeddings for each predicate and entity in Freebase using only their English aliases.", "startOffset": 12, "endOffset": 30}, {"referenceID": 10, "context": "However, we do incorporate additional information from Freebase such as entity descriptions for our linking system to improve overall precision, like previous work in this area (Ling et al., 2015), (Lin et al.", "startOffset": 177, "endOffset": 196}, {"referenceID": 8, "context": ", 2012), (Liang et al., 2013).", "startOffset": 9, "endOffset": 29}, {"referenceID": 5, "context": "(Chung et al., 2015) (Figure 1a) and take the outputs at each time step as the final embeddings for the question.", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "To produce the latent semantic representations for the entity/predicates, we use a two-layer convolutional neural network (CNN) over character-level embeddings followed by a max-pooling layer, similarly to (Yih et al., 2014).", "startOffset": 206, "endOffset": 224}, {"referenceID": 1, "context": "For our attention-based LSTM we use the same architecture as described in (Bahdanau et al., 2014) (Figure 1d).", "startOffset": 74, "endOffset": 97}, {"referenceID": 15, "context": "A similar optimization objective was used to train the semantic similarity modules proposed in (Yih et al., 2014) and (Yih et al.", "startOffset": 95, "endOffset": 113}, {"referenceID": 16, "context": ", 2014) and (Yih et al., 2015).", "startOffset": 12, "endOffset": 30}, {"referenceID": 4, "context": "For entity linking, we experiment with the official Freebase API, as well as an approximate entity linking system we build on top of the Freebase5M/Freebase2M subsets we build similar to (Bordes et al., 2015).", "startOffset": 187, "endOffset": 208}, {"referenceID": 4, "context": "We evaluate our models on the SimpleQuestions dataset (Bordes et al., 2015) which consists of 108,442 single-relation questions and their corresponding source entity, predicate, target entity triples from Freebase.", "startOffset": 54, "endOffset": 75}, {"referenceID": 4, "context": "See (Bordes et al., 2015).", "startOffset": 4, "endOffset": 25}], "year": 2016, "abstractText": "We show that an encoder-decoder framework can be successfully be applied to question-answering with a structured knowledge base. In addition, we propose a new character-level modeling approach for this task, which we use to make our model robust to unseen entities and predicates. We use our model for singlerelation question answering, and demonstrate the effectiveness of our novel approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy by 2% for both Freebase2M and Freebase5M subsets proposed. Importantly, we achieve these results even though our character-level model has 16x less parameters than an equivalent word-embedding model, uses significantly less training data than previous work which relies on data augmentation, and encounters only 1.18% of the entities seen during training when testing.", "creator": "TeX"}}}