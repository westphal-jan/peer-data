{"id": "1604.00734", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks", "abstract": "A key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mention's context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).", "histories": [["v1", "Mon, 4 Apr 2016 03:58:31 GMT  (121kb,D)", "http://arxiv.org/abs/1604.00734v1", "Accepted at NAACL 2016"]], "COMMENTS": "Accepted at NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["matthew francis-landau", "greg durrett", "dan klein"], "accepted": true, "id": "1604.00734"}, "pdf": {"name": "1604.00734.pdf", "metadata": {"source": "CRF", "title": "Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks", "authors": ["Matthew Francis-Landau", "Greg Durrett", "Dan Klein"], "emails": ["mfl@cs.berkeley.edu", "gdurrett@cs.berkeley.edu", "klein@cs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "One of the biggest challenges in linking entities is the resolution of contextually polysemic mentions. For example, Germany could point to a nation, the nation's government, or even a football team. Previous approaches to such cases have often focused on linking collective entities: one might expect nearby mentions in a document to be linked to thematically similar entities, which may give us clues about the identity of the mentions that are currently being solved (Ratinov et al., 2011; Hoffart et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014). But an even simpler approach is to use context information only from the words in the source document itself to ensure that the entity is properly resolved in context. In past work, these approaches have typically relied on heuristics such as tf-idf (Ratinov et1Source)."}, {"heading": "2 Model", "text": "Our model focuses on two core ideas: firstly, that semantics at different granularities in a document is helpful in determining the genres of entities for linking entities, and secondly, that CNNs can distill a block of text into a meaningful topic vector. However, our entity linkage model is a log-linear model that places distributions over target groups that do not include a mention of x and its containing source document. At the moment, we take P (t | x): expw > fC (x, t; \u03b8), where fC has a vector of features based on CNNs with parameters discussed in Section 2.1. Section 2.2 describes how we combine this simple model with a full-fledged entity linkage system. As shown in the middle of Figure 1, each feature in fCar Xiv: 4.7v quadrant 34c1 is associated with a target person, which is a cosmic linkage between a cosmic subject."}, {"heading": "2.1 Convolutional Semantic Similarity", "text": "Figure 1 shows our method of calculating topic vectors and their use to extract characteristics for a potential Wikipedia link. For each of the three text granularities in the source document (the mention mentioning the immediate context and the entire document) and two text granularities on the target subject's page (title and Wikipedia article text), we create vector representations with CNNs as follows: We first embed each word into a d-dimensional vector space using standard embedding techniques (discussed in Section 3.2), which results in a sequence of vector sw1,., wn. Then we map these words into a fixed-size vector using a vector M, Rk \u00b7 d parameterized with a filter bank. We set the result by a reflected linear unit (ReLU) and combine the results with sum pooling, generating the following formulation: j = max wentity \u2212 n (1)."}, {"heading": "2.2 Integrating with a Sparse Model", "text": "An important keyword for solving a mention is the use of hyperlink numbers in Wikipedia (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), which tell us how many times a mention has been linked to Wikipedia on each article. This information can serve as a useful precursor, but only if we can use it effectively by targeting the most important part of a mention. For example, we may never have mentioned President Barack Obama as a linked string on Wikipedia, even though we have seen the Barack Obama subline and it clearly indicates the correct answer. Following Durrett and Klein (2014), we introduce a latent variable q to capture the subset of a mention (known as a query) that we resolve. Query Generation potentially includes the removal of stopwords, plural suffixes, and leading or tailing terms for those terms."}, {"heading": "3 Experimental Results", "text": "We have conducted experiments on 4 different entities linking data sets. \u2022 ACE (NIST, 2005; Bentivogli et al., 2010): This corpus was used in Fahrni and Strube (2014) and Durrett and Klein (2014). \u2022 CoNLL-YAGO (Hoffart et al., 2011): This corpus is based on the CoNLL 2003 data set; the test set consists of 231 news articles and contains a number of rarer entities. \u2022 WP (Heath and Bizer, 2011): This data set consists of short snippets from Wikipedia. \u2022 Wikipedia (Ratinov et al., 2011): This data set consists of 10,000 randomly sampled Wikipedia articles, the task being to resolve the links in each article. 33We do not compare with Ratinov et al. (2011) on this data set because we do not have access to the original Wikipedia."}, {"heading": "3.1 Multiple Granularities of Convolution", "text": "One question we could ask ourselves is how much we gain if we have multiple turns on the source and target sides. Table 3 compares our full range of CNN features, i.e. the six features shown in Figure 1, with two specific turns in isolation. Using turns via the source document (sdoc) and target article text (tdoc) results in a system that is comparable in its entirety to using turns via just the mention (sment) and title of the whole (ttitle), representing two extremes of the system: the use of the maximum context that could provide the most robust representation of topic semantics, and the use of the minimum context that provides the most focused representation of topic semantics (and could more generally allow the system to directly memorize the tension-test pairs observed in training)."}, {"heading": "3.2 Embedding Vectors", "text": "We also examined two different sources of embedding vectors for the coils. Table 4 shows that word vectors trained on Wikipedia outperformed Google News word vectors trained on a larger corpus. Further research revealed that the Google News vectors had a much higher out-of-vocabulary rate. To learn the vectors, we use the standard word2vec toolkit (Mikolov et al., 2013) with a vector length of 300 and a window set of 21 (larger windows produce more semantically focused vectors (Levy and Goldberg, 2014)), 10 negative samples and 10 iterations on Wikipedia."}, {"heading": "3.3 Analysis of Learned Convolutions", "text": "One disadvantage of our system compared to its purely indicator-based variant is that its functioning is less interpretable. However, one way to inspect the learned system is to investigate what leads to a high activation of the various coil filters (rows of matrices Mg from Equation 1). Table 1 shows the n-gram in the ACE dataset, which leads to the maximum activation of three filters from Mdoc. Some filters tend to pick up n-gram, which are characteristic of a particular topic. In other cases, a single filter may be somewhat obscure, as in the third column of Table 1. There are a few possible explanations for this. First, the filter may generally have low activations and therefore have little influence on the final feature calculation. Second, the extreme points of the filter may not be characteristic of its overall behavior, as the majority of the n-gram will lead to more moderate activations. Finally, such a filter may then represent the overlapping of some topics that we will probably never see from this context."}, {"heading": "4 Conclusion", "text": "In this paper, we investigated the use of convolutionary networks to capture semantic similarities between source documents and potential entity-link targets. Using multiple granularities of turns to assess the compatibility of a mention in context and multiple potential link targets results in strong performance in itself; furthermore, such features also enhance an existing entity-linking system based on sparse indicator features and show that these sources of information complement each other."}, {"heading": "Acknowledgments", "text": "This work was partially supported by NSF grant CNS-1237265 and a Google Faculty Research Award. Thanks to the anonymous reviewers for their helpful comments."}], "references": [{"title": "Extending English ACE 2005 Corpus Annotation with Groundtruth Links to Wikipedia", "author": ["Pamela Forner", "Claudio Giuliano", "Alessandro Marchetti", "Emanuele Pianta", "Kateryna Tymoshenko"], "venue": null, "citeRegEx": "Bentivogli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2010}, {"title": "Relational Inference for Wikification", "author": ["Cheng", "Roth2013] Xiao Cheng", "Dan Roth"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Cheng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2013}, {"title": "Large-Scale Named Entity Disambiguation Based on Wikipedia Data", "author": ["Silviu Cucerzan"], "venue": "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-", "citeRegEx": "Cucerzan.,? \\Q2007\\E", "shortCiteRegEx": "Cucerzan.", "year": 2007}, {"title": "Question Answering over Freebase with Multi-Column Convolutional Neural Networks", "author": ["Dong et al.2015] Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "A Joint Model for Entity Analysis: Coreference, Typing, and Linking. In Transactions of the Association for Computational Linguistics (TACL)", "author": ["Durrett", "Klein2014] Greg Durrett", "Dan Klein"], "venue": null, "citeRegEx": "Durrett et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2014}, {"title": "A latent variable model for discourseaware concept and entity disambiguation", "author": ["Fahrni", "Strube2014] Angela Fahrni", "Michael Strube"], "venue": "In Gosse Bouma and Yannick Parmentier 0001,", "citeRegEx": "Fahrni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fahrni et al\\.", "year": 2014}, {"title": "Efficient collective entity linking with stacking", "author": ["He et al.2013] Zhengyan He", "Shujie Liu", "Yang Song", "Mu Li", "Ming Zhou", "Houfeng Wang"], "venue": "In EMNLP,", "citeRegEx": "He et al\\.,? \\Q2013\\E", "shortCiteRegEx": "He et al\\.", "year": 2013}, {"title": "Linked Data: Evolving the Web into a Global Data Space", "author": ["Heath", "Bizer2011] Tom Heath", "Christian Bizer"], "venue": null, "citeRegEx": "Heath et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Heath et al\\.", "year": 2011}, {"title": "Robust Disambiguation", "author": ["Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum"], "venue": null, "citeRegEx": "Hoffart et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2011}, {"title": "Deep Unordered Composition Rivals Syntactic Methods for Text Classification", "author": ["Iyyer et al.2015] Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL)", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Knowledge Base Population: Successful Approaches and Challenges", "author": ["Ji", "Grishman2011] Heng Ji", "Ralph Grishman"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL)", "citeRegEx": "Ji et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2011}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional Neural Networks for Sentence Classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Dependency-Based Word Embeddings", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning to Link with Wikipedia", "author": ["Milne", "Witten2008] David Milne", "Ian H. Witten"], "venue": "In Proceedings of the Conference on Information and Knowledge Management (CIKM)", "citeRegEx": "Milne et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Milne et al\\.", "year": 2008}, {"title": "AIDAlight: High-Throughput Named-Entity Disambiguation", "author": ["Nguyen et al.2014] Dat Ba Nguyen", "Johannes Hoffart", "Martin Theobald", "Gerhard Weikum"], "venue": "In Proceedings of the Workshop on Linked Data on the Web co-located with the 23rd International", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Local and Global Algorithms for Disambiguation to Wikipedia", "author": ["Ratinov et al.2011] Lev Ratinov", "Dan Roth", "Doug Downey", "Mike Anderson"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Ratinov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2011}, {"title": "Modeling Mention, Context and Entity with Neural Networks for Entity Disambiguation", "author": ["Sun et al.2015] Yaming Sun", "Lei Lin", "Duyu Tang", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelli-", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "AdaDelta: An Adaptive Learning Rate Method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).1", "startOffset": 194, "endOffset": 215}, {"referenceID": 17, "context": "Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014).", "startOffset": 249, "endOffset": 357}, {"referenceID": 8, "context": "Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014).", "startOffset": 249, "endOffset": 357}, {"referenceID": 6, "context": "Past approaches to such cases have often focused on collective entity linking: nearby mentions in a document might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014).", "startOffset": 249, "endOffset": 357}, {"referenceID": 18, "context": ", 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al.", "startOffset": 66, "endOffset": 84}, {"referenceID": 3, "context": ", 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014), so we expect them to be effective at isolating the relevant topic semantics for", "startOffset": 32, "endOffset": 70}, {"referenceID": 2, "context": "An important cue for resolving a mention is the use of link counts from hyperlinks in Wikipedia (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), which tell us how often a given mention was linked to each article on", "startOffset": 96, "endOffset": 159}, {"referenceID": 19, "context": "The whole model is trained to maximize the log likelihood of a labeled training corpus using Adadelta (Zeiler, 2012).", "startOffset": 102, "endOffset": 116}, {"referenceID": 19, "context": "The whole model is trained to maximize the log likelihood of a labeled training corpus using Adadelta (Zeiler, 2012). The indicator features fQ and fE are described in more detail in Durrett and Klein (2014). fQ only impacts which query is selected and not the disambiguation to a title.", "startOffset": 103, "endOffset": 208}, {"referenceID": 17, "context": "chor text counts from Wikipedia, string match with proposed Wikipedia titles, and discretized cosine similarities of tf-idf vectors (Ratinov et al., 2011).", "startOffset": 132, "endOffset": 154}, {"referenceID": 17, "context": "chor text counts from Wikipedia, string match with proposed Wikipedia titles, and discretized cosine similarities of tf-idf vectors (Ratinov et al., 2011). Adding tf-idf indicators is the only modification we made to the features of Durrett and Klein (2014).", "startOffset": 133, "endOffset": 258}, {"referenceID": 0, "context": "\u2022 ACE (NIST, 2005; Bentivogli et al., 2010): This corpus was used in Fahrni and Strube (2014) and Durrett and Klein (2014).", "startOffset": 6, "endOffset": 43}, {"referenceID": 0, "context": "\u2022 ACE (NIST, 2005; Bentivogli et al., 2010): This corpus was used in Fahrni and Strube (2014) and Durrett and Klein (2014).", "startOffset": 19, "endOffset": 94}, {"referenceID": 0, "context": "\u2022 ACE (NIST, 2005; Bentivogli et al., 2010): This corpus was used in Fahrni and Strube (2014) and Durrett and Klein (2014).", "startOffset": 19, "endOffset": 123}, {"referenceID": 8, "context": "\u2022 CoNLL-YAGO (Hoffart et al., 2011): This corpus is based on the CoNLL 2003 dataset; the test set consists of 231 news articles and con-", "startOffset": 13, "endOffset": 35}, {"referenceID": 17, "context": "\u2022 Wikipedia (Ratinov et al., 2011): This dataset consists of 10,000 randomly sampled Wikipedia articles, with the task being to resolve the links in each article.", "startOffset": 12, "endOffset": 34}, {"referenceID": 16, "context": "We do not compare to Ratinov et al. (2011) on this dataset because we do not have access to the original Wikipedia dump they used for their work and as a result could not duplicate their results or conduct comparable experiments, a problem which was also noted by Nguyen et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 16, "context": "(2011) on this dataset because we do not have access to the original Wikipedia dump they used for their work and as a result could not duplicate their results or conduct comparable experiments, a problem which was also noted by Nguyen et al. (2014).", "startOffset": 228, "endOffset": 249}, {"referenceID": 16, "context": "Our results outperform those of Durrett and Klein (2014) and Nguyen et al. (2014). In general, we also see that the convolutional networks by themselves can outperform the system using only sparse features, and in all cases these stack to give substantial benefit.", "startOffset": 61, "endOffset": 82}, {"referenceID": 14, "context": "For all experiments, we use word vectors computed by running word2vec (Mikolov et al., 2013) on all", "startOffset": 70, "endOffset": 92}, {"referenceID": 16, "context": "We see that this system outperforms the results of Durrett and Klein (2014) and the AIDALIGHT system of Nguyen et al. (2014). We can", "startOffset": 104, "endOffset": 125}, {"referenceID": 17, "context": "These CNN features also clearly supersede the sparse features based on tf-idf (taken from (Ratinov et al., 2011)), showing that indeed that CNNs are better at learning semantic topic similarity than heuristics like tf-idf.", "startOffset": 90, "endOffset": 112}, {"referenceID": 18, "context": "This model is roughly comparable to Model 2 as presented in Sun et al. (2015).", "startOffset": 60, "endOffset": 78}, {"referenceID": 14, "context": "For learning the vectors, we use the standard word2vec toolkit (Mikolov et al., 2013) with vector length set to 300, window set to 21 (larger windows produce more semantically-focused vectors (Levy and Goldberg, 2014)), 10 negative samples and 10 iterations through Wikipedia.", "startOffset": 63, "endOffset": 85}], "year": 2016, "abstractText": "A key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mention\u2019s context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).1", "creator": "LaTeX with hyperref package"}}}