{"id": "1604.00788", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models", "abstract": "Nearly all previous work in neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The two-fold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT'15 English to Czech translation task, this hybrid approach offers a boost of up to +7.9 BLEU points over models that do not handle unknown words. Our best hybrid system has established a new state-of-the-art result with 19.9 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.", "histories": [["v1", "Mon, 4 Apr 2016 09:30:54 GMT  (53kb)", "http://arxiv.org/abs/1604.00788v1", "11pages, 3 figures. ACL 2016 submission + source embedding analysis and better results"], ["v2", "Thu, 23 Jun 2016 00:50:19 GMT  (60kb)", "http://arxiv.org/abs/1604.00788v2", "11pages, 4 figures. ACL 2016 camera-ready version. SOTA WMT'15 English-Czech 20.7 BLEU (+2.1-11.4 points)"]], "COMMENTS": "11pages, 3 figures. ACL 2016 submission + source embedding analysis and better results", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["minh-thang luong", "christopher d manning"], "accepted": true, "id": "1604.00788"}, "pdf": {"name": "1604.00788.pdf", "metadata": {"source": "CRF", "title": "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models", "authors": [], "emails": ["lmthang@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 4.00 788v 1 [cs.C L] 4A pr2 01"}, {"heading": "1 Introduction", "text": "This year it is so far that it will be able to retaliate, \"he said.\" But it's too early to say what we need to do, \"he said."}, {"heading": "2 Related Work", "text": "Recently, there has been a number of work on character-based neural models that achieve good results in marking parts of the language (Ling et al., 2015a), parsing dependencies (Ballesteros et al., 2015), text classification (Zhang et al., 2015), speech recognition (Chan et al., 2015; Bahdanau et al., 2015b), and language modeling (Kim et al., 2016; Jozefowicz et al., 2016). However, the success of cross-language tasks such as machine translation is not verifiable. 2 Sennrich et al. (2015) suggest that words be segmented into smaller units and translated exactly as at the word level, which do not learn to understand relationships between words. Our work is oriented towards (Luong et al., 2013) and (Li et al., 2015). Similar to the previous model, we build representations for rare words on-the-fly from hierarchical units of words."}, {"heading": "3 Background & Our Models", "text": "In fact, most of us are able to play by the rules that they have established in the EU over the last five years."}, {"heading": "4 Hybrid Neural Machine Translation", "text": "Our hybrid architecture, as illustrated in Figure 1, uses the power of words and characters to achieve the goal of an open vocabulary NMT. At the core of the design is a word-level NMT with the advantage that it is quick and easy to learn, whereas a purely character-based NMT has to deal with very long sequences. Recurring character models enable the word-level system to quickly calculate any source word representation from letters and restore letter-by-letter unknown target words that were originally generated as."}, {"heading": "4.1 Word-based Translation as a Backbone", "text": "At the heart of our hybrid NMT is a deep LSTM encoder decoder that translates at the word level, as in Section 3. We have a vocabulary of W common words for each language. Other words not included in these lists are represented by a universal symbol, one per language. Wetranslate is just like a word-based NMT system in terms of these source and target vocabularies, except for cases where they are included in the source or target output. These correspond to the ones shown in Figure 1. A nice feature of our hybrid approach is that you can control how strongly the world and character-based models merge by varying the vocabulary size, so we take the best of both worlds."}, {"heading": "4.2 Source Character-based Representation", "text": "This is problematic because it discards valuable information about word surface shapes. To fix this, we will learn a deep LSTM model of source word characters. In our running example, for the word \"sweet,\" we will run our deep, character-based LSTM over \"c,\" \"u,\" \"t,\" \"e\" and \"_\" (the boundary symbol), the last hidden state at the top level being used as an on-the-fly representation of the current rare word as shown in the lower left part of the figure. It is useful to note that the layers of the deep, character-based LSTM are always initialized with zero hidden states and cell values. It could be suggested to combine hidden states of the word-based LSTM with the character-based model."}, {"heading": "4.3 Target Character-level Generation", "text": "After that, there will be a further step after processing that deals with these unknown characters, using alignment information derived from the attention mechanism, and then performing simple dictionary searches or identity copies (Jean et al., 2015a; Luong et al., 2015a). While this approach works, it suffers from various problems such as literacy mismatches between source and target vocabularies and multi-word alignments. Our goal is to address all these problems and create a coherent framework that handles an unlimited output vocabulary. Our solution is a separate deep LSTM that \"translates\" given the current word status. We train our system so that whenever the word level generates NMT, we can decode this character level to restore the correct surface shape of the unknown target. This is illustrated in Figure 1."}, {"heading": "4.3.1 Hidden-state Initialization", "text": "In fact, it is not that we are able to solve the problem, but it is not that we can solve it, \"he said in an interview with the Deutsche Presse-Agentur.\" We have not been able to solve it, \"he told the Deutsche Presse-Agentur.\" We have not been able to solve it, \"he said.\" We have not been able to solve it. \""}, {"heading": "4.3.2 Word-Character Generation Strategy", "text": "With the character decoder, we can view the final hidden states as representations for the surface shapes of unknown symbols and could have taken them to the next step. However, for the efficiency reason explained next, we decided against doing so; instead, the word-level decoder is fed \"as it is\" using the appropriate word embedding. During the training, this design selection helps us decouple all execution of the character decoder across instances once the word-level NMT is complete, allowing us to call the back and forth of the character decoder over rare words independently in batch mode. At test times, our strategy is to first run a bar decoder at word level to find the best translation provided by the word-level NMT. Such a translation includes, so we use our character decoder with the bar search to generate the actual words according to the combined translation values."}, {"heading": "5 Experiments", "text": "We rate the effectiveness of our models of the publicly available translation task WMT '15 from English to Czech using newstest2013 (3000 sentences) as a development set and newstest2015 (2656 sentences) as a test set. Two metrics are used: case-sensitive NIST BLEU (Papineni et al., 2002) and chrF3 (Popovic \u0301, 2015).3 The latter measures the amount of overlapping characters and has proven to be a better measure of translation tasks from English."}, {"heading": "5.1 Data", "text": "Among the language pairs available in WMT '15, which all include English, we choose Czech as the target language for several reasons. First and foremost, Czech is a Slavic language with not only rich and complex agility, but also fusion morphology, in which a single morphem can encode several grammatical, syntactic or semantic meanings. As a result, Czech has an enormously large vocabulary (according to Table 1 about 1.5 to 2 times larger than English) and is a sophisticated language to translate into. In addition, this language pair has a large amount of training data so that we can evaluate on a scale. Finally, although our techniques are language-independent, it is easier for us to work with Czech, as Czech uses the Latin alphabet with some diacritics. In terms of preprocessing, we only use the standard tokenization practice.4 For each language, we choose a list of 200 characters, which are used in the most common words, the 98% Vocabulary, as shown in ISK more than in the current table 4K."}, {"heading": "5.2 Training Details", "text": "We train three types of systems, purely word-based, purely character-based and hybrid. Common to these architectures is a word-based NMT, since the character-based systems are essentially word-based systems with longer sequences and the core of hybrid models. In the formation of word-based NMT, we follow Luong et al. (2015a) to use the global attention mechanism along with similar hyperparameters: (a) deep LSTM models, 4 layers, 1024 cells and 1024-dimensional embedding, (b) uniform initialization of parameters in [\u2212 0.1, 0.1] epochal formation with simple SGD models, 1024-dimensional embedding, (s)."}, {"heading": "5.3 Results", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they"}, {"heading": "6 Analysis", "text": "In addition to measuring translation quality, this section analyzes the impact of our source and target components at character level."}, {"heading": "6.1 Rare Word Embeddings", "text": "Quantitatively, we follow Luong et al. (2013) in using the word similarity task, especially on the rare word dataset, to assess the learned representations for complex words. Evaluation metric is the correlation between the similarity values of the Spearman assigned by a model and by human annotators. From the results in Table 3, we can see that the source representations produced by 9Note of our other models, which combine both the separation path approach and the two-layered character components, have not yet converged. We expect even better results in the next version of our paper. Our hybrid 10 models are significantly better than those of the word-based model. It is noteworthy that our deep, recurring character step models use the visualization model of (Luong et al., 2013), which uses recursive neural networks and requires a complex morphological unit that is not selected by 11."}, {"heading": "6.2 Sample Translations", "text": "In Table 4, we show example translations between different systems. In the first example, our hybrid model translates perfectly; the word-based model does not translate \"diagnosis\" because the second \"after\" was misaligned; the character-based model, on the other hand, makes an error in the translation of names; in the second example, the hybrid model surprises us if it can capture the rearrangement of \"fifty years ago\" and \"fifty years ago\" and \"after dpades\u00e1ti lety,\" whereas the other two models do not. The word-based model translates \"Jr.\" due to the incorrect alignment between the second \"and the word\" said \"imprecise.\" The character-based model literally translates the name \"king\" into \"kr\u00e1l,\" which means \"king.\" Finally, both the character-based model and the hybrid model impress us with their ability to accurately translate compound words, e.g. \"11 years old\" and \"jeden\u00e1cti\u00e1.\""}, {"heading": "7 Conclusion", "text": "In this work, we have proposed a novel hybrid architecture that combines the strengths of both word and character-based models. Word models are quick to train and provide high-quality translation; while character models help achieve the goal of open vocabulary NMT. We have demonstrated these two aspects through our experimental results and translation examples; our best hybrid model has outperformed the performance of both the best word-based NMT system and the best non-neural model to establish a new state of the art in English translation in WMT '15 with 19.9 BLEU. In addition, we have succeeded in replacing the standard punctuation technique in NMT with our character components, which has led to an improvement of up to + 7.9 BLEU dots. Our analysis has shown that our model has the ability to generate not only well-shaped words for Czech, a highly inflexible language with an enormous amount of vocabulary and precision for English words."}, {"heading": "Acknowledgment", "text": "This work was supported in part by the NSF Award IIS-1514268 and in part by a gift from Bloomberg L.P. We thank Dan Jurafsky, Andrew Ng and Quoc Le for earlier feedback on the work and Sam Bowman, Ziang Xie and Jiwei Li for their valuable comments on the paper design. Finally, we thank NVIDIA Corporation for donating Tesla K40 GPUs and for helping Andrew Ng and his group use their computing resources."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015a", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1508.04395.", "citeRegEx": "Bahdanau et al\\.,? 2015b", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "EMNLP.", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "CUNI in WMT15: Chimera Strikes Again", "author": ["Ond\u0159ej Bojar", "Ale\u0161 Tamchyna."], "venue": "WMT.", "citeRegEx": "Bojar and Tamchyna.,? 2015", "shortCiteRegEx": "Bojar and Tamchyna.", "year": 2015}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1508.01211.", "citeRegEx": "Chan et al\\.,? 2015", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Bengio."], "venue": "EMNLP.", "citeRegEx": "Bengio.,? 2014", "shortCiteRegEx": "Bengio.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "ACL.", "citeRegEx": "Jean et al\\.,? 2015a", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Montreal neural machine translation systems for WMT\u201915", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "WMT.", "citeRegEx": "Jean et al\\.,? 2015b", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu."], "venue": "arXiv preprint arXiv:1602.02410.", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "AAAI.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky."], "venue": "ACL.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Alignment by agreement", "author": ["Percy Liang", "Ben Taskar", "Dan Klein."], "venue": "NAACL.", "citeRegEx": "Liang et al\\.,? 2006", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W. Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Lu\u00eds Marujo", "Tiago Lu\u00eds."], "venue": "EMNLP.", "citeRegEx": "Ling et al\\.,? 2015a", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan Black."], "venue": "arXiv preprint arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015b", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Stanford neural machine translation systems for spoken language domain", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "IWSLT.", "citeRegEx": "Luong and Manning.,? 2015", "shortCiteRegEx": "Luong and Manning.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "CoNLL.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "ACL.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei jing Zhu."], "venue": "ACL.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour."], "venue": "Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on, pages 285\u2013", "citeRegEx": "Pham et al\\.,? 2014", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "chrF: character n-gram F-score for automatic MT evaluation", "author": ["Maja Popovi\u0107."], "venue": "WMT.", "citeRegEx": "Popovi\u0107.,? 2015", "shortCiteRegEx": "Popovi\u0107.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Barnes-hut-sne", "author": ["Laurens van der Maaten."], "venue": "CoRR, abs/1301.3342.", "citeRegEx": "Maaten.,? 2013", "shortCiteRegEx": "Maaten.", "year": 2013}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "CoRR, abs/1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "NIPS.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Despite being relatively new, NMT has already achieved state-of-the-art translation results for many language pairs such as English-French (Luong et al., 2015b), English-German (Jean et al.", "startOffset": 139, "endOffset": 160}, {"referenceID": 7, "context": ", 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015), and English-Czech (Jean et al.", "startOffset": 25, "endOffset": 91}, {"referenceID": 18, "context": ", 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015), and English-Czech (Jean et al.", "startOffset": 25, "endOffset": 91}, {"referenceID": 16, "context": ", 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015), and English-Czech (Jean et al.", "startOffset": 25, "endOffset": 91}, {"referenceID": 8, "context": ", 2015a; Luong and Manning, 2015), and English-Czech (Jean et al., 2015b).", "startOffset": 53, "endOffset": 73}, {"referenceID": 15, "context": "Luong et al. (2015b) propose to annotate occurrences of target <unk> with positional information to track their alignments, after which simple word dictionary lookup or identity copy can be performed to replace <unk> in the translation.", "startOffset": 0, "endOffset": 21}, {"referenceID": 7, "context": "Jean et al. (2015a) approach the problem similarly but obtain the alignments for unknown words from the attention mechanism.", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "See more on this problem in (Sennrich et al., 2015).", "startOffset": 28, "endOffset": 51}, {"referenceID": 17, "context": "This is problematic as pointed out by Luong et al. (2013): neural networks can learn good representations for frequent words such as \u201cdistinct\u201d, but fail for rare-but-related words like \u201cdistinctiveness\u201d.", "startOffset": 38, "endOffset": 58}, {"referenceID": 14, "context": "There has been a recent line of work on end-toend character-based neural models which achieve good results for part-of-speech tagging (Ling et al., 2015a), dependency parsing (Ballesteros et al.", "startOffset": 134, "endOffset": 154}, {"referenceID": 2, "context": ", 2015a), dependency parsing (Ballesteros et al., 2015), text classification (Zhang et al.", "startOffset": 29, "endOffset": 55}, {"referenceID": 27, "context": ", 2015), text classification (Zhang et al., 2015), speech recognition (Chan et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 4, "context": ", 2015), speech recognition (Chan et al., 2015; Bahdanau et al., 2015b), and language modeling (Kim et al.", "startOffset": 28, "endOffset": 71}, {"referenceID": 1, "context": ", 2015), speech recognition (Chan et al., 2015; Bahdanau et al., 2015b), and language modeling (Kim et al.", "startOffset": 28, "endOffset": 71}, {"referenceID": 11, "context": ", 2015b), and language modeling (Kim et al., 2016; Jozefowicz et al., 2016).", "startOffset": 32, "endOffset": 75}, {"referenceID": 9, "context": ", 2015b), and language modeling (Kim et al., 2016; Jozefowicz et al., 2016).", "startOffset": 32, "endOffset": 75}, {"referenceID": 0, "context": ", 2015; Bahdanau et al., 2015b), and language modeling (Kim et al., 2016; Jozefowicz et al., 2016). However, success has not been shown for cross-lingual tasks such as machine translation.2 Sennrich et al. (2015) propose to segment words into smaller units and translate just like at the word level, which does not learn to understand relationships among words.", "startOffset": 8, "endOffset": 213}, {"referenceID": 17, "context": "Our work takes inspiration from (Luong et al., 2013) and (Li et al.", "startOffset": 32, "endOffset": 52}, {"referenceID": 12, "context": ", 2013) and (Li et al., 2015).", "startOffset": 12, "endOffset": 29}, {"referenceID": 12, "context": "In comparison with (Li et al., 2015), our hybrid architecture is also a hierarchical sequence-to-sequence model, but operates at a different granularity level, word-character.", "startOffset": 19, "endOffset": 36}, {"referenceID": 12, "context": ", 2013) and (Li et al., 2015). Similar to the former, we build representations for rare words on-the-fly from subword units. However, we utilize recurrent neural networks with characters as the basic units; whereas Luong et al. (2013) use recursive neural networks with morphemes as units, which requires existence of a morphological analyzer.", "startOffset": 13, "endOffset": 235}, {"referenceID": 12, "context": ", 2013) and (Li et al., 2015). Similar to the former, we build representations for rare words on-the-fly from subword units. However, we utilize recurrent neural networks with characters as the basic units; whereas Luong et al. (2013) use recursive neural networks with morphemes as units, which requires existence of a morphological analyzer. In comparison with (Li et al., 2015), our hybrid architecture is also a hierarchical sequence-to-sequence model, but operates at a different granularity level, word-character. In contrast, Li et al. (2015) build hierarchical models at the sentence-word level for paragraphs and documents.", "startOffset": 13, "endOffset": 550}, {"referenceID": 10, "context": "It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 63, "endOffset": 137}, {"referenceID": 24, "context": "It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 63, "endOffset": 137}, {"referenceID": 14, "context": "There is a recent work by Ling et al. (2015b) that attempts at character-level NMT; however, the experimental evidence is weak.", "startOffset": 26, "endOffset": 46}, {"referenceID": 6, "context": "Papers, however, differ in terms of: (a) architecture \u2013 from unidirectional, to bidirectional, and deep multi-layer RNNs; and (b) RNN type \u2013 which are long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and the gated recurrent unit (Cho et al.", "startOffset": 181, "endOffset": 215}, {"referenceID": 26, "context": "All our models utilize the deep multi-layer architecture with LSTM as the recurrent unit; detailed formulations are in (Zaremba et al., 2014).", "startOffset": 119, "endOffset": 141}, {"referenceID": 24, "context": "Attention Mechanism \u2013 The early NMT approaches (Sutskever et al., 2014; Cho et al., 2014), which we have described above, use only the last encoder state to initialize the decoder, i.", "startOffset": 47, "endOffset": 89}, {"referenceID": 18, "context": "In all our models, we utilize the global attention mechanism and the bilinear form for the attention scoring function similar to (Luong et al., 2015a).", "startOffset": 129, "endOffset": 150}, {"referenceID": 0, "context": "Recently, Bahdanau et al. (2015a) propose an attention mechanism, a form of random access memory for NMT to cope with long input sequences.", "startOffset": 10, "endOffset": 34}, {"referenceID": 0, "context": "Recently, Bahdanau et al. (2015a) propose an attention mechanism, a form of random access memory for NMT to cope with long input sequences. Luong et al. (2015a) further extend the attention mechanism to different scoring functions, used to compare source and target hidden states, as well as different strategies to place the attention.", "startOffset": 10, "endOffset": 161}, {"referenceID": 18, "context": "Figure 2: Attention mechanism \u2013 shown are the two steps of the attention mechanism described in (Luong et al., 2015a): first, compute a context vector ct based on the current target hidden state ht and all the source hidden states [h\u03041, .", "startOffset": 96, "endOffset": 117}, {"referenceID": 17, "context": "This approach is inspired by the work of (Luong et al., 2013) which also computes on-the-fly representations for rare words.", "startOffset": 41, "endOffset": 61}, {"referenceID": 14, "context": "Lastly, while Ling et al. (2015b) found that it is slow and difficult to train source character-level models and had to resort to pretraining, we demonstrate later in the experiment section that we can train our deep character-level LSTM perfectly fine in an end-to-end fashion.", "startOffset": 14, "endOffset": 34}, {"referenceID": 7, "context": "Afterwards, there will be another post-processing step that handles these unknown tokens by utilizing the alignment information derived from the attention mechanism and then performing simple word dictionary lookup or identity copy (Jean et al., 2015a; Luong et al., 2015a).", "startOffset": 232, "endOffset": 273}, {"referenceID": 18, "context": "Afterwards, there will be another post-processing step that handles these unknown tokens by utilizing the alignment information derived from the attention mechanism and then performing simple word dictionary lookup or identity copy (Jean et al., 2015a; Luong et al., 2015a).", "startOffset": 232, "endOffset": 273}, {"referenceID": 20, "context": "Two metrics are used: case-sensitive NIST BLEU (Papineni et al., 2002) and chrF3 (Popovi\u0107, 2015).", "startOffset": 47, "endOffset": 70}, {"referenceID": 22, "context": ", 2002) and chrF3 (Popovi\u0107, 2015).", "startOffset": 18, "endOffset": 33}, {"referenceID": 21, "context": "2 according to (Pham et al., 2014).", "startOffset": 15, "endOffset": 34}, {"referenceID": 17, "context": "In training word-based NMT, we follow Luong et al. (2015a) to use the global attention mechanism together with similar hyperparameters: (a) deep LSTM models, 4 layers, 1024 cells, and 1024-dimensional embeddings, (b) uniforml initialization of parameters in [\u22120.", "startOffset": 38, "endOffset": 59}, {"referenceID": 17, "context": "Such procedure was proposed by Luong et al. (2015b) and referred as the unk replace technique.", "startOffset": 31, "endOffset": 52}, {"referenceID": 3, "context": "0M monolingual sentences (Bojar and Tamchyna, 2015).", "startOffset": 25, "endOffset": 51}, {"referenceID": 8, "context": "For NMT, to the best of our knowledge, (Jean et al., 2015b) has the best published performance on English-Czech translation.", "startOffset": 39, "endOffset": 59}, {"referenceID": 8, "context": "Our single NMT model (e) outperforms the best single model in (Jean et al., 2015b) by +1.", "startOffset": 62, "endOffset": 82}, {"referenceID": 13, "context": "Obtained from the alignment links produced by the Berkeley aligner (Liang et al., 2006) over the training corpus.", "startOffset": 67, "endOffset": 87}, {"referenceID": 3, "context": "(a) Best WMT\u201915, big data (Bojar and Tamchyna, 2015) - - - - 18.", "startOffset": 26, "endOffset": 52}, {"referenceID": 8, "context": "(b) RNNsearch + unk replace (Jean et al., 2015b) 200K - - - 15.", "startOffset": 28, "endOffset": 48}, {"referenceID": 8, "context": "7 (c) Ensemble 4 models + unk replace (Jean et al., 2015b) 200K - - - 18.", "startOffset": 38, "endOffset": 58}, {"referenceID": 8, "context": "The 512-dimensional attention-based model (h) is best, surpassing even the single word-based model in (Jean et al., 2015b) despite having much fewer parameters.", "startOffset": 102, "endOffset": 122}, {"referenceID": 4, "context": "First, attention is critical for character-based models to work as is obvious from the poor performance of the non-attentional model; this has been verified in speech recognition (Chan et al., 2015).", "startOffset": 179, "endOffset": 198}, {"referenceID": 15, "context": "This suggests that an extreme hybrid system with W =0, a close approximation to those of (Ling et al., 2015b), will not work well.", "startOffset": 89, "endOffset": 109}, {"referenceID": 17, "context": "Quantitatively, we follow Luong et al. (2013) in using the word similarity task, specifically on the Rare Word dataset, to judge the learned representations for complex words.", "startOffset": 26, "endOffset": 46}, {"referenceID": 17, "context": "It is noteworthy that our deep recurrent character-level models can outperform the model of (Luong et al., 2013), which uses recursive neural networks and requires a complex morphological analyzer, by a large margin.", "startOffset": 92, "endOffset": 112}, {"referenceID": 17, "context": "(Luong et al., 2013) 138K 34.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "Nearly all previous work in neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel wordcharacter solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT\u201915 English to Czech translation task, this hybrid approach offers a boost of up to +7.9 BLEU points over models that do not handle unknown words. Our best hybrid system has established a new stateof-the-art result with 19.9 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highlyinflected language with a very complex vocabulary, but also build correct representations for English source words.1", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}