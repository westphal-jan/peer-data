{"id": "1604.00923", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning", "abstract": "In this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy. The ability to evaluate a policy from historical data is important for applications where the deployment of a bad policy can be dangerous or costly. We show empirically that our algorithm produces estimates that often have orders of magnitude lower mean squared error than existing methods---it makes more efficient use of the available data. Our new estimator is based on two advances: an extension of the doubly robust estimator (Jiang and Li, 2015), and a new way to mix between model based estimates and importance sampling based estimates.", "histories": [["v1", "Mon, 4 Apr 2016 15:56:52 GMT  (517kb,D)", "http://arxiv.org/abs/1604.00923v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["philip s thomas", "emma brunskill"], "accepted": true, "id": "1604.00923"}, "pdf": {"name": "1604.00923.pdf", "metadata": {"source": "CRF", "title": "Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning", "authors": ["Philip S. Thomas", "Emma Brunskill"], "emails": ["PHILIPT@CS.CMU.EDU", "EBRUN@CS.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "The ability to predict the performance of a policy without actually having to use it is critical to the responsible use of reinforcement learning algorithms. Consider the setting in which the user of a reinforcement learning algorithm has already deployed a policy, e.g. to determine what advertisements show a user visiting a website (Theocharous et al., 2015), to determine what medical treatment to propose for a patient (Thapa et al., 2005), or to suggest a personalized curriculum for a student (Mandel et al., 2014). In these examples, the use of a bad policy can be costly or dangerous, so it is important that the user of a reinforcement learning algorithm will be able to accurately predict how well a new policy will work without having to use it. In this paper, we propose a new algorithm to tackle this performance prediction problem, which we call the Offpolicy assessment (Estimations, Estimates, Estimates, Estimates, Estimates, Primary Problems, and Estimates)."}, {"heading": "2. Notation", "text": "We assume that the reader is familiar with the discounted objective function (Sutton & Barto, 1998) and that the notational standards MDPNv1 for Markov decision-making processes (Thomas, 2015a, MDPs). For simplicity, our notation assumes that the state, action and reward rates are finite, although our results are transferred to more general settings. We assume that the (possibly unknown) minimum and maximum rewards, rmin and rmax, are finite actions, finite actions, and that these actions have finite meaning. [0, 1] for finite horizon setting and finite action [0, 1) for the infinite and infinite horizon settings. We assume that g (H) is used for the discounted objective function, v (H)."}, {"heading": "3. Off-Policy Policy Evaluation (OPE)", "text": "The problem of non-political policy evaluation (OPE) is defined as follows: We get an evaluation policy, \u03c0e, historical data, D and an approximate model. Our goal is to create an estimator for v (\u03c0e) that has a low mean square error (MSE): MSE (v (D), v (\u03c0e): = E [(v (D) \u2212 v (\u03c0e))) 2]. We use capital letters to designate running variables, and thus the random terms in the expected values are always capital letters (e.g. D is a random variable). We assume that the process that produces states, measures and rewards is an MDP with an unknown initial state distribution, transition function and reward function. We assume that the evaluation policy, \u043fe, behavioral policy, \u03c0i, i, i, i, i, i, i, i, i, i,."}, {"heading": "4. Doubly Robust (DR) Estimator", "text": "The Double Robust (DR) Estimator (Jiang & Li, 2015) is a new unbiased estimator of v (\u03c0e) that achieves promising empirical and theoretical results by using an approximate model of an MDP to reduce the variance of unbiased estimates produced by ordinary importance. (Precup et al., 2000) It is doubly robust that there will be \"good\" estimates if either 1) the model is accurate or 2) the behavioral policy is known. \"good\" means that if the former estimator does not remain unbiased (although it may have a high variance and thus high mean square errors), and if the latter does not hold when the model has the double robust estimator, also has low errors. Twice robust estimators have been introduced and remain popular in the statistical community (Rotnitzky & Robins, 1995; Heejung Robins, 2005).The work, the DR Estimator has a dual, 2015, is a more robust one for Li."}, {"heading": "5. Weighted Doubly Robust (WDR) Estimator", "text": "Empirical and theoretical results indicate that the DR estimator should be used as an internal mechanism instead of current behavior or policy, as Jiang & Li (2015) can significantly reduce the variance of oral meaning (2015). The fact that there is no unbiased estimate may be particularly important when the estimator is used to generate confidence limits on v (\u03c0e) (Thomas, 2015b). In practice, these trust limits often require an impractical amount of data before they are narrow enough to be useful and therefore unbiased confidence limits are used. (Theocharous et al., 2015) When applying these approximate confidence limits, the strict requirement is that an OPE estimator cannot be an unbiased estimator of v (\u03c0e). Moreover, the goal of OPE is often not to produce confidence limits, but to produce the best estimate of v."}, {"heading": "6. Empirical Studies (WDR)", "text": "To compare the empirical advantages of the WDR model. In order to show the empirical advantages of the WDR model, we compared the existing importance of estimators and better motivated our second important contribution, in this section we present an empirical comparison of different OPE methods. [6] We compare the rough data of all models provided in this paper (as well as additional diagrams), the definitions of which are included in the table. [7] We also compare the meaning of methods included in the supplement. [8] Meaning of methods included in the supplement. [8] Meaning of methods included in the supplement. [8] Meaning of methods included in the supplement. [8] Meaning of methods included in the supplement. [8] Meaning of methods included in the explanatory statement. [8] Meaning of methods included in the supplement."}, {"heading": "7. Blending IS and Model (BIM) Estimator", "text": "In this section, we will show how two OPE estimators can be merged into a single estimator that has the desirable properties of both. Before we do so, we will list some terminology. We will divide OPE estimators into three classes. The first class, which we primarily refer to as estimators, includes all estimators that are defined using all factors of importance when L is finite. The second class, which we call purely model-based estimators, includes all estimators that do not contain terms for t 0. The only purely modeled estimate in this paper is AM. Finally, we will call the third class partial estimators."}, {"heading": "8. Model and Guided Importance Sampling Combining (MAGIC) Estimator", "text": "In this section, we propose to use the BIM estimator with WDR as an important estimate (A). The resulting estimator combines purely model-based estimates with estimates of the conducted meaning of the WDR sampling algorithm, and so we call it less obvious than one might expect and therefore important technical details that we derive from Appendix F. The resulting definition of an off-policy j-step return is for all j-step return \u2212 1: g (J) with the approximate model that we expect to put it off in Appendix F. The resulting definition of an off-policy j-step return is for all j-step return \u2212 1: g (J)."}, {"heading": "9. Empirical Studies (MAGIC)", "text": "The first three diagrams in Figure 2 correspond to those in Figure 1, but contain MAGIC. Generally, MAGIC performs very well in tracking or exceeding the best performance of WDR and AM. However, in Figure 2c, MAGIC does not track AM perfectly; the scale is logarithmic, so the difference between MAGIC and AM is small compared to the benefit of MAGIC over WDR. We suspect that the reason MAGIC does not agree with AM is due to errors in our estimates of n and bn.Figure 2d for an experimental setup we call hybrid, where there is partial observability in early trajectories (e.g. initial uncertainty about a student's knowledge of an intelligent tutoring system or uncertainty about the state of the world in a robotic application)."}, {"heading": "10. Conclusion", "text": "We have proposed several new OPE estimates and empirically demonstrated that they exceed existing estimates. Whereas previous OPE estimates using importance samples often do not exceed the approximate model estimate (which does not use importance samples), our new estimates often do, often by orders of magnitude. In cases where the approximate model estimate remains the best estimate, one of our new estimates, MAGIC, performs similarly. In other cases, MAGIC matches or exceeds the performance of previous state-of-the-art estimates."}, {"heading": "A. Preliminaries", "text": "In this section we present additional notation, definitions, properties, (known) theorems, conclusions, and lemmas that are useful when proving theorems. (S0, A0, S1, S1, S1,.) Let's leave the first t-transitions in episode Hti is the first t-transition process in D. Let Ht be the set of all possible part trajectories of length t.For all (digit, s) S, let it is the set of part trajectories and denote part trajectories - Hti is the first t transitions of the ith trajectory in D. Let Ht be the set of all possible partial trajectories of length t.For all (zip, s) S, let it is the set of actions that have non-zero probability in state s, i.e."}, {"heading": "Proof.", "text": "Pr (lim n). (lim n). (lim n). (lim n). (lim n). (lim n). (lim n). (lim n). (lim n). (lim n). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim). (lim)."}, {"heading": "Proof.", "text": "Pr (lim n \u2192 \u221e Yn = X) = Pr ((lim n \u2192 \u221e Yn \u2264 X) (5) (lim n \u2192 \u221e Yn \u2265 X)) SincePr (lim n \u2192 \u221e Yn \u2265 X) \u2265 Pr (lim n \u2192 \u221e Xn \u2265 X) \u2265 Pr (lim n \u2192 \u221e Xn = X) = 1, andPr (lim n \u2192 \u221e Yn \u2264 X) \u2265 Pr (lim n \u2192 \u221e Zn \u2264 X) \u2265 Pr (lim n \u2192 \u221e Zn = X) = 1, we have that (5) is the probability of the common occurrence of two probabilities of an event, and soPr (lim n \u2192 \u221e Yn = X) = 1. Next, we show that if the difference between two sequences converges almost certainly to zero, then we can replace one sequence with the other without changing the near-certain convergence properties of the function: Lemma 5. If one f \u2192 f, then a continuous function (a.n) \u2212 n."}, {"heading": "Proof.", "text": "Pr (lim n) f (Yn) = X) = Pr (lim n) f (Yn \u2212 Xn) = X (a) = Xi (lim n \u00b2 Yn \u2212 Xn = 0) (b) n (lim n \u00b2 Yn \u2212 Xn = 0) (lim n \u00b2) (lim n \u00b2 Yn \u2212 Xn = 0) (f (lim n \u00b2 s = 0) (a) (n \u00b2 s) (c), where (a) f is a continuous function, and where there are sufficient conditions for the event in the above line, and (c), because under our assumptions the two events both occur with probability of one. Thus, we can conclude that f (Yn) a.s. \u2212 \u2192 Next, we examine two standard forms of the strong law of large numbers."}, {"heading": "B. Doubly Robust Derivation and Proofs", "text": "In this appendix, we provide an alternative derivative of the DR estimator using control variables. The idea behind the control variables is as follows: Suppose we want to estimate this type of estimate: = E [X] given a sample of X \u2212 Y + E [Y] would be a unique estimate. However, if we have a sample of another random variable, Y), we have this estimate with a known expected value, E [Y], then the estimator (X \u2212 Y + E [Y] may be havelower variance. Specifically, while Var (X) = Var (X), we have thatVar (X) + Var (Y) \u2212 2Cov (Y, Y) \u2212 2Cov has a lower variance than the 2Cov (X, Y) > Var (Y) > Var (Y).Often Y is referred to as a control variant."}, {"heading": "B.1. Equivalence of DR Definitions", "text": "In this section, we show that our non-recursive definition of the DR estimator is equivalent to the recursive definition of Jiang & Li (2015) when the horizon is finite and known. (2) is equivalent to the DR estimator presented by Jiang & Li (2015) when the finite horizon, L, of the MDP is known. (2) Definition of the DR estimator for a single path (i.e., n = 1) as the last element of a sequence, (Xi) Li = 0. This sequence is defined by the following recurrence relationship. LetX0: = 0 and for all k values. (L) letXk: = v values (SL \u2212 k) + SL values (AL \u2212 k), which we define as such."}, {"heading": "B.2. DR is Unbiased", "text": "While Jiang & Li (2015) have shown that the DR estimator (with finite horizon) is an unbiased estimator of v (\u03c0e), we show in this section that the DR estimator (without assumptions about the horizon) is an unbiased estimator of v (\u03c0e).Theorem 9 (DR - unbiased estimator) If assumption 1 applies, then E [DR (D)] = v (\u03c0e).Evidence. This result has already been shown for the known finite horizon setting (Jiang & Li, 2015), but not yet for the other settings. Since we will use some steps of this evidence in later proofs, the majority of this evidence is attributed to a Lemma.E [DR (D)] = E [1nn \u0445 i = 1DRi (D)] (a) = 1nn \u0445 i = v (\u03c0e), with (a) derived from Lemma 7."}, {"heading": "B.3. Conditions for Consistency of DR", "text": "In this section, we show that the DR estimator is a strongly consistent estimator of v (\u03c0e) because it contains mild technical assumptions and that there is only one particular behavioral policy (theory 10) or that the weights of importance are limited (theorem 11). Theorem 10 (theory 10 - strongly consistent estimator for a behavioral policy). However, if assumptions 1 and 2 are correct, then this evidence is a relatively straightforward application of the law of large numbers. We have from Lemma 7 that E [DRi (D)]] = v (assumption) for all i (1,..., n}. With assumption 2, {DRi (D) ni = 1 is a series of n independent and identically distributed random variables (sinceHi) that depend only on Hi for all i, andDRi (D). We can therefore conclude that the strong law of Khintchine is."}, {"heading": "C. Weighted Doubly Robust Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1. Proof of Theorem 1", "text": "In this section we prove TheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheTheThe"}, {"heading": "C.2. Proof of Theorem 2", "text": "In this section, we prove theorem 2, which states that if assumptions 1 and 4 are true, then WDR (D) a.s. \u2212 v (\u03c0e).Let us remember that WDR can be defined as in (17). First, we apply Lemma 12 to the CWPDIS (D) a.s. \u2212 \u2192 E [\u221e t = 0\u03b3tRHt | H) = v (\u03c0e). (21) Next, we apply Lemma 12 to the Xn term, which uses ft (H t + 1 i) = q (SHit, A Hi t), which has been limited since ancient times. (SHit, A Hi t), we apply Lemma 12 to the Xn term, the ft (H t + 1 i) = q (SHit, A Hi t)."}, {"heading": "D. Extended Empirical Studies (WDR)", "text": "In this section we offer a detailed description of our experiments, which compare the WDR estimator with different sample estimators (IS, PDIS, WIS, CWPDIS) as well as DR and AM. We conducted experiments in three areas: ModelFail, ModelWin and a network world. We describe each domain, then describe the experimental setup and then present empirical results. All three domains have a limited horizon and use \u03b3 = 1.0."}, {"heading": "D.1. The ModelFail Domain", "text": "The ModelFail domain is constructed in such a way that the model would not converge to the true MDP. One way that this can happen is if the model uses functional approximation so that it cannot represent the true MDP. Another way that this can happen is if there is a partial observability that is common in real applications. We are therefore constructing an area where the true underlying MDP has three states (plus the terminal absorbing state), but where the actor cannot say which state it is - he only sees a single state. Although the MDP has three states (denoted by circles) plus the terminal absorbing state (denoted by the double circle), the actor does not observe which state it is only a single state."}, {"heading": "D.2. The ModelWin Domain", "text": "This was designed so that the approximate model of the MDP would converge quickly to the true MDP with a 0.6 probability, while the meaning of sampling-based approaches such as DR and WDR would continue to show a high variance. Recall from our discussion in Section 6 that DR and WDR will be equal to a simple model-based approach if the approximate MDP is perfect and the state transitions and rewards are deterministic. To avoid this, the ModelWin domain has stochastic state transitions, which result in the (b) term in (3) not necessarily having to be zero. ModelWin-MDP is represented in Figure 4. Unlike the ModelFail domain, the agent observes the true underlying states of the ModelWin-MDP, of which there are three plus a terminal absorbing state (not shown). The agent always begins in s1, where he must choose between two actions. The first action, agent, causes the transition to the ModelWin-MDP with a 0.6 probability and a 0.4 probability."}, {"heading": "D.3. The Gridworld Domain", "text": "The third domain we used was the Gridworld domain developed by Thomas (2015b, Section 2.5) for evaluating OPE algorithms. Thomas (2015b) proposed five strategies, \u03c01,.., \u03c05, which can serve as behavioral and evaluation guidelines. Although this setup was developed for evaluating OPE methods, it was not developed with respect to DR- and WDR-methods (since they were introduced later). Specifically, its use of deterministic state transition and reward functions means that AM, DR, and WDR will all work similarly (since the (b) term in (3) is close to zero). Therefore, we conducted experiments with two variants of this Grid world. In the first variant, the approximate model with the horizon was predicted, L = 100. In the second variant, however, Grid is introduced for the effects on the non-essential world in terms of the improbability of L = 101, as we are bound to close the horizon with the improbability."}, {"heading": "D.4. Experimental Setup", "text": "In fact, most of them are able to abide by the rules that they have applied in practice."}, {"heading": "D.5. ModelFail Results", "text": "Figure 1b in Section 6 shows the result on the ModelFail domain in the full data setting. We reproduce this chart in Figure 6. Here, the weighted meaning of the sample methods, WIS and CWPDIS, is obscured by the curve for WDR, while the unweighted sampling methods, IS and PDIS, are obscured by the curve for DR. Also note that although the approximate model is not accurate, which means that the control variants used by DR and WDR may be bad, the DR and WDR estimators perform no worse than PDIS and CWPDIS in approximately the order of magnitude. In Figure 7, we reproduce this experiment in the half data setting. Since AM does not use data for sample collection of importance, it is identical in both settings (half data and full data)."}, {"heading": "D.6. ModelWin Results", "text": "Figure 1c in Section 6 shows the result of ongoing importance samples and guided importance samples, as well as the approximate model estimate on the ModelWin setup in the full data setting. We reproduce this graph in Figure 8. Here, AM has about an order of magnitude less MSE than all other methods, including WDR, and was our motivation for combining AM and WDR with BIM.In Figure 9, we reproduce this experiment in the half data setting. As with the ModelWin setup, this only harms DR and WDR. If there are few trajectories, it seems to affect DR more than WDR, although this may be due to noise (note the large standard error bars on the DR curve if n is small."}, {"heading": "D.7. Gridworld Results", "text": "Figure 1a in section 6 shows the results of using the fourth Gridworld policy, \u03c04, as a behavioural policy and the fifth, \u03c05, as a valuation policy for the Gridworld FH domain in the full data area. We reproduce them in Figure 10. Note that WDR exceeds all other methods by at least one order of magnitude. In Figure 11, we reproduce this experiment in the half-data area as opposed to Gridworld FH. The results are in Figures 12 and 13. Note that AM, when setting the true horizon, is excessive. In the full data setting DR and WDR policy are both right at the top of the curve for AM. This makes sense because the transition function and reward function are deterrent to ministers and only slightly lag behind the results that we both construct approximately in this way."}, {"heading": "D.8. Summary", "text": "The main conclusions from these experiments are that WDR tends to outperform the other major sampling estimators IS, PDIS, WIS, and CWPDIS, as well as the guided sampling measurement method DR. None of these methods achieved square errors within an order of magnitude of WDRs in all of our experiments. Similar results were observed by others, for example in the experiments of Jiang & Li (2015) that AM tends to perform better than DR (although they were not compared with the WDR because it had not yet been introduced), which motivated our introduction of the BIM estimator as a way to WDR and AM. Note that if the transition function and reward function are deterministic and there is no approximate observability (since the grids of these sampling estimators would not provide better data than the approximate WDR measurement), we do not approximate, but approximate, the actual meaning."}, {"heading": "E. Consistency of BIM", "text": "In this appendix, we prove theorem 3 (D), which states that if assumption 4 (D) is correct, there is at least one j (J), so that g (J) (D) (D) (D) would apply a strongly consistent estimate of v (E) and b). We start with the fact that BIM would almost certainly convert to v (E) if it applied the true definition n (D) and bn (B) instead of using the estimate. Let j (D) be such a definition that g (J) can be an index so that g (J).s (D).s that exists by using the weight vectors that have a weight vector of g (J) (D) and a weight force of zero on the other returns, so that y (D).s (D).s We are the weight vectors that have a weight vector of g (J) and a weight vector of zero on the other returns (we have a weight force of D and one of the other)."}, {"heading": "G. MAGIC Details", "text": "In this section we present additional details about the MAGIC algorithm. Specifically, we describe how we estimate the value of n and bn exactly before presenting pseudocode for MAGIC."}, {"heading": "G.1. Estimating \u2126n", "text": "We can write g (j) (D) as the sum of n terms: g (j) (D) (D) = k (D) = k (D) = k (However) (n) (v) (v) (j) i (D), (24) whereg (j) i (D): = (j) t = 0\u03b3twitR (Hi t) + (g) v (v) (SHij + 1) \u2212 j (n) t = 0\u03b3t (witq) p (SHit, A Hi t) \u2212 wit (j) k (SHit)) So, Cov (g) (i) (D) (i) (D), g (J) (D)) = Cov (n) k (D)."}, {"heading": "G.2. Estimating bn", "text": "As already described, we use a confidence interval, CI (g (\u221e) (D), \u03b4), to calculate billions. We explained that the confidence interval we use is a combination of the percentile bootstrap and the Chernoff-Hoeffding inequality. Specifically, we calculate the confidence interval generated by both methods and return the narrower of the two. In practice, this is almost always the confidence interval generated by the percentile bootstrap, and therefore, practical implementations of MAGIC can simply use the percentile bootstrap. We include the loose Chernoff-Hoeffding binding because it enables a simpler theoretical analysis of the MAGIC algorithm."}, {"heading": "G.3. Pseudocode", "text": "This structure is used to store the bootstraps so that we have the behavior of the D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D"}, {"heading": "H. Consistency of MAGIC", "text": "In this section, we demonstrate that if we meet requirements 1 and 4 and meet requirements 1 and 2, we must also meet requirements 1 and 4 to ensure that conditions 1 and 4 are sufficient to ensure that conditions 1 and 4 are sufficient, that conditions 1 and 4 are sufficient to ensure that conditions 1 and 4 are sufficient, that we can first establish two useful characteristics of selection criteria 1 and 2. In this section, we show that selection criteria 1 and 2 are not sufficient. In this section, we show that selection criteria 1 and 2 are not sufficient. In this section, we show that selection criteria 1 and 2 are not sufficient. In this section, we show that selection criteria 2 and 3 are not sufficient."}, {"heading": "H.3. Consistency of \u2126\u0302n", "text": "Here we find that the actual covariance matrix converges into the zero matrix. We then show that the actual covariance matrix converges into the zero matrix as well. We can then show that the actual covariance matrix converges into the zero matrix. (i, j) = E [(g (i) (D) \u2212 E [g (i) (D) \u2212 E (g) (g) (D) (g) (D) (D) (D) (D) \u2212 E) \u2212 E [g (j) (Yn), (31) whereby Yn: (i) (D) \u2212 E [g (i) (D) \u2212 E [g (i) (D)) and the actual covariance matrix (D) (D) \u2212 n)."}, {"heading": "H.4. Consistency of b\u0302n", "text": "Here we show that b \u03c0n \u2212 bn a.s. \u2212 \u2192 0. We have from the definitions of b \u30fb n, l, and u the: b \u043dn (j) \u2212 bn (j) \u2264 g (YY) (D) \u2212 l \u2212 E [g (YY) (D)] + v (\u03c0e) (32) andb \u30fb n (j) \u2212 bn (j) \u2265 g (YY) (D) \u2212 u \u2212 E (D) + v (\u03c0e). (33) We will show that both sides above us converge almost certainly to zero, which implies by Lemma 4 that b \u043dn (j) \u2212 bn (j) almost certainly converge to zero. Let's first consider (32). We have from Annex H.1 that 1) g (YY) (D) a.s. \u2212 cJj, and Wj."}, {"heading": "I. Extended Empirical Studies (MAGIC)", "text": "Here we present detailed results on the MAGIC estimator. These results are based on the same three domains and two experimental setups (full data and half data) introduced in Appendix D and an additional domain we call the hybrid domain. We start with the introduction of the hybrid domain, then discuss minor changes to the experimental setup, and then present the results."}, {"heading": "I.1. The Hybrid Domain", "text": "The purpose of this domain is to identify a common type of problem: areas where there is a partial observability at the beginning of a trajectory, but over time within each trajectory, partial observability disintegrates. This happens, for example, in robotics applications where there may be some uncertainty about the position or pose of a robot. However, as the trajectory progresses, the robot may be able to better locate itself and eliminate or reduce the uncertainty. We emulate this setting by linking the ModelFail and ModelWin domains, meaning that the agent starts in the ModelFail domains, and whenever it would go into the absorbing state, it instead passes into the initial state of the ModelWin domain."}, {"heading": "I.2. Experimental Setup", "text": "We have performed these experiments in the same way as those in Appendix D, except that we have compared different estimators. Specifically, we are introducing curves for the MAGIC estimator, but removing the curves for the low-performance sampling estimators IS, PDIS, WIS, and CWPDIS. Thus, the diagrams contain curves for DR, WDR, AM, and MAGIC. The legend used by all the diagrams in this appendix is included in Figure 18.DR AM WDR MAGIC-BFigure 18: The legend used by all the diagrams in Appendix I. Also for the hybrid range, we have inserted a curve for binary MAGIC (MAGIC-B) using J = {\u2212 1, \u221e}. While MAGIC is mixed between AM and WDR using off-policy Jstep yields of different lengths, binary MAGIC only has weights on AM and WDR."}, {"heading": "I.3. ModelFail Results", "text": "Figure 2b in Section 9 shows the results for the ModelFail domain in the full data setting. We reproduce this chart in Figure 19. In Figure 20 we show the results for ModelFail in the half data setting. There are hardly any differences between the charts - in both cases, MAGIC tracks WDR correctly, so that both WDR and MAGIC AM and DR exceed by at least one order of magnitude for most n."}, {"heading": "I.4. ModelWin Results", "text": "Figure 2c in Section 9 shows the results for the ModelWin domain in the full-data setting. We reproduce this graph in Figure 21. In Figure 22, we show the results for ModelFail in the half-data setting. In both cases, MAGIC tracks AM, although it moves slightly away as the n-value increases, suggesting that there may be room for improvement in our estimates of N and billion. Note, however, that due to the logarithmic scale, the difference between MAGIC and AM is small compared to the distance between MAGIC and DR."}, {"heading": "I.5. Gridworld Results", "text": "Figures 23 to 30 show the results for Gridworld FH and Gridworld-TH in both full and half-data settings. The same general trends can be seen: First, WDR tends to outperform DR, sometimes by an order of magnitude. Furthermore, MAGIC tends to track WDR, as it is usually the most powerful algorithm in these experiments. Finally, in Gridworld-TH, the full-data settings, DR, WDR and MAGIC all degenerate to AM, while in Gridworld-TH, half-data settings degenerate to about half the amount of data."}, {"heading": "I.6. Hybrid Results", "text": "Last but not least, Figures 31 and 32 show the results on the hybrid domain in the Full Data and Half Data settings, respectively. Note that MAGIC significantly outperforms all other methods, including WDR and AM. MAGIC also performs better than MAGIC-B, which shows how important it is to use non-policy j-step returns for different values of j."}, {"heading": "I.7. Summary", "text": "Overall, MAGIC behaves as desired - it tracks WDR or AM, depending on which is better for each application. Note, however, that it does not do this perfectly, especially when only a small amount of data is available. This is probably because it is difficult to estimate the values with small amounts of data, and the confidence interval used in estimating bn will be loose. In some cases, MAGIC finds it difficult to track AM properly. However, this tends to be the case when both methods perform well and possibly below 0.00010.010.010.01110.01010010002 20 200 2.000.000 ea nS qu are the rNumber of episodes, Figure 30: Gridworld-TH, full data. p1p20.00010.0010.010.010.1110002 20 200 2.000M ea nS qu are the rNumber of episodes, n Figure 30: Gridworld-TH, full data. p1p20.00010.010.010.010.1110002 20 200 2.000M ea nS qu are the rNumber of episodes, n Figure 30: Gridworld-TH, fast lose the importance between MDR and 2pDR due to both."}, {"heading": "J. Future Work", "text": "Good performance of MAGIC depends on our ability to efficiently estimate the N and billion values, and therefore improved estimators for these terms could lead to even better performance. If, for example, the weight of the sample is close to zero, then sampling estimates show high variance that is not captured by the covariance matrix we use. Another possible way forward would be to think about how MAGIC could be applied if our basic assumptions are violated. For example, what to do if the transition and reward functions of the MDP are not stationary? Can our estimators be extended to the average reward setting? What to do if the behavioral guidelines are not exactly known? If the approximate model is not initially provided but is constructed from the same data used to create the DR-, WDR-, WDR-, WDR-, orMAGIC-, orMAGIC-, WDR- and MAGIC estimates?"}], "references": [{"title": "The elements of integration and Lebesgue measure", "author": ["Bartle", "Robert G"], "venue": null, "citeRegEx": "Bartle and G.,? \\Q2014\\E", "shortCiteRegEx": "Bartle and G.", "year": 2014}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": "Machine Learning,", "citeRegEx": "Bradtke and Barto,? \\Q1996\\E", "shortCiteRegEx": "Bradtke and Barto", "year": 1996}, {"title": "Bootstrap Methods and their Application", "author": ["A.C. Davison", "D.V. Hinkley"], "venue": null, "citeRegEx": "Davison and Hinkley,? \\Q1997\\E", "shortCiteRegEx": "Davison and Hinkley", "year": 1997}, {"title": "Temporal difference Bayesian model averaging: A Bayesian perspective on adapting lambda", "author": ["C. Downey", "S. Sanner"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Downey and Sanner,? \\Q2010\\E", "shortCiteRegEx": "Downey and Sanner", "year": 2010}, {"title": "Doubly robust policy evaluation and learning", "author": ["M. Dud\u0131\u0301k", "J. Langford", "L. Li"], "venue": "In Proceedings of the TwentyEighth International Conference on Machine Learning,", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "An Introduction to the Bootstrap", "author": ["B. Efron", "R.J. Tibshirani"], "venue": null, "citeRegEx": "Efron and Tibshirani,? \\Q1993\\E", "shortCiteRegEx": "Efron and Tibshirani", "year": 1993}, {"title": "Monte carlo methods, methuen & co", "author": ["J.M. Hammersley", "D.C. Handscomb"], "venue": "Ltd., London, pp", "citeRegEx": "Hammersley and Handscomb,? \\Q1964\\E", "shortCiteRegEx": "Hammersley and Handscomb", "year": 1964}, {"title": "Doubly robust estimation in missing data and causal inference models", "author": ["H. Heejung", "J.M. Robins"], "venue": null, "citeRegEx": "Heejung and Robins,? \\Q2005\\E", "shortCiteRegEx": "Heejung and Robins", "year": 2005}, {"title": "Doubly robust off-policy evaluation for reinforcement learning", "author": ["N. Jiang", "L. Li"], "venue": "ArXiv, arXiv:1511.03722v1,", "citeRegEx": "Jiang and Li,? \\Q2015\\E", "shortCiteRegEx": "Jiang and Li", "year": 2015}, {"title": "TD\u03b3 : Re-evaluating complex backups in temporal difference learning", "author": ["G.D. Konidaris", "S. Niekum", "P.S. Thomas"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Konidaris et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2011}, {"title": "Guided policy search", "author": ["S. Levine", "V. Koltun"], "venue": "In Proceedings of The 30th International Conference on Machine Learning, pp", "citeRegEx": "Levine and Koltun,? \\Q2013\\E", "shortCiteRegEx": "Levine and Koltun", "year": 2013}, {"title": "Weighted importance sampling for off-policy learning with linear function approximation", "author": ["A.R. Mahmood", "H. Hasselt", "R.S. Sutton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mahmood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2014}, {"title": "Offline policy evaluation across representations with applications to educational games", "author": ["T. Mandel", "Y. Liu", "S. Levine", "E. Brunskill", "Z. Popovi\u0107"], "venue": "In Proceedings of the 13th International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Mandel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mandel et al\\.", "year": 2014}, {"title": "Offline evaluation of online reinforcement learning algorithms", "author": ["T. Mandel", "Y. Liu", "E. Brunskill", "Z. Popovi\u0107"], "venue": "In Proceedings of the Thirtieth Conference on Artificial Intelligence,", "citeRegEx": "Mandel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mandel et al\\.", "year": 2016}, {"title": "Mathematical statistics for economics and business, volume 78", "author": ["R.C. Mittelhammer"], "venue": null, "citeRegEx": "Mittelhammer,? \\Q1996\\E", "shortCiteRegEx": "Mittelhammer", "year": 1996}, {"title": "Weighted uniform sampling: a Monte Carlo technique for reducing variance", "author": ["M.J.D. Powell", "J. Swann"], "venue": "Journal of the Institute of Mathematics and its Applications,", "citeRegEx": "Powell and Swann,? \\Q1966\\E", "shortCiteRegEx": "Powell and Swann", "year": 1966}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "Precup et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "Semiparametric regression estimation in the presence of dependent censoring", "author": ["A. Rotnitzky", "J.M. Robins"], "venue": null, "citeRegEx": "Rotnitzky and Robins,? \\Q1995\\E", "shortCiteRegEx": "Rotnitzky and Robins", "year": 1995}, {"title": "Large Sample Methods in Statistics An Introduction With Applications", "author": ["P.K. Sen", "J.M. Singer"], "venue": null, "citeRegEx": "Sen and Singer,? \\Q1993\\E", "shortCiteRegEx": "Sen and Singer", "year": 1993}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Agent based decision support system using reinforcement learning under emergency circumstances", "author": ["D. Thapa", "I. Jung", "G. Wang"], "venue": "Advances in Natural Computation,", "citeRegEx": "Thapa et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Thapa et al\\.", "year": 2005}, {"title": "Personalized ad recommendation systems for life-time value optimization with guarantees", "author": ["G. Theocharous", "P.S. Thomas", "M. Ghavamzadeh"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence,", "citeRegEx": "Theocharous et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theocharous et al\\.", "year": 2015}, {"title": "A notation for Markov decision processes", "author": ["P.S. Thomas"], "venue": "ArXiv, arXiv:1512.09075v1,", "citeRegEx": "Thomas,? \\Q2015\\E", "shortCiteRegEx": "Thomas", "year": 2015}, {"title": "Safe Reinforcement Learning", "author": ["P.S. Thomas"], "venue": "PhD thesis, University of Massachusetts Amherst,", "citeRegEx": "Thomas,? \\Q2015\\E", "shortCiteRegEx": "Thomas", "year": 2015}, {"title": "Policy evaluation using the \u03a9-return", "author": ["P.S. Thomas", "S. Niekum", "G. Theocharous", "G.D. Konidaris"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Thomas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2015}, {"title": "Offpolicy TD(\u03bb) with true online equivalence", "author": ["H. van Hasselt", "A.R. Mahmood", "R.S. Sutton"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Hasselt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2014}, {"title": "Variance reduction in monte-carlo tree search", "author": ["J. Veness", "M. Lanctot", "M. Bowling"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Veness et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2011}, {"title": "Learning a value analysis tool for agent evaluation", "author": ["M. White", "M. Bowling"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence,", "citeRegEx": "White and Bowling,? \\Q1976\\E", "shortCiteRegEx": "White and Bowling", "year": 1976}, {"title": "Optimal unbiased estimators for evaluating agent performance", "author": ["M. Zinkevich", "M. Bowling", "N. Bard", "M. Kan", "D. Billings"], "venue": "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Zinkevich et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2006}, {"title": "theDR estimator for a single trajectory (i.e., n = 1) as the last element", "author": ["Jiang", "Li"], "venue": "XL, of a sequence,", "citeRegEx": "Jiang and Li,? \\Q2015\\E", "shortCiteRegEx": "Jiang and Li", "year": 2015}, {"title": "DR is Unbiased While Jiang & Li (2015) showed that the DR estimator (with finite horizon) is an unbiased estimator of v(\u03c0e)", "author": ["Yk. B"], "venue": null, "citeRegEx": "B.2.,? \\Q2015\\E", "shortCiteRegEx": "B.2.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": ", for determining which advertisement to show a user visiting a website (Theocharous et al., 2015), for determining which medical treatment to suggest for a patient (Thapa et al.", "startOffset": 72, "endOffset": 98}, {"referenceID": 21, "context": ", 2015), for determining which medical treatment to suggest for a patient (Thapa et al., 2005), or for suggesting a personalized curriculum for a student (Mandel et al.", "startOffset": 74, "endOffset": 94}, {"referenceID": 12, "context": ", 2005), or for suggesting a personalized curriculum for a student (Mandel et al., 2014).", "startOffset": 67, "endOffset": 88}, {"referenceID": 16, "context": "This is in line with previous works that all use (root) mean squared error when empirically validating their methods (Precup et al., 2000; Dud\u0131\u0301k et al., 2011; Mahmood et al., 2014; Thomas, 2015b; Jiang & Li, 2015).", "startOffset": 117, "endOffset": 214}, {"referenceID": 4, "context": "This is in line with previous works that all use (root) mean squared error when empirically validating their methods (Precup et al., 2000; Dud\u0131\u0301k et al., 2011; Mahmood et al., 2014; Thomas, 2015b; Jiang & Li, 2015).", "startOffset": 117, "endOffset": 214}, {"referenceID": 11, "context": "This is in line with previous works that all use (root) mean squared error when empirically validating their methods (Precup et al., 2000; Dud\u0131\u0301k et al., 2011; Mahmood et al., 2014; Thomas, 2015b; Jiang & Li, 2015).", "startOffset": 117, "endOffset": 214}, {"referenceID": 14, "context": "For a review of OPE methods, see the works of Precup et al. (2000) or Thomas (2015b, Chapter 3).", "startOffset": 46, "endOffset": 67}, {"referenceID": 14, "context": "For a review of OPE methods, see the works of Precup et al. (2000) or Thomas (2015b, Chapter 3). More recent methods can be found in the works of Jiang & Li (2015) and Mandel et al.", "startOffset": 46, "endOffset": 164}, {"referenceID": 12, "context": "More recent methods can be found in the works of Jiang & Li (2015) and Mandel et al. (2016).", "startOffset": 71, "endOffset": 92}, {"referenceID": 16, "context": "Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning pling (Precup et al., 2000).", "startOffset": 77, "endOffset": 98}, {"referenceID": 4, "context": "The work that introduced the DR estimator for MDPs (Jiang & Li, 2015) derived it as a generalization of a doubly robust estimator for bandits (Dud\u0131\u0301k et al., 2011).", "startOffset": 142, "endOffset": 163}, {"referenceID": 29, "context": "Advantage sum estimators were introduced as a way to lower the variance of on-policy Monte Carlo performance estimates for a setting that is a generalization of the (partially observable) MDP setting (Zinkevich et al., 2006; White & Bowling, 2009).", "startOffset": 200, "endOffset": 247}, {"referenceID": 29, "context": "One may therefore view the DR estimator (Jiang & Li, 2015) as the extension of the advantage sum estimator (Zinkevich et al., 2006) to the off-policy setting or as the extension of the doubly robust estimator for bandits (Dud\u0131\u0301k et al.", "startOffset": 107, "endOffset": 131}, {"referenceID": 4, "context": ", 2006) to the off-policy setting or as the extension of the doubly robust estimator for bandits (Dud\u0131\u0301k et al., 2011) to the sequential setting.", "startOffset": 97, "endOffset": 118}, {"referenceID": 22, "context": ", bootstrap confidence bounds) are used instead (Theocharous et al., 2015; Thomas, 2015b).", "startOffset": 48, "endOffset": 89}, {"referenceID": 14, "context": "For example, in their experiments, Precup et al. (2000), Dud\u0131\u0301k et al.", "startOffset": 35, "endOffset": 56}, {"referenceID": 4, "context": "(2000), Dud\u0131\u0301k et al. (2011), Mahmood et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 4, "context": "(2000), Dud\u0131\u0301k et al. (2011), Mahmood et al. (2014), Thomas (2015b), and Jiang & Li (2015) all use the (root) MSE when evaluating OPE methods.", "startOffset": 8, "endOffset": 52}, {"referenceID": 4, "context": "(2000), Dud\u0131\u0301k et al. (2011), Mahmood et al. (2014), Thomas (2015b), and Jiang & Li (2015) all use the (root) MSE when evaluating OPE methods.", "startOffset": 8, "endOffset": 68}, {"referenceID": 4, "context": "(2000), Dud\u0131\u0301k et al. (2011), Mahmood et al. (2014), Thomas (2015b), and Jiang & Li (2015) all use the (root) MSE when evaluating OPE methods.", "startOffset": 8, "endOffset": 91}, {"referenceID": 16, "context": "Weighted importance sampling has been used before for OPE (Precup et al., 2000), but not in conjunction with the DR estimator.", "startOffset": 58, "endOffset": 79}, {"referenceID": 4, "context": "In the ModelFail experiments, the approximate model This model-based estimator has been called the direct method in previous work (Dud\u0131\u0301k et al., 2011), however, in other previous work direct methods are model-free while indirect methods are model-based (Sutton & Barto, 1998, Section 9.", "startOffset": 130, "endOffset": 151}, {"referenceID": 20, "context": "The \u03bb-return is the foundation of the entire TD(\u03bb) family of algorithms, which includes the original linear-time algorithm (Sutton, 1988), least-squares formulations (Bradtke & Barto, 1996; Mahmood et al.", "startOffset": 123, "endOffset": 137}, {"referenceID": 11, "context": "The \u03bb-return is the foundation of the entire TD(\u03bb) family of algorithms, which includes the original linear-time algorithm (Sutton, 1988), least-squares formulations (Bradtke & Barto, 1996; Mahmood et al., 2014), methods for adapting \u03bb (Downey & Sanner, 2010), true-online methods (van Hasselt et al.", "startOffset": 166, "endOffset": 211}, {"referenceID": 9, "context": "Recent work has suggested that the \u03bb-return could be replaced by more statistically principled complex returns like the \u03b3-return (Konidaris et al., 2011) or \u03a9return (Thomas et al.", "startOffset": 129, "endOffset": 153}, {"referenceID": 25, "context": ", 2011) or \u03a9return (Thomas et al., 2015).", "startOffset": 19, "endOffset": 40}, {"referenceID": 16, "context": "First, notice that \u2211\u221e t=0 \u03b3 t\u03c1i t R Hi t is the per-decision importance sampling (PDIS) estimator, which is known to be an unbiased estimator of v(\u03c0e) (Precup et al., 2000; Thomas, 2015b).", "startOffset": 151, "endOffset": 187}, {"referenceID": 16, "context": "See the works of (Precup et al., 2000) or (Thomas, 2015b, Lemma 1).", "startOffset": 17, "endOffset": 38}, {"referenceID": 16, "context": "In this definition theX term is the per-decision importance sampling (PDIS) estimator, which is known to be an unbiased and strongly consistent estimator of v(\u03c0e) (Precup et al., 2000; Thomas, 2015b).", "startOffset": 163, "endOffset": 199}, {"referenceID": 16, "context": "We begin with the per-decision importance sampling (PDIS) estimator, which is known to be an unbiased and strongly consistent estimator of v(\u03c0e) (Precup et al., 2000; Thomas, 2015b).", "startOffset": 145, "endOffset": 181}, {"referenceID": 23, "context": "The Gridworld Domain The third domain that we used was the gridworld domain developed by Thomas (2015b, Section 2.5) for evaluating OPE algorithms. It is a 4 \u00d7 4 gridworld with four actions, L = 100, and deterministic transition and reward functions. This domain was developed specifically for evaluating different OPE methods. Thomas (2015b) proposed five policies, \u03c01, .", "startOffset": 89, "endOffset": 343}], "year": 2016, "abstractText": "In this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy. The ability to evaluate a policy from historical data is important for applications where the deployment of a bad policy can be dangerous or costly. We show empirically that our algorithm produces estimates that often have orders of magnitude lower mean squared error than existing methods\u2014it makes more efficient use of the available data. Our new estimator is based on two advances: an extension of the doubly robust estimator (Jiang & Li, 2015), and a new way to mix between model based estimates and importance sampling based estimates.", "creator": "LaTeX with hyperref package"}}}