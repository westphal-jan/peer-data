{"id": "1604.01350", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2016", "title": "Bounded Optimal Exploration in MDP", "abstract": "Within the framework of probably approximately correct Markov decision processes (PAC-MDP), much theoretical work has focused on methods to attain near optimality after a relatively long period of learning and exploration. However, practical concerns require the attainment of satisfactory behavior within a short period of time. In this paper, we relax the PAC-MDP conditions to reconcile theoretically driven exploration methods and practical needs. We propose simple algorithms for discrete and continuous state spaces, and illustrate the benefits of our proposed relaxation via theoretical analyses and numerical examples. Our algorithms also maintain anytime error bounds and average loss bounds. Our approach accommodates both Bayesian and non-Bayesian methods.", "histories": [["v1", "Tue, 5 Apr 2016 18:00:02 GMT  (1762kb,D)", "http://arxiv.org/abs/1604.01350v1", "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI), 2016"]], "COMMENTS": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI), 2016", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["kenji kawaguchi"], "accepted": true, "id": "1604.01350"}, "pdf": {"name": "1604.01350.pdf", "metadata": {"source": "CRF", "title": "Bounded Optimal Exploration in MDP", "authors": ["Kenji Kawaguchi"], "emails": ["kawaguch@mit.edu"], "sections": [{"heading": null, "text": "The formulation of sequential decision-making as the Markov decision-making process (MDP) has been successfully applied to a number of real-world problems (MDPs provide the ability to design adaptable agents that can work effectively in unsafe environments). In many situations, the environment we want to model has unknown aspects, and therefore the actor needs to learn an MDP by interacting with the environment. In other words, the actor needs to explore the unknown aspects of the environment in order to learn the MDP. A considerable amount of theoretical work on MDPs has focused on efficient exploration, and a number of principled methods have been derived with the aim of learning an MDP in order to obtain approximately optimal policy. Kearns and Singh (2002) and Strehl and Littman (2008a) consider discrete state spaces, whereas Bernstein and Shimkin (2010) and Parr (2013) investigated continuous state spaces. However, hayrics are still frequently applied in 2012 (Li)."}, {"heading": "Reachability in Model Learning", "text": "For each state-action pair (s, f), M (s, a) is a compilation of all transition models and P (s, a) is a compilation of possible future samples as S (s, a) = P (s, s, a) > 0 (s, a): S (s, a): M (s, a) \u00b7 S (s, a) \u00b7 S (s, a) as a compilation of possible future samples as S (s, a) = P (s, s, a) > 0 (s, a) > (a): M (s, a) \u00b7 S (s, a) \u00b7 S (s, s, a) represent the model update rule; f (s, a) forms a model (in M (s, a) and a new sample (s, s, a): M (s, a) a corresponding new model (s, a)."}, {"heading": "PAC in Reachable MDP", "text": "Using the concept of accessibility in the Learning model, we define the term \"probably roughly correct\" in anh-attainable MDP (PAC-RMDP (h)). Let's leave P (x1, x2,.., xn) a polynomial in x1, x2,., while a reward method L is PDP-RMDP (h) in relation to a removal function d, if for all > 0 and for all Higgs values. \u2212 Definition 1. (PAC-RMDP (h) An algorithm with a policy At and a learning method L is PDP-RMDP (h) in relation to a removal function d, if for all > 0 and for all Higgs values. \u2212 Definition 1. (PAC-RMDP (h), 1) there is an algorithm with a policy At and a learning method L-RMDP (h)."}, {"heading": "Parameter: h \u2265 0", "text": "To illustrate the proposed concept, we will first consider a simple case that includes limited state and action spaces with an unknown transitional function P. Without loss of universality, we assume that the reward function R is known."}, {"heading": "Algorithm", "text": "To derive the algorithm, we use the principle of optimism in the face of uncertainty, so that V-A (s) \u2265 V-D-D-L, t, h (s) for all s-S. This can be achieved by using the following internal value function: V-A (s) = max a, P-D-ML, t, h, (s, a). In the following, we shall consider the special case in which we use the sample mean estimate (which determines L-D). That is, we shall use P-T (s), a-D (s), n (s), n (s), n (s), n (s), n (s), n (s), n (s), n (s), n (s), n (s), n (s), c (s), n (s), n (s), n (s), n (s), c (s), n (c), c, c, c, c, c, c, c, c, c, c, c, c, c, c, c), c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c), c, c, c, c, c, c, c, c, c, c, c), c, c, c, c, c, c, c, c, c, c, c, c), c, c, c, c, c, c, c, c, c, c), c, c, c, c, c, c, c, c, c), c, c, c, c, c, c), c, c, c, c, c, c, c), c, c, c, c, c, c), c, c, c, c, c, c, c), c, c, c), c, c, c, c, c), c, c, c, c, c, c), c, c, c, c), c, c, c, c, c), c, c, c, c), c, c, c), c, c, c, c,"}, {"heading": "Analysis", "text": "We first show that algorithm 1 PAC-RMDP (h) for all h-RMDP (h) for all h-RMDP (h) = 0-RMDP (h) for all h-RMDP (h) = 0-RMDP (h). We assume that algorithm 1 is used with the random mean estimator, the L. We fix the distance function as d (P), P (\u00b7 s, a) = explicit exploration runtime (definition 3). We assume that algorithm 1 is used with the random mean estimator, the L. theorem 1. (PAC-RMDP) LetAt be a Policy of Algorithm 1. Let z = max (h, ln (2 | S | S | S | P, a). The evidence is in the Appendix.Theorem 1. (PAC-RMDP) LetAt be a Policy of Algorithm 1. Let z = max."}, {"heading": "Experimental Example", "text": "We compare the proposed algorithms with MBIE (Strehl and Littman 2008a), Variance-based Exploration (VBE) (Sorg, Singh and Lewis 2010), Bayesian Exploration Bonus (BEB) (Kolter and Ng 2009), and BOLT (Araya-Lo \u0301 pez, Thomas and Buffet 2012). These algorithms are designed to be optimal PAC-MDP or Near-Bayes, but have been used with parameter settings that do not make them optimal either PAC-MDP or Near-Bayes. Unlike the experiments of previous research, we present the results with several theoretically meaningful values 2 and a theoretically inconclusive value to illustrate their effectiveness."}, {"heading": "Algorithm", "text": "Let us first define the variables used in our algorithm, and then explain how the algorithm works. Let us immediately (i) be the vector of the model parameters for the ith component. Let Xt, i-Rt \u00b7 ni consist of t-input vectors \u03a6T (i) (s, a-R1 \u00b7 ni at time. We then designate the eigenvalue composition of the input matrix as XTt, iXt, i = Ut, iDt, i (\u03bb (1), i (n)) UTt, i (1), where Dt, i (\u03bb (1), i-Rni \u00b7 ni represent a diagonal matrix. For the simplicity of notation, we arrange the eigenvectors and eigenvalues such that the diagonal elements of Dt, i (1),"}, {"heading": "Analysis", "text": "Following on from the previous paper (Strehl and Littman 2008b; Li et al. 2011), we assume an access to an exact planning algorithm, which would be loosened by the use of a planning method that provides a margin of error. We assume that algorithm 2 is used with an estimate of the smallest squares, the L. We fix the distance function as d (P), P (s, a), P (s, a), P (s), P (s), P (s, a). \u2212 Es (s, a). \u2212 P (s, a) [s) | (since the unknown aspect is the mean, this choice makes sense). In the following, we use n to determine the average value of {n), n (nS)}. The evidence is given in the appendix."}, {"heading": "Experimental Examples", "text": "In fact, the fact is that most of us are able to hold our own, and that they are able to hold their own, \"he said."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Within the framework of probably approximately correct Markov decision processes (PAC-MDP), much theoretical work has focused on methods to attain near optimality after a relatively long period of learning and exploration. However, practical concerns require the attainment of satisfactory behavior within a short period of time. In this paper, we relax the PAC-MDP conditions to reconcile theoretically driven exploration methods and practical needs. We propose simple algorithms for discrete and continuous state spaces, and illustrate the benefits of our proposed relaxation via theoretical analyses and numerical examples. Our algorithms also maintain anytime error bounds and average loss bounds. Our approach accommodates both Bayesian and nonBayesian methods.", "creator": "LaTeX with hyperref package"}}}