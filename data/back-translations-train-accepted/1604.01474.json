{"id": "1604.01474", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "Self-Paced Multi-Task Learning", "abstract": "In this paper, we propose a novel multi-task learning (MTL) framework, called Self-Paced Multi-Task Learning (SPMTL). Different from previous works treating all tasks and instances equally when training, SPMTL attempts to jointly learn the tasks by taking into consideration the complexities of both tasks and instances. This is inspired by the cognitive process of human brain that often learns from the easy to the hard. We construct a compact SPMTL formulation by proposing a new task-oriented regularizer that can jointly prioritize the tasks and the instances. Thus it can be interpreted as a self-paced learner for MTL. A simple yet effective algorithm is designed for optimizing the proposed objective function. An error bound for a simplified formulation is also analyzed theoretically. Experimental results on toy and real-world datasets demonstrate the effectiveness of the proposed approach, compared to the state-of-the-art methods.", "histories": [["v1", "Wed, 6 Apr 2016 03:44:03 GMT  (211kb,D)", "http://arxiv.org/abs/1604.01474v1", null], ["v2", "Mon, 3 Apr 2017 02:28:32 GMT  (153kb,D)", "http://arxiv.org/abs/1604.01474v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changsheng li", "junchi yan", "fan wei", "weishan dong", "qingshan liu", "hongyuan zha"], "accepted": true, "id": "1604.01474"}, "pdf": {"name": "1604.01474.pdf", "metadata": {"source": "CRF", "title": "Self-Paced Multi-Task Learning", "authors": ["Changsheng Li", "Fan Wei", "Junchi Yan", "Weishan Dong", "Qingshan Liu", "Hongyuan Zha"], "emails": ["dongweis}@cn.ibm.com.", "fanwei@stanford.edu.", "jcyan@sei.ecnu.edu.cn", "qsliu@nuist.edu.cn.", "zha@cc.gatech.edu"], "sections": [{"heading": null, "text": "In fact, it is such that the greater people who are able to surpass themselves, to surpass themselves, to surpass themselves, to overtake, to overtake, to overtake, to overtake, to overtake, to overtake, to overtake, to overtake, to overtake, to overtake, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy"}, {"heading": "II. SELF-PACED MULTI-TASK LEARNING", "text": "Suppose we were to perform m-learning tasks {Ti} mi = 1. For the i-th task Ti = 1. \"The training consists of ni data points {(xij, yij) nij = 1.\" The total number of tasks to be performed is n = 1. \"The predictive model for the ith task is defined as f (pi, xij) = pTi xij.\" The aim of the MTL is to derive optimal prediction models for all tasks simultaneously. In practical applications, information is often divided into the same task group, and information in different task groups is also overlapped to some extent. Thus, the MTL problem can be formulated by GO-MTL [21]."}, {"heading": "III. OPTIMIZATION", "text": "We solve the optimization problem (4) alternately by the following three main steps: i) Optimize w (with fixed U and V: the optimal w can be achieved by dividing the optimization function into individual problems for each task Ti: min w (i) 0 (0,1) niLi: = 1ni w (i) L (i) 0 (i) - 1 (i) - 1 (i) - 1 (i) - 2 (5) First we assume that L (i) 1 (i) 2 (i) - 2 (i) - 2 (i) - L (i) - 1 (i) - 1 (i) - 1 (i) - 2 (i) - 2 (i) - 2 (i) - 2 (i) - 0 < k1 (i) - 2 (i) - 0 < k1) - L (i) - 2 (i) - 2 (i)."}, {"heading": "IV. THEORETICAL ANALYSIS", "text": "Here we relax the two penalty terms. (2) This relaxation makes our following theoretical analysis approximate, while more mathematically comprehensible and usable. (2) In fact, this simplification shares the same motivation for our crude formulation as intuitively we want U to simplify V. The true values are yrij = v rT i U rTxij. (2) We observe yij = yrij + eij eij + eij the distribution with the mean 0. (2) In each turn to solve for U, V is to solvemin U = 1 ni ni i = 1 w (i) j (yij \u2212 v T i Txij) 2. (14) Let the weighted error (or the \"average\" error) be defined."}, {"heading": "V. EXPERIMENTS", "text": "We compare the experiments with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29] and a recently proposed method called Calibration [30].1 In addition, we also compare with Single Task Learning (STL), where the tasks are learned independently of each other. Note that we do not compare with SeqMT [22] because it is difficult to have a fair comparison, as SeqMT is tailored to classification tasks, while our current model focuses on regression problems. In all data sets, we randomly select the training instances from each task with different training ratios (10%, 20% and 30%) and use the rest of the instances to form the test set. We evaluate all algorithms in terms of Root Mean Square Errors (rMSE) and Normalized Mean Problems (MSE)."}, {"heading": "A. Toy Example", "text": "There are 100 cases in each task; each instance is represented by a 15-dimensional feature vector. We generate parameter vectors for 4 latent tasks, i.e. the next 10 tasks are generated by combining the second and third latent tasks, the latter task being generated by a linear combination of the last two latent tasks. All coefficients of linear combinations are drawn."}, {"heading": "B. Real-World Data Experiments", "text": "In this sector, we conduct the experiments on two real datasets: oHSUM and Isolet. The first dataset is an online bibliographic document, which is able to make 53 requests, which are our tasks. In total, there are 7,546 instances. The second dataset is of 150 people, who are able to be able to be able to be. Thus, there are 52 samples of each person, who is able to be able to be able to be able to be. The second dataset is of 150 people, who are able to be able to be able to be."}, {"heading": "VI. CONCLUSION", "text": "We proposed a new MTL algorithm, SPMTL. Unlike previous work, which treated tasks and instances equally during learning, our model gradually encompasses instances and tasks in a simple to hard order. To achieve this goal, we propose a new task-oriented, self-determined regulator in our 6 formulation. Experiments with both synthetic and real data sets have verified the effectiveness of SPMTL."}, {"heading": "VII. APPENDIX", "text": "The optimal solution of (5) should therefore w (i) 1 (i) 1 (i) 2 (0) 2 (0) 2 (0) 2 (0) 2 (0) 2 (0) 2 (0) 0 (0) 2 (0) 2 (0) 2 (0) 0 (0) 0 (0) 0 (0) 0 (0) 0 (0) 0 (0) 1 (0). For a given j we have Li (i) w (i) j (i) j (i) j (i) 1 (i) i (i) ni (i) ni (i) i (i) ni (i) i) ni (i) ni (i) ni (i) ni (i) ni (i) ni (i) ni (i) ni (i) ni (i) i) ni (i) i (i) i (i) i (i) i (i) ni (i) ni (i) ni (i) ni (i) ni (i) ni (i) ni) ni (i) ni (i) ni) ni)."}], "references": [{"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine learning, vol. 28, no. 1, pp. 41\u201375, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "A multi-task learning formulation for predicting disease progression", "author": ["J. Zhou", "L. Yuan", "J. Liu", "J. Ye"], "venue": "KDD, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Boosted multi-task learning for face verification with applications to web image and video search", "author": ["X. Wang", "C. Zhang", "Z. Zhang"], "venue": "CVPR, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Predicting multiple attributes via relative multi-task learning", "author": ["L. Chen", "Q. Zhang", "B. Li"], "venue": "CVPR, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "ICML, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple task learning using iteratively reweighted least square", "author": ["J. Pu", "Y.-G. Jiang", "J. Wang", "X. Xue"], "venue": "IJCAI, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, vol. 73, no. 3, pp. 243\u2013272, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Tree-guided group lasso for multi-task regression with structured sparsity", "author": ["S. Kim", "E.P. Xing"], "venue": "2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic multi-task feature selection", "author": ["Y. Zhang", "D.-Y. Yeung", "Q. Xu"], "venue": "NIPS, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-task feature learning via efficient l 2, 1-norm minimization", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "UAI, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "An efficient projection for l1,\u221e regularization", "author": ["A. Quattoni", "X. Carreras", "M. Collins", "T. Darrell"], "venue": "ICML, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Trace norm regularization: Reformulations, algorithms, and multi-task learning", "author": ["T.K. Pong", "P. Tseng", "S. Ji", "J. Ye"], "venue": "SIAM Journal on Optimization, vol. 20, no. 6, pp. 3465\u20133489, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning multiple related tasks using latent independent component analysis", "author": ["J. Zhang", "Z. Ghahramani", "Y. Yang"], "venue": "NIPS, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "JMLR, vol. 6, pp. 1817\u20131853, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1817}, {"title": "A convex formulation for learning shared structures from multiple tasks", "author": ["J. Chen", "L. Tang", "J. Liu", "J. Ye"], "venue": "ICML, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-task learning with gaussian matrix generalized inverse gaussian model", "author": ["M. Yang", "Y. Li"], "venue": "ICML, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian max-margin multi-task learning with data augmentation", "author": ["C. Li", "J. Zhu", "J. Chen"], "venue": "ICML, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-task model and feature joint learning", "author": ["Y. Li", "X. Tian", "T. Liu", "D. Tao"], "venue": "IJCAI, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "A dirty model for multi-task learning", "author": ["A. Jalali", "S. Sanghavi", "C. Ruan", "P.K. Ravikumar"], "venue": "NIPS, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Integrating low-rank and group-sparse structures for robust multi-task learning", "author": ["J. Chen", "J. Zhou", "J. Ye"], "venue": "KDD, 2011.  7", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daume III"], "venue": "ICML, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Curriculum learning of multiple tasks", "author": ["A. Pentina", "V. Sharmanska", "C.H. Lampert"], "venue": "CVPR, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "B. Packer", "D. Koller"], "venue": "NIPS, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Selfpaced learning with diversity", "author": ["L. Jiang", "D. Meng", "S.-I. Yu", "Z. Lan", "S. Shan", "A. Hauptmann"], "venue": "NIPS, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-view self-paced learning for clustering", "author": ["C. Xu", "D. Tao", "C. Xu"], "venue": "IJCAI, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-paced learning for matrix factorization", "author": ["Q. Zhao", "D. Meng", "L. Jiang", "Q. Xie", "Z. Xu", "A.G. Hauptmann"], "venue": "AAAI, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "A selfpaced multiple-instance learning framework for co-saliency detection", "author": ["D. Zhang", "D. Meng", "C. Li", "L. Jiang", "Q. Zhao", "J. Han"], "venue": "ICCV, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Regularized multi-task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "KDD, 2004.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-stage multi-task feature learning", "author": ["P. Gong", "J. Ye", "C. Zhang"], "venue": "JMLR, vol. 14, no. 1, pp. 2979\u20133010, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient multi-task feature learning with calibration", "author": ["P. Gong", "J. Zhou", "W. Fan", "J. Ye"], "venue": "KDD, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "UAI, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Factorial multi-task learning: a bayesian nonparametric approach", "author": ["S. Gupta", "D. Phung", "S. Venkatesh"], "venue": "ICML, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "One basic assumption in MTL is that there exists common or related information among the tasks; learning such information can result in better generalization performance than independently learning each individual task [1].", "startOffset": 219, "endOffset": 222}, {"referenceID": 1, "context": "Due to its empirical success, MTL has been applied to various domains, including disease modeling and prediction [2], web image and video search [3], relative attributes learning [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "Due to its empirical success, MTL has been applied to various domains, including disease modeling and prediction [2], web image and video search [3], relative attributes learning [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 3, "context": "Due to its empirical success, MTL has been applied to various domains, including disease modeling and prediction [2], web image and video search [3], relative attributes learning [4].", "startOffset": 179, "endOffset": 182}, {"referenceID": 4, "context": "Many multi-task learning methods have been proposed, which in general can be categorized into two classes [5], [6].", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "Many multi-task learning methods have been proposed, which in general can be categorized into two classes [5], [6].", "startOffset": 111, "endOffset": 114}, {"referenceID": 6, "context": "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,\u221e norm regularization [11] and trace norm regularization [12].", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,\u221e norm regularization [11] and trace norm regularization [12].", "startOffset": 97, "endOffset": 100}, {"referenceID": 8, "context": "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,\u221e norm regularization [11] and trace norm regularization [12].", "startOffset": 145, "endOffset": 148}, {"referenceID": 9, "context": "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,\u221e norm regularization [11] and trace norm regularization [12].", "startOffset": 210, "endOffset": 214}, {"referenceID": 10, "context": "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,\u221e norm regularization [11] and trace norm regularization [12].", "startOffset": 241, "endOffset": 245}, {"referenceID": 11, "context": "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,\u221e norm regularization [11] and trace norm regularization [12].", "startOffset": 276, "endOffset": 280}, {"referenceID": 12, "context": "The other class of methods assumes that the model parameters used by the tasks are related to each other [13], [14], [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "The other class of methods assumes that the model parameters used by the tasks are related to each other [13], [14], [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "The other class of methods assumes that the model parameters used by the tasks are related to each other [13], [14], [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "edu [16], [17], [18].", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "edu [16], [17], [18].", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "edu [16], [17], [18].", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "However, the assumption that all the tasks share some common information is fairly strong, recent works propose task grouping or task outlier detecting, which assume that there exists common information only within a subset of tasks, or there exists outlier task having no relation with the other tasks [19], [20], [21].", "startOffset": 303, "endOffset": 307}, {"referenceID": 19, "context": "However, the assumption that all the tasks share some common information is fairly strong, recent works propose task grouping or task outlier detecting, which assume that there exists common information only within a subset of tasks, or there exists outlier task having no relation with the other tasks [19], [20], [21].", "startOffset": 309, "endOffset": 313}, {"referenceID": 20, "context": "However, the assumption that all the tasks share some common information is fairly strong, recent works propose task grouping or task outlier detecting, which assume that there exists common information only within a subset of tasks, or there exists outlier task having no relation with the other tasks [19], [20], [21].", "startOffset": 315, "endOffset": 319}, {"referenceID": 21, "context": "Recently, SeqMT [22] uses a heuristic algorithm to rank all the tasks, and learns the tasks sequentially for multi-task classification tasks.", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "Recently, self-paced learning (SPL) [23] proposes a paradigm advocating that learning should be first done for \u2018simple\u2019 or \u2018easy\u2019 instances, and then gradually move to \u2018complex\u2019 or \u2018hard\u2019 instances, inspired by the cognitive process of humans.", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "Self-paced learning has been empirically demonstrated to be helpful for avoiding bad local minima, especially in the presence of heavy noise and outliers [23], [24].", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "Self-paced learning has been empirically demonstrated to be helpful for avoiding bad local minima, especially in the presence of heavy noise and outliers [23], [24].", "startOffset": 160, "endOffset": 164}, {"referenceID": 24, "context": "By now, it has been successfully applied to many research areas, such as multi-view learning [25], matrix factorization [26], and multi-instance learning [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 25, "context": "By now, it has been successfully applied to many research areas, such as multi-view learning [25], matrix factorization [26], and multi-instance learning [27].", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "By now, it has been successfully applied to many research areas, such as multi-view learning [25], matrix factorization [26], and multi-instance learning [27].", "startOffset": 154, "endOffset": 158}, {"referenceID": 20, "context": "Thus the MTL problem can be formulated by GO-MTL [21] as:", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "Previous works have demonstrated that adding prior information, such as the easy-to-hard strategy, can alleviate the problem of bad local minima [23].", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "w (i) j \u2208 [0, 1],\u2200j = 1, .", "startOffset": 10, "endOffset": 16}, {"referenceID": 22, "context": "So far, many self-paced regularizers have been proposed for various tasks [23], [27], but almost all of them focus on imposing weights on the instance level.", "startOffset": 74, "endOffset": 78}, {"referenceID": 26, "context": "So far, many self-paced regularizers have been proposed for various tasks [23], [27], but almost all of them focus on imposing weights on the instance level.", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": ", w (i) ni ] \u2208 [0, 1]i , and thus w = [w, .", "startOffset": 15, "endOffset": 21}, {"referenceID": 6, "context": "We know that minimizing the l2,1 norm of a matrix can make the matrix sparse in rows or columns [7]; thus minimizing the second term in (3) makes the w\u2019s corresponding to large empirical loss L (i.", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "w \u2208 [0, 1]i ,\u2200i = 1, .", "startOffset": 4, "endOffset": 10}, {"referenceID": 27, "context": "Note our method can be naturally applied to classification tasks by using a classification loss function L j as in [28].", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "min w(i)\u2208[0,1]ni Li := 1 ni wL\u0302\u2212\u03bb\u2016w\u20161+ \u03b3 \u221a ni \u2016w\u20162.", "startOffset": 9, "endOffset": 14}, {"referenceID": 4, "context": "We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and a recently proposed method, called Calibration [30].", "startOffset": 74, "endOffset": 77}, {"referenceID": 20, "context": "We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and a recently proposed method, called Calibration [30].", "startOffset": 86, "endOffset": 90}, {"referenceID": 28, "context": "We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and a recently proposed method, called Calibration [30].", "startOffset": 99, "endOffset": 103}, {"referenceID": 29, "context": "We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and a recently proposed method, called Calibration [30].", "startOffset": 156, "endOffset": 160}, {"referenceID": 21, "context": "Note that we do not compare with SeqMT [22] because it is hard to have a fair comparison since SeqMT is tailored to solve classification tasks, while our current model focuses on regression problems.", "startOffset": 39, "endOffset": 43}, {"referenceID": 30, "context": "We evaluate all the algorithms in terms of both root mean squared error (rMSE) and normalized mean squared error (nMSE), which are commonly used in multi-task learning problem [31], [32].", "startOffset": 176, "endOffset": 180}, {"referenceID": 31, "context": "We evaluate all the algorithms in terms of both root mean squared error (rMSE) and normalized mean squared error (nMSE), which are commonly used in multi-task learning problem [31], [32].", "startOffset": 182, "endOffset": 186}, {"referenceID": 28, "context": "Each English letter corresponds to a label (1-26) and we treat the labels as regression values as [29].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "Since w j \u2208 [0, 1], we know that in the optimal solution, either \u2202Li(w (i) j ) \u2202w (i) j = 0 or w j = 0, 1.", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "It is clear that w (i) j \u221a\u2211 s(w (i) s )2 \u2208 [0, 1].", "startOffset": 43, "endOffset": 49}], "year": 2017, "abstractText": "In this paper, we propose a novel multi-task learning (MTL) framework, called Self-Paced Multi-Task Learning (SPMTL). Different from previous works treating all tasks and instances equally when training, SPMTL attempts to jointly learn the tasks by taking into consideration the complexities of both tasks and instances. This is inspired by the cognitive process of human brain that often learns from the easy to the hard. We construct a compact SPMTL formulation by proposing a new task-oriented regularizer that can jointly prioritize the tasks and the instances. Thus it can be interpreted as a selfpaced learner for MTL. A simple yet effective algorithm is designed for optimizing the proposed objective function. An error bound for a simplified formulation is also analyzed theoretically. Experimental results on toy and real-world datasets demonstrate the effectiveness of the proposed approach, compared to the stateof-the-art methods.", "creator": "LaTeX with hyperref package"}}}