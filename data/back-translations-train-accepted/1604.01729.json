{"id": "1604.01729", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text", "abstract": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while maintaining or modestly improving descriptive quality. Further, we show that such techniques can be beneficial for describing unseen object classes with no paired training data (zeroshot captioning).", "histories": [["v1", "Wed, 6 Apr 2016 19:01:28 GMT  (708kb,D)", "https://arxiv.org/abs/1604.01729v1", null], ["v2", "Tue, 29 Nov 2016 20:37:42 GMT  (1584kb,D)", "http://arxiv.org/abs/1604.01729v2", "Accepted at EMNLP 2016. Project page:this http URL"]], "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["subhashini venugopalan", "lisa anne hendricks", "raymond j mooney", "kate saenko"], "accepted": true, "id": "1604.01729"}, "pdf": {"name": "1604.01729.pdf", "metadata": {"source": "CRF", "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text", "authors": ["Subhashini Venugopalan", "UT Austin", "Lisa Anne Hendricks", "Raymond Mooney", "Kate Saenko"], "emails": ["vsub@cs.utexas.edu", "anne@berkeley.edu", "mooney@cs.utexas.edu", "saenko@bu.edu"], "sections": [{"heading": "1 Introduction", "text": "The ability to automatically describe videos in natural language (NL) enables many important applications, including content-based video retrieval and video description for the visually impaired. The most effective newer methods (Venugopalan et al., 2015a; Yao et al., 2015) use recurring neural networks (RNN) and treat the problem as machine translation (MT) from video to natural language. Deep learning methods such as RNNNs require large training corpus, but lack high-quality paired video sentence data. In contrast, raw text corpories are widely available and feature rich linguistic structures that can support video descriptions. Most work in statistical MT uses both a language model trained on a large corpus of monolingual target language data and a translation model trained on more limited bilingual data. This paper examines methods to incorporate knowledge of language corpories to support general linguistic laws."}, {"heading": "2 LSTM-based Video Description", "text": "We use the successful S2VT Video Description Framework from Venugopalan et al. (2015a) as ourar Xiv: 160 4.01 729v 2 [cs.C L] 29 Nov 2underlying model and briefly describe it here. S2VT uses a sequence to sequence the approach (Sutskever et al., 2014; Cho et al., 2014), which maps an input ~ x = (x1,..., xT) onto a fixed-dimensional vector and then decodes it into a sequence of output words ~ y = (y1,..., yN). As shown in Fig. 1, it uses a stack of two LSTM layers. Input ~ x on the first LSTM layer is a sequence of image features obtained from the penultimate layer (fc7) of a convolutionary neural network (CNN)."}, {"heading": "3 Approach", "text": "Existing visual labeling models (Vinyals et al., 2015; Donahue et al., 2015) are built exclusively from texts from the caption datasets and tend to reveal some linguistic irregularities associated with a restricted language model and a small vocabulary. However, our first approach (early fusion) is to integrate prior linguistic knowledge into a CNN / LSTM-based network for video and evaluate its effectiveness in improving overall description. Our first approach is to transfer parts of the network modeling language to large corporations of raw text before moving on and then to \"fine-tune\" the parameters to the paired video corpus. An LSTM model learns to estimate the likelihood of an output sequence. To learn a language model, we train the LSTM layer to predict the next word in light of the previous S2T architecture."}, {"heading": "4 Experiments", "text": "After the evaluation in Venugopalan et al. (2015a), we compare our models on the Youtube dataset (Chen and Dolan, 2011), as well as two large film description corpora: MPII-MD (Rohrbach et al., 2015) and M-VAD (Torabi et al., 2015). Evaluation metrics. We evaluate performance using machine translation (MT) metrics METEOR (Denkowski and Lavie, 2014) and BLEU (Papineni et al., 2002) to compare machine-generated descriptions with human ones. For film corpora that have only one description, we use only METEOR (Denkowski and Lavie, 2014) and BLEU (Papineni et al., 2002) to compare machine-generated descriptions with human ones."}, {"heading": "4.1 Youtube Video Dataset Results", "text": "Comparing the proposed techniques in Table 1 shows that Deep Fusion performs well on both METEOR and BLEU; the embedding of gloves considerably increases METEOR, and the combination of the two achieves the best. Our final model is an interaction (weighted average) of the glove, and the two Glove + Deep Fusion models trained on external and domain-internal COCO (Lin et al., 2014) sets. We note here that the state of the art on this dataset is achieved by HRNE (Pan et al., 2015) (METEOR 33.1), which proposes a superior imaging pipeline that devotes attention to encoding the video. Human ratings also correlate well with the METEOR values, confirming that our methods provide a modest improvement in descriptive quality. However, the inclusion of linguistic knowledge improves the grammaticality of the results, resulting from the validation of the results. <"}, {"heading": "4.2 Movie Description Results", "text": "Both MPII-MD and M-VAD have only one basic truth description for each video, making both learning and evaluation very difficult (e.g. Fig.3). METEOR values are relatively low in both sets of data, as generated sentences are compared to a single reference translation. S2VT is a new implementation of the basic S2VT model with the new vocabulary and architecture (embedding dimension). We observe that the ability of external language skills to improve METEOR values in these challenging sets of data is small but consistent. Again, human evaluations show a significant improvement in grammatical quality (with p < 0.0001)."}, {"heading": "5 Related Work", "text": "Following the success of LSTM-based machine translation models (Sutskever et al., 2014; Bahdanau et al., 2015) and image captions (Vinyals et al., 2015; Donahue et al., 2015), current video description works (Venugopalan et al., 2015b; Venugopalan et al., 2015a; Yao et al., 2015) suggest CNN RNN-based models that generate a vector representation for the video and \"decode\" it with an LSTM sequence model to generate a description. Venugopalan et al al al al. (2015b) also include external data such as images with captions to improve video description, but in this work our focus is on integrating external linguistic knowledge for video capture. We specifically investigate the use of distributed semantic embedding and LSTM-based language models that are trained on external text portions."}, {"heading": "6 Conclusion", "text": "This paper examines various techniques for integrating linguistic knowledge from text corpora to video subtitling. We empirically evaluate our approaches in YouTube clips as well as in two movie description corpora. Our results show significant improvements in human assessment of grammar with a modest improvement in the overall description quality of sentences in all datasets. While the proposed techniques are evaluated in a specific network of video captions, they are general and can be applied to other video and picture captioning models (Hendricks etal., 2016; Venugopalan et al., 2016). The code and models are shared at http: / / vsubhashini. github.io / language _ fusion.html."}, {"heading": "Acknowledgements", "text": "This work was supported by NSF awards IIS1427425 and IIS-1212798 as well as ONR ATL Grant N00014-11-1-010 and DARPA under AFRL Grant FA8750-13-2-0026. Raymond Mooney and Kate Saenko also confirm support through a Google Fellowship. Lisa Anne Hendricks is supported by the National Defense Science and Engineering Graduate (NDSEG) Fellowship."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate. ICLR", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Delving deeper into convolutional networks for learning video representations. ICLR", "author": ["Li Yao", "Chris Pal", "Aaron C. Courville"], "venue": null, "citeRegEx": "Ballas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ballas et al\\.", "year": 2016}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["Chen", "Dolan2011] David Chen", "William Dolan"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "On the properties of neural machine translation: Encoder\u2013 decoder approaches. Syntax, Semantics and Structure in Statistical Translation, page 103", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Meteor universal: Language specific translation evaluation for any target", "author": ["Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie"], "venue": null, "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Donahue et al.2015] Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": null, "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Gulcehre et al.2015] C. Gulcehre", "O. Firat", "K. Xu", "K. Cho", "L. Barrault", "H.C. Lin", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1503.03535", "citeRegEx": "Gulcehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Neural network ensembles", "author": ["Hansen", "Salamon1990] L.K. Hansen", "P. Salamon"], "venue": "IEEE TPAMI,", "citeRegEx": "Hansen et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 1990}, {"title": "Deep compositional captioning: Describing novel object categories without paired training data", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko", "Trevor Darrell"], "venue": null, "citeRegEx": "Hendricks et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hendricks et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pan et al.2015] Pingbo Pan", "Zhongwen Xu", "Yi Yang", "Fei Wu", "Yueting Zhuang"], "venue": null, "citeRegEx": "Pan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A dataset for movie description", "author": ["Marcus Rohrbach", "Niket Tandon", "Bernt Schiele"], "venue": null, "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "Lstm neural networks for language modeling", "author": ["R. Schluter", "H. Ney"], "venue": "In INTERSPEECH", "citeRegEx": "Sundermeyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Using descriptive video services to create a large data source for video annotation research", "author": ["Torabi et al.2015] Atousa Torabi", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": null, "citeRegEx": "Torabi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Torabi et al\\.", "year": 2015}, {"title": "Sequence to sequence - video", "author": ["M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": null, "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko"], "venue": null, "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Captioning images with diverse objects. arXiv preprint arXiv:1606.07770", "author": ["L.A. Hendricks", "M. Rohrbach", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": null, "citeRegEx": "Venugopalan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["Yu et al.2015] Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Further, LSTMs are also effective as language models (LMs) (Sundermeyer et al., 2010).", "startOffset": 59, "endOffset": 85}, {"referenceID": 6, "context": "Our next two approaches, inspired by recent MT work (Gulcehre et al., 2015), integrate an LSTM LM with the existing video-to-text model.", "startOffset": 52, "endOffset": 75}, {"referenceID": 18, "context": "We use the successful S2VT video description framework from Venugopalan et al. (2015a) as our ar X iv :1 60 4.", "startOffset": 60, "endOffset": 87}, {"referenceID": 16, "context": "S2VT uses a sequence to sequence approach (Sutskever et al., 2014; Cho et al., 2014) that maps an input ~x = (x1, .", "startOffset": 42, "endOffset": 84}, {"referenceID": 3, "context": "S2VT uses a sequence to sequence approach (Sutskever et al., 2014; Cho et al., 2014) that maps an input ~x = (x1, .", "startOffset": 42, "endOffset": 84}, {"referenceID": 21, "context": "Existing visual captioning models (Vinyals et al., 2015; Donahue et al., 2015) are trained solely on text from the caption datasets and tend to exhibit some linguistic irregularities associated with a restricted language model and a small vocabulary.", "startOffset": 34, "endOffset": 78}, {"referenceID": 5, "context": "Existing visual captioning models (Vinyals et al., 2015; Donahue et al., 2015) are trained solely on text from the caption datasets and tend to exhibit some linguistic irregularities associated with a restricted language model and a small vocabulary.", "startOffset": 34, "endOffset": 78}, {"referenceID": 6, "context": "This is similar to the technique proposed by Gulcehre et al. (2015) for incorporating language models trained on monolingual corpora for machine translation.", "startOffset": 45, "endOffset": 68}, {"referenceID": 10, "context": "There are many approaches (Mikolov et al., 2013; Pennington et al., 2014) that use large text corpora to learn vector-space representations of words that capture fine-grained semantic and syntactic regularities.", "startOffset": 26, "endOffset": 73}, {"referenceID": 13, "context": "There are many approaches (Mikolov et al., 2013; Pennington et al., 2014) that use large text corpora to learn vector-space representations of words that capture fine-grained semantic and syntactic regularities.", "startOffset": 26, "endOffset": 73}, {"referenceID": 13, "context": "Specifically, we replace the embedding matrix from one-hot vectors and instead use 300-dimensional GloVe vectors (Pennington et al., 2014) pre-trained on 6B tokens from Gigaword and Wikipedia 2014.", "startOffset": 113, "endOffset": 138}, {"referenceID": 14, "context": "(2015a), we compare our models on the Youtube dataset (Chen and Dolan, 2011), as well as two large movie description corpora: MPII-MD (Rohrbach et al., 2015) and M-VAD (Torabi et al.", "startOffset": 134, "endOffset": 157}, {"referenceID": 17, "context": ", 2015) and M-VAD (Torabi et al., 2015).", "startOffset": 18, "endOffset": 39}, {"referenceID": 16, "context": "Following the evaluation in Venugopalan et al. (2015a), we compare our models on the Youtube dataset (Chen and Dolan, 2011), as well as two large movie description corpora: MPII-MD (Rohrbach et al.", "startOffset": 28, "endOffset": 55}, {"referenceID": 12, "context": "We evaluate performance using machine translation (MT) metrics METEOR (Denkowski and Lavie, 2014) and BLEU (Papineni et al., 2002) to compare the machinegenerated descriptions to human ones.", "startOffset": 107, "endOffset": 130}, {"referenceID": 11, "context": "We note here that the state-of-the-art on this dataset is achieved by HRNE (Pan et al., 2015) (METEOR 33.", "startOffset": 75, "endOffset": 93}, {"referenceID": 16, "context": "Following the success of LSTM-based models on Machine Translation (Sutskever et al., 2014; Bahdanau et al., 2015), and image captioning (Vinyals et al.", "startOffset": 66, "endOffset": 113}, {"referenceID": 0, "context": "Following the success of LSTM-based models on Machine Translation (Sutskever et al., 2014; Bahdanau et al., 2015), and image captioning (Vinyals et al.", "startOffset": 66, "endOffset": 113}, {"referenceID": 21, "context": ", 2015), and image captioning (Vinyals et al., 2015; Donahue et al., 2015), recent video description works (Venugopalan et al.", "startOffset": 30, "endOffset": 74}, {"referenceID": 5, "context": ", 2015), and image captioning (Vinyals et al., 2015; Donahue et al., 2015), recent video description works (Venugopalan et al.", "startOffset": 30, "endOffset": 74}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015), and image captioning (Vinyals et al., 2015; Donahue et al., 2015), recent video description works (Venugopalan et al., 2015b; Venugopalan et al., 2015a; Yao et al., 2015) propose CNN-RNN based models that generate a vector representation for the video and \u201cdecode\u201d it using an LSTM sequence model to generate a description. Venugopalan et al. (2015b) also incorporate external data such as images with captions to improve video description, however in this work, our focus", "startOffset": 8, "endOffset": 383}, {"referenceID": 15, "context": "LSTMs have proven to be very effective language models (Sundermeyer et al., 2010).", "startOffset": 55, "endOffset": 81}, {"referenceID": 6, "context": "Gulcehre et al. (2015) developed an LSTM model for machine translation that incorporates a monolingual language model for the target language showing improved results.", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "Gulcehre et al. (2015) developed an LSTM model for machine translation that incorporates a monolingual language model for the target language showing improved results. We utilize similar approaches (late fusion, deep fusion) to train an LSTM for translating video to text that exploits large monolingual-English corpora (Wikipedia, BNC, UkWac) to improve RNN based video description networks. However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of our approach is that the output of the monolingual language model is used (as an input) when training the full underlying video description network.", "startOffset": 0, "endOffset": 432}, {"referenceID": 20, "context": "Contemporaneous to us, Yu et al. (2015), Pan et al.", "startOffset": 23, "endOffset": 40}, {"referenceID": 10, "context": "(2015), Pan et al. (2015) and Ballas et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 1, "context": "(2015) and Ballas et al. (2016) propose video description models focusing primarily on improving the video representation itself using a hierarchical visual pipeline, and attention.", "startOffset": 11, "endOffset": 32}], "year": 2016, "abstractText": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.", "creator": "LaTeX with hyperref package"}}}