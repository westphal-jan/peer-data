{"id": "1604.01870", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2016", "title": "Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis", "abstract": "We study the stochastic optimization of canonical correlation analysis (CCA), whose objective is nonconvex and does not decouple over training samples. Although several stochastic optimization algorithms have been recently proposed to solve this problem, no global convergence guarantee was provided by any of them. Based on the alternating least squares formulation of CCA, we propose a globally convergent stochastic algorithm, which solves the resulting least squares problems approximately to sufficient accuracy with state-of-the-art stochastic gradient methods for convex optimization. We provide the overall time complexity of our algorithm which significantly improves upon that of previous work. Experimental results demonstrate the superior performance of our algorithm.", "histories": [["v1", "Thu, 7 Apr 2016 04:14:54 GMT  (478kb)", "http://arxiv.org/abs/1604.01870v1", null], ["v2", "Wed, 20 Apr 2016 17:58:52 GMT  (461kb)", "http://arxiv.org/abs/1604.01870v2", null], ["v3", "Fri, 20 May 2016 03:09:29 GMT  (198kb)", "http://arxiv.org/abs/1604.01870v3", null], ["v4", "Mon, 14 Nov 2016 18:11:15 GMT  (648kb)", "http://arxiv.org/abs/1604.01870v4", "Accepted by NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weiran wang", "jialei wang", "dan garber", "nati srebro"], "accepted": true, "id": "1604.01870"}, "pdf": {"name": "1604.01870.pdf", "metadata": {"source": "CRF", "title": "Globally Convergent Stochastic Optimization for Canonical Correlation Analysis", "authors": ["Weiran Wang", "Jialei Wang"], "emails": ["weiranwang@ttic.edu", "jialei@uchicago.edu", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 4.01 870v 1 [cs.L G] 7A pr"}, {"heading": "1 Introduction", "text": "Correlation analysis (CCA, [1]) and its extensions are ubiquitous techniques in scientific research fields to uncover the most common sources of variability in multiple views of the same phenomenon. (xN, yN) where N is the training variable, xi, Rdx and yi, Rdy for i = 1,., N. We also designate the data matrices for each view1 of X = [x1,., xN], Rdx and Y = [y1,., yN], the goal of (regulated) linear CCA is to find linear projections of each view in such a way that the correlation between the projections is maximized: max u, vu, xyv, xyv, xyv, xyv."}, {"heading": "2 Alternating least squares", "text": "Our solution to (1) is inspired by the alternating smallest square formula of CCA [7, Algorithm 5.2], as shown in Algorithm 1. A similar algorithm is recently used by [8] for large linear CCA with high dimensional, sparse inputs, and by [4, 5] for the motivating power of stochastic CCA algorithms. Let the non-zero singular values of T be 1, 2, 2, 2, 2, 2, 3, 4, 5, and the corresponding (unit length) left and right singular vector pairs (a1, b1). (ar, br), with a1, and b = Rank (T). Now define C = [0 T], the 0, R (dx + dy), (dx + dy). It is easy to verify that the non-zero eigenvalues of C are."}, {"heading": "3 Our algorithm", "text": "It is not the first time that we see ourselves able to realize the results of our algorithms. (...) It is not the first time that we apply the results of algorithms. (...) It is not the first time that we apply the results of algorithms. (...) It is the second time that we apply the results of algorithms. (...) It is the second time that we apply the results of algorithms. (...) It is the second time that we apply the results of algorithms. (...) It is the first time that we apply the results of algorithms. (...) It is the second time that we apply the results of algorithms. (...) It is the second time that we apply the results of algorithms. (...) It is the first time that we apply the results of algorithms. (...) It is the second time that we apply the results of algorithms. (...) It is the third time that we apply the results of algorithms. (...) It is the second time that we apply the results of algorithms. (...) It is the third time that we apply the results of algorithms. (...) It is the second time that we apply the results of algorithms. (...) It is the third time that we apply the results of algorithms."}, {"heading": "3.1 Analysis of inexact power iterations", "text": "The key to the representation of the convergence of algorithm 2 is the convergence of inexact potential iterations, in which the smallest problems are solved only to necessary accuracy. (From now on we distinguish the iterations of our unnormalized iterates and the iterations of the exact potential iterations (algorithm 1) and designate the latter by asterisks, i.e. we distinguish the iterations of our unnormalized iterates and V iterates. (1) We denote the exact optimum of ft (u) and gt (v) of u iterates and V iterates. (2) The following iterations of the inexact and exact potential iterations. (Lemma 3.1) Assume that algorithm 1 and algorithm 2 begin with the same initialization, i.e., u-iterations 0 and V-iterations 0 = exact potential iterations."}, {"heading": "3.2 Stochastic optimization of regularized least squares", "text": "We will now discuss the inner loop of our algorithm, which roughly solves the partial problems of form (4). As already mentioned, the partial problems have the finite sum structure, for which several stochastic optimization methods, such as SAG / SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates. All these algorithms can easily be applied to the problem of regulated smallest squares (4), and we choose SVRG because it is memory efficient and easy to implement. We will give the sketch of SVRG for (4) in algorithm 3. Note that f (u) = application N = 1 f \u2212 i (u), where each component is f \u2212 i (u) = 12% u yi 2 + acceleration x2%, in algorithm 3%, and f (u) = application N \u2212 Grade i (u), where each component \u2212 u = 12% u \u2212 yvation, in essence of the algorithm."}, {"heading": "3.3 Extension to multi-dimensional projections", "text": "In practice, we may be interested in projections of more than one dimension. CCA's goal for extracting L-dimensional projections is Max U, Rdx \u00b7 L, V, Rdy \u00b7 Ltr (U, xxyV) s.t. U, xxxU = V, xxyV = I. To adapt algorithm 2 to this problem, one option is to extract the dimensions sequentially and the declared correlation each time we extract a new dimension [19]. A simpler approach is to extract the L dimensions simultaneously using (approximate) orthogonal iterations (an extension of the performance siterations to multiple dimensions, [9]). In this case, our sub-problems become multi-dimensional regressions, and our normalization steps are of the form Ut, Ut, U, T (U, t, xxU, T) \u2212 1 2; the same normalization is used by [4, 5]."}, {"heading": "4 Related work", "text": "In recent years, continuous efforts have been made to expand basic methods such as principal component analysis (PCA) and partial minimum squares with stochastic / online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30]. However, as pointed out by [23], the CCA target is more difficult due to whitening constraints. Our algorithm is inspired by the stochastic PCA algorithm of [28], which transforms the non-convex PCA target into a small number of well-conditioned, regulated minimum square problems (solved by SVRG) by shifting and reversing the covariance matrix and ongoing power siterations on the transformed matrix."}, {"heading": "5 Experiments", "text": "In this section, we demonstrate our algorithm designated by ALS-VR (Alternating Least Squares with Variance Reduction) on three real-world datasets: Mediamill [32], JW11 (a subset of the XRMB corpus [33]), and MNIST [34]. These datasets were also used by [4, 5] to demonstrate their stochastic CCA algorithms. We give the description and size of these datasets in Table 1. We extract L = 5-dimensional predictions, and for the approximate solution obtained after each pass over the data, we evaluate the canonical correlation between the predictions \u2212 degree \u2212 and compare it with the exact solution by SVD. We compare ALS-VR with the batch AppGrad, and its stochastic version s-AppGrad [4] with both gradients and standardization steps obtained after each pass over the data, we evaluate the canonical correlation between the predictions \u2212 degree \u2212 and compare it with the exact solution by SVD. We compare ALS-VR with the batch AppGrad, and its stochastic version s-AppGrad [4] with both SGB-AppGrad [4] SGB-AppGrad [4] with both gradients and standardization steps achieved with minibatches of 200 SGB-RGB-RGB-Grad \u2212 degree \u2212 and compare them with the exact solution by SVD. We compare ALS-VR with the SGB-VR with the batch AppGrad, and its stochastic version s-AppGrad [4-AppGrad] SGB-AppGrad [4] and its stochastic version s-AppGrad [4] SGB-AppGrad [4] SGB-AppGrad [4] SGB-AppGrad [4] with both SGB-SGB-SGB-SGB-SGB-SGB-SGB-SGB-SGB-SGB-SGB-SGB-SGB-SGB-SGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB-RGB"}, {"heading": "6 Conclusions", "text": "We have proposed a globally convergent stochastic algorithm for CCA, whose goal is non-convex and which does not allow decoupling of training samples. Our algorithm uses the alternately formulation of the smallest squares / potential iterations of CCA and solves the problems with the smallest squares using, for example, state-of-the-art stochastic gradient descendence methods. The total time complexity of our algorithm significantly improves both theoretically and empirically compared to previous work. A simple application of our algorithm is the approximate CCA kernel [35] using random Fourier characteristics [36, 37]. The algorithm of [35] first maps original inputs into a high-dimensional random feature space and then performs linear CCA on top of it. The resulting high-dimensional CCA problem can be solved with our stochastic algorithm, as is done by [6, 38]. A further future analysis consists in extending the unspecified dimensional potential for the overall algorithm."}, {"heading": "A Proof of Theorem 2.1", "text": "It is easy to see that until the end of the first iteration of algorithm 1 = > individual cases (1) and 1 (2) remain in these spaces (2). (2) Let us first consider these 2 (2). (6) Since these 2 (2) and 2 (2) do not exist, it is equivalent to using the following updates: TT 2 (2) and 2 (2). (7) This indicates that algorithm 1 (2) and 2 (2) do not exist. (7) This means that algorithm 1 (2) and 2 (2) do not exist. (7) This means that algorithm 1 does not exist. (7) This means that algorithm 1 (2) does not exist. (8) It is equivalent to using the following updates: TT 2 (2)."}, {"heading": "B Proof of Lemma 3.1", "text": "The proof that the error detection of the non-normalized iterate is completely analogous. (i) We prove the limit for non-normalized iterate recursive. (i) First, the case applies to t = 0 trivial. (i) We can set the error of the non-normalized iterate to ft (u) using the exact solution. (i) First, the case for t = 0 is trivial. (u) First, we can set the error of the non-normalized iterate to ft (u) using the exact solution. (u) First: 1 xxu - 2 x \u2212 2 x \u2212 2 x \u2212 2 x \u2212 1 x \u2212 2 \u2212 2 x \u2212 2 \u2212 1 x \u2212 2 \u2212 1 x \u2212 1 \u2212 1 \u2212 1 x \u2212 1 x \u2212 2 \u2212 1 x \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 u x \u2212 2 x \u2212 2 x \u2212 2 x \u2212 4 x x x x x x x x x x x x 4 \u2212 2 \u2212 2 \u2212 4 \u2212 x x x x x x x x x \u2212 2 \u2212 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 4 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 4 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 u \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 x \u2212 1 \u2212 1 \u2212 u \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 1 \u2212 2 \u2212 u \u2212 1 \u2212 4 \u2212 4 \u2212 2 \u2212 4 \u2212 4 \u2212 4 \u2212 4 \u2212 2 \u2212 2 \u2212 4 \u2212 4 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 u \u2212 2 \u2212 4 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 u \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212"}], "references": [{"title": "Relations between two sets of variates", "author": ["Harold Hotelling"], "venue": "Biometrika, 28(3/4):321\u2013377,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1936}, {"title": "Canonical ridge and econometrics of joint production", "author": ["H.D. Vinod"], "venue": "Journal of Econometrics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1976}, {"title": "On the regularization of canonical correlation analysis", "author": ["Tijl De Bie", "Bart De Moor"], "venue": "www.esat.kuleuven.ac.be/sista-cosic-docarch/,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Zhuang Ma", "Yichao Lu", "Dean Foster"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Stochastic optimization for deep CCA via nonlinear orthogonal iterations", "author": ["Weiran Wang", "Raman Arora", "Nati Srebro", "Karen Livescu"], "venue": "In 53nd Annual Allerton Conference on Communication, Control and Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Scale up nonlinear component analysis with doubly stochastic gradients", "author": ["Bo Xie", "Yingyu Liang", "Le Song"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Linear Algebra for Signal Processing, volume 69 of The IMA Volumes in Mathematics and its Applications, chapter The Canonical Correlations of Matrix Pairs and their Numerical Computation, pages 27\u201349", "author": ["Gene H. Golub", "Hongyuan Zha"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Large scale canonical correlation analysis with iterative least squares", "author": ["Yichao Lu", "Dean P. Foster"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Multi-view learning of word embeddings via CCA", "author": ["Paramveer Dhillon", "Dean Foster", "Lyle Ungar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Using CCA to improve CCA: A new spectral method for estimating vector models of words", "author": ["Paramveer Dhillon", "Jordan Rodu", "Dean Foster", "Lyle Ungar"], "venue": "In Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "Technical Report HAL 00860051,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Un-regularizing: Approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham Kakade", "Aaron Sidford"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "A universal catalyst for first-order optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Introductory Lectures on Convex Optimization", "author": ["Y. Nesterov"], "venue": "A Basic Course. Number 87 in Applied Optimization. Springer-Verlag,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["Daniela M. Witten", "Robert Tibshirani", "Trevor Hastie"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Krasulina. A method of stochastic approximation for the determination of the least eigenvalue of a symmetric matrix", "author": ["P. T"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1969}, {"title": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix", "author": ["Erkki Oja", "Juha Karhunen"], "venue": "J. Math. Anal. Appl.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1985}, {"title": "Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension", "author": ["Manfred K. Warmuth", "Dima Kuzmin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Stochastic optimization for PCA and PLS", "author": ["Raman Arora", "Andy Cotter", "Karen Livescu", "Nati Srebro"], "venue": "In 50th Annual Allerton Conference on Communication,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Stochastic optimization of PCA with capped MSG", "author": ["Raman Arora", "Andy Cotter", "Nati Srebro"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Memory limited, streaming PCA", "author": ["Ioannis Mitliagkas", "Constantine Caramanis", "Prateek Jain"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "The fast convergence of incremental PCA", "author": ["Akshay Balsubramani", "Sanjoy Dasgupta", "Yoav Freund"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["Ohad Shamir"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Fast and simple pca via convex optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": "[math.OC],", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["Chi Jin", "Sham M. Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford"], "venue": "[cs.DS],", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Principal component projection without principal component analysis", "author": ["Roy Frostig", "Cameron Musco", "Christopher Musco", "Aaron Sidford"], "venue": "[cs.DS],", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Adaptive canonical correlation analysis based on matrix manifolds", "author": ["Florian Yger", "Maxime Berar", "Gilles Gasso", "Alain Rakotomamonjy"], "venue": "In Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "The challenge problem for automated detection of 101 semantic concepts in multimedia", "author": ["Cees G.M. Snoek", "Marcel Worring", "Jan C. van Gemert", "Jan-Mark Geusebroek", "Arnold W.M. Smeulders"], "venue": "In Proceedings of the 14th ACM international conference on Multimedia,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "X-Ray Microbeam Speech Production", "author": ["John R. Westbury"], "venue": "Database User\u2019s Handbook Version", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1998}, {"title": "Randomized nonlinear component analysis", "author": ["David Lopez-Paz", "Suvrit Sra", "Alex Smola", "Zoubin Ghahramani", "Bernhard Schoelkopf"], "venue": "In Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Ben Recht"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Large-scale approximate kernel canonical correlation analysis", "author": ["Weiran Wang", "Karen Livescu"], "venue": "In Proc. of the 4nd Int. Conf. Learning Representations (ICLR 2016),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Canonical correlation analysis (CCA, [1]) and its extensions are ubiquitous techniques in scientific research areas for revealing the common sources of variability in multiple views of the same phenomenon.", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "u\u03a3xxu = v\u03a3yyv = 1 where \u03a3xy = 1 NXY \u22a4 is the cross-covariance matrix, \u03a3xx = 1 NXX +\u03b3xI and \u03a3yy = 1 NYY + \u03b3yI are the auto-covariance matrices, and (\u03b3x, \u03b3y) \u2265 0 are regularization parameters [2, 3].", "startOffset": 190, "endOffset": 196}, {"referenceID": 2, "context": "u\u03a3xxu = v\u03a3yyv = 1 where \u03a3xy = 1 NXY \u22a4 is the cross-covariance matrix, \u03a3xx = 1 NXX +\u03b3xI and \u03a3yy = 1 NYY + \u03b3yI are the auto-covariance matrices, and (\u03b3x, \u03b3y) \u2265 0 are regularization parameters [2, 3].", "startOffset": 190, "endOffset": 196}, {"referenceID": 3, "context": "There have been recent attempts to solve (1) based on stochastic gradient descent (SGD) methods [4, 5, 6], but none of these work provides rigorous convergence analysis for their stochastic CCA algorithms.", "startOffset": 96, "endOffset": 105}, {"referenceID": 4, "context": "There have been recent attempts to solve (1) based on stochastic gradient descent (SGD) methods [4, 5, 6], but none of these work provides rigorous convergence analysis for their stochastic CCA algorithms.", "startOffset": 96, "endOffset": 105}, {"referenceID": 5, "context": "There have been recent attempts to solve (1) based on stochastic gradient descent (SGD) methods [4, 5, 6], but none of these work provides rigorous convergence analysis for their stochastic CCA algorithms.", "startOffset": 96, "endOffset": 105}, {"referenceID": 7, "context": "A similar algorithm is recently used by [8] for large scale linear CCA with high dimensional sparse inputs, and by [4, 5] for motivating stochastic CCA algorithms.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "A similar algorithm is recently used by [8] for large scale linear CCA with high dimensional sparse inputs, and by [4, 5] for motivating stochastic CCA algorithms.", "startOffset": 115, "endOffset": 121}, {"referenceID": 4, "context": "A similar algorithm is recently used by [8] for large scale linear CCA with high dimensional sparse inputs, and by [4, 5] for motivating stochastic CCA algorithms.", "startOffset": 115, "endOffset": 121}, {"referenceID": 8, "context": ", stochastic variance reduced gradient (SVRG, [10]) to obtain approximate matrix/vector product for power iterations.", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "This observation motivated the stochastic CCA algorithms of [4, 5].", "startOffset": 60, "endOffset": 66}, {"referenceID": 4, "context": "This observation motivated the stochastic CCA algorithms of [4, 5].", "startOffset": 60, "endOffset": 66}, {"referenceID": 3, "context": "In contrast, the stochastic algorithms of [4, 5] use (possibly adaptive) estimate of covariance from a minibatch for normalization, in order to reduce the cost of (frequent) normalization after each stochastic gradient descent step, which further introduces noise to the updates.", "startOffset": 42, "endOffset": 48}, {"referenceID": 4, "context": "In contrast, the stochastic algorithms of [4, 5] use (possibly adaptive) estimate of covariance from a minibatch for normalization, in order to reduce the cost of (frequent) normalization after each stochastic gradient descent step, which further introduces noise to the updates.", "startOffset": 42, "endOffset": 48}, {"referenceID": 9, "context": ", such as those used in natural language processing [11, 12]), an advantage of gradient based methods over the closedform solution is that it can take into account the input sparsity.", "startOffset": 52, "endOffset": 60}, {"referenceID": 10, "context": ", such as those used in natural language processing [11, 12]), an advantage of gradient based methods over the closedform solution is that it can take into account the input sparsity.", "startOffset": 52, "endOffset": 60}, {"referenceID": 11, "context": "As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates.", "startOffset": 136, "endOffset": 144}, {"referenceID": 12, "context": "As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates.", "startOffset": 136, "endOffset": 144}, {"referenceID": 13, "context": "As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates.", "startOffset": 151, "endOffset": 155}, {"referenceID": 8, "context": "As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates.", "startOffset": 162, "endOffset": 166}, {"referenceID": 8, "context": "We quote the convergence rate of SVRG below from [10].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "Remarks We can also apply the recently developed accelerations techniques for first order optimization methods [16, 17].", "startOffset": 111, "endOffset": 119}, {"referenceID": 15, "context": "Remarks We can also apply the recently developed accelerations techniques for first order optimization methods [16, 17].", "startOffset": 111, "endOffset": 119}, {"referenceID": 16, "context": "Within our framework, we can use accelerated gradient descent instead and reduce the dependence on condition number to \u221a \u03ba [18].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "To adapt Algorithm 2 to this problem, one option is to extract the dimensions sequentially and remove the explained correlation from \u03a3xy each time we extract a new dimension [19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 3, "context": "In this case, our subproblems become multi-dimensional regressions and our normalization steps are of the form Ut \u2190 \u0168t(\u0168t \u03a3xx\u0168t) 1 2 ; the same normalization is used by [4, 5].", "startOffset": 169, "endOffset": 175}, {"referenceID": 4, "context": "In this case, our subproblems become multi-dimensional regressions and our normalization steps are of the form Ut \u2190 \u0168t(\u0168t \u03a3xx\u0168t) 1 2 ; the same normalization is used by [4, 5].", "startOffset": 169, "endOffset": 175}, {"referenceID": 18, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 19, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 20, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 21, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 22, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 23, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 24, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 25, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 5, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 26, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 27, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 28, "context": "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].", "startOffset": 194, "endOffset": 241}, {"referenceID": 21, "context": "But as pointed out by [23], the CCA objective is more challenging due to the whitening constraints.", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "Our algorithm is inspired by the stochastic PCA algorithm of [28] which transforms the nonconvex PCA objective into a small number of well-conditioned regularized least squares problems (solved by SVRG) through shifting and inverting the covariance matrix and running power iterations on the transformed matrix.", "startOffset": 61, "endOffset": 65}, {"referenceID": 29, "context": "[31] propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Similar to our algorithm, the stochastic CCA algorithms of [4, 5] are motivated by the alternating least squares formulation.", "startOffset": 59, "endOffset": 65}, {"referenceID": 4, "context": "Similar to our algorithm, the stochastic CCA algorithms of [4, 5] are motivated by the alternating least squares formulation.", "startOffset": 59, "endOffset": 65}, {"referenceID": 5, "context": "[6] proposed a stochastic algorithm based on the Lagrangian formulation of the CCA objective.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "5 Experiments In this section we demonstrate our algorithm, denoted by ALS-VR (Alternating Least Squares with Variance Reduction), on three real-world datasets: Mediamill [32], JW11 (a subset of the XRMB corpus [33]), and MNIST [34].", "startOffset": 171, "endOffset": 175}, {"referenceID": 31, "context": "5 Experiments In this section we demonstrate our algorithm, denoted by ALS-VR (Alternating Least Squares with Variance Reduction), on three real-world datasets: Mediamill [32], JW11 (a subset of the XRMB corpus [33]), and MNIST [34].", "startOffset": 211, "endOffset": 215}, {"referenceID": 32, "context": "5 Experiments In this section we demonstrate our algorithm, denoted by ALS-VR (Alternating Least Squares with Variance Reduction), on three real-world datasets: Mediamill [32], JW11 (a subset of the XRMB corpus [33]), and MNIST [34].", "startOffset": 228, "endOffset": 232}, {"referenceID": 3, "context": "These datasets have also been used by [4, 5] for demonstrating their stochastic CCA algorithms.", "startOffset": 38, "endOffset": 44}, {"referenceID": 4, "context": "These datasets have also been used by [4, 5] for demonstrating their stochastic CCA algorithms.", "startOffset": 38, "endOffset": 44}, {"referenceID": 3, "context": "We compare ALS-VR with batch AppGrad, and its stochastic version s-AppGrad [4] with both gradient and normalization steps estimated with minibatchs of 200 samples.", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "4 We do not compare the algorithm of [5] because the main difference between it and AppGrad is that it uses adaptive The authors of [4] suggest that the minibatch size shall be at least the same magnitude as L.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "4 We do not compare the algorithm of [5] because the main difference between it and AppGrad is that it uses adaptive The authors of [4] suggest that the minibatch size shall be at least the same magnitude as L.", "startOffset": 132, "endOffset": 135}], "year": 2017, "abstractText": "We study the stochastic optimization of canonical correlation analysis (CCA), whose objective is nonconvex and does not decouple over training samples. Although several stochastic optimization algorithms have been recently proposed to solve this problem, no global convergence guarantee was provided by any of them. Based on the alternating least squares formulation of CCA, we propose a globally convergent stochastic algorithm, which solves the resulting least squares problems approximately to sufficient accuracy with state-of-the-art stochastic gradient methods for convex optimization. We provide the overall time complexity of our algorithm which significantly improves upon that of previous work. Experimental results demonstrate the superior performance of our algorithm.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}