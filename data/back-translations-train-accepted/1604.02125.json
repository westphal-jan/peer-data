{"id": "1604.02125", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2016", "title": "Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes", "abstract": "We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images. The motivation for this work comes from the fact that some ambiguities in language simply cannot be resolved without simultaneously reasoning about an associated image. If we consider the sentence \"I shot an elephant in my pajamas\", looking at the language alone (and not reasoning about common sense), it is unclear if it is the person or the elephant that is wearing the pajamas or both. Our approach involves producing a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly re-ranked to select the most consistent pair. We show that our semantic segmentation and prepositional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation. We also show that multiple hypotheses are crucial to improved multiple-module reasoning. Our vision and language approach significantly outperforms a state-of-the-art NLP system (Stanford Parser [16,27]) by 17.91% (28.69% relative) in one experiment, and by 12.83% (25.28% relative) in another. We also make small improvements over a state-of-the-art vision system (DeepLab-CRF [13]).", "histories": [["v1", "Thu, 7 Apr 2016 19:26:56 GMT  (7574kb,D)", "https://arxiv.org/abs/1604.02125v1", "*The first two authors contributed equally"], ["v2", "Thu, 5 May 2016 19:05:27 GMT  (7575kb,D)", "http://arxiv.org/abs/1604.02125v2", "*The first two authors contributed equally"], ["v3", "Fri, 2 Sep 2016 01:47:35 GMT  (5865kb,D)", "http://arxiv.org/abs/1604.02125v3", "*The first two authors contributed equally"], ["v4", "Mon, 26 Sep 2016 04:08:19 GMT  (6799kb,D)", "http://arxiv.org/abs/1604.02125v4", "*The first two authors contributed equally. Conference on Empirical Methods in Natural Language Processing (EMNLP) 2016"]], "COMMENTS": "*The first two authors contributed equally", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["gordon christie", "ankit laddha", "aishwarya agrawal", "stanislaw antol", "yash goyal", "kevin kochersberger", "dhruv batra"], "accepted": true, "id": "1604.02125"}, "pdf": {"name": "1604.02125.pdf", "metadata": {"source": "CRF", "title": "Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes", "authors": ["Gordon Christie", "Ankit Laddha", "Aishwarya Agrawal", "Stanislaw Antol", "Yash Goyal", "Kevin Kochersberger", "Dhruv Batra"], "emails": ["ankit1991laddha@gmail.com", "gordonac@vt.edu", "aish@vt.edu", "santol@vt.edu", "ygoyal@vt.edu", "kbk@vt.edu", "dbatra@vt.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if most of them are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "2 Related Work", "text": "Most work at the interface of vision and NLP tends to be \"pipeline\" systems in which vision tasks take 1-best inputs from NLP (e.g., sentence parsings) without trying to improve the performance of NLP and vice versa. For example, Fidler et al. (2013) we use prepositions to improve object segmentation and scene classification, but only look at the most likely parses of the sentence and do not resolve ambiguities in the text. (2014) we examine the role of objects, attributes and action classifications to generate human descriptions. While they achieve impressive results in generating descriptions, they rely on perfect vision modules to generate sentences. Our work uses current (still imperfect) vision and NLP modules to think about images and provided descriptions, while improving both vision and language modules."}, {"heading": "3 Approach", "text": "To highlight the universality of our approach and to show that our approach is compatible with a broad class of implementations of semantic segmentation and PPAR modules, we present our approach with modules abstracted as \"black boxes\" that meet a few general requirements and minimal assumptions. In Section 4, we describe each of the modules in detail, specifying their respective characteristics and other details."}, {"heading": "3.1 What is a Module?", "text": "The aim of a module is to take input variables x-X (images or sets) and predict output variables y-Y (semantic segmentation) and z-Z (prepositional appendage, expressed in sentence parse).The two requirements for a module are that it must be able to generate scores S (y-x) for potential solutions and a list of plausible hypotheses Y = {y1, y2,..., yM}.Multiple hypotheses. To be useful, the set Y must provide an accurate summary of the score landscape. Therefore, the hypotheses should be plausible (i.e. high scoring) and mutually non-redundant (i.e. diverse).Our approach (described below) applies to any choice of different hypotheses generators. In our experiments, we use the k-best algorithm of Huang and Chiang (2005) for the sentence parsing module Divate-module-4-Segmentation."}, {"heading": "3.2 Joint Reasoning Across Multiple Modules", "text": "We show how to dovetail information from both segmentations and PPAR modules. Remember that our central focus is on consistency - correct hypotheses from different modules - both of which are correct in a consistent manner, but false hypotheses will be wrong in irreconcilable ways. Thus, our goal is to look for a pair (semantic segmentations, propositions) that is mutually consistent. We develop a \"mediator\" model that identifies high scoring hypotheses and Z = {z1,., zM} denote the M PPAR hypotheses as a factor model in which each node corresponds to a module (setic segmentation and PAR). We work with such a factor in accordance with each other. Specifically, we can express the MEDIATOR model as a factor graph in which each node corresponds to a module (setic and PAR)."}, {"heading": "4 Experiments", "text": "We describe the setup of our experiments, provide details on how to implement the modules, and describe the consistency of multiple sets on pages 4ASCAL.Datasets. Access to richly annotated image and image datasets is critical to performing quantitative evaluations. As this is the first paper to examine the problem of joint segmentation and PPAR, no standard datasets exist for this task, so we had to curate our own annotations for PPAR to three image datasets (Rashtchian et al., 2010), PASCAL-50S (Vedantam et al., 2014) (extends the UIUC PASCAL dataset al., 2010) from 5 image descriptions per image to 50), and PASCAL-Context50S (Mottaghi et al., 2014) (which uses the PASCAL image annotations and the sets as PASCAL."}, {"heading": "4.1 Single-Module Results", "text": "We performed a 10-fold cross-validation of the ABSTRACT-50S dataset to determine M (= 10) and the weight of the hinged loss for MEDIATOR (C.) The results are presented in Table 1. Our approach significantly outperforms the 1-best results of the Stanford parser (De Marneffe et al., 2006) by 20.66% (36.42% relative), demonstrating the need for multiple hypotheses and considerations of visual characteristics when selecting a sentence section. Oracle identifies the best achievable performance using these 10 hypotheses."}, {"heading": "4.2 Multiple-Module Results", "text": "In fact, most of us are able to surpass ourselves, both in terms of the way they move and in terms of the way they move and in terms of the way they move. (...) It's not as if they do it. (...) It's as if they do it. (...) It's as if they do it. (...) It's as if they do it. (...) It's not as if they do it, as if they do it, as if they do it, as if they do it, as if they do it, as if they do it, as if they do it, as if they do it, as if they do it, as if they do it, as if they do it, as if they do it. (...)"}, {"heading": "5 Discussions and Conclusion", "text": "We have presented an approach to simultaneous reasoning on prepositional attachments that includes solving image captions and semantic segmentation in images, integrating the beliefs in the modules to select the best pair from a variety of hypotheses. Our Full Model (MEDIATOR) significantly improves the accuracy of PPAR over the Stanford parser by 17.91% for PASCAL-50S and 12.83% for PASCAL-Context-50S, and achieves a small improvement in semantic segmentation over DeepLab-CRF for PASCAL-50S. These results demonstrate the need for information exchange between modules, as well as the need for a set of hypotheses to accurately capture the uncertainties of each module. Large gains in PPAR confirm our intuition that vision is very helpful in dealing with ambiguities in language."}, {"heading": "Appendix Overview", "text": "In fact, most people are able to decide for themselves what they want and what they don't want."}], "references": [{"title": "Optimizing Expected Intersection-over-Union with Candidate-Constrained CRFs", "author": ["Ahmed et al.2015] Faruk Ahmed", "Dany Tarlow", "Dhruv Batra"], "venue": null, "citeRegEx": "Ahmed et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ahmed et al\\.", "year": 2015}, {"title": "VQA: Visual Question Answering", "author": ["Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": null, "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "2016. Routledge encyclopedia of philosophy entry. http://online.sfsu.edu/ kbach/ambguity.html", "author": ["Kent Bach"], "venue": null, "citeRegEx": "Bach.,? \\Q2016\\E", "shortCiteRegEx": "Bach.", "year": 2016}, {"title": "Word Sense Disambiguation with Pictures", "author": ["Barnard", "Johnson2005] Kobus Barnard", "Matthew Johnson"], "venue": "Artificial Intelligence,", "citeRegEx": "Barnard et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Barnard et al\\.", "year": 2005}, {"title": "Diverse M-Best Solutions in Markov Random Fields", "author": ["Batra et al.2012] Dhruv Batra", "Payman Yadollahpour", "Abner Guzman-Rivera", "Gregory Shakhnarovich"], "venue": null, "citeRegEx": "Batra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Batra et al\\.", "year": 2012}, {"title": "An Efficient MessagePassing Algorithm for the M-Best MAP Problem", "author": ["Dhruv Batra"], "venue": "In UAI", "citeRegEx": "Batra.,? \\Q2012\\E", "shortCiteRegEx": "Batra.", "year": 2012}, {"title": "Do You See What I Mean? Visual Resolution of Linguistic Ambiguities", "author": ["Andrei Barbu", "Daniel Harari", "Boris Katz", "Shimon Ullman"], "venue": null, "citeRegEx": "Berzak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Berzak et al\\.", "year": 2015}, {"title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs", "author": ["George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L Yuille"], "venue": "In ICLR", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Generating Typed Dependency Parses from Phrase Structure Parses", "author": ["Bill MacCartney", "Christopher D Manning"], "venue": null, "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "The Pascal Visual Object Classes (VOC) Challenge", "author": ["L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "A Sentence is Worth a Thousand Pixels", "author": ["Fidler et al.2013] Sanja Fidler", "Abhishek Sharma", "Raquel Urtasun"], "venue": null, "citeRegEx": "Fidler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fidler et al\\.", "year": 2013}, {"title": "Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings", "author": ["Gella et al.2016] Spandana Gella", "Mirella Lapata", "Frank Keller"], "venue": "In NAACL HLT", "citeRegEx": "Gella et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gella et al\\.", "year": 2016}, {"title": "A Visual Turing Test for Computer Vision Systems", "author": ["Geman et al.2014] Donald Geman", "Stuart Geman", "Neil Hallonquist", "Laurent Younes"], "venue": "In PNAS", "citeRegEx": "Geman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Geman et al\\.", "year": 2014}, {"title": "A Systematic Exploration of Diversity in Machine Translation", "author": ["Gimpel et al.2013] K. Gimpel", "D. Batra", "C. Dyer", "G. Shakhnarovich"], "venue": null, "citeRegEx": "Gimpel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2013}, {"title": "DivMCuts: Faster Training of Structural SVMs with Diverse M-Best Cutting-Planes", "author": ["Pushmeet Kohli", "Dhruv Batra"], "venue": "In AISTATS", "citeRegEx": "Guzman.Rivera et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guzman.Rivera et al\\.", "year": 2013}, {"title": "Cascaded Classification Models: Combining Models for Holistic Scene Understanding", "author": ["Heitz et al.2008] Geremy Heitz", "Stephen Gould", "Ashutosh Saxena", "Daphne Koller"], "venue": null, "citeRegEx": "Heitz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Heitz et al\\.", "year": 2008}, {"title": "What are you talking about? Text-to-Image Coreference", "author": ["Kong et al.2014] Chen Kong", "Dahua Lin", "Mohit Bansal", "Raquel Urtasun", "Sanja Fidler"], "venue": null, "citeRegEx": "Kong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "Image Retrieval with Structured Object Queries Using Latent Ranking SVM", "author": ["Lan et al.2012] Tian Lan", "Weilong Yang", "Yang Wang", "Greg Mori"], "venue": null, "citeRegEx": "Lan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2012}, {"title": "A pooling approach to modelling spatial relations for image retrieval and annotation", "author": ["Malinowski", "Fritz2014] Mateusz Malinowski", "Mario Fritz"], "venue": "arXiv preprint arXiv:1411.5190", "citeRegEx": "Malinowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2014}, {"title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images", "author": ["Marcus Rohrbach", "Mario Fritz"], "venue": null, "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "Globally Optimal Solutions for Energy Minimization in Stereo Vision Using Reweighted Belief Propagation", "author": ["Chen Yanover", "Yair Weiss"], "venue": null, "citeRegEx": "Meltzer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Meltzer et al\\.", "year": 2005}, {"title": "Efficient Estimation of Word Representations in Vector Space. In ICLR", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Annotating (Anaphoric) ambiguity", "author": ["Poesio", "Artstein2005] Massimo Poesio", "Ron Artstein"], "venue": "In Corpus Linguistics Conference", "citeRegEx": "Poesio et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Poesio et al\\.", "year": 2005}, {"title": "Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets", "author": ["Prasad et al.2014] Adarsh Prasad", "Stefanie Jegelka", "Dhruv Batra"], "venue": null, "citeRegEx": "Prasad et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2014}, {"title": "Empirical Minimum Bayes Risk Prediction: How to extract an extra few% performance from vision models with just three more parameters", "author": ["Daniel Tarlow", "Dhruv Batra"], "venue": null, "citeRegEx": "Premachandran et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Premachandran et al\\.", "year": 2014}, {"title": "Collecting Image Annotations Using Amazon\u2019s Mechanical Turk", "author": ["Peter Young", "Micah Hodosh", "Julia Hockenmaier"], "venue": "In NAACL HLT Workshop on Creating Speech and Language Data with Amazon\u2019s Mechan-", "citeRegEx": "Rashtchian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "A Maximum Entropy Model for Prepositional Phrase Attachment", "author": ["Jeff Reynar", "Salim Roukos"], "venue": "In Proceedings of the workshop on Human Language Technology. ACL", "citeRegEx": "Ratnaparkhi et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Ratnaparkhi et al\\.", "year": 1994}, {"title": "Active Learning for Structured Probabilistic Models With Histogram Approximation", "author": ["Sun et al.2015] Qing Sun", "Ankit Laddha", "Dhruv Batra"], "venue": null, "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "A Comparative Study of Energy Minimization Methods for Markov Random", "author": ["Ramin Zabih", "Daniel Scharstein", "Olga Veksler", "Vladimir Kolmogorov", "Aseem Agarwala", "Marshall Tappen", "Carsten Rother"], "venue": null, "citeRegEx": "Szeliski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Szeliski et al\\.", "year": 2008}, {"title": "CIDEr: Consensus-based Image Description Evaluation", "author": ["Vedantam", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "CVPR.", "citeRegEx": "Vedantam et al\\.,? 2014", "shortCiteRegEx": "Vedantam et al\\.", "year": 2014}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Discriminative Re-ranking of Diverse Segmentations", "author": ["Dhruv Batra", "Greg Shakhnarovich"], "venue": null, "citeRegEx": "Yadollahpour et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yadollahpour et al\\.", "year": 2013}, {"title": "See No Evil, Say No Evil: Description Generation from Densely Labeled Images", "author": ["Yatskar et al.2014] Mark Yatskar", "Michel Galley", "Lucy Vanderwende", "Luke Zettlemoyer"], "venue": "In Lexical and Computational Semantics", "citeRegEx": "Yatskar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yatskar et al\\.", "year": 2014}, {"title": "Visual Madlibs: Fill in the Blank Description Generation and Question Answering", "author": ["Yu et al.2015] Licheng Yu", "Eunbyung Park", "Alexander C. Berg", "Tamara L. Berg"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Bringing Semantics Into Focus Using Visual Abstraction", "author": ["Zitnick", "Parikh2013] C. Lawrence Zitnick", "Devi Parikh"], "venue": null, "citeRegEx": "Zitnick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zitnick et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "We also make small improvements over DeepLab-CRF (Chen et al., 2015).", "startOffset": 49, "endOffset": 68}, {"referenceID": 26, "context": "This problem of determining whether a prepositional phrase (\u201cwith tuna\u201d) modifies a noun phrase (\u201csushi\u201d) or verb phrase (\u201ceating\u201d) is formally known as Prepositional Phrase Attachment Resolution (PPAR) (Ratnaparkhi et al., 1994).", "startOffset": 203, "endOffset": 229}, {"referenceID": 29, "context": "We demonstrate our approach on three datasets \u2013 ABSTRACT-50S (Vedantam et al., 2014), PASCAL-50S, and PASCAL-", "startOffset": 61, "endOffset": 84}, {"referenceID": 7, "context": "We also make small but consistent improvements over DeepLab-CRF (Chen et al., 2015).", "startOffset": 64, "endOffset": 83}, {"referenceID": 10, "context": "For instance, Fidler et al. (2013) use prepositions to improve object segmentation and scene classification, but only consider the mostlikely parse of the sentence and do not resolve ambi-", "startOffset": 14, "endOffset": 35}, {"referenceID": 32, "context": "Analogously, Yatskar et al. (2014) investigate the role of object, attribute, and action classification annotations for generating human-like descriptions.", "startOffset": 13, "endOffset": 35}, {"referenceID": 11, "context": "In a more recent work, Gella et al. (2016) studied the problem of reasoning about an image and a verb, where they attempt to pick the correct sense", "startOffset": 23, "endOffset": 43}, {"referenceID": 6, "context": "Berzak et al. (2015) resolve linguistic ambiguities in sentences coupled with videos that represent different interpretations of the sentences.", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "Berzak et al. (2015) resolve linguistic ambiguities in sentences coupled with videos that represent different interpretations of the sentences. Perhaps the work closest to us is Kong et al. (2014), who leverage information from an RGBD image and its sentential description to improve 3D semantic parsing and resolve ambiguities related to coreference resolution in the sentences (e.", "startOffset": 0, "endOffset": 197}, {"referenceID": 33, "context": ", 2015), Visual Madlibs (Yu et al., 2015), and image captioning (Vinyals et al.", "startOffset": 24, "endOffset": 41}, {"referenceID": 4, "context": "In our experiments, we use the k-best algorithm of Huang and Chiang (2005) for the sentence parsing module and the DivMBest algorithm (Batra et al., 2012) for the semantic seg-", "startOffset": 134, "endOffset": 154}, {"referenceID": 9, "context": "The standard measure for evaluating semantic segmentation is average Jaccard Index (or Intersectionover-Union) (Everingham et al., 2010), while for evaluating sentence parses w.", "startOffset": 111, "endOffset": 136}, {"referenceID": 29, "context": "we had to curate our own annotations for PPAR on three image caption datasets \u2013 ABSTRACT50S (Vedantam et al., 2014), PASCAL-50S (Vedantam et al.", "startOffset": 92, "endOffset": 115}, {"referenceID": 29, "context": ", 2014), PASCAL-50S (Vedantam et al., 2014) (expands the UIUC PASCAL sentence dataset (Rashtchian et al.", "startOffset": 20, "endOffset": 43}, {"referenceID": 25, "context": ", 2014) (expands the UIUC PASCAL sentence dataset (Rashtchian et al., 2010) from 5", "startOffset": 50, "endOffset": 75}, {"referenceID": 29, "context": "ABSTRACT-50S (Vedantam et al., 2014): 25,000 sentences (50 per image) with 500 images from abstract scenes made from clipart.", "startOffset": 13, "endOffset": 36}, {"referenceID": 29, "context": "PASCAL-50S (Vedantam et al., 2014): 50,000 sentences (50 per image) for the images in the UIUC PASCAL sentence dataset (Rashtchian et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 25, "context": ", 2014): 50,000 sentences (50 per image) for the images in the UIUC PASCAL sentence dataset (Rashtchian et al., 2010).", "startOffset": 92, "endOffset": 117}, {"referenceID": 7, "context": "We use DeepLab-CRF (Chen et al., 2015) and DivMBest (Batra et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 4, "context": ", 2015) and DivMBest (Batra et al., 2012) to produce M diverse segmentations of the images.", "startOffset": 21, "endOffset": 41}, {"referenceID": 7, "context": "the output of DeepLab-CRF (Chen et al., 2015) and for the PPAR module this is the 1-best output of the Stanford Parser (De Marneffe et al.", "startOffset": 26, "endOffset": 45}, {"referenceID": 15, "context": "This ablation of our system is similar to (Heitz et al., 2008) and helps us in disentangling the benefits of multiple", "startOffset": 42, "endOffset": 62}, {"referenceID": 21, "context": "sual entity, we use word2vec (Mikolov et al., 2013) similarities to map the nouns in the sentences to the corresponding dataset categories.", "startOffset": 29, "endOffset": 51}, {"referenceID": 7, "context": "\u2022 Semantic Segmentation Score Features (\u03c6S(y)) (2-dim): We use ranks and solution scores from DeepLab-CRF (Chen et al., 2015).", "startOffset": 106, "endOffset": 125}, {"referenceID": 7, "context": "We also make small improvements over DeepLabCRF (Chen et al., 2015) in the case of PASCAL-50S.", "startOffset": 48, "endOffset": 67}], "year": 2016, "abstractText": "We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images. Some ambiguities in language cannot be resolved without simultaneously reasoning about an associated image. If we consider the sentence \u201cI shot an elephant in my pajamas\u201d, looking at language alone (and not using common sense), it is unclear if it is the person or the elephant wearing the pajamas or both. Our approach produces a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly reranked to select the most consistent pair. We show that our semantic segmentation and prepositional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation. Multiple hypotheses are also shown to be crucial to improved multiple-module reasoning. Our vision and language approach significantly outperforms the Stanford Parser (De Marneffe et al., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two different experiments. We also make small improvements over DeepLab-CRF (Chen et al., 2015).", "creator": "LaTeX with hyperref package"}}}