{"id": "1604.02201", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Apr-2016", "title": "Transfer Learning for Low-Resource Neural Machine Translation", "abstract": "The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 Bleu, improving the state-of-the-art on low-resource machine translation.", "histories": [["v1", "Fri, 8 Apr 2016 00:16:35 GMT  (118kb,D)", "http://arxiv.org/abs/1604.02201v1", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["barret zoph", "deniz yuret", "jonathan may", "kevin knight"], "accepted": true, "id": "1604.02201"}, "pdf": {"name": "1604.02201.pdf", "metadata": {"source": "CRF", "title": "Transfer Learning for Low-Resource Neural Machine Translation", "authors": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight"], "emails": ["jonmay}@isi.edu", "dyuret@ku.edu.tr", "knight@isi.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance,"}, {"heading": "2 NMT Background", "text": "In the neural encoder decoder framework for MT (Neco and Forcada, 1997; Castan o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a), we use a recurrent neural network (encoder) to convert a source set into a dense fixed-length vector. Subsequently, we use another recursive network (decoder) to convert this vector into a target set. In this essay, we use a two-layer encoder decoder system (Figure 1) with long-term short-term memories (LSTM) (Hochreiter and Schmidhuber, 1997), which is trained to optimize the maximum probability (via a Softmax layer) with backpropagation over time (Werbos, 1990). Additionally, we use an attention mechanism that allows the target decoder to look back on the attentiveness model, specifically the source code or the 201among."}, {"heading": "3 Transfer Learning", "text": "In natural language processing, transfer learning methods have been successfully applied to speech recognition, document classification and sentiment analysis (Wang and Zheng, 2015). Deep learning models detect multiple levels of representation, some of which can be useful across tasks, making them particularly suitable for transferring learning to neural machines (Bengio, 2012). For example, Cires an et al. (2012) we use a revolutionary neural network to recognize handwritten characters and show positive effects of transfer between models of Latin and Chinese characters. Our first study to apply transfer learning to neural machine translation is simple and effective. We first train an NMT model on a dataset where there is a large amount of bilingual data (e.g. French-English) that we call the parent model."}, {"heading": "4 Experiments", "text": "To assess how well our transfer method works, we apply it to a variety of low-resource languages, both on its own and to re-evaluate a strong SBMT baseline. We report large increases in BLEU across the board with our transfer method. For all our experiments with low-resource languages, we use French as our native language and for childhood source languages, we use Hausa, Turkish, Uzbek and Urdu. The target language is always English. Table 1 also shows the size of training data for the infant languages where the language with the most data has only 1.8 million English tokens. For comparison, our parent uses a French-English model using a training set of 300 million English tokens and reaches 26 BLEU on the development kit. Table 1 also shows that the SBMT system scores along with the NMT baselines that do not use transfer. There is a large gap between the SBMT and the NMT systems without using the transfer method. The SBMT system, the static paper used in this tree, is a machine translate into English."}, {"heading": "4.1 Transfer Results", "text": "The results of our transfer learning method applied to the four above languages are shown in Table 2. Parent models were trained at the 2015 WMT (Bojar et al., 2015) Franco-English corpus for 5 epochs. Our base systems for NMT (\"NMT\" col-umn) all receive a major BLEU improvement in the use of the transfer method (the \"Xfer\" column) with an average BLEU improvement of 5.6. In addition, we continue to improve our BLEU values using unknown word exchanges from Luong et al. (2015b) and 8 models (the \"Final\" column), bringing the average BLEU improvement to 7.5. Overall, our method allows the NMT system to achieve competitive values and beat the SBMT system in one of the four language pairs."}, {"heading": "4.2 Re-scoring Results", "text": "We also use the NMT model with transfer learning to re-evaluate n-leaderboards (n = 1000) from the SBMT system. Table 3 shows the results of the re-scoring. Transfer NMT models offer the highest gains over the use of re-scoring with a neural language model or an NMT system that does not use transfer. The neural language model is an LSTM RNN with 2 layers and 1000 hidden states. It has a target vocabulary of 100K and is trained using noise-contrast estimation (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2014; Williams et al., 2015). Additionally, it is trained with a drop-out probability of 0.2, as in (Zaremba et al., 2014)."}, {"heading": "5 Analysis", "text": "We analyze the effects of using different parent models by regulating different parts of the child model and trying different regulatory techniques."}, {"heading": "5.1 Different Parent Languages", "text": "In the above experiments, we use French-English as a parent language pair. A description of the data used in this section is presented in Table 4. Our experimental results are presented in Table 5, where we use French and German as parent languages. If we simply train a model without transfer on a small Spanish-English learning set, we get a BLEU score of 16.4. If we use our transfer method with French and German as parent languages, we get BLEU scores of 31.0 and 29.8, respectively. As expected, French is a better parent for Spanish than German, which could be due to the fact that the mother tongue is more similar to the child's language. Overall, we can see that choosing the mother tongue can make a difference in the BLEU score, so that in our home, Turkish, Uzbek and Urdu experiments a parent language that is more optimal than French could improve the results."}, {"heading": "5.2 Effects of having Similar Parent Language", "text": "Next, we will consider a best-case scenario in which the mother tongue of the children's language is as similar as possible. Here, we are developing a synthetic model BLEU PPL size Uzbek-English 1.8 m 10.7 22.4 Uzbek-English transfer 1.8 m 15.0 (+ 4.3) 13.9 French \"-English 1.8 m 13.3 28.2 French\" -English transfer 1.8 m 20.0 (+ 6.7) 10.9 Children's language (called French), which is exactly like French, unless its vocabulary is randomly mixed. (e.g. \"international\" is now \"pomme,\" etc.) This language, which looks incomprehensible to human eyes, nevertheless has the same distribution and relational characteristics as actual French, i.e. the word that was \"roi\" (king) before the vocabulary distribution, will probably share distribution characteristics and thus embed similarity to the word that parents can have a better French before the relocation."}, {"heading": "5.3 Ablation Analysis", "text": "In all experiments mentioned above, only the target input and output embeddings are fixed during the training. In this section, we analyze what happens when different parts of the model are fixed to see what is best for performance. Figure 2 shows a diagram of the components of a sequence-to-sequence model. Table 7 shows how we begin to enable more components of the child-like NMT model and see the effects on performance in the model. We see that the optimal setting for transferring French-English to Uzbek-English with regard to BLEU performance objectiveAttentionTarget RNN Source RNNSource wortTarget Input Input Input InbeddingSource RNNSource WordTarget Input PerbeddingSource wordTarget Input Input Input Input Output Target RNNNTarget PredTarget Output Output Output Output Output Output Output Output Output Output OutputingSource RNNNNSource Output Output Output Output OutputingSource WordTarget Output Output Output Output Output Output Output OutbeddingSource worddingSource Output Output Output Output Output Output Output Output Output Output Output Output Output Output Output - Output Output Output Output Output - Output - Output Output Output - Output - Output Output - Output - Output - Output - Output - Output - Output - Output - Output - Output - Output - Output - Output - Output - Output - Output - Output - Output - Output - Output - Output - Output - Output Output Output - Output Output Output - Output Output - Output Output Output - - - - - Output - Output Output - - - Output Output Output Output Output - - - - Output - Output - Output - Output Output Output Output Output - - - - - Output Output Output - Output Output Output - Output Output Output - - - Output Output Output - - - - - Output Output Output - - - - Output Output"}, {"heading": "5.4 Learning Curve", "text": "In Figure 3, we present learning curves for both a transfer model and a non-transfer model to training and development sets. We see that the confusion of the final training set is very similar for both the transfer and non-transfer models, but the confusion of the development set for the transfer model is much better. However, the fact that the two models start from very different points and approach each other at very different points, but have similar training set performance, indicates that our architecture and training algorithm are capable of achieving a good minimum of the training target regardless of initialization. However, the training target appears to have a large pool of models of similar performance and not all of them are good at generalizing to the development set. The transfer model, which starts with a point and stays close to a point that is known to performing well on a related task, is led to a final point in the weight room that generalizes much better to the development set."}, {"heading": "5.5 Dictionary Initialization", "text": "With the transfer method, we always initialize embedding of the input language for the child model with randomly assigned embedding of the parent (who has a different input language).A smarter method might be to initialize embedding of the children with similar embedding of the parents, measuring the similarity by word-to-word-t table probabilities. To get these probabilities, we put together Uzbek and English-French t tables obtained from the Berkeley Aligner (Liang et al., 2006).Figure 4 shows that this dictionary-based mapping results in a faster improvement in the early part of the training. However, the final performance is similar to our standard model, suggesting that the training is capable of untangling the dictionary permutation introduced by randomly assigned embedding."}, {"heading": "5.6 Different Parent Models", "text": "In the above experiments, we use a parent model trained on a large French-English bilingual corpus. It could be assumed that our gains come from using the English half of the corpus as an additional language model resource. Therefore, we study transfer learning for the child model with parent models that use only the English side of the bilingual corpus. Table 8 shows the disadvantages of these experiments, in which we train a parent model to copy English sentences (English) and another parent model to encrypt coded English sentences (English-English) without permutations. In addition, we train a parent model that is only an RNN language model. These results show that our transfer learning does not simply import an English language model, but uses translation parameters that we have learned from the large bilingual text of the parents."}, {"heading": "6 Conclusion", "text": "Overall, our transfer method significantly improves NMT scores in low-resource languages, allowing our transfer NMT system to come close to the performance of a very strong SBMT system, and even outperforms its performance in home English. In addition, we consistently and significantly improve state-of-the-art SBMT systems in low-resource languages when the transfer NMT system is used for reassessment. Our experiments suggest that there is room for improvement in selecting parent languages that are more similar to children's languages, provided data can be found for such parents."}, {"heading": "7 Acknowledgments", "text": "This work was carried out with funds from DARPA (HR0011-15-C-0115) and ARL / ARO (W911NF-10-1-0533)."}], "references": [{"title": "Neural machine translation", "author": ["Bahdanau et al.2014] D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Pragmatic neural language modelling in machine translation", "author": ["Baltescu", "Blunsom2014] P. Baltescu", "P. Blunsom"], "venue": "arXiv preprint arXiv:1412.7119", "citeRegEx": "Baltescu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baltescu et al\\.", "year": 2014}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "A connectionist approach to machine translation", "author": ["Casta\u00f1o", "Casacuberta1997] M.A. Casta\u00f1o", "F. Casacuberta"], "venue": "In Proc. Eurospeech", "citeRegEx": "Casta\u00f1o et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Casta\u00f1o et al\\.", "year": 1997}, {"title": "11,001 new features for statistical machine translation", "author": ["Chiang et al.2009] D. Chiang", "K. Knight", "W. Wang"], "venue": "In Proc. NAACL", "citeRegEx": "Chiang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2009}, {"title": "Transfer learning for latin and chinese characters with deep neural networks", "author": ["U. Meier", "J. Schmidhuber"], "venue": "In Proc. IJCNN", "citeRegEx": "Cire\u015fan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cire\u015fan et al\\.", "year": 2012}, {"title": "What\u2019s in a translation rule", "author": ["Galley et al.2004] M. Galley", "M. Hopkins", "K. Knight", "D. Marcu"], "venue": "In Proc. HLT-NAACL", "citeRegEx": "Galley et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "Scalable inference and training of contextrich syntactic translation models", "author": ["Galley et al.2006] M. Galley", "J. Graehl", "K. Knight", "D. Marcu", "S. DeNeefe", "W. Wang", "I. Thayer"], "venue": "In Proc. ACLCOLING", "citeRegEx": "Galley et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Alignment by agreement", "author": ["Liang et al.2006] P. Liang", "B. Taskar", "D. Klein"], "venue": "In Proc. NAACL", "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Luong et al.2015a] M. Luong", "H. Pham", "C. Manning"], "venue": "In Proc. EMNLP", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong et al.2015b] T. Luong", "I. Sutskever", "Q. Le", "O. Vinyals", "W. Zaremba"], "venue": "In Proc. ACL", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426", "author": ["Mnih", "Teh2012] A. Mnih", "Y.W. Teh"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "Asynchronous translations with recurrent neural nets", "author": ["Neco", "Forcada1997] R. Neco", "M. Forcada"], "venue": "In Proc. International Conference on Neural Networks", "citeRegEx": "Neco et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Neco et al\\.", "year": 1997}, {"title": "A survey on transfer learning", "author": ["Pan", "Yang2010] S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Pan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["O. Vinyals", "Q.V. Le"], "venue": "In Proc. NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Vaswani et al.2013] A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "In Proc. EMNLP", "citeRegEx": "Vaswani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Transfer learning for speech and language processing", "author": ["Wang", "Zheng2015] D. Wang", "T. Fang Zheng"], "venue": "arXiv preprint arXiv:1511.06066", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proc. IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Scaling recurrent neural network language models. CoRR, abs/1502.00512", "author": ["Williams et al.2015] W. Williams", "N. Prasad", "D. Mrva", "T. Ash", "T. Robinson"], "venue": null, "citeRegEx": "Williams et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization. CoRR, abs/1409.2329", "author": ["Zaremba et al.2014] W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "Neural machine translation (NMT) (Sutskever et al., 2014) is a promising paradigm for extracting translation knowledge from parallel text.", "startOffset": 33, "endOffset": 57}, {"referenceID": 6, "context": "Table 1 shows that for 4 low-resource languages, a standard stringto-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feed-input techniques from Luong et al.", "startOffset": 103, "endOffset": 145}, {"referenceID": 7, "context": "Table 1 shows that for 4 low-resource languages, a standard stringto-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feed-input techniques from Luong et al.", "startOffset": 103, "endOffset": 145}, {"referenceID": 6, "context": "Table 1 shows that for 4 low-resource languages, a standard stringto-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feed-input techniques from Luong et al. (2015a).", "startOffset": 104, "endOffset": 281}, {"referenceID": 15, "context": "Figure 1: The encoder-decoder framework for neural machine translation (NMT) (Sutskever et al., 2014).", "startOffset": 77, "endOffset": 101}, {"referenceID": 15, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector.", "startOffset": 47, "endOffset": 170}, {"referenceID": 0, "context": "In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; Casta\u00f1o and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector.", "startOffset": 47, "endOffset": 170}, {"referenceID": 18, "context": "In this paper, we use a two-layer encoder-decoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained to optimize maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990).", "startOffset": 240, "endOffset": 254}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014; Luong et al., 2015a), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector. We then use another recurrent network (decoder) to convert that vector to a target sentence. In this paper, we use a two-layer encoder-decoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) trained to optimize maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990). Additionally, we use an attention mechanism that allows the target decoder to look back at the source encoder, specifically the local attention plus feed-input model from Luong et al. (2015a).", "startOffset": 8, "endOffset": 702}, {"referenceID": 2, "context": "Deep learning models discover multiple levels of representation, some of which may be useful across tasks, which makes them particularly suited to transfer learning (Bengio, 2012).", "startOffset": 165, "endOffset": 179}, {"referenceID": 2, "context": "Deep learning models discover multiple levels of representation, some of which may be useful across tasks, which makes them particularly suited to transfer learning (Bengio, 2012). For example, Cire\u015fan et al. (2012) use a convolutional neural network to recognize handwritten characters and show positive effects of transfer between models for Latin and Chinese characters.", "startOffset": 166, "endOffset": 216}, {"referenceID": 7, "context": "The SBMT system used in this paper is a stringto-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004).", "startOffset": 93, "endOffset": 135}, {"referenceID": 6, "context": "The SBMT system used in this paper is a stringto-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004).", "startOffset": 93, "endOffset": 135}, {"referenceID": 4, "context": "Additionally, the SBMT models use thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009).", "startOffset": 98, "endOffset": 119}, {"referenceID": 20, "context": "5 as in Zaremba et al. (2014). The common parent model is trained with a dropout probability of 0.", "startOffset": 8, "endOffset": 30}, {"referenceID": 10, "context": "Additionally, when we use unknown word replacement from Luong et al. (2015b) and ensemble together 8 models (the \u2018Final\u2019 column) we further improve upon our BLEU scores, bringing the average BLEU improvement to 7.", "startOffset": 56, "endOffset": 77}, {"referenceID": 16, "context": "It has a target vocabulary of 100K and is trained using noise-contrastive estimation (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2014; Williams et al., 2015).", "startOffset": 85, "endOffset": 178}, {"referenceID": 19, "context": "It has a target vocabulary of 100K and is trained using noise-contrastive estimation (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2014; Williams et al., 2015).", "startOffset": 85, "endOffset": 178}, {"referenceID": 20, "context": "2 as in (Zaremba et al., 2014).", "startOffset": 8, "endOffset": 30}, {"referenceID": 9, "context": "To get these probabilities we compose UzbekEnglish and English-French t-tables obtained from the Berkeley Aligner (Liang et al., 2006).", "startOffset": 114, "endOffset": 134}], "year": 2016, "abstractText": "The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves BLEU scores across a range of low-resource languages. Our key idea is to first train a highresource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs. Ensembling and unknown word replacement add another 2 BLEU which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for rescoring, we can improve the SBMT system by an average of 1.3 BLEU, improving the state-of-the-art on low-resource machine translation.", "creator": "LaTeX with hyperref package"}}}