{"id": "1604.02993", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "Using Sentence-Level LSTM Language Models for Script Inference", "abstract": "There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from documents. These systems operate on structured verb-argument events produced by an NLP pipeline. We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, finding the latter to be roughly comparable to the former in terms of predicting missing events in documents.", "histories": [["v1", "Mon, 11 Apr 2016 15:21:05 GMT  (106kb,D)", "https://arxiv.org/abs/1604.02993v1", null], ["v2", "Wed, 8 Jun 2016 19:05:14 GMT  (303kb,D)", "http://arxiv.org/abs/1604.02993v2", "To appear in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["karl pichotta", "raymond j mooney"], "accepted": true, "id": "1604.02993"}, "pdf": {"name": "1604.02993.pdf", "metadata": {"source": "CRF", "title": "Using Sentence-Level LSTM Language Models for Script Inference", "authors": ["Karl Pichotta", "Raymond J. Mooney"], "emails": ["pichotta@cs.utexas.edu", "mooney@cs.utexas.edu"], "sections": [{"heading": null, "text": "There is a small but growing body of research on statistical scripts, models of event sequences that make it possible to derive probable implicit events from documents. These systems work with structured verb argument events generated by an NLP pipeline. We compare these systems with newer models of recurring neural networks that rely directly on raw values to predict sentences, the latter being roughly comparable to the former in terms of predicting missing events in documents."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Statistical Script Learning", "text": "Scripts, structured models of stereotypical sequences of events, originate from 1970s AI research, particularly from the groundbreaking work of Schank and Abelson (1977), in which scripts are modeled as temporally ordered sequences of symbolically structured events, which are less likely and brittle and pose serious problems for automated learning.In recent years, there has been an increasing amount of research on statistical script learning systems that allow statistical inferences from implicit events in the text.Chambers and Jurafsky (2008; 2009) describe a number of simple event-based systems that relate to (verb, dependence) pairs that relate to a particular discourse unit. For example, Andrew Wiles won the Abel Prize in 2016 for proving Fermat's last theorem, such a system will ideally be able to accept new facts such as (subject) or (subject, Andrew Wiles) for the entirety."}, {"heading": "2.2 Recurrent Neural Networks", "text": "Recursive neural networks (RNNs) are neural networks whose arithmetic diagrams have complex units. Specifically, RNN sequence models are RNNs that have a sequence of inputs x1,.. \u2212 xT to a sequence of outputs y1,.., yT via a learned latent vector whose value at the time t is a function of its value at the previous time t \u2212 1.The most basic RNN sequence models, so-called \"vanilla RNNs\" (Elman, 1990), are described by the following equations: zt = f (Wi, zxt + Wz, zzt \u2212 1) ot = g (Wz, ozt), where xt is the vector that describes the input at the time t; zt is the vector that indicates the hidden state at the time t; ot is the vector that describes the predicted output at the time t; f and g are elementary non-linear functions (typical, ischid, or hyperbolic)."}, {"heading": "2.3 Sentence-Level RNN Language Models", "text": "Recently, RNN sequence models have proven to be extremely effective for speech models at the word and character level (Mikolov et al., 2011; Jozefowicz et al., 2016). At each step in time, these models take a word or character as input, update a hidden state vector and then predict the word or character of the next period. There is also an increasing number of studies on the formation of RNN encoder models for NLP problems. These systems first encode the entire input into the hidden state vector of the network and then, in a second step, decode the entire output from this vector (Sutskever et al., 2014; Vinyals et al., 2015; Serban et al., 2016).RNN language models at the sentence level, for example the skipped thought vector system of Kiros et al al al al al al al al al al al al al al al al al al al al al al al. (2015), conceptually build a bridge between these standard sequences, we are trained to predict in the next two sentences (during the following phrases)."}, {"heading": "3 Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Narrative Cloze Evaluation", "text": "The evaluation of inference-focused statistical scripting systems is not easy. Chambers and Jurafsky (2008) introduced the evaluation of the narrative cloze, in which a single event is kept out of a document, and systems are judged by the ability to derive that event from the remaining events. This evaluation has been used by a number of published scripting systems (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al.) This automated evaluation measures the ability of systems to model and predict events as they occur in texts. The exact definition of the narrative cloze evaluation depends on the formulation of events used in a scripting system. For example, Chambers and Jurafsky et al., Jans et al. (2012), and Rudinger et al."}, {"heading": "3.2 Models", "text": "In fact, it is so that it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way and a way it is about a way in which it is about a way in which it is about a way in which it is about a way and a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way and a way and a way"}, {"heading": "4 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental Details", "text": "We train a number of LSTM encoder decoder networks that vary in their inputs and outputs. Models are trained in English Wikipedia, with 1% of the documents held as validation sets. Our test set consists of 10,000 invisible sentences (of articles not included in training or validation). We train models with stochastic descent with dynamics, minimizing the cross-entropy error of output predictions. All models are implemented in TensorFlow (Abadi et al., 2015). We use a vocabulary of the 50,000 most common tokens that replace all other tokens with an out-of-vocabulary token."}, {"heading": "4.2 Experimental Evaluation", "text": "In the neeisrrrllllrlllrlllrrrrllrrrllrrrllrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrr"}, {"heading": "4.3 Adding Additional Context", "text": "The above results relate to systems that encode information about a set and decode6This is also a minor misuse of notation, since the second transformation uses a statistical parser instead of an encoder / decoder, in the spirit of the Skip-gram system by Kiros et al. (2015), but we may want to attach more of the document to conditions. To investigate this, we are conducting an experiment that varies the number of previous sentences entered without attention during the encoding step of t1 t2 text models. We train three different models that use either one, three or five sentences as input and are trained to output the follow-up sentence. Table 3 gives the results of executing these models to 10,000 sentences from the validation set. As you can see, other additional context sets in the training setup studied, depending on the metric, have a mixed effect. This is perhaps partly due to the fact that we have fixed hyper parameters between experiments, another would benefit from a longer parameter system and another would benefit from a longer one."}, {"heading": "4.4 Qualitative Analysis", "text": "Figure 4 gives an example of automatic predictions of nexus text, along with the input set and the gold standard next sentence. Note that gold standard sequences often contain new details that are not obviously derivable from the previous text. Top system predictions, on the other hand, are often relatively short. This is probably due partly to the fact that entropy loss does not directly punish short sentences, and partly to the fact that many details in the gold standard sequence are inherently difficult to predict."}, {"heading": "4.5 Discussion", "text": "The generally low order of magnitude of BLEU values presented in Table 1, especially when compared to the values typically cited in the results of machine translation, indicates the difficulty of the task. In open domain text, a sentence is typically not directly predictable compared to previous text; if it were, it would probably not be specified. In the task of verb argument prediction in Table 2, the difference between t1 t2 and e1 e2 [0] is relatively marginal, raising the general question of how much explicit syntactical analysis is required to perform the task of event deduction, especially in the encoder / decoder setup. These results provide evidence that a sentence-level RNN language model based on raw data can predict as well or nearly as well as an event-mediated script model of what comes next in a document."}, {"heading": "5 Future Work", "text": "There are a number of other extensions to this work. Firstly, in this work (and more generally in Neural Machine Translation research), although generated text is evaluated with BLEU, systems are optimized for a symbolic cross-entropy error, which is another goal (Luong et al. (2016) give an example of a system that improves cross-entropy error but lowers the BLEU score in the context of Neural Machine Translation). Finding differentiable objective functions that more directly target more complex valuation metrics such as BLEU is an interesting future research direction.Although we argue that BLEU is a natural token sequence level analogous to the cumulative formulation of the Narrative Cloze task, it is evidently not the best metric for evaluating conclusions from text and comparing these automated metrics with human judgments is an important direction for future work. Pichotta and Mooney (Results from a 2016 review) could repeat themselves."}, {"heading": "6 Related Work", "text": "The use of scripts in artificial intelligence dates back to the 1970s (Minsky, 1974; Schank and Abelson, 1977); in this conception, scripts were composed of complex events, without probabilistic semantics that were automatically difficult to learn. In recent years, a growing research collective has been studying probabilistic coexistence models with simpler events; Jans et al. (2012) provide a superior model of the coexistence of (verb, dependence) pairs that can be used to derive such pairs from documents; Jans et al. (2012) provide a superior model within the same general framework. Chambers and Jurafsky (2009) provide a model of generalization of couple events to collections of such sequences. Rudinger et al. (2015) apply a discriminatory language model to (verb, dependence) that models a task and raises the question of the extent to which event-conditional consequences can be carried out."}, {"heading": "7 Conclusion", "text": "We found that models that work with raw text are roughly comparable to identical models that work on predicate-argument event structures in predicting the latter, and that text models provide better predictions of raw text. This provides evidence that encoder / decoder models mediated by automatically extracted events may not learn much more structure to perform the task of delivered event forecasting than systems trained on raw text alone."}, {"heading": "Acknowledgments", "text": "Thanks to Stephen Roller, Amelia Harrison and the UT NLP group for their help and feedback. Thanks also to the anonymous reviewers for their very helpful suggestions. This research was partially supported by the DARPA DEFT program within the AFRL grant FA8750-13-2-0026."}, {"heading": "A Supplemental Material", "text": "Our Wikipedia dump, on which the training, development and test kits are built, dates back to January 2, 2014. We analyze text with version 3.3.1 of the Stanford CoreNLP system. We use a vocabulary consisting of the 50,000 most common tokens, and replace all others with an out-of-vocabulary pseudotoken. We train with a stack of stochastic gradient descent with dynamics, with a stack size of 10 sequences at an initial learning rate of 0.1, damping the learning rate by 0.99 each time the average test error of the previous hundred updates is greater than any of the average losses in the previous ten groups of a hundred updates. Our impulse parameter is 0.95. Our embedding vectors are 100-dimensional, and our hidden LSTM state is 500-dimensional. We train all models for 300k batch updates (with the exception of the models in comparison to \u00a7 3 update, we train all the stacks for 15k-longer ones)."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng."], "venue": "Software available from", "citeRegEx": "Vanhoucke et al\\.,? 2015", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["KyungHyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the 2015 International Conference on Learning Representations (ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Generating coherent event schemas at scale", "author": ["Stephen Soderland", "Mausam", "Oren Etzioni"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Balasubramanian et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Balasubramanian et al\\.", "year": 2013}, {"title": "Unsupervised learning of narrative event chains", "author": ["Chambers", "Jurafsky2008] Nathanael Chambers", "Daniel Jurafsky"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Chambers et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chambers et al\\.", "year": 2008}, {"title": "Unsupervised learning of narrative schemas and their participants", "author": ["Chambers", "Jurafsky2009] Nathanael Chambers", "Dan Jurafsky"], "venue": "In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-09),", "citeRegEx": "Chambers et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chambers et al\\.", "year": 2009}, {"title": "Event schema induction with a probabilistic entity-driven model", "author": ["Nathanael Chambers"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP-2013)", "citeRegEx": "Chambers.,? \\Q2013\\E", "shortCiteRegEx": "Chambers.", "year": 2013}, {"title": "Probabilistic frame induction", "author": ["Hoifung Poon", "Lucy Vanderwende"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Cheung et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cheung et al\\.", "year": 2013}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "In NIPS Deep Learning Workshop", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Bill MacCartney", "Christopher D. Manning"], "venue": "In Proceedings of the 5th International Conference on Language Resources", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "A unified Bayesian model of scripts, frames and language", "author": ["Ferraro", "Van Durme2016] Francis Ferraro", "Benjamin Van Durme"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)", "citeRegEx": "Ferraro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ferraro et al\\.", "year": 2016}, {"title": "What happens next? Event prediction using a compositional neural network model", "author": ["Granroth-Wilding", "Clark2016] Mark GranrothWilding", "Stephen Clark"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)", "citeRegEx": "Granroth.Wilding et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Granroth.Wilding et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Proceedings of the 29th Annual Conference on Neural", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Skip ngrams and ranking functions for predicting script events", "author": ["Jans et al.2012] Bram Jans", "Steven Bethard", "Ivan Vuli\u0107", "Marie Francine Moens"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association", "citeRegEx": "Jans et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jans et al\\.", "year": 2012}, {"title": "One vector is not enough: Entityaugmented distributional semantics for discourse relations. Transactions of the Association for Computational Linguistics (TACL)", "author": ["Ji", "Eisenstein2015] Yangfeng Ji", "Jacob Eisenstein"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "arXiv preprint arXiv:1602.02410", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Herbst."], "venue": "Proceedings of", "citeRegEx": "Herbst.,? 2007", "shortCiteRegEx": "Herbst.", "year": 2007}, {"title": "Recursive deep models for discourse parsing", "author": ["Li et al.2014] Jiwei Li", "Rumeng Li", "Eduard Hovy"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Implicit discourse relation classification via multi-task neural networks", "author": ["Liu et al.2016] Yang Liu", "Sujian Li", "Xiaodong Zhang", "Zhifang Sui"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": "In Proceedings of the 4th International Conference on Learning Representations (ICLR-16)", "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Plot induction and evolutionary search for story generation", "author": ["McIntyre", "Lapata2010] Neil McIntyre", "Mirella Lapata"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "McIntyre et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McIntyre et al\\.", "year": 2010}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["Anoop Deoras", "Stefan Kombrink", "Lukas Burget", "Jan Cernock\u1ef3"], "venue": "In Proceedings of the 12th Annual Conference of the Inter-", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "A framework for representing knowledge", "author": ["Marvin Minsky"], "venue": "Technical report, MIT-AI Laboratory", "citeRegEx": "Minsky.,? \\Q1974\\E", "shortCiteRegEx": "Minsky.", "year": 1974}, {"title": "Inducing neural models of script knowledge", "author": ["Modi", "Titov2014] Ashutosh Modi", "Ivan Titov"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning", "citeRegEx": "Modi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Modi et al\\.", "year": 2014}, {"title": "Learning scripts as Hidden Markov Models", "author": ["Orr et al.2014] J Walker Orr", "Prasad Tadepalli", "Janardhan Rao Doppa", "Xiaoli Fern", "Thomas G Dietterich"], "venue": "In Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI-14)", "citeRegEx": "Orr et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Orr et al\\.", "year": 2014}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Statistical script learning with multi-argument events", "author": ["Pichotta", "Mooney2014] Karl Pichotta", "Raymond J. Mooney"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association", "citeRegEx": "Pichotta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pichotta et al\\.", "year": 2014}, {"title": "Learning statistical scripts with LSTM recurrent neural networks", "author": ["Pichotta", "Mooney2016] Karl Pichotta", "Raymond J. Mooney"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)", "citeRegEx": "Pichotta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pichotta et al\\.", "year": 2016}, {"title": "Script induction as language modeling", "author": ["Pushpendre Rastogi", "Francis Ferraro", "Benjamin Van Durme"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP-", "citeRegEx": "Rudinger et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rudinger et al\\.", "year": 2015}, {"title": "Scripts, Plans, Goals and Understanding: An Inquiry into Human Knowledge Structures", "author": ["Schank", "Abelson1977] Roger C. Schank", "Robert P. Abelson"], "venue": null, "citeRegEx": "Schank et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Schank et al\\.", "year": 1977}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Proceedings of the 28th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Proceedings of the 29th Annual Conference on Neural Information Processing Systems (NIPS-15),", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 29, "context": "discriminative language models (Rudinger et al., 2015) and Long Short Term Memory (LSTM) recurrent neural nets (Pichotta and Mooney, 2016), are able to infer held-out events more accurately than previous approaches.", "startOffset": 31, "endOffset": 54}, {"referenceID": 14, "context": "A number of other systems inferring the same types of pair events have been shown to provide superior performance in modeling events in documents (Jans et al., 2012; Rudinger et al., 2015).", "startOffset": 146, "endOffset": 188}, {"referenceID": 29, "context": "A number of other systems inferring the same types of pair events have been shown to provide superior performance in modeling events in documents (Jans et al., 2012; Rudinger et al., 2015).", "startOffset": 146, "endOffset": 188}, {"referenceID": 14, "context": "A number of other systems inferring the same types of pair events have been shown to provide superior performance in modeling events in documents (Jans et al., 2012; Rudinger et al., 2015). Pichotta and Mooney (2014) give a cooccurrence based script system that models and infers more complex multi-argument events from text.", "startOffset": 147, "endOffset": 217}, {"referenceID": 14, "context": "A number of other systems inferring the same types of pair events have been shown to provide superior performance in modeling events in documents (Jans et al., 2012; Rudinger et al., 2015). Pichotta and Mooney (2014) give a cooccurrence based script system that models and infers more complex multi-argument events from text. For example, in the above example, their model would ideally be able to infer a single event like accept(Wiles, prize), as opposed to the two simpler pairs from which it is composed. They provide evidence that modeling and inferring more complex multi-argument events also yields superior performance on the task of inferring simpler (verb, dependency) pair events. These events are constructed using only coreference information; that is, the learned event co-occurrence models do not directly incorporate noun information. More recently, Pichotta and Mooney (2016) presented an LSTM-based script inference model", "startOffset": 147, "endOffset": 893}, {"referenceID": 9, "context": "The most basic RNN sequence models, socalled \u201cvanilla RNNs\u201d (Elman, 1990), are described by the following equations:", "startOffset": 60, "endOffset": 73}, {"referenceID": 22, "context": "RNN sequence models have recently been shown to be extremely effective for word-level and character-level language models (Mikolov et al., 2011; Jozefowicz et al., 2016).", "startOffset": 122, "endOffset": 169}, {"referenceID": 16, "context": "RNN sequence models have recently been shown to be extremely effective for word-level and character-level language models (Mikolov et al., 2011; Jozefowicz et al., 2016).", "startOffset": 122, "endOffset": 169}, {"referenceID": 33, "context": "These systems first encode the entire input into the network\u2019s hidden state vector and then, in a second step, decode the entire output from this vector (Sutskever et al., 2014; Vinyals et al., 2015; Serban et al., 2016).", "startOffset": 153, "endOffset": 220}, {"referenceID": 34, "context": "These systems first encode the entire input into the network\u2019s hidden state vector and then, in a second step, decode the entire output from this vector (Sutskever et al., 2014; Vinyals et al., 2015; Serban et al., 2016).", "startOffset": 153, "endOffset": 220}, {"referenceID": 31, "context": "These systems first encode the entire input into the network\u2019s hidden state vector and then, in a second step, decode the entire output from this vector (Sutskever et al., 2014; Vinyals et al., 2015; Serban et al., 2016).", "startOffset": 153, "endOffset": 220}, {"referenceID": 7, "context": "(2015) train an encoder-decoder model to encode a sentence into a fixed-length vector and subsequently decode both the following and preceding sentence, using Gated Recurrent Units (Chung et al., 2014).", "startOffset": 181, "endOffset": 201}, {"referenceID": 14, "context": "This evaluation has been used by a number of published script systems (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015).", "startOffset": 70, "endOffset": 168}, {"referenceID": 29, "context": "This evaluation has been used by a number of published script systems (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015).", "startOffset": 70, "endOffset": 168}, {"referenceID": 5, "context": "Chambers and Jurafsky (2008) introduced the Narrative Cloze evaluation, in which a single event is held out from a document and systems are judged by the ability to infer this held-out event given the remaining events.", "startOffset": 0, "endOffset": 29}, {"referenceID": 5, "context": "For example, Chambers and Jurafsky (2008), Jans et al.", "startOffset": 13, "endOffset": 42}, {"referenceID": 5, "context": "For example, Chambers and Jurafsky (2008), Jans et al. (2012), and Rudinger et al.", "startOffset": 13, "endOffset": 62}, {"referenceID": 5, "context": "For example, Chambers and Jurafsky (2008), Jans et al. (2012), and Rudinger et al. (2015) evaluate inference of heldout (verb, dependency) pairs from documents; Pichotta and Mooney (2014) evaluate inference of", "startOffset": 13, "endOffset": 90}, {"referenceID": 5, "context": "For example, Chambers and Jurafsky (2008), Jans et al. (2012), and Rudinger et al. (2015) evaluate inference of heldout (verb, dependency) pairs from documents; Pichotta and Mooney (2014) evaluate inference of", "startOffset": 13, "endOffset": 188}, {"referenceID": 26, "context": "evaluate inferred text against gold standard text, we argue that the BLEU metric (Papineni et al., 2002), commonly used to evaluate Statistical Machine Translation systems, is a natural evaluation metric.", "startOffset": 81, "endOffset": 104}, {"referenceID": 33, "context": "This evaluation takes some inspiration from the evaluation of neural encoder-decoder translation models (Sutskever et al., 2014; Bahdanau et al., 2015), which use similar architectures for the task", "startOffset": 104, "endOffset": 151}, {"referenceID": 1, "context": "This evaluation takes some inspiration from the evaluation of neural encoder-decoder translation models (Sutskever et al., 2014; Bahdanau et al., 2015), which use similar architectures for the task", "startOffset": 104, "endOffset": 151}, {"referenceID": 29, "context": "Exactly what constitutes an event varies: it may be a (verb, dependency) pair inferred as relating to a particular discourse entity (Chambers and Jurafsky, 2008; Rudinger et al., 2015), a simplex verb (Chambers and Jurafsky, 2009; Orr et al.", "startOffset": 132, "endOffset": 184}, {"referenceID": 25, "context": ", 2015), a simplex verb (Chambers and Jurafsky, 2009; Orr et al., 2014), or a verb with multiple arguments (Pichotta and Mooney, 2014).", "startOffset": 24, "endOffset": 71}, {"referenceID": 2, "context": "In the present work, we adopt a representation of events as verbs with multiple arguments (Balasubramanian et al., 2013; Pichotta and Mooney, 2014; Modi and Titov, 2014).", "startOffset": 90, "endOffset": 169}, {"referenceID": 34, "context": "We use the attention mechanism of Vinyals et al. (2015). In short, these models", "startOffset": 34, "endOffset": 56}, {"referenceID": 34, "context": "For more details on the model, see Vinyals et al. (2015).", "startOffset": 35, "endOffset": 57}, {"referenceID": 32, "context": "To extract events from text, we use the Stanford Dependency Parser (De Marneffe et al., 2006; Socher et al., 2013).", "startOffset": 67, "endOffset": 114}, {"referenceID": 14, "context": "Jans et al. (2012), Pichotta and Mooney (2016)) present results on the \u201cRecall at k\u201d metric, judging gold-standard recall against a list of top k event inferences; this metric is equivalent to \u201cRecall at 1.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "Jans et al. (2012), Pichotta and Mooney (2016)) present results on the \u201cRecall at k\u201d metric, judging gold-standard recall against a list of top k event inferences; this metric is equivalent to \u201cRecall at 1.", "startOffset": 0, "endOffset": 47}, {"referenceID": 14, "context": "Note also that we judge only against a system\u2019s single most confident prediction (as opposed to some prior work (Jans et al., 2012; Pichotta and Mooney, 2014) which takes the top k predictions\u2014the numbers presented here are therefore noticeably lower).", "startOffset": 112, "endOffset": 158}, {"referenceID": 20, "context": "Neural Machine Translation research), though generated text is evaluated using BLEU, systems are optimized for per-token cross-entropy error, which is a different objective (Luong et al. (2016) give an example of a system which improves cross-entropy error but reduces BLEU score in the", "startOffset": 174, "endOffset": 194}, {"referenceID": 23, "context": "The use of scripts in AI dates back to the 1970s (Minsky, 1974; Schank and Abelson, 1977); in this conception, scripts were composed of complex events with no probabilistic semantics, which were difficult to learn automatically.", "startOffset": 49, "endOffset": 89}, {"referenceID": 5, "context": "Chambers and Jurafsky (2008) propose a model of co-occurrence of (verb, dependency) pairs, which can be used to infer such", "startOffset": 0, "endOffset": 29}, {"referenceID": 13, "context": "pairs from documents; Jans et al. (2012) give a superior model in the same general framework.", "startOffset": 22, "endOffset": 41}, {"referenceID": 5, "context": "Chambers and Jurafsky (2009) give a method of", "startOffset": 0, "endOffset": 29}, {"referenceID": 29, "context": "Rudinger et al. (2015) apply a discriminative language model to the (verb, dependency) sequence modeling task, raising the question of to what extent event in-", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "There is a body of related work focused on learning models of co-occurring events to automatically induce templates of complex events comprising multiple verbs and arguments, aimed ultimately at maximizing coherency of templates (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013).", "startOffset": 229, "endOffset": 296}, {"referenceID": 6, "context": "There is a body of related work focused on learning models of co-occurring events to automatically induce templates of complex events comprising multiple verbs and arguments, aimed ultimately at maximizing coherency of templates (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013).", "startOffset": 229, "endOffset": 296}, {"referenceID": 2, "context": "There is a body of related work focused on learning models of co-occurring events to automatically induce templates of complex events comprising multiple verbs and arguments, aimed ultimately at maximizing coherency of templates (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013).", "startOffset": 229, "endOffset": 296}, {"referenceID": 2, "context": ", 2013; Balasubramanian et al., 2013). Ferraro and Van Durme (2016) give a model integrating various levels of event information of increasing abstraction, evaluating both on coherence of induced templates and log-likelihood of predictions of held-out events.", "startOffset": 8, "endOffset": 68}, {"referenceID": 2, "context": ", 2013; Balasubramanian et al., 2013). Ferraro and Van Durme (2016) give a model integrating various levels of event information of increasing abstraction, evaluating both on coherence of induced templates and log-likelihood of predictions of held-out events. McIntyre and Lapata (2010) describe a system that learns a model of co-occurring events and uses this model to automatically generate stories via a Genetic Algorithm.", "startOffset": 8, "endOffset": 287}, {"referenceID": 18, "context": "Li et al. (2014) and Ji and Eisenstein", "startOffset": 0, "endOffset": 17}, {"referenceID": 19, "context": "(2015), use RNNs for discourse parsing; Liu et al. (2016) use a Convolutional Neural Network for implicit discourse relation classification.", "startOffset": 40, "endOffset": 58}], "year": 2016, "abstractText": "There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from documents. These systems operate on structured verb-argument events produced by an NLP pipeline. We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, finding the latter to be roughly comparable to the former in terms of predicting missing events in documents.", "creator": "LaTeX with hyperref package"}}}