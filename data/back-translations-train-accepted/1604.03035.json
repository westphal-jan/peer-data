{"id": "1604.03035", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "Learning Global Features for Coreference Resolution", "abstract": "There is compelling evidence that coreference prediction would benefit from modeling global information about entity-clusters. Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difficulty of crafting informative cluster-level features. We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters directly from their mentions. We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coreference system that outperforms the state of the art without requiring any additional search.", "histories": [["v1", "Mon, 11 Apr 2016 17:15:34 GMT  (667kb,D)", "http://arxiv.org/abs/1604.03035v1", "Accepted to NAACL 2016"]], "COMMENTS": "Accepted to NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sam wiseman", "alexander m rush", "stuart m shieber"], "accepted": true, "id": "1604.03035"}, "pdf": {"name": "1604.03035.pdf", "metadata": {"source": "CRF", "title": "Learning Global Features for Coreference Resolution", "authors": ["Sam Wiseman"], "emails": ["swiseman@seas.harvard.edu", "srush@seas.harvard.edu", "shieber@seas.harvard.edu"], "sections": [{"heading": "1 Introduction", "text": "While structured, non-local correlation models seem promising to avoid many common correlation errors (as discussed further in Section 3), the results of applying such models in practice are clearly mixed, and state-of-the-art results can be achieved through a fully local reference ranking system. In this work, we postulate that a global context is indeed necessary to further improve correlation resolution, but argue that informative cluster functions are very difficult to develop and limit their effectiveness. Accordingly, we suggest instead to learn representations of mention clusters by embedding them sequentially using a recursive neural network (as shown in Section 4). Our model does not have any manually defined cluster functions, but learns a global representation from the individual mentions that exist in each cluster. We integrate these representations into a mention ranking-style correlation system. The entire model, including the recursive neural network, and multiple mention-system, is a correlation ranking-point."}, {"heading": "2 Background and Notation", "text": "We assume that it will be a way in which there will be a sequence of clusters (X (m)) Mm = 1 in such a way that all mentions in a particular cluster are worth mentioning. (1) The mentions within a particular cluster can be arranged in such a way that all mentions in a particular cluster are worth mentioning. (1) The mentions in a single cluster we mentioned in such a cluster. (1) The mentions in a particular cluster we mentioned in such a cluster. (1) The mentions in another cluster we mentioned the notation X (m) j'th mentions in the m'th cluster.A valid cluster of each mention in exactly one cluster, and so we can represent a cluster with a vector z. (1, M) N where zn = iff xn is a member of X (m)."}, {"heading": "3 The Role of Global Features", "text": "Here, we motivate the use of global attributes to solve co-references by focusing on the problems that can arise when pronominal mentions are solved in a purely local way. See Clark and Manning (2015) and Stoyanov and Eisner (2012) for more general motivation to use global models."}, {"heading": "3.1 Pronoun Problems", "text": "Recent empirical work has shown that resolution of pronominal mentions accounts for a significant percentage of total errors made by modern mention systems. Wiseman et al. (2015) shows that at the CoNLL 2012 English Development Group, nearly 59% of mention errors and nearly 24% of memory errors include pronominal mentions. Martschat and Strube (2015) found a similar pattern in their comparison of mention locations, mention combinations and latent tree models. To see why pronouns can be so problematic, consider the following passage from the \"Broadcast Conversation\" part of the CoNLL development (bc / msnbc / 0000 / 018).We include mentions in brackets and enter the same subscript to merged mentions. (This example is also shown in Figure 2.) and DA [1] think that's what Linda is."}, {"heading": "3.2 Issues with Global Features", "text": "We believe that a major reason for the relative ineffectiveness of global traits in correlation problems is that, as mentioned by Clark and Manning (2015), traits at the cluster level are difficult to define. Specifically, it is difficult to define individual traits of fixed length on clusters that may be of variable size (or shape). As a result, global correlation traits tend to be either too coarse or too sparse. Thus, in early attempts to define traits at the cluster level, the rough quantifier was simply applied, which predicts all, none or most traits at the mention level defined in the mentions (or pairs of mentions) in a cluster (Culotta et al., 2007; Rahman and Ng, 2011). For example, a cluster would have the trait \"most feminine = true\" if more than half of the mentions (or pairs of mentions) in the cluster had true \"female characteristics.\""}, {"heading": "4 Learning Global Features", "text": "To get around the problems mentioned above by defining global characteristics, we propose to learn the representation of characteristics at the cluster level implicitly by identifying the state of a (partial) cluster with the hidden state of an RNN that has consumed the sequence of mentions that make up the (partial) cluster. Before providing technical details, we provide some preliminary evidence that such learned representations capture important context information by showing in Figure 1 the learned end states of all clusters in the CoNLL development theorem projected using T-SNE (van der Maaten and Hinton, 2012). Each point in the visualization represents the learned characteristics for an entity cluster and the headers of the mentions are represented for representative points. Note that the model learns to capture clusters roughly by simple distinctions such as dominant types (nominal, correct, pronominal), and subjective (but also to separate them from groups), and so on."}, {"heading": "4.1 Recurrent Neural Networks", "text": "A recurrent neural network is a parameterized nonlinear function RNN that maps an input sequence of vectors recursively to a sequence of hidden states. Let (mj) Jj = 1 be a sequence of J input vectors mj-RD and allow h0 = 0. Applying an RNN to such a sequence results in RNN (mj, hj \u2212 1; \u03b8), which is the set of parameters for the model that are divided over time. There are several types of RNN, but by far the most commonly used in processing natural language is the Long Short-Term Memory Network (LSTM) (Hochreiter and Schmidhuber, 1997), especially for language modeling (e.g. Zaremba et al. (2014) and machine translation (e.g. Sutskever et al. (2014))), and we use LSTMs in all experiments."}, {"heading": "4.2 RNNs for Cluster Features", "text": "Our main contribution will be to use RNNs to generate feature representations of entity clusters that form the basis of the global term g. Remember that we consider a cluster X (m) as a sequence of mentions (X (m) j = 1 (ordered in linear document or -der). We therefore propose to embed the state (states) of X (m) by running an RNN in order over the cluster. To run an RNN over the mentions, we need an embedding function hc to map a reference to a real vector. First, following Wiseman et al. (2015), we define a (xn): X \u2192 {0, 1} F as the default set of local indicator attributes on a mention, such as its header, its gender, and so on. (We will explain the attributes below) We then use a non-linear feature to set it to an xlinear one."}, {"heading": "5 Coreference with Global Features", "text": "We now describe how the RNN defined above is used within an end-to-end coreference system."}, {"heading": "5.1 Full Model and Training", "text": "We remember that our goal is to maximize the results of both a local ranking term and a global term based on the current clusters: arg max y1,..., yN, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n"}, {"heading": "5.2 Search", "text": "The local target requires O (n2) time, while the complete cluster problem is NP-Hard. Previous work with global characteristics has used integer linear programmers for the exact search (Chang et al., 2013; Peng et al., 2015), or beam search with (delayed) early update training for an approximate solution (Bjo \ufffd rkelund and Kuhn, 2014). In contrast, we simply use the greedy search at the test date, which also uses O (n2) times.3 The complete algorithm m3While beam search is a natural method of reducing search errors at the test date, it cannot help if the training includes a local marginal target (as in our case), as the results do not need to be calibrated via local decisions. Accordingly, we have tried to train various locally standardized versions of our model, but this algorithm requires 1 Greedy search with global RNN1: NY procedures."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Methods", "text": "We are conducting experiments on the CoNLL 2012 English shared task (Pradhan et al., 2012) using the OntoNotes corpus (Hovy et al., 2006), consisting of 3,493 documents in various ranges and formats. We are using the experimental division described in the joint task. (For all experiments we are using the Berkeley Coreference System (Durrett and Klein, 2013) to mention and calculate properties inspa and \u03c6p. Features We are using the raw BASIC + feature sets described by Wiseman et al. (2015), with the following modifications: \u2022 We are removing all features that associate a feature of antecedent with a feature of current mention, such as bi-head features. \u2022 We are adding real head features, an up-to-date speaker indicator feature and a 2-characteristic that we are performing. We are also experimenting with training approaches and model variants saying their own."}, {"heading": "6.2 Results", "text": "In Table 1, we present our main results on the CoNLL test group and compare them with other newer state systems. We see a statistically significant improvement of over 0.8 CoNLL points over the previous state of the art and the highest F1 values on all three CoNLL metrics. We now look in more detail at the impact of global features and RNNNs on performance. For these experiments, we report on MUC, B3, and CEAFe values in Table 2, as well as errors shown by mentioning them in Table 3. Table 3 further partitions errors in FL, FN, and WL categories defined in Section 5.1."}, {"heading": "6.3 Qualitative Analysis", "text": "In this section, we will look in detail at the effects of the g-term in the RNN scoring function on the two error categories that improve most under the RNN model (as shown in Table 3), namely pronominal WL errors and pronominal FL errors. We will consider an example from CoNLL development in each category where the base model MR makes a mistake, but the greedy RNN model does not. Figure 3 includes the resolution of the ambiguous pronoun \"his,\" which is shown in bold and bold in the figure. While the base model MR incorrectly predicts that \"his\" corefer correlates with the next gender history \"Justin\" - resulting in a WL error that is shown in Figure 3 as \"his,\" which is shown in bold in the figure and in bold in the figure."}, {"heading": "7 Related Work", "text": "Unstructured approaches to co-referencing are typically divided into mention pair models that classify (almost) every mention pair in a document as co-referential or non-referential (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention ranking models that select a single precursor for each anaphorical mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically differ from those that produce an accumulation of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, more recently, those who learn a latent mention of mentions (Hagon, 2010; Klein and Domi, 2010; and Domi, 2010)."}, {"heading": "8 Conclusion", "text": "We have presented a simple, state-of-the-art approach to integrating global information into an end-to-end co-referencing system that eliminates the need to define global characteristics and allows for simple (greedy) inferences."}, {"heading": "Acknowledgments", "text": "We would like to thank you for your support with a Google Research Award."}], "references": [{"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Understanding the Value of Features for Coreference Resolution", "author": ["Bengtson", "Roth2008] Eric Bengtson", "Dan Roth"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bengtson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bengtson et al\\.", "year": 2008}, {"title": "Learning structured perceptrons for coreference Resolution with Latent Antecedents and Non-local Features", "author": ["Bj\u00f6rkelund", "Kuhn2014] Anders Bj\u00f6rkelund", "Jonas Kuhn"], "venue": null, "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "End-to-end coreference resolution via hypergraph partitioning", "author": ["Cai", "Strube2010] Jie Cai", "Michael Strube"], "venue": "In 23rd International Conference on Computational Linguistics (COLING),", "citeRegEx": "Cai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2010}, {"title": "A Constrained Latent Variable Model for Coreference Resolution", "author": ["Chang et al.2013] Kai-Wei Chang", "Rajhans Samdani", "Dan Roth"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2013}, {"title": "Entity-centric coreference resolution with model stacking", "author": ["Clark", "Manning2015] Kevin Clark", "Christopher D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Clark et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2015}, {"title": "First-order Probabilistic Models for Coreference Resolution", "author": ["Culotta et al.2007] Aron Culotta", "Michael Wick", "Robert Hall", "Andrew McCallum"], "venue": "In Human Language Technology Conference of the North American Chapter of the Association of Computa-", "citeRegEx": "Culotta et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Culotta et al\\.", "year": 2007}, {"title": "Search-based structured prediction", "author": ["John Langford", "Daniel Marcu"], "venue": "Machine Learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Specialized Models and Ranking for Coreference Resolution", "author": ["Denis", "Baldridge2008] Pascal Denis", "Jason Baldridge"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Denis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Denis et al\\.", "year": 2008}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Easy Victories and Uphill Battles in Coreference Resolution", "author": ["Durrett", "Klein2013] Greg Durrett", "Dan Klein"], "venue": "In Proceedings of the 2013 Confer-", "citeRegEx": "Durrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2013}, {"title": "A Joint Model for Entity Analysis: Coreference, Typing, and Linking", "author": ["Durrett", "Klein2014] Greg Durrett", "Dan Klein"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Durrett et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2014}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Latent Structure Perceptron with Feature Induction for Unrestricted Coreference Resolution", "author": ["C\u0131\u0301cero Nogueira Dos Santos", "Ruy Luiz Milidi\u00fa"], "venue": "In Joint Conference on EMNLP and CoNLL-Shared", "citeRegEx": "Fernandes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fernandes et al\\.", "year": 2012}, {"title": "Coreference Resolution in a Modular", "author": ["Haghighi", "Klein2010] Aria Haghighi", "Dan Klein"], "venue": "Entitycentered Model. In The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Haghighi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Ontonotes: the 90% Solution", "author": ["Hovy et al.2006] Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel"], "venue": "In Proceedings of the human language technology conference of the NAACL, Companion Volume: Short Papers,", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Statistical Significance Tests for Machine Translation Evaluation", "author": ["Philipp Koehn"], "venue": "In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Koehn.,? \\Q2004\\E", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Error-driven Analysis of Challenges in Coreference Resolution", "author": ["Kummerfeld", "Dan Klein"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kummerfeld et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kummerfeld et al\\.", "year": 2013}, {"title": "rnn: Recurrent Library for Torch", "author": ["Yand Waghmare", "Sagar ad Wang", "Jin-Hwa Kim"], "venue": "arXiv preprint arXiv:1511.07889", "citeRegEx": "L\u00e9onard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "L\u00e9onard et al\\.", "year": 2015}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Li et al.2016] Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky"], "venue": "In NAACL HLT", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Latent structures for coreference resolution", "author": ["Martschat", "Strube2015] Sebastian Martschat", "Michael Strube"], "venue": null, "citeRegEx": "Martschat et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Martschat et al\\.", "year": 2015}, {"title": "Analyzing and visualizing coreference resolution errors", "author": ["Thierry G\u00f6ckel", "Michael Strube"], "venue": "In NAACL HLT,", "citeRegEx": "Martschat et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Martschat et al\\.", "year": 2015}, {"title": "Toward Conditional Models of Identity Uncertainty with Application to Proper Noun Coreference", "author": ["McCallum", "Wellner2003] Andrew McCallum", "Ben Wellner"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "McCallum et al\\.,? \\Q2003\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2003}, {"title": "Identifying Anaphoric and Non-anaphoric Noun Phrases to Improve Coreference Resolution", "author": ["Ng", "Cardie2002] Vincent Ng", "Claire Cardie"], "venue": "In Proceedings of the 19th international conference on Computational linguistics-Volume", "citeRegEx": "Ng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2002}, {"title": "A joint framework for coreference resolution and mention head detection", "author": ["Peng et al.2015] Haoruo Peng", "Kai-Wei Chang", "Dan Roth"], "venue": "In Proceedings of the 19th Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Joint unsupervised coreference resolution with markov logic", "author": ["Poon", "Domingos2008] Hoifung Poon", "Pedro M. Domingos"], "venue": "In 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Poon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2008}, {"title": "Conll-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes", "author": ["Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang"], "venue": "In Joint Conference on EMNLP and CoNLL-Shared", "citeRegEx": "Pradhan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation", "author": ["Xiaoqiang Luo", "Marta Recasens", "Eduard Hovy", "Vincent Ng", "Michael Strube"], "venue": "In Proceedings of the Association", "citeRegEx": "Pradhan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2014}, {"title": "Supervised Models for Coreference Resolution", "author": ["Rahman", "Ng2009] Altaf Rahman", "Vincent Ng"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2Volume", "citeRegEx": "Rahman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rahman et al\\.", "year": 2009}, {"title": "Narrowing the modeling gap: A cluster-ranking approach to coreference resolution", "author": ["Rahman", "Ng2011] Altaf Rahman", "Vincent Ng"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Rahman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rahman et al\\.", "year": 2011}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross et al.2011] St\u00e9phane Ross", "Geoffrey J. Gordon", "Drew Bagnell"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "A Machine Learning Approach to Coreference Resolution of Noun Phrases", "author": ["Soon et al.2001] Wee Meng Soon", "Hwee Tou Ng", "Daniel Chung Yong Lim"], "venue": null, "citeRegEx": "Soon et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Soon et al\\.", "year": 2001}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Easy-first Coreference Resolution", "author": ["Stoyanov", "Eisner2012] Veselin Stoyanov", "Jason Eisner"], "venue": "In COLING,", "citeRegEx": "Stoyanov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stoyanov et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Visualizing non-metric similarities in multiple maps", "author": ["van der Maaten", "Geoffrey E. Hinton"], "venue": "Machine Learning,", "citeRegEx": "Maaten et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2012}, {"title": "Learning anaphoricity and antecedent ranking features for coreference resolution", "author": ["Wiseman et al.2015] Sam Wiseman", "Alexander M. Rush", "Stuart M. Shieber", "Jason Weston"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Wiseman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wiseman et al\\.", "year": 2015}, {"title": "Learning Structural SVMs with Latent Variables", "author": ["Yu", "Joachims2009] Chun-Nam John Yu", "Thorsten Joachims"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Yu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2009}, {"title": "Recurrent neural network regularization. CoRR, abs/1409.2329", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 37, "context": "information that can help resolve difficult pronominal mentions, which remain a persistent source of errors for modern coreference systems (Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Wiseman et al., 2015; Martschat and Strube, 2015).", "startOffset": 139, "endOffset": 242}, {"referenceID": 6, "context": ", Bengtson and Roth (2008)), which score all pairs of mentions in a cluster, as well as with certain cluster-based models (see discussion in Culotta et al. (2007)).", "startOffset": 141, "endOffset": 163}, {"referenceID": 37, "context": "Wiseman et al. (2015) show that on the CoNLL 2012 English development set, almost 59% of mention-ranking precision errors and almost 24% of recall errors involve pronominal mentions.", "startOffset": 0, "endOffset": 22}, {"referenceID": 37, "context": "Wiseman et al. (2015) show that on the CoNLL 2012 English development set, almost 59% of mention-ranking precision errors and almost 24% of recall errors involve pronominal mentions. Martschat and Strube (2015) found a similar pattern in their comparison of mention-ranking, mention-pair, and latent-tree models.", "startOffset": 0, "endOffset": 211}, {"referenceID": 37, "context": "Furthermore, among recent, state-of-theart systems, mention-ranking systems (which are completely local) perform at least as well as their more structured counterparts (Durrett and Klein, 2014; Clark and Manning, 2015; Wiseman et al., 2015; Peng et al., 2015).", "startOffset": 168, "endOffset": 259}, {"referenceID": 25, "context": "Furthermore, among recent, state-of-theart systems, mention-ranking systems (which are completely local) perform at least as well as their more structured counterparts (Durrett and Klein, 2014; Clark and Manning, 2015; Wiseman et al., 2015; Peng et al., 2015).", "startOffset": 168, "endOffset": 259}, {"referenceID": 6, "context": "Thus, early attempts at defining cluster-level features simply applied the coarse quantifier predicates all, none, most to the mention-level features defined on the mentions (or pairs of mentions) in a cluster (Culotta et al., 2007; Rahman and Ng, 2011).", "startOffset": 210, "endOffset": 253}, {"referenceID": 38, "context": ", Zaremba et al. (2014)) and machine translation (e.", "startOffset": 2, "endOffset": 24}, {"referenceID": 35, "context": ", Sutskever et al. (2014)), and we use LSTMs in all experiments.", "startOffset": 2, "endOffset": 26}, {"referenceID": 37, "context": "First, following Wiseman et al. (2015) define \u03c6a(xn) : X \u2192 {0, 1}F as a standard set of local indicator features on a mention, such as its head word, its gender, and so on.", "startOffset": 17, "endOffset": 39}, {"referenceID": 37, "context": "We begin by defining the local model f(xn, y) with the two layer neural network of Wiseman et al. (2015), which has a specialization for the nonanaphoric case, as follows:", "startOffset": 83, "endOffset": 105}, {"referenceID": 37, "context": "where \u03c6a (mentioned above) and \u03c6p are \u201craw\u201d (that is, unconjoined) features on the context of xn and on the pairwise affinity between mentions xn and antecedent y, respectively (Wiseman et al., 2015).", "startOffset": 177, "endOffset": 199}, {"referenceID": 13, "context": "While at training time we do have oracle clusters, we do not have oracle antecedents (y)n=1, so following past work we treat the oracle antecedent as latent (Yu and Joachims, 2009; Fernandes et al., 2012; Chang et al., 2013; Durrett and Klein, 2013).", "startOffset": 157, "endOffset": 249}, {"referenceID": 4, "context": "While at training time we do have oracle clusters, we do not have oracle antecedents (y)n=1, so following past work we treat the oracle antecedent as latent (Yu and Joachims, 2009; Fernandes et al., 2012; Chang et al., 2013; Durrett and Klein, 2013).", "startOffset": 157, "endOffset": 249}, {"referenceID": 4, "context": "Past work with global features has used integer linear programming solvers for exact search (Chang et al., 2013; Peng et al., 2015), or beam search with (delayed) early update training for an approximate solution (Bj\u00f6rkelund and Kuhn, 2014).", "startOffset": 92, "endOffset": 131}, {"referenceID": 25, "context": "Past work with global features has used integer linear programming solvers for exact search (Chang et al., 2013; Peng et al., 2015), or beam search with (delayed) early update training for an approximate solution (Bj\u00f6rkelund and Kuhn, 2014).", "startOffset": 92, "endOffset": 131}, {"referenceID": 27, "context": "We run experiments on the CoNLL 2012 English shared task (Pradhan et al., 2012).", "startOffset": 57, "endOffset": 79}, {"referenceID": 16, "context": "The task uses the OntoNotes corpus (Hovy et al., 2006), consist-", "startOffset": 35, "endOffset": 54}, {"referenceID": 37, "context": "Features We use the raw BASIC+ feature sets described by Wiseman et al. (2015), with the following modifications:", "startOffset": 57, "endOffset": 79}, {"referenceID": 31, "context": "We also experimented with training approaches and model variants that expose the model to its own predictions (Daum\u00e9 III et al., 2009; Ross et al., 2011; Bengio et al., 2015), but found that these yielded a negligible performance improvement.", "startOffset": 110, "endOffset": 174}, {"referenceID": 0, "context": "We also experimented with training approaches and model variants that expose the model to its own predictions (Daum\u00e9 III et al., 2009; Ross et al., 2011; Bengio et al., 2015), but found that these yielded a negligible performance improvement.", "startOffset": 110, "endOffset": 174}, {"referenceID": 25, "context": "02 Peng et al. (2015) - - 72.", "startOffset": 3, "endOffset": 22}, {"referenceID": 25, "context": "02 Peng et al. (2015) - - 72.22 - - 60.50 - - 56.37 63.03 Wiseman et al. (2015) 76.", "startOffset": 3, "endOffset": 80}, {"referenceID": 17, "context": "05 under the bootstrap resample test (Koehn, 2004)) compared with Wiseman et al.", "startOffset": 37, "endOffset": 50}, {"referenceID": 24, "context": "We compare against recent state of the art systems, including (in order) Bjorkelund and Kuhn (2014), Martschat and Strube (2015), Clark and Manning (2015), Peng et al. (2015), and Wiseman et al.", "startOffset": 156, "endOffset": 175}, {"referenceID": 24, "context": "We compare against recent state of the art systems, including (in order) Bjorkelund and Kuhn (2014), Martschat and Strube (2015), Clark and Manning (2015), Peng et al. (2015), and Wiseman et al. (2015). F1 gains are significant (p < 0.", "startOffset": 156, "endOffset": 202}, {"referenceID": 17, "context": "05 under the bootstrap resample test (Koehn, 2004)) compared with Wiseman et al. (2015) for all metrics.", "startOffset": 38, "endOffset": 88}, {"referenceID": 9, "context": "For training, we use document-size minibatches, which allows for efficient pre-computation of RNN states, and we minimize the loss described in Section 5 with AdaGrad (Duchi et al., 2011) (after clipping LSTM gradients to lie (elementwise) in (\u221210, 10)).", "startOffset": 167, "endOffset": 187}, {"referenceID": 19, "context": "We use a single-layer LSTM (without \u201cpeep-hole\u201d connections), as implemented in the element-rnn library (L\u00e9onard et al., 2015).", "startOffset": 104, "endOffset": 126}, {"referenceID": 33, "context": "For regularization, we apply Dropout (Srivastava et al., 2014) with a rate of 0.", "startOffset": 37, "endOffset": 62}, {"referenceID": 35, "context": "Following Wiseman et al. (2015) we use the costweights \u03b1 = \u30080.", "startOffset": 10, "endOffset": 32}, {"referenceID": 37, "context": "Finally, for baselines we consider the mention-ranking system (MR) of Wiseman et al. (2015) using our updated feature-set, as well as a non-local baseline with oracle history (Avg, OH), which averages the representations hc(xj) for all xj \u2208X(m), rather than feed them through an RNN; errors are still backpropagated through the hc representations during learning.", "startOffset": 70, "endOffset": 92}, {"referenceID": 20, "context": "This visualization resembles the \u201csaliency\u201d technique of Li et al. (2016), and it attempts to gives a sense of the contribution of a (preceding) cluster in the calculation of the NA score.", "startOffset": 57, "endOffset": 74}, {"referenceID": 32, "context": "Unstructured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al.", "startOffset": 164, "endOffset": 229}, {"referenceID": 6, "context": "Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently,", "startOffset": 90, "endOffset": 240}, {"referenceID": 13, "context": "those that learn a latent tree of mentions (Fernandes et al., 2012; Bj\u00f6rkelund and Kuhn, 2014; Martschat and Strube, 2015).", "startOffset": 43, "endOffset": 122}, {"referenceID": 12, "context": "tory of) the state of a cluster is apparently novel, though it bears some similarity to the recent work of Dyer et al. (2015), who use LSTMs to embed the state of a transition based parser\u2019s stack.", "startOffset": 107, "endOffset": 126}], "year": 2016, "abstractText": "There is compelling evidence that coreference prediction would benefit from modeling global information about entity-clusters. Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difficulty of crafting informative clusterlevel features. We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters directly from their mentions. We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coreference system that outperforms the state of the art without requiring any additional search.", "creator": "LaTeX with hyperref package"}}}