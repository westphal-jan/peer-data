{"id": "1604.03357", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "Improving sentence compression by learning to predict gaze", "abstract": "We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches.", "histories": [["v1", "Tue, 12 Apr 2016 11:57:05 GMT  (852kb,D)", "http://arxiv.org/abs/1604.03357v1", "NAACL 2016. Received Best Short Paper Award"]], "COMMENTS": "NAACL 2016. Received Best Short Paper Award", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sigrid klerke", "yoav goldberg", "anders s\u00f8gaard"], "accepted": true, "id": "1604.03357"}, "pdf": {"name": "1604.03357.pdf", "metadata": {"source": "CRF", "title": "Improving sentence compression by learning to predict gaze", "authors": ["Sigrid Klerke"], "emails": ["skl@hum.ku.dk", "yoav.goldberg@gmail.com", "soegaard@hum.ku.dk"], "sections": [{"heading": "1 Introduction", "text": "Sentence compression is a basic operation in text simplification which has the potential to improve statistical machine translation and automatic summarization (Berg-Kirkpatrick et al., 2011; Klerke et al., 2015), as well as as helping poor readers in need of assistive technologies (Canning et al., 2000). This work proposes the use of eye-tracking records to improve sentence compression for text simplification systems and is motivated by two observations: (i) Sentence compression is the task of automatic making sets easier to process by shorting them. (ii) Eye-tracking measures as first pass reading time and time spent on regressions, i.e., during second and later passes over the text, are known to correlate with recorded text difficulty (Rayner et al., 2012).These two observations recently lead to Klerke et al. (2015) to suggest the use of eye-tracking measures as metrics in text simplification."}, {"heading": "2 Gaze during reading", "text": "Readers spend longer fixating on rare words that are semantically ambiguous, and on words that are able to identify with simpler words in sentence simplification, but it is not clear whether they are words that are removed in the context of sentence compression. Demberg and Keller (2008) show that syntactical complexity is also an important predictor of reading time. Phrases that are often removed in sentence compression are like parenthetic quantifiers, often associated with non-local dependencies."}, {"heading": "3 Sentence compression using multi-task deep bi-LSTMs", "text": "This year, it is so far that it only takes one year to get there like never before."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Gaze data", "text": "We use the Dundee Corpus (Kennedy et al., 2003) as an eye-tracking corpus with tokenization and similar metrics to the Dundee Treebank (Barrett et al., 2015). The corpus contains eye-tracking records of ten native English speakers who read 20 newspaper articles from The Independent. We use data from nine subjects for training and one for development. We do not evaluate the eye prediction because the task is only included as a way to regulate the compression model."}, {"heading": "4.2 Compression data", "text": "We use three different set compression data sets, ZIFF-DAVIS (Knight and Marcu, 2002), BROADCAST (Clarke and Lapata, 2006), and the publicly available subset of GOOGLE (Filippova et al., 2015), the first two consisting of manually compressed newswire text in English, while the third is heuristically composed of pairs of newswire headlines and first sentences, resulting in the most aggressive compressions, as illustrated in Table 1. We present the characteristics of the data set in Table 2. We use the data sets published by the authors and do not apply any additional pre-processing. CCG supertagging data comes from CCGbank, 1, and we use sections 0-18 for training and section 19 for development."}, {"heading": "4.3 Baselines and system", "text": "Both the baseline and our systems are three-layer Bi-LSTM models trained for 30 iterations with pre-trained (SENNA) embedding, the input and hidden levels are 50 dimensions, and at the output level we predict sequences of two labels indicating whether the labeled word should be deleted or not. Our baseline (BASELINE-LSTM) is a multi-task Learning1http: / / groups.inf.ed.ac.uk / ccg / bi-LSTM, which predicts both CCG supertags and sentence compression (word deletion) at the outer level. Our first extension is MULTITASK-LSTM, which predicts CCG supertags, sentence compression and reading actions at the outer level."}, {"heading": "4.4 Results and discussion", "text": "Our results are presented in Table 3. We note that across all three sets of data, including all three of BROADCAST's annotations, viewing characteristics lead to improvements over our three-layer Bi-LSTM base models. Furthermore, CASCADED-LSTM is consistently better than MULTITASK-LSTM. Our models fully compete with state-of-the-art models. Thus, the best model in Elming et al. (2013) reaches 0.7207 in ZIFF-DAVIS, Clarke and Lapata (2008) 0.7509 in BROADCAST, 2 and the LSTM model in Filippova et al. (2015) reaches 0.80 in GOOGLE with much more training data. The high numbers on the small subset of GOOGLE reflect that news headlines tend to have a fairly predictable relationship with 2On a \"randomly selected\" annotator; unfortunately, they do not say which. James Clarke (pc) does not include the first architectural characteristics."}, {"heading": "Acknowledgments", "text": "Yoav Goldberg received the support of the Israeli scholarship No. 1555 / 15. Anders S\u00f8gaard received the support of the ERC Starting Grant No. 313695. Thanks to Joachim Bingel and Maria Barrett for the preparation of the data and for helpful discussions as well as to the anonymous reviewers for their suggestions for improvement."}], "references": [{"title": "The dundee treebank", "author": ["\u017deljko Agi\u0107", "Anders S\u00f8gaard"], "venue": "In The 14th International Workshop on Treebanks and Linguistic Theories (TLT", "citeRegEx": "Barrett et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barrett et al\\.", "year": 2015}, {"title": "Jointly learning to extract and compress", "author": ["Dan Gillick", "Dan Klein"], "venue": "In Proceedings of ACL", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2011}, {"title": "Cohesive generation of syntactically simplified newspaper", "author": ["Canning et al.2000] Y. Canning", "J. Tait", "J. Archibald", "R. Crawley"], "venue": null, "citeRegEx": "Canning et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Canning et al\\.", "year": 2000}, {"title": "Multitask learning: a knowledge-based source of inductive bias", "author": ["Rich Caruana"], "venue": "In ICML", "citeRegEx": "Caruana.,? \\Q1993\\E", "shortCiteRegEx": "Caruana.", "year": 1993}, {"title": "Constraint-based sentence compression an integer programming approach", "author": ["Clarke", "Lapata2006] James Clarke", "Mirella Lapata"], "venue": "In COLING", "citeRegEx": "Clarke et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Clarke et al\\.", "year": 2006}, {"title": "Global inference for sentence compression: An integer linear programming approach", "author": ["Clarke", "Lapata2008] James Clarke", "Mirella Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Clarke et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Clarke et al\\.", "year": 2008}, {"title": "Sentence compression beyond word deletion", "author": ["Cohn", "Lapata2008] Trevor Cohn", "Mirella Lapata"], "venue": "In COLING", "citeRegEx": "Cohn et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2008}, {"title": "Sentence compression as tree transduction", "author": ["Cohn", "Lapata2009] Trevor Cohn", "Mirella Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cohn et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2009}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Data from eye-tracking corpora as evidence for theories of syntactic processing", "author": ["Demberg", "Keller2008] Vera Demberg", "Frank Keller"], "venue": null, "citeRegEx": "Demberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Demberg et al\\.", "year": 2008}, {"title": "Downstream effects of tree-to-dependency conversions", "author": ["Elming et al.2013] Jakob Elming", "Anders Johannsen", "Sigrid Klerke", "Emanuele Lapponi", "H\u00e9ctor Mart\u0131\u0301nez Alonso", "Anders S\u00f8gaard"], "venue": null, "citeRegEx": "Elming et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Elming et al\\.", "year": 2013}, {"title": "Dependency tree based sentence compression", "author": ["Filippova", "Strube2008] Katja Filippova", "Michael Strube"], "venue": "In Proceedings of the Fifth International Natural Language Generation Conference", "citeRegEx": "Filippova et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2008}, {"title": "Sentence compression by deletion with LSTMs", "author": ["Enrique Alfonseca", "Carlos A Colmenares", "Lukasz Kaiser", "Oriol Vinyals"], "venue": null, "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bidirectional LSTM-CRF models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Processing of finnish compound words in reading. Reading as a perceptual process, pages 65\u201387", "author": ["Hy\u00f6n\u00e4", "Pollatsek2000] Jukka Hy\u00f6n\u00e4", "Alexander Pollatsek"], "venue": null, "citeRegEx": "Hy\u00f6n\u00e4 et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hy\u00f6n\u00e4 et al\\.", "year": 2000}, {"title": "The dundee corpus", "author": ["Kennedy et al.2003] Alan Kennedy", "Robin Hill", "Jo\u00ebl Pynte"], "venue": "ECEM", "citeRegEx": "Kennedy et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kennedy et al\\.", "year": 2003}, {"title": "Reading metrics for estimating task efficiency with mt output", "author": ["Klerke et al.2015] Sigrid Klerke", "Sheila Castilho", "Maria Barrett", "Anders S\u00f8gaard"], "venue": "In EMNLP Workshop on Cognitive Aspects of Computational Language Learning", "citeRegEx": "Klerke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Klerke et al\\.", "year": 2015}, {"title": "Summarization beyond sentence extraction: a probabilistic approach to sentence compression", "author": ["Knight", "Marcu2002] Kevin Knight", "Daniel Marcu"], "venue": "Artificial Intelligence,", "citeRegEx": "Knight et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Knight et al\\.", "year": 2002}, {"title": "Fine-grained opinion mining with recurrent neural networks and word embeddings", "author": ["Liu et al.2015] Pengfei Liu", "Shafiq Joty", "Helen Meng"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Discriminative sentence compression with soft syntactic evidence", "author": ["Ryan T McDonald"], "venue": "In EACL", "citeRegEx": "McDonald.,? \\Q2006\\E", "shortCiteRegEx": "McDonald.", "year": 2006}, {"title": "Discriminative sentence compression with conditional random fields", "author": ["Tadashi Nomoto"], "venue": "Information Processing and Management: an International Journal,", "citeRegEx": "Nomoto.,? \\Q2007\\E", "shortCiteRegEx": "Nomoto.", "year": 2007}, {"title": "Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar", "author": ["Tracy H King", "Richard Crouch", "Annie Zaenen"], "venue": null, "citeRegEx": "Riezler et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2003}, {"title": "Learning to simplify sentences with quasi-synchronous grammar and integer programming", "author": ["Woodsend", "Lapata2011] Kristian Woodsend", "Mirella Lapata"], "venue": null, "citeRegEx": "Woodsend et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2011}, {"title": "End-toend learning of semantic role labeling using recurrent neural networks", "author": ["Zhou", "Xu2015] Jie Zhou", "Wei Xu"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Sentence compression is a basic operation in text simplification which has the potential to improve statistical machine translation and automatic summarization (Berg-Kirkpatrick et al., 2011; Klerke et al., 2015), as well as helping poor readers in need of assistive technologies (Canning et al.", "startOffset": 160, "endOffset": 212}, {"referenceID": 17, "context": "Sentence compression is a basic operation in text simplification which has the potential to improve statistical machine translation and automatic summarization (Berg-Kirkpatrick et al., 2011; Klerke et al., 2015), as well as helping poor readers in need of assistive technologies (Canning et al.", "startOffset": 160, "endOffset": 212}, {"referenceID": 2, "context": ", 2015), as well as helping poor readers in need of assistive technologies (Canning et al., 2000).", "startOffset": 75, "endOffset": 97}, {"referenceID": 10, "context": "Several approaches to sentence compression have been proposed, from noisy channel models (Knight and Marcu, 2002) over conditional random fields (Elming et al., 2013) to tree-to-tree machine translation models (Woodsend and Lapata, 2011).", "startOffset": 145, "endOffset": 166}, {"referenceID": 14, "context": "These two observations recently lead Klerke et al. (2015) to suggest using eye-tracking measures as metrics in text simplification.", "startOffset": 37, "endOffset": 58}, {"referenceID": 10, "context": "Several approaches to sentence compression have been proposed, from noisy channel models (Knight and Marcu, 2002) over conditional random fields (Elming et al., 2013) to tree-to-tree machine translation models (Woodsend and Lapata, 2011). More recently, Filippova et al. (2015) successfully used LSTMs for sentence compression on a large scale parallel dataset.", "startOffset": 146, "endOffset": 278}, {"referenceID": 10, "context": "Several approaches to sentence compression have been proposed, from noisy channel models (Knight and Marcu, 2002) over conditional random fields (Elming et al., 2013) to tree-to-tree machine translation models (Woodsend and Lapata, 2011). More recently, Filippova et al. (2015) successfully used LSTMs for sentence compression on a large scale parallel dataset. We do not review the literature here, and only compare to Filippova et al. (2015).", "startOffset": 146, "endOffset": 444}, {"referenceID": 22, "context": "Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees (Riezler et al., 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syntactic information in their model (McDonald, 2006; Clarke and Lapata, 2008).", "startOffset": 117, "endOffset": 227}, {"referenceID": 21, "context": "Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees (Riezler et al., 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syntactic information in their model (McDonald, 2006; Clarke and Lapata, 2008).", "startOffset": 117, "endOffset": 227}, {"referenceID": 20, "context": ", 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syntactic information in their model (McDonald, 2006; Clarke and Lapata, 2008).", "startOffset": 153, "endOffset": 194}, {"referenceID": 11, "context": "Recently, however, Filippova et al. (2015) presented an approach to sentence compression using LSTMs with word embeddings, but without syntactic features.", "startOffset": 19, "endOffset": 43}, {"referenceID": 19, "context": "Bi-LSTMs have already been used for finegrained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al.", "startOffset": 67, "endOffset": 85}, {"referenceID": 14, "context": ", 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015).", "startOffset": 28, "endOffset": 48}, {"referenceID": 3, "context": "Both MULTI-TASK-LSTM and CASCADEDLSTM do multi-task learning (Caruana, 1993).", "startOffset": 61, "endOffset": 76}, {"referenceID": 3, "context": "Both MULTI-TASK-LSTM and CASCADEDLSTM do multi-task learning (Caruana, 1993). In multi-task learning, the induction of a model for one task is used as a regularizer on the induction of a model for another task. Caruana (1993) did multitask learning by doing parameter sharing across several deep networks, letting them share hidden layers; a technique also used by Collobert et al.", "startOffset": 62, "endOffset": 226}, {"referenceID": 3, "context": "Both MULTI-TASK-LSTM and CASCADEDLSTM do multi-task learning (Caruana, 1993). In multi-task learning, the induction of a model for one task is used as a regularizer on the induction of a model for another task. Caruana (1993) did multitask learning by doing parameter sharing across several deep networks, letting them share hidden layers; a technique also used by Collobert et al. (2011) for various NLP tasks.", "startOffset": 62, "endOffset": 389}, {"referenceID": 16, "context": "We use the Dundee Corpus (Kennedy et al., 2003) as our eye-tracking corpus with tokenization and measures similar to the Dundee Treebank (Barrett et al.", "startOffset": 25, "endOffset": 47}, {"referenceID": 0, "context": ", 2003) as our eye-tracking corpus with tokenization and measures similar to the Dundee Treebank (Barrett et al., 2015).", "startOffset": 97, "endOffset": 119}, {"referenceID": 12, "context": "We use three different sentence compression datasets, ZIFF-DAVIS (Knight and Marcu, 2002), BROADCAST (Clarke and Lapata, 2006), and the publically available subset of GOOGLE (Filippova et al., 2015).", "startOffset": 174, "endOffset": 198}, {"referenceID": 10, "context": "For example, the best model in Elming et al. (2013) achieves 0.", "startOffset": 31, "endOffset": 52}, {"referenceID": 10, "context": "For example, the best model in Elming et al. (2013) achieves 0.7207 on ZIFF-DAVIS, Clarke and Lapata (2008) achieves 0.", "startOffset": 31, "endOffset": 108}, {"referenceID": 10, "context": "For example, the best model in Elming et al. (2013) achieves 0.7207 on ZIFF-DAVIS, Clarke and Lapata (2008) achieves 0.7509 on BROADCAST,2 and the LSTM model in Filippova et al. (2015) achieves 0.", "startOffset": 31, "endOffset": 185}], "year": 2016, "abstractText": "We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}