{"id": "1604.03853", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2016", "title": "Hierarchical Compound Poisson Factorization", "abstract": "Non-negative matrix factorization models based on a hierarchical Gamma-Poisson structure capture user and item behavior effectively in extremely sparse data sets, making them the ideal choice for collaborative filtering applications. Hierarchical Poisson factorization (HPF) in particular has proved successful for scalable recommendation systems with extreme sparsity. HPF, however, suffers from a tight coupling of sparsity model (absence of a rating) and response model (the value of the rating), which limits the expressiveness of the latter. Here, we introduce hierarchical compound Poisson factorization (HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF to high-dimensional extremely sparse matrices. More importantly, HCPF decouples the sparsity model from the response model, allowing us to choose the most suitable distribution for the response. HCPF can capture binary, non-negative discrete, non-negative continuous, and zero-inflated continuous responses. We compare HCPF with HPF on nine discrete and three continuous data sets and conclude that HCPF captures the relationship between sparsity and response better than HPF.", "histories": [["v1", "Wed, 13 Apr 2016 16:12:01 GMT  (1201kb,D)", "https://arxiv.org/abs/1604.03853v1", "Under Review for ICML 2016"], ["v2", "Thu, 26 May 2016 11:09:19 GMT  (1201kb,D)", "http://arxiv.org/abs/1604.03853v2", "Will appear on Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&amp;CP volume 48"]], "COMMENTS": "Under Review for ICML 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["mehmet emin basbug", "barbara e engelhardt"], "accepted": true, "id": "1604.03853"}, "pdf": {"name": "1604.03853.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Compound Poisson Factorization", "authors": ["Mehmet E. Basbug", "Barbara E. Engelhardt"], "emails": ["MEHMETBASBUG@YAHOO.COM", "BEE@PRINCETON.EDU"], "sections": [{"heading": "1. Introduction", "text": "The idea behind NMF is that the contributions of each feature to a factor are not negative. Although the motivation behind this choice has roots in cerebral representations of objects, the non-negativity has great appeal in applications such as collaborative filtering (Gopalet classification).We have been a central subject in statistics since the introduction of primary component analysis (PCA), classification (PCA).The goal is to embed data in a lower dimensional space with minimal loss of information.The dimensional reduction aspect of matrix factorization is increasingly important in exploratory data analysis as the dimensionality of the data has exploded.The approach of the 33rd International Conference on Machine Factorization (NMF), was first developed for factorizing matrices for face recognition (Lee & Seung, 1999).The idea behind NMF is that the contributions of each feature to a factor are not negative. Although the motivation behind this choice of objectives does not have great attractiveness in representations such as representations."}, {"heading": "2. Exponential Dispersion Models", "text": "Additive exponential dispersion models (EDMs) are a generalization of the natural exponential family, in which the unequal dispersion parameter scales the log partition function (Jorgensen, 1997). We first give a formal definition of additive EDM and introduce seven useful members (Table 1). Definition 1. A family of distributions F\u044b = {p (\u0432, \u03b8) | \u03b8 = dom (\u0432) R, \u0432, \u0432 R +} is called additive exponential dispersion parameter (Table 1). Definition 1. A family of distribution models is exp (x, \u03b8) = exp (xi). The sum of additive EDMs with common natural parameter and base log partition is the natural parameter."}, {"heading": "3. Compound Poisson Distributions", "text": "We start with the general definition of a degenerated distribution to zero and discuss the distribution to zero."}, {"heading": "4. Hierarchical Compound Poisson Factorization (HCPF)", "text": "Next, we describe the generative process for HCPF and the Gamma-Poisson structure. (...) We explain the intuition behind the choices of HCPF (...). (...) We then put the stotastic derivative possibilities for HCPF. (...) We can put the generative model of HCPF with the element distribution (...) at the center. (...) We can put the number of users and items (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (..."}, {"heading": "5. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Data sets for collaborative filtering", "text": "We performed matrix factorization on 12 different datasets with varying degrees of scarcity, response characteristics, and size (Table 2), including Amazon's Fine Food Ratings (McAuley & Leskovec, 2013), Movielens (Harper & Konstan, 2015), Netflix (Bell & Koren, 2007), and Yelp, where the responses are 1 to 5. The only exception is Movielens, where the maximum rating is 10. Social media datasets include Merck (Ma et al., 2015), where the response is the number of likes, not a negative holistic value. Commercial datasets include Bestbuy, where the response is the number of user visits to a product page. Biochemical datasets include Merck (Ma et al., 2015), where the response is the number of likes, not a negative holistic value. Commercial datasets include the number of users on a product page, where the answer is the number of the buy."}, {"heading": "5.2. Experimental setup", "text": "When calculating the test and validation log probability, the log probability of missing entries is adjusted to reflect the actual savings ratio. Test log probability of missing (LNM) and not missing entries (LNM) and the test log probability of not missing entries are then calculated so that it is not missing (LCNM). Test log probability of missing (LNM) and not missing entries (LNM). Test log probability of missing entries is calculated so that the analysis (LCNM) is not missing. Test log probability of missing (LNM) entries of missing (LNM) and not missing entries (LNM)."}, {"heading": "5.3. Overall performance", "text": "In this analysis, we quantify how well these models capture both thrift and reaction behavior in ultra-sparse matrices."}, {"heading": "5.4. Response model evaluated using test log likelihood", "text": "In this section, we explore which model most accurately captures the answer. In a movie rating data set, the question is posed: \"Can we predict what rating a user would give a movie because we know that he is rating that movie?\" In Table 4, we report a better response to the full matrix and test protocol, which is focused only on the non-missing entities. First, we point out that the HPF training is based only on the non-missing entities, rather than the HPF training on the full matrix. The only exception is that the non-missing test protocols are likely."}, {"heading": "5.5. Sparsity model evaluated using AUC", "text": "In a data set for movie ratings or a data set for purchase, an important question is: \"Can we predict whether a user would rate a particular movie or buy a particular product?\" To understand the quality of our performance in this task, we evaluated the sparsity model separately by calculating the area below the ROC curve (AUC), adjusting all HCPF models to the binarized full matrix by scanning the full matrix and the HPF. To calculate the AUC, the true designation is whether the data is missing (0) or not missing (1), and we used the estimated probability of a non-missing entry, Pr (X + 6 = 0), as a model prediction. As discussed in Section 5.3, when we adjust HPF to the full matrix, we compromise performance in terms of thrift and response. HCPF, on the other hand, enjoys the decoupling effect with simultaneous expectation (see Eq)."}, {"heading": "6. Conclusion", "text": "Inspired by the convergence theorem, we are introducing HCPF. Similar to HPF, HCPF has the favorable gamma-poisson structure to model long-term user and article activity. Unlike HPF, HCPF is able to model binary, non-negative discrete, non-negative continuous, and non-inflated continuous data. More importantly, HCPF decouples the sparsity and response models, allowing us to determine the most appropriate distribution for the non-missing response entries. We show that this decoupling effect dramatically improves test log probability when compared to HPF on high-dimensional, extremely sparse hierarchies. HCF also shows the better distribution for the non-missing response entries."}, {"heading": "Acknowledgements", "text": "BEE was funded by NIH R00 HG006265, NIH R01 MH101822 and a Sloan Faculty Fellowship. MEB was partially funded by the Princeton Innovation J. Insley Blair Pyne Fund Award. We would like to thank Robert E. Schapire for valuable discussions."}], "references": [{"title": "Lessons from the netflix prize challenge", "author": ["R.M. Bell", "Y. Koren"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Bell and Koren,? \\Q2007\\E", "shortCiteRegEx": "Bell and Koren", "year": 2007}, {"title": "The million song dataset", "author": ["T. Bertin-Mahieux", "D.P.W. Ellis", "B. Whitman", "P. Lamere"], "venue": "In International Society for Music Information Retrieval Conference,", "citeRegEx": "Bertin.Mahieux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bertin.Mahieux et al\\.", "year": 2011}, {"title": "Gap: a factor model for discrete data", "author": ["J. Canny"], "venue": "In ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Canny,? \\Q2004\\E", "shortCiteRegEx": "Canny", "year": 2004}, {"title": "Bayesian inference for nonnegative matrix factorisation models", "author": ["A.T. Cemgil"], "venue": "Computational Intelligence and Neuroscience,", "citeRegEx": "Cemgil,? \\Q2009\\E", "shortCiteRegEx": "Cemgil", "year": 2009}, {"title": "Algorithms for nonnegative matrix factorization with the \u03b2-divergence", "author": ["C. F\u00e9votte", "J. Idier"], "venue": "Neural Computation,", "citeRegEx": "F\u00e9votte and Idier,? \\Q2011\\E", "shortCiteRegEx": "F\u00e9votte and Idier", "year": 2011}, {"title": "Nonnegative matrix factorization with the itakura-saito divergence: With application to music analysis", "author": ["C. F\u00e9votte", "N. Bertin", "J.L. Durrieu"], "venue": "Neural Computation,", "citeRegEx": "F\u00e9votte et al\\.,? \\Q2009\\E", "shortCiteRegEx": "F\u00e9votte et al\\.", "year": 2009}, {"title": "Scalable recommendation with poisson factorization", "author": ["P. Gopalan", "J.M. Hofman", "D.M. Blei"], "venue": "arXiv preprint arXiv:1311.1704,", "citeRegEx": "Gopalan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2013}, {"title": "The movielens datasets: History and context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Transactions on Interactive Intelligent Systems,", "citeRegEx": "Harper and Konstan,? \\Q2015\\E", "shortCiteRegEx": "Harper and Konstan", "year": 2015}, {"title": "Stochastic variational inference", "author": ["M.D. Hoffman", "D.M. Blei", "C. Wang", "J. Paisley"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "The theory of dispersion models", "author": ["B. Jorgensen"], "venue": "CRC Press,", "citeRegEx": "Jorgensen,? \\Q1997\\E", "shortCiteRegEx": "Jorgensen", "year": 1997}, {"title": "Transcriptome and genome sequencing uncovers functional variation in humans", "author": ["T. Lappalainen", "M. Sammeth", "M.R. Friedlnder", "P. ACt Hoen", "J. Monlong", "M.A. Rivas", "M. GonzlezPorta", "N. Kurbatova", "T. Griebel", "P.G. Ferreira", "M. Barann"], "venue": "Nature,", "citeRegEx": "Lappalainen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lappalainen et al\\.", "year": 2013}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": null, "citeRegEx": "Lee and Seung,? \\Q1999\\E", "shortCiteRegEx": "Lee and Seung", "year": 1999}, {"title": "Statistical analysis with missing data", "author": ["R.J.A. Little", "D.B. Rubin"], "venue": null, "citeRegEx": "Little and Rubin,? \\Q2014\\E", "shortCiteRegEx": "Little and Rubin", "year": 2014}, {"title": "Probabilistic factor models for web site recommendation", "author": ["H. Ma", "C. Liu", "I. King", "M.R. Lyu"], "venue": "In ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Ma et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2011}, {"title": "Deep neural nets as a method for quantitative structure\u2013activity relationships", "author": ["J. Ma", "R.P. Sheridan", "A. Liaw", "G.E. Dahl", "V. Svetnik"], "venue": "Journal of Chemical Information and Modeling,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Collaborative filtering and the missing at random assumption", "author": ["B. Marlin", "R.S. Zemel", "S. Roweis", "M. Slaney"], "venue": "arXiv preprint arXiv:1206.5267,", "citeRegEx": "Marlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Marlin et al\\.", "year": 2012}, {"title": "Collaborative prediction and ranking with non-random missing data", "author": ["B.M. Marlin", "R.S. Zemel"], "venue": "In ACM Conference on Recommender Systems,", "citeRegEx": "Marlin and Zemel,? \\Q2009\\E", "shortCiteRegEx": "Marlin and Zemel", "year": 2009}, {"title": "From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews", "author": ["J.J. McAuley", "J. Leskovec"], "venue": "In International Conference on World Wide Web,", "citeRegEx": "McAuley and Leskovec,? \\Q2013\\E", "shortCiteRegEx": "McAuley and Leskovec", "year": 2013}, {"title": "The tencent dataset and kddcup12", "author": ["Y. Niu", "Y. Wang", "G. Sun", "A. Yue", "B. Dalessandro", "C. Perlich", "B. Hamner"], "venue": "In KDD-Cup Workshop,", "citeRegEx": "Niu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2012}, {"title": "Liii. on lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science,", "citeRegEx": "Pearson,? \\Q1901\\E", "shortCiteRegEx": "Pearson", "year": 1901}, {"title": "Shrink globally, act locally: Sparse bayesian regularization and prediction", "author": ["Polson", "N. G", "J.G. Scott"], "venue": "Bayesian Statistics,", "citeRegEx": "Polson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Polson et al\\.", "year": 2010}, {"title": "Learning the beta-divergence in tweedie compound poisson matrix factorization models", "author": ["U. Simsekli", "A.T. Cemgil", "Y.K. Yilmaz"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Simsekli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simsekli et al\\.", "year": 2013}, {"title": "On the sum of independent zero-truncated Poisson random variables", "author": ["J. Springael", "I. Van Nieuwenhuyse"], "venue": "University of Antwerp, Faculty of Applied Economics,", "citeRegEx": "Springael and Nieuwenhuyse,? \\Q2006\\E", "shortCiteRegEx": "Springael and Nieuwenhuyse", "year": 2006}, {"title": "Document clustering based on non-negative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "In ACM SIGIR Conference on Research and Development in Informaion Retrieval,", "citeRegEx": "Xu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 19, "context": "Matrix factorization has been a central subject in statistics since the introduction of principal component analysis (PCA) (Pearson, 1901).", "startOffset": 123, "endOffset": 138}, {"referenceID": 6, "context": "Although the motivation behind this choice has roots in cerebral representations of objects, non-negativeness has found great appeal in applications such as collaborative filtering (Gopalan et al., 2013), document classification (Xu et al.", "startOffset": 181, "endOffset": 203}, {"referenceID": 23, "context": ", 2013), document classification (Xu et al., 2003), and signal processing (F\u00e9votte et al.", "startOffset": 33, "endOffset": 50}, {"referenceID": 5, "context": ", 2003), and signal processing (F\u00e9votte et al., 2009).", "startOffset": 31, "endOffset": 53}, {"referenceID": 15, "context": "The first one assumes that entries are missing at random; that is, we observe a uniformly sampled subset of the data (Marlin et al., 2012).", "startOffset": 117, "endOffset": 138}, {"referenceID": 3, "context": "Recent work showed that the NMF objective function is equivalent to a factorized Poisson likelihood (Cemgil, 2009).", "startOffset": 100, "endOffset": 114}, {"referenceID": 6, "context": "The authors proposed a Bayesian treatment of the Poisson model with Gamma conjugate priors on the latent factors, laying the foundation for hierarchical Poisson factorization (HPF) (Gopalan et al., 2013).", "startOffset": 181, "endOffset": 203}, {"referenceID": 2, "context": "The Gamma-Poisson structure is also used in earlier work for matrix factorization because of its favorable behavior (Canny, 2004; Ma et al., 2011).", "startOffset": 116, "endOffset": 146}, {"referenceID": 13, "context": "The Gamma-Poisson structure is also used in earlier work for matrix factorization because of its favorable behavior (Canny, 2004; Ma et al., 2011).", "startOffset": 116, "endOffset": 146}, {"referenceID": 5, "context": "An extension of the Poisson factorization to non-discrete data using data augmentation has been considered (F\u00e9votte et al., 2009; F\u00e9votte & Idier, 2011).", "startOffset": 107, "endOffset": 152}, {"referenceID": 21, "context": "Along the similar lines, the connection between beta divergences and compound Poisson Gamma distribution is exploited to develop a non-negative matrix factorization model for sparse positive data (Simsekli et al., 2013).", "startOffset": 196, "endOffset": 219}, {"referenceID": 6, "context": "More recent work introduced a stochastic variational inference algorithm for scalable collaborative filtering using HPF (Gopalan et al., 2013).", "startOffset": 120, "endOffset": 142}, {"referenceID": 6, "context": "For a collaborative filtering problem, HPF treats missing entries as true zero responses when applied to both missing and non-missing entries (Gopalan et al., 2013).", "startOffset": 142, "endOffset": 164}, {"referenceID": 6, "context": ", missing data and responses together), one might binarize the data to improve performance of the HPF (Gopalan et al., 2013).", "startOffset": 102, "endOffset": 124}, {"referenceID": 6, "context": "Section 4 describes the generative model for hierarchical compound Poisson factorization (HCPF) and the mean field stochastic variational inference (SVI) algorithm for HCPF, which allows us to fit HCPF to data sets with millions of rows and columns quickly (Gopalan et al., 2013).", "startOffset": 257, "endOffset": 279}, {"referenceID": 9, "context": "Additive exponential dispersion models (EDMs) are a generalization of the natural exponential family where the nonzero dispersion parameter scales the log-partition function (Jorgensen, 1997).", "startOffset": 174, "endOffset": 191}, {"referenceID": 9, "context": "Theorem 1 (Jorgensen, 1997).", "startOffset": 10, "endOffset": 27}, {"referenceID": 3, "context": "We leverage the fact that the contributions of Poisson factors can be written as a multinomial distribution (Cemgil, 2009).", "startOffset": 108, "endOffset": 122}, {"referenceID": 8, "context": "Using this, we can write out the stochastic variational inference algorithm for HCPF (Hoffman et al., 2013), where \u03c4 and \u03be are the learning rate delay and learning rate power, and \u03c4 > 0 and 0.", "startOffset": 85, "endOffset": 107}, {"referenceID": 18, "context": "The social media data sets include wordpress and tencent (Niu et al., 2012), where the response is the number of likes, a non-negative integer value.", "startOffset": 57, "endOffset": 75}, {"referenceID": 14, "context": "istry data sets include merck (Ma et al., 2015), which captures molecules (users) and chemical characteristics (items) where the response is the chemical activity.", "startOffset": 30, "endOffset": 47}, {"referenceID": 1, "context": "In echonest (Bertin-Mahieux et al., 2011), the response is the number of times a user listened a song.", "startOffset": 12, "endOffset": 41}, {"referenceID": 10, "context": "The genomics data set geuvadis includes genes and individuals, where the response is the gene expression level for a user of a gene (Lappalainen et al., 2013).", "startOffset": 132, "endOffset": 158}, {"referenceID": 6, "context": "Earlier work noted that HPF is not sensitive to hyperparameter settings within reason (Gopalan et al., 2013).", "startOffset": 86, "endOffset": 108}], "year": 2016, "abstractText": "Non-negative matrix factorization models based on a hierarchical Gamma-Poisson structure capture user and item behavior effectively in extremely sparse data sets, making them the ideal choice for collaborative filtering applications. Hierarchical Poisson factorization (HPF) in particular has proved successful for scalable recommendation systems with extreme sparsity. HPF, however, suffers from a tight coupling of sparsity model (absence of a rating) and response model (the value of the rating), which limits the expressiveness of the latter. Here, we introduce hierarchical compound Poisson factorization (HCPF) that has the favorable GammaPoisson structure and scalability of HPF to highdimensional extremely sparse matrices. More importantly, HCPF decouples the sparsity model from the response model, allowing us to choose the most suitable distribution for the response. HCPF can capture binary, non-negative discrete, non-negative continuous, and zero-inflated continuous responses. We compare HCPF with HPF on nine discrete and three continuous data sets and conclude that HCPF captures the relationship between sparsity and response better than HPF.", "creator": "LaTeX with hyperref package"}}}