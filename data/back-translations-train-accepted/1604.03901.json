{"id": "1604.03901", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2016", "title": "Single-Image Depth Perception in the Wild", "abstract": "This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset \"Depth in the Wild\" consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild.", "histories": [["v1", "Wed, 13 Apr 2016 18:19:35 GMT  (4819kb,D)", "http://arxiv.org/abs/1604.03901v1", null], ["v2", "Fri, 6 Jan 2017 16:05:35 GMT  (9305kb,D)", "http://arxiv.org/abs/1604.03901v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["weifeng chen", "zhao fu", "dawei yang", "jia deng"], "accepted": true, "id": "1604.03901"}, "pdf": {"name": "1604.03901.pdf", "metadata": {"source": "CRF", "title": "Single-Image Depth Perception in the Wild", "authors": ["Weifeng Chen", "Zhao Fu", "Dawei Yang", "Jia Deng"], "emails": ["wfchen@umich.edu", "zhaofu@umich.edu", "ydawei@umich.edu", "jiadeng@umich.edu"], "sections": [{"heading": null, "text": "Deep Network with Pixel-wise Prediction Metric DepthRGB-D Data Relative Depth Annotation Training ImageFigure 1: We use crowd-sourced annotations of relative depth and train a deep network to extract depth from a single image taken in unrestricted environments (\"in the wild\")."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Work", "text": "In fact, the fact is that most of them will be able to demonstrate that they are able, that they are able to achieve their goals, and that they are able to achieve their goals."}, {"heading": "3 Dataset Construction", "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "4 Learning with Relative Depth", "text": "In fact, it is a matter of a way in which people are able to determine for themselves what they want and what they want. (...) In fact, it is a matter of a way in which they are able to determine for themselves. (...) It is as if they were able to determine for themselves. (...) It is as if they were able to determine for themselves. (...) It is as if they were able to determine for themselves. (...) It is as if they were able to determine for themselves. (...) It is as if they were able to determine for themselves. (...) \"(...)\" (...) It is as if they were able to determine for themselves. (...) It is as if they are able to determine for themselves what they want. \"(...) It is as if they are able.\" (...)"}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 NYU Depth", "text": "We evaluate our method using the NYU Depth Dataset [4], which consists of indoor scenes with ground-truth-kinect depth. We use the same equipment as Zoran et al. [14]: Pairs of points are sampled from the training images (the subset of NYU Depth consisting of 795 images per image). We use the same training data and test data as Zoran et al. [14]: Pairs of points are generated by comparing the ground-truth-kinect depth; the same procedure is applied to the test to generate the pairs of points for evaluation (about 3K pairs per image). We use the same training data and test data as Zoran et al. [14] As the system of Zoran et al depth is generated, our network says one of the three ordinary relationships to the test pairs is equal (=), closer (<), or farther (>). We report on WKDR, which predicts unpredictability between the basic ratios and the predicted parameters and the ratios."}, {"heading": "5.2 Depth in the Wild", "text": "In this section, we are experimenting with our new Depth in the Wild (DIW) dataset. We split the dataset into 421K training images and 74K test images. 4.We report on the WHDR (Weighted Human Disagreement Rate) 5 out of 5 methods in Tab. 3: (1) the state-of-the-art system from Eigen et al. [8] trained to full NYU depth; (2) our network trained to full NYU depth (Ours _ Full); (3) our network pre-trained to full NYU depth and fine-tuned to DIW (Ours _ NYU _ DIW); (4) our network trained from scratch to DIW (Ours _ DIW); (5) a basic method that uses only the location of the query points: classify the bottom point closer or guess randomly if the two points are tuned to the same height (Query _ Location _ Only).We see that despite the depth of the query points, the lower points are scored to be closer to the same height (if both points are matched to the same query points)."}, {"heading": "6 Conclusions", "text": "We introduced a new dataset that consists of images in the wild that are annotated with relative depth between pairs of random dots. We proposed a new algorithm that learns to estimate the metric depth based on annotations of relative depth. We demonstrated that our algorithm exceeds the state of the art. Furthermore, we demonstrated that our algorithm, in combination with existing RGB-D data and our new relative depth annotations, significantly improves depth perception in the wild."}], "references": [{"title": "Depthtransfer: Depth extraction from video using nonparametric sampling", "author": ["K. Karsch", "C. Liu", "S.B. Kang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic photo pop-up", "author": ["D. Hoiem", "A.A. Efros", "M. Hebert"], "venue": "ACM Transactions on Graphics (TOG), vol. 24, no. 3, pp. 577\u2013584, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Make3d: Learning 3d scene structure from a single still image", "author": ["A. Saxena", "M. Sun", "A. Ng"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 31, no. 5, pp. 824\u2013840, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "Computer Vision\u2013ECCV 2012, pp. 746\u2013760, Springer, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Vision meets robotics: The kitti dataset", "author": ["A. Geiger", "P. Lenz", "C. Stiller", "R. Urtasun"], "venue": "The International Journal of Robotics Research, p. 0278364913491297, 2013. 4.38% of images are duplicates downloaded using different query keywords. We have removed test images that have duplicates in the training set. All weights are 1. Also a pair of points can only have two possible ordinal relations (farther or closer) for DIW. 9  Input Eigen Ours_NYU_DIW Input Eigen Ours_NYU_DIW Figure 10: Qualitative results on our Depth in the Wild (DIW) dataset by our method and the method of Eigen et al. [8].", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep convolutional neural fields for depth estimation from a single image", "author": ["F. Liu", "C. Shen", "G. Lin"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5162\u20135170, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Pulling things out of perspective", "author": ["L. Ladicky", "J. Shi", "M. Pollefeys"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 89\u201396, IEEE, 2014. 10", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pp. 2650\u20132658, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Coupled depth learning", "author": ["M.H. Baig", "L. Torresani"], "venue": "arXiv preprint arXiv:1501.04537, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs", "author": ["B. Li", "C. Shen", "Y. Dai", "A. van den Hengel", "M. He"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1119\u20131127, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving the kinect by cross-modal stereo", "author": ["W.W.-C. Chiu", "U. Blanke", "M. Fritz"], "venue": "BMVC, vol. 1, p. 3, Citeseer, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning depth from single monocular images", "author": ["A. Saxena", "S.H. Chung", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, pp. 1161\u20131168, 2005.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "The visual perception of 3-d shape from multiple cues: Are observers capable of perceiving metric structure", "author": ["J.T. Todd", "J.F. Norman"], "venue": "Perception & Psychophysics, vol. 65, no. 1, pp. 31\u201347, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning ordinal relationships for mid-level vision", "author": ["D. Zoran", "P. Isola", "D. Krishnan", "W.T. Freeman"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pp. 388\u2013396, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A category-level 3d object dataset: Putting the kinect to work", "author": ["A. Janoch", "S. Karayev", "Y. Jia", "J.T. Barron", "M. Fritz", "K. Saenko", "T. Darrell"], "venue": "Consumer Depth Cameras for Computer Vision, pp. 141\u2013165, Springer, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Sun rgb-d: A rgb-d scene understanding benchmark suite", "author": ["S. Song", "S.P. Lichtenberg", "J. Xiao"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 567\u2013576, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "A large dataset of object scans", "author": ["S. Choi", "Q.-Y. Zhou", "S. Miller", "V. Koltun"], "venue": "arXiv preprint arXiv:1602.02481, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Intrinsic images in the wild", "author": ["S. Bell", "K. Bala", "N. Snavely"], "venue": "ACM Transactions on Graphics (TOG), vol. 33, no. 4, p. 159, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Shape, illumination, and reflectance from shading", "author": ["J.T. Barron", "J. Malik"], "venue": "TPAMI, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "3-d depth reconstruction from a single still image", "author": ["A. Saxena", "S.H. Chung", "A.Y. Ng"], "venue": "International journal of computer vision, vol. 76, no. 1, pp. 53\u201369, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "From shading to local shape", "author": ["Y. Xiong", "A. Chakrabarti", "R. Basri", "S.J. Gortler", "D.W. Jacobs", "T. Zickler"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 37, no. 1, pp. 67\u201379, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Direction matters: Depth estimation with a surface normal classifier", "author": ["C. Hane", "L. Ladicky", "M. Pollefeys"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 381\u2013389, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Single image depth estimation from predicted semantic labels", "author": ["B. Liu", "S. Gould", "D. Koller"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pp. 1253\u20131260, IEEE, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Scene intrinsics and depth from a single image", "author": ["E. Shelhamer", "J. Barron", "T. Darrell"], "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops, pp. 37\u201344, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Monocular object instance segmentation and depth ordering with cnns", "author": ["Z. Zhang", "A.G. Schwing", "S. Fidler", "R. Urtasun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pp. 2614\u20132622, 2015. 11", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards unified depth and semantic prediction from a single image", "author": ["P. Wang", "X. Shen", "Z. Lin", "S. Cohen", "B. Price", "A.L. Yuille"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2800\u20132809, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning data-driven reflectance priors for intrinsic image decomposition", "author": ["T. Zhou", "P. Krahenbuhl", "A.A. Efros"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pp. 3469\u20133477, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning lightness from human judgement on relative reflectance", "author": ["T. Narihira", "M. Maire", "S.X. Yu"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pp. 2965\u20132973, IEEE, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6229, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Relative attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pp. 503\u2013510, IEEE, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Attribute and simile classifiers for face verification", "author": ["N. Kumar", "A.C. Berg", "P.N. Belhumeur", "S.K. Nayar"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on, pp. 365\u2013372, IEEE, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning globally-consistent local distance functions for shape-based image retrieval and classification", "author": ["A. Frome", "Y. Singer", "F. Sha", "J. Malik"], "venue": "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pp. 1\u20138, IEEE, 2007.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to rank: from pairwise approach to listwise approach", "author": ["Z. Cao", "T. Qin", "T.-Y. Liu", "M.-F. Tsai", "H. Li"], "venue": "Proceedings of the 24th international conference on Machine learning, pp. 129\u2013136, ACM, 2007.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 133\u2013142, ACM, 2002.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Depth map prediction from a single image using a multiscale deep network", "author": ["D. Eigen", "C. Puhrsch", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, pp. 2366\u20132374, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431\u2013 3440, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Holistically-nested edge detection", "author": ["S. Xie", "Z. Tu"], "venue": "CoRR, vol. abs/1504.06375, 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked hourglass networks for human pose estimation", "author": ["A. Newell", "K. Yang", "J. Deng"], "venue": "arXiv preprint arXiv:1603.06937, 2016.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1\u20139, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Im2depth: Scalable exemplar based depth transfer", "author": ["M.H. Baig", "V. Jagadeesh", "R. Piramuthu", "A. Bhardwaj", "W. Di", "N. Sundaresan"], "venue": "Applications of Computer Vision (WACV), 2014 IEEE Winter Conference on, pp. 145\u2013152, IEEE, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014. 12", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 68, "endOffset": 77}, {"referenceID": 1, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 68, "endOffset": 77}, {"referenceID": 2, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 68, "endOffset": 77}, {"referenceID": 3, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 4, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 5, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 6, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 7, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 8, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 9, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 3, "context": "Current RGB-D datasets were collected by depth sensors [4, 5], which are limited in range and resolution, and often fail on specular or transparent objects [11].", "startOffset": 55, "endOffset": 61}, {"referenceID": 4, "context": "Current RGB-D datasets were collected by depth sensors [4, 5], which are limited in range and resolution, and often fail on specular or transparent objects [11].", "startOffset": 55, "endOffset": 61}, {"referenceID": 10, "context": "Current RGB-D datasets were collected by depth sensors [4, 5], which are limited in range and resolution, and often fail on specular or transparent objects [11].", "startOffset": 156, "endOffset": 160}, {"referenceID": 3, "context": "For example, NYU depth [4] consists entirely of indoor scenes with no human presence; KITTI [5] consists entirely of road scenes captured from a car; Make3D [3, 12] consists entirely of outdoor scenes of the Stanford campus (Figure.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "For example, NYU depth [4] consists entirely of indoor scenes with no human presence; KITTI [5] consists entirely of road scenes captured from a car; Make3D [3, 12] consists entirely of outdoor scenes of the Stanford campus (Figure.", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "For example, NYU depth [4] consists entirely of indoor scenes with no human presence; KITTI [5] consists entirely of road scenes captured from a car; Make3D [3, 12] consists entirely of outdoor scenes of the Stanford campus (Figure.", "startOffset": 157, "endOffset": 164}, {"referenceID": 11, "context": "For example, NYU depth [4] consists entirely of indoor scenes with no human presence; KITTI [5] consists entirely of road scenes captured from a car; Make3D [3, 12] consists entirely of outdoor scenes of the Stanford campus (Figure.", "startOffset": 157, "endOffset": 164}, {"referenceID": 12, "context": "Crowdsourcing seems viable, but humans are not good at estimating metric depth, or 3D metric structure in general [13].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "But humans are better at judging relative depth [13]: \u201cIs point A closer than point B?\u201d is often a much easier question for humans.", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "[14] shows that it is possible to learn to estimate metric depth using only annotations of relative depth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14], but is also simpler.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] first learns a classifier to predict the ordinal relation between two points in an image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] but also the state-of-the-art image-todepth system by Eigen et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] trained with ground-truth metric depth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "RGB-D Datasets: Prior work on constructing RGB-D datasets has relied on either Kinect [4, 15, 16, 17] or LIDAR [3, 5].", "startOffset": 86, "endOffset": 101}, {"referenceID": 14, "context": "RGB-D Datasets: Prior work on constructing RGB-D datasets has relied on either Kinect [4, 15, 16, 17] or LIDAR [3, 5].", "startOffset": 86, "endOffset": 101}, {"referenceID": 15, "context": "RGB-D Datasets: Prior work on constructing RGB-D datasets has relied on either Kinect [4, 15, 16, 17] or LIDAR [3, 5].", "startOffset": 86, "endOffset": 101}, {"referenceID": 16, "context": "RGB-D Datasets: Prior work on constructing RGB-D datasets has relied on either Kinect [4, 15, 16, 17] or LIDAR [3, 5].", "startOffset": 86, "endOffset": 101}, {"referenceID": 2, "context": "RGB-D Datasets: Prior work on constructing RGB-D datasets has relied on either Kinect [4, 15, 16, 17] or LIDAR [3, 5].", "startOffset": 111, "endOffset": 117}, {"referenceID": 4, "context": "RGB-D Datasets: Prior work on constructing RGB-D datasets has relied on either Kinect [4, 15, 16, 17] or LIDAR [3, 5].", "startOffset": 111, "endOffset": 117}, {"referenceID": 2, "context": "Existing Kinect-based datasets are limited to indoor scenes; existing LIDARbased datasets are biased towards scenes of man-made structures [3, 5].", "startOffset": 139, "endOffset": 145}, {"referenceID": 4, "context": "Existing Kinect-based datasets are limited to indoor scenes; existing LIDARbased datasets are biased towards scenes of man-made structures [3, 5].", "startOffset": 139, "endOffset": 145}, {"referenceID": 17, "context": "Intrinsic Images in the Wild: Our work draws inspiration from Intrinsic Images in the Wild [18], a seminal work that crowdsources annotations of relative reflectance on unconstrained images.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 5, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 6, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 7, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 8, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 9, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 11, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 18, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 18, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 19, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 20, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 21, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 22, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 23, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 3, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 66, "endOffset": 72}, {"referenceID": 4, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 66, "endOffset": 72}, {"referenceID": 5, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 99, "endOffset": 121}, {"referenceID": 7, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 99, "endOffset": 121}, {"referenceID": 9, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 99, "endOffset": 121}, {"referenceID": 13, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 99, "endOffset": 121}, {"referenceID": 24, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 99, "endOffset": 121}, {"referenceID": 25, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 99, "endOffset": 121}, {"referenceID": 13, "context": "But the networks in these previous works, with the exception of [14], were trained exclusively using ground-truth metric depth, whereas our approach uses relative depth.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "[14], which proposes to use a deep network to repeatedly classify pairs of points sampled based on superpixel segmentation, and to reconstruct per-pixel metric depth by solving an additional optimization problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Learning with Ordinal Relations: Several recent works [27, 28] have used the ordinal relations from the Intrinsic Images in the Wild dataset [18] to estimate surface refletance.", "startOffset": 54, "endOffset": 62}, {"referenceID": 27, "context": "Learning with Ordinal Relations: Several recent works [27, 28] have used the ordinal relations from the Intrinsic Images in the Wild dataset [18] to estimate surface refletance.", "startOffset": 54, "endOffset": 62}, {"referenceID": 17, "context": "Learning with Ordinal Relations: Several recent works [27, 28] have used the ordinal relations from the Intrinsic Images in the Wild dataset [18] to estimate surface refletance.", "startOffset": 141, "endOffset": 145}, {"referenceID": 13, "context": "[14], Zhou et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] first learn a deep network to classify the ordinal relations between pair of points and then make them globally consistent through energy minimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] learn a \u201clightness potential\u201d network that takes an image patch and predicts the metric reflectance of the center pixel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Making it fully convolutional (as the authors mentioned in [28]) only solves it partially: as long as the lightness potential network has downsampling layers, which is the case in [28], the final output will be downsampled accordingly.", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "Making it fully convolutional (as the authors mentioned in [28]) only solves it partially: as long as the lightness potential network has downsampling layers, which is the case in [28], the final output will be downsampled accordingly.", "startOffset": 180, "endOffset": 184}, {"referenceID": 28, "context": "Additional resolution augmentation (such as the \u201cshift and stitch\u201d approach [29]) is thus needed.", "startOffset": 76, "endOffset": 80}, {"referenceID": 29, "context": "Beyond intrinsic images, ordinal relations have been used widely in computer vision and machine learning, including object recognition [30], face recognition [31], image retrieval [32], and learning to rank [33, 34].", "startOffset": 135, "endOffset": 139}, {"referenceID": 30, "context": "Beyond intrinsic images, ordinal relations have been used widely in computer vision and machine learning, including object recognition [30], face recognition [31], image retrieval [32], and learning to rank [33, 34].", "startOffset": 158, "endOffset": 162}, {"referenceID": 31, "context": "Beyond intrinsic images, ordinal relations have been used widely in computer vision and machine learning, including object recognition [30], face recognition [31], image retrieval [32], and learning to rank [33, 34].", "startOffset": 180, "endOffset": 184}, {"referenceID": 32, "context": "Beyond intrinsic images, ordinal relations have been used widely in computer vision and machine learning, including object recognition [30], face recognition [31], image retrieval [32], and learning to rank [33, 34].", "startOffset": 207, "endOffset": 215}, {"referenceID": 33, "context": "Beyond intrinsic images, ordinal relations have been used widely in computer vision and machine learning, including object recognition [30], face recognition [31], image retrieval [32], and learning to rank [33, 34].", "startOffset": 207, "endOffset": 215}, {"referenceID": 13, "context": "[14] first learn a classifier to predict ordinal relations between centers of superpixels, and then reconcile", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Network Design: Networks that output the same resolution as the input are aplenty, including the recent designs for depth estimation [8, 35] and those for semantic segmentation [36] and edge detection [37].", "startOffset": 133, "endOffset": 140}, {"referenceID": 34, "context": "Network Design: Networks that output the same resolution as the input are aplenty, including the recent designs for depth estimation [8, 35] and those for semantic segmentation [36] and edge detection [37].", "startOffset": 133, "endOffset": 140}, {"referenceID": 35, "context": "Network Design: Networks that output the same resolution as the input are aplenty, including the recent designs for depth estimation [8, 35] and those for semantic segmentation [36] and edge detection [37].", "startOffset": 177, "endOffset": 181}, {"referenceID": 36, "context": "Network Design: Networks that output the same resolution as the input are aplenty, including the recent designs for depth estimation [8, 35] and those for semantic segmentation [36] and edge detection [37].", "startOffset": 201, "endOffset": 205}, {"referenceID": 37, "context": "In this work, we use a variant of the recently introduced \u201chourglass\u201d network, which has been used to achieve state-of-the-art results on human pose estimation [38].", "startOffset": 160, "endOffset": 164}, {"referenceID": 38, "context": "6 illustrates the design, which consists of a series of convolutions (using a variant of the inception [39] module) and downsampling, followed by a series of convolutions and upsampling, interleaved with skip connections that add back features from high resolutions.", "startOffset": 103, "endOffset": 107}, {"referenceID": 37, "context": "We refer the reader to [38] for comparing the design to related work.", "startOffset": 23, "endOffset": 27}, {"referenceID": 38, "context": "Figure 7: The variant of the Inception Module [39] used in our model.", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "We evaluate our method using the NYU Depth dataset [4], which consists of indoor scenes with ground-truth Kinect depth.", "startOffset": 51, "endOffset": 54}, {"referenceID": 13, "context": "[14]: point pairs are sampled from the training images (the subset of NYU Depth consisting of 795 images with semantic labels) using superpixel segmentation and their ground-truth ordinal relations are generated by comparing the ground-truth Kinect depth; the same procedure is applied to the test set to generate the point pairs for evaluation (around 3K pairs per image).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14], our network predicts one of the three ordinal relations on the test pairs: equal (=), closer (<), or farther (>).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "4% Zoran [14] 43.", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "7% Eigen(A) [8] 37.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "7% Eigen(V) [8] 34.", "startOffset": 12, "endOffset": 15}, {"referenceID": 13, "context": "43 Zoran [14] 1.", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "54 Eigen(A) [8] 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "19 Eigen(V) [8] 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 25, "context": "12 Wang [26] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": "22 Liu [6] 0.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "23 Li [10] 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "23 Karsch [1] 1.", "startOffset": 10, "endOffset": 13}, {"referenceID": 39, "context": "35 Baig [40] 1.", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "Details for each metric can be found in [8].", "startOffset": 40, "endOffset": 43}, {"referenceID": 7, "context": "[8], one using AlexNet (Eigen(A)) and one using VGGNet (Eigen(V)).", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] define two points to have equal depths if the ratio between their groundtruth depths is within a pre-determined range.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Our network is trained with the same data 3 but outperforms [14] on all three metrics.", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "Following [14], we also compare with the state-of-art image-to-depth system by Eigen et al.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "[8], which is trained on pixel-wise ground-truth metric depth from the full NYU Depth training set (220K images).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Granted, this comparison is not entirely fair because [8] is not optimized for predicting ordinal relations.", "startOffset": 54, "endOffset": 57}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "We see that although imperfect, the recovered metric depth by our method is overall reasonable and qualitatively similar to that by the state-of-art system [8] trained on ground-truth metric depth.", "startOffset": 156, "endOffset": 159}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1]) that have access to ground-truth metric depth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "WKDR stands for \u201cWeighted Kinect Disagreement Rate\u201d; the weight is set to 1 as in [14] The code released by Zoran et al.", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "[14] indicates that they train with a random subset of 800 pairs per image instead of the all pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8], and the method of Zoran et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "All depth maps except ours are directly from [14].", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "Figure 9: Point pairs generated through superpixel segmentation [14] (left) versus point pairs generated through random sampling with distance constraints (right).", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "[14], we train our network using the same point pairs, which are pairs of centers of superpixels (Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Method Eigen(V) [8] Ours_Full Ours_NYU_DIW Ours_DIW Query_Location_Only WHDR 25.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "[8] (VGGNet [41] version)", "startOffset": 0, "endOffset": 3}, {"referenceID": 40, "context": "[8] (VGGNet [41] version)", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "[8] trained on full NYU Depth; (2) our network trained on full NYU Depth (Ours_Full); (3) our network pre-trained on full NYU Depth and fine-tuned on DIW (Ours_NYU_DIW); (4) our network trained from scratch on DIW (Ours_DIW); (5) a baseline method that uses only the location of the query points: classify the lower point to be closer or guess randomly if the two points are at the same height (Query_Location_Only).", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset \u201cDepth in the Wild\u201d consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild. Deep Network with Pixel-wise Prediction Metric Depth RGB-D Data Relative Depth Annotations", "creator": "LaTeX with hyperref package"}}}