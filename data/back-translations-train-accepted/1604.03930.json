{"id": "1604.03930", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2016", "title": "Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis", "abstract": "This paper considers the problem of canonical-correlation analysis (CCA) (Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a pair of symmetric matrices. These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009).", "histories": [["v1", "Wed, 13 Apr 2016 19:57:46 GMT  (317kb)", "https://arxiv.org/abs/1604.03930v1", null], ["v2", "Fri, 27 May 2016 18:03:11 GMT  (316kb)", "http://arxiv.org/abs/1604.03930v2", "International Conference on Machine Learning (ICML) 2016"]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["rong ge 0001", "chi jin", "sham m kakade", "praneeth netrapalli", "aaron sidford"], "accepted": true, "id": "1604.03930"}, "pdf": {"name": "1604.03930.pdf", "metadata": {"source": "CRF", "title": "Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis", "authors": ["Rong Ge", "Chi Jin", "Sham M. Kakade", "Praneeth Netrapalli", "Aaron Sidford"], "emails": ["rongge@cs.duke.edu", "chijin@cs.berkeley.edu", "sham@cs.washington.edu", "praneeth@microsoft.com", "asid@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 4.03 930v 2 [cs.L G] 27 MWe provide simple iterative algorithms with improved runtimes to solve these problems, which converge globally linearly and have moderate dependencies on the conditional numbers and eigenvalue gaps of the matrices involved. We obtain our results by reducing CCA to the top-k generalized eigenvector problem. We solve this problem through a general framework that only requires black box access to an approximate linear system solver. Instantiating this framework with accelerated gradient descent gives us a runtime of O (zk \u0445 log (1 / \u0432), where z is the total number of non-erotic entries, \u0443 is the conditional number and \u03c1 is the relative eigenvalue gap of the corresponding matrices. Our algorithm is linear in input size and number of components k (k), with z being the total number of non-entries."}, {"heading": "1 Introduction", "text": "These problems arise, of course, in the statistical settings we wish to use to find two large sets of data with which we can examine the similarity or dissimilarity of each set of data. Email: rongge @ cs.duke.edu \u2020 UC Berkeley. Email: chijin @ cs.berkeley.edu @ cs.edu @ cs.washington.edu @ Microsoft Research New England. Email: praneeth @ microsoft.com @ microsoft.com"}, {"heading": "1.1 Our Approach", "text": "rE \"s tis rf\u00fc ide rf\u00fcnde f\u00fc ide r\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc the f\u00fc the f\u00fc the f\u00fc the f\u00fc the f\u00fc the f\u00fc the"}, {"heading": "1.2 Previous Work", "text": "While so far there has been limited work on the detectable solution of CCA and generalized eigenvectors, we note that there is an impressive literature on the implementation of PCA (Rokhlin et al., 2009; Halko et al., 2011; Musco and Musco, 2015; Garber and Hazan, 2015; Jin et al., 2015) and on solving positive semidefinitive linear systems (Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004). Our analysis in this paper builds extensively on this work and our results should be considered as a principled application to the generalized eigenvector problem. Recently, there has been much interest in designing scalable algorithms for CCA (Ma et al., 2015; Wang et al., 2015; Wang and Livescu, 2015; Michaeli et al., 2015). To our knowledge, there is no proven guarantee of safe methods for this problem."}, {"heading": "1.3 Our Results", "text": "In order to state our results, we must specify the eigenvalues of B \u2212 1A (their existence is guaranteed by Lemma 9 in the appendix).The eigengap is able to specify our results. (The eigengap is in the informal version of Theorem 6).Given the two matrices A and B \u00b2 -Rd \u00b7 d there is an algorithm that generalizes the eigenvectors up to an error in time O (zk).we are an error in time O (B).we are an error in time O (B).we are an error in time O (B).we are an error in time O (B).we are an error in time O (B).we are an error in time O (B).we are an error in time O (B).we are an error in time O (B).we are an error in time O (B).we are an error in time O (B).we are an error in time O (B).we are an error in time O (B).we are an error."}, {"heading": "1.4 Paper Overview", "text": "In Section 2, we present our notation. In Section 3, we formally define the problems we solve and their relevant parameters. In Section 4, we present our results for the generalized eigenvector problem. In Section 5, we present our results for the CCA problem. In Section 6, we argue that generalized eigenvector calculation is as difficult as a linear system solution and that our dependence on B is nearly optimal. In Section 7, we present experimental results of our algorithms on some real-world datasets. Due to space constraints, evidence is moved to the appendix."}, {"heading": "2 Notation", "text": "We use bold uppercase letters (A, B, \u00b7 \u00b7) to denote matrices, and bold lowercase letters (u, v, \u00b7 \u00b7) for vectors. For symmetric positive semidefinitive (PSD) matrix B, we leave that a matrix B def = \u221a u Bu denotes the B norm of u, and we leave < u, v > B def = u Bv the inner product of u and v in the B norm. We say that a matrix W B is orthonormal when W BW = I. We allow \u03c3i (A) to denote the largest singular value of A, \u03c3min (A) and \u03c3max (A) to denote the smallest or largest singular values of A. Likewise, we allow \u03bbi (A) to refer to the largest eigenvalue of A in the order of magnitude. We allow nnz (A) to denote the number of nonular values in A."}, {"heading": "3 Problem Statement", "text": "In this section we will remember the generalized eigenvalue problem, define our error metrics and introduce all relevant parameters. Remember that the generalized eigenvalue problem is to find k vectors wi, i [k] in such a way that the vectors wi are specified with wi = vi, where vi is an eigenvector of B \u2212 1A with eigenvalue \u03bbi, so that it is such that | \u03bb1 | \u2265 \u00b7 \u00b7 \u00b7 \u0445\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0442\u0438\u0441\u0438\u0441\u0442\u0438\u0441\u0442\u0438\u0441\u0442i. Our goal is to restore the uppermost k-own space, i.e., the chip {v1, \u00b7 \u00b7, vk}. To quantify the error in the estimation of eigenspace, we will use the largest eigenspace, the largest eigenspace, which is a standard concept of the distance between the subspaces (Golub and Van Loan, 2012).Definition 4 (the largest main angle), we will leave the K-spaces, W-K-K-principles and W-K-K-principles (W-K-K-K)."}, {"heading": "4 Our Results", "text": "In this section we present our algorithms and results for solving the generalized eigenvector problem (Section 4.1). First, however, we formally define a linear system solver as follows: Linear system solver: In each of our main results (theorems 5 and 6) we assume black box access to an approximate linear system solver. Starting from a PSD matrix B, a vector b, a first estimate u0 and an error parameter \u03b4, we must reduce the error by a multiplicative approach, i.e. the output u1 with an approximate linear system solver."}, {"heading": "4.1 Top-1 Setting", "text": "Our algorithm helps to calculate the uppermost generalized eigenvector, called GenELin in algorithm 1. \u00b7 The algorithm implements an approximate potential method in which each iteration consists of an approximate multiplication of a vector by B \u2212 1A. \u00b7 To do this, GenELin solves a linear system in B and then scales the resulting vector to have unit B standard. Our main result is that there is an oracle to solve the linear systems, 4 the number of iterations taken by algorithm 1 to calculate the uppermost eigenvector to an accuracy of 1."}, {"heading": "4.2 Top-k Setting", "text": "In this section, we give an extension of our algorithm and result for the calculation of the top-k generalized eigenvector system: \u00b7 nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p"}, {"heading": "5 Application to CCA", "text": "We now outline how the CCA problem can be reduced to generalized eigenvectors."}, {"heading": "6 Reduction to Linear System", "text": "Here we show that the solution of linear systems in B is inherent in the solution of the above generalized eigenvector problem in the worst case scenario. Let M be a symmetrically positive definitive matrix and assume that we want to solve the linear system Mx = m, i.e. calculate x * with Mx \u0445 = m. If we calculate A = mm and B = M thenargmax x Bx = 1x Ax = B \u2212 1mm B \u2212 1mand consistently calculate the uppermost generalized eigenvector, the solution of the linear system results. Therefore, the problem of calculating top-down generalized eigenvectors is generally more difficult than the problem of solving symmetrical positive positively defined linear systems. Furthermore, it is known that any method that starts from m and iteratively applies M to linear combinations of the calculated problem, so that it must solve far more problems than the upper order (1994)."}, {"heading": "7 Simulations", "text": "In this section we present our experimental results, which CCA performs on three benchmark datasets summarized in Table 4. We would like to demonstrate two things from these simulations: 1) The behavior of CCALin verifies our theoretical result on relatively small datasets and 2) scalability of CCALin a comparison with other existing algorithms on large datasets. Now, let us specify the error metrics we use in our experiments, the first being the main angles between the estimated and true subspaces. Let Wx and Wy be the estimated subspaces andVx, Vy the true canonical subspaces. We will use principle angles successx = \u03b8 (Wx, Vx) under the Sxx standard, successy = Douglas (Wx, Vx) under the Syy standard and successB = CCC ((Vx 0 Vy)."}, {"heading": "7.1 Small-scale Datasets", "text": "MNIST dataset (LeCun et al., 1998) consists of 60,000 handwritten digits from 0 to 9. Each digit is an image represented by 392 x 392 real values in [0,1]. Here, CCA is performed between the left half of the image and the right half of the image. The data matrix is dense, but the dimension is relatively small. Penn Tree Bank (PTB) dataset comes from the complete Wall Street Journal Part of Penn Tree Bank, which consists of 1.17 million tokens and a vocabulary size of 43k (Marcus et al., 1993), which has already been used to successfully learn the word embedding by CCA (Dhillon et al., 2011). Here, the task is to learn correlated components between two consecutive words. We use only the top 10,000 most common words. Each set of data matrix X is an indicator vector and therefore it is very sparse and X is diagonally input. As the arithmetic matrix we are very poorly configured by ALX, some data matrices are replaced."}, {"heading": "7.2 Large-scale Dataset", "text": "The URL reputation dataset contains 2.4 million URLs and 3.2 million features, including both host-based features and lexical features. Each feature is either real or binary. For experiments in this section, we follow the setting of (Ma et al., 2015). We use the first 2 million samples and run CCA between a subset of host-based features and a subset of lexical features to extract the top 20 components. Although the data matrix X is relatively sparse, unlike PTB it has strong correlations between different coordinates, making X much denser (nnz (X'X) / d21 \u2248 10 \u2212 3). Classic algorithms are impractical for this dataset on a typical computer, as either memory is exhausted or requires an unaffordable amount of time. As we cannot estimate the principal angles, we will evaluate the CC performance of ACCs."}, {"heading": "8 Conclusion", "text": "In summary, we have provided the first verifiable, globally linear convergent algorithms for solving canonical correlation analysis and generalized eigenvector problems. We have shown that our algorithms for recovering the top-k components are much faster than traditional methods based on fast matrix multiplication and singular value decomposition when k-n and the state numbers and eigenvalue gaps of the matrices involved are moderate. Furthermore, we have provided empirical evidence that our algorithms can be useful in practice, and we hope that these results will serve as a basis for further improvements in performing large-scale data analysis both in theory and practice."}, {"heading": "A Solving Linear System via Accelerated Gradient Descent", "text": "Algorithm 4 Nesterov's accelerated gradient descent Input: learning rate \u03b7, factor Q, starting point x0, T. Output: Minimizer x x for f = 0, \u00b7 \u00b7 \u00b7 \u00b7, T \u2212 1 do yt + 1 \u2190 xt \u2212 (1 / \u03b2) \u00b7 f (xt) xt + 1 \u2190 yt + 1 + (\u221a Q \u2212 1) / (\u221a Q + 1) \u00b7 (yt + 1 \u2212 yt) End for Return yT. Since we use accelerated gradient descent in our main theorems, we present the algorithm and cite its result about iteration complexity here without proof for the sake of completeness. Theorem 8 (((Nesterov, 1983): f is \u03b1-strongly convex and \u03b2-smooth, then accelerated gradient descent with learning rate \u03b7 = 1\u03b2 and Q = \u03b2 / \u03b1 satisfactory: f (xt) \u2212 f (x) \u2264 2 (f (x0) \u2212 f (x) \u00b7 p)."}, {"heading": "B Proofs of Main Theorem", "text": "In this section, we will first prove theorems 5, 6, and 7.B.1 (part 1) for the SettingWe that B \u2212 1A is a eigenvalue of B \u2212 1A with eigenvalue of B \u2212 1A. (B \u2212 1 / 2ui) = B \u2212 1 / 2 (B \u2212 1 / 2ui) is a eigenvalue of B \u2212 1A with eigenvalue. (B \u2212 1 / 2ui) The proof is simple. (B \u2212 1 / 2ui) = B \u2212 1 / 2 (B \u2212 1 / 2AB \u2212 1 / 2ui) is a eigenvalue of B \u2212 2ui. Denote the eigenvalues of B \u2212 1A of (2 / 2ui), the above problem further tells us that v i Bvj = u i that we define the angle between W \u2212 1 and v1 in the B standard."}], "references": [{"title": "Efficient dimensionality reduction for canonical correlation analysis", "author": ["H. Avron", "C. Boutsidis", "S. Toledo", "A. Zouzias"], "venue": "SIAM Journal on Scientific Computing, 36(5):S111\u2013S131.", "citeRegEx": "Avron et al\\.,? 2014", "shortCiteRegEx": "Avron et al\\.", "year": 2014}, {"title": "Origins and levels of monthly and seasonal forecast skill for united states surface air temperatures determined by canonical correlation analysis", "author": ["T. Barnett", "R. Preisendorfer"], "venue": "Monthly Weather Review, 115(9):1825\u20131850.", "citeRegEx": "Barnett and Preisendorfer,? 1987", "shortCiteRegEx": "Barnett and Preisendorfer", "year": 1987}, {"title": "Prediction of enso episodes using canonical correlation analysis", "author": ["A.G. Barnston", "C.F. Ropelewski"], "venue": "Journal of climate, 5(11):1316\u20131345.", "citeRegEx": "Barnston and Ropelewski,? 1992", "shortCiteRegEx": "Barnston and Ropelewski", "year": 1992}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "Proceedings of the 26th annual international conference on machine learning, pages 129\u2013136. ACM.", "citeRegEx": "Chaudhuri et al\\.,? 2009", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Multi-view learning of word embeddings via cca", "author": ["P. Dhillon", "D.P. Foster", "L.H. Ungar"], "venue": "Advances in Neural Information Processing Systems, pages 199\u2013207.", "citeRegEx": "Dhillon et al\\.,? 2011", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Detection of neural activity in functional mri using canonical correlation analysis", "author": ["O. Friman", "J. Cedefamn", "P. Lundberg", "M. Borga", "H. Knutsson"], "venue": "Magnetic Resonance in Medicine, 45(2):323\u2013330.", "citeRegEx": "Friman et al\\.,? 2001", "shortCiteRegEx": "Friman et al\\.", "year": 2001}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["R. Frostig", "R. Ge", "S.M. Kakade", "A. Sidford"], "venue": "ICML2015.", "citeRegEx": "Frostig et al\\.,? 2015", "shortCiteRegEx": "Frostig et al\\.", "year": 2015}, {"title": "Fast and simple pca via convex optimization", "author": ["D. Garber", "E. Hazan"], "venue": "arXiv preprint arXiv:1509.05647.", "citeRegEx": "Garber and Hazan,? 2015", "shortCiteRegEx": "Garber and Hazan", "year": 2015}, {"title": "Matrix computations, volume 3", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "JHU Press.", "citeRegEx": "Golub and Loan,? 2012", "shortCiteRegEx": "Golub and Loan", "year": 2012}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "Martinsson", "P.-G.", "J.A. Tropp"], "venue": "SIAM review, 53(2):217\u2013 288.", "citeRegEx": "Halko et al\\.,? 2011", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "venue": "Neural computation, 16(12):2639\u20132664.", "citeRegEx": "Hardoon et al\\.,? 2004", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Methods of conjugate gradients for solving linear systems, volume 49", "author": ["M.R. Hestenes", "E. Stiefel"], "venue": "NBS.", "citeRegEx": "Hestenes and Stiefel,? 1952", "shortCiteRegEx": "Hestenes and Stiefel", "year": 1952}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, 28(3/4):321\u2013377.", "citeRegEx": "Hotelling,? 1936", "shortCiteRegEx": "Hotelling", "year": 1936}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["C. Jin", "S.M. Kakade", "C. Musco", "P. Netrapalli", "A. Sidford"], "venue": "CoRR, abs/1510.08896.", "citeRegEx": "Jin et al\\.,? 2015", "shortCiteRegEx": "Jin et al\\.", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "NIPS2013, pages 315\u2013323.", "citeRegEx": "Johnson and Zhang,? 2013", "shortCiteRegEx": "Johnson and Zhang", "year": 2013}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["S.M. Kakade", "D.P. Foster"], "venue": "Learning theory, pages 82\u201396. Springer.", "citeRegEx": "Kakade and Foster,? 2007", "shortCiteRegEx": "Kakade and Foster", "year": 2007}, {"title": "Discriminative features via generalized eigenvectors", "author": ["N. Karampatziakis", "P. Mineiro"], "venue": "arXiv preprint arXiv:1310.1934.", "citeRegEx": "Karampatziakis and Mineiro,? 2013", "shortCiteRegEx": "Karampatziakis and Mineiro", "year": 2013}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A universal catalyst for first-order optimization", "author": ["H. Lin", "J. Mairal", "Z. Harchaoui"], "venue": "arXiv preprint arXiv:1506.02186.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Large scale canonical correlation analysis with iterative least squares", "author": ["Y. Lu", "D.P. Foster"], "venue": "Advances in Neural Information Processing Systems, pages 91\u201399.", "citeRegEx": "Lu and Foster,? 2014", "shortCiteRegEx": "Lu and Foster", "year": 2014}, {"title": "Finding Linear Structure in Large Datasets with Scalable Canonical Correlation Analysis", "author": ["Z. Ma", "Y. Lu", "D.P. Foster"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, JMLR Proceedings.", "citeRegEx": "Ma et al\\.,? 2015", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Nonparametric Canonical Correlation Analysis", "author": ["T. Michaeli", "W. Wang", "K. Livescu"], "venue": "CoRR, abs/1511.04839.", "citeRegEx": "Michaeli et al\\.,? 2015", "shortCiteRegEx": "Michaeli et al\\.", "year": 2015}, {"title": "Stronger approximate singular value decomposition via the block lanczos and power methods", "author": ["C. Musco", "C. Musco"], "venue": "arXiv preprint arXiv:1504.05477.", "citeRegEx": "Musco and Musco,? 2015", "shortCiteRegEx": "Musco and Musco", "year": 2015}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady, volume 27, pages 372\u2013376.", "citeRegEx": "Nesterov,? 1983", "shortCiteRegEx": "Nesterov", "year": 1983}, {"title": "Core-sets for canonical correlation analysis", "author": ["S. Paul"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 1887\u20131890. ACM.", "citeRegEx": "Paul,? 2015", "shortCiteRegEx": "Paul", "year": 2015}, {"title": "A randomized algorithm for principal component analysis", "author": ["V. Rokhlin", "A. Szlam", "M. Tygert"], "venue": "SIAM Journal on Matrix Analysis and Applications, 31(3):1100\u20131124.", "citeRegEx": "Rokhlin et al\\.,? 2009", "shortCiteRegEx": "Rokhlin et al\\.", "year": 2009}, {"title": "Non-asymptotic theory of random matrices: extreme singular values", "author": ["M. Rudelson", "R. Vershynin"], "venue": "arXiv preprint arXiv:1003.2990.", "citeRegEx": "Rudelson and Vershynin,? 2010", "shortCiteRegEx": "Rudelson and Vershynin", "year": 2010}, {"title": "Conducting and interpreting canonical correlation analysis in personality research: A user-friendly primer", "author": ["A. Sherry", "R.K. Henson"], "venue": "Journal of personality assessment, 84(1):37\u201348.", "citeRegEx": "Sherry and Henson,? 2005", "shortCiteRegEx": "Sherry and Henson", "year": 2005}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "author": ["J.R. Shewchuk"], "venue": "Technical report, Pittsburgh, PA, USA.", "citeRegEx": "Shewchuk,? 1994", "shortCiteRegEx": "Shewchuk", "year": 1994}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 22(8):888\u2013905.", "citeRegEx": "Shi and Malik,? 2000", "shortCiteRegEx": "Shi and Malik", "year": 2000}, {"title": "Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems", "author": ["D.A. Spielman", "Teng", "S.-H."], "venue": "Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 81\u201390. ACM.", "citeRegEx": "Spielman et al\\.,? 2004", "shortCiteRegEx": "Spielman et al\\.", "year": 2004}, {"title": "Stochastic optimization for deep CCA via nonlinear orthogonal iterations", "author": ["W. Wang", "R. Arora", "K. Livescu", "N. Srebro"], "venue": "volume abs/1510.02054. 16", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Large-Scale Approximate Kernel Canonical Correlation Analysis", "author": ["W. Wang", "K. Livescu"], "venue": "CoRR, abs/1511.04773.", "citeRegEx": "Wang and Livescu,? 2015", "shortCiteRegEx": "Wang and Livescu", "year": 2015}, {"title": "Globally convergent stochastic optimization for canonical correlation analysis", "author": ["W. Wang", "J. Wang", "N. Srebro"], "venue": "arXiv preprint arXiv:1604.01870.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["D.M. Witten", "R. Tibshirani", "T. Hastie"], "venue": "Biostatistics, page kxp008. 17", "citeRegEx": "Witten et al\\.,? 2009", "shortCiteRegEx": "Witten et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 12, "context": "This paper considers the problem of canonical-correlation analysis (CCA) (Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a pair of symmetric matrices.", "startOffset": 73, "endOffset": 90}, {"referenceID": 30, "context": "These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009).", "startOffset": 139, "endOffset": 203}, {"referenceID": 10, "context": "These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009).", "startOffset": 139, "endOffset": 203}, {"referenceID": 35, "context": "These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009).", "startOffset": 139, "endOffset": 203}, {"referenceID": 1, "context": "Canonical-correlation analysis (CCA) and the generalized eigenvector problem are fundamental problems in scientific computing, data analysis, and statistics (Barnett and Preisendorfer, 1987; Friman et al., 2001).", "startOffset": 157, "endOffset": 211}, {"referenceID": 5, "context": "Canonical-correlation analysis (CCA) and the generalized eigenvector problem are fundamental problems in scientific computing, data analysis, and statistics (Barnett and Preisendorfer, 1987; Friman et al., 2001).", "startOffset": 157, "endOffset": 211}, {"referenceID": 15, "context": "Algorithms for solving them are commonly used to extract features to compare and contrast large data sets and are used commonly in regression (Kakade and Foster, 2007), clustering (Chaudhuri et al.", "startOffset": 142, "endOffset": 167}, {"referenceID": 3, "context": "Algorithms for solving them are commonly used to extract features to compare and contrast large data sets and are used commonly in regression (Kakade and Foster, 2007), clustering (Chaudhuri et al., 2009), classification (Karampatziakis and Mineiro, 2013), word embeddings (Dhillon et al.", "startOffset": 180, "endOffset": 204}, {"referenceID": 16, "context": ", 2009), classification (Karampatziakis and Mineiro, 2013), word embeddings (Dhillon et al.", "startOffset": 24, "endOffset": 58}, {"referenceID": 4, "context": ", 2009), classification (Karampatziakis and Mineiro, 2013), word embeddings (Dhillon et al., 2011) and more.", "startOffset": 76, "endOffset": 98}, {"referenceID": 1, "context": "Despite the prevalence of these problems and the breadth of research on solving them in practice ((Barnett and Preisendorfer, 1987; Barnston and Ropelewski, 1992; Sherry and Henson, 2005; Karampatziakis and Mineiro, 2013) to name a few), there are relatively few results on obtaining provably efficient algorithms.", "startOffset": 98, "endOffset": 221}, {"referenceID": 2, "context": "Despite the prevalence of these problems and the breadth of research on solving them in practice ((Barnett and Preisendorfer, 1987; Barnston and Ropelewski, 1992; Sherry and Henson, 2005; Karampatziakis and Mineiro, 2013) to name a few), there are relatively few results on obtaining provably efficient algorithms.", "startOffset": 98, "endOffset": 221}, {"referenceID": 28, "context": "Despite the prevalence of these problems and the breadth of research on solving them in practice ((Barnett and Preisendorfer, 1987; Barnston and Ropelewski, 1992; Sherry and Henson, 2005; Karampatziakis and Mineiro, 2013) to name a few), there are relatively few results on obtaining provably efficient algorithms.", "startOffset": 98, "endOffset": 221}, {"referenceID": 16, "context": "Despite the prevalence of these problems and the breadth of research on solving them in practice ((Barnett and Preisendorfer, 1987; Barnston and Ropelewski, 1992; Sherry and Henson, 2005; Karampatziakis and Mineiro, 2013) to name a few), there are relatively few results on obtaining provably efficient algorithms.", "startOffset": 98, "endOffset": 221}, {"referenceID": 0, "context": "Can we develop simple iterative practical methods that solve this problem in close to linear time when k is small and the condition number and eigenvalue gaps are bounded? While there has been recent work on solving these problems using iterative methods (Avron et al., 2014; Paul, 2015; Lu and Foster, 2014; Ma et al., 2015) we are unaware of previous provable global convergence results and more strongly, linearly convergent scalable algorithms.", "startOffset": 255, "endOffset": 325}, {"referenceID": 25, "context": "Can we develop simple iterative practical methods that solve this problem in close to linear time when k is small and the condition number and eigenvalue gaps are bounded? While there has been recent work on solving these problems using iterative methods (Avron et al., 2014; Paul, 2015; Lu and Foster, 2014; Ma et al., 2015) we are unaware of previous provable global convergence results and more strongly, linearly convergent scalable algorithms.", "startOffset": 255, "endOffset": 325}, {"referenceID": 19, "context": "Can we develop simple iterative practical methods that solve this problem in close to linear time when k is small and the condition number and eigenvalue gaps are bounded? While there has been recent work on solving these problems using iterative methods (Avron et al., 2014; Paul, 2015; Lu and Foster, 2014; Ma et al., 2015) we are unaware of previous provable global convergence results and more strongly, linearly convergent scalable algorithms.", "startOffset": 255, "endOffset": 325}, {"referenceID": 20, "context": "Can we develop simple iterative practical methods that solve this problem in close to linear time when k is small and the condition number and eigenvalue gaps are bounded? While there has been recent work on solving these problems using iterative methods (Avron et al., 2014; Paul, 2015; Lu and Foster, 2014; Ma et al., 2015) we are unaware of previous provable global convergence results and more strongly, linearly convergent scalable algorithms.", "startOffset": 255, "endOffset": 325}, {"referenceID": 26, "context": "While there has been limited previous work on provably solving CCA and generalized eigenvectors, we note that there is an impressive body of literature on performing PCA(Rokhlin et al., 2009; Halko et al., 2011; Musco and Musco, 2015; Garber and Hazan, 2015; Jin et al., 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 169, "endOffset": 276}, {"referenceID": 9, "context": "While there has been limited previous work on provably solving CCA and generalized eigenvectors, we note that there is an impressive body of literature on performing PCA(Rokhlin et al., 2009; Halko et al., 2011; Musco and Musco, 2015; Garber and Hazan, 2015; Jin et al., 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 169, "endOffset": 276}, {"referenceID": 23, "context": "While there has been limited previous work on provably solving CCA and generalized eigenvectors, we note that there is an impressive body of literature on performing PCA(Rokhlin et al., 2009; Halko et al., 2011; Musco and Musco, 2015; Garber and Hazan, 2015; Jin et al., 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 169, "endOffset": 276}, {"referenceID": 7, "context": "While there has been limited previous work on provably solving CCA and generalized eigenvectors, we note that there is an impressive body of literature on performing PCA(Rokhlin et al., 2009; Halko et al., 2011; Musco and Musco, 2015; Garber and Hazan, 2015; Jin et al., 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 169, "endOffset": 276}, {"referenceID": 13, "context": "While there has been limited previous work on provably solving CCA and generalized eigenvectors, we note that there is an impressive body of literature on performing PCA(Rokhlin et al., 2009; Halko et al., 2011; Musco and Musco, 2015; Garber and Hazan, 2015; Jin et al., 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 169, "endOffset": 276}, {"referenceID": 11, "context": ", 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 56, "endOffset": 125}, {"referenceID": 24, "context": ", 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 56, "endOffset": 125}, {"referenceID": 20, "context": "There has been much recent interest in designing scalable algorithms for CCA(Ma et al., 2015; Wang et al., 2015; Wang and Livescu, 2015; Michaeli et al., 2015).", "startOffset": 76, "endOffset": 159}, {"referenceID": 32, "context": "There has been much recent interest in designing scalable algorithms for CCA(Ma et al., 2015; Wang et al., 2015; Wang and Livescu, 2015; Michaeli et al., 2015).", "startOffset": 76, "endOffset": 159}, {"referenceID": 33, "context": "There has been much recent interest in designing scalable algorithms for CCA(Ma et al., 2015; Wang et al., 2015; Wang and Livescu, 2015; Michaeli et al., 2015).", "startOffset": 76, "endOffset": 159}, {"referenceID": 22, "context": "There has been much recent interest in designing scalable algorithms for CCA(Ma et al., 2015; Wang et al., 2015; Wang and Livescu, 2015; Michaeli et al., 2015).", "startOffset": 76, "endOffset": 159}, {"referenceID": 35, "context": "Heuristic-based approachs (Witten et al., 2009; Lu and Foster, 2014) compute efficiently, but only give suboptimal result due to coarse approximation.", "startOffset": 26, "endOffset": 68}, {"referenceID": 19, "context": "Heuristic-based approachs (Witten et al., 2009; Lu and Foster, 2014) compute efficiently, but only give suboptimal result due to coarse approximation.", "startOffset": 26, "endOffset": 68}, {"referenceID": 20, "context": "The work in (Ma et al., 2015) provides one natural iterative procedure, where the per iterate computational complexity is low.", "startOffset": 12, "endOffset": 29}, {"referenceID": 20, "context": "Also of note is that many recent algorithms (Ma et al., 2015; Wang et al., 2015) have mini-batch variations, but there\u2019s no guarantees for mini-batch style algorithm for CCA yet.", "startOffset": 44, "endOffset": 80}, {"referenceID": 32, "context": "Also of note is that many recent algorithms (Ma et al., 2015; Wang et al., 2015) have mini-batch variations, but there\u2019s no guarantees for mini-batch style algorithm for CCA yet.", "startOffset": 44, "endOffset": 80}, {"referenceID": 20, "context": "S-AppGrad (Ma et al., 2015) \u00d5( \u03c1 log 1 \u01eb )", "startOffset": 10, "endOffset": 27}, {"referenceID": 34, "context": "Subsequent to the submission of this paper, we learned of the closely related work in (Wang et al., 2016), which presents a number of additional interesting results.", "startOffset": 86, "endOffset": 105}, {"referenceID": 20, "context": "(Ma et al., 2015) only shows local convergence for S-AppGrad.", "startOffset": 0, "endOffset": 17}, {"referenceID": 34, "context": "This table was inspired by (Wang et al., 2016) in order to facilitate comparison to existing work.", "startOffset": 27, "endOffset": 46}, {"referenceID": 14, "context": "(AGD), it is immediate to apply Theorem 7 and give the corresponding rates if we instantiate it by other popular algorithms, including gradient descent (GD), stochastic variance reduction (SVRG) (Johnson and Zhang, 2013), and its accelerated version (ASVRG) (Frostig et al.", "startOffset": 195, "endOffset": 220}, {"referenceID": 6, "context": "(AGD), it is immediate to apply Theorem 7 and give the corresponding rates if we instantiate it by other popular algorithms, including gradient descent (GD), stochastic variance reduction (SVRG) (Johnson and Zhang, 2013), and its accelerated version (ASVRG) (Frostig et al., 2015; Lin et al., 2015).", "startOffset": 258, "endOffset": 298}, {"referenceID": 18, "context": "(AGD), it is immediate to apply Theorem 7 and give the corresponding rates if we instantiate it by other popular algorithms, including gradient descent (GD), stochastic variance reduction (SVRG) (Johnson and Zhang, 2013), and its accelerated version (ASVRG) (Frostig et al., 2015; Lin et al., 2015).", "startOffset": 258, "endOffset": 298}, {"referenceID": 29, "context": "Moreover, it is well known that any method which starts at m and iteratively applies M to linear combinations of the points computed so far must apply M at least \u03a9( \u221a \u03ba(B)) in order to halve the error in the standard norm for the problem (Shewchuk, 1994).", "startOffset": 238, "endOffset": 254}, {"referenceID": 17, "context": "MNIST dataset(LeCun et al., 1998) consists of 60,000 handwritten digits from 0 to 9.", "startOffset": 13, "endOffset": 33}, {"referenceID": 21, "context": "17 million tokens and a vocabulary size of 43k(Marcus et al., 1993), which has already been used to successfully learn the word embedding by CCA(Dhillon et al.", "startOffset": 46, "endOffset": 67}, {"referenceID": 4, "context": ", 1993), which has already been used to successfully learn the word embedding by CCA(Dhillon et al., 2011).", "startOffset": 84, "endOffset": 106}, {"referenceID": 20, "context": "For experiments in this section, we follow the setting of (Ma et al., 2015).", "startOffset": 58, "endOffset": 75}, {"referenceID": 20, "context": "We compare our algorithm to S-AppGrad (Ma et al., 2015) which is an iterative algorithm and PCA-CCA (Ma et al.", "startOffset": 38, "endOffset": 55}, {"referenceID": 20, "context": ", 2015) which is an iterative algorithm and PCA-CCA (Ma et al., 2015), NW-CCA (Witten et al.", "startOffset": 52, "endOffset": 69}, {"referenceID": 35, "context": ", 2015), NW-CCA (Witten et al., 2009) and DW-CCA (Lu and Foster, 2014) which are one-shot estimation procedures.", "startOffset": 16, "endOffset": 37}, {"referenceID": 19, "context": ", 2009) and DW-CCA (Lu and Foster, 2014) which are one-shot estimation procedures.", "startOffset": 19, "endOffset": 40}], "year": 2016, "abstractText": "This paper considers the problem of canonical-correlation analysis (CCA) (Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a pair of symmetric matrices. These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009). We provide simple iterative algorithms, with improved runtimes, for solving these problems that are globally linearly convergent with moderate dependencies on the condition numbers and eigenvalue gaps of the matrices involved. We obtain our results by reducing CCA to the top-k generalized eigenvector problem. We solve this problem through a general framework that simply requires black box access to an approximate linear system solver. Instantiating this framework with accelerated gradient descent we obtain a running time of O ( zk \u221a \u03ba \u03c1 log(1/\u01eb) log (k\u03ba/\u03c1) ) where z is the total number of nonzero entries, \u03ba is the condition number and \u03c1 is the relative eigenvalue gap of the appropriate matrices. Our algorithm is linear in the input size and the number of components k up to a log(k) factor. This is essential for handling large-scale matrices that appear in practice. To the best of our knowledge this is the first such algorithm with global linear convergence. We hope that our results prompt further research and ultimately improve the practical running time for performing these important data analysis procedures on large data sets.", "creator": "LaTeX with hyperref package"}}}