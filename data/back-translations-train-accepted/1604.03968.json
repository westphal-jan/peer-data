{"id": "1604.03968", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2016", "title": "Visual Storytelling", "abstract": "We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.", "histories": [["v1", "Wed, 13 Apr 2016 20:27:43 GMT  (3042kb,D)", "http://arxiv.org/abs/1604.03968v1", "to appear in NAACL 2016"]], "COMMENTS": "to appear in NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["ting-hao huang", "francis ferraro", "nasrin mostafazadeh", "ishan misra", "aishwarya agrawal", "jacob devlin", "ross b girshick", "xiaodong he", "pushmeet kohli", "dhruv batra", "c lawrence zitnick", "devi parikh", "lucy vanderwende", "michel galley", "margaret mitchell"], "accepted": true, "id": "1604.03968"}, "pdf": {"name": "1604.03968.pdf", "metadata": {"source": "CRF", "title": "Visual Storytelling", "authors": ["Ting-Hao (Kenneth) Huang", "Francis Ferraro", "Nasrin Mostafazadeh", "Ishan Misra", "Aishwarya Agrawal", "Jacob Devlin", "Ross Girshick", "Xiaodong He", "Pushmeet Kohli", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh", "Lucy Vanderwende", "Michel Galley", "Margaret Mitchell"], "emails": ["jdevlin@microsoft.com", "lucyv@microsoft.com", "mgalley@microsoft.com", "memitc@microsoft.com"], "sections": [{"heading": null, "text": "The first release of this dataset, ARE 1 v.1, includes 81,743 unique photos in 20,211 sequences that focus on both the descriptive (caption) and storytelling language. We establish several strong baselines for the storytelling task and motivate an automatic metric to measure progress. Modeling concrete descriptions and pictorial and social language, as provided in this dataset and storytelling task, has the potential to move artificial intelligence from a basic understanding of typical visual scenes to an increasingly human understanding of grounded event structure and subjective expression."}, {"heading": "1 Introduction", "text": "Beyond the understanding of simple objects and concrete scenes lies the interpretation of causal structures; there is a sense of visual input to combine disparate moments, since they produce a coherent narrative of events over time, requiring the transition from looking at individual images - static moments without context - to image sequences that depict events, how they occur and change. On the vision side, the transition from single images to images in context helps us to create an artificial intelligence (AI) that can think about a visual moment in light of what they have already seen. On the language side, the transition from a literary description to a narrative helps to learn more evaluation, entertaining and abstract images, T.H. and F. contributed equally to this work. 1Sequential images narrative datasets are made available on sind.ai.language This is the difference between, for example, \"sitting next to each other\" and having a good time. \""}, {"heading": "2 Motivation and Related Work", "text": "Work in the fields of vision and language has exploded: researchers are studying captions (Lin et al., 2014; Carpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), answering questions (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski et al., 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al., 2013), and visual concepts (Krishna et al., 2016; Fang et al., 2015). This work focuses on the direct, literal description of image contents. While this is an encouraging first step in linking vision and language, it is far removed from the skills that intelligent agents need for naturalistic interactions. There is a considerable difference that is still unexplored, the visual image that is a \"remark\" that is found in a space that most \"- a remark of a space."}, {"heading": "3 Dataset Construction", "text": "This year, more than ever before in the history of the city, where it has reached the point where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place is a place, where it is a place is a place, where it is a place is a place, where it is a place is a place is a place, where it is a place is a place, where it is a place is a place is a place, where it is a place is a place is a place, where it is a place is a place, where it is a place is a place is a place, where it is a place is a place, where it is a place is a place, where it is a place is a place is a place, where it is a place is a place, where it is a place is a place, where it is, where it is a place is a place, where it is a place is a place, where it is a place, where it is a place is a place is a place, where it is, where it is a place is a place is a place, where it is a place, where it is, where it is a place is, where it is"}, {"heading": "4 Data Analysis", "text": "Our data set includes 10,117 Flickr albums with 210,819 unique photos. Each album has an average of 20.8 photos (\u03c3 = 9.0). The average time span of each album is 7.9 hours (\u03c3 = 11.4). Further details on each level of the data set are given in Table 2.6We use normalized, pointed mutual information to identify the words that are most closely related to each level (Table 3). Top words for descriptions-4https: / / github.com / tylin / coco-ui 5 We use these names that are at least 10,000 points.https: / / ssa.gov / oact / babynames / names.zip 6We exclude words that are only seen once. In-isolation reflect an unclear context: references to people often do not lack social specificity because people are simply referred to as \"man\" or \"woman.\" Individual images often do not convey much information about underlying events or actions that result in unambiguous use, etc. \""}, {"heading": "5 Automatic Evaluation Metric", "text": "Given the nature of the complex task of storytelling, the best and most reliable assessment for assessing the quality of stories generated is human judgment. However, automatic assessment metrics are useful for identifying rapid progress. To better understand which metric could serve as a proxy for human assessment, we calculate pairs of correlation coefficients between automatic metrics and human assessments based on 3,000 stories sampled from the SIS training set. 7 For human assessments, we re-crowdsourced on MTurk and ask five judges per story to rate how strongly they agree with the statement, \"If these were my photos, I would like to use a story like this to share my experiences with my friends.\" 7 We take the average of the five assessments as the final grade for the story. For the automatic metrics, we use METEOR, 8 smoothed BLEU (Lin and Och, 2004), and Skip-Thoughts (each story given a Kiros for similarities in 2015)."}, {"heading": "6 Baseline Experiments", "text": "We report on basic experiments in the creation of storylines in the SIS series and on the testing of half of the SIS validation system (valtest). An example from each system is presented in Table 5. To highlight some differences between the storylines and the labels, we also have to deal with the descriptions of the individual storylines, which of course extend the caption per picture, instead of capturing them in the order of the individual storylines. The results are shown in the table. (2015) and in the pictures of the individual storylines of the individual storylines. (2014) In order to illustrate the storylines of the individual storylines, we have set an image sequence in the order of recurring neuronal networks. (RNN) This is of course used in reverse order of the storylines of the individual storylines of the individual storylines of the individual storylines and storylines of the individual storylines of the individual storylines."}, {"heading": "7 Conclusion and Future Work", "text": "We have introduced the first sequential vision-to-language dataset, which progressively evolves from images in isolation to stories in sequence. We argue that modelling the more figurative and social language captured in this dataset is essential to the evolution of AI towards a more human understanding. We have created several strong foundations for the task of visual storytelling and motivated METEOR as an automatic metric to assess progress in this progressing task."}], "references": [{"title": "Vqa: Visual question answering", "author": ["C. Lawrence Zitnick", "Devi Parikh."], "venue": "International Conference on Computer Vision (ICCV).", "citeRegEx": "Zitnick and Parikh.,? 2015", "shortCiteRegEx": "Zitnick and Parikh.", "year": 2015}, {"title": "D\u00e9j\u00e0 image-captions: A corpus of expressive descriptions in repetition", "author": ["Chen et al.2015] Jianfu Chen", "Polina Kuznetsova", "David Warren", "Yejin Choi"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Language models for image captioning: The quirks and what works", "author": ["Devlin et al.2015] Jacob Devlin", "Hao Cheng", "Hao Fang", "Saurabh Gupta", "Li Deng", "Xiaodong He", "Geoffrey Zweig", "Margaret Mitchell"], "venue": "In Proceedings of the 53rd Annual Meeting of the As-", "citeRegEx": "Devlin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Image description using visual dependency representations", "author": ["Elliott", "Keller2013] Desmond Elliott", "Frank Keller"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Elliott et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2013}, {"title": "A survey of current datasets for vision and language research", "author": ["Nasrin Mostafazadeh", "Ting-Hao K. Huang", "Lucy Vanderwende", "Jacob Devlin", "Michel Galley", "Margaret Mitchell"], "venue": null, "citeRegEx": "Ferraro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ferraro et al\\.", "year": 2015}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["Gao et al.2015] Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "venue": null, "citeRegEx": "Gao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Fei-Fei2015] Andrej Karpathy", "Li FeiFei"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Visual genome: Connecting language and vision", "author": ["Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalanditis", "Li-Jia Li", "David A Shamma", "Michael Bernstein", "Li Fei-Fei"], "venue": null, "citeRegEx": "Krishna et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishna et al\\.", "year": 2016}, {"title": "A diversitypromoting objective function for neural conversation models", "author": ["Li et al.2016] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": "NAACL HLT 2016", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skipbigram statistics", "author": ["Lin", "Och2004] Chin-Yew Lin", "Franz Josef Och"], "venue": "In Proceedings of the 42Nd Annual Meeting on Association for Computational Linguis-", "citeRegEx": "Lin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin et al.2014] Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Malinowski", "Fritz2014] Mateusz Malinowski", "Mario Fritz"], "venue": null, "citeRegEx": "Malinowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2014}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Associa-", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Video event understanding using natural language descriptions", "author": ["Percy Liang", "Li Fei-Fei"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Ramanathan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2013}, {"title": "Exploring models and data for image question answering", "author": ["Ren et al.2015] Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "Advances in Neural Information Processing", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Recognition using visual phrases", "author": ["Sadeghi", "Ali Farhadi"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Sadeghi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sadeghi et al\\.", "year": 2011}, {"title": "The new data and new challenges in multimedia research", "author": ["Thomee et al.2015] Bart Thomee", "David A Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li"], "venue": "arXiv preprint arXiv:1503.01817", "citeRegEx": "Thomee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thomee et al\\.", "year": 2015}, {"title": "Show and tell: a neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Embers of society: Firelight talk among the ju/hoansi bushmen", "author": ["Polly W Wiessner"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Wiessner.,? \\Q2014\\E", "shortCiteRegEx": "Wiessner.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Young et al.2014] Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al.", "startOffset": 85, "endOffset": 235}, {"referenceID": 19, "context": "Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al.", "startOffset": 85, "endOffset": 235}, {"referenceID": 21, "context": "Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al.", "startOffset": 85, "endOffset": 235}, {"referenceID": 1, "context": "Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al.", "startOffset": 85, "endOffset": 235}, {"referenceID": 22, "context": "Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al.", "startOffset": 85, "endOffset": 235}, {"referenceID": 15, "context": ", 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al.", "startOffset": 54, "endOffset": 138}, {"referenceID": 6, "context": ", 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al.", "startOffset": 54, "endOffset": 138}, {"referenceID": 14, "context": ", 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al., 2013), and visual concepts (Krishna et al.", "startOffset": 101, "endOffset": 126}, {"referenceID": 8, "context": ", 2013), and visual concepts (Krishna et al., 2016; Fang et al., 2015).", "startOffset": 29, "endOffset": 70}, {"referenceID": 20, "context": "Storytelling itself is one of the oldest known human activities (Wiessner, 2014), providing a way to educate, preserve culture, instill morals, and share advice; focusing AI research towards this task therefore has the potential to bring about more humanlike intelligence and understanding.", "startOffset": 64, "endOffset": 80}, {"referenceID": 17, "context": "\u201d Using the Flickr data release (Thomee et al., 2015), we aggregate 5-grams of photo titles and descriptions, using Stanford CoreNLP (Manning et al.", "startOffset": 32, "endOffset": 53}, {"referenceID": 13, "context": ", 2015), we aggregate 5-grams of photo titles and descriptions, using Stanford CoreNLP (Manning et al., 2014) to extract possessive dependency patterns.", "startOffset": 87, "endOffset": 109}, {"referenceID": 11, "context": "In both DII and DIS tasks, workers are asked to follow the instructions for image captioning proposed in MS COCO (Lin et al., 2014) such as describe all the important parts.", "startOffset": 113, "endOffset": 131}, {"referenceID": 5, "context": "5 Table 2: A summary of our dataset, following the proposed analyses of Ferraro et al. (2015), including the Frazier and Yngve measures of syntactic complexity.", "startOffset": 72, "endOffset": 94}, {"referenceID": 2, "context": "We use Gated Recurrent Units (GRUs) (Cho et al., 2014) for both the image encoder and story decoder.", "startOffset": 36, "endOffset": 54}, {"referenceID": 3, "context": "In the baseline system, we generate the story using a simple beam search (size=10), which has been successful in image captioning previously (Devlin et al., 2015).", "startOffset": 141, "endOffset": 162}, {"referenceID": 9, "context": "This is a predictable result given the label bias problem inherent in maximum likelihood training; recent work has looked at ways to address this issue directly (Li et al., 2016).", "startOffset": 161, "endOffset": 178}, {"referenceID": 2, "context": "To train the story generation model, we use a sequence-to-sequence recurrent neural net (RNN) approach, which naturally extends the single-image captioning technique of Devlin et al. (2015) and Vinyals et al.", "startOffset": 169, "endOffset": 190}, {"referenceID": 2, "context": "To train the story generation model, we use a sequence-to-sequence recurrent neural net (RNN) approach, which naturally extends the single-image captioning technique of Devlin et al. (2015) and Vinyals et al. (2014) to multiple images.", "startOffset": 169, "endOffset": 216}], "year": 2016, "abstractText": "We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND1 v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.", "creator": "LaTeX with hyperref package"}}}