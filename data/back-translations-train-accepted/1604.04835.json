{"id": "1604.04835", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2016", "title": "SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions", "abstract": "Knowledge graph embedding represents the entities and relations as numerical vectors, and then knowledge analysis could be promoted as a numerical method. So far, most methods merely concentrate on the fact triples that are composed by the symbolic entities and relations, while the textual information which is supposed to be most critical in NLP could hardly play a reasonable role. For this end, this paper proposes the method SSP which jointly learns from the symbolic triples and textual descriptions. Our model could interact both two information sources by characterizing the correlations, by which means, the textual descriptions could make effects to discover semantic relevance and offer precise semantic embedding. Extensive experiments show our method achieves the substantial improvements against the state-of-the-art baselines on the tasks of knowledge graph completion and entity classification.", "histories": [["v1", "Sun, 17 Apr 2016 07:15:33 GMT  (79kb,D)", "https://arxiv.org/abs/1604.04835v1", "Submitted to ACL.2016"], ["v2", "Tue, 25 Oct 2016 02:39:15 GMT  (82kb,D)", "http://arxiv.org/abs/1604.04835v2", "Submitted to AAAI.2017"], ["v3", "Sat, 17 Jun 2017 04:33:41 GMT  (84kb,D)", "http://arxiv.org/abs/1604.04835v3", "Submitted to AAAI.2017"]], "COMMENTS": "Submitted to ACL.2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["han xiao 0005", "minlie huang", "lian meng", "xiaoyan zhu"], "accepted": true, "id": "1604.04835"}, "pdf": {"name": "1604.04835.pdf", "metadata": {"source": "META", "title": "SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions", "authors": ["Han Xiao", "Minlie Huang", "Lian Meng", "Xiaoyan Zhu"], "emails": ["bookman@vip.163.com;", "aihuang@tsinghua.edu.cn;", "zxy-dcs@tsinghua.edu.cn;", "mengl15@foxmail.com;"], "sections": [{"heading": "Introduction", "text": "In order to provide a numerical framework for the knowledge graph, a knowledge diagram that attempts to represent the symbols with vectors must always be represented in a continuous low-dimensional vector space. To this end, a number of embedding methods have been proposed, such as TransE (Bordes et al. 2013), PTransE (Lin, Liu and Sun 2015), PTransE (Lin, Liu and Sun). As a key branch of embedding models, the translation methods adopt the principle of translating the head into the tail by means of a relationship-specific vector."}, {"heading": "Related Work", "text": "We analyzed previous studies and divided the embedding methods into two areas: triple-only embedding models that use symbolic triples only, and \"TextAware\" embedding models that use textual descriptions."}, {"heading": "Triple-only Embedding Models", "text": "TransE (Bordes et al. 2013) is a groundbreaking work for this branch that better measures the plausibility of a triple and a smaller value. The following variants transform entities into different subspaces. ManifoldE (Xiao, Huang and Zhu 2016a) opens a classical branch in which a variety is applied to translation. TransH (Wang et al. 2014b) uses the relationship-specific hyperplane to lay the entities. TransR (Lin et al. 2015) uses the relationship-related matrix to rotate the embedding space. Similar research includes TransG (Xiao, Huang and Zhu et al. 2016b), TransD (Ji et al.) and TransM (Fan et al. 2014). Other research includes TransG (Xiao, Huang and Zhu), a structural knowledge structure (Wang et al. 2016b)."}, {"heading": "Text-aware Embedding Models", "text": "The \"Text-Aware\" embedding, which attempts to represent knowledge graphs with text information, generally goes back to NTN (Socher et al. 2013). NTN uses entity names and embeds an entity as average word embedding vectors of the name. (Wang et al. 2014a) attempts to align the knowledge graph with the corpus and then jointly aligns knowledge embedding and word embedding. However, the necessity of alignment information limits this method both in its performance and in its practical applicability. Therefore, (Zhong et al. 2015) proposes the \"Common\" method, which only aligns the free-base entity to the corresponding wiki page. DKRL (Xie et al. 2016) expands the translation-based embedding methods from the triple specific method to the \"Text-Aware\" model. More importantly, DKRL uses a general CNN structure to represent the textual state of knowledge by applying it expressively."}, {"heading": "Methodology", "text": "In this section we will first present our model and then offer two different perspectives to explain the advantages of our model. First, let us introduce some notations: All symbols h, t indicate the head and tail entity or the embedding of the entity from the triples, sh (or st) is the semantic vector generated from the texts, and d is the dimension of the embedding. The data involved in our model are the knowledge triples and the textual descriptions of entities. In experiments, we adopt the \"entity descriptions\" of Freebase and the textual definitions of orders as textual information."}, {"heading": "Model Description", "text": "The previous analysis in the introduction suggests to characterize the strong correlation between triples and texts. = > In terms of the interaction between the symbolic triples and textual descriptions, this paper attempts to limit the embedding method of a specific triple theme in the semantic subspace. In particular, we use a hyperplane with normal vectors. = S (sh, st) as the subspace in which the translation methods based on S: R2d 7 \u2192 Rd is the semantic theme of composition that will be discussed in the next section, and st is the header-specific and customized semantic vectors, according to the score function in the translation method, in which the translation-based methods of S: R2d 7 \u2192 Rd is the semantic theme that means the semantic composition function that will be discussed in the next section."}, {"heading": "Correlation Perspective", "text": "Specifically, our model attempts to place the loss of h \"\u2212 t on the hyperplane, where h\" = h + r is the translated head unit. Mathematically, a line is on a hyperplane, so are all the points of that line. Accordingly, the loss is on the hyperplane, which implies that head and tail are also on it, as a starting and ending point. Thus, there is an important limitation that the units should appear side by side in a triple space composed by the associated textual semantics. This limitation is implemented as a square form to characterize the strong correlation between texts and triples, in other words, to interact with both sources of information. A strong interaction between the textual descriptions and symbolic triples complements each other in a stronger semantic form that guarantees the semantic effects. More specifically, the embedding is decided in the training process not only by triples, but also by another semantic form based on our semantic basis."}, {"heading": "Semantic Perspective", "text": "There are two semantic effects for textual descriptions: the discovery of semantic relevance and the supply of precise semantic expressions. Our model characterizes the strong correlations with a semantic hyperplane that is able to take into account the advantages of both semantic effects. First, however, the semantically relevant units are approximately on a consistent hyperplane. Therefore, the loss vector between them (h \u2032 \u2212 t) is also around the hyperplane. Based on this geometric insight, if a head unit coincides with a negative tail, the triple is far away from the hyperplane, requiring a large loss to be classified. Conversely, even if a correct triple is a considerable loss, the score function after which is projected onto the hyperplane may be relatively smaller (or better), meaning the semantic relevance achieved from the texts promotes the embedding of the hyperplane."}, {"heading": "Objectives & Training", "text": "There are two parts in the objective function, each embed-specific and theme-specific. To balance the two parts, a hyper-parameter \u00b5 is introduced. Overall, the total loss is: L = Lembed + \u00b5Ltopic (1) It is noteworthy that there is only the first part in the default setting, where \u00b5 = 0 in fact.In the concept of the embed-related target, the rank-based hinged loss is applied, which means maximizing the discriminatory margin between the golden triples and the negative themes: L embedded = (h, r, t)."}, {"heading": "Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Datasets & General Settings", "text": "Our experiments are conducted on three public benchmark datasets, which are the subsets of WORDS and Freebase. As for the statistics of these datasets, we strongly recommend that readers refer to them (Xie et al. 2016) and (Lin et al. 2015). The entity descriptions of FB15K and FB20K are the same as DKRL (Xie et al. 2016), which each make up a small part of the corresponding wiki page. WN18 \"s textual information is the definitions we extract from the WORDER. It is noteworthy that Zero-Shot Learning involves FB20K, which is also created by the authors of the DKRL."}, {"heading": "Knowledge Graph Completion", "text": "The same protocol used in previous studies is applied 3. First, for each test triple (h, r, t), we replace the tail t (or the head h) with each unit e in the knowledge chart. Then, a probable evaluation of this corrupt triple margin includes the calculation using the score function fr (h, t). By evaluating these results in ascending order, we then get the rank of the original triple scale. The evaluation metrics are the average of the ranking as Mean Rank and the percentage of the tests triple whose rank is not greater than 10 (as HITS @ 10). This is called \"Raw,\" filtering out the corrupted triple scales that exist in education, validation or test datasets, this is the \"filter\" setting. If a corrupt triple scale exists in the knowledge chart, we rank it before the original triple scale, which is also correct to eliminate this effect, the filter is preferred. \""}, {"heading": "Entity Classification", "text": "This task is essentially a multi-label classification that focuses on predicting entity types, which is critical and widely used in many NLP & IR tasks (Neelakantan and Chang 2015).The entity in Freebase always has types, for example the entity types of the \"Scots\" are Human Language, Rosetta Languoid.We adopt the same data sets as DKRL, for the details of which we refer readers (Xie et al. 2016).Overall, this is a multi-label classification task with 50 classes, which means that the method should provide a set of types for each entity and not just a single type.Evaluation protocol In education, we use the concatenation of semantic vectors and embedding vectors (se, e) as an entity representation that is representative of the entity."}, {"heading": "Semantic Relevance Analysis", "text": "One advantage of modeling semantic relevance is the ability to correctly classify triples that could not be distinguished by mere use of information from symbolic triples. Therefore, we perform statistical analysis of the results in Link Prediction, as in the Table.4. The number in each cell means the number of triples whose rank is greater than m in TransE and smaller than n in our models. For example, the number 601 means that there are 601 triples whose ranks are less than 100 in SSP (p.), while their ranks are more than 500 in TransE. Note that SSP (p.) is the default setting and SSP (J.) the common setting. The statistical results indicate that many triples benefit from the semantic relevance that text descriptions provide. The experiments also justify the theoretical analysis of semantic relevance and demonstrate the effectiveness of our models."}, {"heading": "Precise Semantic Expression Analysis", "text": "To justify this assertion, we collected negative triples using link prediction, which are rated significantly better by TransE than the golden triples (i.e., these are hard examples of TransE), and then plotted the difference in the SSP score between each corresponding pair of negative and golden triples, as Fig.3. All of these triples are predicted incorrectly by TransE, but with precise semantic expression, our model distinguishes 82.0% (h) and 83.2% (joint) from them. In the histogram, the right bars indicate that SSP makes the right decision, while TransE fails and the left bars mean that both SSP and TransE fail. The experiments justify the theoretical analysis of the precise semantic expression and show the effectiveness of our models."}, {"heading": "Conclusion", "text": "In this paper, we propose the Knowledge Chart Embedding Model SSP, which collectively learns from symbolic triples and textual descriptions. SSP could interact between triples and texts by characterizing the strong correlations between fact triples and textual descriptions. Textual descriptions have a major impact on the discovery of semantic relevance and then provide a precise semantic expression. Extensive experiments show that our method achieves significant improvements over modern fundamentals, commendably supported in part by the National Basic Research Program (973 Program) under grant number 2013CB329403 and the National Science Foundation of China under grant number 61272227 / 61332007."}], "references": [{"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Communications of the ACM 55(4):77\u201384.", "citeRegEx": "Blei,? 2012", "shortCiteRegEx": "Blei", "year": 2012}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data, 1247\u20131250. ACM.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y Bengio"], "venue": "In Proceedings of the Twenty-fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "Advances in Neural Information Processing Systems, 2787\u20132795.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Transition-based knowledge graph embedding with relational mapping properties", "author": ["M. Fan", "Q. Zhou", "E. Chang", "T.F. Zheng"], "venue": "Proceedings of the 28th Pacific Asia Conference on Language, Information, and Computation, 328\u2013337.", "citeRegEx": "Fan et al\\.,? 2014", "shortCiteRegEx": "Fan et al\\.", "year": 2014}, {"title": "GAKE: Graph aware knowledge embedding", "author": ["J. Feng", "M. Huang", "Y. Yang", "X. Zhu"], "venue": "COLING 2016, 26th International Conference on Computational Linguistics.", "citeRegEx": "Feng et al\\.,? 2016", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, 249\u2013256.", "citeRegEx": "Glorot and Bengio,? 2010", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Semantically smooth knowledge graph embedding", "author": ["S. Guo", "Q. Wang", "B. Wang", "L. Wang", "L. Guo"], "venue": "Proceedings of ACL.", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "Learning to represent knowledge graphs with gaussian embedding", "author": ["S. He", "K. Liu", "G. Ji", "J. Zhao"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, 623\u2013632. ACM.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "A latent factor model for highly multi-relational data", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "G.R. Obozinski"], "venue": "Advances in Neural Information Processing Systems, 3167\u20133175.", "citeRegEx": "Jenatton et al\\.,? 2012", "shortCiteRegEx": "Jenatton et al\\.", "year": 2012}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Y. Lin", "Z. Liu", "M. Sun"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "HLTNAACL, 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM 38(11):39\u201341.", "citeRegEx": "Miller,? 1995", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "Inferring missing entity type instances for knowledge base completion: New dataset and methods", "author": ["A. Neelakantan", "Chang", "M.-W."], "venue": "arXiv preprint arXiv:1504.06658.", "citeRegEx": "Neelakantan et al\\.,? 2015", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "A threeway model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 809\u2013816.", "citeRegEx": "Nickel et al\\.,? 2011", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, 926\u2013934.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Exploring topic coherence over many models and many topics", "author": ["K. Stevens", "P. Kegelmeyer", "D. Andrzejewski", "D. Buttler"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 952\u2013961. As-", "citeRegEx": "Stevens et al\\.,? 2012", "shortCiteRegEx": "Stevens et al\\.", "year": 2012}, {"title": "Knowledge graph and text jointly embedding", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "EMNLP, 1591\u2013 1601. Citeseer.", "citeRegEx": "Wang et al\\.,? 2014a", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 1112\u20131119.", "citeRegEx": "Wang et al\\.,? 2014b", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge base completion using embeddings and rules", "author": ["Q. Wang", "B. Wang", "L. Guo"], "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "TransA: An adaptive approach for knowledge graph embedding", "author": ["H. Xiao", "M. Huang", "Y. Hao", "X. Zhu"], "venue": "arXiv preprint arXiv:1509.05490.", "citeRegEx": "Xiao et al\\.,? 2015", "shortCiteRegEx": "Xiao et al\\.", "year": 2015}, {"title": "From one point to a manifold: Knowledge graph embedding for precise link prediction", "author": ["H. Xiao", "M. Huang", "X. Zhu"], "venue": "Proceedings of the 25th International Joint Conference on Artificial Intelligence.", "citeRegEx": "Xiao et al\\.,? 2016a", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "TransG : A generative model for knowledge graph embedding", "author": ["H. Xiao", "M. Huang", "X. Zhu"], "venue": "Proceedings of the 29th international conference on computational linguistics. Association for Computational Linguistics.", "citeRegEx": "Xiao et al\\.,? 2016b", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Representation learning of knowledge graphs with entity descriptions", "author": ["R. Xie", "Z. Liu", "J. Jia", "H. Luan", "M. Sun"], "venue": null, "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Aligning knowledge and text embeddings by entity descriptions", "author": ["H. Zhong", "J. Zhang", "Z. Wang", "H. Wan", "Z. Chen"], "venue": "Proceedings of EMNLP, 267\u2013272.", "citeRegEx": "Zhong et al\\.,? 2015", "shortCiteRegEx": "Zhong et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "To this end, a number of embedding methods have been proposed, such as TransE (Bordes et al. 2013), PTransE (Lin, Liu, and Sun 2015), KG2E (He et al.", "startOffset": 78, "endOffset": 98}, {"referenceID": 8, "context": "2013), PTransE (Lin, Liu, and Sun 2015), KG2E (He et al. 2015), etc.", "startOffset": 46, "endOffset": 62}, {"referenceID": 25, "context": "The existing embedding methods with textual semantics such as DKRL (Xie et al. 2016) and \u201cJointly\u201d (Zhong et al.", "startOffset": 67, "endOffset": 84}, {"referenceID": 26, "context": "2016) and \u201cJointly\u201d (Zhong et al. 2015), have achieved much success.", "startOffset": 20, "endOffset": 39}, {"referenceID": 14, "context": "We evaluate the effectiveness of our model Semantic Subspace Projection (SSP) with two tasks on three benchmark datasets that are the subsets of Wordnet (Miller 1995) and Freebase (Bollacker et al.", "startOffset": 153, "endOffset": 166}, {"referenceID": 1, "context": "We evaluate the effectiveness of our model Semantic Subspace Projection (SSP) with two tasks on three benchmark datasets that are the subsets of Wordnet (Miller 1995) and Freebase (Bollacker et al. 2008).", "startOffset": 180, "endOffset": 203}, {"referenceID": 3, "context": "TransE (Bordes et al. 2013) is a pioneering work for this branch, which translates the head entity to the tail one by the relation vector, or h + r = t.", "startOffset": 7, "endOffset": 27}, {"referenceID": 20, "context": "TransH (Wang et al. 2014b) utilizes the relationspecific hyperplane to lay the entities.", "startOffset": 7, "endOffset": 26}, {"referenceID": 10, "context": "TransR (Lin et al. 2015) applies the relation-related matrix to rotate the embedding space.", "startOffset": 7, "endOffset": 24}, {"referenceID": 5, "context": "), (Feng et al. 2016) and TransM (Fan et al.", "startOffset": 3, "endOffset": 21}, {"referenceID": 4, "context": "2016) and TransM (Fan et al. 2014).", "startOffset": 17, "endOffset": 34}, {"referenceID": 7, "context": "SSE (Guo et al. 2015) aims at discovering the geometric structure of embedding topologies and then based on these assumptions, designs a semantically smoothing score function.", "startOffset": 4, "endOffset": 21}, {"referenceID": 8, "context": "Also, KG2E (He et al. 2015) involves probabilistic analysis to characterize the uncertainty concepts of knowledge graph.", "startOffset": 11, "endOffset": 27}, {"referenceID": 2, "context": "There are also some other works such as SE (Bordes et al. 2011), LFM (Jenatton et al.", "startOffset": 43, "endOffset": 63}, {"referenceID": 9, "context": "2011), LFM (Jenatton et al. 2012), NTN (Socher et al.", "startOffset": 11, "endOffset": 33}, {"referenceID": 17, "context": "2012), NTN (Socher et al. 2013) and RESCAL (Nickel, Tresp, and Kriegel 2011), TransA (Xiao et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 22, "context": "2013) and RESCAL (Nickel, Tresp, and Kriegel 2011), TransA (Xiao et al. 2015), etc.", "startOffset": 59, "endOffset": 77}, {"referenceID": 17, "context": "\u201cText-Aware\u201d Embedding, which attempts to representing knowledge graph with textual information, generally dates back to NTN (Socher et al. 2013).", "startOffset": 125, "endOffset": 145}, {"referenceID": 19, "context": "(Wang et al. 2014a) attempts to aligning the knowledge graph with the corpus then jointly conducting knowledge embedding and word embedding.", "startOffset": 0, "endOffset": 19}, {"referenceID": 26, "context": "Thus, (Zhong et al. 2015) proposes the \u201cJointly\u201d method that only aligns the freebase entity to the corresponding wiki-page.", "startOffset": 6, "endOffset": 25}, {"referenceID": 25, "context": "DKRL (Xie et al. 2016) extends the translationbased embedding methods from the triple-specific one to the \u201cText-Aware\u201d model.", "startOffset": 5, "endOffset": 22}, {"referenceID": 0, "context": "There are at least two methods that could be leveraged to generate the semantic vectors: topic model (Blei 2012) such as LSA, LDA, NMF (Stevens et al.", "startOffset": 101, "endOffset": 112}, {"referenceID": 18, "context": "There are at least two methods that could be leveraged to generate the semantic vectors: topic model (Blei 2012) such as LSA, LDA, NMF (Stevens et al. 2012) and word embedding such as CBOW (Mikolov, Yih, and Zweig 2013), Skip-Gram (Mikolov et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 12, "context": "2012) and word embedding such as CBOW (Mikolov, Yih, and Zweig 2013), Skip-Gram (Mikolov et al. 2013).", "startOffset": 80, "endOffset": 101}, {"referenceID": 20, "context": "Sampling Method\u201d as introduced in (Wang et al. 2014b) and the method selects the negative samples from the set of", "startOffset": 34, "endOffset": 53}, {"referenceID": 6, "context": "We initialize the embedding vectors by the similar methods used in the deep neural network (Glorot and Bengio 2010) and pre-train the topic model with Non-negative Matrix Factorization (NMF) (Stevens et al.", "startOffset": 91, "endOffset": 115}, {"referenceID": 18, "context": "We initialize the embedding vectors by the similar methods used in the deep neural network (Glorot and Bengio 2010) and pre-train the topic model with Non-negative Matrix Factorization (NMF) (Stevens et al. 2012).", "startOffset": 191, "endOffset": 212}, {"referenceID": 18, "context": "For the topic-related objective, we take the advantage of the NMF Topic Model (Stevens et al. 2012), which is both simple and effective.", "startOffset": 78, "endOffset": 99}, {"referenceID": 25, "context": "About the statistics of these datasets, we strongly suggest the readers to refer to (Xie et al. 2016) and (Lin et al.", "startOffset": 84, "endOffset": 101}, {"referenceID": 10, "context": "2016) and (Lin et al. 2015).", "startOffset": 10, "endOffset": 27}, {"referenceID": 25, "context": "The entity descriptions of FB15K and FB20K are the same as DKRL (Xie et al. 2016), each of which is a small part of the corresponding wiki-page.", "startOffset": 64, "endOffset": 81}, {"referenceID": 26, "context": "Note that \u201cJointly\u201d refers to (Zhong et al. 2015).", "startOffset": 30, "endOffset": 49}, {"referenceID": 25, "context": "We adopt the same datasets as DKRL, for the details of which, we refer readers to (Xie et al. 2016).", "startOffset": 82, "endOffset": 99}], "year": 2017, "abstractText": "Knowledge graph embedding represents entities and relations in knowledge graph as low-dimensional, continuous vectors, and thus enables knowledge graph compatible with machine learning models. Though there have been a variety of models for knowledge graph embedding, most methods merely concentrate on the fact triples, while supplementary textual descriptions of entities and relations have not been fully employed. To this end, this paper proposes the semantic space projection (SSP) model which jointly learns from the symbolic triples and textual descriptions. Our model builds interaction between the two information sources, and employs textual descriptions to discover semantic relevance and offer precise semantic embedding. Extensive experiments show that our method achieves substantial improvements against baselines on the tasks of knowledge graph completion and entity classification. Papers, Posters, Slides, Datasets and Codes: http://www.ibookman.net/conference.html", "creator": "TeX"}}}