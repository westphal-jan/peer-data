{"id": "1604.05073", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2016", "title": "Speed-Constrained Tuning for Statistical Machine Translation Using Bayesian Optimization", "abstract": "We address the problem of automatically finding the parameters of a statistical machine translation system that maximize BLEU scores while ensuring that decoding speed exceeds a minimum value. We propose the use of Bayesian Optimization to efficiently tune the speed-related decoding parameters by easily incorporating speed as a noisy constraint function. The obtained parameter values are guaranteed to satisfy the speed constraint with an associated confidence margin. Across three language pairs and two speed constraint values, we report overall optimization time reduction compared to grid and random search. We also show that Bayesian Optimization can decouple speed and BLEU measurements, resulting in a further reduction of overall optimization time as speed is measured over a small subset of sentences.", "histories": [["v1", "Mon, 18 Apr 2016 10:27:49 GMT  (273kb,D)", "http://arxiv.org/abs/1604.05073v1", "To appear at NAACL 2016"]], "COMMENTS": "To appear at NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniel beck", "adri\u00e0 de gispert", "gonzalo iglesias", "aurelien waite", "bill byrne"], "accepted": true, "id": "1604.05073"}, "pdf": {"name": "1604.05073.pdf", "metadata": {"source": "CRF", "title": "Speed-Constrained Tuning for Statistical Machine Translation Using Bayesian Optimization", "authors": ["Daniel Beck", "Adri\u00e0 de Gispert", "Gonzalo Iglesias", "Aurelien Waite", "Bill Byrne"], "emails": ["debeck1@sheffield.ac.uk", "agispert@sdl.com", "giglesias@sdl.com", "rwaite@sdl.com", "bbyrne@sdl.com"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Bayesian Optimization", "text": "We are interested in finding a global maximizer of an objective function f: \u03b8? = arg max \u03b8 \u0442 f (\u03b8) (1), where \u03b8 is a parameter vector from a search space. It is assumed that f does not have a simple closed shape, but can be evaluated at any \u03b8 point. In this essay, we take f as a BLEU score generated by an SMT system on a tuning set, and \u03b8 will be the PBMT decoder parameters. Bayesian Optimization is a powerful framework to address this problem efficiently. It works by defining an earlier model via f and evaluating it sequentially. Valuation points are selected to maximize the benefit of the measurement, as estimated by an acquisition function that exchanges the exploration of uncertain regions in the Middle East for the exploitation of promising regions, based on functional evaluations of all the points collected so far, as it is estimated by an acquisition function that replaces the exploration of unsafe regions in the Middle East against the exploitation of promising regions, based on functional evaluations of all the points collected so far, as it is not playable and is not viable."}, {"heading": "2.1 Prior Model", "text": "The first step in implementing BO is the definition of the previous model on the function of interest. While there are a number of different approaches in the literature, in this paper we follow the concepts presented in Snoek et al. (2012) and implemented in the Spearmint1 toolkit, which we will explain in detail in this paragraph.The previous over f is defined as a Gaussian process (GP) (Rasmussen and Williams, 2006): f-GP (m (\u03b8), k (\u03b8, \u03b8))) (2), where m and k are the mean and core functions (or covariance).The mean function is fixed on the zero constant function, as is common in GP models. This is not a major limitation, as the rear value above f will generally have a non-zero mean. We use the mate core, which makes little assumptions about functional smoothness (or covariance).The EU observations, we do not expect BLS values to differ from our assessments, but rather that in practice we expect noise levels to differ."}, {"heading": "2.2 Adding Constraints", "text": "The optimization problem of Equation 1 can be expanded to include an additional limitation for some measurements c (\u03b8): \u03b8? = arg max \u03b8 \u0445 f (\u03b8) s.t. c (\u03b8) > t (3) In our configuration c (\u03b8) is the decryption speed of a configuration \u03b8, and t is the minimum speed at which the decoder should run. This formulation assumes that c is deterministic in the face of a number of parameters. However, as we show in Section 3.2, velocity measurements are inherently noisy and return different values when using the same decoder parameters. Therefore, we follow Gelbart et al. (2014) and redefine Equation 3 by assuming a probable model p over c: \u03b8? = arg max."}, {"heading": "2.3 Acquisition Function", "text": "The previous model combined with observations results in a posterior distribution over f. The posterior mean gives information about potential optimizations in the areas we would like to exploit. Posterior variance encodes uncertainty in unknown regions, i.e. regions we want to explore. This exploration / exploitation trade-off is a fundamental aspect not only in BO, but in many other global optimization methods. Acquisition functions are heuristics that use information from the posterior to propose new valuation points. Of course, they encode the exploration / exploitation trade-off by taking full posterior information into account. A proposal is made by maximizing this function, which can be accomplished using standard optimization techniques, as they are much cheaper to propose new valuation points. Most acquisition functions used in the literature are based on improving the best assessment received so far, but this approach has been shown to be limited in 2014."}, {"heading": "2.4 Decoupling Constraints", "text": "In this scenario, BO could spend more time retrieving speed limitation values (since they are cheaper to obtain) and less time retrieving the BLEU lens. We use PESC as a capture function because it can easily handle decoupled constraints (Gelbart, 2015, paragraph 4.3). Effectively, we modify algorithm 1 to update either the target or the constraint posterior for each iteration, depending on what is achieved by maximizing PESC in line 3. This type of decoupling is not allowed by standard capture functions used in BO.on, and the constraint posterior p (c | D) does not depend on objective measurements."}, {"heading": "3 Speed Tuning Experiments", "text": "We report on translation results in three language pairs selected for the different challenges they present for SMT systems: Spanish-English, English-German, and Chinese-English. For each language pair, we use generic parallel data extracted from the Web. Data sizes are 1.7, 1.1, and 0.3 billion words, and for Spanish-English and English-German, we use mixed domain tuning / test sentences that each have about 1K sentences and were created to represent different domains evenly, including world news, health, sports, science, and others. For Chinese-English, we use indomain sentences (2K sentences) that were created by randomly extracting unique parallel sentences from internal parallel text collections; these indomain data result in higher BLEU values than in the other tasks as will be reported later. In all cases, we have a reference translation (2K sentences) that was achieved by an internal quick implementation of an EU-based decoder search system with the Manning rewrite order in 2008."}, {"heading": "3.1 Decoder Parameters", "text": "We adjust three standard decoder parameters \u03b8 = (d, s, n) that directly affect the translation speed. We describe them as follows: Distortion Limit. The maximum number of source words that the decoder may skip when generating sentences from left to right on the target page.s: Batch Size. The maximum number of hypotheses that may survive the truncation of the histogram in each decoder stack. n: Number of translations. The maximum number of alternative translations per source phrase that are taken into account when decoding."}, {"heading": "3.2 Measuring Decoding Speed", "text": "To get a better understanding of the velocity measurements, we decipher the English-German fine adjustment 100 times with a slow decoder parameter setting, i.e. \u03b8 = (5, 100, 100), and repeat it for a quick adjustment with \u03b8 = (0, 1, 1). We record velocity measurements in the number of translated words per minute (wpm) 3.The diagrams in Figure 1 show histograms containing the measurements obtained for slow and fast settings. While both fit into a Gaussian distribution, the velocity ranges in the slow setting from about 750 to 950 wpm and in the fast setting from 90K to 120K wpm. This means that velocity measurements have heteroscedasticity: they follow Gaussian distributions with different deviations depending on the decoder parameter values. This is a problem for our BO setting, because the GP we use to model the restriction assumes homogeneity adjustment or over roughness."}, {"heading": "3.3 BO Details and Baselines", "text": "All BO experiments use Spearmint (Snoek et al., 2012) with default values BO pair led D, unless explicitly stated otherwise. We set the minimum and maximum values for d, s and n as [0, 10], [1, 500] and [1, 100], respectively. We model d in linear scale, but s and n in logarithmic scale for both BO and baselines. This scaling is based on the intuition that optimal values for s and n will be in the lower interval values, which has been confirmed in preliminary experiments on all three datasets. We perform two sets of experiments using 2000wpm and 5000wpm as minimum speed constraints. In addition, we use the following BO settings: Standard (BO-S): In this setting, each BO iteration performs a complete decoding of the tuning set to obtain both the BLEU score and the decoding of the speed."}, {"heading": "3.4 Results", "text": "The results of the CPU-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-"}, {"heading": "3.5 BO Time Analysis", "text": "The complexity of GP-based BO is O (n3), n is the number of GP observations or function evaluations (Rasmussen and Williams, 2006). Since the objective function f is expected to be expensive, this should not be a problem for low budgets. However, as the number of iterations grows, it may reach a point where the time spent on GP calculations exceeds the time spent evaluating the function. This is examined in Figure 4, where the time spent on decoding compared to BO (in logarithmic scale) for the Sino-English ratio is shown as a function of optimization siteration. For BO-S (top), the decoding time is generally constant, but can peak depending on the parameters selected, up or down. For BO-D (bottom), most of the decoding runs are faster (when BO is queried for speed), and shoot up significantly only if the complete decoding time is considered the problem for this BO."}, {"heading": "3.6 Reoptimizing Feature Weights", "text": "However, there is no reason to believe that the best characteristic weights for slow adjustment are also the best weights for the quick adjustments we wish.To assess this, we now need to repair the decoder parameters \u03b8 and MERT in Chinese-English with the fast adjustments found by BO-S. We run MERT from flat weights (MERT-flat) and optimal weights (MERT-opt) previously matched with MERT for the MERT baseline. Table 4 reports on the results. We find that MERT-opt is able to recover from the BLEU drops observed during the quick adjustment and close the gap with the slow baseline (from 41.9 to 42.4 BLEU at 1.8 Kwpm)."}, {"heading": "4 Related Work", "text": "Bayesian Optimization is already used for hyperparameters optimization in machine learning systems (Snoek et al., 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning incorporates human feedback (Brochu et al., 2010a). Recently, it has also been successfully used in several NLP applications. Wang et al. (2015) use BO to optimize sentiment analysis and question-answer systems. They adopt a multi-step approach in which hyperparameters are optimized using small data sets and then used as a starting point for subsequent BO stages with increasing amounts of data. Yogatama et al. (2015) use BO to optimize text representations in a number of classification tasks. They find that there is no representation that is optimal for all tasks, justifying an automatic tuning approach. Wang et al. (2014) use an extraction model that optimizes a parameter based on optimization."}, {"heading": "5 Conclusion", "text": "For a better modeling of speed limitation and possibly a better generalization of speed measurements across tuning and test sets, however, one way would be to use randomized sets. Warped GPs (Snelson et al., 2003) could be a more precise model, since they can learn transformations for heteroscedastic data without relying on a fixed transformation, as we do with protocol speed measurements. Modeling of the target function could also be improved. In our experiments, we used a GP with a Mate 52 kernel, but this assumes that f is doubly differentiable and has lip tip continuity (Brochu et al., 2010b). As this does not apply to the BLEU score, we use alternative smoother metrics such as the BLEU linear corpus (Tromble et al, 2008 or BLyeek)."}], "references": [{"title": "Random Search for Hyper-Parameter", "author": ["Bergstra", "Bengio2012] James Bergstra", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Algorithms for Hyper-Parameter Optimization", "author": ["R\u00e9mi Bardenet", "Yoshua Bengio", "Bal\u00e1zs K\u00e9gl"], "venue": "In Proceedings of NIPS", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "A Bayesian Interactive Optimization Approach to Procedural Animation Design", "author": ["Brochu et al.2010a] Eric Brochu", "Tyson Brochu", "Nando de Freitas"], "venue": "In Proceedings of ACM SIGGRAPH/Eurographics Symposium on Computer Animation", "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning. arXiv:1012.2599v1 [cs.LG", "author": ["Brochu et al.2010b] Eric Brochu", "Vlad M. Cora", "Nando de Freitas"], "venue": null, "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Direct Error Rate Minimization for Statistical Machine Translation", "author": ["Chung", "Galley2012] Tagyoung Chung", "Michel Galley"], "venue": "In Proceedings of WMT,", "citeRegEx": "Chung et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2012}, {"title": "A Simple and Effective Hierarchical Phrase Reordering Model", "author": ["Galley", "Manning2008] Michel Galley", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Galley et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2008}, {"title": "Bayesian Optimization with Unknown Constraints", "author": ["Jasper Snoek", "Ryan P. Adams"], "venue": "In Proceedings of UAI", "citeRegEx": "Gelbart et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gelbart et al\\.", "year": 2014}, {"title": "Constrained Bayesian Optimization and Applications", "author": ["Michael A. Gelbart"], "venue": "Ph.D. thesis,", "citeRegEx": "Gelbart.,? \\Q2015\\E", "shortCiteRegEx": "Gelbart.", "year": 2015}, {"title": "Batch Bayesian Optimization via Local Penalization", "author": ["Zhenwen Dai", "Philipp Hennig", "Neil D. Lawrence"], "venue": null, "citeRegEx": "Gonz\u00e1lez et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gonz\u00e1lez et al\\.", "year": 2015}, {"title": "Modeling an Augmented Lagrangian for Blackbox Constrained Optimization", "author": ["Genetha A. Gray", "Sebastien Le Digabel", "Herbert K.H. Lee", "Pritam Ranjan", "Garth Wells", "Stefan M. Wild"], "venue": null, "citeRegEx": "Gramacy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gramacy et al\\.", "year": 2014}, {"title": "Predictive Entropy Search for Bayesian Optimization with Unknown Constraints", "author": ["Michael A. Gelbart", "Matthew W. Hoffman", "Ryan P. Adams", "Zoubin Ghahramani"], "venue": "Proceedings of ICML", "citeRegEx": "Hern\u00e1ndezLobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndezLobato et al\\.", "year": 2015}, {"title": "Sequential Model-based Optimization for General Algorithm Configuration", "author": ["Hutter et al.2011] Frank Hutter", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "In Proceedings of LION", "citeRegEx": "Hutter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2011}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Bayesian Optimisation for Machine", "author": ["Miao et al.2014] Yishu Miao", "Ziyu Wang", "Phil Blunsom"], "venue": null, "citeRegEx": "Miao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2014}, {"title": "The Alignment Template Approach to Statistical Machine Translation", "author": ["Och", "Ney2004] Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Och et al\\.", "year": 2004}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2001}, {"title": "Gaussian processes for machine learning, volume 1", "author": ["Rasmussen", "Christopher K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2006}, {"title": "Bbn system description for wmt10 system combination task", "author": ["Bing Zhang", "Spyros Matsoukas", "Richard Schwartz"], "venue": "In Proceedings of WMT and MetricsMATR,", "citeRegEx": "Rosti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rosti et al\\.", "year": 2010}, {"title": "Taking the Human Out of the Loop : A Review of Bayesian Optimization", "author": ["Kevin Swersky", "Ziyu Wang", "Ryan P. Adams", "Nando de Freitas"], "venue": "Technical Report 1,", "citeRegEx": "Shahriari et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shahriari et al\\.", "year": 2015}, {"title": "Practical Bayesian optimization of Machine Learning Algorithms", "author": ["Snoek et al.2012] Jasper Snoek", "Hugo Larochelle", "Ryan P. Adams"], "venue": "In Proceedings of NIPS", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Multi-task Bayesian Optimization", "author": ["Jasper Snoek", "Ryan P. Adams"], "venue": "In Proceedings of NIPS", "citeRegEx": "Swersky et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2013}, {"title": "Freeze-Thaw Bayesian Optimization. arXiv:1406.3896v1 [stat.ML", "author": ["Jasper Snoek", "Ryan P. Adams"], "venue": null, "citeRegEx": "Swersky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2014}, {"title": "Lattice Minimum Bayes-Risk decoding for statistical machine translation", "author": ["Tromble et al.2008] Roy Tromble", "Shankar Kumar", "Franz Och", "Wolfgang Macherey"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Tromble et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tromble et al\\.", "year": 2008}, {"title": "Bayesian Multi-Scale Optimistic Optimization", "author": ["Wang et al.2014] Ziyu Wang", "Babak Shakibi", "Lin Jin", "Nando de Freitas"], "venue": "In Proceedings of AISTATS", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Efficient Hyper-parameter Optimization for NLP Applications", "author": ["Wang et al.2015] Lidan Wang", "Minwei Feng", "Bowen Zhou", "Bing Xiang", "Sridhar Mahadevan"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Research in Statistical Machine Translation (SMT) aims to improve translation quality, typically measured by BLEU scores (Papineni et al., 2001), over a baseline system.", "startOffset": 121, "endOffset": 144}, {"referenceID": 18, "context": "We propose to use Bayesian Optimization (Brochu et al., 2010b; Shahriari et al., 2015) for this constrained optimization task.", "startOffset": 40, "endOffset": 86}, {"referenceID": 18, "context": "BO is particularly well-suited when f is non-convex, nondifferentiable and costly to evaluate (Shahriari et al., 2015).", "startOffset": 94, "endOffset": 118}, {"referenceID": 19, "context": "While a number of different approaches exist in the literature, in this work we follow the concepts presented in Snoek et al. (2012) and implemented in the Spearmint1 toolkit, which we detail in this Section.", "startOffset": 113, "endOffset": 133}, {"referenceID": 6, "context": "So, we follow Gelbart et al. (2014) and redefine Equation 3 by assuming a probabilistic model p over c(\u03b8):", "startOffset": 14, "endOffset": 36}, {"referenceID": 6, "context": "However, it has been shown that this approach has some pathologies in the presence of constrained functions (Gelbart et al., 2014).", "startOffset": 108, "endOffset": 130}, {"referenceID": 6, "context": "This acquisition function has been empirically shown to obtain better results when dealing with constraints and it can easily take advantage of a scenario known as decoupled constraints (Gelbart et al., 2014), where the objective (BLEU) and the constraint (speed) values can come from different sets of measurements.", "startOffset": 186, "endOffset": 208}, {"referenceID": 6, "context": "A simple way to reduce the effect of heteroscedasticity is to take the logarithm of the speed measurements, which is also a standard practice when modeling non-negative measures in a GP (Gelbart et al., 2014).", "startOffset": 186, "endOffset": 208}, {"referenceID": 19, "context": "All BO experiments use Spearmint (Snoek et al., 2012) with default values unless explicitly stated otherwise.", "startOffset": 33, "endOffset": 53}, {"referenceID": 19, "context": "Since speed measurements are faster in this case, we enforce BO to query for speed more often by modeling the task duration as described by Snoek et al. (2012). We use a higher constraint tolerance (\u03b4 = 0.", "startOffset": 140, "endOffset": 160}, {"referenceID": 13, "context": "We note anecdotally that we have attempted to replicate the feature weight tuning procedure of Miao et al. (2014) but obtained mixed results on our test sets.", "startOffset": 95, "endOffset": 114}, {"referenceID": 19, "context": "Bayesian Optimization has been previously used for hyperparameter optimization in machine learning systems (Snoek et al., 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al.", "startOffset": 107, "endOffset": 150}, {"referenceID": 1, "context": "Bayesian Optimization has been previously used for hyperparameter optimization in machine learning systems (Snoek et al., 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al.", "startOffset": 107, "endOffset": 150}, {"referenceID": 11, "context": ", 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al.", "startOffset": 43, "endOffset": 64}, {"referenceID": 0, "context": ", 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al., 2010a). Recently, it has also been used successfully in several NLP applications. Wang et al. (2015) use BO to tune sentiment analysis and question answering systems.", "startOffset": 8, "endOffset": 273}, {"referenceID": 0, "context": ", 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al., 2010a). Recently, it has also been used successfully in several NLP applications. Wang et al. (2015) use BO to tune sentiment analysis and question answering systems. They introduce a multi-stage approach where hyperparameters are optimized using small datasets and then used as starting points for subsequent BO stages using increasing amounts of data. Yogatama et al. (2015) employ BO to optimize text representations in a set of classification tasks.", "startOffset": 8, "endOffset": 549}, {"referenceID": 0, "context": ", 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al., 2010a). Recently, it has also been used successfully in several NLP applications. Wang et al. (2015) use BO to tune sentiment analysis and question answering systems. They introduce a multi-stage approach where hyperparameters are optimized using small datasets and then used as starting points for subsequent BO stages using increasing amounts of data. Yogatama et al. (2015) employ BO to optimize text representations in a set of classification tasks. They find that there is no representation that is optimal for all tasks, which further justifies an automatic tuning approach. Wang et al. (2014) use a model based on optimistic optimization to tune parameters of a term extraction system.", "startOffset": 8, "endOffset": 772}, {"referenceID": 0, "context": ", 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al., 2010a). Recently, it has also been used successfully in several NLP applications. Wang et al. (2015) use BO to tune sentiment analysis and question answering systems. They introduce a multi-stage approach where hyperparameters are optimized using small datasets and then used as starting points for subsequent BO stages using increasing amounts of data. Yogatama et al. (2015) employ BO to optimize text representations in a set of classification tasks. They find that there is no representation that is optimal for all tasks, which further justifies an automatic tuning approach. Wang et al. (2014) use a model based on optimistic optimization to tune parameters of a term extraction system. In SMT, Miao et al. (2014) use BO for feature weight tuning and report better results in some language pairs when compared to traditional tuning algorithms.", "startOffset": 8, "endOffset": 892}, {"referenceID": 6, "context": "Our approach is heavily based on the work of Gelbart et al. (2014) and Hern\u00e1ndez-Lobato et al.", "startOffset": 45, "endOffset": 67}, {"referenceID": 6, "context": "Our approach is heavily based on the work of Gelbart et al. (2014) and Hern\u00e1ndez-Lobato et al. (2015) which uses BO in the presence of unknown constraints.", "startOffset": 45, "endOffset": 102}, {"referenceID": 6, "context": "Our approach is heavily based on the work of Gelbart et al. (2014) and Hern\u00e1ndez-Lobato et al. (2015) which uses BO in the presence of unknown constraints. They set speed and memory constraints on neural network trainings and report better results compared to those of naive models which explicitly put high costs on regions that violate constraints. A different approach based on augmented Lagrangians is proposed by Gramacy et al. (2014). The authors apply BO in a water decontamination setting where the goal is to find the optimal pump positioning subject to restrictions on water and contaminant flows.", "startOffset": 45, "endOffset": 440}, {"referenceID": 22, "context": "Since that does not hold for the BLEU score, using alternative smoother metrics such as linear corpus BLEU (Tromble et al., 2008) or expected BLEU (Rosti et al.", "startOffset": 107, "endOffset": 129}, {"referenceID": 17, "context": ", 2008) or expected BLEU (Rosti et al., 2010) could yield better results.", "startOffset": 25, "endOffset": 45}, {"referenceID": 20, "context": "Other recent developments in Bayesian Optimisation could be applied to our settings, like multi-task optimization (Swersky et al., 2013) or freeze-thaw optimization (Swersky et al.", "startOffset": 114, "endOffset": 136}, {"referenceID": 21, "context": ", 2013) or freeze-thaw optimization (Swersky et al., 2014).", "startOffset": 36, "endOffset": 58}, {"referenceID": 19, "context": "Parallel approaches do exist (Snoek et al., 2012; Gonz\u00e1lez et al., 2015), but we find it easy enough to harness parallel computation in decoding tuning sets and by decoupling BLEU measurements from speed measurements.", "startOffset": 29, "endOffset": 72}, {"referenceID": 8, "context": "Parallel approaches do exist (Snoek et al., 2012; Gonz\u00e1lez et al., 2015), but we find it easy enough to harness parallel computation in decoding tuning sets and by decoupling BLEU measurements from speed measurements.", "startOffset": 29, "endOffset": 72}], "year": 2016, "abstractText": "We address the problem of automatically finding the parameters of a statistical machine translation system that maximize BLEU scores while ensuring that decoding speed exceeds a minimum value. We propose the use of Bayesian Optimization to efficiently tune the speed-related decoding parameters by easily incorporating speed as a noisy constraint function. The obtained parameter values are guaranteed to satisfy the speed constraint with an associated confidence margin. Across three language pairs and two speed constraint values, we report overall optimization time reduction compared to grid and random search. We also show that Bayesian Optimization can decouple speed and BLEU measurements, resulting in a further reduction of overall optimization time as speed is measured over a small subset of sentences.", "creator": "LaTeX with hyperref package"}}}