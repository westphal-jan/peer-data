{"id": "1604.05529", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2016", "title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss", "abstract": "Bi-directional long short-term memory (LSTM) networks have recently proven successful for various NLP sequence modeling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel bi-LSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words. The new model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that bi-LSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed.", "histories": [["v1", "Tue, 19 Apr 2016 11:53:09 GMT  (415kb,D)", "http://arxiv.org/abs/1604.05529v1", "To appear in ACL 2016 (short)"], ["v2", "Thu, 19 May 2016 15:28:03 GMT  (579kb,D)", "http://arxiv.org/abs/1604.05529v2", "In ACL 2016 (short)"], ["v3", "Thu, 21 Jul 2016 08:17:43 GMT  (578kb,D)", "http://arxiv.org/abs/1604.05529v3", "In ACL 2016 (short)"]], "COMMENTS": "To appear in ACL 2016 (short)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["barbara plank", "anders s\u00f8gaard", "yoav goldberg"], "accepted": true, "id": "1604.05529"}, "pdf": {"name": "1604.05529.pdf", "metadata": {"source": "CRF", "title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss", "authors": ["Barbara Plank", "Anders S\u00f8gaard"], "emails": ["b.plank@rug.nl", "soegaard@hum.ku.dk", "yoav.goldberg@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Recently, bidirectional short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for speech modeling (Ling et al., 2015), POS tagging (Ling et al., 2015), and semantic role designation (Zhou and Xu, 2015), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role designation (Zhou and Xu, 2015). LSTMs are recursive neural networks (RNs) in which layers are designed to prevent disappearing gradients. Bidirectional LSTMs make a sequence before transmission to the next layer."}, {"heading": "2 Tagging with bi-LSTMs", "text": "In fact, it is in such a way that most people who are able to enter the world, not into the world, but into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into, into the world, into the world, into the world, into the world, into, into the world, into the world, into the world, into the world, into the world, into the world, into, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into, into the world, into the world, into the world, into, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, into the world, the world, into the world, into, into the world, into the world, into the world, into the world, into the world, into the world, the world, into the world, the world, the world, the world, into the world, the world, into the world, the world, into the world, into the world, into the world, the world,"}, {"heading": "3 Experiments", "text": "All bi-LSTM models have been implemented in CNN, 1 a flexible neural network library. For all models, we use the same hyperparameters set in English dev, i.e. SGD training with crossentropy loss, no mini-batches, 20 epochs, default learning rate, 128 dimensions for word embedding, 100 for character and byte embedding, 100 hidden states and Gaussian noise with \u03c3 = 0.2. Because the training is stochastical in nature, we use a solid seed throughout. Embedding is not initiated with pre-trained embedding unless otherwise reported. In this case, we use standard polyglot embedding. 2 For reproducibility, all codes are published at ANONYMIZED.Taggers, we do not want to initiate the embedding with pre-trained embedding unless otherwise reported."}, {"heading": "3.1 Datasets", "text": "For the multilingual experiments, we use data from Universal Dependencies v1.2 (Nivre et al., 2015) (17 POS). We consider all languages that have at least 60k tokens, resulting in 22 languages."}, {"heading": "3.2 Results", "text": "The mentioned hsrc\u00fceFnlhsrc\u00fc\u00fc\u00fcehnlrc\u00fcehsrcnlhsrc\u00fc\u00fc\u00fc\u00fceG nvo eeisn nlrcehnc\u00fcehnlrc\u00fcehnc\u00fcehnc\u00fcehncso ni ni ende nlrrlrc\u00fcehnc\u00fcehnc\u00fcehnc\u00fcehnc\u00fcehnc\u00fcehnc\u00fcehnc\u00fcehnc\u00fcehnc\u00fcehnc\u00fcehnllcso ni ende nlrlrlrrrrrrf\u00fc for eeirlrlrcehnlrc\u00fceGe ni nde nlrlrlrlrc\u00fcehnc\u00fcehnc\u00fcehnc\u00fcehnc\u00fcehnc\u00fcehnc\u00fcehnc\u00fcehnlcu, \"so os os os os os os os lrrrf\u00fc ide nlrlrlrlrlrrlrlrf\u00fc, eaeaeerrrrf\u00fc, cos for c\u00fcrrrrrrrrrrrrrrrf\u00fc, rrrrrrrrrrrrrrrfu, rrrrrrrrfu, rrrrrrrfos, rrrrrrfos, rrrrrrfos, rrrrrrrrrrrfu, rrfos, rfos, rrrrrrrrrrrrrfos, rrrfos, rrrrfos, rrrrrrfos, rrrfos."}, {"heading": "4 Related Work", "text": "Our early applications include text classification (Chrupa\u0142a, 2013; Zhang et al., 2015). Re-5We observe the same pattern with more than 40 iterations, and these representations have been successfully applied to a number of structured prediction tasks. Santos and Zadrozny (2014) were the first to propose character-based models at POS tagging, using a revolutionary neural network (CNN) and evaluating their model in English (PTB) and Portuguese, showing that the model achieves modern performance close to taggers with a number of handmade features. Ling et al. (2015) extend this line and compare a novel bi-LSTM model by learning word representations through character embeddings (PTB). They evaluate their model using a language modeling and POS tagging facility, and show that Bi-LSTMs et et et etler uses the German word classification of Santos and Zadny (2015) for character rating only."}, {"heading": "5 Conclusions", "text": "We propose a novel multi-task Bi-LSTM and evaluate a series of representations for Bi-LSTM POS marking in 22 languages. Modelling subtoken information and integrating an auxiliary loss that makes the model for predicting frequency provide a very effective tagger that establishes a new state of the art on UD v1.2."}, {"heading": "A Supplemental Material", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Improved transitionbased parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A. Smith"], "venue": null, "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Tnt: a statistical part-of-speech tagger", "author": ["Thorsten Brants"], "venue": "In Proceedings of the sixth conference on Applied natural language processing,", "citeRegEx": "Brants.,? \\Q2000\\E", "shortCiteRegEx": "Brants.", "year": 2000}, {"title": "Open-domain name error detection using a multi-task rnn", "author": ["Cheng et al.2015] Hao Cheng", "Hao Fang", "Mari Ostendorf"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Natural language understanding with distributed representation. ArXiv, abs/1511.07916", "author": ["Kyunghyun Cho"], "venue": null, "citeRegEx": "Cho.,? \\Q2015\\E", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Multilingual language processing from bytes", "author": ["Gillick et al.2015] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "A primer on neural network models for natural language processing. ArXiv, abs/1510.00726", "author": ["Yoav Goldberg"], "venue": null, "citeRegEx": "Goldberg.,? \\Q2015\\E", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Fast or accurate?\u2013a comparative evaluation of pos tagging models", "author": ["Nicolai Erbs", "Torsten Zesch"], "venue": "In Proceedings of the International Conference of the German Society for Computational Linguistics", "citeRegEx": "Horsmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Horsmann et al\\.", "year": 2015}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Non-lexical neural architecture for fine-grained pos tagging", "author": ["Kevin L\u00f6ser", "Alexandre Allauzen"], "venue": null, "citeRegEx": "Labeau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Labeau et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Fine-grained opinion mining with recurrent neural networks and word embeddings", "author": ["Liu et al.2015] Pengfei Liu", "Shafiq Joty", "Helen Meng"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Universal dependencies 1.2. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague", "author": ["Zhu"], "venue": null, "citeRegEx": "2015.,? \\Q2015\\E", "shortCiteRegEx": "2015.", "year": 2015}, {"title": "Adapting taggers to twitter using not-so-distant supervision", "author": ["Plank et al.2014] Barbara Plank", "Dirk Hovy", "Ryan McDonald", "Anders S\u00f8gaard"], "venue": "In COLING", "citeRegEx": "Plank et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2014}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D Santos", "Bianca Zadrozny"], "venue": "In ICML,", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Semisupervised condensed nearest neighbor for part-ofspeech tagging", "author": ["Anders S\u00f8gaard"], "venue": "In Proceedings of ACL", "citeRegEx": "S\u00f8gaard.,? \\Q2011\\E", "shortCiteRegEx": "S\u00f8gaard.", "year": 2011}, {"title": "Generating text with recurrent neural networks", "author": ["James Martens", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Partof-speech tagging with bidirectional long shortterm memory recurrent neural network. pre-print, abs/1510.06168", "author": ["Wang et al.2015] Peilu Wang", "Yao Qian", "Frank K. Soong", "Lei He", "Hai Zhao"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Endto-end learning of semantic role labeling using recurrent neural networks", "author": ["Zhou", "Xu2015] Jie Zhou", "Wei Xu"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015), POS tagging (Ling et al.", "startOffset": 169, "endOffset": 188}, {"referenceID": 13, "context": ", 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al.", "startOffset": 21, "endOffset": 59}, {"referenceID": 20, "context": ", 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al.", "startOffset": 21, "endOffset": 59}, {"referenceID": 0, "context": ", 2015), transition-based dependency parsing (Ballesteros et al., 2015), fine-grained sentiment analysis (Liu et al.", "startOffset": 45, "endOffset": 71}, {"referenceID": 14, "context": ", 2015), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al.", "startOffset": 41, "endOffset": 59}, {"referenceID": 10, "context": ", 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015).", "startOffset": 28, "endOffset": 48}, {"referenceID": 4, "context": "Recurrent neural networks (RNNs) (Elman, 1990) allow the computation of fixed-size vector representations for word sequences of arbitrary length.", "startOffset": 33, "endOffset": 46}, {"referenceID": 13, "context": "In order to use subtoken information, we use a hierarchical bidirectional LSTM (Ling et al., 2015; Ballesteros et al., 2015).", "startOffset": 79, "endOffset": 124}, {"referenceID": 0, "context": "In order to use subtoken information, we use a hierarchical bidirectional LSTM (Ling et al., 2015; Ballesteros et al., 2015).", "startOffset": 79, "endOffset": 124}, {"referenceID": 0, "context": ", 2015; Ballesteros et al., 2015). We compute subtoken-level (either characters ~c or unicode byte ~b) embeddings of words using a biLSTM at the lower level. This representation is then concatenated with the (learned) word embeddings vector ~ w which forms the input to the biLSTM at the next layer. This model, illustrated in Figure 1 (left), is inspired by Ballesteros et al. (2015).", "startOffset": 8, "endOffset": 385}, {"referenceID": 1, "context": "We hence use three different types of taggers: our implementation of a bi-LSTM; TNT (Brants, 2000)\u2014a second order HMM with suffix trie handling for OOVs; and a freely available CRF-based tagger.", "startOffset": 84, "endOffset": 98}, {"referenceID": 9, "context": "as it was among the best performing taggers evaluated in Horsmann et al. (2015).3 We comple-", "startOffset": 57, "endOffset": 80}, {"referenceID": 16, "context": "ment the NN-based and HMM-based tagger with a CRF tagger, using a freely available implementation (Plank et al., 2014)4 based on crfsuite.", "startOffset": 98, "endOffset": 118}, {"referenceID": 5, "context": "The only system we are aware of that evaluates on UD is Gillick et al. (2015) (last column).", "startOffset": 56, "endOffset": 78}, {"referenceID": 13, "context": "We examined RNNs and confirm the finding of Ling et al. (2015) that they performed worse than their LSTM counterparts.", "startOffset": 44, "endOffset": 63}, {"referenceID": 21, "context": "sets when learning with neural network based approaches (Zhang et al., 2015).", "startOffset": 56, "endOffset": 76}, {"referenceID": 21, "context": "Early applications include text classification (Chrupa\u0142a, 2013; Zhang et al., 2015).", "startOffset": 47, "endOffset": 83}, {"referenceID": 18, "context": "Character embeddings were first introduced by Sutskever et al. (2011) for language modeling.", "startOffset": 46, "endOffset": 70}, {"referenceID": 11, "context": "Ling et al. (2015) extend this line and compare a novel bi-LSTM model, learning word representations through character embeddings.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "Ling et al. (2015) extend this line and compare a novel bi-LSTM model, learning word representations through character embeddings. They evaluate their model on a language modeling and POS tagging setup, and show that bi-LSTMs outperform the CNN approach of Santos and Zadrozny (2014). However, Ling et al.", "startOffset": 0, "endOffset": 284}, {"referenceID": 11, "context": "Ling et al. (2015) extend this line and compare a novel bi-LSTM model, learning word representations through character embeddings. They evaluate their model on a language modeling and POS tagging setup, and show that bi-LSTMs outperform the CNN approach of Santos and Zadrozny (2014). However, Ling et al. (2015)\u2019s evaluation still focuses on Indoeuropean languages only.", "startOffset": 0, "endOffset": 313}, {"referenceID": 11, "context": "Similarly, Labeau et al. (2015) evaluate character embeddings for German.", "startOffset": 11, "endOffset": 32}, {"referenceID": 11, "context": "Similarly, Labeau et al. (2015) evaluate character embeddings for German. BiLSTMs for tagging are also reported in Wang et al. (2015), however, they only explore word embeddings, orthographic information and evaluate on WSJ only.", "startOffset": 11, "endOffset": 134}, {"referenceID": 2, "context": "A related study is Cheng et al. (2015) who propose a multi-task RNN for named entity recognition, by jointly predicting the next token and current tokens name label.", "startOffset": 19, "endOffset": 39}], "year": 2017, "abstractText": "Bi-directional long short-term memory (LSTM) networks have recently proven successful for various NLP sequence modeling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel biLSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words. The new model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that biLSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed.", "creator": "LaTeX with hyperref package"}}}