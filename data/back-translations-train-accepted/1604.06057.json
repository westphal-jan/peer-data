{"id": "1604.06057", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2016", "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation", "abstract": "Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete MDP with stochastic transitions, and (2) the classic ATARI game `Montezuma's Revenge'.", "histories": [["v1", "Wed, 20 Apr 2016 18:47:48 GMT  (1158kb,D)", "http://arxiv.org/abs/1604.06057v1", "13 pages, 7 figures"], ["v2", "Tue, 31 May 2016 14:45:58 GMT  (1171kb,D)", "http://arxiv.org/abs/1604.06057v2", "14 pages, 7 figures"]], "COMMENTS": "13 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE stat.ML", "authors": ["tejas d kulkarni", "karthik narasimhan", "ardavan saeedi", "josh tenenbaum"], "accepted": true, "id": "1604.06057"}, "pdf": {"name": "1604.06057.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation", "authors": ["Tejas D. Kulkarni", "Karthik R. Narasimhan"], "emails": ["tejask@mit.edu", "karthikn@mit.edu", "ardavans@mit.edu", "jbt@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Learning purposeful behavior with scant feedback from complex environments is a fundamental challenge for artificial intelligence. Learning in this environment requires agents to present knowledge on multiple levels of spatial-temporal abstractions and to efficiently explore the environment. Lately, nonlinear functional approaches coupled with amplification learning [17, 20, 29] have made it possible to learn abstractions of high-dimensional state spaces, but the task of exploring with scant feedback still poses a major challenge. Existing methods such as Boltzmann Exploration and Thomson Sampling [36, 24] offer significant improvements over -greedy, but are limited due to the underlying models that work at the level of basic measures. In this work, we propose a framework that integrates deep reinforcement learning with hierarchical value functions (h-DQN), where the agent is motivated to mitigate intrinsic goals (via lernotions) to help solve the problem of exploring efficiently."}, {"heading": "2 Literature Review", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Reinforcement Learning with Temporal Abstractions", "text": "Learning and acting at different levels of temporal abstraction is a central challenge in tasks involving long-term planning. In the context of the attachment process, Sutton et al. [39] have proposed the option framework, which includes abstractions on the scope of action. At each step, the actor chooses either a one-step \"primitive\" action or a \"multi-step\" action policy (option); each option defines a policy on actions (either primitive or other options) and can be terminated according to a stochastic function \u03b2. Therefore, the traditional MDP attitude can be extended to a semi-Markov decision-making process (SMDP) with the use of options. Recently, several methods have been proposed to learn options in real-time through the use of different reward functions [40] or by composing existing options [33]. Value functions have also been generalized to consider goals together with states [26]. This universal value function (V, falls; for example, provides an optimal behavior toward the universal option)."}, {"heading": "2.2 Intrinsically motivated RL", "text": "Singh et al. [32] examined agents with intrinsic reward structures to learn generic options that are applicable to a variety of tasks. Singh et al. [31] use an idea of \"prominent events\" as sub-targets and learn options to arrive at such events. In another paper, Singh et al. [28] adopt an evolutionary perspective to optimize the space of reward functions for the agent, leading to an idea of extrinsic and intrinsic motivated behavior. Schmidhuber [28] offers a coherent formulation of intrinsic motivation that is measured against the improvements of a predictive world model created by the learning algorithm. Mohamed and Rezende [21] have recently proposed an idea of intrinsic motivated learning within the framework of mutual information maximization."}, {"heading": "2.3 Object-based RL", "text": "Object-based representations [7, 3], which can exploit the underlying structure of a problem, have been proposed to alleviate the curse of dimensionality in RL. Diuk et al. [7] propose an object-oriented MDP, using representation based on objects and their interactions, defining each state as a set of value mappings to all possible relationships between objects, and introducing an algorithm for solving deterministic object-oriented MDPs. Their representation is similar to that of Guestrin et al. [13], which describe an object-based representation in the context of planning. Contrary to these approaches, our representation does not require explicit coding for relations between objects and can be used in stochastic areas."}, {"heading": "2.4 Deep Reinforcement Learning", "text": "Recent advances in functional alignment with deep neural networks have shown promising results in handling high-dimensional sensory input. Deep Q networks and their variants have been successfully applied to various areas such as Atari games [20] and Go [29], but still perform poorly in environments with sparse, delayed reward signals. Strategies such as prioritized experience reproduction [27] and bootstrapping [24] have been proposed to alleviate the problem of learning from sparse rewards. These approaches lead to significant improvements over previous work, but are difficult to achieve when the reward signal has a long-delayed horizon. This is because the exploration strategy is insufficient to obtain the required feedback."}, {"heading": "2.5 Cognitive Science and Neuroscience", "text": "The nature and origin of intrinsic goals in humans is a delicate subject, but there are some remarkable findings from existing literature. There is converging evidence in developmental psychology that human infants, primates, children and adults in different cultures base their core knowledge on certain cognitive systems, including - units, agents and their actions, numerical sizes, space, social structures and intuitive theories [34, 19]. Even newborns and infants seem to represent the visual world in terms of coherent visual units centered around spatio-temporal principles of cohesion, continuity and contact. They also seem to explicitly represent other actors, assuming that the behavior of an agent is targeted and efficient. Infants can also distinguish relative sizes of objects, relative distances and higher order, numerical relationships such as the ratio of object size to spatial size. While curiosity-driven activities use infants to form this knowledge to construct intrinsic goals, they can also create sub-structural goals in physics."}, {"heading": "3 Model", "text": "Consider a Markov Decision Process (MDP), represented by the states that do so. We define the extrinsic reward function as F: (s). The agent's goal is to maximize this function over long periods of time. For example, this function can take the form of the agent's survival time or score in a game. We define the extrinsic reward function as F: (s) \u2192 R. The agent's goal is to maximize this function over long periods of time. The method like -greedy is useful for local exploration, but not able to provide impetus for the agent to explore different areas of government space. To address this, we use an idea of goals g: G that provide intrinsic motivation for the agent."}, {"heading": "4 Experiments", "text": "We are conducting experiments in two different areas with delayed rewards: the first is a national MDP with stochastic transitions, and the second is an ATARI 2600 game called \"Montezuma's Revenge.\""}, {"heading": "4.1 Discrete MDP with delayed rewards", "text": "We consider a stochastic decision-making process in which the extrinsic reward depends on the history of the visited states in addition to the current state. We have chosen this task to demonstrate the importance of the intrinsic motivation for exploration in such environments. There are 6 possible states and the agent always starts at s2. The agent moves deterministically to the left when he chooses left measures; but the right to act is successful only 50% of the time, leading to another step to the left. The final state is s1 and the agent receives the reward of 1 when he first visits s6 and then s1. The reward for going to s1 without visiting s6 is 0.01. This is a modified version of the MDP in [24], with the reward structure adding complexity to the task. The process is shown in Figure 2.We consider each state as a possible destination for exploration that each state is a possible destination for exploration."}, {"heading": "4.2 ATARI game with delayed rewards", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "5 Conclusion", "text": "We introduced h-DQN, a framework of hierarchical value functions operating on different time scales. Temporary decomposition of the value function allows the agent to execute intrinsically motivated behavior, which in turn allows efficient exploration in environments.1Example trajectory of a run on \"Montezuma's Revenge\" - https: / / goo.gl / 3Z64Ji4 / 18 / 2016 Reward.htmlfile: / / Users / tejas / Documents / Deepening RelationalRL / dqn / Bar% 20graph.html 1 / 1with delayed rewards. We also observe that parameterization of intrinsic motivation in the space of units and relationships offers a promising opportunity for building agents with extended exploration time. We also plan to explore alternative parameterization of targets with h-DQN in the future."}, {"heading": "Acknowledgements", "text": "We thank Vaibhav Unhelkar, Ramya Ramakrishnan, Sam Gershman, Michael Littman, Vlad Firoiu, Will Whitney, Max Kleiman-Weiner and Pedro Tsividis for critical feedback and discussion. We are grateful for support from the Center for Brain, Machines and Minds (NSF STC award CCF - 1231216) and the MIT OpenMind team."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective", "author": ["M.M. Botvinick", "Y. Niv", "A.C. Barto"], "venue": "Cognition, 113(3):262\u2013280", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Object focused q-learning for autonomous agents", "author": ["L.C. Cobo", "C.L. Isbell", "A.L. Thomaz"], "venue": "Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems, pages 1061\u20131068. International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving generalization for temporal difference learning: The successor representation", "author": ["P. Dayan"], "venue": "Neural Computation, 5(4):613\u2013624", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "Feudal reinforcement learning", "author": ["P. Dayan", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 271\u2013271. Morgan Kaufmann Publishers", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1993}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["T.G. Dietterich"], "venue": "J. Artif. Intell. Res.(JAIR), 13:227\u2013303", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "An object-oriented representation for efficient reinforcement learning", "author": ["C. Diuk", "A. Cohen", "M.L. Littman"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 240\u2013247. ACM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Attend", "author": ["S. Eslami", "N. Heess", "T. Weber", "Y. Tassa", "K. Kavukcuoglu", "G.E. Hinton"], "venue": "infer, repeat: Fast scene understanding with generative models. arXiv preprint arXiv:1603.08575", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to segment moving objects in videos", "author": ["K. Fragkiadaki", "P. Arbelaez", "P. Felsen", "J. Malik"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 4083\u20134090. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "The successor representation and temporal context", "author": ["S.J. Gershman", "C.D. Moore", "M.T. Todd", "K.A. Norman", "P.B. Sederberg"], "venue": "Neural Computation, 24(6):1553\u20131568", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Binding via reconstruction clustering", "author": ["K. Greff", "R.K. Srivastava", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1511.06418", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Generalizing plans to new environments in relational mdps", "author": ["C. Guestrin", "D. Koller", "C. Gearhart", "N. Kanodia"], "venue": "Proceedings of the 18th international joint conference on Artificial intelligence, pages 1003\u20131010. Morgan Kaufmann Publishers Inc.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient solution algorithms for factored mdps", "author": ["C. Guestrin", "D. Koller", "R. Parr", "S. Venkataraman"], "venue": "Journal of Artificial Intelligence Research, pages 399\u2013468", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M. Hausknecht", "P. Stone"], "venue": "arXiv preprint arXiv:1507.06527", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient inference in occlusion-aware generative models of images", "author": ["J. Huang", "K. Murphy"], "venue": "arXiv preprint arXiv:1511.06362", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Evolving deep unsupervised convolutional networks for vision-based reinforcement learning", "author": ["J. Kout\u0144\u0131k", "J. Schmidhuber", "F. Gomez"], "venue": "Proceedings of the 2014 conference on Genetic and evolutionary computation, pages 541\u2013548. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convolutional inverse graphics network", "author": ["T.D. Kulkarni", "W.F. Whitney", "P. Kohli", "J. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems, pages 2530\u20132538", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Building machines that learn and think like people", "author": ["B.M. Lake", "T.D. Ullman", "J.B. Tenenbaum", "S.J. Gershman"], "venue": "arXiv preprint arXiv:1604.00289", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Variational information maximisation for intrinsically motivated reinforcement learning", "author": ["S. Mohamed", "D.J. Rezende"], "venue": "Advances in Neural Information Processing Systems, pages 2116\u20132124", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A. De Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen"], "venue": "Massively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": "arXiv preprint arXiv:1506.08941", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep exploration via bootstrapped dqn", "author": ["I. Osband", "C. Blundell", "A. Pritzel", "B. Van Roy"], "venue": "arXiv preprint arXiv:1602.04621", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "One-shot generalization in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "I. Danihelka", "K. Gregor", "D. Wierstra"], "venue": "arXiv preprint arXiv:1603.05106", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Universal value function approximators", "author": ["T. Schaul", "D. Horgan", "K. Gregor", "D. Silver"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1312\u20131320", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "arXiv preprint arXiv:1511.05952", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Formal theory of creativity", "author": ["J. Schmidhuber"], "venue": "fun, and intrinsic motivation ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Where do rewards come from", "author": ["S. Singh", "R.L. Lewis", "A.G. Barto"], "venue": "Proceedings of the annual conference of the cognitive science society, pages 2601\u20132606", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Intrinsically motivated reinforcement learning: An evolutionary perspective", "author": ["S. Singh", "R.L. Lewis", "A.G. Barto", "J. Sorg"], "venue": "Autonomous Mental Development, IEEE Transactions on, 2(2):70\u2013 82", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Intrinsically motivated reinforcement learning", "author": ["S.P. Singh", "A.G. Barto", "N. Chentanez"], "venue": "Advances in neural information processing systems, pages 1281\u20131288", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Linear options", "author": ["J. Sorg", "S. Singh"], "venue": "Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1, AAMAS \u201910, pages 31\u201338, Richland, SC", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Core knowledge", "author": ["E.S. Spelke", "K.D. Kinzler"], "venue": "Developmental science, 10(1):89\u201396", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Design principles of the hippocampal cognitive map", "author": ["K.L. Stachenfeld", "M. Botvinick", "S.J. Gershman"], "venue": "Advances in neural information processing systems, pages 2528\u20132536", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["B.C. Stadie", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1507.00814", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Introduction to reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "volume 135. MIT Press Cambridge", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1998}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2, pages 761\u2013768. International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial intelligence, 112(1):181\u2013211", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1999}, {"title": "et al", "author": ["C. Szepesvari", "R.S. Sutton", "J. Modayil", "S. Bhatnagar"], "venue": "Universal option models. In Advances in Neural Information Processing Systems, pages 990\u2013998", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding visual concepts with continuation learning", "author": ["W.F. Whitney", "M. Chang", "T. Kulkarni", "J.B. Tenenbaum"], "venue": "arXiv preprint arXiv:1602.06822", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "Recently, non-linear function approximators coupled with reinforcement learning [17, 20, 29] have made it possible to learn abstractions over highdimensional state spaces, but the task of exploration with sparse feedback still remains a major challenge.", "startOffset": 80, "endOffset": 92}, {"referenceID": 19, "context": "Recently, non-linear function approximators coupled with reinforcement learning [17, 20, 29] have made it possible to learn abstractions over highdimensional state spaces, but the task of exploration with sparse feedback still remains a major challenge.", "startOffset": 80, "endOffset": 92}, {"referenceID": 28, "context": "Recently, non-linear function approximators coupled with reinforcement learning [17, 20, 29] have made it possible to learn abstractions over highdimensional state spaces, but the task of exploration with sparse feedback still remains a major challenge.", "startOffset": 80, "endOffset": 92}, {"referenceID": 35, "context": "Existing methods like Boltzmann exploration and Thomson sampling [36, 24] offer significant improvements over -greedy, but are limited due to the underlying models functioning at the level of basic actions.", "startOffset": 65, "endOffset": 73}, {"referenceID": 23, "context": "Existing methods like Boltzmann exploration and Thomson sampling [36, 24] offer significant improvements over -greedy, but are limited due to the underlying models functioning at the level of basic actions.", "startOffset": 65, "endOffset": 73}, {"referenceID": 36, "context": "Reinforcement learning (RL) formalizes control problems as finding a policy \u03c0 that maximizes expected future rewards [37].", "startOffset": 117, "endOffset": 121}, {"referenceID": 37, "context": "Recently, value functions have also been generalized as V (s, g) in order to represent the utility of state s for achieving a given goal g \u2208 G [38, 26].", "startOffset": 143, "endOffset": 151}, {"referenceID": 25, "context": "Recently, value functions have also been generalized as V (s, g) in order to represent the utility of state s for achieving a given goal g \u2208 G [38, 26].", "startOffset": 143, "endOffset": 151}, {"referenceID": 38, "context": "A collection of these policies can be hierarchically arranged with temporal dynamics for learning or planning within the framework of semi-Markov decision processes [39, 40].", "startOffset": 165, "endOffset": 173}, {"referenceID": 39, "context": "A collection of these policies can be hierarchically arranged with temporal dynamics for learning or planning within the framework of semi-Markov decision processes [39, 40].", "startOffset": 165, "endOffset": 173}, {"referenceID": 38, "context": "[39] proposed the options framework, which involves abstractions over the space of actions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "Recently, several methods have been proposed to learn options in real-time by using varying reward functions [40] or by composing existing options [33].", "startOffset": 109, "endOffset": 113}, {"referenceID": 32, "context": "Recently, several methods have been proposed to learn options in real-time by using varying reward functions [40] or by composing existing options [33].", "startOffset": 147, "endOffset": 151}, {"referenceID": 25, "context": "Value functions have also been generalized to consider goals along with states [26].", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "Other related work for hierarchical formulations include the model of Dayan and Hinton [5] which consisted of \u201cmanagers\u201d taking decisions at various levels of granularity, percolating all the way down to atomic actions made by the agent.", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": "The MAXQ framework [6] built up on this work to decompose the value function of an MDP into combinations of value functions of smaller constituent MDPs, as did Guestrin et al.", "startOffset": 19, "endOffset": 22}, {"referenceID": 13, "context": "[14] in their factored MDP formulation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Our approach does not use separate Q-functions for each option, but instead treats the option as part of the input, similar to [26].", "startOffset": 127, "endOffset": 131}, {"referenceID": 31, "context": "[32] explored agents with intrinsic reward structures in order to learn", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] take an evolutionary perspective to optimize over the space of reward functions for the agent, leading to a notion of extrinsically and intrinsically motivated behavior.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Schmidhuber [28] provides a coherent formulation of intrinsic motivation, which is measured by the improvements to a predictive world model made by the learning algorithm.", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "Mohamed and Rezende [21] have recently proposed a notion of intrinsically motivated learning within the framework of mutual information maximization.", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "Object-based representations [7, 3] that can exploit the underlying structure of a problem have been proposed to alleviate the curse of dimensionality in RL.", "startOffset": 29, "endOffset": 35}, {"referenceID": 2, "context": "Object-based representations [7, 3] that can exploit the underlying structure of a problem have been proposed to alleviate the curse of dimensionality in RL.", "startOffset": 29, "endOffset": 35}, {"referenceID": 6, "context": "[7] propose an Object-Oriented MDP, using a representation based on objects and their interactions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13], who describe an object-based representation in the context of planning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Deep Q-Networks and its variants have been successfully applied to various domains including Atari games [20] and Go [29], but still perform poorly on environments with sparse, delayed reward signals.", "startOffset": 105, "endOffset": 109}, {"referenceID": 28, "context": "Deep Q-Networks and its variants have been successfully applied to various domains including Atari games [20] and Go [29], but still perform poorly on environments with sparse, delayed reward signals.", "startOffset": 117, "endOffset": 121}, {"referenceID": 26, "context": "Strategies such as prioritized experience replay [27] and bootstrapping [24] have been proposed to alleviate the problem of learning from sparse rewards.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Strategies such as prioritized experience replay [27] and bootstrapping [24] have been proposed to alleviate the problem of learning from sparse rewards.", "startOffset": 72, "endOffset": 76}, {"referenceID": 33, "context": "There is converging evidence in developmental psychology that human infants, primates, children, and adults in diverse cultures base their core knowledge on certain cognitive systems including \u2013 entities, agents and their actions, numerical quantities, space, social-structures and intuitive theories [34, 19].", "startOffset": 301, "endOffset": 309}, {"referenceID": 18, "context": "There is converging evidence in developmental psychology that human infants, primates, children, and adults in diverse cultures base their core knowledge on certain cognitive systems including \u2013 entities, agents and their actions, numerical quantities, space, social-structures and intuitive theories [34, 19].", "startOffset": 301, "endOffset": 309}, {"referenceID": 3, "context": "Decomposition of the successor representation yields reasonable sub-goals for spatial navigation problems [4, 10, 35].", "startOffset": 106, "endOffset": 117}, {"referenceID": 9, "context": "Decomposition of the successor representation yields reasonable sub-goals for spatial navigation problems [4, 10, 35].", "startOffset": 106, "endOffset": 117}, {"referenceID": 34, "context": "Decomposition of the successor representation yields reasonable sub-goals for spatial navigation problems [4, 10, 35].", "startOffset": 106, "endOffset": 117}, {"referenceID": 1, "context": "[2] have written a general overview of hierarchical reinforcement learning in the context of cognitive science and neuroscience.", "startOffset": 0, "endOffset": 3}, {"referenceID": 38, "context": "We use the temporal abstraction of options [39] to define policies \u03c0g for each goal g.", "startOffset": 43, "endOffset": 47}, {"referenceID": 29, "context": "One can also view this setup as similar to optimizing over the space of optimal reward functions to maximize fitness [30].", "startOffset": 117, "endOffset": 121}, {"referenceID": 19, "context": "We use the Deep Q-Learning framework [20] to learn policies for both the controller and the meta-controller.", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "Following [20], the parameters \u03b81,i\u22121 from the previous iteration are held fixed when optimising the loss function.", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "This is a modified version of the MDP in [24], with the reward structure adding complexity to the task.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "For instance, the basic DQN [20] achieves a score of 0 while even the best performing system, Gorila DQN [22], manages only 4.", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "For instance, the basic DQN [20] achieves a score of 0 while even the best performing system, Gorila DQN [22], manages only 4.", "startOffset": 105, "endOffset": 109}, {"referenceID": 33, "context": "Inspired by the developmental psychology literature [34] and object-oriented MDPs [7], we use entities or objects in the scene to parameterize goals in this environment.", "startOffset": 52, "endOffset": 56}, {"referenceID": 6, "context": "Inspired by the developmental psychology literature [34] and object-oriented MDPs [7], we use entities or objects in the scene to parameterize goals in this environment.", "startOffset": 82, "endOffset": 85}, {"referenceID": 8, "context": "Unsupervised detection of objects in visual scenes is an open problem in computer vision, although there has been recent progress in obtaining objects directly from image or motion data [9, 8, 11].", "startOffset": 186, "endOffset": 196}, {"referenceID": 7, "context": "Unsupervised detection of objects in visual scenes is an open problem in computer vision, although there has been recent progress in obtaining objects directly from image or motion data [9, 8, 11].", "startOffset": 186, "endOffset": 196}, {"referenceID": 10, "context": "Unsupervised detection of objects in visual scenes is an open problem in computer vision, although there has been recent progress in obtaining objects directly from image or motion data [9, 8, 11].", "startOffset": 186, "endOffset": 196}, {"referenceID": 0, "context": "We use the Arcade Learning Environment [1] to perform experiments.", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "There has been recent work [8, 11, 25, 18, 41, 12, 16] in using deep generative models to disentangle multiple factors of variations (objects, pose, location, etc) from pixel data.", "startOffset": 27, "endOffset": 54}, {"referenceID": 10, "context": "There has been recent work [8, 11, 25, 18, 41, 12, 16] in using deep generative models to disentangle multiple factors of variations (objects, pose, location, etc) from pixel data.", "startOffset": 27, "endOffset": 54}, {"referenceID": 24, "context": "There has been recent work [8, 11, 25, 18, 41, 12, 16] in using deep generative models to disentangle multiple factors of variations (objects, pose, location, etc) from pixel data.", "startOffset": 27, "endOffset": 54}, {"referenceID": 17, "context": "There has been recent work [8, 11, 25, 18, 41, 12, 16] in using deep generative models to disentangle multiple factors of variations (objects, pose, location, etc) from pixel data.", "startOffset": 27, "endOffset": 54}, {"referenceID": 40, "context": "There has been recent work [8, 11, 25, 18, 41, 12, 16] in using deep generative models to disentangle multiple factors of variations (objects, pose, location, etc) from pixel data.", "startOffset": 27, "endOffset": 54}, {"referenceID": 11, "context": "There has been recent work [8, 11, 25, 18, 41, 12, 16] in using deep generative models to disentangle multiple factors of variations (objects, pose, location, etc) from pixel data.", "startOffset": 27, "endOffset": 54}, {"referenceID": 15, "context": "There has been recent work [8, 11, 25, 18, 41, 12, 16] in using deep generative models to disentangle multiple factors of variations (objects, pose, location, etc) from pixel data.", "startOffset": 27, "endOffset": 54}, {"referenceID": 14, "context": "There has been some recent work in using recurrent networks in conjunction with reinforcement learning [15, 23].", "startOffset": 103, "endOffset": 111}, {"referenceID": 22, "context": "There has been some recent work in using recurrent networks in conjunction with reinforcement learning [15, 23].", "startOffset": 103, "endOffset": 111}], "year": 2016, "abstractText": "Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete MDP with stochastic transitions, and (2) the classic ATARI game \u2018Montezuma\u2019s Revenge\u2019.", "creator": "LaTeX with hyperref package"}}}