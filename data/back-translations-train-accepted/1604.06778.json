{"id": "1604.06778", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2016", "title": "Benchmarking Deep Reinforcement Learning for Continuous Control", "abstract": "Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released open-source in order to facilitate experimental reproducibility and to encourage adoption by other researchers.", "histories": [["v1", "Fri, 22 Apr 2016 18:57:24 GMT  (2048kb,D)", "http://arxiv.org/abs/1604.06778v1", null], ["v2", "Mon, 25 Apr 2016 06:16:06 GMT  (2046kb,D)", "http://arxiv.org/abs/1604.06778v2", "14 pages, to appear in ICML 2016"], ["v3", "Fri, 27 May 2016 19:25:59 GMT  (2309kb,D)", "http://arxiv.org/abs/1604.06778v3", "14 pages, ICML 2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO", "authors": ["yan duan", "xi chen", "rein houthooft", "john schulman", "pieter abbeel"], "accepted": true, "id": "1604.06778"}, "pdf": {"name": "1604.06778.pdf", "metadata": {"source": "META", "title": "Benchmarking Deep Reinforcement Learning for Continuous Control", "authors": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "emails": ["ROCKYDUAN@EECS.BERKELEY.EDU", "C.XI@EECS.BERKELEY.EDU", "REIN.HOUTHOOFT@UGENT.BE", "JOSCHU@EECS.BERKELEY.EDU", "PABBEEL@CS.BERKELEY.EDU"], "sections": [{"heading": "1. Introduction", "text": "It is about the question to what extent people are able to survive themselves, and the question to what extent they are able to survive themselves, and to what extent they are able to survive themselves. The question to what extent they are able to survive themselves is not only in the question, but also in the question to what extent they are able to survive themselves, and whether they are able to survive themselves. The question to what extent they are able to survive themselves, but also in the question to what extent they are able to survive themselves, to survive, to survive, to survive, to survive, to survive, to survive, to predominate, to predominate, to predominate, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive, to survive."}, {"heading": "2. Preliminaries", "text": "In this section, we define the notation used in the following sections. The implemented tasks correspond to the standard interface of a finite horizon discounted Markov decision process (MDP), defined by the tuple (S, A, P, r, \u03c10, \u03b3, T), where S is a (possibly infinite) series of states, A is a set of measures, P: S \u00b7 A \u00b7 S \u2192 R \u2265 0 is the transition probability distribution, r: S \u00b7 A \u2192 R is the reward function, 0: S \u2192 R \u2265 0 is the initial state distribution, \u03b3 (0, 1) is the discount factor, and T is the horizon. For partially observable tasks corresponding to the interface of a partially observable Markov decision process (POMDP), two additional components are required, namely a series of observations, and O: S \u00b2 \u00b2 \u00b2 is the observation probability distribution. Most of our implemented algorithms optimize a chastic policy."}, {"heading": "3. Tasks", "text": "The tasks in the presented benchmark can be divided into four categories: basic tasks, motion tasks, partially observable tasks and hierarchical tasks. For each task, we provide a short description and motivation for inclusion in the test bed. More detailed specifications can be found in the supplementary materials and in the source code. We opt to implement all tasks with physical simulators and not with symbolic equations, as the earlier approach is less prone to errors and allows easy modification of each task. Tasks with simple dynamics are implemented with the help of Box2D (Catto, 2011), an open source, freely available 2D physics simulator. Tasks with more complicated dynamics, such as locomotion, are accomplished with the help of MuJoCo (Todorov et al., 2012), a 3D physics simulator with better modeling of contacts."}, {"heading": "3.1. Basic Tasks", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "3.2. Locomotion Tasks", "text": "In this category, we implement six locomotion tasks of varying dynamics and difficulty; the goal for all tasks is to move forward as quickly as possible; these tasks are more challenging than the basic tasks due to the high degree of freedom; in addition, a large amount of exploration is required to learn to move forward without being stuck with local optimises; and since we punish excessive controls and falling over during the initial phase of learning, when the robot is not yet able to move forward for a sufficient distance without falling, apparent local optima exist, including staying at the origin or diving forward cautiously. Swimmers (Purcell, 1977; Coulom, 2002; Levine & Koltun, 2013; Schulman et al, 2015a): The float is a planar robot with 3 connections and 2 activated joints. Fluid is simulated by viscosity forces applied to each connection so that the float can move forward."}, {"heading": "3.3. Partially Observable Tasks", "text": "To evaluate algorithms in more realistic environments, for each of the five basic tasks described in Section 3.1, we implement three variations of partially observable tasks, resulting in a total of 5 x 3 = 15 additional tasks, which are described below. Limited Sensors: For this variation, we limit observations to providing only position information (including common angles), excluding velocities. An actor must now learn to derive velocity information in order to recover full state. Similar tasks have been investigated in Gomez & Miikkulainen (1998); Scha \ufffd fer & Udluft (2005); Heess et al. (2015a); Wierstra et al. (2007)."}, {"heading": "3.4. Hierarchical Tasks", "text": "Many tasks in the real world have a hierarchical structure, where higher-level decisions can reuse lower-level skills (Parr & Russell, 1998; Sutton et al., 1999; Dietterich, 2000).We suggest several tasks that require both low-level motor control and high-level decision.These two components each operate on a different time scale and require a natural hierarchy to learn the task efficiently. Composition + Food Collection: For this task, the agent must learn to control either the float or the ant robot to collect food and avoid bombs in a limited region.The agent receives range sensors via nearby food and bomb units. He receives a positive reward when he reaches a food unit, or a negative reward when he reaches a bomb. Locommotion + Maze: For this task, the agent must learn to control either the float or the positive target when he reaches a target messenger within a designated range, or a target messenger is visible in a laboratory."}, {"heading": "4. Algorithms", "text": "In this section, we briefly summarize the algorithms implemented in our benchmark and note changes to the application to general parameterized policies. We implement a number of gradient-based search methods as well as two gradient-free methods for comparison with the gradient-based approaches."}, {"heading": "4.1. Batch Algorithms", "text": "Most of the implemented algorithms are batch algorithms for reducing the deviation. At each iteration, N-trajectories = approximately the same course (2010), all trajectories are recorded taking into account the current policy (2013). (These algorithms estimate the gradient-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-to-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-to-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-to-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-to-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-to-ratio-ratio-ratio-ratio-ratio-ratio-ratio-to-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-to-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-ratio-to-ratio-ratio-ratio-ratio-ratio-ratio-ratio"}, {"heading": "4.2. Online Algorithms", "text": "Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2015): Compared to batch algorithms, the DDPG algorithm continuously improves policy in environmental research. It applies the gradient descent to policy using minibatch data sampled from a replay pool, where the gradient value is calculated via the \"2 loss of the Bellman error L = 1B-B i = 1 (yi \u2212 Q\u03c6 (si, ai)) -2, with yi = ri + \u03b3\u03bcul (si), where B is the batch size. To improve the stability of the algorithm, we use target networks both for the critic and for policy in the formation of the regression target yi = ri + \u0421Q \u00b2."}, {"heading": "4.3. Recurrent Variants", "text": "The only modification required is to replace \u03c0 (ait | sit) with \u03c0 (ait | oi1: t, ai1: t \u2212 1), where oi1: t and a1: t \u2212 1 are the histories of past and current observations and past actions. Recurring versions of amplification learning algorithms have been studied in many existing papers, such as Bakker (2001), Scha \ufffd fer & Udluft (2005), Wierstra et al. (2007) and Heess et al. (2015a)."}, {"heading": "5. Experiment Setup", "text": "In this section we will work on the experimental setup used to generate outcomes. Performance metrics: For each reporting unit (a specific algorithm that runs on a specific task) we define its performance as 1 \u2211 Ii = 1Ni \u2211 I = 1 \u2211 Ni n = 1Rin, where I am the number of training siterations, Ni is the number of trajectories collected in the iteration, and Rin is the undiscounted return for the nth iteration, hyperparameter tuning: For the DPG algorithms we used the hyperparametrics that are reported in Lillicrap et al. (2015) For the other algorithms we follow the approach in (Mnih et al., 2015), and we select two tasks in each category on which a network search is performed."}, {"heading": "6. Results and Discussion", "text": "The most important evaluation results are presented in Table 1. The tasks on which the network search is carried out are large (*). In each entry, the pair of numbers shows the mean and standard deviation of the normative and locomotive tasks. However, even for high-value tasks such as Ant, REINFORCE, we can find that REINFORCE sometimes suffers from premature conversion to local optimization, as noted by Peters & Schaal (2008), which explains the gaps between REINFORCE and TNPG on tasks such as Walker (Figure 4). By visualizing the final policy, we can see that REINFORCE results in policy tend to leap forward and maximize a short-term return."}, {"heading": "7. Related Work", "text": "In this section, we review existing benchmarks for continuous control tasks. The earliest efforts to evaluate reinforcement learning algorithms began in the form of individual control problems described symbolically. Some common tasks include the reverse pendulum (Stephenson, 1908; Donaldson, 1960; Widrow, 1964), the mountain automobile (Moore, 1990) and the Akrobot (DeJong & Spong, 1994).These problems are often incorporated into broader benchmarks.Some reinforcement learner benchmarks contain low-dimensional continuous control tasks, such as those presented above, including RLLib (Abeyruwan, 2013), MMLF (Metzen & Edgington, 2011), RL-Toolbox (Neumann, 2006), JRLF (Kochenderfer, 2006), Beliefbox (Dimitrakakis et al, al)."}, {"heading": "8. Conclusion", "text": "Results show that among the algorithms implemented, TNPG, TRPO and DDPG are effective methods for forming deep neural network policies. However, the poor performance of the proposed hierarchical tasks requires the development of new algorithms. Implementing and evaluating existing and newly proposed algorithms will be our ongoing efforts. By providing an open source publication of the benchmark, we encourage other researchers to evaluate their algorithms in terms of the proposed tasks."}, {"heading": "1. Task Specifications", "text": "Below are some specifications for the observations, actions, and rewards of the task. Please refer to the benchmark source code (https: / / github.com / rllab / rllab) for the full specification of the physical parameters."}, {"heading": "1.1. Basic Tasks", "text": "The reward function is given by r (s, a).The reward function is given by r (s, a): = cos (s, a): = cos (s, a).The episode ends when | x | > 3, with a penalty of \u2212 100.Mountain Car: The observation is given by the horizontal position x and the horizontal speed x (s, a).The reward function is ended by r (s, a): = cos (s, c).The episode ends when | x | > 3, with a penalty of \u2212 100.Mountain Car: The observation is given by the horizontal position x and the horizontal speed x (s, c): The reward is given by r (s, a)."}, {"heading": "1.2. Locomotion Tasks", "text": "The reward is given by r (s, a) = vx \u2212 0.005% a \u00b2 22, where vx is the contact velocity. \u00b7 No contact condition is applied. \u00b7 Hopper: The 20-dimensional observation includes joint angles, joint velocities, the coordinates of the mass center of gravity and limiting forces. The reward is given by r (s, a): = vx \u2212 0.005 \u00b7 a \u00b2 22 + 1, where the last term is a bonus for life. \u2212 The episode ends when zbody < 0.7 where zbody is the Z coordinate of the body."}, {"heading": "1.3. Partially Observable Tasks", "text": "Limited Sensors: The full description is given in the main text. Loud observations and delayed actions: For all tasks we use a Gaussan noise with \u03c3 = 0.1. The time delay is as follows: Cart-Pole Balancing 0.15 sec, Cart-Pole Swing Up 0.15 sec, Mountain Car 0.15 sec, Acrobot Swing Up 0.06 sec and Double Inverted Pendulum Balancing 0.06 sec. This corresponds to 3 discretion frames for each task. System identifiations: For Cart-Pole Balancing and Cart-Pole Swing Up, the bar length varies evenly between 50% and 150%. For Mountain Car, the width of the valley varies evenly between 75% and 125%. For Acrobot Swing Up, each bar length varies evenly between 50% and 150%. For Double Inverted Pendulum Balancing, each bar length varies evenly between 83% and 167%."}, {"heading": "1.4. Hierarchical Tasks", "text": "Locomotion + Food Collection: During each episode, 8 food units and 8 bombs are placed in the area. Collecting one food unit gives + 1 reward, and collecting a bomb gives \u2212 1. Therefore, the best cumulative reward for a particular episode is 8.Locomotion + Maze: During each episode, a + 1 reward is given if the robot reaches the target. Otherwise, the robot receives a zero reward during the entire episode."}, {"heading": "2. Experiment Parameters", "text": "For all batch gradient-based algorithms, we use the same time-varying characteristic coding for the linear baseline: \u03c6s, t = concat (s, s, 0.01 t, (0.01 t) 2, (0.01 t) 3, 1), where s is the state vector and represents an elementary product.Table 2 shows the experimental parameters for all four categories. We will then detail the hyperparameter search range for the selected tasks and report the best hyperparameters shown in Tables 3, 4, 5, 6, 7 and 8."}], "references": [{"title": "RLLib: Lightweight standard and on/off policy reinforcement learning library (C++)", "author": ["S. Abeyruwan"], "venue": "http://web. cs.miami.edu/home/saminda/rilib.html,", "citeRegEx": "Abeyruwan,? \\Q2013\\E", "shortCiteRegEx": "Abeyruwan", "year": 2013}, {"title": "Reinforcement learning with long short-term memory", "author": ["B. Bakker"], "venue": "In NIPS, pp", "citeRegEx": "Bakker,? \\Q2001\\E", "shortCiteRegEx": "Bakker", "year": 2001}, {"title": "The Arcade Learning Environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "J. Artif. Intell. Res.,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "ApproxRL: A Matlab toolbox for approximate RL and DP", "author": ["L. Busoniu"], "venue": "http://busoniu.net/files/repository/ readme-approxrl.html,", "citeRegEx": "Busoniu,? \\Q2010\\E", "shortCiteRegEx": "Busoniu", "year": 2010}, {"title": "Box2D: A 2D physics engine for games", "author": ["E. Catto"], "venue": null, "citeRegEx": "Catto,? \\Q2011\\E", "shortCiteRegEx": "Catto", "year": 2011}, {"title": "Reinforcement learning using neural networks, with applications to motor control", "author": ["Coulom", "R\u00e9mi"], "venue": "PhD thesis, Institut National Polytechnique de Grenoble-INPG,", "citeRegEx": "Coulom and R\u00e9mi.,? \\Q2002\\E", "shortCiteRegEx": "Coulom and R\u00e9mi.", "year": 2002}, {"title": "Policy evaluation with temporal differences: A survey and comparison", "author": ["C. Dann", "G. Neumann", "J. Peters"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Dann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dann et al\\.", "year": 2014}, {"title": "A survey on policy search for robotics, foundations and trends in robotics", "author": ["M.P. Deisenroth", "G. Neumann", "J. Peters"], "venue": "Found. Trends Robotics,", "citeRegEx": "Deisenroth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2013}, {"title": "Swinging up the Acrobot: An example of intelligent control", "author": ["G. DeJong", "M.W. Spong"], "venue": "In ACC,", "citeRegEx": "DeJong and Spong,? \\Q1994\\E", "shortCiteRegEx": "DeJong and Spong", "year": 1994}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. FeiFei"], "venue": "In CVPR, pp", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["T.G. Dietterich"], "venue": "J. Artif. Intell. Res,", "citeRegEx": "Dietterich,? \\Q2000\\E", "shortCiteRegEx": "Dietterich", "year": 2000}, {"title": "Beliefbox: A framework for statistical methods in sequential decision making", "author": ["C. Dimitrakakis", "N. Tziortziotis", "A. Tossou"], "venue": "http://code.google.com/p/beliefbox/,", "citeRegEx": "Dimitrakakis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dimitrakakis et al\\.", "year": 2007}, {"title": "Error decorrelation: a technique for matching a class of functions", "author": ["P.E.K. Donaldson"], "venue": "In Proc. 3th Intl. Conf. Medical Electronics,", "citeRegEx": "Donaldson,? \\Q1960\\E", "shortCiteRegEx": "Donaldson", "year": 1960}, {"title": "Reinforcement learning in continuous time and space", "author": ["K. Doya"], "venue": "Neural Comput.,", "citeRegEx": "Doya,? \\Q2000\\E", "shortCiteRegEx": "Doya", "year": 2000}, {"title": "Infinite horizon model predictive control for nonlinear periodic tasks", "author": ["Erez", "Tom", "Tassa", "Yuval", "Todorov", "Emanuel"], "venue": "Manuscript under review,", "citeRegEx": "Erez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Erez et al\\.", "year": 2011}, {"title": "The pascal visual object classes (VOC) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "Int. J. Comput. Vision,", "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "One-shot learning of object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Fei.Fei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2006}, {"title": "Computer control of a double inverted pendulum", "author": ["K. Furuta", "T. Okutani", "H. Sone"], "venue": "Comput. Electr. Eng.,", "citeRegEx": "Furuta et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Furuta et al\\.", "year": 1978}, {"title": "SWITCHBOARD: Telephone speech corpus for research and development", "author": ["J.J. Godfrey", "E.C. Holliman", "J. McDaniel"], "venue": "In ICASSP,", "citeRegEx": "Godfrey et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Godfrey et al\\.", "year": 1992}, {"title": "2-d pole balancing with recurrent evolutionary networks", "author": ["F. Gomez", "R. Miikkulainen"], "venue": "In ICANN,", "citeRegEx": "Gomez and Miikkulainen,? \\Q1998\\E", "shortCiteRegEx": "Gomez and Miikkulainen", "year": 1998}, {"title": "Deep learning for real-time Atari game play using offline monte-carlo tree search planning", "author": ["X. Guo", "S. Singh", "H. Lee", "R.L. Lewis", "X. Wang"], "venue": "In NIPS,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Completely derandomized self-adaptation in evolution strategies", "author": ["N. Hansen", "A. Ostermeier"], "venue": "Evol. Comput.,", "citeRegEx": "Hansen and Ostermeier,? \\Q2001\\E", "shortCiteRegEx": "Hansen and Ostermeier", "year": 2001}, {"title": "Memory-based control with recurrent neural networks", "author": ["N. Heess", "J. Hunt", "T. Lillicrap", "D. Silver"], "venue": null, "citeRegEx": "Heess et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2015}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["N. Heess", "G. Wayne", "D. Silver", "T. Lillicrap", "T. Erez", "T. Tassa"], "venue": "In NIPS,", "citeRegEx": "Heess et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2015}, {"title": "The open-source TEXPLORE code release for reinforcement learning on robots", "author": ["T. Hester", "P. Stone"], "venue": "RoboCup", "citeRegEx": "Hester and Stone,? \\Q2013\\E", "shortCiteRegEx": "Hester and Stone", "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "Mohamed", "A.-R", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.S.G. Dahl", "B. Kingsbury"], "venue": "IEEE Signal Process. Mag,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "The Aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions. In ASR2000-Automatic Speech Recognition: Challenges for the new Millenium ISCA Tutorial and Research", "author": ["Hirsch", "H.-G", "D. Pearce"], "venue": null, "citeRegEx": "Hirsch et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hirsch et al\\.", "year": 2000}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "A natural policy gradient", "author": ["S.M. Kakade"], "venue": "In NIPS, pp", "citeRegEx": "Kakade,? \\Q2002\\E", "shortCiteRegEx": "Kakade", "year": 2002}, {"title": "Stochastic real-valued reinforcement learning to solve a nonlinear control problem", "author": ["H. Kimura", "S. Kobayashi"], "venue": "In IEEE SMC, pp", "citeRegEx": "Kimura and Kobayashi,? \\Q1999\\E", "shortCiteRegEx": "Kimura and Kobayashi", "year": 1999}, {"title": "Policy search for motor primitives in robotics", "author": ["J. Kober", "J. Peters"], "venue": "In NIPS, pp", "citeRegEx": "Kober and Peters,? \\Q2009\\E", "shortCiteRegEx": "Kober and Peters", "year": 2009}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Guided policy search", "author": ["S. Levine", "V. Koltun"], "venue": "In ICML, pp", "citeRegEx": "Levine and Koltun,? \\Q2013\\E", "shortCiteRegEx": "Levine and Koltun", "year": 2013}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": null, "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T. Lillicrap", "J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": null, "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "In ICCV, pp", "citeRegEx": "Martin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2001}, {"title": "BOXES: An experiment in adaptive control", "author": ["D. Michie", "R.A. Chambers"], "venue": "Machine Intelligence,", "citeRegEx": "Michie and Chambers,? \\Q1968\\E", "shortCiteRegEx": "Michie and Chambers", "year": 1968}, {"title": "Efficient memory-based learning for robot control", "author": ["A. Moore"], "venue": "Technical report,", "citeRegEx": "Moore,? \\Q1990\\E", "shortCiteRegEx": "Moore", "year": 1990}, {"title": "A case study in approximate linearization: The Acrobot example", "author": ["R.M. Murray", "J. Hauser"], "venue": "Technical report, UC Berkeley, EECS Department,", "citeRegEx": "Murray and Hauser,? \\Q1991\\E", "shortCiteRegEx": "Murray and Hauser", "year": 1991}, {"title": "3D balance in legged locomotion: modeling and simulation for the one-legged case", "author": ["S.S. Murthy", "M.H. Raibert"], "venue": "ACM SIGGRAPH Computer Graphics,", "citeRegEx": "Murthy and Raibert,? \\Q1984\\E", "shortCiteRegEx": "Murthy and Raibert", "year": 1984}, {"title": "A reinforcement learning toolbox and RL benchmarks for the control of dynamical systems. Dynamical principles for neuroscience and intelligent biomimetic", "author": ["G. Neumann"], "venue": null, "citeRegEx": "Neumann,? \\Q2006\\E", "shortCiteRegEx": "Neumann", "year": 2006}, {"title": "dotrl: A platform for rapid reinforcement learning methods development and validation", "author": ["B. Papis", "P. Wawrzy\u0144ski"], "venue": "In FedCSIS,", "citeRegEx": "Papis and Wawrzy\u0144ski,? \\Q2013\\E", "shortCiteRegEx": "Papis and Wawrzy\u0144ski", "year": 2013}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["Parr", "Ronald", "Russell", "Stuart"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Parr et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Parr et al\\.", "year": 1998}, {"title": "Policy Gradient Toolbox", "author": ["J. Peters"], "venue": "http://www.ausy. tu-darmstadt.de/Research/PolicyGradientToolbox,", "citeRegEx": "Peters,? \\Q2002\\E", "shortCiteRegEx": "Peters", "year": 2002}, {"title": "Reinforcement learning by rewardweighted regression for operational space control", "author": ["J. Peters", "S. Schaal"], "venue": "In ICML, pp", "citeRegEx": "Peters and Schaal,? \\Q2007\\E", "shortCiteRegEx": "Peters and Schaal", "year": 2007}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["J. Peters", "S. Schaal"], "venue": "Neural networks,", "citeRegEx": "Peters and Schaal,? \\Q2008\\E", "shortCiteRegEx": "Peters and Schaal", "year": 2008}, {"title": "Policy gradient methods for robot control", "author": ["J. Peters", "S. Vijaykumar", "S. Schaal"], "venue": "Technical report,", "citeRegEx": "Peters et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2003}, {"title": "Relative entropy policy search", "author": ["J. Peters", "K. M\u00fclling", "Y. Alt\u00fcn"], "venue": "In AAAI,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Life at low Reynolds number", "author": ["E.M. Purcell"], "venue": "Am. J. Phys,", "citeRegEx": "Purcell,? \\Q1977\\E", "shortCiteRegEx": "Purcell", "year": 1977}, {"title": "Animation of dynamic legged locomotion", "author": ["M.H. Raibert", "J.K. Hodgins"], "venue": "In ACM SIGGRAPH Computer Graphics,", "citeRegEx": "Raibert and Hodgins,? \\Q1991\\E", "shortCiteRegEx": "Raibert and Hodgins", "year": 1991}, {"title": "CLS2: Closed loop simulation system", "author": ["M. Riedmiller", "M. Blum", "T. Lampe"], "venue": "http://ml.informatik. uni-freiburg.de/research/clsquare,", "citeRegEx": "Riedmiller et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Riedmiller et al\\.", "year": 2012}, {"title": "The cross-entropy method for combinatorial and continuous optimization", "author": ["R. Rubinstein"], "venue": "Methodol. Comput. Appl. Probab.,", "citeRegEx": "Rubinstein,? \\Q1999\\E", "shortCiteRegEx": "Rubinstein", "year": 1999}, {"title": "Solving partially observable reinforcement learning problems with recurrent neural networks", "author": ["A.M. Sch\u00e4fer", "S. Udluft"], "venue": "In ECML Workshops,", "citeRegEx": "Sch\u00e4fer and Udluft,? \\Q2005\\E", "shortCiteRegEx": "Sch\u00e4fer and Udluft", "year": 2005}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "In ICML, pp", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M.I. Jordan", "P. Abbeel"], "venue": null, "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "On induced stability", "author": ["A. Stephenson"], "venue": "Philos. Mag.,", "citeRegEx": "Stephenson,? \\Q1908\\E", "shortCiteRegEx": "Stephenson", "year": 1908}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Learning Tetris using the noisy cross-entropy method", "author": ["I. Szita", "A. L\u0151rincz"], "venue": "Neural Comput.,", "citeRegEx": "Szita and L\u0151rincz,? \\Q2006\\E", "shortCiteRegEx": "Szita and L\u0151rincz", "year": 2006}, {"title": "\u03b5-MDPs: Learning in varying environments", "author": ["I. Szita", "B. Tak\u00e1cs", "A. L\u00f6rincz"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Szita et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Szita et al\\.", "year": 2003}, {"title": "Synthesis and stabilization of complex behaviors through online trajectory optimization", "author": ["Tassa", "Yuval", "Erez", "Tom", "Todorov", "Emanuel"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "Tassa et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tassa et al\\.", "year": 2012}, {"title": "Temporal difference learning and TDGammon", "author": ["G. Tesauro"], "venue": "Commun. ACM,", "citeRegEx": "Tesauro,? \\Q1995\\E", "shortCiteRegEx": "Tesauro", "year": 1995}, {"title": "MuJoCo: A physics engine for model-based control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": "In IROS, pp", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Learning of nonparametric control policies with high-dimensional state features", "author": ["H. van Hoof", "J. Peters", "G. Neumann"], "venue": "In AISTATS,", "citeRegEx": "Hoof et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hoof et al\\.", "year": 2015}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["M. Watter", "J. Springenberg", "J. Boedecker", "M. Riedmiller"], "venue": "In NIPS,", "citeRegEx": "Watter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watter et al\\.", "year": 2015}, {"title": "Learning to control a 6-degree-of-freedom walking robot", "author": ["P. Wawrzy\u0144ski"], "venue": "In IEEE EUROCON, pp", "citeRegEx": "Wawrzy\u0144ski,? \\Q2007\\E", "shortCiteRegEx": "Wawrzy\u0144ski", "year": 2007}, {"title": "Pattern recognition and adaptive control", "author": ["B. Widrow"], "venue": "IEEE Trans. Ind. Appl.,", "citeRegEx": "Widrow,? \\Q1964\\E", "shortCiteRegEx": "Widrow", "year": 1964}, {"title": "Solving deep memory POMDPs with recurrent policy gradients", "author": ["D. Wierstra", "A. Foerster", "J. Peters", "J. Schmidhuber"], "venue": "In ICANN,", "citeRegEx": "Wierstra et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wierstra et al\\.", "year": 2007}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Mach. Learn.,", "citeRegEx": "Williams,? \\Q1992\\E", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "SkyAI: Highly modularized reinforcement learning library", "author": ["A. Yamaguchi", "T. Ogasawara"], "venue": "In IEEE-RAS Humanoids,", "citeRegEx": "Yamaguchi and Ogasawara,? \\Q2010\\E", "shortCiteRegEx": "Yamaguchi and Ogasawara", "year": 2010}, {"title": "Automated directory assistance system - from theory to practice", "author": ["D. Yu", "Ju", "Y.-C", "Wang", "Y.-Y", "G. Zweig", "A. Acero"], "venue": "In Interspeech,", "citeRegEx": "Yu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 33, "context": "Recently, significant progress has been made by combining advances in deep learning for learning feature representations (Krizhevsky et al., 2012; Hinton et al., 2012) with reinforcement learning, tracing back to much earlier work of Tesauro (1995) and Bertsekas & Tsitsiklis (1995).", "startOffset": 121, "endOffset": 167}, {"referenceID": 26, "context": "Recently, significant progress has been made by combining advances in deep learning for learning feature representations (Krizhevsky et al., 2012; Hinton et al., 2012) with reinforcement learning, tracing back to much earlier work of Tesauro (1995) and Bertsekas & Tsitsiklis (1995).", "startOffset": 121, "endOffset": 167}, {"referenceID": 21, "context": "Notable examples are training agents to play Atari games based on raw pixels (Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015a) and to acquire advanced manipulation skills using raw sensory inputs (Levine et al.", "startOffset": 77, "endOffset": 138}, {"referenceID": 35, "context": ", 2015a) and to acquire advanced manipulation skills using raw sensory inputs (Levine et al., 2015; Lillicrap et al., 2015; Watter et al., 2015).", "startOffset": 78, "endOffset": 144}, {"referenceID": 36, "context": ", 2015a) and to acquire advanced manipulation skills using raw sensory inputs (Levine et al., 2015; Lillicrap et al., 2015; Watter et al., 2015).", "startOffset": 78, "endOffset": 144}, {"referenceID": 65, "context": ", 2015a) and to acquire advanced manipulation skills using raw sensory inputs (Levine et al., 2015; Lillicrap et al., 2015; Watter et al., 2015).", "startOffset": 78, "endOffset": 144}, {"referenceID": 23, "context": ", 2012; Hinton et al., 2012) with reinforcement learning, tracing back to much earlier work of Tesauro (1995) and Bertsekas & Tsitsiklis (1995).", "startOffset": 8, "endOffset": 110}, {"referenceID": 23, "context": ", 2012; Hinton et al., 2012) with reinforcement learning, tracing back to much earlier work of Tesauro (1995) and Bertsekas & Tsitsiklis (1995). Notable examples are training agents to play Atari games based on raw pixels (Guo et al.", "startOffset": 8, "endOffset": 144}, {"referenceID": 2, "context": "Along with this recent progress, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a popular benchmark for evaluating algorithms designed for tasks with high-dimensional state inputs and discrete actions.", "startOffset": 71, "endOffset": 95}, {"referenceID": 3, "context": "For instance, algorithms based on Q-learning quickly become infeasible when naive discretization of the action space is performed, due to the curse of dimensionality (Bellman, 1957; Lillicrap et al., 2015).", "startOffset": 166, "endOffset": 205}, {"referenceID": 36, "context": "For instance, algorithms based on Q-learning quickly become infeasible when naive discretization of the action space is performed, due to the curse of dimensionality (Bellman, 1957; Lillicrap et al., 2015).", "startOffset": 166, "endOffset": 205}, {"referenceID": 17, "context": ", 1998), Caltech101 (Fei-Fei et al., 2006), CIFAR (Krizhevsky & Hinton, 2009), ImageNet (Deng et al.", "startOffset": 20, "endOffset": 42}, {"referenceID": 10, "context": ", 2006), CIFAR (Krizhevsky & Hinton, 2009), ImageNet (Deng et al., 2009), PASCAL VOC (Everingham et al.", "startOffset": 53, "endOffset": 72}, {"referenceID": 16, "context": ", 2009), PASCAL VOC (Everingham et al., 2010), BSDS500 (Martin et al.", "startOffset": 20, "endOffset": 45}, {"referenceID": 37, "context": ", 2010), BSDS500 (Martin et al., 2001), SWITCHBOARD (Godfrey et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 19, "context": ", 2001), SWITCHBOARD (Godfrey et al., 1992), TIMIT (Garofolo et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 71, "context": ", 1993), Aurora (Hirsch & Pearce, 2000), and VoiceSearch (Yu et al., 2007).", "startOffset": 57, "endOffset": 74}, {"referenceID": 5, "context": "Box2D (Catto, 2011), an open-source, freely available 2D physics simulator.", "startOffset": 6, "endOffset": 19}, {"referenceID": 63, "context": "Tasks with more complicated dynamics, such as locomotion, are implemented using MuJoCo (Todorov et al., 2012), a 3D physics simulator with better modeling of contacts.", "startOffset": 87, "endOffset": 109}, {"referenceID": 56, "context": "Cart-Pole Balancing: This classic task in dynamics and control theory has been originally described by Stephenson (1908), and first studied in a learning context by Donaldson (1960), Widrow (1964), and Michie & Chambers (1968).", "startOffset": 103, "endOffset": 121}, {"referenceID": 13, "context": "Cart-Pole Balancing: This classic task in dynamics and control theory has been originally described by Stephenson (1908), and first studied in a learning context by Donaldson (1960), Widrow (1964), and Michie & Chambers (1968).", "startOffset": 165, "endOffset": 182}, {"referenceID": 13, "context": "Cart-Pole Balancing: This classic task in dynamics and control theory has been originally described by Stephenson (1908), and first studied in a learning context by Donaldson (1960), Widrow (1964), and Michie & Chambers (1968).", "startOffset": 165, "endOffset": 197}, {"referenceID": 13, "context": "Cart-Pole Balancing: This classic task in dynamics and control theory has been originally described by Stephenson (1908), and first studied in a learning context by Donaldson (1960), Widrow (1964), and Michie & Chambers (1968). An inverted pendulum is mounted on a pivot point on a cart.", "startOffset": 165, "endOffset": 227}, {"referenceID": 14, "context": "This is a nonlinear extension of the previous task (Doya, 2000).", "startOffset": 51, "endOffset": 63}, {"referenceID": 39, "context": "Mountain Car: We implement a continuous version of the classic task described by Moore (1990). A car has to escape a valley by repetitive application of tangential forces.", "startOffset": 81, "endOffset": 94}, {"referenceID": 14, "context": "Acrobot Swing Up: In this widely-studied task an underactuated, two-link robot has to swing itself into an upright position (DeJong & Spong, 1994; Murray & Hauser, 1991; Doya, 2000).", "startOffset": 124, "endOffset": 181}, {"referenceID": 18, "context": "This task is more difficult than single-pole balancing, since the system is even more unstable and requires the controller to actively maintain balance (Furuta et al., 1978).", "startOffset": 152, "endOffset": 173}, {"referenceID": 50, "context": "Swimmer (Purcell, 1977; Coulom, 2002; Levine & Koltun, 2013; Schulman et al., 2015a): The swimmer is a planar robot with 3 links and 2 actuated joints.", "startOffset": 8, "endOffset": 84}, {"referenceID": 15, "context": "Hopper (Murthy & Raibert, 1984; Erez et al., 2011; Levine & Koltun, 2013; Schulman et al., 2015a): The hopper is a planar monopod robot with 4 rigid links, corresponding to the torso, upper leg, lower leg, and foot, along with 3 actuated joints.", "startOffset": 7, "endOffset": 97}, {"referenceID": 15, "context": "Walker (Raibert & Hodgins, 1991; Erez et al., 2011; Levine & Koltun, 2013; Schulman et al., 2015a): The walker is a planar biped robot consisting of 7 links, corresponding to two legs and a torso, along with 6 actuated joints.", "startOffset": 7, "endOffset": 98}, {"referenceID": 66, "context": "Half-Cheetah (Wawrzy\u0144ski, 2007; Heess et al., 2015b): The half-cheetah is a planar biped robot with 9 rigid links, including two legs and a torso, along with 6 actuated joints.", "startOffset": 13, "endOffset": 52}, {"referenceID": 61, "context": "Simple Humanoid (Tassa et al., 2012; Schulman et al., 2015b): This is a simplified humanoid model with 13 rigid links, including the head, body, arms, and legs, along with 10 actuated joints.", "startOffset": 16, "endOffset": 60}, {"referenceID": 61, "context": "Full Humanoid (Tassa et al., 2012): This is a humanoid model with 19 rigid links and 28 actuated joints.", "startOffset": 14, "endOffset": 34}, {"referenceID": 23, "context": "Similar tasks have been explored in Gomez & Miikkulainen (1998); Sch\u00e4fer & Udluft (2005); Heess et al. (2015a); Wierstra et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 23, "context": "Similar tasks have been explored in Gomez & Miikkulainen (1998); Sch\u00e4fer & Udluft (2005); Heess et al. (2015a); Wierstra et al. (2007).", "startOffset": 90, "endOffset": 135}, {"referenceID": 1, "context": "Similar tasks have been proposed in Bakker (2001).", "startOffset": 36, "endOffset": 50}, {"referenceID": 60, "context": "System Identification: For this category, the underlying physical model parameters are varied across different episodes (Szita et al., 2003).", "startOffset": 120, "endOffset": 140}, {"referenceID": 58, "context": "Many real-world tasks exhibit hierarchical structure, where higher level decisions can reuse lower level skills (Parr & Russell, 1998; Sutton et al., 1999; Dietterich, 2000).", "startOffset": 112, "endOffset": 173}, {"referenceID": 11, "context": "Many real-world tasks exhibit hierarchical structure, where higher level decisions can reuse lower level skills (Parr & Russell, 1998; Sutton et al., 1999; Dietterich, 2000).", "startOffset": 112, "endOffset": 173}, {"referenceID": 69, "context": "REINFORCE (Williams, 1992): This algorithm estimates the gradient of expected return \u2207\u03b8\u03b7(\u03c0\u03b8) using the likelihood ratio trick:", "startOffset": 10, "endOffset": 26}, {"referenceID": 29, "context": "Truncated Natural Policy Gradient (TNPG) (Kakade, 2002; Peters et al., 2003; Bagnell & Schneider, 2003; Schulman et al., 2015a): Natural Policy Gradient improves upon REINFORCE by computing an ascent direction that approximately ensures a small change in the policy distribution.", "startOffset": 41, "endOffset": 127}, {"referenceID": 48, "context": "Truncated Natural Policy Gradient (TNPG) (Kakade, 2002; Peters et al., 2003; Bagnell & Schneider, 2003; Schulman et al., 2015a): Natural Policy Gradient improves upon REINFORCE by computing an ascent direction that approximately ensures a small change in the policy distribution.", "startOffset": 41, "endOffset": 127}, {"referenceID": 29, "context": "Truncated Natural Policy Gradient (TNPG) (Kakade, 2002; Peters et al., 2003; Bagnell & Schneider, 2003; Schulman et al., 2015a): Natural Policy Gradient improves upon REINFORCE by computing an ascent direction that approximately ensures a small change in the policy distribution. This direction is derived to be I(\u03b8)\u2207\u03b8\u03b7(\u03c0\u03b8), where I(\u03b8) is the Fisher information matrix (FIM). We use the step size suggested by Peters & Schaal (2008):", "startOffset": 42, "endOffset": 433}, {"referenceID": 55, "context": "TNPG makes it practical to apply natural gradient in policy search setting with high-dimensional parameters, and we refer the reader to Schulman et al. (2015a) for more details.", "startOffset": 136, "endOffset": 160}, {"referenceID": 8, "context": "Following Deisenroth et al. (2013), we choose \u03c1 to be \u03c1(R) = R\u2212Rmin, whereRmin is the minimum return among all trajectories collected in the current iteration.", "startOffset": 10, "endOffset": 35}, {"referenceID": 49, "context": "Relative Entropy Policy Search (REPS) (Peters et al., 2010): This algorithm limits the loss of information per iteration and aims to ensure a smooth learning progress (Deisenroth et al.", "startOffset": 38, "endOffset": 59}, {"referenceID": 8, "context": ", 2010): This algorithm limits the loss of information per iteration and aims to ensure a smooth learning progress (Deisenroth et al., 2013).", "startOffset": 115, "endOffset": 140}, {"referenceID": 53, "context": "Cross Entropy Method (CEM) (Rubinstein, 1999; Szita & L\u0151rincz, 2006): Unlike previously mentioned methods, which perform exploration through stochastic actions, CEM performs exploration directly in the policy parameter space.", "startOffset": 27, "endOffset": 68}, {"referenceID": 36, "context": "Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2015): Compared to batch algorithms, the DDPG algorithm continuously improves the policy as it explores the environment.", "startOffset": 42, "endOffset": 66}, {"referenceID": 3, "context": "The critic Q is trained via gradient descent on the ` loss of the Bellman error L = 1 B \u2211B i=1(yi \u2212 Q\u03c6(si, ai)), where yi = ri + \u03b3Q\u03c6\u2032(s \u2032 i, \u03bc \u2032 \u03b8\u2032(s \u2032 i)). To improve stability of the algorithm, we use target networks for both the critic and the policy when forming the regression target yi. We refer the reader to Lillicrap et al. (2015) for a more detailed description of the algorithm.", "startOffset": 66, "endOffset": 340}, {"referenceID": 1, "context": "Recurrent versions of reinforcement learning algorithms have been studied in many existing works, such as Bakker (2001), Sch\u00e4fer & Udluft (2005), Wierstra et al.", "startOffset": 106, "endOffset": 120}, {"referenceID": 1, "context": "Recurrent versions of reinforcement learning algorithms have been studied in many existing works, such as Bakker (2001), Sch\u00e4fer & Udluft (2005), Wierstra et al.", "startOffset": 106, "endOffset": 145}, {"referenceID": 1, "context": "Recurrent versions of reinforcement learning algorithms have been studied in many existing works, such as Bakker (2001), Sch\u00e4fer & Udluft (2005), Wierstra et al. (2007), and Heess et al.", "startOffset": 106, "endOffset": 169}, {"referenceID": 1, "context": "Recurrent versions of reinforcement learning algorithms have been studied in many existing works, such as Bakker (2001), Sch\u00e4fer & Udluft (2005), Wierstra et al. (2007), and Heess et al. (2015a).", "startOffset": 106, "endOffset": 195}, {"referenceID": 36, "context": "Hyperparameter Tuning: For the DDPG algorithm, we used the hyperparametes reported in Lillicrap et al. (2015). For the other algorithms, we follow the approach in (Mnih et al.", "startOffset": 86, "endOffset": 110}, {"referenceID": 55, "context": "The log-standard deviation is parameterized by a global vector independent of the state, as done in Schulman et al. (2015a). For all partially observable tasks, we use a recurrent neural network with a single hidden layer consisting of 32 LSTM hidden units (Hochreiter & Schmidhuber, 1997).", "startOffset": 100, "endOffset": 124}, {"referenceID": 36, "context": "For the DDPG algorithm which trains a deterministic policy, we follow Lillicrap et al. (2015). For both the policy and the Q function, we use the same architecture of a feedforward neural network with 2 hidden layers, consisting of 400 and 300 hidden units with relu activations.", "startOffset": 70, "endOffset": 94}, {"referenceID": 45, "context": "However we observe that REINFORCE sometimes suffers from premature convergence to local optima as noted by Peters & Schaal (2008), which explains the performance gaps between REINFORCE and TNPG on tasks such as Walker (Figure 4(a)).", "startOffset": 107, "endOffset": 130}, {"referenceID": 45, "context": "Its final outcome is greatly affected by the performance of the initial policy, an observation that is consistent with the original work of Peters et al. (2010). This leads to a bad performance on average,", "startOffset": 140, "endOffset": 161}, {"referenceID": 45, "context": "Moreover, the tasks presented here do not assume the existence of a stationary distribution, which is assumed in Peters et al. (2010). In particular, for many of our tasks, transient behavior is of much greater interest than steady-state behavior, which agrees with previous observation by van Hoof et al.", "startOffset": 113, "endOffset": 134}, {"referenceID": 45, "context": "Moreover, the tasks presented here do not assume the existence of a stationary distribution, which is assumed in Peters et al. (2010). In particular, for many of our tasks, transient behavior is of much greater interest than steady-state behavior, which agrees with previous observation by van Hoof et al. (2015),", "startOffset": 113, "endOffset": 313}, {"referenceID": 57, "context": "Some widely adopted tasks include the inverted pendulum (Stephenson, 1908; Donaldson, 1960; Widrow, 1964), mountain car (Moore, 1990), and Acrobot (DeJong & Spong, 1994).", "startOffset": 56, "endOffset": 105}, {"referenceID": 13, "context": "Some widely adopted tasks include the inverted pendulum (Stephenson, 1908; Donaldson, 1960; Widrow, 1964), mountain car (Moore, 1990), and Acrobot (DeJong & Spong, 1994).", "startOffset": 56, "endOffset": 105}, {"referenceID": 67, "context": "Some widely adopted tasks include the inverted pendulum (Stephenson, 1908; Donaldson, 1960; Widrow, 1964), mountain car (Moore, 1990), and Acrobot (DeJong & Spong, 1994).", "startOffset": 56, "endOffset": 105}, {"referenceID": 39, "context": "Some widely adopted tasks include the inverted pendulum (Stephenson, 1908; Donaldson, 1960; Widrow, 1964), mountain car (Moore, 1990), and Acrobot (DeJong & Spong, 1994).", "startOffset": 120, "endOffset": 133}, {"referenceID": 0, "context": "Some reinforcement learning benchmarks contain lowdimensional continuous control tasks, such as the ones introduced above, including RLLib (Abeyruwan, 2013), MMLF (Metzen & Edgington, 2011), RL-Toolbox (Neumann, 2006), JRLF (Kochenderfer, 2006), Beliefbox (Dimitrakakis et al.", "startOffset": 139, "endOffset": 156}, {"referenceID": 42, "context": "Some reinforcement learning benchmarks contain lowdimensional continuous control tasks, such as the ones introduced above, including RLLib (Abeyruwan, 2013), MMLF (Metzen & Edgington, 2011), RL-Toolbox (Neumann, 2006), JRLF (Kochenderfer, 2006), Beliefbox (Dimitrakakis et al.", "startOffset": 202, "endOffset": 217}, {"referenceID": 12, "context": "Some reinforcement learning benchmarks contain lowdimensional continuous control tasks, such as the ones introduced above, including RLLib (Abeyruwan, 2013), MMLF (Metzen & Edgington, 2011), RL-Toolbox (Neumann, 2006), JRLF (Kochenderfer, 2006), Beliefbox (Dimitrakakis et al., 2007), Policy Gradient Toolbox (Peters, 2002), and ApproxRL (Busoniu, 2010).", "startOffset": 256, "endOffset": 283}, {"referenceID": 45, "context": ", 2007), Policy Gradient Toolbox (Peters, 2002), and ApproxRL (Busoniu, 2010).", "startOffset": 33, "endOffset": 47}, {"referenceID": 4, "context": ", 2007), Policy Gradient Toolbox (Peters, 2002), and ApproxRL (Busoniu, 2010).", "startOffset": 62, "endOffset": 77}, {"referenceID": 7, "context": "Tdlearn (Dann et al., 2014) includes a 20-link pole balancing task, DotRL (Papis & Wawrzy\u0144ski, 2013) includes a variable-DOF octopus arm and a 6-DOF planar cheetah model, PyBrain (Schaul et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 52, "context": "Other libraries such as CL-Square (Riedmiller et al., 2012) and RLPark (Degris et al.", "startOffset": 34, "endOffset": 59}], "year": 2016, "abstractText": "Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released open-source in order to facilitate experimental reproducibility and to encourage adoption by other researchers.", "creator": "LaTeX with hyperref package"}}}