{"id": "1604.06970", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2016", "title": "Bayesian Inference of Recursive Sequences of Group Activities from Tracks", "abstract": "We present a probabilistic generative model for inferring a description of coordinated, recursively structured group activities at multiple levels of temporal granularity based on observations of individuals' trajectories. The model accommodates: (1) hierarchically structured groups, (2) activities that are temporally and compositionally recursive, (3) component roles assigning different subactivity dynamics to subgroups of participants, and (4) a nonparametric Gaussian Process model of trajectories. We present an MCMC sampling framework for performing joint inference over recursive activity descriptions and assignment of trajectories to groups, integrating out continuous parameters. We demonstrate the model's expressive power in several simulated and complex real-world scenarios from the VIRAT and UCLA Aerial Event video data sets.", "histories": [["v1", "Sun, 24 Apr 2016 00:55:27 GMT  (668kb,D)", "http://arxiv.org/abs/1604.06970v1", "10 pages, 6 figures, in Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI'16), Phoenix, AZ, 2016"]], "COMMENTS": "10 pages, 6 figures, in Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI'16), Phoenix, AZ, 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["ernesto brau", "colin reimer dawson", "alfredo carrillo", "david sidi", "clayton t morrison"], "accepted": true, "id": "1604.06970"}, "pdf": {"name": "1604.06970.pdf", "metadata": {"source": "CRF", "title": "Bayesian Inference of Recursive Sequences of Group Activities from Tracks", "authors": ["Ernesto Brau", "Colin Dawson", "Alfredo Carrillo", "David Sidi", "Clayton T. Morrison"], "emails": ["brauavil@bc.edu", "cdawson@oberlin.edu", "isaac85@email.arizona.edu", "dsidi@email.arizona.edu", "claytonm@email.arizona.edu"], "sections": [{"heading": "1 Introduction", "text": "We have a number of open challenges and are a very active field of research (Aggarwal and Ryoo 2011; Vishwakarma and Agrawal 2013; Sukthankar et al. 2014), which includes topics of visual recognition of individual behavior (Poppe 2010), paired interactions between individuals participating in a common activity (Barbu et al. 2012; Kwak, Han and Han 2013), coordinated action sequences as an expression of planned activity (Geib and Goldman 2009), and multiple groups of individuals interacting over wide time scales. In this paper, we address the last of this kind by presenting a framework for the automatic interpretation of a high-level human activity structure observed in multiple intertwined instances of activity. We assume that subordinate visual processing provides high-quality traces of individuals moving through the scene."}, {"heading": "2 Related Work", "text": "A number of researchers have suggested playing different roles in a coordinated activity (Ryoo and Aggarwal 2011; Barbu et al. 2012; Lan, Sigal and Mori 2012; Kwak, Han, and Han 2013) These models capture the semantic activities with their structure. It is difficult to play a role in scenes involving an arbitrary number of individuals, especially when they are performing the identification of non-participants (Kwak, Han, and Han 2013)."}, {"heading": "3 Model", "text": "We present a probabilistic generative model that describes how coordinated activities of groups of individuals lead to the observed physical trajectories of the actors involved. In Section 3.1, we introduce some terms. In Section 3.2, we then precisely define the representations that our model applies to activities, groups, activity sequences, and spatial trajectories. Next, in Section 3.3, we define the factors in the common probability distribution that make up the generative model. Finally, in Section 3.4, we define the specific activities that are used to model the scenes that we use for evaluation in Section 5."}, {"heading": "3.1 Terminology", "text": "In this context, it has to be stated that the two are persons who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to"}, {"heading": "3.2 Representation", "text": "The simplest activities are physical activities, such as walking, running and standing, which directly restrict the movement of a group of individuals over a certain period of time. Thus, for example, a RUN activity is expected to yield trajectories at speeds corresponding to typical human walking. We refer to the set of physical activity marks of Aphys. Likewise, intentional activities include MEET and MOVE-TO. We refer to the set of intentional activity marks of Aint. All activities are reported by A = Aint. The group of participants in Activity C is led by ZC. We let JC = | ZC divide the size of the group into groups."}, {"heading": "3.3 Generative Model", "text": "This year is the highest in the history of the country."}, {"heading": "3.4 Specific Activities", "text": "In this work, we limit ourselves to six specific activities, three intended (FREE-FOR-ALL, MEET and MOVE-TO) and three physical (STAND, WALK and RUN).FFA has a single role that allows all activities to take place. A MEET activity assigns two roles, APPROACHER and WAITER, with unequal probability; an APPROACHER performs a MEETs (recursive) and MOVE-TOs, and a FURTHER performs only STAND < WALK < EMRUN. MOVE-TO produces only one role, MOVER, which switches evenly between the three physical activities."}, {"heading": "4 Inference", "text": "Given a series of J-individual trajectories Y = (y1,.., yJ) as shown in Figure 1 (c), we would like to find an activity tree as shown in Figure 1 (a) that best describes them. Specifically, we want to maximize the probability of an activity tree taking into account the observed data tree. (1) The integrals are given by (4) and (5). Generally, the integral in (7) cannot be analytically calculated. However, since each factor in (7) is a normal pdf model, p (Y | j) dX \u2212 The integrals are given by (4) and (5)."}, {"heading": "4.1 Proposal distribution", "text": "Our proposal consists of sampling movements that perform edits to the current hypothetical activity trees in order to produce a new tree sample. When drawing a sample from the group, we select a step uniformly at random to apply it. When applying a step, we must ensure that the resulting activity tree is valid (e.g., start and end times must be uniformly; or activity sequences must be possible given the role), which requires accounting that goes beyond the scope of this documentation. We have also developed a series of activity detectors to efficiently explore the space. These detectors provide rough estimates of the groupings of individuals in each frame, and activities that are performed by each group (see Section 4.1, \"Detectors\"). We use these detectors in two ways to initialize the sampler to a state that is achieved by transforming the results of the detectors into an activity (0)."}, {"heading": "5 Experiments and Results", "text": "We evaluate the model in two ways: First, we show the validity of the model by deriving from synthetic data two different types of complex structured scenarios. In the first case, groups of individuals engage in activities and dissolve, forming different groups over time. In the second case, we show a recursively structured activity in which a meeting is part of a higher-level meeting. Then, we evaluate the model using real data, in particular two publicly available group activity datasets, VIRAT (Oh et al. 2011) and the UCLA dataset for air events (Shu et al. 2015)."}, {"heading": "5.1 Evaluation", "text": "Performance is measured by how well activities in the scene are labeled and how well individuals are grouped, regardless of the activity designation. In the following, we define the basic truth and the activity trees derived from it. Activity designation For each activity a and video image f, we compare the number of individuals performing an f. We first define performance by an individual in a frame, then calculate the total number and associated performance measures. For an individual j in frame i, the true positives are the groups of individuals who have the same label as j in the frame. Let's define the number of false positives similarly in a frame. The number of false negatives in a frame i, the true positives in frame i, and the true negatives in the frame (Z\\ ig \u00b2 ji) are the groups in which Z is the total number of all individuals."}, {"heading": "5.2 Results", "text": "The performance of our algorithm is summarized in Tables 1 and 2. Table 1 shows the activity that characterizes precision and recall on the two synthetic scenes (SYNTH1 and SYNTH2), a video sequence obtained from the VIRAT dataset, and four different video sequences from the UCLA Flight Event Dataset (UAED). Table 2 shows the performance measured by our Group Assessment Metric described above. Synthetic Data The second (SYNTH2) includes two videos in which one video depicts a series of trajectories on the ground level. In the first, SYNTH1, five actors participate in a series of meetings in which group memberships are repeatedly exchanged over 20 frames. The second (SYNTH2) shows five actors meeting in a side meeting, four of which participate in joining the global meeting."}, {"heading": "6 Discussion", "text": "We have presented a probabilistic generative model of complex multi-agent activities on arbitrary time scales. Activities define component roles between groups of actors and take into account an infinitely deep, recursive, hierarchical structure. To our knowledge, no existing model of trajectory-based activity detection offers this expressiveness in a common model and describes both interactions between groups and between individuals. Physical and intended (superordinate) activities explain hierarchical correlations between individual trajectories. To our knowledge, no existing model of trajectory-based activity detection offers this expressiveness in a common model. Of course, the modeling framework is extendable. We are currently implementing several extensions, including (1) the development of additional activities, including (1) the follow-up, exchange of objects and interaction with vehicles and entrances to buildings, (2) the addition of prior knowledge of the spatial arrangement of the scene, which naturally limits what activities are possible, such as streets, sidewalks, inaccessible buildings and other spatial features that we can improve in 2013 by both the spatial behavior (the speed of multiple events) and (the ability of 3)."}, {"heading": "Acknowledgements", "text": "This research was supported by grants from the DARPA Mind's Eye program W911NF-10-C-0081 (subcontracted to iRobot, 92003) and the DARPA SSIM program W911NF10-2-0064. We would like to thank Paul R. Cohen and Christopher Geyer in particular for helpful discussions and advice."}], "references": [{"title": "M", "author": ["J.K. Aggarwal", "Ryoo"], "venue": "S.", "citeRegEx": "Aggarwal and Ryoo 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Z", "author": ["A. Barbu", "A. Bridge", "Z. Burchill", "D. Coroian", "S. Dickinson", "S. Fidler", "A. Michaux", "S. Mussman", "N. Siddharth", "D. Salvi", "L. Schmidt", "J. Shangguan", "J.M. Siskind", "J. Waggoner", "S. Wang", "J. Wei", "Y. Yin", "Zhang"], "venue": "2012. Video in sentences out. In UAI", "citeRegEx": "Barbu et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian 3d tracking from monocular video", "author": ["Brau"], "venue": null, "citeRegEx": "Brau,? \\Q2013\\E", "shortCiteRegEx": "Brau", "year": 2013}, {"title": "Group level activity recognition in crowded environments across multiple cameras", "author": ["Chang"], "venue": null, "citeRegEx": "Chang,? \\Q2010\\E", "shortCiteRegEx": "Chang", "year": 2010}, {"title": "Probabilistic group-level motion analysis and scenario recognition", "author": ["Krahnstoever Chang", "M.-C. Ge 2011] Chang", "N. Krahnstoever", "W. Ge"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Recognizing human group action by layered model with multiple cues. Neurocomputing 136:124\u2013135", "author": ["Cheng"], "venue": null, "citeRegEx": "Cheng,? \\Q2014\\E", "shortCiteRegEx": "Cheng", "year": 2014}, {"title": "Bayesian treed models. Machine Learning 48(1-3):299\u2013320", "author": ["George Chipman", "H. McCulloch 2002] Chipman", "E. George", "R. McCulloch"], "venue": null, "citeRegEx": "Chipman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chipman et al\\.", "year": 2002}, {"title": "and Savarese", "author": ["W. Choi"], "venue": "S.", "citeRegEx": "Choi and Savarese 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise. 226\u2013231", "author": ["Ester"], "venue": null, "citeRegEx": "Ester,? \\Q1996\\E", "shortCiteRegEx": "Ester", "year": 1996}, {"title": "F", "author": ["C. Garate", "S. Zaidenberg", "J. Badie", "Bremond"], "venue": "2014. Group tracking and behavior recognition in long video surveillance sequences. In VISIGRAPP", "citeRegEx": "Garate et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["C.W. Geib", "Goldman"], "venue": "P.", "citeRegEx": "Geib and Goldman 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "J", "author": ["S. Kwak", "B. Han", "Han"], "venue": "H.", "citeRegEx": "Kwak. Han. and Han 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "G", "author": ["T. Lan", "Y. Wang", "W. Yang", "Mori"], "venue": "2010. Beyond actions: Discriminative models for contextual group activities. In Advances in neural information processing systems, 1216\u2013", "citeRegEx": "Lan et al. 2010", "shortCiteRegEx": null, "year": 1224}, {"title": "Social roles in hierarchical models for human activity", "author": ["Sigal Lan", "T. Mori 2012] Lan", "L. Sigal", "G. Mori"], "venue": null, "citeRegEx": "Lan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2012}, {"title": "Group event detection with a varying number of group members for video surveillance. Circuits and Systems for Video Technology, IEEE Transactions on 20(8):1057\u20131067", "author": ["Lin"], "venue": null, "citeRegEx": "Lin,? \\Q2010\\E", "shortCiteRegEx": "Lin", "year": 2010}, {"title": "R", "author": ["Neal"], "venue": "M.", "citeRegEx": "Neal 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "T", "author": ["S. Odashima", "M. Shimosaka", "T. Kaneko", "R. Fukui", "Sato"], "venue": "2012. Collective activity localization with contextual spatial pyramid. In Computer Vision\u2013ECCV", "citeRegEx": "Odashima et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "J", "author": ["S. Oh", "A. Hoogs", "A. Perera", "N. Cuntoor", "C.-C. Chen", "Lee"], "venue": "T.; Mukherjee, S.; Aggarwal, J.; Lee, H.; Davis, L.; et al.", "citeRegEx": "Oh et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "M", "author": ["Pratola"], "venue": "T.", "citeRegEx": "Pratola 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "C", "author": ["C.E. Rasmussen", "Williams"], "venue": "K. I.", "citeRegEx": "Rasmussen and Williams 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "J", "author": ["M.S. Ryoo", "Aggarwal"], "venue": "K.", "citeRegEx": "Ryoo and Aggarwal 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Joint inference of groups, events and human roles in aerial videos", "author": ["Shu"], "venue": null, "citeRegEx": "Shu,? \\Q2015\\E", "shortCiteRegEx": "Shu", "year": 2015}, {"title": "H", "author": ["G. Sukthankar", "R.P. Goldman", "C.W. Geib", "D.V. Pynadath", "Bui"], "venue": "H., eds.", "citeRegEx": "Sukthankar et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Agrawal", "author": ["S. Vishwakarma"], "venue": "A.", "citeRegEx": "Vishwakarma and Agrawal 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A generic framework for video understanding applied to group behavior recognition", "author": ["Boulay Zaidenberg", "S. Bremond 2012] Zaidenberg", "B. Boulay", "F. Bremond"], "venue": "In AVSS,", "citeRegEx": "Zaidenberg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zaidenberg et al\\.", "year": 2012}, {"title": "Parsing collective behaviors by hierarchical model with varying structure", "author": ["Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2012\\E", "shortCiteRegEx": "Zhang", "year": 2012}, {"title": "Beyond particle flow: Bag of trajectory graphs for dense crowd event recognition", "author": ["Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2013\\E", "shortCiteRegEx": "Zhang", "year": 2013}, {"title": "T", "author": ["G. Zhu", "S. Yan", "Han"], "venue": "X.; and Xu, C.", "citeRegEx": "Zhu et al. 2011", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [], "year": 2016, "abstractText": "We present a probabilistic generative model for inferring a description of coordinated, recursively structured group activities at multiple levels of temporal granularity based on observations of individuals\u2019 trajectories. The model accommodates: (1) hierarchically structured groups, (2) activities that are temporally and compositionally recursive, (3) component roles assigning different subactivity dynamics to subgroups of participants, and (4) a nonparametric Gaussian Process model of trajectories. We present an MCMC sampling framework for performing joint inference over recursive activity descriptions and assignment of trajectories to groups, integrating out continuous parameters. We demonstrate the model\u2019s expressive power in several simulated and complex real-world scenarios from the VIRAT and UCLA Aerial Event video data sets.", "creator": "LaTeX with hyperref package"}}}