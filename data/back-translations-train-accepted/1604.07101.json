{"id": "1604.07101", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2016", "title": "Double Thompson Sampling for Dueling Bandits", "abstract": "In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for dueling bandit problems. As indicated by its name, D-TS selects both the first and the second candidates according to Thompson Sampling. Specifically, D-TS maintains a posterior distribution for the preference matrix, and chooses the pair of arms for comparison by sampling twice from the posterior distribution. This simple algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as its special case. For general Copeland dueling bandits, we show that D-TS achieves $O(K^2 \\log T)$ regret. For Condorcet dueling bandits, we further simplify the D-TS algorithm and show that the simplified D-TS algorithm achieves $O(K \\log T + K^2 \\log \\log T)$ regret. Simulation results based on both synthetic and real-world data demonstrate the efficiency of the proposed D-TS algorithm.", "histories": [["v1", "Mon, 25 Apr 2016 00:38:16 GMT  (1289kb,D)", "http://arxiv.org/abs/1604.07101v1", "27 pages, 5 figures"], ["v2", "Thu, 27 Oct 2016 17:36:57 GMT  (1363kb,D)", "http://arxiv.org/abs/1604.07101v2", "27 pages, 5 figures, 9 tables; accepted by 30th Conference on Neural Information Processing Systems (NIPS), 2016"]], "COMMENTS": "27 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["huasen wu", "xin liu 0002"], "accepted": true, "id": "1604.07101"}, "pdf": {"name": "1604.07101.pdf", "metadata": {"source": "CRF", "title": "Double Thompson Sampling for Dueling Bandits", "authors": ["Huasen Wu", "Xin Liu", "R. Srikant"], "emails": ["hswu@ucdavis.edu", "liu@cs.ucdavis.edu", "rsrikant@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "This model can be applied in systems such as the Information Retrieval (IR), which beats all other arms. Previous Dueling Bandit algorithms such as Interleaved Filter (IF) [3] and Beat-The-Mean (BTM) belong to the \"exploit\" family, which requires the time horizon as input and is applied only to finite horizon settings. Algorithms that work for both finite and finite horizon settings."}, {"heading": "2 Related Work", "text": "Unlike traditional MAB, there are different definitions of the best weapons, depending on the preference matrix and application requirements. Most existing work focuses on Condorcet Dueling Bandits, where there is an arm called the Condorcet Winner, but which beats all other arms. Under certain strict assumptions such as strong stochastic transitivity and stochastic triangle inequality, [3] suggests that the first \"exploring-then-exploit\" algorithms, called Interleaved Filter 1 (IF1) and Interleaved Filter 2 (IF2), is followed by a Beat-TheMean (BTM) algorithm in [7], where the strong stochastic transitivity assumption is relaxed. IF1 is shown to achieve O (K logK logK log T) regret, while IF2 and BTM are shown to achieve O (K log T)."}, {"heading": "3 System Model", "text": "We consider a dueling bandit problem with K weapons, referred to by A = {1, 2,., K.} At any time t > 0, a pair of weapons (1), a (2) t, a (2) t, and a noisy comparison result wt's is achieved, which is \"1,\" if a (1) t over time and the distribution of comparison results is characterized by the preference matrix P = [pij] K-K, where the probability of poor i-beats poor j, i.e., pij = P {i}, j = 1, 2,., K.Similar to existing work, we assume that the scoreboard order does not affect the preference."}, {"heading": "4 Double Thompson Sampling", "text": "In this section, we introduce the Double Thompson Sampling (D-TS) algorithm and examine its regrets. First, we propose the D-TS algorithm for general Copeland bandits and show that it achieves O (K2 log T) remorse. Then, for Condorcet dueling bandits, we refine the regret to O (K log T + K2 log T) with a simplified version of D-TS. Inspired by the findings of Condorcet dueling bandits, we also discuss how to refine the regret of D-TS in general Copeland dueling bandits."}, {"heading": "4.1 D-TS Algorithm", "text": "We present a D-TS algorithm, described in algorithm 1, for general Copeland duels. As indicated by its name, the basic idea of D-TS is to select both the first and second candidates for comparison by Thompson Sampling. In D-TS, we assume a prior beta distribution for the preference probability of each pair (i, j) with i 6 = j. These distributions are then based on the comparison results Bij (t \u2212 1) and Bji (t \u2212 1), where Bij (t \u2212 1) (resp. Bji (t \u2212 1)) is the number of time slots when arm i (resp. j) arm j (resp. i) precedes t (time index t is omitted in the pseudo codes without causing confusion). At each time slot t, prior to the implementation of Thompson Sampling, we first use the RUCB [4] of each pair to eliminate the weapons, which does not lead to a Copeland candidate likely to become a winner."}, {"heading": "4.2 Regret Analysis", "text": "In this section, we examine the limits of regret for D-TS in dueling bandits. For general Copeland dueling bandits, we show that regret for D-TS is limited by O (K2 log T). For Condorcet dueling bandits, we show that a simplified version of D-TS can reduce regret to O (K log T + K2 log T). Finally, we discuss refining the limits for D-TS dueling bandits in general. We introduce certain notations used in the analysis. First, 1 / 2 is an important benchmark for pij in dueling bandits, and we leave the gap between pij and 1 / 2, i.e."}, {"heading": "4.2.3 Discussion: Refining the Regret Bound in General Copeland Dueling Bandits", "text": "As shown in section 5, we can see that the original version of D-TS actually exceeds the CCB algorithm [22], which has shown that it achieves O (K (LC + 1) log T regret, with LC being the number of weapons to which Copeland winners lose. Inspired by the findings of the Condorcet duels with bandits and the simulation results for general Copeland duels with bandits, we make the following conjecture: When using D-TS in a Copeland dueling bandit with a preference matrix P = [pij] K \u00b7 K, his regret will be limited as follows: RD-TS (T) = O (KLC log T + K 2 log T). (20) Where LC is the number of weapons that a Copeland winner with a preference matrix P = [pij] K \u00d7 K loses, his regret will be as follows."}, {"heading": "5 Experiments", "text": "In order to evaluate the proposed D-TS and D-TS (s) algorithms, we conduct experiments based on synthetic and real data. Here, we present the results for experiments based on Microsoft Learning to Rank (MSLR), but also based on regret. Based on this data, we derive a preference matrix for 136 rankings, in which each ranking is a function that maps a user's query to a document ranking and can be viewed as an arm in dueling bandits. We use the two 5-armed submatrices in [2], one for Condorcet dueling bandit and the other for non-Condorcet dueling bandit. Experiments and discussions can be compared in Appendix D. We compare D-TS and D-TS (s) with the following algorithms: BTM [7], RUCB [4], RUCB [4], RUCB [4], RCS]."}, {"heading": "6 Conclusions and Future Work", "text": "In this paper, we examine Thompson sampling for dueling bandits. We propose a D-TS algorithm that better matches the nature of dueling bandits, selecting a pair of arms each time for comparison. We introduce a second-round sample to address the dilemma of dueling bandits, where we aim to compare the best arm against itself, but comparing an arm against itself does not provide any information. We also introduce a RUCB / RLCB-based elimination to help D-TS out of suboptimal comparisons. The proposed D-TS algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as a special case. We get theoretical regrets tied to D-TS, which is O (K2 log T) for general Copeland dueling bandits. The limit is further refined to O (K log T + K2 log T) with a simplified version of our TS-based algorithms showing that the results of the D-based bandit algorithms are much worse in practice."}, {"heading": "Acknowledgement", "text": "We thank Masrour Zoghi from the University of Amsterdam and Dr. Junpei Komiyama from the University of Tokyo for the helpful discussions about the experiments."}, {"heading": "Appendices", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Preliminaries", "text": "Fact 1 (Chernoff-Hoeffding boundary [25]) 1) Bernoulli random variables: Take into account Bernoulli random variables X1, X2,., Xn with E [Xi] = pi. Let's leave X-n = 1n-n-n-n-i = 1Xi and p = E [X-p] = 1 n-n-n-n-n-i-i = 1-pi. Then we point for each example (0, p], P {X-n-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-y-p-y-p-p-p-y p-p-y p-p-p-p-y p-y p-p-p-p-p-y p-y p-p-p-p-p-p-p-p-p-p-p-y p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p"}, {"heading": "B Regret Analysis in General Copeland Dueling Bandits", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proof of Lemma 1", "text": "Leave p-ji (t) = Bji (t \u2212 1) Bji (t \u2212 1 = 1) Bji (t \u2212 1) (j \u2212 1) (j \u2212 1) (t \u2212 1) (t \u2212 1) (n \u2212 1) Bji (t \u2212 1) < xji < 1 / 2. Define the following events: E p-ji (t) = (i, j) (t) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n). Define the following events: E p-ji (t) = (i, j) ji (t) = (i, j) (t) = P-fi (t) = (i), p-fi (t), t-fi (t), t-fi (t), t-fi (t) (t) (t), fi-t (t), t (t), fi-t (t) (t)."}, {"heading": "B.2 Proof of Lemma 2", "text": "This lemon can be easily explained by the concentration property of RLCB lji (t) = a = a = a = a = a = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "B.3 Proof of Lemma 4", "text": "Let me be the Copeland winner (or one of the winners) in the dueling bandit. We prove Lemma 4 by analyzing the RUCB ui-j (t) at t. Using the Chernoff-Hoeffding boundary, we have this for all j 6 = i, P {ui-j (t) < pi-j} \u2264 e \u2212 2\u03b1 log t = 1t2\u03b1. (33) Note that these are the arms that lose i-j. (4) < p-j-j 6 = i 1 (pi-j > 1 / 2). Let Li-j-j = [j-j-K, pi-j > 1 / 2} be the arms that lose i-j."}, {"heading": "B.4 Proof of Lemma 3", "text": "To grasp the number of time windows when comparing an unvictorious arm against itself, we need to examine the necessary conditions for this event. (If the upper limit of the Copeland value is not reached, then the event (a (1) t, a (2) t (i, i) t (a) t (i, i) only occurs if there is at least one k (2) K with pki > 1 / 2, so that lki (t) \u2264 1 / 2; and b) t (2) ki (t) \u2264 1 / 2 for all k with lki (t) (t). (t) For k with pki > 1 / 2, we define the following probability value ki (t) = P (2) \u2264 (t) ki (t) > 1 / 2 | Ht). (36) Note that all information about k. (t), lki (t), lki (t), and qki (t) k."}, {"heading": "C Regret Analysis in Condorcet Dueling Bandits", "text": "In this appendix, we discuss the proof for theorem 2. We can show an O (K2 log T) for similar (13) events (13) and that for general Copeland duels as stated in Proposition 1 = 0.5. The full proof for Proposition 1 is omitted here due to the similarity, and we will only discuss the analysis for the part we refine.To refine the tie to O (K log T + K2 log T), we only need to use the third term in Eq. (18), which refine the tie for the number of comparisons between i / i C and j 6 = j p. Note: Under D-TS (s log T), if we fix the first candidate as a (1), we compare i with all j to find the best arm j. Thus, each arm j will compete with j instead of i. Thus, unlike the analysis for the original D-TS, the proof should be modified accordingly."}, {"heading": "D Additional Experimental Results", "text": "This appendix contains additional experimental results for further data sets, including results for experiments based on synthetic data. Since the simulation complexity is O (K2), we adjust the number of independent experiments to save time, setting the number of independent experiments to 500, 50, 10 for K < 10, 10 \u2264 K \u2264 100 or K > 100, respectively. Most algorithms except SCB are stable and the results can show the correct trend and relative relationship."}, {"heading": "D.1 Condorcet Dueling Bandits", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1.1 Datasets", "text": "In addition to the MSLR dataset, we also conduct experiments based on the following datasets, including four synthetic datasets and the ArXiv.org dataset. Linear Order: This is the arithmetic dataset in [8], where there are eight arms with the preference probability given by pij = 0.5 + 0.05 (j \u2212 i). In fact, there is a strict linear order between these armies. Cyclic: A dataset taken from [8], where the preference matrix is given by Table 1. In this dataset, arm 1 is the Condorcet winner with p1j = 0.6, and the other arms have a cyclic preference relationship with one arm that beats another with a high probability. Strong transitivity does not hold in this dataset, and the Condorcet winner is not necessary when comparing all other arms with a fixed arm, i.e., p1 < maxj for a strong transitivity problem of 0.5 p1 with a penetration of 0.0."}, {"heading": "D.1.2 Performance Comparisons", "text": "Since we represent the difference between i and j when comparing these two arms themselves, while the normalized difference between the two species. \"\" We want the difference between the two species. \"\" We want the difference between the two species. \"\" We want the difference. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" \"We.\" \"\" We. \"\" \".\" \"\" We. \"\" \"\" \".\" \"\" We. \"\" \"\" \"\" We. \"\" \"\". \"\" \"\" We. \".\" \"\" \"\" \"We.\". \"\" \"\" \".\" \"\" \"\" We. \"\" \"\" \".\" \"\" \".\" \"\" \"\" \"We.\" \"\". \"\" \"\". \"\" \".\" \"\". \"\" \".\" \"\". \"\" \"\". \".\" \".\" \"\". \"\" \".\". \"\". \"\". \"\" \".\" \"\" \".\" \"\" \".\". \".\" \"\". \"\". \"\" \"\". \"\" \".\" \".\" \".\" \".\". \".\" \"\" \".\" \"\". \".\". \"\". \".\" \"\" \".\". \".\" \"\". \".\". \"\" \".\" \".\". \".\" \".\". \"\" \".\". \".\". \".\". \".\" \".\". \"\". \"\". \".\". \"\". \".\". \".\". \".\" \".\". \".\" \".\". \".\". \".\". \"\". \".\". \".\". \".\" \".\". \".\" \".\". \".\". \".\". \".\" \".\". \".\" \".\". \".\" \".\". \"\" \".\". \".\" \".\". \".\" \".\" \".\". \"\". \".\". \"\". \".\". \".\" \".\". \".\". \".\". \".\". \".\". \".\". \".\". \""}, {"heading": "D.2 General Copeland Dueling Bandits", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.2.1 Datasets", "text": "Similar to Condorcet Dueling Bandits, we evaluate the algorithms for general Copeland Dueling Bandits by conducting experiments based on the MSLR dataset and the following synthetic datasets. Non-Condorcet Cyclic: This is a type of Dueling Bandit where cyclic preference relationships occur between individual arms. We consider two scenarios with K = 6 and K = 9, whose preference matrices are given by Table 4 and Table 6. In the case of K = 6, there are three Copeland winners with Copeland Score 3. There is a cyclic preference relationship between the three winners and the three unvictorious arms, respectively. In addition, each Copeland winner is beaten by an unvictorious arm with a 0.9 probability. In the case of K = 9, there are three groups of weapons, with Copeland Score 3. There is a cyclic preference relationship between the three winners and the three unvictorious arms."}, {"heading": "D.2.2 Performance Comparison", "text": "s dueling bandit algorithms, including BTM, RUCB, RCS, CCB, RMED1, and RMED2, are growing rapidly as they continue to search for the Condorcet winner, which does not exist here. SAVAGE's general Copeland dueling bandit algorithm generates logarithmic regret, but is usually much greater than CCB and D-TS. In certain cases, SCB achieves comparable regret to CCB, but this algorithm can cause very high regret if it does not identify the Copeland winners. If CCB and D-TS are compared, we can see that D-TS exceeds the performance of CB in all cases."}], "references": [{"title": "Generic exploration and k-armed voting bandits", "author": ["T. Urvoy", "F. Clerot", "R. F\u00e9raud", "S. Naamane"], "venue": "International Conference on Machine Learning (ICML), pages 91\u201399,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Copeland dueling bandits", "author": ["M. Zoghi", "Z.S. Karnin", "S. Whiteson", "M. de Rijke"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "The k-armed dueling bandits problem", "author": ["Y. Yue", "J. Broder", "R. Kleinberg", "T. Joachims"], "venue": "Journal of Computer and System Sciences, 78(5):1538\u20131556,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Relative upper confidence bound for the k-armed dueling bandit problem", "author": ["M. Zoghi", "S. Whiteson", "R. Munos", "M.D. Rijke"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 10\u201318,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Y. Yue", "T. Joachims"], "venue": "International Conference on Machine Learning (ICML), pages 1201\u2013 1208. ACM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Relative confidence sampling for efficient on-line ranker evaluation", "author": ["M. Zoghi", "S.A. Whiteson", "M. De Rijke", "R. Munos"], "venue": "Proceedings of the 7th ACM international conference on Web search and data mining, pages 73\u201382. ACM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Beat the mean bandit", "author": ["Y. Yue", "T. Joachims"], "venue": "International Conference on Machine Learning (ICML), pages 241\u2013248,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Regret lower bound and optimal algorithm in dueling bandit problem", "author": ["J. Komiyama", "J. Honda", "H. Kashima", "H. Nakagawa"], "venue": "Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning, 47(2-3):235\u2013256,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika, pages 285\u2013294,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1933}, {"title": "An empirical evaluation of thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "Advances in Neural Information Processing Systems, pages 2249\u20132257,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Further optimal regret bounds for thompson sampling", "author": ["S. Agrawal", "N. Goyal"], "venue": "Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, pages 99\u2013107,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal regret analysis of thompson sampling in stochastic multi-armed bandit problem with multiple plays", "author": ["J. Komiyama", "J. Honda", "H. Nakagawa"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Thompson sampling for budgeted multi-armed bandits", "author": ["Y. Xia", "H. Li", "T. Qin", "N. Yu", "T.-Y. Liu"], "venue": "International Joint Conference on Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Thompson sampling for complex online problems", "author": ["A. Gopalan", "S. Mannor", "Y. Mansour"], "venue": "International Conference on Machine Learning (ICML), pages 100\u2013108,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Thompson sampling for learning parameterized Markov decision processes", "author": ["A. Gopalan", "S. Mannor"], "venue": "Proceedings of The 28th Conference on Learning Theory, pages 861\u2013898,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to optimize via posterior sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "Mathematics of Operations Research, 39(4):1221\u20131243,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse dueling bandits", "author": ["K. Jamieson", "S. Katariya", "A. Deshpande", "R. Nowak"], "venue": "Conference on Learning Theory (COLT),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "An information-theoretic analysis of thompson sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "arXiv preprint arXiv:1403.5341,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Contextual dueling bandits", "author": ["M. Dud\u0131\u0301k", "K. Hofmann", "R.E. Schapire", "A. Slivkins", "M. Zoghi"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Algorithms with logarithmic or sublinear regret for constrained contextual bandits", "author": ["H. Wu", "R. Srikant", "X. Liu", "C. Jiang"], "venue": "The 29th Annual Conference on Neural Information Processing Systems (NIPS), Montr\u00e9al, Canada, Dec.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Concentration of measure for the analysis of randomized algorithms", "author": ["D.P. Dubhashi", "A. Panconesi"], "venue": "Cambridge University Press,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "The k-armed dueling bandits problem", "author": ["Y. Yue", "J. Broder", "R. Kleinberg", "T. Joachims"], "venue": "Conference on Learning Theory (COLT),", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "This simple algorithm applies to general Copeland dueling bandits [1, 2], including Condorcet dueling bandits [3, 4] as its special case.", "startOffset": 66, "endOffset": 72}, {"referenceID": 1, "context": "This simple algorithm applies to general Copeland dueling bandits [1, 2], including Condorcet dueling bandits [3, 4] as its special case.", "startOffset": 66, "endOffset": 72}, {"referenceID": 2, "context": "This simple algorithm applies to general Copeland dueling bandits [1, 2], including Condorcet dueling bandits [3, 4] as its special case.", "startOffset": 110, "endOffset": 116}, {"referenceID": 3, "context": "This simple algorithm applies to general Copeland dueling bandits [1, 2], including Condorcet dueling bandits [3, 4] as its special case.", "startOffset": 110, "endOffset": 116}, {"referenceID": 2, "context": "The dueling bandit problem [3] is a variation of the classical multi-armed bandit (MAB) problem, where the feedback comes in the format of pairwise comparison.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "This model can be applied in systems such as information retrieval (IR) [5, 6], where user preferences are easier to obtain and typically more stable.", "startOffset": 72, "endOffset": 78}, {"referenceID": 5, "context": "This model can be applied in systems such as information retrieval (IR) [5, 6], where user preferences are easier to obtain and typically more stable.", "startOffset": 72, "endOffset": 78}, {"referenceID": 2, "context": "Earlier dueling bandit algorithms such as Interleaved Filter (IF) [3] and Beat-The-Mean (BTM) [7] belong to the \u201cexplorethen-exploit\u201d family, which requires the time horizon as an input and only applies to finite-horizon settings.", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "Earlier dueling bandit algorithms such as Interleaved Filter (IF) [3] and Beat-The-Mean (BTM) [7] belong to the \u201cexplorethen-exploit\u201d family, which requires the time horizon as an input and only applies to finite-horizon settings.", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "Algorithms that work for both finite and infinite horizon settings are studied recently [4, 8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 7, "context": "Algorithms that work for both finite and infinite horizon settings are studied recently [4, 8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 3, "context": "[4] extends the classic Upper Confidence Bound (UCB) method [9] to dueling bandits and proposes a Relative Upper Confidence Bound (RUCB) algorithm that achieves O(K log T ) regret in K-armed dueling bandits.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[4] extends the classic Upper Confidence Bound (UCB) method [9] to dueling bandits and proposes a Relative Upper Confidence Bound (RUCB) algorithm that achieves O(K log T ) regret in K-armed dueling bandits.", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "The more recent work [8] proposes a Relative Minimum Empirical Divergence (RMED) algorithm, which is shown to be optimal in the sense of both asymptotic order and constant coefficient, under the Condorcet assumption.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "A recent work [2] studies the Copeland dueling bandits and proposes a Copeland Confidence Bound (CCB) algorithm, which can be viewed as an extension of RUCB in this general setting.", "startOffset": 14, "endOffset": 17}, {"referenceID": 9, "context": "Thompson in [10] and is also referred to as Thompson Sampling.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "Empirical studies [11] show that this simple framework achieves performance similar to or better than other types of algorithms in practice.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "proposes a logarithmic bound for its expected regret, whose constant factor is further improved in [13].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "Recently, Thompson Sampling becomes a popular method that is widely applied in other more complex online learning problems, such as MAB with multiple plays [14], MAB with budget constraints [15], online problems with complex actions and feedbacks [16].", "startOffset": 156, "endOffset": 160}, {"referenceID": 13, "context": "Recently, Thompson Sampling becomes a popular method that is widely applied in other more complex online learning problems, such as MAB with multiple plays [14], MAB with budget constraints [15], online problems with complex actions and feedbacks [16].", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "Recently, Thompson Sampling becomes a popular method that is widely applied in other more complex online learning problems, such as MAB with multiple plays [14], MAB with budget constraints [15], online problems with complex actions and feedbacks [16].", "startOffset": 247, "endOffset": 251}, {"referenceID": 9, "context": "Unfortunately, the simple framework of Thompson Sampling for traditional MAB [10, 11, 12, 14] and other online learning problems [16, 17] may not directly apply to the dueling bandit problem, due to its unique properties.", "startOffset": 77, "endOffset": 93}, {"referenceID": 10, "context": "Unfortunately, the simple framework of Thompson Sampling for traditional MAB [10, 11, 12, 14] and other online learning problems [16, 17] may not directly apply to the dueling bandit problem, due to its unique properties.", "startOffset": 77, "endOffset": 93}, {"referenceID": 12, "context": "Unfortunately, the simple framework of Thompson Sampling for traditional MAB [10, 11, 12, 14] and other online learning problems [16, 17] may not directly apply to the dueling bandit problem, due to its unique properties.", "startOffset": 77, "endOffset": 93}, {"referenceID": 14, "context": "Unfortunately, the simple framework of Thompson Sampling for traditional MAB [10, 11, 12, 14] and other online learning problems [16, 17] may not directly apply to the dueling bandit problem, due to its unique properties.", "startOffset": 129, "endOffset": 137}, {"referenceID": 15, "context": "Unfortunately, the simple framework of Thompson Sampling for traditional MAB [10, 11, 12, 14] and other online learning problems [16, 17] may not directly apply to the dueling bandit problem, due to its unique properties.", "startOffset": 129, "endOffset": 137}, {"referenceID": 12, "context": "Unlike the multi-play MAB [14], dueling bandits allow to compare one arm against itself and a good learning algorithm is expected to end up with comparing the winner against itself 1.", "startOffset": 26, "endOffset": 30}, {"referenceID": 5, "context": "Recently, [6] combines Thompson Sampling with RUCB [4] for Condorcet dueling bandits and proposes an algorithm called Relative Confidence Sampling (RCS).", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Recently, [6] combines Thompson Sampling with RUCB [4] for Condorcet dueling bandits and proposes an algorithm called Relative Confidence Sampling (RCS).", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "Moreover, this simple framework also applies to general Copeland dueling bandits and achieves much better performance than the state-of-the-art algorithms such as Copeland Confidence Bounds (CCB) [2].", "startOffset": 196, "endOffset": 199}, {"referenceID": 14, "context": "One may suggest treating each pair of arms as a complex action and apply the simple framework of Thompson Sampling in [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 3, "context": "In order to avoid getting stuck on suboptimal comparisons, we leverage the RUCB method [4] to eliminate the arms that are unlikely to be the winners.", "startOffset": 87, "endOffset": 90}, {"referenceID": 11, "context": "Analyzing regret bounds for Thompson Sampling is much more difficult than UCB-type algorithms, as noted in literature for traditional MAB [12, 13, 19, 16].", "startOffset": 138, "endOffset": 154}, {"referenceID": 16, "context": "Analyzing regret bounds for Thompson Sampling is much more difficult than UCB-type algorithms, as noted in literature for traditional MAB [12, 13, 19, 16].", "startOffset": 138, "endOffset": 154}, {"referenceID": 14, "context": "Analyzing regret bounds for Thompson Sampling is much more difficult than UCB-type algorithms, as noted in literature for traditional MAB [12, 13, 19, 16].", "startOffset": 138, "endOffset": 154}, {"referenceID": 1, "context": "Our conjecture is that the original D-TS will achieve an O(KLC log T + K log log T ) regret, where LC is the number of arms that beats the Copeland winners, similar to that in [2].", "startOffset": 176, "endOffset": 179}, {"referenceID": 0, "context": "The simulation results show that, for both Condorcet dueling bandits and general Copeland dueling bandits, D-TS outperforms existing algorithms such as SAVAGE [1], RUCB [4], RMED [8], and CCB [2] in many practical settings.", "startOffset": 159, "endOffset": 162}, {"referenceID": 3, "context": "The simulation results show that, for both Condorcet dueling bandits and general Copeland dueling bandits, D-TS outperforms existing algorithms such as SAVAGE [1], RUCB [4], RMED [8], and CCB [2] in many practical settings.", "startOffset": 169, "endOffset": 172}, {"referenceID": 7, "context": "The simulation results show that, for both Condorcet dueling bandits and general Copeland dueling bandits, D-TS outperforms existing algorithms such as SAVAGE [1], RUCB [4], RMED [8], and CCB [2] in many practical settings.", "startOffset": 179, "endOffset": 182}, {"referenceID": 1, "context": "The simulation results show that, for both Condorcet dueling bandits and general Copeland dueling bandits, D-TS outperforms existing algorithms such as SAVAGE [1], RUCB [4], RMED [8], and CCB [2] in many practical settings.", "startOffset": 192, "endOffset": 195}, {"referenceID": 2, "context": "The dueling bandit problem is first brought by [3].", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "Under certain strict assumptions such as strong stochastic transitivity and stochastic triangle inequality, [3] proposes the first \u201cexplore-then-exploit\u201d algorithms called Interleaved Filter 1 (IF1) and Interleaved Filter 2 (IF2), followed-up by a Beat-TheMean (BTM) algorithm in [7], where the strong stochastic transitivity assumption is relaxed.", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "Under certain strict assumptions such as strong stochastic transitivity and stochastic triangle inequality, [3] proposes the first \u201cexplore-then-exploit\u201d algorithms called Interleaved Filter 1 (IF1) and Interleaved Filter 2 (IF2), followed-up by a Beat-TheMean (BTM) algorithm in [7], where the strong stochastic transitivity assumption is relaxed.", "startOffset": 280, "endOffset": 283}, {"referenceID": 0, "context": "Another algorithm called SAVAGE (Sensitivity Analysis of VAriables for Generic Exploration) is proposed in [1] and shown to outperform the IF and BTM algorithms by simulations, although only O(K log T ) regret is obtained for SAVAGE.", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "Algorithms that work for both finite and infinite horizon settings are studied in [4] and [8].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "Algorithms that work for both finite and infinite horizon settings are studied in [4] and [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "[4] extends the classic Upper Confidence Bound (UCB) method [9] to dueling bandits and proposes a Relative Upper Confidence Bound (RUCB) algorithm that achieves O(K log T ) regret in K-armed dueling bandits.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[4] extends the classic Upper Confidence Bound (UCB) method [9] to dueling bandits and proposes a Relative Upper Confidence Bound (RUCB) algorithm that achieves O(K log T ) regret in K-armed dueling bandits.", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "The more recent work [8] proposes a Relative Minimum Empirical Divergence (RMED) algorithm, which is shown to be optimal in the sense of both the asymptotic order O(K log T ) and the constant coefficient, under the Condorcet assumption.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "A recent work [2] studies the Copeland dueling bandits and proposes a Copeland Confidence Bound (CCB) algorithm, which can be viewed as an extension of RUCB in this general setting.", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "Because Borda dueling bandits can be reduced to the traditional MAB problem [1, 4], only a few researchers study this problem with special structures, such as sparseness [20].", "startOffset": 76, "endOffset": 82}, {"referenceID": 3, "context": "Because Borda dueling bandits can be reduced to the traditional MAB problem [1, 4], only a few researchers study this problem with special structures, such as sparseness [20].", "startOffset": 76, "endOffset": 82}, {"referenceID": 17, "context": "Because Borda dueling bandits can be reduced to the traditional MAB problem [1, 4], only a few researchers study this problem with special structures, such as sparseness [20].", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "By treating each pair as an action, the SAVAGE algorithm [1] applies to dueling bandits with all the above three definitions of winners, but it is an \u201cexplore-then-exploit\u201d algorithm and exploring all pairs of arms may result in a large regret.", "startOffset": 57, "endOffset": 60}, {"referenceID": 9, "context": "Dating back to 1933, Thompson Sampling [10] is one of the earliest algorithms for \u201cexplorationand-exploitation\u201d tradeoff and has been widely applied in traditional MAB and other more complex problems recently.", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "Its effectiveness has been demonstrated by empirical studies [11], which shows that Thompson Sampling achieves performance better than or close to that of UCB-type algorithms in practice.", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "[12] proposes a logarithmic bound for the standard frequentist expected regret, whose constant factor is further improved in [13].", "startOffset": 125, "endOffset": 129}, {"referenceID": 18, "context": "On the other hand, through information-theoretic analysis, [21, 19] derives the bounds for its Bayesian expected regret.", "startOffset": 59, "endOffset": 67}, {"referenceID": 16, "context": "On the other hand, through information-theoretic analysis, [21, 19] derives the bounds for its Bayesian expected regret.", "startOffset": 59, "endOffset": 67}, {"referenceID": 12, "context": "Recently, Thompson Sampling becomes a popular method and has been widely applied in many variations of the traditional MAB, such as MAB with multiple plays [14], MAB with budget constraints [15], and MAB with complex actions and feedbacks [16].", "startOffset": 156, "endOffset": 160}, {"referenceID": 13, "context": "Recently, Thompson Sampling becomes a popular method and has been widely applied in many variations of the traditional MAB, such as MAB with multiple plays [14], MAB with budget constraints [15], and MAB with complex actions and feedbacks [16].", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "Recently, Thompson Sampling becomes a popular method and has been widely applied in many variations of the traditional MAB, such as MAB with multiple plays [14], MAB with budget constraints [15], and MAB with complex actions and feedbacks [16].", "startOffset": 239, "endOffset": 243}, {"referenceID": 15, "context": "Moreover, Thompson Sampling is also used in an even more complex learning problem - parameterized Markov decision process [17], which proposes a TSMDP algorithm for this problem.", "startOffset": 122, "endOffset": 126}, {"referenceID": 5, "context": "A recent work [6] combines Thompson Sampling with RUCB [4] for Condorcet dueling bandits and proposes an algorithm called Relative Confidence Sampling (RCS).", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "A recent work [6] combines Thompson Sampling with RUCB [4] for Condorcet dueling bandits and proposes an algorithm called Relative Confidence Sampling (RCS).", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "This assumption is made in literature [2] and usually holds in practice, e.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": ", with pi\u2217j > 1/2 for all j 6= i\u2217 [3, 4, 8].", "startOffset": 34, "endOffset": 43}, {"referenceID": 3, "context": ", with pi\u2217j > 1/2 for all j 6= i\u2217 [3, 4, 8].", "startOffset": 34, "endOffset": 43}, {"referenceID": 7, "context": ", with pi\u2217j > 1/2 for all j 6= i\u2217 [3, 4, 8].", "startOffset": 34, "endOffset": 43}, {"referenceID": 0, "context": "However, the existence of Condorcet winner is not guaranteed in practice [1, 2].", "startOffset": 73, "endOffset": 79}, {"referenceID": 1, "context": "However, the existence of Condorcet winner is not guaranteed in practice [1, 2].", "startOffset": 73, "endOffset": 79}, {"referenceID": 0, "context": "A more general and practical definition is the Copeland winner, which is the arm (or arms) that maximizes the number of other arms it beats [1, 2].", "startOffset": 140, "endOffset": 146}, {"referenceID": 1, "context": "A more general and practical definition is the Copeland winner, which is the arm (or arms) that maximizes the number of other arms it beats [1, 2].", "startOffset": 140, "endOffset": 146}, {"referenceID": 1, "context": "To measure the performance of a dueling bandit algorithm \u0393, we adopt the definition of cumulative regret for Copeland dueling bandits [2].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "As pointed out in [2], the results will apply to other definitions of regret because the above definition bounds the number of queries to non-winner arms.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "At each time-slot t, before implementing Thompson Sampling, we first use the RUCB [4] of each pair to eliminate the arms that are unlikely to be the Copeland winner, resulting in a candidate set Ct (Lines 4 to 6).", "startOffset": 82, "endOffset": 85}, {"referenceID": 10, "context": ", [11, 12, 19, 16, 14].", "startOffset": 2, "endOffset": 22}, {"referenceID": 16, "context": ", [11, 12, 19, 16, 14].", "startOffset": 2, "endOffset": 22}, {"referenceID": 14, "context": ", [11, 12, 19, 16, 14].", "startOffset": 2, "endOffset": 22}, {"referenceID": 12, "context": ", [11, 12, 19, 16, 14].", "startOffset": 2, "endOffset": 22}, {"referenceID": 5, "context": "The existing algorithm RCS [6] uses RUCB to choose the second arm.", "startOffset": 27, "endOffset": 30}, {"referenceID": 17, "context": "501 for all j > 1, and arm-2 is not the Condorcet winner, but with p2j = 1 for all j > 2 (in fact, arm-2 is the Borda winner for larger K [20]).", "startOffset": 138, "endOffset": 142}, {"referenceID": 14, "context": "This issue can be addressed by RUCB-based elimination as follows: when arm-2 has been compared with arm-1 sufficiently, we know that arm-1 likely beats Without RUCB-based elimination, the D-TS algorithm may still be able to get out of this trap in this case because the optimal action will be taken with a positive probability [16], but its regret will be much larger and difficult to be bound theoretically.", "startOffset": 327, "endOffset": 331}, {"referenceID": 2, "context": "The scaling behavior of this bound with respect to the time horizon T is order optimal, since a lower bound \u03a9(log T ) has been shown in [3, 8].", "startOffset": 136, "endOffset": 142}, {"referenceID": 7, "context": "The scaling behavior of this bound with respect to the time horizon T is order optimal, since a lower bound \u03a9(log T ) has been shown in [3, 8].", "startOffset": 136, "endOffset": 142}, {"referenceID": 11, "context": "We prove this lemma by borrowing the idea in [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "For any 0 < \u2264 1, we choose xji \u2208 (pji, 1/2) such thatD(xji||1/2) = D(pji||1/2)/(1+ ), which also implies 1 D(xji||pji) = O( 1 2 ) as shown in [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 11, "context": "Sketch for the Proof of Lemma 3: We prove Lemma 3 similarly to [13], but we need to adjust it to address the correlation between the selection of two candidates under D-TS.", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "However, we cannot expect that the coefficient will decay exponentially as in traditional MAB [13], because lki(t) \u2264 1/2 may indicate that arm k has not performed well when compared with i in the past.", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "By considering all possible realizations of the history Ht\u22121, we can relax the condition of lki(t) \u2264 1/2 and obtain results similar to [13].", "startOffset": 135, "endOffset": 139}, {"referenceID": 2, "context": "In particular, we expect to achieve lower regret as [3, 4, 8] for Condorcet dueling bandits.", "startOffset": 52, "endOffset": 61}, {"referenceID": 3, "context": "In particular, we expect to achieve lower regret as [3, 4, 8] for Condorcet dueling bandits.", "startOffset": 52, "endOffset": 61}, {"referenceID": 7, "context": "In particular, we expect to achieve lower regret as [3, 4, 8] for Condorcet dueling bandits.", "startOffset": 52, "endOffset": 61}, {"referenceID": 19, "context": "3 Discussion: Refining the Regret Bound in General Copeland Dueling Bandits As will be demonstrated in Section 5, we can see that the original version of D-TS in fact outperforms the CCB algorithm [22], which has been shown to achieve O(K(LC + 1) log T regret, where LC is the number of arms that the Copeland winners lose to.", "startOffset": 197, "endOffset": 201}, {"referenceID": 1, "context": "Based on this dataset, [2] derives a preference matrix for 136 rankers, where each ranker is a function that maps a user\u2019s query to a document ranking and can be viewed as one arm in dueling bandits.", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "We use the two 5-armed submatrices in [2], one for Condorcet dueling bandit and the other for non-Condorcet dueling bandit.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 99, "endOffset": 102}, {"referenceID": 1, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 108, "endOffset": 111}, {"referenceID": 7, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 122, "endOffset": 125}, {"referenceID": 6, "context": "3 as [7].", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "For RMED, we use the same settings as [8], where f(K) = 0.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "[8] has shown that RMED is optimal in Condorcet dueling bandits, not only in the sense of asymptotic order, but also the coefficients in the lower bound.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": ", with side information [22] or budget constraints [24]) is another interesting direction for future research.", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": ", with side information [22] or budget constraints [24]) is another interesting direction for future research.", "startOffset": 51, "endOffset": 55}, {"referenceID": 0, "context": "[1] T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] O.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[24] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[25] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[26] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "(Chernoff-Hoeffding Bound [25]) 1) Bernoulli random variables: Given Bernoulli random variables X1, X2, .", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "2) General random variables over [0,1]: Let X1, X2, .", "startOffset": 33, "endOffset": 38}, {"referenceID": 0, "context": ", Xn be random variables over [0,1] and E[Xi|X1, X2, .", "startOffset": 30, "endOffset": 35}, {"referenceID": 0, "context": "The conclusions follow by using the Chernoff-Hoeffding bound for general random variables over [0,1].", "startOffset": 95, "endOffset": 100}, {"referenceID": 11, "context": "For the second term, letting Lji(T ) = log T D(xji||1/2) , similar to the proof of Lemma 4 in [13], we can show that it is bounded as follows:", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "where the term P{(a t , a (2) t ) = (i, j), E p\u0304 ji(t), qE ji(t), Nij(t \u2212 1) > Lji(T )} \u2264 P{(a (1) t , a (2) t ) = (i, j), E p\u0304 ji(t), qE ji(t), Nij(t\u2212 1)|Lji(T )} \u2264 1 T , as shown in [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "The third term can be bounded similarly to Lemma 3 in [13], where the only difference is that i and j are compared when (a t , a (2) t ) = (i, j) or (a (1) t , a (2) t ) = (j, i).", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "For any 0 < \u2264 1, we choose xji \u2208 (pji, 1/2) such thatD(xji||1/2) = D(pji||1/2)/(1+ ), which also implies 1 D(xji||pji) = O( 1 2 ) as shown in [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 11, "context": "Given k with pki > 1/2 andHt\u22121 such that \u03b6\u0302\u2217(t) \u2265 \u03b6\u2217 and lki(t) \u2264 1/2, similarly to [13], we can show that P { (a (1) t , a (2) t ) = (i, i)|Ht\u22121 } \u2264 1\u2212 qki(t) qki(t) P { (a (1) t , a (2) t ) = (i, k)|Ht\u22121 } .", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "According to Lemma 2 in [13], E [ 1 qki(\u03c4n+1) ] is bounded as follows:", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "We can show an O(K log T ) regret for D-TSsimilarly to [13] and that for general Copeland dueling bandits, as stated in Proposition 1.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "Thus, different from the analysis for the original D-TS, the proof should be modified accordingly, which becomes more similar to the analysis of Thompson Sampling for traditional MAB [13].", "startOffset": 183, "endOffset": 187}, {"referenceID": 11, "context": "Similar to [13], for j 6= j\u2217 i , we choose two numbers xji, yji, such that pji < xji < yji < pi = maxj pji.", "startOffset": 11, "endOffset": 15}, {"referenceID": 11, "context": "To further refine the bound to O(K log T +K log T ), we only need to refine the proof of Lemma 4 in [13], because other components are bounded byO(1).", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "To show how to refine this proof, we recall the key step in the proof of Lemma 4 in [13] , which bounds the expected number of steps when (i, j) (j 6= j\u2217 i ) are compared while p\u0304ji < xji and \u03b8 (2) ji (t) \u2265 yji:", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "For any > 0, choosing appropriate xji and yji such that D(xji||yji) = D(pji||pi )/(1 + ), similar to [13], we have", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "where (c) holds because a t = i could only happen at t = \u03c4 (i) m+1; (d) is true because, given Nij(\u03c4 (i) m+1 \u2212 1) > Lji(n), the events E p\u0304 ji(t) and E ji(t) are independent of t = \u03c4 (i) m+1, and thus the probability can be bounded according to the concentration property of Thompson samples (in the proof of Lemma 3 in [13]): P{E p\u0304 ji(t), qE \u03b8 ji(t)|Nij(t\u2212 1) > Lji(n)} \u2264 e\u2212L \u03b1 ji(n)D(xji||yji) = 1 n1+2\u03b1 .", "startOffset": 320, "endOffset": 324}, {"referenceID": 7, "context": "LinearOrder: This is the Arithmetic dataset in [8], where there are eight arms with the preference probability given by pij = 0.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "Cyclic: A dataset adopted from [8], where the preference matrix is given by Table 1.", "startOffset": 31, "endOffset": 34}, {"referenceID": 17, "context": "We still treat this problem as a Condorcet dueling bandit problem and try to find the Condorcet winner, although a Borda winner may be more appropriate here [20].", "startOffset": 157, "endOffset": 161}, {"referenceID": 6, "context": "ArXiv: A 6-armed dueling bandits with a preference matrix given in Table 3, which is derived [7] by conducting pairwise interleaving experiments based on the search engine of ArXiv.", "startOffset": 93, "endOffset": 96}, {"referenceID": 22, "context": "As a special case, another definition of regret widely used in Condorcet dueling bandits [26, 8] is based on the preference-probability gap to the winner (G2W-based).", "startOffset": 89, "endOffset": 96}, {"referenceID": 7, "context": "As a special case, another definition of regret widely used in Condorcet dueling bandits [26, 8] is based on the preference-probability gap to the winner (G2W-based).", "startOffset": 89, "endOffset": 96}, {"referenceID": 22, "context": "Here we also present results for the G2W-based regret defined as [26] (the definition in [8] is the same except for an additional 1/2 factor):", "startOffset": 65, "endOffset": 69}, {"referenceID": 7, "context": "Here we also present results for the G2W-based regret defined as [26] (the definition in [8] is the same except for an additional 1/2 factor):", "startOffset": 89, "endOffset": 92}, {"referenceID": 17, "context": "These scenarios may seldomly happen in practice as we see from the MSLR and ArXiv datasets, or when they happen, the definition of \u201cwinner\u201d may need to be adjusted [20].", "startOffset": 164, "endOffset": 168}, {"referenceID": 7, "context": "When one arm is beaten by the other arm with high probability, it can be eliminated quickly by RMED using the empirical divergence [8].", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "500-Armed Dueling Bandits: The 500-armed dueling bandit constructed in [2], where there are three Copeland winners that form a cycle and have Copeland score 498, and the other arms have Copeland scores from 0 to 496.", "startOffset": 71, "endOffset": 74}], "year": 2016, "abstractText": "In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for dueling bandit problems. As indicated by its name, D-TS selects both the first and the second candidates according to Thompson Sampling. Specifically, D-TS maintains a posterior distribution for the preference matrix, and chooses the pair of arms for comparison by sampling twice from the posterior distribution. This simple algorithm applies to general Copeland dueling bandits [1, 2], including Condorcet dueling bandits [3, 4] as its special case. For general Copeland dueling bandits, we show that D-TS achieves O(K log T ) regret. For Condorcet dueling bandits, we further simplify the D-TS algorithm and show that the simplified D-TS algorithm achieves O(K log T + K log log T ) regret. Simulation results based on both synthetic and real-world data demonstrate the efficiency of the proposed D-TS algorithm.", "creator": "LaTeX with hyperref package"}}}