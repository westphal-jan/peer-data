{"id": "1604.07255", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2016", "title": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft", "abstract": "The ability to reuse or transfer knowledge from one task to another in lifelong learning problems, such as Minecraft, is one of the major challenges faced in AI. Reusing knowledge across tasks is crucial to solving tasks efficiently with lower sample complexity. We provide a Reinforcement Learning agent with the ability to transfer knowledge by learning reusable skills, a type of temporally extended action (also know as Options (Sutton et. al. 1999)). The agent learns reusable skills using Deep Q Networks (Mnih et. al. 2015) to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks (DSNs), are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture. The H-DRLN is a hierarchical version of Deep QNetworks and learns to efficiently solve tasks by reusing knowledge from previously learned DSNs. The H-DRLN exhibits superior performance and lower learning sample complexity (by taking advantage of temporal extension) compared to the regular Deep Q Network (Mnih et. al. 2015) in subdomains of Minecraft. We also show the potential to transfer knowledge between related Minecraft tasks without any additional learning.", "histories": [["v1", "Mon, 25 Apr 2016 13:45:50 GMT  (1644kb,D)", "http://arxiv.org/abs/1604.07255v1", null], ["v2", "Thu, 27 Oct 2016 13:18:20 GMT  (1256kb,D)", "http://arxiv.org/abs/1604.07255v2", null], ["v3", "Wed, 30 Nov 2016 17:35:27 GMT  (4065kb,D)", "http://arxiv.org/abs/1604.07255v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["chen tessler", "shahar givony", "tom zahavy", "daniel j mankowitz", "shie mannor"], "accepted": true, "id": "1604.07255"}, "pdf": {"name": "1604.07255.pdf", "metadata": {"source": "CRF", "title": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft", "authors": ["Chen Tessler", "Shahar Givony", "Tom Zahavy", "Daniel J. Mankowitz", "Shie Mannor"], "emails": ["{@campus.technion.ac.il", "danielm@tx.technion.ac.il,", "shie@ee.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "This is one of the fundamental learning processes that we have been going through in recent years. (...) There are different ways to realize knowledge transfers in the real world. (...) There are many ways in which we can learn. (...) There are many ways in which we can learn. (...) There are many ways in which we can learn. (...) There are many ways in which we can learn. (...) There are many ways in which we can learn. (...) There are many ways in which we can. (...) There are many ways in which we can learn. (...) There are many ways in which we can learn. (...). (...). (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...). (...).\" (...). (. \"(...).\" (. \"(...).\" (...). \"(.\" (...). \"(.\" (.). \"(...).\" (. \"(...).\" (). \"(.\" (.). \"(...).\" (. \"(.).\" (...). \"(.\" (.). \"(.).\" (. \"(.).\" (.).). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).).\" (.). \"(.\" (.). \"(.). (.).\" (.). \"(.).\" (. (.). \"(.).). (.).\" (. (.). (.). (. (.). (.). \"(. (.). (.). (.). (.).).\" (.).). (. (.). (.). (.). (.). (.).). (.). (.). (.).). (. (."}, {"heading": "2 Background", "text": "The aim of an RL agent is to maximise its expected return by using a Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-"}, {"heading": "3 Hierarchical Deep RL Network", "text": "The hierarchical Deep RL Network (H-DRLN) is a new architecture based on the DQN that facilitates the reuse of skills in lifelong learning. In this section, we provide a detailed description of this network architecture as well as necessary modifications that we have implemented to convert a vanilla DQN into its hierarchical counterpart. H-DRLN Architecture: A diagram of this architecture is shown in Figure 2. Here, the outputs of the H-DRLN consist of primitive skills (e.g. Left (L), Right (R) and Forward (F) as well as skills. H-DRLN learns a policy that determines when primitive actions are performed and when pre-learned skills are reused. The pre-learned skills are presented with deep networks and are called Deep Skill Networks (DSNs). They are executed a-priori on various subtasks using the Vanilla DQN algorithm and the regular experience DRA (DRLN)."}, {"heading": "4 Experiments", "text": "In order to solve new tasks as encountered in a lifelong learning scenario, the agent must be able to adapt to new game dynamics and reuse skills learned from solving previous tasks. In our experiments, we demonstrate (1) the agent's ability to learn a DSN on a Minecraft task called a one-room domain. We then demonstrate (2) the agent's ability to reuse that DSN to solve a new task, designating the two-room domain shown in Figure 5 by learning a hierarchical Deep RL Network (H-DRLN) that includes that DSN as the starting point for action. Finally, we demonstrate (3) the potential to transfer knowledge between related tasks without additional learning. Deep Network Architecture - The deep network architecture used to represent the DSN and H-DRLN is the same as the Vanilla QDN architecture [Mnial, 2015]."}, {"heading": "4.1 Training a DSN", "text": "Our first experiment involved training a DSN in the one-room domain (Figure 3), using the vanilla DQN parameters that worked on the Atari domain [Mnih et al., 2015] as a starting point, and then running a grid search to find the optimal parameters for learning a DSN for the Oneroom domain of Minecraft. Some of the best parameter settings we found include: (1) a higher learning ratio (iterations between emulator states, n-replay = 16), (2) a higher learning rate (learning rate = 0.0025) and (3) less exploration (eps ends - 400K). We implemented these modifications because the standard Minecraft emulator 2 has a slow frame rate (approximately 400 ms per emulator timestep), and these modifications allowed the agent to recover his game performance between the standard Minecraft emulator states, which, seen from Marvel, is similar to Marvel's (roughly 100 K)."}, {"heading": "4.2 Training a H-DRLN with a DSN", "text": "In fact, it is a purely reactionary project, capable of responding to and satisfying the needs of the people."}, {"heading": "5 Discussion", "text": "We have provided the first results for learning Deep Skill Networks (DSNs) in Minecraft, a lifelong learning domain. The3 https: / / www.youtube.com / watch? v = RwjfE4kc6j8DSNs are learned using a Minecraft-specific variant of the DQN [Mnih et al., 2015] algorithm. Our Minecraft agent is also learning how to reuse these DSNs for new tasks using our novel Hierarchical Deep RL Network (H-DRLN) architecture. In addition, we show that H-DRLN provides superior learning performance and faster convergence compared to vanilla DQN by using temporal extensions [Sutton et al., 1999]. Our work can also be interpreted as a form of curriculum learning [Bengio et al., 2009] for RL."}], "references": [{"title": "Online multi-task learning for policy gradient methods", "author": ["Haitham B Ammar", "Eric Eaton", "Paul Ruvolo", "Matthew Taylor"], "venue": "Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Ammar et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Safe policy search for lifelong reinforcement learning with sublinear regret", "author": ["Haitham Bou Ammar", "Rasul Tutunov", "Eric Eaton"], "venue": "arXiv preprint arXiv:1505.05798,", "citeRegEx": "Ammar et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "ACM Transactions on Intelligent Systems and Technology (TIST)", "author": ["Aijun Bai", "Feng Wu", "Xiaoping Chen. Online planning for large markov decision processes with hierarchical decomposition"], "venue": "6(4):45,", "citeRegEx": "Bai et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["Marc G Bellemare", "Georg Ostrovski", "Arthur Guez", "Philip S Thomas", "R\u00e9mi Munos"], "venue": "arXiv preprint arXiv:1512.04860,", "citeRegEx": "Bellemare et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of the 26th annual international conference on machine learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston. Curriculum learning"], "venue": "pages 41\u201348. ACM,", "citeRegEx": "Bengio et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Pac-inspired option discovery in lifelong reinforcement learning", "author": ["Emma Brunskill", "Lihong Li"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14),", "citeRegEx": "Brunskill and Li. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the Twenty Ninth International Conference on Machine Learning", "author": ["B.C. da Silva", "G.D. Konidaris", "A.G. Barto. Learning parameterized skills"], "venue": "June", "citeRegEx": "da Silva et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Ella: An efficient lifelong learning algorithm", "author": ["Eric Eaton", "Paul L Ruvolo"], "venue": "Proceedings of the 30th international conference on machine learning (ICML-13), pages 507\u2013515,", "citeRegEx": "Eaton and Ruvolo. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Hierarchical solution of markov decision processes using macro-actions", "author": ["Hauskrecht et al", "1998] Milos Hauskrecht", "Nicolas Meuleau", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier"], "venue": "In Proceedings of the Fourteenth Conference on Uncertainty in Artificial", "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}, {"title": "In Advances in neural information processing systems", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks"], "venue": "pages 1097\u20131105,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive Partitions (ASAP)", "author": ["Daniel J. Mankowitz", "Timothy A. Mann", "Shie Mannor. Adaptive Skills"], "venue": "arXiv preprint arXiv:1602.03351,", "citeRegEx": "Mankowitz et al.. 2016a", "shortCiteRegEx": null, "year": 2016}, {"title": "Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)", "author": ["Daniel J. Mankowitz", "Timothy A. Mann", "Shie Mannor"], "venue": "arXiv preprint arXiv:1602.03348,", "citeRegEx": "Mankowitz et al.. 2016b", "shortCiteRegEx": null, "year": 2016}, {"title": "Scaling up approximate value iteration with options", "author": ["Mann", "Mannor", "2013] Timothy A. Mann", "Shie Mannor"], "venue": null, "citeRegEx": "Mann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2013}, {"title": "et al", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540),", "citeRegEx": "Mnih et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["Emilio Parisotto", "Jimmy Lei Ba", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1511.06342,", "citeRegEx": "Parisotto et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-time models for temporally abstract planning", "author": ["Doina Precup", "Richard S Sutton"], "venue": "Advances in Neural Information Processing Systems 10 (Proceedings of NIPS\u201997),", "citeRegEx": "Precup and Sutton. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Policy distillation", "author": ["Andrei A Rusu", "Sergio Gomez Colmenarejo", "Caglar Gulcehre", "Guillaume Desjardins", "James Kirkpatrick", "Razvan Pascanu", "Volodymyr Mnih", "Koray Kavukcuoglu", "Raia Hadsell"], "venue": "arXiv preprint arXiv:1511.06295,", "citeRegEx": "Rusu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "EMCL 2000 Proceedings of the 17th International Conference on Machine Learning", "author": ["Peter Stone", "Manuela Veloso", "Park Ave", "Florham Park. Layered Learning"], "venue": "1810(June):369\u2013381,", "citeRegEx": "Stone et al.. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Adaptive Behavior", "author": ["Peter Stone", "Richard S Sutton", "Gregory Kuhlmann. Reinforcement learning for robocup soccer keepaway"], "venue": "13(3):165\u2013188,", "citeRegEx": "Stone et al.. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "pages 120\u2013129", "author": ["Steven C Suddarth", "YL Kergosien. Rule-injection hints as a means of improving network performance", "learning time. In Neural Networks"], "venue": "Springer,", "citeRegEx": "Suddarth and Kergosien. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial Intelligence, 112(1), August", "citeRegEx": "Sutton et al.. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Lifelong robot learning", "author": ["Sebastian Thrun", "Tom M Mitchell"], "venue": "Springer,", "citeRegEx": "Thrun and Mitchell. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Hado Van Hasselt", "Arthur Guez", "David Silver"], "venue": "arXiv preprint arXiv:1509.06461,", "citeRegEx": "Van Hasselt et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Nando de Freitas", "Marc Lanctot"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Graying the black box: Understanding dqns", "author": ["Tom Zahavy", "Nir Ben Zrihem", "Shie Mannor"], "venue": "arXiv preprint arXiv:1602.02658,", "citeRegEx": "Zahavy et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Lifelong learning is defined as the ability to accumulate knowledge across multiple tasks and then reuse or transfer this knowledge in order to solve sub-sequent tasks [Eaton and Ruvolo, 2013].", "startOffset": 168, "endOffset": 192}, {"referenceID": 22, "context": "This is one of the fundamental learning problems in AI [Thrun and Mitchell, 1995; Eaton and Ruvolo, 2013].", "startOffset": 55, "endOffset": 105}, {"referenceID": 7, "context": "This is one of the fundamental learning problems in AI [Thrun and Mitchell, 1995; Eaton and Ruvolo, 2013].", "startOffset": 55, "endOffset": 105}, {"referenceID": 18, "context": "Many of the tasks that are encountered by an agent in a lifelong learning setting can be naturally decomposed into skill hierarchies [Stone et al., 2000; Stone et al., 2005; Bai et al., 2015].", "startOffset": 133, "endOffset": 191}, {"referenceID": 19, "context": "Many of the tasks that are encountered by an agent in a lifelong learning setting can be naturally decomposed into skill hierarchies [Stone et al., 2000; Stone et al., 2005; Bai et al., 2015].", "startOffset": 133, "endOffset": 191}, {"referenceID": 2, "context": "Many of the tasks that are encountered by an agent in a lifelong learning setting can be naturally decomposed into skill hierarchies [Stone et al., 2000; Stone et al., 2005; Bai et al., 2015].", "startOffset": 133, "endOffset": 191}, {"referenceID": 21, "context": "Reinforcement Learning (RL) provides a generalized approach to skill learning through the options framework [Sutton et al., 1999].", "startOffset": 108, "endOffset": 129}, {"referenceID": 6, "context": "Options are Temporally Extended Actions (TEAs) and are also referred to as skills [da Silva et al., 2012] and macro-actions [Hauskrecht et al.", "startOffset": 82, "endOffset": 105}, {"referenceID": 15, "context": "Options have been shown both theoretically [Precup and Sutton, 1997; Sutton et al., 1999] and experimentally [Mann and Mannor, 2013] to speed up the convergence rate of RL algorithms.", "startOffset": 43, "endOffset": 89}, {"referenceID": 21, "context": "Options have been shown both theoretically [Precup and Sutton, 1997; Sutton et al., 1999] and experimentally [Mann and Mannor, 2013] to speed up the convergence rate of RL algorithms.", "startOffset": 43, "endOffset": 89}, {"referenceID": 10, "context": "Recent work in RL has provided insights into learning reusable skills [Mankowitz et al., 2016a; Mankowitz et al., 2016b], but this has been limited to low dimensional problems.", "startOffset": 70, "endOffset": 120}, {"referenceID": 11, "context": "Recent work in RL has provided insights into learning reusable skills [Mankowitz et al., 2016a; Mankowitz et al., 2016b], but this has been limited to low dimensional problems.", "startOffset": 70, "endOffset": 120}, {"referenceID": 13, "context": "With the emergence of Deep Reinforcement Learning, specifically Deep QNetworks (DQNs)[Mnih et al., 2015], RL agents are now equipped with a powerful non-linear function approximator that can learn rich and complex policies.", "startOffset": 85, "endOffset": 104}, {"referenceID": 13, "context": "g Atari video games [Mnih et al., 2015]).", "startOffset": 20, "endOffset": 39}, {"referenceID": 23, "context": "While different variations of the DQN algorithm exist [Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015], we will refer to the Vanilla DQN [Mnih et al.", "startOffset": 54, "endOffset": 144}, {"referenceID": 17, "context": "While different variations of the DQN algorithm exist [Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015], we will refer to the Vanilla DQN [Mnih et al.", "startOffset": 54, "endOffset": 144}, {"referenceID": 24, "context": "While different variations of the DQN algorithm exist [Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015], we will refer to the Vanilla DQN [Mnih et al.", "startOffset": 54, "endOffset": 144}, {"referenceID": 3, "context": "While different variations of the DQN algorithm exist [Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015], we will refer to the Vanilla DQN [Mnih et al.", "startOffset": 54, "endOffset": 144}, {"referenceID": 13, "context": ", 2015], we will refer to the Vanilla DQN [Mnih et al., 2015] unless otherwise stated.", "startOffset": 42, "endOffset": 61}, {"referenceID": 13, "context": "In our paper, we focus on learning reusable RL skills using Deep Q Networks [Mnih et al., 2015], by solving subproblems in Minecraft.", "startOffset": 76, "endOffset": 95}, {"referenceID": 13, "context": "Deep Q Networks: The DQN algorithm [Mnih et al., 2015] approximates the optimal Q function with a Convolutional Neural Network (CNN) [Krizhevsky et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 9, "context": ", 2015] approximates the optimal Q function with a Convolutional Neural Network (CNN) [Krizhevsky et al., 2012], by optimizing the network weights such that the expected Temporal Difference (TD) error of the optimal bellman equation (Equation 1) is minimized:", "startOffset": 86, "endOffset": 111}, {"referenceID": 21, "context": "Skills, Options, Macro-actions [Sutton et al., 1999]: A skill \u03c3 is a temporally extended control structure defined by a triple \u03c3 =< I, \u03c0, \u03b2 > where I is the set of states where the skill can be initiated, \u03c0 is the intra-skill policy, which determines how the skill behaves in encountered states, and \u03b2 is the set of termination probabilities determining when a skill will stop execution.", "startOffset": 31, "endOffset": 52}, {"referenceID": 13, "context": "Skill - Experience Replay: We extend the regular ER [Mnih et al., 2015] to incorporate skills and have termed this Skill Experience Replay (S-ER).", "startOffset": 52, "endOffset": 71}, {"referenceID": 13, "context": "Deep Network Architecture - The deep network architecture used to represent the DSN and H-DRLN is the same as that of the vanilla DQN architecture [Mnih et al., 2015].", "startOffset": 147, "endOffset": 166}, {"referenceID": 13, "context": "To do so we used the Vanilla DQN parameters that worked on the Atari domain [Mnih et al., 2015] as a starting point and then performed a grid search to find the optimal parameters for learning a DSN for the Minecraft oneroom domain.", "startOffset": 76, "endOffset": 95}, {"referenceID": 13, "context": "The rest of the parameters from the Vanilla DQN remained unchanged, since Minecraft and Atari [Mnih et al., 2015] share relatively similar in-game screen resolutions.", "startOffset": 94, "endOffset": 113}, {"referenceID": 13, "context": "DSNs are learned using a Minecraft-specific variation of the DQN [Mnih et al., 2015] algorithm.", "startOffset": 65, "endOffset": 84}, {"referenceID": 21, "context": "In addition, we show that the H-DRLN provides superior learning performance and faster convergence compared to the vanilla DQN, by making use of temporal extension [Sutton et al., 1999].", "startOffset": 164, "endOffset": 185}, {"referenceID": 4, "context": "Our work can also be interpreted as a form of curriculum learning [Bengio et al., 2009] for RL.", "startOffset": 66, "endOffset": 87}, {"referenceID": 25, "context": "Recently, it has been shown that Deep Networks tend to implicitly capture the hierarchical composition of a given task [Zahavy et al., 2016].", "startOffset": 119, "endOffset": 140}, {"referenceID": 14, "context": "In addition, it is possible to distill much of the knowledge from multiple teacher networks into a single student network [Parisotto et al., 2015; Rusu et al., 2015].", "startOffset": 122, "endOffset": 165}, {"referenceID": 16, "context": "In addition, it is possible to distill much of the knowledge from multiple teacher networks into a single student network [Parisotto et al., 2015; Rusu et al., 2015].", "startOffset": 122, "endOffset": 165}, {"referenceID": 20, "context": "We wish to perform a similar technique as well as add auxiliary tasks to train the teacher networks (DSNs) [Suddarth and Kergosien, 1990], ultimately guiding learning in the student network (our H-DRLN).", "startOffset": 107, "endOffset": 137}], "year": 2016, "abstractText": "The ability to reuse or transfer knowledge from one task to another in lifelong learning problems, such as Minecraft, is one of the major challenges faced in AI. Reusing knowledge across tasks is crucial to solving tasks efficiently with lower sample complexity. We provide a Reinforcement Learning agent with the ability to transfer knowledge by learning reusable skills, a type of temporally extended action (also know as Options (Sutton et. al. 1999)). The agent learns reusable skills using Deep Q Networks (Mnih et. al. 2015) to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks (DSNs), are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture. The H-DRLN is a hierarchical version of Deep QNetworks and learns to efficiently solve tasks by reusing knowledge from previously learned DSNs. The H-DRLN exhibits superior performance and lower learning sample complexity (by taking advantage of temporal extension) compared to the regular Deep Q Network (Mnih et. al. 2015) in subdomains of Minecraft. We also show the potential to transfer knowledge between related Minecraft tasks without any additional learning.", "creator": "LaTeX with hyperref package"}}}