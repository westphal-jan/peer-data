{"id": "1604.07928", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2016", "title": "Distributed Flexible Nonlinear Tensor Factorization", "abstract": "Tensor factorization is an important approach to multiway data analysis. Compared with popular multilinear methods, nonlinear tensor factorization models are able to capture more complex relationships in data. However, they are computationally expensive and incapable of exploiting the data sparsity. To overcome these limitations, we propose a new tensor factorization model. The model employs a Gaussian process (GP) to capture the complex nonlinear relationships. The GP can be projected to arbitrary sets of tensor elements, and thus can avoid the expensive computation of the Kronecker product and is able to flexibly incorporate meaningful entries for training. Furthermore, to scale up the model to large data, we develop a distributed variational inference algorithm in MapReduce framework. To this end, we derive a tractable and tight variational evidence lower bound (ELBO) that enables efficient parallel computations and high quality inferences. In addition, we design a non-key-value Map-Reduce scheme that can prevent the costly data shuffling and fully use the memory-cache mechanism in fast MapReduce systems such as SPARK. Experiments demonstrate the advantages of our method over existing approaches in terms of both predictive performance and computational efficiency. Moreover, our approach shows a promising potential in the application of Click-Through-Rate (CTR) prediction for online advertising.", "histories": [["v1", "Wed, 27 Apr 2016 04:18:32 GMT  (572kb)", "https://arxiv.org/abs/1604.07928v1", "Gaussian process, tensor factorization, multidimensional arrays, large scale, spark, map-reduce"], ["v2", "Sun, 22 May 2016 00:00:23 GMT  (596kb)", "http://arxiv.org/abs/1604.07928v2", "Gaussian process, tensor factorization, multidimensional arrays, large scale, spark, map-reduce"]], "COMMENTS": "Gaussian process, tensor factorization, multidimensional arrays, large scale, spark, map-reduce", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.DC stat.ML", "authors": ["shandian zhe", "kai zhang 0001", "pengyuan wang", "kuang-chih lee", "zenglin xu", "yuan qi", "zoubin ghahramani"], "accepted": true, "id": "1604.07928"}, "pdf": {"name": "1604.07928.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Pengyuan Wang", "Zoubin Ghahraman"], "emails": ["szhe@purdue.edu", "kzhang980@gmail.com", "pengyuan@yahoo-inc.com", "kclee@yahoo-inc.com", "zlxu@uestc.edu.cn", "alanqi@cs.purdue.edu", "zoubin@eng.cam.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 4th"}, {"heading": "1 Introduction", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "2 Background", "text": "First of all, we present the background knowledge."}, {"heading": "3 Model", "text": "It is not as if it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is not in which it is about a way in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in"}, {"heading": "4 Distributed Variational Inference", "text": "The real-world tensor data often includes a large number of entries, say, millions of zeros and billions of zeros. Even if you only use non-zero entries for training, an exact conclusion of the proposed model may still not be possible, which motivates us to develop a distributed variable inference algorithm presented as follows."}, {"heading": "4.1 Tractable Variational Evidence Lower Bound", "text": "Since the GP covariance concept - k (XS), XS) (see equations (2) and (3)) - all latent factors are intertwined, the exact conclusion is difficult in a parallel way. Therefore, we first derive a tractable variable evidence having a lower limit (ELBO), following the sparse Gaussian process framework of Titsias [23]. The key idea is to create a small set of inducing points B = {b1,.,., bp} and latent targets v = {v1,.,., vp} (p, N). Then we supplement the original model with a common multivariate Gaussian model of the latent tensor entries m and targets v, p)., B) = N (mv]; [00], [KSS KSB KBS KBB]), where we add a common multivariate Gaussian model of the latent tenor entor entries m and targets v, p)."}, {"heading": "4.2 Tight and Parallelizable Variational Evidence Lower Bound", "text": "In this section, we further deduce strict ELBOs, which include the optimal variation variables for q (v) and q (z). Furthermore, it is very likely that the quality of the conclusions will be improved by narrower boundaries. Due to the spatial limitation, we present here only key ideas and results. Detailed discussions are given in section 1 of the supplementary material. We take functional derivatives from L1 with respect to q (v) in (4). By setting the derivatives to zero, we obtain the optimal q (v) (which is a Gaussian distribution) and then replace it in L1, manipulating the conditions, we achieve the following narrower ELBO.Theorem 4.1 For continuous data, we achieve havelog (p, U | B)."}, {"heading": "4.3 Distributed Inference on Tight Bound", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1 Distributed Gradient-based Optimization", "text": "Given the tighter ELBOs in (6) and (7), we develop a distributed algorithm to optimize the latent factors U, the inductive points B, the variation parameters \u03bb (for binary data), and the kernel parameters. We distribute the calculations over several computational nodes q (MAP step), and then collect the results to calculate the ELBO and its gradient (REDUCE step). A standard routine, such as gradient descent and L-BFGS, is then used to solve the optimization problem. In binary data, we further find that \u03bb can be updated with a simple fix point titeration: \u03bb (t + 1) = (KBB + A1) \u2212 1 (t) + a5) (8), where a5 = optimization j k (B, xij) (2yij \u2212 1) N (B, xij) and xij)."}, {"heading": "4.3.2 Key-Value-Free MAPREDUCE", "text": "In this section, we present the detailed design of the MAPREDUCE procedures to fulfill our distributed conclusions. Basically, we first assign a series of tensor entries St on each MAPPER to calculate the corresponding components of the ELBO and the gradients. Then, the REDUCER aggregates local results from each MAPPER to obtain the integrated global ELBO and gradients. We first consider the standard (key value) design. For brevity, we take the gradient calculation for the latent factors as an example. For each tensor entry i on a MAPPER, we calculate the corresponding gradients (1) i1,."}, {"heading": "4.4 Algorithm Complexity", "text": "Suppose we use N-tensor entries for training, with p-inducing points and T-MAPPER, the time complexity for each MAPPER node is O (1T p2N). Since p-N is a fixed constant (p = 100 in our experiments), the time complexity is linear in the number of tensor entries. The space complexity for each MAPPER node is O (\u2211 Kj = 1 mjrj + p2 + N T K) to store the latent factors, their gradients, the covariance matrix on inducing points and the indices of the latent factors for each tensor entry. Again, the space complexity is linear in the number of tensor entries. In comparison, InfTucker uses the Kronecker product properties to calculate the gradients, and must insert eigenvalue substitution of the covariance matrices in each tenor entry."}, {"heading": "5 Related work", "text": "In this context, it should be noted that both are very complex issues, and it is a very complex issue."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Evaluation on Small Tensor Data", "text": "This year it is more than ever before in the history of the city."}, {"heading": "6.2 Scalability Analysis", "text": "To investigate the scalability of the proposed distributed inference algorithm, we used the following large real data sets: (1) ACC, A real tensor that describes triple interactions (user, action, resource) in a code repository management system [29]. The tensor is of size 3K x 150 x 30K, with 0.009% non-zero. (2) DBLP: a binary tensor that represents a triple bibliographic relationship (author, conference, keyword) [29]. The tensor was extracted from the DBLP database and contains 10K x 200 x 10K elements, where 0.001% are unequal entries. (3) NELL: a binary tensor that represents the knowledge predicates in the form of (unit, entity, relationship) [28]. The tensor size is 20K x 12.3K x 280 and 0.0001% are null."}, {"heading": "6.3 Evaluation on Large Tensor Data", "text": "We compared our approach with three state-of-the-art factorization methods: GigaTensor [11], Distributed infinite Tucker decomposition (DinTucker) [28], and InfTuckerEx [29]. Both GigaTensor and DinTucker are developed on a single computer, while InfTuckerEx expands the number of latent factors to 3 and DBLP datasets, and sets 5 to NELL datasets."}, {"heading": "6.4 Application on Click-Through-Rate Prediction", "text": "In this section, we report on the results of applying our nonlinear tensor factorization approach to click-through rate (CTR) predictions for online advertising. We used the online ad click log of a major Internet company, from which we extracted a four-mode tensor (user, advertiser, publisher, page section). We used the log of the first three days in May 2015, trained our model on the data of one day and used it to predict click behavior the next day. In other words, the size of the extracted tensors for the three days is 179K \u00d7 81K \u00d7 35 \u00d7 355, 167K \u00d7 78K \u00d7 35 \u00d7 354 and 213K \u00d7 37 \u00d7 354. These tensors are very sparse (2.7 \u00d7 10 \u2212 8% non-negatives on average). In other words, the clicks observed are very rare (we do not want our prediction to be completely zero (i.e., not click)."}, {"heading": "7 Conclusion", "text": "In this work, we have proposed a new nonlinear and flexible tensor factorization model. By eliminating the covariance structure of the Kronecker product, the model is able to properly utilize data diversity and is flexible enough to integrate any subset of meaningful tensor entries for training. In addition, we have derived a streamlined ELBO for both continuous and binary problems, on the basis of which we have further developed an efficient distributed variable inference algorithm within the framework of MAPREDUCE. In the future, we will consider applying asynchronous inferences to the narrow ELBO, such as [20] to further improve the scalability of our model."}, {"heading": "1 Tight Variational Evidence Lower Bound", "text": "The naive variation results derived from the sparse Gaussian procedural framework for the lower limit (ELBO) (see section 4.1 of the essay) are given by L1 (U, B, q (v)) = log (p (U))) + q (v) log p (v) dv (v) dv (v) q (yij, \u03b2) dv (9) for continuous tensor andL2 (U, B, q (v), q (z)))) = log (p (U)) + q (v) log (p (v) q (v) q (v) q (v) q (zj) log (zj) log (p (yij) log (p (v) q (zj) q (zj) q (zj) q (zj) q (zj) (zj) log (v) q (v) q (zj) q (zj) c (c), q (q) v (c), q (q), q (q (q) v v (c), c (c, c, 1) dzjdv b (10), j b, j (j), q (q), q (q), q (q (q), q (c), q (q), q (b, q (c), q (c, q), q (c, q (c), q (c, q (zj), q (c), q (c, q (zj c), q (zj c), q (zj b), q (zj (zj zj zj), q (zj zj zj zj zj (zj), zj (zj zj zj zj (v (v), zj zj zj zj (v (v), zj zj zj zj (v (v), b, b, b, b, b, b, b (zj zj zj (zj zj zj zj zj), b), b (zj (zj zj zj zj (zj zj), b), b (zj (zj"}, {"heading": "1.1 Continuous Tensor", "text": "Considering U and B, we use functional derivatives [3] to calculate the optimal q (v). Since q (v) is a probability density function, we use Lagrange multipliers to impose the constraint and obtain the optimal q (v) through solution mechanisms (L1 (q) + iq (v) dv \u2212 1))). Since q (v) is a probability density function, we use Lagrange multipliers to impose the constraint and obtain the optimal q (v) through solution mechanisms (L1 (q) + iq (v) dv \u2212 1)))). \u2212 aq (v) = 0, \u0445 (L1 (q) + \u03bb (v) dv \u2212 1 (v \u2212 1))."}, {"heading": "1.2 Binary Tensor", "text": "The case for binary tensors is more complex (< < < < p (z) = p (z) = p (z) = p (z). \u2212 p (z) In addition, q (v) and q (z) are linked in the original case (see (10)). To eliminate the additional variations q (v) and q (z), we use the following steps. \u2212 p (z), we calculate the optimal q (v) and insert it into L2 (this is similar to the ongoing case) to achieve an intermediate limit, L (z), B) = max q (v), L (z), U, B) = 12 log | KBB | \u2212 1 2 log | KBB + A1 (2)."}, {"heading": "2 Gradients of the Tight ELBO", "text": "In this section we present how the gradients of the large ELBOs in (11) and (15) BB BB BB BB BB BB BB BB BB BB BB BB BB BB BB BB BB BB BB BB BB BB B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B"}, {"heading": "3 Fixed Point Iteration for \u03bb", "text": "In this section, we provide the convergence proof for the fixed point titeration of the variation parameters q = q (in the narrow ELBO for binary tensors).While the fixed point titeration can be optimized collectively by gradient-based approaches with U, B and the kernel parameters, we find empirically that the combination of this fixed point titeration can converge much faster. Fixed point titeration is achieved by \u03bb (t + 1) = (KBB + A1) \u2212 1 (A1\u03bb (t) + a5) (18), whereas A1 = q k (B, xij) k (xij) k (xij) k (xij), B), a5 = sp (xij) \u2212 N (k (B, xij) (t) (t), (t), (t), (t), (t) k (2yij), (q) k (B, xij), (B), j), (j), we show that the fixed point (x) is always (BO), but that we also (x)."}], "references": [{"title": "Scalable tensor factorizations for incomplete data", "author": ["E. Acar", "D.M. Dunlavy", "T.G. Kolda", "M. Morup"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Laser: A scalable response prediction platform for online advertising", "author": ["D. Agarwal", "B. Long", "J. Traupman", "D. Xin", "L. Zhang"], "venue": "In Proceedings of the 7th ACM international conference on Web search and data mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Pattern recognition and machine learning. springer", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Dfacto: Distributed factorization of tensors", "author": ["J.H. Choi", "S. Vishwanathan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Probabilistic models for incomplete multidimensional arrays. AISTATS", "author": ["W. Chu", "Z. Ghahramani"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Optimizing shuffle performance in spark. University of California, Berkeley-Department of Electrical Engineering and Computer", "author": ["A. Davidson", "A. Or"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Distributed variational inference in sparse gaussian process regression and latent variable models", "author": ["Y. Gal", "M. van der Wilk", "C. Rasmussen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Foundations of the PARAFAC procedure: Model and conditions for an\u201dexplanatory\u201dmulti-mode factor analysis", "author": ["R.A. Harshman"], "venue": "UCLA Working Papers in Phonetics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1970}, {"title": "Hierarchical multilinear models for multiway data", "author": ["P. Hoff"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Zero-truncated poisson tensor factorization for massive binary tensors", "author": ["C. Hu", "P. Rai", "L. Carin"], "venue": "In UAI", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Gigatensor: scaling tensor analysis up by 100 times-algorithms and discoveries", "author": ["U. Kang", "E. Papalexakis", "A. Harpale", "C. Faloutsos"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "A multilinear singular value decomposition", "author": ["L.D. Lathauwer", "B.D. Moor", "J. Vandewalle"], "venue": "SIAM J. Matrix Anal. Appl,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Gaussian process latent variable models for visualisation of high dimensional data", "author": ["N.D. Lawrence"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Random function priors for exchangeable arrays with applications to graphs and relational data", "author": ["J.R. Lloyd", "P. Orbanz", "Z. Ghahramani", "D.M. Roy"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Old and new matrix algebra useful for statistics", "author": ["T.P. Minka"], "venue": "See www. stat. cmu. edu/minka/papers/matrix. html", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "A unifying view of sparse approximate gaussian process regression", "author": ["J. Qui\u00f1onero-Candela", "C.E. Rasmussen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Scalable probabilistic tensor factorization for binary and count data", "author": ["P. Rai", "C. Hu", "M. Harding", "L. Carin"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Scalable Bayesian low-rank decomposition of incomplete multiway tensors", "author": ["P. Rai", "Y. Wang", "S. Guo", "G. Chen", "D. Dunson", "L. Carin"], "venue": "In Proceedings of the 31th International Conference on Machine Learning (ICML)", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Non-negative tensor factorization with applications to statistics and computer vision", "author": ["A. Shashua", "T. Hazan"], "venue": "In Proceedings of the 22th International Conference on Machine Learning (ICML),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Asynchronous distributed learning of topic models", "author": ["P. Smyth", "M. Welling", "A.U. Asuncion"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Provable sparse tensor decomposition. arXiv preprint arXiv:1502.01425", "author": ["W. Sun", "J. Lu", "H. Liu", "G. Cheng"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Modelling relational data using bayesian clustered tensor factorization. In Advances in neural information processing", "author": ["I. Sutskever", "J.B. Tenenbaum", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Variational learning of inducing variables in sparse gaussian processes", "author": ["M.K. Titsias"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["L. Tucker"], "venue": "Psychometrika, 31,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1966}, {"title": "Infinite Tucker decomposition: Nonparametric Bayesian models for multiway data analysis", "author": ["Z. Xu", "F. Yan", "Y. Qi"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML)", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Bayesian conditional tensor factorizations for high-dimensional classification", "author": ["Y. Yang", "D. Dunson"], "venue": "Journal of the Royal Statistical Society B, revision submitted", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing", "author": ["M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Dintucker: Scaling up gaussian process models on multidimensional arrays with billions of elements", "author": ["S. Zhe", "Y. Qi", "Y. Park", "I. Molloy", "S. Chari"], "venue": "arXiv preprint arXiv:1311.2663", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}], "referenceMentions": [{"referenceID": 23, "context": "Classical tensor factorization models include Tucker [24] and CANDECOMP/PARAFAC (CP) [8] decompositions, which have been widely used in real-world applications.", "startOffset": 53, "endOffset": 57}, {"referenceID": 7, "context": "Classical tensor factorization models include Tucker [24] and CANDECOMP/PARAFAC (CP) [8] decompositions, which have been widely used in real-world applications.", "startOffset": 85, "endOffset": 88}, {"referenceID": 24, "context": "[25] proposed Infinite Tucker decomposition (InfTucker), which generalizes the Tucker model to infinite feature space using a Tensor-variate Gaussian process (TGP) and thus is powerful to model intricate nonlinear interactions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "However, InfTucker and its variants [28, 29] are computationally expensive, because the Kronecker product between the covariances of all the modes requires the TGP to model the entire tensor structure.", "startOffset": 36, "endOffset": 44}, {"referenceID": 27, "context": "cantly better than, or at least as good as two popular large-scale nonlinear factorization methods based on TGP: one uses hierarchical modeling to perform distributed infinite Tucker decomposition [28]; the other further enhances InfTucker by using Dirichlet process mixture prior over the latent factors and employs an online learning scheme [29].", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "Our method also outperforms GigaTensor [11], a typical large-scale CP factorization algorithm, by a large margin.", "startOffset": 39, "endOffset": 43}, {"referenceID": 24, "context": "For convenience, we will use the same notations in [25].", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "The infinite Tucker decomposition (InfTucker) generalizes the Tucker model to infinite feature space via a tensor-variate Gaussian process (TGP) [25].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": ",U}, Equation (1) actually defines a Gaussian process (GP) on tensors, namely tensor-variate GP (TGP) [25],", "startOffset": 102, "endOffset": 106}, {"referenceID": 27, "context": "[28, 29] proposed to improve the scalability by modeling subtensors instead, the sampled subtensors can still be very sparse.", "startOffset": 0, "endOffset": 8}, {"referenceID": 22, "context": "Therefore, we first derive a tractable variational evidence lower bound (ELBO), following the sparse Gaussian process framework by Titsias [23].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "Using a very similar derivation to [23], we can obtain a tractable", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "Efficient MAPREDUCE systems, such as SPARK [27], can fully optimize the nonshuffling MAP and REDUCE, where most of the data are buffered in memory and disk I/Os are circumvented to the utmost; by contrast, the performance with data shuffling degrades severely [6].", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "Efficient MAPREDUCE systems, such as SPARK [27], can fully optimize the nonshuffling MAP and REDUCE, where most of the data are buffered in memory and disk I/Os are circumvented to the utmost; by contrast, the performance with data shuffling degrades severely [6].", "startOffset": 260, "endOffset": 263}, {"referenceID": 24, "context": "Therefor it has a higher time and space complexity (see [25] for details) and is not scalable to large dimensions.", "startOffset": 56, "endOffset": 60}, {"referenceID": 23, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 80, "endOffset": 83}, {"referenceID": 18, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 4, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 21, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 0, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 8, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 25, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 17, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 20, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 9, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 16, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 10, "context": "To deal with big data, several distributed factorization algorithms have been recently developed, such as GigaTensor [11] and DFacTo [4].", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "To deal with big data, several distributed factorization algorithms have been recently developed, such as GigaTensor [11] and DFacTo [4].", "startOffset": 133, "endOffset": 136}, {"referenceID": 24, "context": "Infinite Tucker decomposition [25], and its distributed or online extensions [28, 29] address this issue by modeling tensors or subtensors via a tensor-variate Gaussian process (TGP).", "startOffset": 30, "endOffset": 34}, {"referenceID": 27, "context": "Infinite Tucker decomposition [25], and its distributed or online extensions [28, 29] address this issue by modeling tensors or subtensors via a tensor-variate Gaussian process (TGP).", "startOffset": 77, "endOffset": 85}, {"referenceID": 13, "context": "In theory, all such nonlinear factorization models belong to the random function prior models [14] for exchangeable multidimensional arrays.", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "Our distributed variational inference algorithm is based on sparse GP [16], an efficient approximation framework to scale up GP models.", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "Recently, Titsias [23] proposed a variational learning framework for sparse GP, based on which Gal et al.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "[7] derived a tight variational lower bound for distributed inference of GP regression and GPLVM [13].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[7] derived a tight variational lower bound for distributed inference of GP regression and GPLVM [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "The derivation of the tight ELBO in our model for continuous tensors is similar to [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 18, "context": "We compared with CP, nonnegative CP (NN-CP) [19], high order SVD (HOSVD) [12], Tucker, infinite Tucker (InfTucker) Xu et al.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "We compared with CP, nonnegative CP (NN-CP) [19], high order SVD (HOSVD) [12], Tucker, infinite Tucker (InfTucker) Xu et al.", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "[25] and its extension (InfTuckerEx) which uses the Dirichlet process mixture (DPM) prior to model latent clusters and local TGP to perform scalable, online factorization [29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "(3) NELL: a binary tensor representing the knowledge predicates, in the form of (entity, entity, relationship) [28].", "startOffset": 111, "endOffset": 115}, {"referenceID": 10, "context": "3 Evaluation on Large Tensor Data We then compared our approach with three state-of-the-art large-scale tensor factorization methods: GigaTensor [11], Distributed infinite Tucker decomposition (DinTucker) [28], and InfTuckerEx [29].", "startOffset": 145, "endOffset": 149}, {"referenceID": 27, "context": "3 Evaluation on Large Tensor Data We then compared our approach with three state-of-the-art large-scale tensor factorization methods: GigaTensor [11], Distributed infinite Tucker decomposition (DinTucker) [28], and InfTuckerEx [29].", "startOffset": 205, "endOffset": 209}, {"referenceID": 27, "context": "Following the settings in [29, 28], we randomly chose 80% of nonzero entries for training, and then sampled 50 test data sets from the remaining entries.", "startOffset": 26, "endOffset": 34}, {"referenceID": 1, "context": "Note that training CTR prediction models with comparable clicks and non-click samples is common in online advertising systems [2].", "startOffset": 126, "endOffset": 129}], "year": 2016, "abstractText": "Tensor factorization is a powerful tool to analyse multi-way data. Compared with traditional multi-linear methods, nonlinear tensor factorization models are capable of capturing more complex relationships in the data. However, they are computationally expensive and may suffer severe learning bias in case of extreme data sparsity. To overcome these limitations, in this paper we propose a distributed, flexible nonlinear tensor factorization model. Our model can effectively avoid the expensive computations and structural restrictions of the Kronecker-product in existing TGP formulations, allowing an arbitrary subset of tensorial entries to be selected to contribute to the training. At the same time, we derive a tractable and tight variational evidence lower bound (ELBO) that enables highly decoupled, parallel computations and high-quality inference. Based on the new bound, we develop a distributed inference algorithm in the MAPREDUCE framework, which is key-value-free and can fully exploit the memory cache mechanism in fast MAPREDUCE systems such as SPARK. Experimental results fully demonstrate the advantages of our method over several state-of-the-art approaches, in terms of both predictive performance and computational efficiency. Moreover, our approach shows a promising potential in the application of Click-Through-Rate (CTR) prediction for online advertising.", "creator": "LaTeX with hyperref package"}}}