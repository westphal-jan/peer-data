{"id": "1604.08633", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2016", "title": "Word Ordering Without Syntax", "abstract": "Recent work on word ordering has argued that syntactic structure is important, or even required, for effectively recovering the order of a sentence. We find that, in fact, an n-gram language model with a simple heuristic gives strong results on this task. Furthermore, we show that a long short-term memory (LSTM) language model is comparatively effective at recovering order, with our basic model outperforming a state-of-the-art syntactic model by 11.5 BLEU points. Additional data and larger beams yield further gains, at the expense of training and search time.", "histories": [["v1", "Thu, 28 Apr 2016 22:09:49 GMT  (182kb,D)", "http://arxiv.org/abs/1604.08633v1", null], ["v2", "Sat, 24 Sep 2016 03:41:45 GMT  (137kb)", "http://arxiv.org/abs/1604.08633v2", "EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["allen schmaltz", "alexander m rush", "stuart m shieber"], "accepted": true, "id": "1604.08633"}, "pdf": {"name": "1604.08633.pdf", "metadata": {"source": "CRF", "title": "Word Ordering Without Syntax", "authors": ["Allen Schmaltz", "Stuart M. Shieber"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The task of word order, or linearization, is to restore the original order of a mixed sentence, which has been standardized in a recent line of research to isolate the performance of text-to-text generation models (Zhang and Clark, 2011; Liu et al., 2015; Liu and Zhang, 2015; Zhang and Clark, 2015).The dominant argument of this work is that the joint restoration of explicit syntactic structures is critical to determining the correct word sequence of the original sentence. As such, these methods either generate a predetermined parse structure or rely on it to reproduce the order. Separately, Elman (1990) investigated linearization in his groundbreaking work on recursive neural networks. Elman assessed the ability of early recurrent neural networks to predict word order in simple sentences."}, {"heading": "2 Background: Linearization", "text": "The task of linearization is to restore the original order of a mixed sentence. We start with a vocabulary V and get a sequence of disordered phrases x1,..., xN, with xn \u0394V + for 1 \u2264 n \u2264 N. Let's define M as the total number of tokens (i.e. the sum of the lengths of the phrases), consider two types of task: (1) WORDS, where each xn consists of a single word and M = N, and (2) WORDS + BNPS, ar Xiv: 160 4.08 633v 1 [cs.C L] April 28, where base-noun phrases (noun phrases that do not contain inner nouns) are also provided, and M \u2265 N. While the first is closer to Elman's formulation, the second has become more established in recent literature."}, {"heading": "3 Related Work: Syntactic Linearization", "text": "The most recent approaches to linearization are based on the reconstruction of the syntactic structure to discuss the word order. < Z represent all the projective dependence sparse trees via M-words. The goal is to find y-trees (PTB) (Marcus et al., 1993) without external data, by Liu et al. (2015) uses a transitional parser with beam search to construct a sentence and a parse tree. The scoring function is a linear model f (x, y) = inequality (x, y, z), and is formed with an early update of structured perception structures to match both a given order and a syntactic tree."}, {"heading": "4 LM-Based Linearization", "text": "Unlike most previous work, we use an LM directly for word order. We look at two types of language models: an n-gram model and a long-term short-term memory network (Hochreiter and Schmidhuber, 1997). For the purpose of this work, we define a common abstraction for both models. Let h-H define the current state of the model with which we estimate the probability of the next word (wi \u2212 1) that we use for the next word of the next word (wi \u2212 1). For n-gram language models, H, and q, of course, can be defined to produce an estimate of the probability of the next word q (wi \u2212 1)."}, {"heading": "5 Experiments", "text": "This year, it has come to the point where there is only one person who is able to come out on top."}, {"heading": "6 Conclusion", "text": "We have shown that strong interface language models restore word order more accurately than previous models trained with explicit syntactical annotations, and we have created strong foundations for the task to be used by the community to compare future text generation models."}, {"heading": "Acknowledgments", "text": "We thank Yue Zhang and Jiangming Liu for their help in using ZGen and for reviewing the task for a valid settlement."}], "references": [{"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cog-", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Transition-based syntactic linearization", "author": ["Yijia Liu", "Yue Zhang", "Wanxiang Che", "Bing Qin."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Annotated gigaword", "author": ["Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme."], "venue": "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX \u201912, pages 95\u2013100, Strouds-", "citeRegEx": "Napoles et al\\.,? 2012", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, Strouds-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Yara parser: A fast and accurate dependency parser", "author": ["Mohammad Sadegh Rasooli", "Joel R. Tetreault."], "venue": "CoRR, abs/1503.06733.", "citeRegEx": "Rasooli and Tetreault.,? 2015", "shortCiteRegEx": "Rasooli and Tetreault.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "CoRR, abs/1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Syntax-based grammaticality improvement using ccg and guided search", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201911, pages 1147\u20131157, Stroudsburg, PA,", "citeRegEx": "Zhang and Clark.,? 2011", "shortCiteRegEx": "Zhang and Clark.", "year": 2011}, {"title": "Discriminative syntax-based word ordering for text generation", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Comput. Linguist., 41(3):503\u2013538, September.", "citeRegEx": "Zhang and Clark.,? 2015", "shortCiteRegEx": "Zhang and Clark.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "The task has been standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation models (Zhang and Clark, 2011; Liu et al., 2015; Liu and Zhang, 2015; Zhang and Clark, 2015).", "startOffset": 143, "endOffset": 228}, {"referenceID": 1, "context": "The task has been standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation models (Zhang and Clark, 2011; Liu et al., 2015; Liu and Zhang, 2015; Zhang and Clark, 2015).", "startOffset": 143, "endOffset": 228}, {"referenceID": 9, "context": "The task has been standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation models (Zhang and Clark, 2011; Liu et al., 2015; Liu and Zhang, 2015; Zhang and Clark, 2015).", "startOffset": 143, "endOffset": 228}, {"referenceID": 0, "context": "It is, therefore, an interesting question to ask whether a network can learn any aspects of that underlying abstract structure (Elman, 1990).", "startOffset": 127, "endOffset": 140}, {"referenceID": 0, "context": "Independently, Elman (1990) also explored linearization in his seminal work on recurrent neural networks.", "startOffset": 15, "endOffset": 28}, {"referenceID": 2, "context": "The current state of the art on the Penn Treebank (PTB) (Marcus et al., 1993), without external data, of Liu et al.", "startOffset": 56, "endOffset": 77}, {"referenceID": 8, "context": "This work improves upon past work which used best-first search over a similar objective (Zhang and Clark, 2011).", "startOffset": 88, "endOffset": 111}, {"referenceID": 1, "context": ", 1993), without external data, of Liu et al. (2015) uses a transitionbased parser with beam search to construct a sentence and a parse tree.", "startOffset": 35, "endOffset": 53}, {"referenceID": 7, "context": "Existing work with LSTMs has generated stateof-the-art results in language modeling (Zaremba et al., 2014), along with a variety of other NLP tasks.", "startOffset": 84, "endOffset": 106}, {"referenceID": 1, "context": "Searching over all permutations Y is intractable, so we instead follow past work on linearization (Liu et al., 2015) and LSTM generation (Sutskever et al.", "startOffset": 98, "endOffset": 116}, {"referenceID": 6, "context": ", 2015) and LSTM generation (Sutskever et al., 2014) in adapting beam search for our generation step.", "startOffset": 28, "endOffset": 52}, {"referenceID": 1, "context": "In practice, the results in Liu et al. (2015) and Liu and Zhang (2015) use section 0 instead of 22 for validation (author Model WORDS WORDS+BNPS", "startOffset": 28, "endOffset": 46}, {"referenceID": 1, "context": "In practice, the results in Liu et al. (2015) and Liu and Zhang (2015) use section 0 instead of 22 for validation (author Model WORDS WORDS+BNPS", "startOffset": 28, "endOffset": 71}, {"referenceID": 3, "context": "For experiments marked GW we augment the PTB with a subset of the Annotated Gigaword corpus (Napoles et al., 2012).", "startOffset": 92, "endOffset": 114}, {"referenceID": 3, "context": "For experiments marked GW we augment the PTB with a subset of the Annotated Gigaword corpus (Napoles et al., 2012). We follow Liu and Zhang (2015) and train on a sample of 900k sentences from AFP, augmented with the PTB training set.", "startOffset": 93, "endOffset": 147}, {"referenceID": 1, "context": "We compare the models of Liu et al. (2015) (known as ZGEN), a 5-gram LM using Kneser-Ney smoothing (NGRAM), and an LSTM.", "startOffset": 25, "endOffset": 43}, {"referenceID": 7, "context": "The LSTM is based on the LSTM-Medium setup of Zaremba et al. (2014). The LSTM beam search is implemented on a GPU with batched forward operations for efficiency.", "startOffset": 46, "endOffset": 68}, {"referenceID": 4, "context": "We compare the performance of the models using the BLEU metric (Papineni et al., 2002).", "startOffset": 63, "endOffset": 86}, {"referenceID": 5, "context": "As a lightweight test, we parse the output with the Yara Parser (Rasooli and Tetreault, 2015) and compare the unlabeled attachment scores (UAS) to the trees produced by the syntactic system.", "startOffset": 64, "endOffset": 93}], "year": 2017, "abstractText": "Recent work on word ordering has argued that syntactic structure is important, or even required, for effectively recovering the order of a sentence. We find that, in fact, an n-gram language model with a simple heuristic gives strong results on this task. Furthermore, we show that a long short-term memory (LSTM) language model is comparatively effective at recovering order, with our basic model outperforming a state-of-the-art syntactic model by 11.5 BLEU points. Additional data and larger beams yield further gains, at the expense of training and search time.", "creator": "LaTeX with hyperref package"}}}