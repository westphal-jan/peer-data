{"id": "1605.00937", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2016", "title": "Dictionary Learning for Massive Matrix Factorization", "abstract": "Sparse matrix factorization is a popular tool to obtain interpretable data decompositions, which are also effective to perform data completion or denoising. Its applicability to large datasets has been addressed with online and randomized methods, that reduce the complexity in one of the matrix dimension, but not in both of them. In this paper, we tackle very large matrices in both dimensions. We propose a new factoriza-tion method that scales gracefully to terabyte-scale datasets, that could not be processed by previous algorithms in a reasonable amount of time. We demonstrate the efficiency of our approach on massive functional Magnetic Resonance Imaging (fMRI) data, and on matrix completion problems for recommender systems, where we obtain significant speed-ups compared to state-of-the art coordinate descent methods.", "histories": [["v1", "Tue, 3 May 2016 15:05:32 GMT  (1739kb,D)", "https://arxiv.org/abs/1605.00937v1", null], ["v2", "Thu, 26 May 2016 06:33:22 GMT  (1367kb,D)", "http://arxiv.org/abs/1605.00937v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG q-bio.QM", "authors": ["arthur mensch", "julien mairal", "bertrand thirion", "ga\u00ebl varoquaux"], "accepted": true, "id": "1605.00937"}, "pdf": {"name": "1605.00937.pdf", "metadata": {"source": "META", "title": "Dictionary Learning for Massive Matrix Factorization", "authors": ["Arthur Mensch", "Julien Mairal", "Bertrand Thirion", "Ga\u00ebl Varoquaux"], "emails": ["ARTHUR.MENSCH@M4X.ORG", "JULIEN.MAIRAL@INRIA.FR", "BETRAND.THIRION@INRIA.FR", "GAEL.VAROQUAUX@INRIA.FR"], "sections": [{"heading": null, "text": "In fact, most of us are in a position to put ourselves in another world, in which they go into another world, in which they find themselves in another world, in which they find themselves in another world, in which they find themselves in another world, in which they find themselves in another world, in which they live in another world, in which they find themselves in another world, in another world, in which they find themselves in another world, in another world, in another world, in which they find themselves in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they live, in which they do, in which they do, in which they live, in which they do, in which they do, in which they live, in which they do, in which they live, in which they do not exist, in which they do not exist, in which they do not exist, in which they do not exist."}, {"heading": "1. Background on Dictionary Learning", "text": "In this section, we present dictionary learning as a matrix factorization problem and present stochastic algorithms that observe a column (or minibatch) with each iteration."}, {"heading": "1.1. Problem Statement", "text": "The aim of matrix factorization is to dissect a matrix X-Rp-n - typically n signals of dimension p - as the product of two smaller matrices: X-DA with D-Rp-k, A-Rk-n, (1) with potential savings or structural requirements for D and A. In statistical signaling applications, this is often a dictionary learning problem that imposes sparse coefficients A. In such a case, we refer to D as \"dictionary\" and A as sparse codes. However, we use this terminology throughout the paper. Dictionary learning is typically done by minimizing a square, data-fitting term, with limitations and / or penalties via the code and dictionary: min D-CA = [1,...,..., n] this element Rk-nn-i = 1-Rp-1 and 2-rape."}, {"heading": "1.2. Streaming Signals with Online Algorithms", "text": "In stochastic optimization, the number of signals is assumed to be n = large (or potentially infinite), and the dictionary D can be written as a solution for min D-Cf (D), where f (D) = Ex [l (x, D)] (4) l (x >, D) = min \u03b1 Rk1 2 (x \u2212 D\u03b1 22 + 0 (\u03b1), where the signals x \u2212 clips are used as i.i.d. samples from an unknown probability distribution. Based on this formulation, Mairal et al. (2010) have introduced an online dictionary learning approach that draws a single signal in the iteration t (or a minibatch) and whose sparse code, based on the current dictionary Dt \u2212 1 according to the argmin function Rk1 (xxt), describes that the iteration in the iteration t (or a minibatch)."}, {"heading": "1.3. Handling Missing Values", "text": "The factorization of missing value matrices has aroused considerable interest in signal processing and machine learning, especially as a solution for recommendation systems. In the context of dictionary learning, Szab\u00f3 et al. (2011) made a similar attempt to adapt the framework to missing values. Formally, a mask M, represented as a binary diagonal matrix in {0, 1} p \u00b7 p, is assigned to each signal x, so that the algorithm can observe the product mtxt only when iterating t instead of a full signal text. In this context, of course, we derive the following objective rules: f \u2212 xi (D) = Ex, M \u2212 l (x, M, D)] (11) l (x, M, D) = min \u03b1 Rkp2Tr regime M (x \u2212 D\u03b1), which does not update the signal."}, {"heading": "2. Dictionary Learning for Massive Data", "text": "Using the formalism revealed above, we are now looking at the problem of factoring a large matrix X in Rp \u00b7 n into two factors D in Rp \u00b7 k and A in Rk \u00b7 n with the following setting: Both n and p are large (more than 100,000 to several million), whereas k is reasonable (less than 1,000 and often close to 100), which is not the dictionary default setting; some entries of X may be missing. In other words, we want to regain a good dictionary D with due regard to appropriate regulation. To achieve our goal, we propose to use a target similar to (11), where the masks are now random variables independent of the samples. In other words, we want to combine ideas from the online dictionary Learning with random subsampling, in a principled manner. This leads us to consider an infinite stream of samples (mtxt) \u2265, selectively selected from the sample date, i.i.i."}, {"heading": "2.1. Approximate Surrogate Function", "text": "To approximate the surrogate (8) of \u03b1t calculated in (13), we consider ht defined by ht (D) = 12 Tr (D > DCt \u2212 D > Bt) + \u03bbt \u2211 i = 1 si p (\u03b1i) (14) with the same matrix Ct as in (8), which is updated as Ct \u2190 (1 \u2212 1 t) Ct \u2212 1 + 1t \u03b1t\u03b1 > t, (15) and Bt in (8) with the matrixBt = (t \u2211 i = 1 Mi) \u2212 1 t \u2211 i = 1 Mixi\u03b1 > i, (16), which is the same as (7) if Mi = I. Since Mi is a diagonal matrix, \u0445t i = 1 Mi is also diagonal and simply counts how often a series has been seen by the algorithm. Bt therefore behaves like Ex [x\u03b1 (x, Dt) \u2212 \u2212 \u2212 as in the fully observed algorithm."}, {"heading": "2.2. Efficient Dictionary Update Rules", "text": "With a spare function in hand, we will now describe how to update the codes when only partial access to the data is possible; the complexity for calculating the sparse codes is obviously independent of p (13), which consists in solving a reduced linear regression from Mtxt to Rst \u2212 1 to Rst \u00b7 k. Therefore, we will focus here on the single iterative complexity of a factor that requires a reduction in the dimensionality of the dictionary update phase. We propose two strategies to achieve that both block coordinates descend by using the projection (.)"}, {"heading": "2.3. Discussion", "text": "Relation to the classical matrix completion formula. Our model is related to the classical \"2-penalized matrix completion model\" (e.g. Bell & Koren, 2007). We write n-i = 1-Mi (xi \u2212 D > \u03b1i)."}, {"heading": "3. Experiments", "text": "The proposed algorithm is designed to handle massive datasets: masking data allows a sequence (mtxt) t to be streamed instead of (xt) t, thereby reducing the computational complexity of individual iterations and the IO stress of a factor r = pE (Tr M), while at the same time allowing access to an accurate description of the data. Therefore, we are analyzing in detail how our algorithm improves performance for the sparse decomposition of fMRI datasets. Moreover, as it is based on data masks, our algorithm is well suited for matrix completion to reconstruct a dataset (xt) t from the masked dataset (Mtxt) t. We demonstrate the accuracy of our algorithm on explicit recommendation systems and demonstrate considerable computing speeds compared to an efficient algorithm based on coordinate descent. We are using scikit (Peglearn Dreon, a Pythal package released in 2011 and Pythal)."}, {"heading": "3.1. Sparse Matrix Factorization for fMRI", "text": "However, the data is very redundant both spatially and temporally, and the results can be enormous: we use the entire HCP dataset (Van Essen et al., 2013), with n = 2.4 \u00b7 106 (2000 datasets, 1 200 time points) and 2 \u00b7 105, a total of 2 TB of dictatorial data. We can achieve this by imposing spatial localization components with a few brain regions, by imposing the thriftiness of dictatorial learning in the dictatorial areas of the dictatorial system, which can be achieved by imposing thriftiness in the dictatorial areas: in our formalism, this is achieved with \"1-ball projection for D. We have achieved C = Bk1 and 2 history."}, {"heading": "3.2. Collaborative Filtering with Missing Data", "text": "We validate the performance of the proposed algorithm on referral systems for explicit feedback, a well-studied matrix completion problem. We evaluate the scalability of our method on datasets of different dimensions: MovieLens 1M, MovieLens 10M, and 140M ratings Netflix dataset.We compare our algorithms with a method based on coordinate descent (Yu et al., 2012) that requires state-of-the-art convergence-time performance on our largest dataset. Although stochastic downward methods for matrix factorization can offer slightly better single-run performance (Tak\u00e1cs et al., 2009), they are notoriously difficult to match and require a precise grid search for learning plans. In contrast, coordinate departure methods do not require arbitrary parameter settings and are more efficient in practice."}, {"heading": "4. Conclusion", "text": "Whether sensor data, fMRI, or e-commerce databases, sample sizes and number of features are increasing rapidly, rendering current matrix factoring approaches insoluble. We have introduced an online algorithm that uses random feature undersampling and offers up to eight times the speed and memory gains for large data. Data sets are getting bigger, and they often have more redundancies. Such approaches, where online and randomized methods merge, result in even greater speeds for next-generation data."}, {"heading": "Acknowledgements", "text": "The research that led to these results was supported by the ANR (MACARON Project, ANR-14-CE23-0003-01 - NiConnect Project, ANR-11-BINF-0004NiConnect) and was funded by the Seventh Framework Programme of the European Union (FP7 / 2007-2013) under Funding Agreement 604102 (HBP)."}], "references": [{"title": "kSVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["Aharon", "Michal", "Elad", "Michael", "Bruckstein", "Alfred"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Aharon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Aharon et al\\.", "year": 2006}, {"title": "Lessons from the Netflix prize challenge", "author": ["Bell", "Robert M", "Koren", "Yehuda"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Bell et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bell et al\\.", "year": 2007}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["Bingham", "Ella", "Mannila", "Heikki"], "venue": "In Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Bingham et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bingham et al\\.", "year": 2001}, {"title": "Convex factorization machines", "author": ["Blondel", "Mathieu", "Fujino", "Akinori", "Ueda", "Naonori"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Blondel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2015}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["Bottou", "L\u00e9on"], "venue": "In Proceedings of COMPSTAT,", "citeRegEx": "Bottou and L\u00e9on.,? \\Q2010\\E", "shortCiteRegEx": "Bottou and L\u00e9on.", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["Cand\u00e8s", "Emmanuel J", "Recht", "Benjamin"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2009}, {"title": "Near-optimal signal recovery from random projections: Universal encoding strategies", "author": ["Cand\u00e8s", "Emmanuel J", "Tao", "Terence"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "Efficient projections onto the l 1ball for learning in high dimensions", "author": ["Duchi", "John", "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Chandra", "Tushar"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Halko", "Nathan", "Martinsson", "Per-Gunnar", "Tropp", "Joel A"], "venue": "[math],", "citeRegEx": "Halko et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2009}, {"title": "Matrix completion and low-rank SVD via fast alternating least squares", "author": ["Hastie", "Trevor", "Mazumder", "Rahul", "Lee", "Jason", "Zadeh", "Reza"], "venue": "[stat],", "citeRegEx": "Hastie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2014}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["Johnson", "William B", "Lindenstrauss", "Joram"], "venue": "Contemporary mathematics,", "citeRegEx": "Johnson et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 1984}, {"title": "Stochastic majorization-minimization algorithms for large-scale optimization", "author": ["Mairal", "Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mairal and Julien.,? \\Q2013\\E", "shortCiteRegEx": "Mairal and Julien.", "year": 2013}, {"title": "Sparse Modeling for Image and Vision Processing", "author": ["Mairal", "Julien"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "Mairal and Julien.,? \\Q2014\\E", "shortCiteRegEx": "Mairal and Julien.", "year": 2014}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["Mairal", "Julien", "Bach", "Francis", "Ponce", "Jean", "Sapiro", "Guillermo"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Mairal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Analysis of fMRI Data by Blind Separation into Independent Spatial Components", "author": ["M.J. McKeown", "S. Makeig", "G.G. Brown", "T.P. Jung", "S.S. Kindermann", "A.J. Bell", "T.J. Sejnowski"], "venue": "Human Brain Mapping,", "citeRegEx": "McKeown et al\\.,? \\Q1998\\E", "shortCiteRegEx": "McKeown et al\\.", "year": 1998}, {"title": "Scikit-learn: machine learning in Python", "author": ["Matthieu", "Duchesnay", "\u00c9douard"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Matthieu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Matthieu et al\\.", "year": 2011}, {"title": "Efficient dictionary learning via very sparse random projections", "author": ["Pourkamali-Anaraki", "Farhad", "Becker", "Stephen", "Hughes", "Shannon M"], "venue": "In Proceedings of the IEEE International Conference on Sampling Theory and Applications,", "citeRegEx": "Pourkamali.Anaraki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pourkamali.Anaraki et al\\.", "year": 2015}, {"title": "Factorization machines", "author": ["Rendle", "Steffen"], "venue": "In Proceedings of the IEEE International Conference on Data Mining,", "citeRegEx": "Rendle and Steffen.,? \\Q2010\\E", "shortCiteRegEx": "Rendle and Steffen.", "year": 2010}, {"title": "Onlineupdating regularized kernel matrix factorization models for large-scale recommender systems", "author": ["Rendle", "Steffen", "Schmidt-Thieme", "Lars"], "venue": "In Proceedings of the ACM Conference on Recommender systems,", "citeRegEx": "Rendle et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rendle et al\\.", "year": 2008}, {"title": "Maximum-margin matrix factorization", "author": ["Srebro", "Nathan", "Rennie", "Jason", "Jaakkola", "Tommi S"], "venue": "In Advances in Neural Information Processing Systems, pp", "citeRegEx": "Srebro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2004}, {"title": "Online group-structured dictionary learning", "author": ["Szab\u00f3", "Zolt\u00e1n", "P\u00f3czos", "Barnab\u00e1s", "Lorincz", "Andr\u00e1s"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szab\u00f3 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Szab\u00f3 et al\\.", "year": 2011}, {"title": "Scalable collaborative filtering approaches for large recommender systems", "author": ["Tak\u00e1cs", "G\u00e1bor", "Pil\u00e1szy", "Istv\u00e1n", "N\u00e9meth", "Botty\u00e1n", "Tikk", "Domonkos"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Tak\u00e1cs et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tak\u00e1cs et al\\.", "year": 2009}, {"title": "The WU-Minn Human Connectome Project: An overview", "author": ["Van Essen", "David C", "Smith", "Stephen M", "Barch", "Deanna M", "Behrens", "Timothy E. J", "Yacoub", "Essa", "Ugurbil", "Kamil"], "venue": null, "citeRegEx": "Essen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Essen et al\\.", "year": 2013}, {"title": "Cohort-level brain mapping: learning cognitive atoms to single out specialized regions", "author": ["Varoquaux", "Ga\u00ebl", "Schwartz", "Yannick", "Pinel", "Philippe", "Thirion", "Bertrand"], "venue": "In Proceedings of the Information Processing in Medical Imaging Conference,", "citeRegEx": "Varoquaux et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Varoquaux et al\\.", "year": 2013}, {"title": "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems", "author": ["Yu", "Hsiang-Fu", "Hsieh", "Cho-Jui", "Dhillon", "Inderjit"], "venue": "In Proceedings of the International Conference on Data Mining,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 19, "context": "in recommender systems (Srebro et al., 2004; Cand\u00e8s & Recht, 2009).", "startOffset": 23, "endOffset": 66}, {"referenceID": 19, "context": "The first one addresses an optimization problem involving a convex penalty, such as the trace or max norms (Srebro et al., 2004).", "startOffset": 107, "endOffset": 128}, {"referenceID": 13, "context": ", 2015, and references therein), and stochastic majorization-minimization methods for dictionary learning with sparse and/or structured regularization (Mairal et al., 2010; Mairal, 2013).", "startOffset": 151, "endOffset": 186}, {"referenceID": 14, "context": "Biological datasets (McKeown et al., 1998) and physical acquisitions with an underlying sparse structure enabling compressed sensing (Cand\u00e8s & Tao, 2006) are good examples.", "startOffset": 20, "endOffset": 42}, {"referenceID": 8, "context": "Recently, those have been used to compute PCA (Halko et al., 2009), a classical matrix decomposition technique.", "startOffset": 46, "endOffset": 66}, {"referenceID": 13, "context": "As such, it is non-trivial extension of the online dictionary learning method of Mairal et al. (2010), where, at every iteration, signals are partially observed with a random mask, and with low-complexity update rules that depend on the (small) mask size instead of the signal size.", "startOffset": 81, "endOffset": 102}, {"referenceID": 0, "context": "(2015) use random projection with k-SVD, a batch dictionary learning algorithm (Aharon et al., 2006) that does not scale well in the number of training signals.", "startOffset": 79, "endOffset": 100}, {"referenceID": 14, "context": "For instance, Pourkamali-Anaraki et al. (2015) use random projection with k-SVD, a batch dictionary learning algorithm (Aharon et al.", "startOffset": 14, "endOffset": 47}, {"referenceID": 0, "context": "(2015) use random projection with k-SVD, a batch dictionary learning algorithm (Aharon et al., 2006) that does not scale well in the number of training signals. Online matrix decomposition in the context of missing values was also proposed by Szab\u00f3 et al. (2011), but without scalability in the signal (row) size.", "startOffset": 80, "endOffset": 263}, {"referenceID": 0, "context": "(2015) use random projection with k-SVD, a batch dictionary learning algorithm (Aharon et al., 2006) that does not scale well in the number of training signals. Online matrix decomposition in the context of missing values was also proposed by Szab\u00f3 et al. (2011), but without scalability in the signal (row) size. On a massive fMRI dataset (2TB, n = 2.4\u00b7106, p = 2\u00b7105), we were able to learn interpretable dictionaries in about 10 hours on a single workstation, an order of magnitude faster than the online approach of Mairal et al. (2010). On collaborative filtering experiments, where sparsity is not needed, our algorithm performs favorably well compared to state-of-the-art coordinate descent methods.", "startOffset": 80, "endOffset": 541}, {"referenceID": 9, "context": "In large n and large p settings, typical in recommender systems, this problem is solved via block coordinate descent, which boils down to alternating least squares if regularizations on D and \u03b1 are quadratic (Hastie et al., 2014).", "startOffset": 208, "endOffset": 229}, {"referenceID": 13, "context": "lation, Mairal et al. (2010) have introduced an online dictionary learning approach that draws a single signal xt at iteration t (or a minibatch), and computes its sparse code \u03b1t using the current dictionary Dt\u22121 according to", "startOffset": 8, "endOffset": 29}, {"referenceID": 20, "context": "In the context of dictionary learning, a similar effort has been made by Szab\u00f3 et al. (2011) to adapt the framework to missing values.", "startOffset": 73, "endOffset": 93}, {"referenceID": 13, "context": "Adapting the online algorithm of Mairal et al. (2010) would consist of drawing a sequence of pairs (xt,Mt), and building the surrogate", "startOffset": 33, "endOffset": 54}, {"referenceID": 20, "context": "This is the approach chosen by Szab\u00f3 et al. (2011). Nevertheless, the complexity of their update rules is linear in the full signal size p, which makes them unadapted to the large-p regime that we consider.", "startOffset": 31, "endOffset": 51}, {"referenceID": 13, "context": "Following Mairal et al. (2010), we perform one cycle over the columns warm-started on Dt\u22121.", "startOffset": 10, "endOffset": 31}, {"referenceID": 7, "context": ")\u22641[fj ], and a classical result (see Duchi et al., 2008) states that there exists a scalar lj such that dj = Slj [fj ], S\u03bb(u) = sign(u).max(|u| \u2212 \u03bb, 0) (22) where S\u03bb is the soft-thresholding operator, applied elementwise to the entries of fj . Similar to the case `2, the \u201clazy\u201d projection consists of tracking the coefficient lj for each dictionary element and updating it after each gradient step, which only involves st coefficients. For such sparse updates followed by a projection onto the `1-ball, Duchi et al. (2008) proposed an algorithm to find the threshold lj in O(st log(p)) operations.", "startOffset": 38, "endOffset": 525}, {"referenceID": 19, "context": "Srebro et al. (2004) introduced the trace-norm regularization to solve a convex problem equivalent to (23).", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "Matrix factorization has long been used on functional Magnetic Resonance Imaging (McKeown et al., 1998).", "startOffset": 81, "endOffset": 103}, {"referenceID": 23, "context": "Historically, such decomposition have been obtained with the classical dictionary learning objective on transposed data (Varoquaux et al., 2013): the code A holds sparse spatial maps and voxel time-series are streamed.", "startOffset": 120, "endOffset": 144}, {"referenceID": 24, "context": "We compare our algorithm to a coordinate-descent based method (Yu et al., 2012), that provides state-of-the art convergence time performance on our largest dataset.", "startOffset": 62, "endOffset": 79}, {"referenceID": 21, "context": "Although stochastic gradient descent methods for matrix factorization can provide slightly better single-run performance (Tak\u00e1cs et al., 2009), these are notoriously hard to tune and require a precise grid search to uncover a working schedule of learning rates.", "startOffset": 121, "endOffset": 142}, {"referenceID": 9, "context": "on train data following Hastie et al. (2014) (alternated debiasing).", "startOffset": 24, "endOffset": 45}], "year": 2016, "abstractText": "Sparse matrix factorization is a popular tool to obtain interpretable data decompositions, which are also effective to perform data completion or denoising. Its applicability to large datasets has been addressed with online and randomized methods, that reduce the complexity in one of the matrix dimension, but not in both of them. In this paper, we tackle very large matrices in both dimensions. We propose a new factorization method that scales gracefully to terabyte-scale datasets. Those could not be processed by previous algorithms in a reasonable amount of time. We demonstrate the efficiency of our approach on massive functional Magnetic Resonance Imaging (fMRI) data, and on matrix completion problems for recommender systems, where we obtain significant speed-ups compared to state-of-the art coordinate descent methods. Matrix factorization is a flexible tool for uncovering latent factors in low-rank or sparse models. For instance, building on low-rank structure, it has proven very powerful for matrix completion, e.g. in recommender systems (Srebro et al., 2004; Cand\u00e8s & Recht, 2009). In signal processing and computer vision, matrix factorization with a sparse regularization is often called dictionary learning and has proven very effective for denoising and visual feature encoding (see Mairal, 2014, for a review). It is also flexible enough to accommodate a large set of constraints and regularizations, and has gained significant attention in scientific domains where interpretability is a key aspect, such as geProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s). netics and neuroscience (Varoquaux et al., 2011). As a widely-used model, the literature of matrix factorization is very rich and two main classes of formulations have emerged. The first one addresses an optimization problem involving a convex penalty, such as the trace or max norms (Srebro et al., 2004). These penalties promote lowrank structures, have strong theoretical guarantees (Cand\u00e8s & Recht, 2009), but they do not encourage sparse factors and lack scalability for very-large datasets. For these reasons, our paper is focused on a second type of approach, that relies on nonconvex optimization. Specifically, the motivation of our work originally came from the need to analyze huge-scale fMRI datasets, and the difficulty of current algorithms to process them. To gain scalability, stochastic (or online) optimization methods have been developed; unlike classical alternate minimization procedures, they learn matrix decompositions by observing a single matrix column (or row) at each iteration. In other words, they stream data along one matrix dimension. Their cost per iteration is significantly reduced, leading to faster convergence in various practical contexts. More precisely, two approaches have been particularly successful: stochastic gradient descent (see Bottou, 2010) has been widely used in recommender systems (see Bell & Koren, 2007; Rendle & Schmidt-Thieme, 2008; Rendle, 2010; Blondel et al., 2015, and references therein), and stochastic majorization-minimization methods for dictionary learning with sparse and/or structured regularization (Mairal et al., 2010; Mairal, 2013). Yet, stochastic algorithms for dictionary learning are currently unable to deal efficiently with matrices that are large in both dimensions. In a somehow orthogonal way, the growth of dataset size has proven to be manageable by randomized methods, that exploit random projections (Johnson & Lindenstrauss, 1984; Bingham & Mannila, 2001) to reduce data dimension ar X iv :1 60 5. 00 93 7v 2 [ st at .M L ] 2 6 M ay 2 01 6 Dictionary Learning for Massive Matrix Factorization without deteriorating signal content. Due to the way they are generated, large-scale datasets generally have an intrinsic dimension that is significantly smaller than their ambient dimension. Biological datasets (McKeown et al., 1998) and physical acquisitions with an underlying sparse structure enabling compressed sensing (Cand\u00e8s & Tao, 2006) are good examples. In this context, matrix factorization can be performed by using random summaries of coefficients. Recently, those have been used to compute PCA (Halko et al., 2009), a classical matrix decomposition technique. Yet, using random projections as a pre-processing step is not appealing in our applicative context since the factors learned on reduced data loses interpretability. Main contribution. In this paper, we propose a dictionary learning algorithm that (i) scales both in the signal dimension (number of rows) and number of signals (number of columns), (ii) deals with various structured sparse regularization penalties, (iii) handles missing values, and (iv) provides an explicit dictionary with easy interpretation. As such, it is non-trivial extension of the online dictionary learning method of Mairal et al. (2010), where, at every iteration, signals are partially observed with a random mask, and with low-complexity update rules that depend on the (small) mask size instead of the signal size. To the best of our knowledge, our algorithm is the first that enjoys all aforementioned features; in particular, we are not aware of any other dictionary learning algorithm that is scalable in both matrix dimensions. For instance, Pourkamali-Anaraki et al. (2015) use random projection with k-SVD, a batch dictionary learning algorithm (Aharon et al., 2006) that does not scale well in the number of training signals. Online matrix decomposition in the context of missing values was also proposed by Szab\u00f3 et al. (2011), but without scalability in the signal (row) size. On a massive fMRI dataset (2TB, n = 2.4\u00b7106, p = 2\u00b7105), we were able to learn interpretable dictionaries in about 10 hours on a single workstation, an order of magnitude faster than the online approach of Mairal et al. (2010). On collaborative filtering experiments, where sparsity is not needed, our algorithm performs favorably well compared to state-of-the-art coordinate descent methods. In both experiments, benefits for the practitioner were significant. 1. Background on Dictionary Learning In this section, we introduce dictionary learning as a matrix factorization problem, and present stochastic algorithms that observe one column (or a minibatch) at every iteration. 1.1. Problem Statement The goal of matrix factorization is to decompose a matrix X \u2208 Rp\u00d7n \u2013 typically n signals of dimension p \u2013 as a product of two smaller matrices: X \u2248 DA with D \u2208 Rp\u00d7k, A \u2208 Rk\u00d7n, (1) with potential sparsity or structure requirements on D and A. In statistical signal applications, this is often a dictionary learning problem, enforcing sparse coefficients A. In such a case, we call D the \u201cdictionary\u201d and A the sparse codes. We use this terminology throughout the paper. Learning the dictionary is typically performed by minimizing a quadratic data-fitting term, with constraints and/or penalties over the code and the dictionary: min D\u2208C A=[\u03b11,...,\u03b1n]\u2208R n \u2211 i=1 1 2 \u2225\u2225xi \u2212D\u03b1i\u2225\u222522 + \u03bb\u03a9(\u03b1i), (2) where C is a convex set of Rp\u00d7k, and a \u03a9 : R \u2192 R is a penalty over the code, to enforce structure or sparsity. In large n and large p settings, typical in recommender systems, this problem is solved via block coordinate descent, which boils down to alternating least squares if regularizations on D and \u03b1 are quadratic (Hastie et al., 2014). Constraints and penalties. The constraint set C is traditionally a technical constraint ensuring that the coefficients \u03b1 do not vanish, making the effect of the penalty \u03a9 disappear. However, other constraints can also be used to enforce sparsity or structure on the dictionary (see Varoquaux et al., 2013). In our paper,C is the Cartesian product of a `1 or `2 norm ball: C = {D \u2208 Rp\u00d7k s.t. \u03c8(dj) \u2264 1 \u2200j = 1, . . . , k}, (3) where D = [d1, . . . ,dk] and \u03c8 = \u2016 \u00b7 \u20161 or \u03c8 = \u2016 \u00b7 \u20162. The choice of \u03c8 and \u03a9 typically offers some flexibility in the regularization effect that is desired for a specific problem; for instance, classical dictionary learning uses \u03c8 = \u2016 \u00b7 \u20162 and \u03a9 = \u2016\u00b7\u20161, leading to sparse coefficients \u03b1, whereas our experiments on fMRI uses \u03c8 = \u2016\u00b7\u20161 and \u03a9 = \u2016\u00b7\u20162, leading to sparse dictionary elements dj that can be interpreted as brain activation maps. 1.2. Streaming Signals with Online Algorithms In stochastic optimization, the number of signals n is assumed to be large (or potentially infinite), and the dictionary D can be written as a solution of min D\u2208C f(D) where f(D) = Ex [ l(x,D) ] (4) l(x,D) = min \u03b1\u2208Rk 1 2 \u2016x\u2212D\u03b1\u20162 + \u03bb\u03a9(\u03b1), where the signals x are assumed to be i.i.d. samples from an unknown probability distribution. Based on this formuDictionary Learning for Massive Matrix Factorization lation, Mairal et al. (2010) have introduced an online dictionary learning approach that draws a single signal xt at iteration t (or a minibatch), and computes its sparse code \u03b1t using the current dictionary Dt\u22121 according to \u03b1t \u2190 argmin \u03b1\u2208Rk 1 2 \u2016xt \u2212Dt\u22121\u03b1\u20162 + \u03bb\u03a9(\u03b1). (5) Then, the dictionary is updated by approximately minimizing the following surrogate function", "creator": "LaTeX with hyperref package"}}}