{"id": "1605.01636", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2016", "title": "Maximal Sparsity with Deep Networks?", "abstract": "The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer. Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights. It is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available. While the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy. In particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse representations. In contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal $\\ell_0$-norm representations in regimes where existing methods fail. The resulting system is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3D scene.", "histories": [["v1", "Thu, 5 May 2016 15:58:55 GMT  (1206kb,D)", "http://arxiv.org/abs/1605.01636v1", null], ["v2", "Tue, 10 May 2016 05:38:46 GMT  (1296kb,D)", "http://arxiv.org/abs/1605.01636v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bo xin", "yizhou wang 0001", "wen gao 0001", "david p wipf", "baoyuan wang"], "accepted": true, "id": "1605.01636"}, "pdf": {"name": "1605.01636.pdf", "metadata": {"source": "CRF", "title": "Maximal Sparsity with Deep Networks?", "authors": ["Bo Xin", "Yizhou Wang", "Wen Gao", "David Wipf"], "emails": ["boxin@pku.edu.cn", "yizhou.wang@pku.edu.cn", "wgao@pku.edu.cn", "davidwipf@gmail.com"], "sections": [{"heading": null, "text": "Keywords: sparse estimation, compressed scanning, deep deployment, deep networks, restricted isometric property (RIP)"}, {"heading": "1. Introduction", "text": "It is not as if the search for a maximum frugal vector x-plane can be presented in such a way that it is represented with the Fewest numberar Xiv: 160 5.01 636v 1 [cs.L] 5of features in the realizable region.1 Unfortunately, however, the direct attack on (1) is intolerable combinatorial optimization processes and therefore efficient alternatives that most likely return a maximum frugal x-plane in restricted regimes."}, {"heading": "1.1 Paper Overview", "text": "In Section 2, we begin by testing the iterative hard threshold algorithm (IHT) for estimating a sparse vector. IHT was chosen because it can be deployed directly for learning purposes, is representative of many sparse estimation paradigms, and is accessible to theoretical analysis. Next, we discuss the limitations of IHT, including its high sensitivity to correlated designs, and motivate a DNN-like unfolded alternative. Later, Section 3 looks at this unfolded IHT network with common layered weights and activations that are the standard template for existing methods. We explicitly quantify the degree to which such networks can compensate for corrections, exposing the breakpoint at which a possible common weight construction is likely to fail."}, {"heading": "1.2 Summary of Contributions", "text": "Our technical and empirical contributions can be distilled into the following points: \u2022 We must rigorously dissect the benefits of deploying conventional, frugal estimation algorithms to generate traceable deep networks, including a precise characterization of how different architectural decisions can affect the ability to improve effective, restrictive isometric constants that measure the degree of disruptive correlation contained in a dictionary, helping to quantify the limits of common layer weights and motivate more flexible network constructions that take into account the multi-resolution structure in a previously unexplored way. Importantly, we must imagine that our analyses represent important factors present in other DNN-related domains. \u2022 Based on these theoretical insights and a better understanding of the key factors that determine performance, we determine the degree to which it is favorable to deviate from strict conformity to a particular unfolded algorithm scription."}, {"heading": "2. From Iterative Hard Thesholding (IHT) to Deep Neural Networks", "text": "In this section, IHT is presented first, before the DNN analogue is presented in detail."}, {"heading": "2.1 Introduction to IHT", "text": "By knowing an upper limit of true cardinality, the solution (1) can be replaced by the equivalence problem (2) (2). (2) In addition, the solution (2) can be minimized by what can be regarded as mathematically efficient projected gradient iterations (Blumensath and Davies, 2009). Letx (t) refers to the estimation of some maximally frugal x-values by t-iterations. IHT first calculates the gradient of the square object measured on x (t), where x (t) = x = x-value (t) \u2212 p > y. (3) We then take the uncontrolled gradient step (t + 1) = x (t)."}, {"heading": "2.2 Unfolding IHT Iterations", "text": "The success of IHT in restoring maximum sparse solutions depends crucially on the RIP-based condition that \u03b43k [\u03a6] < 1 / \u221a 32, which severely limits the degree of correlation structure in \u03a6 that can be tolerated. While dictionaries with columns drawn independently and uniformly from the surface of a hypersphere are highly likely to meet this condition (Cande and Tao, 2005), we cannot rely on this kind of IHT restoration guarantee for many / most practical problems of interest. In fact, except for randomized dictionaries in large dimensions where tight boundaries exist, we cannot even calculate the value of 3k, which requires the calculation of the spectral norm of (m 3k) subsets of dictionary columns. There are many ways that nature could structure a dictionary so that IHT (or most other existing sparse estimative algorithms) could fail."}, {"heading": "3. Analysis using Shared Layer-Wise Weights and Activations", "text": "For simplicity in this section, we limit ourselves to setting a fixed threshold that needs to be set across all levels; however, many of the conclusions that have emerged from our analysis are applicable to a much broader range of activation functions. Generally, it is difficult to analyze how arbitrary this is and how the results affect the defined parameterization of (5), whereas the elements of the \"2 column norm\" of n (0, 1 / 2) and the resulting columns are uniformed. Moreover, any \"2 column norm is converted to such that normalization is not even possible."}, {"heading": "4. Analysis using Layer-Wise Independent Weights and Activations", "text": "In the previous section we observed how the following formalDefinition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition.Definition..Definition..Definition..Definition..Definition..Definition..Definition..Definition...Definition...Definition...Definition....Definition...Definition....Definition.....Definition.....Definition.....Definition.......Definition..........Definition........Definition.......Definition...Definition..........Definition.........Definition....Definition................Definition...........Definition...........Definition.......................Definition................Definition....................................................."}, {"heading": "5. Discriminative Multi-Resolution Sparse Estimation", "text": "In fact, it is not as if it could be a way of locating itself on two different levels, with a cluster-level distribution mechanism that limits the structure of the dictatorial system to a single universal level (fine), but in reality it may be that the structure of the dictatorial system is distributed on a multitude of levels that are supported on two different levels. If the yardsticks are clearly defined, we have seen that it is possible to define a multi-resolution (fine), but in practice it is possible to display the structure of a multitude of structures that extend over a multitude of levels that form a continuity between several levels. If the yardsticks are clearly defined, we have seen that it is possible to define a multi-resolution IHT algorithm that guarantees success in restoring the optimal support pattern; and indeed it is."}, {"heading": "6. Construction of Training Sets", "text": "If the ultimate goal is to learn an accurate model for calculating the minimum \"0 standard,\" even though the maximum number of data collected is high, then one must be careful how we construct the training data. This problem is particularly acute in the operational regime considered here, namely, sparsely linear inverse problems, where the dictionary coherence is high. As a graphic example of this point, let us assume that we have a dictionary that provides very compact representations of a signal class of interest Y. Even if we have access to a large number of observations y Y, our work is still ahead of us to construct a workable training environment. This is because, in general, calculating the maximum sparse x values correlated to each y is a NP-hard problem, and if the dictionary represents coherent fast, safe systems such as OMP and IHT will fail."}, {"heading": "7. Feedforward Network Experiments", "text": "With existing sparse optimization algorithms, the goal is to estimate x * when presented with y and \u03a6. In contrast, we intend to use a large corpus of training pairs {x *, y} to learn a mapping from y to x * using a deep architecture. To isolate the various factors that affect the performance of feedback networks in particular, this section describes experiments with a variety of different methods of data generation. Later, Section 9 will consider a competing, recurring LSTM architecture."}, {"heading": "7.1 Network Design", "text": "In short, we are building a 20-layer network with residual connections (He et al., 2015a) and batch normalization (Ioffe and Szegedy, 2015). Since our sparse data will ultimately have no indication of local smoothness, we are using fully connected layers instead of coils. For nonlinearity, we are using rectilinear units (ReLU). We also include a final Softmax layer that outputs a vector p-Rm, where pj-Rm [0, 1] provides an estimate of the probability that xj is 6 = 0. The detailed network structure implemented by MXNet (Chen et al., 2015) can be found in Figure 1."}, {"heading": "7.2 Basic Sparse Estimation Experimental Setup", "text": ", \"(), (), (), (), (), (), (,\" (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (, (), (), (), (), (), (, (), (), (, (), (), (, (), (), (), (, (), (), (, (), (), (), (), (), (, (), (), (), (), (, (), (), (), (, (), (), (), (), (, (), (), (), (, (), (), (), (), (, (), (, (), (), (), (, (), (), (, (), (), (, (), (, (), (), (), (), (, (), (, (), (), (, (), (), (), (, (, (), (), (), (, (, (), (, (), (, (), (), (, (, (), (), (), (, (), (, (), (), (, (), (), (, (), (, (), (), (), (, (), (), (), (, (), (), (), (, ("}, {"heading": "7.2.1 Accuracy Results", "text": "Figure 2 illustrates how different methods perform under consideration of both evaluation variables. In view of the correlated \u03a6 matrix, the recovery performance of IHT and to a lesser extent \"l minimization7, we calculate spred (d) using the largest (in magnitude) n elements of all estimates x. 8. http: / / www.eecs.berkeley.edu / yang / software / l1benchmark / index.htmlusing ISTA is rather modest given that the associated RIP constant will be quite large in design terms. In contrast, our method achieves a uniformly higher accuracy between both metrics, including existing learning-based methods trained with the same data. This improvement is likely the result of three significant factors: (i) initialize existing learning methods by using weights derived from the original sparse estimation algorithms, but such initialization is based on locally based solutions that are directly related to the problem structure (i.e., not based on the description of the differential differentiation structure)."}, {"heading": "7.2.2 Computational Efficiency", "text": "Not surprisingly, the learning-based methods have a dramatic advantage over ISTA-based \"1 minimization\" and IHT, which both require a high number of iterations to approximate each other. By contrast, ISTA-Net, IHT-Net, and our method require only a handful of shifts. Although these learning-based approaches all require a potentially costly familiarization phase, we only need to adjust the network model once in advance for each task that is of interest, and then the subsequent test / deployment will always be much more efficient."}, {"heading": "7.3 Variants of the Basic Experiment", "text": "Here we vary a number of different factors from the basic experiment, but in any case we consider all others fixed."}, {"heading": "7.3.1 Varying training set size", "text": "In Figure 3, we see that adding more training data to our method can further increase accuracy, which is no surprise as it is essentially based on learning, so as long as the capacity of the network allows it and the optimization ends in a good pelvis, we can expect some improvement with additional data."}, {"heading": "7.3.2 Alternative network structures", "text": "In this section, we examine various network architectures to quantify key factors influencing performance: 1. We remove the residual network connections. 2. We replace ReLU with activations with hard thresholds. Specifically, we use the so-called HELU\u03c3 function introduced in (Wang et al., 2015), which represents a continuous and piecemeal linear approach of the scalar operator with hard thresholds, which is used by HELU\u03c3 (x) = 0 when using HELU\u03c3 (x) = p p p p > p p > p > p p > p > p > p > p > p > p > p (x \u2212 1 + \u03c3) when using 1 \u2212 1 < x < x < x < x > p > p > p > p > p > p > p > p p > p p > p p > p p > p p > p p > p > p; p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > lt; p > p > p > p > p > p > lt; p > p > p < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p."}, {"heading": "7.3.3 Different distributions for x\u2217", "text": "From a practical point of view, we would like to estimate the support pattern of all essential elements of x *, but these elements do not all have to have the same amplitudes. Furthermore, in practice, we can expect that the true amplitude distribution sometimes deviates from the original training set. To investigate the robustness of such incongruities and different amplitude distributions, we consider two sets of candidate data: the original from Section 7.2 and similarly generated data, but with the even distribution of non-zero elements replaced by the Gaussian N (\u00b1 0.3, 0.1), selecting the mean with equal probability of \u2212 0.3 or 0.3, thus avoiding tiny quantities with high probability. Figure 5 reports accuracy among different distributions for both training and testing, including non-matching cases. The designation \"U2U\" refers to training and testing with the uniformly distributed amplitudes described in Section 7.2, while \"Test Set 2N\" is a training set and N2N."}, {"heading": "8. Practical Application: Photometric Stereo", "text": "Photometric stereo is a powerful technique for restoring high-resolution surface norms from a 3D scene using variations in appearance in 2D images under different illuminations. For example, when images of an ideal Lambertian surface are taken under illumination from three known directions, surface orientation can be clearly determined using a simple, least square fit (Woodham, 1980). In practice, however, the estimation process is often disrupted by non-Lambertian effects such as spectral lights, shadows, or image noise. To take these external factors into account, robust estimation methods have been proposed that break down an observation matrix of stacked images under different lighting conditions into an ideal Lambertian component and a sparse concept of error (Wu et al., 2010; Ikehata et al., 2012). While this approach is theoretically applicable, it requires solutions of the order of 104 \u2212 106 clear surface problems that we would like to have a ressionary estimate for."}, {"heading": "8.1 Problem Details", "text": "Suppose we had q observations of a given surface point from a Lambertian scene under different illumination directions. Then, the resulting measurements designated by o > Rq can be expressed aso = \u03c1Ln, (22) where n > R3 denotes the true surface norm, each row of L > Rq \u00b7 3 defines an illumination direction, and \u03c1 is the diffuse albedo that acts here as a scalar multiplier (Woodham, 1980). If spectral highlights, shadows, or other coarse outliers are present, then the observations are more realistically modelled aso = \u03c1Ln + e, (23) where e is an unknown sparse vector (Wu et al., 2010; Ikehata et al., 2012). In this revised scenario, we could consider n usingmin n, e, e, null, o = zero, where n, mmgr is only the surface orm."}, {"heading": "8.2 Results", "text": "In fact, we have to be able to hide, and we have to be able to hide, \"he said.\" We have to be able to hide, \"he said.\" We have to be able to hide. We have to be able to hide, \"he said.\" We have to be able to hide, \"he said.\" We have to be able to be able to hide, \"he said.\" We have to be able to hide, \"he said."}, {"heading": "9. Alternative LSTM Networks", "text": "In this section, we turn to a recursive LSTM structure and perform some preliminary evaluations. As a high-level motivation, there are many similarities between deployed sparse estimation algorithms, such as the adaptive IHT discussed in Section 4, and an unfolded LSTM network. Both provide an input y for each unfolded layer, and both implicitly use gating functions to turn activations on or off during learning, so that partial support patterns or other information can be stored in deeper layers. Although we will move a much more detailed, self-contained exploration into a future paper, we nevertheless present an initial empirical proof concept with a vanilla form of the LSTM network that is not explicitly tailored to sparse estimation problems beyond the final multi-level classification layer."}, {"heading": "10. Conclusions", "text": "There is a clear relationship between iterative optimization rules that promote thrift (such as those of IHT) and the layers of deep networks. Building on this perspective, we have shown that deep networks with handcrafted multi-resolution structures can be proven to solve certain classes of sparse restoration problems where existing algorithms fail. Similar to CNN-based functions, SIFT can often outperform many computer vision tasks, we argue that a discriminatory approach can outperform the manual structuring of 10. In fact, the runtime of our method is only twice as long as that of Rnd4, which uses a single, low-dimensional minimum of squares. Layers / iterations and compensatory measures for dictionary coherence under more general conditions. We also believe that many of the underlying principles being studied here are also applied to other applications that operate on the boundary between optimization and learning-based systems."}, {"heading": "10.1 Proof of Proposition 2", "text": "Consider some x-factors where x-1 = 1 and x-1 = i = for i-1, with the caveat that x-1 = 1 + O () and x-1 + O () = 1 + O (27) and therefore x-1 + 1 + 1 + O () must match. (28) To ensure that x-1 = Hk (3 x-1 + 0), the largest k elements of z, 1 + 1 + 1 + O () must be matched with {1, 1 + O () and zi = x-i for all i-1. Together, these conditions are necessary to ensure that the Hk operator produces x-1, but they also imply that z = e1 + O (), where e1 is a zero vector with a \"1\" in the first position, since no element in the addition of {1, 2} can be larger than this (4)."}, {"heading": "10.2 Proof of Proposition 3", "text": "(30) Let's first of all assume that W is invertable. Then, the iteration (30) is consistent with the modified lens emin x1 2 \"Wy - Wi - Wi - Dx 2 2\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"(31) Because the iteration (30) with y = x\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s"}, {"heading": "10.3 Proof of Corollary 4", "text": "Consider some projection operators P = W > W to zero [\u2206 > r], where W is formed with orthonormal rows projected equally onto the orthogonal complement of range [\u2206 r], followed by W\u03a6D = W [A + \u0445 r] ND = W A (33), if D = (N) \u2212 1. Since elements of A iid are drawn from N (0, 1 / \u221a n) and W has orthonormal rows, elements of WA-R (n \u2212 r) \u00b7 m also exhibit iid elements from the same distribution, so the specified selections for W and D allow us to obtain the (as expected) worst possible upper limit in the sequence."}, {"heading": "10.4 Proof of Proposition 7", "text": "Let us use the set of column indices of \"1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 2 = 1 = 1 = 2 = 1 = 1 = 1 = 2 = 1 = 1 = 2 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 2 = 2 = 2 = 2 = 2 = 1 = 2 = 2 = 1 = 2 = 1 = 2 = 2 = 2 = 2 = 2 = 1 = 2 = 3 = 3 + 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 3 + 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 3 + 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 3 + 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 + 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2"}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["S. Amari"], "venue": "Neural Computation,", "citeRegEx": "Amari.,? \\Q1998\\E", "shortCiteRegEx": "Amari.", "year": 1998}, {"title": "Improved bounds on the restricted isometry constants for gaussian matrices", "author": ["B. Bah", "J. Tanner"], "venue": "SIAM J. Matrix Analysis Applications,", "citeRegEx": "Bah and Tanner.,? \\Q2010\\E", "shortCiteRegEx": "Bah and Tanner.", "year": 2010}, {"title": "Electromagnetic brain mapping", "author": ["S. Baillet", "J.C. Mosher", "R.M. Leahy"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Baillet et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Baillet et al\\.", "year": 2001}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sciences,", "citeRegEx": "Beck and Teboulle.,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2009}, {"title": "Iterative thresholding for sparse approximations", "author": ["T. Blumensath", "M.E. Davies"], "venue": "J. Fourier Analysis and Applications,", "citeRegEx": "Blumensath and Davies.,? \\Q2008\\E", "shortCiteRegEx": "Blumensath and Davies.", "year": 2008}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Blumensath and Davies.,? \\Q2009\\E", "shortCiteRegEx": "Blumensath and Davies.", "year": 2009}, {"title": "Normalized iterative hard thresholding: Guaranteed stability and performance", "author": ["T. Blumensath", "M.E. Davies"], "venue": "IEEE J. Selected Topics Signal Processing,", "citeRegEx": "Blumensath and Davies.,? \\Q2010\\E", "shortCiteRegEx": "Blumensath and Davies.", "year": 2010}, {"title": "Decoding by linear programming", "author": ["E. Cand\u00e8s", "T. Tao"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2005\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2005}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "Iterative hard thresholding for compressed sensing with partially known support", "author": ["R.E. Carrillo", "L.F. Polania", "K.E. Barner"], "venue": "International Conference on Accoustics, Speech, and Signal Processing,", "citeRegEx": "Carrillo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Carrillo et al\\.", "year": 2011}, {"title": "MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["T. Chen", "M. Li", "Y. Li", "M. Lin", "N. Wang", "M. Wang", "T. Xiao", "B. Xu", "C. Zhang", "Z. Zhang"], "venue": "arXiv preprint arXiv:1512.01274,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Sparse channel estimation via matching pursuit with application to equalization", "author": ["S.F. Cotter", "B.D. Rao"], "venue": "IEEE Trans. on Communications,", "citeRegEx": "Cotter and Rao.,? \\Q2002\\E", "shortCiteRegEx": "Cotter and Rao.", "year": 2002}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Donoho.,? \\Q2006\\E", "shortCiteRegEx": "Donoho.", "year": 2006}, {"title": "Optimally sparse representation in general (nonorthogonal) dictionaries via `1 minimization", "author": ["D.L. Donoho", "M. Elad"], "venue": "Proc. National Academy of Sciences,", "citeRegEx": "Donoho and Elad.,? \\Q2003\\E", "shortCiteRegEx": "Donoho and Elad.", "year": 2003}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["E. Elhamifar", "R. Vidal"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Elhamifar and Vidal.,? \\Q2013\\E", "shortCiteRegEx": "Elhamifar and Vidal.", "year": 2013}, {"title": "Adaptive sparseness using jeffreys prior", "author": ["M.A.T. Figueiredo"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Figueiredo.,? \\Q2002\\E", "shortCiteRegEx": "Figueiredo.", "year": 2002}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Gregor and LeCun.,? \\Q2010\\E", "shortCiteRegEx": "Gregor and LeCun.", "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep unfolding: Model-based inspiration of novel deep architectures", "author": ["J.R. Hershey", "J. Le Roux", "F. Weninger"], "venue": "arXiv preprint arXiv:1409.2574v4,", "citeRegEx": "Hershey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hershey et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Robust photometric stereo using sparse regression", "author": ["S. Ikehata", "D.P. Wipf", "Y. Matsushita", "K. Aizawa"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Ikehata et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ikehata et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Linear and Nonlinear Programming", "author": ["D.G. Luenberger"], "venue": null, "citeRegEx": "Luenberger.,? \\Q1984\\E", "shortCiteRegEx": "Luenberger.", "year": 1984}, {"title": "Sparse signal reconstruction perspective for source localization with sensor arrays", "author": ["D.M. Malioutov", "M. \u00c7etin", "A.S. Willsky"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "Malioutov et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Malioutov et al\\.", "year": 2005}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G. Hinton"], "venue": "Proc. 27th International Conference on Machine Learning,", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition", "author": ["Y.C. Pati", "R. Rezaiifar", "P.S. Krishnaprasad"], "venue": "In Twenty-Seventh Asilomar Conference on Signals, Systems and Computers,", "citeRegEx": "Pati et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pati et al\\.", "year": 1993}, {"title": "Learning efficient sparse and low rank models", "author": ["P. Sprechmann", "A.M. Bronstein", "G. Sapiro"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Sprechmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sprechmann et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Greed is good: Algorithmic results for sparse approximation", "author": ["J.A. Tropp"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Tropp.,? \\Q2004\\E", "shortCiteRegEx": "Tropp.", "year": 2004}, {"title": "A deterministic analysis of noisy sparse subspace clustering for dimensionality-reduced data", "author": ["Y. Wang", "Y.X. Wang", "A. Singh"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Sparse estimation with structured dictionaries", "author": ["D.P. Wipf"], "venue": "Advances in Nerual Information Processing", "citeRegEx": "Wipf.,? \\Q2012\\E", "shortCiteRegEx": "Wipf.", "year": 2012}, {"title": "Latent variable Bayesian models for promoting sparsity", "author": ["D.P. Wipf", "B.D. Rao", "S. Nagarajan"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Wipf et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wipf et al\\.", "year": 2011}, {"title": "Photometric method for determining surface orientation from multiple images", "author": ["R.J. Woodham"], "venue": "Optical Engineering,", "citeRegEx": "Woodham.,? \\Q1980\\E", "shortCiteRegEx": "Woodham.", "year": 1980}, {"title": "Robust photometric stereo via low-rank matrix completion and recovery", "author": ["L. Wu", "A. Ganesh", "B. Shi", "Y. Matsushita", "Y. Wang", "Y. Ma"], "venue": "Asian Conference on Computer Vision,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 13, "context": "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm minimization (Donoho and Elad, 2003; Tibshirani, 1996), greedy approaches like orthogonal matching pursuit (OMP) (Pati et al.", "startOffset": 120, "endOffset": 161}, {"referenceID": 29, "context": "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm minimization (Donoho and Elad, 2003; Tibshirani, 1996), greedy approaches like orthogonal matching pursuit (OMP) (Pati et al.", "startOffset": 120, "endOffset": 161}, {"referenceID": 26, "context": "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm minimization (Donoho and Elad, 2003; Tibshirani, 1996), greedy approaches like orthogonal matching pursuit (OMP) (Pati et al., 1993; Tropp, 2004), and many flavors of iterative thresholding (Beck and Teboulle, 2009; Blumensath and Davies, 2008).", "startOffset": 220, "endOffset": 252}, {"referenceID": 30, "context": "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm minimization (Donoho and Elad, 2003; Tibshirani, 1996), greedy approaches like orthogonal matching pursuit (OMP) (Pati et al., 1993; Tropp, 2004), and many flavors of iterative thresholding (Beck and Teboulle, 2009; Blumensath and Davies, 2008).", "startOffset": 220, "endOffset": 252}, {"referenceID": 3, "context": ", 1993; Tropp, 2004), and many flavors of iterative thresholding (Beck and Teboulle, 2009; Blumensath and Davies, 2008).", "startOffset": 65, "endOffset": 119}, {"referenceID": 4, "context": ", 1993; Tropp, 2004), and many flavors of iterative thresholding (Beck and Teboulle, 2009; Blumensath and Davies, 2008).", "startOffset": 65, "endOffset": 119}, {"referenceID": 11, "context": "Variants of these algorithms find practical relevance in numerous disparate application domains, including feature selection (Cotter and Rao, 2002; Figueiredo, 2002), outlier removal (Cand\u00e8s and Tao, 2005; Ikehata et al.", "startOffset": 125, "endOffset": 165}, {"referenceID": 15, "context": "Variants of these algorithms find practical relevance in numerous disparate application domains, including feature selection (Cotter and Rao, 2002; Figueiredo, 2002), outlier removal (Cand\u00e8s and Tao, 2005; Ikehata et al.", "startOffset": 125, "endOffset": 165}, {"referenceID": 7, "context": "Variants of these algorithms find practical relevance in numerous disparate application domains, including feature selection (Cotter and Rao, 2002; Figueiredo, 2002), outlier removal (Cand\u00e8s and Tao, 2005; Ikehata et al., 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al.", "startOffset": 183, "endOffset": 227}, {"referenceID": 21, "context": "Variants of these algorithms find practical relevance in numerous disparate application domains, including feature selection (Cotter and Rao, 2002; Figueiredo, 2002), outlier removal (Cand\u00e8s and Tao, 2005; Ikehata et al., 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al.", "startOffset": 183, "endOffset": 227}, {"referenceID": 12, "context": ", 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al.", "startOffset": 29, "endOffset": 43}, {"referenceID": 2, "context": ", 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al., 2001; Malioutov et al., 2005) among many others.", "startOffset": 69, "endOffset": 115}, {"referenceID": 24, "context": ", 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al., 2001; Malioutov et al., 2005) among many others.", "startOffset": 69, "endOffset": 115}, {"referenceID": 16, "context": "Although seemingly unrelated at first glance, the layers of a deep neural network (DNN) can be viewed as iterations of some algorithm that have been unfolded into a network structure (Gregor and LeCun, 2010; Hershey et al., 2014).", "startOffset": 183, "endOffset": 229}, {"referenceID": 19, "context": "Although seemingly unrelated at first glance, the layers of a deep neural network (DNN) can be viewed as iterations of some algorithm that have been unfolded into a network structure (Gregor and LeCun, 2010; Hershey et al., 2014).", "startOffset": 183, "endOffset": 229}, {"referenceID": 16, "context": "This in turn can lead to a dramatically reduced computational burden relative to purely optimization-based approaches, which can require hundreds or even thousands of iterations to sufficiently converge (Gregor and LeCun, 2010; Sprechmann et al., 2015).", "startOffset": 203, "endOffset": 252}, {"referenceID": 27, "context": "This in turn can lead to a dramatically reduced computational burden relative to purely optimization-based approaches, which can require hundreds or even thousands of iterations to sufficiently converge (Gregor and LeCun, 2010; Sprechmann et al., 2015).", "startOffset": 203, "endOffset": 252}, {"referenceID": 16, "context": "For example, (Gregor and LeCun, 2010) promotes a soft-threshold function inspired by and iterative shrinkage-thresholding algorithm (ISTA) for minimizing the `1-norm, a well-known convex approximation to the canonical `0 norm sparsity penalty from (1).", "startOffset": 13, "endOffset": 37}, {"referenceID": 27, "context": "In contrast, (Sprechmann et al., 2015) advocates a wider class of functions derived from proximal operators (Parikh and Boyd, 2014).", "startOffset": 13, "endOffset": 38}, {"referenceID": 31, "context": "Finally, it has also been suggested that replacing typically continuous activation functions with hard-threshold operators may lead to sparser representations (Wang et al., 2015).", "startOffset": 159, "endOffset": 178}, {"referenceID": 5, "context": "Iterative hard-thresholding (IHT) attempts to minimize (2) using what can be viewed as computationally-efficient projected gradient iterations (Blumensath and Davies, 2009).", "startOffset": 143, "endOffset": 172}, {"referenceID": 5, "context": "In the context of IHT, it has been shown (Blumensath and Davies, 2009) that if y = \u03a6x\u2217, with \u2016x\u20160 \u2264 k and \u03b43k[\u03a6] < 1/ \u221a 32, then at iteration t of (5)", "startOffset": 41, "endOffset": 70}, {"referenceID": 8, "context": "Moreover, it can be shown that this x\u2217 is also the unique, optimal solution to (1) (Cand\u00e8s et al., 2006).", "startOffset": 83, "endOffset": 104}, {"referenceID": 6, "context": "Other values of \u03bc or even a positive definite matrix, adaptively chosen, can lead to a faster convergence rate (Blumensath and Davies, 2010).", "startOffset": 111, "endOffset": 140}, {"referenceID": 7, "context": "While dictionaries with columns drawn independently and uniformly from the surface of a unit hypersphere3 will satisfy this condition with high probability provided k is small enough (Cand\u00e8s and Tao, 2005), for many/most practical problems of interest we cannot rely on this type of IHT recovery guarantee.", "startOffset": 183, "endOffset": 205}, {"referenceID": 16, "context": "To see this, note that from a qualitative standpoint it is quite clear that the iterations of sparsity-promoting algorithms like IHT resemble the layers of neural networks (Gregor and LeCun, 2010).", "startOffset": 172, "endOffset": 196}, {"referenceID": 1, "context": "For example, in the extreme case if r = n\u2212 1, then columns of \u00c3 will be reduced to a n\u2212 r = 1 dimensional subspace, and no RIP conditions can possibly hold (see (Bah and Tanner, 2010) for details of how the RIP constant scales with the dimensions of Gaussian iid matrices).", "startOffset": 161, "endOffset": 183}, {"referenceID": 0, "context": "Given that multiplying a gradient by a positive-definite, symmetric matrix guarantees a descent direction is preserved, the inclusion of BB> could be viewed as as a form of natural gradient direction to be learned during training (Amari, 1998).", "startOffset": 230, "endOffset": 243}, {"referenceID": 28, "context": "In spirit, (15) can be viewed as something like a highway network element (Srivastava et al., 2015) or a LSTM cell (Hochreiter and Schmidhuber, 1997), where elements can be turned on and off via a gating mechanism separate from the activation function.", "startOffset": 74, "endOffset": 99}, {"referenceID": 20, "context": ", 2015) or a LSTM cell (Hochreiter and Schmidhuber, 1997), where elements can be turned on and off via a gating mechanism separate from the activation function.", "startOffset": 23, "endOffset": 57}, {"referenceID": 20, "context": "\u2022 The support sets \u03a9(t) on and \u03a9 (t) off allow the network to \u2018remember\u2019 previously learned cluster-level sparsity patterns, in much the same that LSTM gates allow long term dependencies to propagate (Hochreiter and Schmidhuber, 1997) or highway networks (Srivastava et al.", "startOffset": 200, "endOffset": 234}, {"referenceID": 28, "context": "\u2022 The support sets \u03a9(t) on and \u03a9 (t) off allow the network to \u2018remember\u2019 previously learned cluster-level sparsity patterns, in much the same that LSTM gates allow long term dependencies to propagate (Hochreiter and Schmidhuber, 1997) or highway networks (Srivastava et al., 2015) facilitate information flow unfettered to deeper layers.", "startOffset": 255, "endOffset": 280}, {"referenceID": 22, "context": "We deploy this tool, along with batch-normalization (Ioffe and Szegedy, 2015) to aid convergence, for our basic feedforward pipeline.", "startOffset": 52, "endOffset": 77}, {"referenceID": 25, "context": "We replace the non-integrable hard-threshold operator with simple rectilinear (ReLu) units (Nair and Hinton, 2010), which are functionally equivalent to one-sided softthresholding.", "startOffset": 91, "endOffset": 114}, {"referenceID": 16, "context": ", see (Gregor and LeCun, 2010; Sprechmann et al., 2015; Wang et al., 2015).", "startOffset": 6, "endOffset": 74}, {"referenceID": 27, "context": ", see (Gregor and LeCun, 2010; Sprechmann et al., 2015; Wang et al., 2015).", "startOffset": 6, "endOffset": 74}, {"referenceID": 31, "context": ", see (Gregor and LeCun, 2010; Sprechmann et al., 2015; Wang et al., 2015).", "startOffset": 6, "endOffset": 74}, {"referenceID": 13, "context": "Provided the dictionary satisfies minimal assumptions related to a quantity called matrix spark (Donoho and Elad, 2003), x\u2217 will provably represent the maximally sparse feasible solution.", "startOffset": 96, "endOffset": 119}, {"referenceID": 22, "context": ", 2015a) and batch normalization (Ioffe and Szegedy, 2015).", "startOffset": 33, "endOffset": 58}, {"referenceID": 10, "context": "The detailed network structure, which was implemented using MXNet (Chen et al., 2015), can be found in Figure 1.", "startOffset": 66, "endOffset": 85}, {"referenceID": 23, "context": "pattern with exactly n nonzeros is sufficient to produce a unique feasible solution (referred to as a basic feasible solution in the linear programming literature (Luenberger, 1984)).", "startOffset": 163, "endOffset": 181}, {"referenceID": 8, "context": "7 These include standard `1 minimization via ISTA iterations (Cand\u00e8s et al., 2006), IHT (Blumensath and Davies, 2009), an ISTA-based network (Gregor and LeCun, 2010), and an IHT-inspired network (Wang et al.", "startOffset": 61, "endOffset": 82}, {"referenceID": 5, "context": ", 2006), IHT (Blumensath and Davies, 2009), an ISTA-based network (Gregor and LeCun, 2010), and an IHT-inspired network (Wang et al.", "startOffset": 13, "endOffset": 42}, {"referenceID": 16, "context": ", 2006), IHT (Blumensath and Davies, 2009), an ISTA-based network (Gregor and LeCun, 2010), and an IHT-inspired network (Wang et al.", "startOffset": 66, "endOffset": 90}, {"referenceID": 31, "context": ", 2006), IHT (Blumensath and Davies, 2009), an ISTA-based network (Gregor and LeCun, 2010), and an IHT-inspired network (Wang et al., 2015).", "startOffset": 120, "endOffset": 139}, {"referenceID": 31, "context": "In particular, we utilize the socalled HELU\u03c3 function introduced in (Wang et al., 2015), which is a continuous and piecewise linear approximation of the scalar hard-threshold operator given by", "startOffset": 68, "endOffset": 87}, {"referenceID": 34, "context": "For example, when images of an ideal Lambertian surface are obtained under illumination from three known directions, the surface orientation can be uniquely determined using a simple least-squares fit (Woodham, 1980).", "startOffset": 201, "endOffset": 216}, {"referenceID": 35, "context": "a sparse error term (Wu et al., 2010; Ikehata et al., 2012).", "startOffset": 20, "endOffset": 59}, {"referenceID": 21, "context": "a sparse error term (Wu et al., 2010; Ikehata et al., 2012).", "startOffset": 20, "endOffset": 59}, {"referenceID": 34, "context": "Then the resulting measurements, denoted o \u2208 Rq, can be expressed as o = \u03c1Ln, (22) where n \u2208 R3 denotes the true surface normal, each row of L \u2208 Rq\u00d73 defines a lighting direction, and \u03c1 is the diffuse albedo, acting here as a scalar multiplier (Woodham, 1980).", "startOffset": 244, "endOffset": 259}, {"referenceID": 35, "context": "where e is an an unknown sparse vector (Wu et al., 2010; Ikehata et al., 2012).", "startOffset": 39, "endOffset": 78}, {"referenceID": 21, "context": "where e is an an unknown sparse vector (Wu et al., 2010; Ikehata et al., 2012).", "startOffset": 39, "endOffset": 78}, {"referenceID": 21, "context": "Following (Ikehata et al., 2012), we use 32-bit HDR gray-scale images of the object Bunny (256\u00d7256) with foreground masks under different lighting conditions whose directions, or rows of L, are randomly selected from a hemisphere with the object placed at the center.", "startOffset": 10, "endOffset": 32}, {"referenceID": 34, "context": "We compare our method against the baseline least squares estimate from (Woodham, 1980), `1 norm minimization, and a sparse Bayesian learning (SBL) approach specifically developed in (Ikehata et al.", "startOffset": 71, "endOffset": 86}, {"referenceID": 21, "context": "We compare our method against the baseline least squares estimate from (Woodham, 1980), `1 norm minimization, and a sparse Bayesian learning (SBL) approach specifically developed in (Ikehata et al., 2012) for surface normal estimation.", "startOffset": 182, "endOffset": 204}, {"referenceID": 20, "context": "Using a cell design from (Hochreiter and Schmidhuber, 1997), we adopt a two-layer LSTM network with a fixed size of 11 steps.", "startOffset": 25, "endOffset": 59}, {"referenceID": 32, "context": "The later was chosen because it represents an algorithm explicitly designed to handle dictionary correlations (Wipf, 2012).", "startOffset": 110, "endOffset": 122}, {"referenceID": 33, "context": "This modification was introduced because it represents a challenging scenario for many traditional sparse optimization algorithms for technical reasons related to local minima detailed in (Wipf et al., 2011).", "startOffset": 188, "endOffset": 207}, {"referenceID": 7, "context": "Examples include outlier removal (Cand\u00e8s and Tao, 2005; Ikehata et al., 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al.", "startOffset": 33, "endOffset": 77}, {"referenceID": 21, "context": "Examples include outlier removal (Cand\u00e8s and Tao, 2005; Ikehata et al., 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al.", "startOffset": 33, "endOffset": 77}, {"referenceID": 12, "context": ", 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al.", "startOffset": 29, "endOffset": 43}, {"referenceID": 2, "context": ", 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al., 2001; Malioutov et al., 2005) applications where, once learned, a deep model can produce maximally sparse representations as new input signals arrive.", "startOffset": 69, "endOffset": 115}, {"referenceID": 24, "context": ", 2012), compressive sensing (Donoho, 2006), and source localization (Baillet et al., 2001; Malioutov et al., 2005) applications where, once learned, a deep model can produce maximally sparse representations as new input signals arrive.", "startOffset": 69, "endOffset": 115}, {"referenceID": 14, "context": "In contrast, other influential domains such as subspace clustering (Elhamifar and Vidal, 2013) effectively require solving sparse recovery problems with a novel dictionary at each instance such that any attempt to construct a viable training set would be infeasible.", "startOffset": 67, "endOffset": 94}, {"referenceID": 5, "context": "We may then apply Theorem 5 from (Blumensath and Davies, 2009) using this revised system and conclude that", "startOffset": 33, "endOffset": 62}, {"referenceID": 5, "context": "This allows us to apply Theorem 5 from (Blumensath and Davies, 2009), from which we can infer that after at most", "startOffset": 39, "endOffset": 68}, {"referenceID": 8, "context": "and all that is required for a unique, maximally sparsity with kx + kc nonzeros is the much weaker inequality \u03b4(2[kx+kc]) ([U ,A(Jc)]) < 1 (Cand\u00e8s et al., 2006).", "startOffset": 139, "endOffset": 160}, {"referenceID": 9, "context": "In particular, using modifications of Theorem 1 from (Carrillo et al., 2011) and the fact that \u03b4(3kx+kc) ([U ,A(Jc)]) < 1/ \u221a 32, it follows that after \u03c4 + t iterations, the A-IHT reconstruction error is bounded by", "startOffset": 53, "endOffset": 76}], "year": 2017, "abstractText": "The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer. Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights. It is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available. While the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy. In particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse representations. In contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal `0-norm representations in regimes where existing methods fail. The resulting system is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3D scene.", "creator": "LaTeX with hyperref package"}}}