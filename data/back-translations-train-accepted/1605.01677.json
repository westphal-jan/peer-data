{"id": "1605.01677", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2016", "title": "Copeland Dueling Bandit Problem: Regret Lower Bound, Optimal Algorithm, and Computationally Efficient Algorithm", "abstract": "We study the K-armed dueling bandit problem, a variation of the standard stochastic bandit problem where the feedback is limited to relative comparisons of a pair of arms. The hardness of recommending Copeland winners, the arms that beat the greatest number of other arms, is characterized by deriving an asymptotic regret bound. We propose Copeland Winners Relative Minimum Empirical Divergence (CW-RMED) and derive an asymptotically optimal regret bound for it. However, it is not known whether the algorithm can be efficiently computed or not. To address this issue, we devise an efficient version (ECW-RMED) and derive its asymptotic regret bound. Experimental comparisons of dueling bandit algorithms show that ECW-RMED significantly outperforms existing ones.", "histories": [["v1", "Thu, 5 May 2016 18:08:13 GMT  (2332kb,D)", "https://arxiv.org/abs/1605.01677v1", null], ["v2", "Tue, 24 May 2016 12:42:15 GMT  (2105kb,D)", "http://arxiv.org/abs/1605.01677v2", "To appear in ICML2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["junpei komiyama", "junya honda", "hiroshi nakagawa"], "accepted": true, "id": "1605.01677"}, "pdf": {"name": "1605.01677.pdf", "metadata": {"source": "CRF", "title": "Copeland Dueling Bandit Problem: Regret Lower Bound, Optimal Algorithm, and Computationally Efficient Algorithm", "authors": ["Junpei Komiyama", "Junya Honda", "Hiroshi Nakagawa"], "emails": ["JUNPEI@KOMIYAMA.INFO", "HONDA@STAT.T.U-TOKYO.AC.JP", "NAKAGAWA@DL.ITC.U-TOKYO.AC.JP"], "sections": [{"heading": "1. Introduction", "text": "A multi-armed bandit problem is a crystallized instance of a sequential decision problem in an uncertain environment, and it can model many real-life scenarios. This problem affects conceptual entities known as weapons. In each round, the forecaster pulls one of the K-arms and receives corresponding reward feedback. However, the forecaster's goal is to maximize the cumulative reward over rounds achieved by running an algorithm that balances exploration (gathering information) and exploitation (using information). In evaluating the performance of a bandit algorithm known as regret, mea-sures how much the algorithm researches, it is desirable to receive rewards as direct feedback from one arm, in a number of practical cases such direct feedback is not available. In this paper, we consider a version of the stochastic bandit problem known as the duel problem."}, {"heading": "1.1. Related work", "text": "Early algorithms for solving the dueling bandit problem partially relativizes the adoption of the overall order because there is a problem, such as Interleaved Filter (Yue et al., 2012) and Beat the Mean Bandit (Yue & Joachims, 2011), requiring the arms toar Xiv: 160 5.01 677v 2 [stat.ML] 2 4M aybe total ordered.Urvoy et al. (2013) as a large class of consecutive learning problems, which also include the dueling bandit problem and introduce the notion of Condorcet, Copeland and Borda dueling bandit problems. Several algorithms, such as the monitoring of the relative upper class Bound (RUCB) (Zoghi et al., 2014), and the Relative Minimum Empirical Divergence (RMED) (Komiyama et al), have suggested since 2015 that the Condorcet dueling bandit problems are effectively solved."}, {"heading": "2. Problem Setup", "text": "The K-armed duel problem involves K arms indexed as [K]: = = j = j = j = 2,., K. \"Let M\" RK \"\u00b7 K be a preference matrix whose ij input corresponds to \u00b5i, j = the probability that arm i is preferred as the winner. In each turn t = 1, 2,., T, the forecaster draws a pair of arms p\" (t) = (t), m \"(t)))) 2 and receives a relative feedback X\" l \"(t), m\" (t) (t)., \"Bernoulli\" (t), m \"(t)), indicating which of (t), m\" (t). We say poor i \"beats arm j\" if \u00b5i. \"by definition, j > 1 / 2. We assume j\" i, \"j.\""}, {"heading": "3. Regret Lower Bound", "text": "In this section, we derive an asymptotic regret when there is satisfaction. (...) In the context of the standard multi-armed bandit problem (...), it requires (...) a strong consistent algorithm in the sense that it works well with any set of model parameters. (...) We extend this result to the Copeland dueling bandit problem. (...) We first define terms that are relevant to the characterization of regret: the subsets of power of the superior and the questioner with a specified size. (...) Let Smi: = {S \u00b2 2Si: The terms that matter to regret. (...)"}, {"heading": "3.1. Comparison with the Consistency in Condorcet dueling bandits", "text": "A dueling bandit algorithm is strongly consistent in the sense of Condorcet if it exhibits a subpolynomial regret for any M-MCond, where MCond is the series of preference matrices in which the Condorcet winner (i.e. the Copeland winner i1 of Li1 = 0) exists (Komiyama et al., 2015a). Although the definitions of regret are slightly different in the two dueling bandit problems, they are the same in those drawing pairs that include non-Copeland winners, the regret increases and thus a subpolynomic regret in the sense of the Condorcet dueling bandit problem with the Copeland dueling bandit problem. Therefore, a strongly consistent Copeland dueling bandit algorithm containing non-Copeland winners is also strongly consistent in the sense of the Condorcet dueling bandit problem, and thus a subpolynomial regret in the sense of the Condorcet dueling bandit problem with the Copeland dueling bandit problem is not true in the sense of the Condorcet-dueling bandit algorithm problem."}, {"heading": "4. Algorithms", "text": "In this section, we first introduce the CW-RMED algorithm inspired by the DMED algorithm (Honda & Takemura, 2010) to solve the problem of multi-armed bandits, and then derive an asymptotically optimal repentance boing for CW-RMED. To our knowledge, however, it is not known whether optimization in the subroutine can be efficiently calculated or not. To solve this problem, we develop another algorithm called ECW-RMED, which is computationally efficient and has a regret boing that is nearly optimal."}, {"heading": "4.1. CW-RMED", "text": "Algorithm 1 is CW-RMED. At the beginning of each round t = 1, 2,.., T, if there is a pair (i, j) that is not drawn O (\u221a log t) times or p = i, j (t) is very close to 1 / 2, it immediately draws that pair. Otherwise, it enters the loop in which each pair is drawn in LC one after the other. After drawing each pair, it checks whether the current observation is sufficient or not. If the observation is sufficient to identify some i (t) as a Copeland winner, it uses the pairs of the pairs drawn in the next loop by adding the pairs with the number of observations below the minimum requirement for identifying i (t) as a winner with high reliability."}, {"heading": "4.1.1. COMPUTATION OF AN OPTIMAL SOLUTION", "text": "Here we discuss the arithmetical aspects of CWRMED. Verifying (3) is relatively easy since we can sort."}, {"heading": "1 0.5 0.6 0.6 0.6", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 0.4 0.5 0.9 0.1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 0.4 0.1 0.5 0.9", "text": "(1 CW-RMED) and ECW-RMED Algorithms (1). (1). (1). (1). (1). (1). (1). (1). (2). (2). (2). (2). (2). (2). (3). (3). (3). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5).). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5).).). (5). (5). (5).). (5)"}, {"heading": "4.2. ECW-RMED", "text": "In this section, we propose ECW-RMED (algorithm 1). The difference between CW-RMED (section 4.1) and ECW-RMED is the amount of exploration. For a candidate of the Copeland winners i1, an attempt is made to ensure that neither Li1 \u2265 mini Li + 1 nor Li2 \u2264 mini Li \u2212 1 occurs for any i2 6 = i1, meaning that i1 is a Copeland winner. Namely, for i1 \u0445 C-cop, letREi1 ({2, j}): {qi, j} i > j (0, 1 / dKL (0, j, 1 / 2) K (K \u2212 1) / 2: j i qi1 qi1, j = 1 / dKL (1, j, 1 / 2)."}, {"heading": "4.2.1. EFFICIENT COMPUTATION OF ECW-RMED", "text": "In this section we will show an efficient method to find a solution. Since the inequality (5) is split for each i2 6 = i1, the solution is sufficient for each i2. Leave S: = S: = S: = S: i2, i1}, k: = | S: \u2212 (L: i2 \u2212 L: i1 + 1). Furthermore, leave cj: = r: j, i2 ({\u00b5: i, j (t)}) / dKL (\u00b5: j, i2 (t), 1 / 2) \u2265 0 and ej: = qj, i2dKL (\u00b5: j: j, i2 (t: i, j (t)})."}, {"heading": "4.3. Relation between CW-RMED and ECW-RMED", "text": "The following theorem, the proof of which is in Appendix G, refers to the optimal limit of regret and that of ECW-RMED. Theorem 6. (Optimality of ECW-RMED) The following inequality always applies: CE-RMED i1 ({\u00b5i, j}) \u2265 C-RMED i1 ({\u00b5i, j}). (8) In addition, the following equality applies to C-RMED: CE-RMED i1 ({\u00b5i, j}) = C-RMED i1 ({\u00b5i, j}). (9) Inequality (8) states that the leading logarithmic constant of the limit between CW-RMED is always as good as that of ECW-RMED, which is natural since CW-RMED is asymptotically optimal as stated in Theorem 4. Nevertheless, (9) ECW-RMED has exactly the same constant when Copeland winners are not unique."}, {"heading": "4.4. Comparison of ECW-RMED and CCB", "text": "In this section we discuss qualitatively the improvement of the limit of regret indicated by ECW-RMED. Let us note that CCB has an asymptotic limit of regret (K + L1 + 1). (10) Theorem 3 in Zoghi et al. (2015a) shows that CCB has an asymptotic limit of regret (K + L1 + 1). (10) Theorem 3 in Zoghi et al. ({\u00b5i, j}) contains an asymptotic limit of regret (K, j, 1 / 2) = 1 if i = i1, j \u00b2 K, 1 / (Li2 \u2212 L1 + 1) if i = i2, j \u00b2 i1\\ {i1}, 0 otherwise, which implies that ECW-RMED (\u00b5i, j) is a leading constant of min i1 (C] CE, i1 (Lii1, j)."}, {"heading": "4.5. On hyperparameters \u03b1 and \u03b2", "text": "CW / ECW-RMED have two hyperparameters \u03b1 and \u03b2. The hyperparameter \u03b1 is necessary from both a theoretical and a practical point of view. It urges the drawing of each pair for o (log t) times to ensure the quality of the estimator. On the other hand, we suspect that the parameter \u03b2 is a theoretical artifact. Technically, the hyperparameter \u03b2 is necessary to limit regret when the quality of the estimate is low (i.e. inequality (32) in the appendix). A very small or zero \u03b2 is practically sufficient: it can be confirmed that the determination \u03b2 = 0 yields almost the same results as shown in section 5."}, {"heading": "5. Numerical Experiment", "text": "In order to evaluate the empirical performance of the proposed algorithms, we performed computer simulations with the following data sets (preference matrices): We have the submatrices of a 136-136 preference matrix from Zoghi et al. (2015b), which is derived from the Microsoft Learning to Rank (MSLR) database (each of which has a ranking feature in the base datasets); the value is the probability that the ranking i beats ranker j is based on the informational click model (Hofmann et al., 2013); we selected subsets of the rankings in our experiments and made sub-preference matrices; we excluded cases with extremely small gaps based on the informational click model (Hofmann et al., 2013)."}, {"heading": "6. Conclusion", "text": "We investigated the stochastic duel-bandit problem. The severity of the problem of recommending Copeland winners was revealed by deriving a lower limit of regret. CW-RMED, an asymptotically optimal algorithm, was proposed. Furthermore, ECW-RMED, an almost optimal algorithm, was proposed and an efficient calculation method for it was given. ECW-RMED significantly exceeded the most modern algorithms in an experiment."}, {"heading": "Acknowledgements", "text": "This work was partially supported by JSPS KAKENHI funding numbers 15J09850 and 16H00881."}, {"heading": "A. Preference Matrices in the Experiment", "text": "The following table shows the preference matrices used in the numerical experiment in section 5."}, {"heading": "B. Additional Experiment", "text": "We performed additional simulations with the following datasets. ArXiv is a preference matrix based on the six retrieval functions in the full-text search engine of ArXiv.org (Yue & Joachims, 2011), which is presented in Table 3 (d), where there is an order between the weapons. Although the fact \u00b54.6 = 1 / 2 contradicts our assumption, the Copeland winner is probably arm 1. Cyclic is the preference matrix of Table 2.MSLR Fix are the two size 5 x 5 matrices provided by Zoghi et al. (2015a) in Table 3 (e) and 3 (f)."}, {"heading": "C. Comparison of Regret Bounds", "text": "In this section, we clarify the differences between the remorse boundaries of CW-RMED, ECW-RMED and CCB by calculating them in the cyclic preference matrix (Table 2). In the cyclic preference matrix, we have C = 1, L1 = 0 and \u2206 = min (i, j), Pi6 = j | \u00b5i, j \u2212 1 / 2 | = 0.1. Table 4 shows the remorse boundaries of the three algorithms. These boundaries are calculated as follows: First, the remorse boundary of CW-RMED (inequality (2)) states that the risk of arm 1 being a non-Coptic winner is smaller than Log T: It requires N1,2 (T), N1,3 (T), N1,4 (T), (Log T), / (2dKL), that the arms (0.6, 0.5), (Log), (Log), (Log), (Log), (Log), (Log), (Log), (Log, (Log), (Log, (,), (Log, (Log,), (Log, (,), (Log, (Log,), (Log, (,), (Log, (,), (Log, (Log, (,), (Log, (Log,), (, (,), (Log, (Log, (,), (Log, (,), (Log, (,), (Log, (,), (, (Log, (,), (, (,), (Log, (, (,), (,), (, (,), (, (, (, (,), (, (, (,), (, (,), (,), (, (, (, (,), (,), (, (,), (, (,), (, (, (,), (, (,), (, (, (, (,), (,), (, (, (,), (, (, (,), (,), ("}, {"heading": "D. Facts", "text": "Fact 7 is a concentration inequality that limits the tail probability to the empirical mean values. Fact 8 is used to limit the KL divergence from below. Fact 9 is later used to prove Lemma11.Fact 7. (The Chernoff limit) Let X1,.., Xn i.i.d. binary random variables. Let X = 1n \u2211 n i = 1Xi and \u00b5 = E [X] then for each > 0, P (X limit) \u2264 exp (\u2212 dKL (\u00b5 +, \u00b5) n) and P (X limit) \u2264 exp (\u2212 dKL (\u00b5 \u2212, \u00b5) n).Fact 8. (The Pinsker inequality) For p, q limit (0, 1) \u2264 exp (\u2212 dKL \u2212 & 2\u00b52) & 2 (p \u2212 q).Then the KL divergence between two Bernoulli distributions is limited."}, {"heading": "E. Proofs on the Regret Lower Bound", "text": "In this section we prove that Lemma 1 and theorem T) A = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i = i, i = i = i = i, i = i = i, i = i, i = i, i = i, i = i, i = i, i = i, i = i, i = i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i"}, {"heading": "F. Proof on an Efficient Computation of ECW-RMED", "text": "The proof of theory 5. First, we show that there is an optimal solution for (6) such a solution. (6) This solution is smaller. (18) If a pair of i < j exists in such a way that there is an optimal solution for (17), then (17) the exchange of the values for (e) and (17) a satisfactory solution for (17) does not increase the objective value, since (17) an objective solution for (c) and the recursive application of this swap operation results in another optimal solution for (17). Restriction to (6) for (17) is equivalent to (17)."}, {"heading": "G. Proof of Theorem 6", "text": "Proof of theorem 6. First, one can verify that (5) for each i2 6 = i1 corresponds to the constraints of (2) for l = L1 \u2212 1. Second, Equation (4) implies that the constraints of (2) for all i2 are 6 = i1, l \u2265 L1. Combining these two facts, one comes to the conclusion that {qi, j} and REi1 ({\u00b5i, j}) are necessary for each preference matrix {\u00b5i, j} in which two or more Copeland winners exist. To derive (9), it is also sufficient to specify Ri1 ({\u00b5i, j}) = REi1 ({\u00b5i, j}) for each preference matrix {\u00b5i, j} in which two or more Copeland winners exist. In this case, L1 = L2, and l in (2) runs for {L1 \u2212 1, L1}."}, {"heading": "H. Proofs of Theorems 3 and 4", "text": "In this section, we provide complete proofs for theorems 3 and 4. We define the following events that are relevant for limiting regret: letter Xi, j (t): {Ni, j (t) < \u03b1 (p) and pair (i). Let Yi, j (t) be the event that the pair (i, j) is added in LN. Note that the event (i), j (t) and the pair (i, j) are added in LN. Note that the pair (i, j) is added in LN. (i) Note that Pi6 = jX ci (t) and the pair (i, j) reach the algorithms line 5 in algorithm 1, and Yi, j (t) implies the event, the pair (i, j)."}, {"heading": "I. Proof of Lemma 10", "text": "We have, T \u2211 t = 1 P [X \u2032 i, j (t)] = T \u2211 n = 1 P [T't = n {Ni, j (t) < \u03b1 \u221a log t, j (t) \u2212 1 / 2 | < \u03b2 / log t, Ni, j (t) = n}] \u2264 \u03b1 log T + T \u2211 n = 1 P [T't = n {p't, j (t) \u2212 1 / 2 | < \u03b2 / log t, Ni, j (t) \u2265 log T, Ni, j (t) = n}]. (25) Let us leave F (T) = \u2264 (\u03b1) log T)."}, {"heading": "J. Proof of Lemma 11", "text": "In other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words, in other words."}, {"heading": "K. Proof of Lemma 12", "text": "Following Hogan (1973), we define the continuity of a point-to-sentence diagram."}, {"heading": "L. Proof of Lemma 13", "text": "The proof for Lemma 13: We have T = 1 P = 1 P's (i ', j's) = 1 P's (i', j's) = 1 P's (i ', j's) = 1 P's (i', j's) = 1 P's (i ', j's) = 1 P's (i', j's) = 1 P's (i ', j's) = 1 P's (i', j's) = 1 P's (i ') Pi6 = j's (i', j's) Pi6 = j's (t's), j's (t's), j's (t's), j's (t's), j's (t's), j's (t's), j's (t's), j's (j's), j's (t's), j's (t's), j's (j's), j's (t's)."}], "references": [{"title": "Axiomatic foundations for ranking systems", "author": ["Altman", "Alon", "Tennenholtz", "Moshe"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Altman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Altman et al\\.", "year": 2008}, {"title": "Partial monitoring - classification, regret bounds, and algorithms", "author": ["Bart\u00f3k", "G\u00e1bor", "Foster", "Dean P", "P\u00e1l", "D\u00e1vid", "Rakhlin", "Alexander", "Szepesv\u00e1ri", "Csaba"], "venue": "Math. Oper. Res.,", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2014}, {"title": "A bayesian interactive optimization approach to procedural animation design", "author": ["Brochu", "Eric", "Tyson", "de Freitas", "Nando"], "venue": "In Proceedings of the 2010 Eurographics/ACM SIGGRAPH Symposium on Computer Animation,", "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Top-k selection based on adaptive sampling of noisy preferences", "author": ["Busa-Fekete", "R\u00f3bert", "Sz\u00f6r\u00e9nyi", "Bal\u00e1zs", "Cheng", "Weiwei", "Weng", "Paul", "H\u00fcllermeier", "Eyke"], "venue": "In ICML,", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2013}, {"title": "PAC rank elicitation through adaptive sampling of stochastic pairwise preferences", "author": ["Busa-Fekete", "R\u00f3bert", "Sz\u00f6r\u00e9nyi", "Bal\u00e1zs", "H\u00fcllermeier", "Eyke"], "venue": "In AAAI,", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2014}, {"title": "Elements of information theory (2", "author": ["Cover", "Thomas M", "Thomas", "Joy A"], "venue": "ed.). Wiley,", "citeRegEx": "Cover et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cover et al\\.", "year": 2006}, {"title": "Contextual dueling bandits", "author": ["Dud\u0131\u0301k", "Miroslav", "Hofmann", "Katja", "Schapire", "Robert E", "Slivkins", "Aleksandrs", "Zoghi", "Masrour"], "venue": "In COLT, pp", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2015}, {"title": "Utility-based dueling bandits as a partial monitoring", "author": ["Gajane", "Pratik", "Urvoy", "Tanguy"], "venue": "game. CoRR,", "citeRegEx": "Gajane et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gajane et al\\.", "year": 2015}, {"title": "Preference learning in recommender systems", "author": ["Gemmis", "Marco De", "Iaquinta", "Leo", "Lops", "Pasquale", "Musto", "Cataldo", "Narducci", "Fedelucio", "Semeraro", "Giovanni"], "venue": "In In Preference Learning (PL-09) ECML/PKDD-09 Workshop,", "citeRegEx": "Gemmis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gemmis et al\\.", "year": 2009}, {"title": "Fidelity, soundness, and efficiency of interleaved comparison methods", "author": ["Hofmann", "Katja", "Whiteson", "Shimon", "de Rijke", "Maarten"], "venue": "Transactions on Information Systems,", "citeRegEx": "Hofmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2013}, {"title": "Point-to-set maps in mathematical programming", "author": ["Hogan", "William W"], "venue": "SIAM Review,", "citeRegEx": "Hogan and W.,? \\Q1973\\E", "shortCiteRegEx": "Hogan and W.", "year": 1973}, {"title": "An Asymptotically Optimal Bandit Algorithm for Bounded Support Models", "author": ["Honda", "Junya", "Takemura", "Akimichi"], "venue": "In COLT, pp", "citeRegEx": "Honda et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Honda et al\\.", "year": 2010}, {"title": "Nantonac collaborative filtering: recommendation based on order responses", "author": ["Kamishima", "Toshihiro"], "venue": "In KDD, pp", "citeRegEx": "Kamishima and Toshihiro.,? \\Q2003\\E", "shortCiteRegEx": "Kamishima and Toshihiro.", "year": 2003}, {"title": "Polynomial algorithms in linear programming", "author": ["L.G. Khachiyan"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "Khachiyan,? \\Q1980\\E", "shortCiteRegEx": "Khachiyan", "year": 1980}, {"title": "Regret lower bound and optimal algorithm in dueling bandit problem", "author": ["Komiyama", "Junpei", "Honda", "Junya", "Kashima", "Hisashi", "Nakagawa", "Hiroshi"], "venue": "In COLT,", "citeRegEx": "Komiyama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Komiyama et al\\.", "year": 2015}, {"title": "Regret lower bound and optimal algorithm in finite stochastic partial monitoring", "author": ["Komiyama", "Junpei", "Honda", "Junya", "Nakagawa", "Hiroshi"], "venue": "In NIPS,", "citeRegEx": "Komiyama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Komiyama et al\\.", "year": 2015}, {"title": "Combinatorial Optimization: Theory and Algorithms", "author": ["Korte", "Bernhard", "Vygen", "Jens"], "venue": "4th edition,", "citeRegEx": "Korte et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Korte et al\\.", "year": 2007}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Lai", "Tze Leung", "Robbins", "Herbert"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1985}, {"title": "LETOR: A benchmark collection for research on learning to rank for information", "author": ["Qin", "Tao", "Liu", "Tie-Yan", "Xu", "Jun", "Li", "Hang"], "venue": "retrieval. Inf. Retr.,", "citeRegEx": "Qin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Qin et al\\.", "year": 2010}, {"title": "Generic exploration and k-armed voting bandits", "author": ["Urvoy", "Tanguy", "Cl\u00e9rot", "Fabrice", "Feraud", "Rapha\u00ebl", "Naamane", "Sami"], "venue": "In ICML, pp", "citeRegEx": "Urvoy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Urvoy et al\\.", "year": 2013}, {"title": "Efficient partial monitoring with prior information", "author": ["Vanchinathan", "Hastagiri P", "Bart\u00f3k", "G\u00e1bor", "Krause", "Andreas"], "venue": "In NIPS, pp", "citeRegEx": "Vanchinathan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vanchinathan et al\\.", "year": 2014}, {"title": "Beat the mean bandit", "author": ["Yue", "Yisong", "Joachims", "Thorsten"], "venue": "In ICML, pp", "citeRegEx": "Yue et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2011}, {"title": "The k-armed dueling bandits problem", "author": ["Yue", "Yisong", "Broder", "Josef", "Kleinberg", "Robert", "Joachims", "Thorsten"], "venue": "In COLT,", "citeRegEx": "Yue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2009}, {"title": "The k-armed dueling bandits problem", "author": ["Yue", "Yisong", "Broder", "Josef", "Kleinberg", "Robert", "Joachims", "Thorsten"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Yue et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2012}, {"title": "Crowdsourcing translation: Professional quality from non-professionals", "author": ["Zaidan", "Omar", "Callison-Burch", "Chris"], "venue": "Proceedings of the Conference,", "citeRegEx": "Zaidan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zaidan et al\\.", "year": 2011}, {"title": "Relative upper confidence bound for the k-armed dueling bandit problem", "author": ["Zoghi", "Masrour", "Whiteson", "Shimon", "Munos", "R\u00e9mi", "de Rijke", "Maarten"], "venue": "In ICML,", "citeRegEx": "Zoghi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2014}, {"title": "Mergerucb: A method for large-scale online ranker evaluation", "author": ["Zoghi", "Masrour", "Whiteson", "Shimon", "de Rijke", "Maarten"], "venue": "In WSDM, pp", "citeRegEx": "Zoghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": "In this paper, we consider a version of the standard stochastic bandit problem called the K-armed dueling bandit problem (Yue et al., 2009), in which the forecaster receives relative feedback, which specifies which of the two arms is preferred.", "startOffset": 121, "endOffset": 139}, {"referenceID": 8, "context": "Although the original motivation of the dueling bandit problem arose in the field of information retrieval, learning under relative feedback is universal to many fields, such as recommender systems (Gemmis et al., 2009), graphical design (Brochu et al.", "startOffset": 198, "endOffset": 219}, {"referenceID": 2, "context": ", 2009), graphical design (Brochu et al., 2010), and natural language processing (Zaidan & Callison-Burch, 2011), which involve explicit or implicit feedback provided by humans.", "startOffset": 26, "endOffset": 47}, {"referenceID": 23, "context": "Related work Early algorithms for solving the dueling bandit problem, such as Interleaved Filter (Yue et al., 2012) and Beat the Mean Bandit (Yue & Joachims, 2011), require the arms to ar X iv :1 60 5.", "startOffset": 97, "endOffset": 115}, {"referenceID": 25, "context": "Several algorithms, such as Relative Upper Confidence Bound (RUCB) (Zoghi et al., 2014), and Relative Minimum Empirical Divergence (RMED) (Komiyama et al.", "startOffset": 67, "endOffset": 87}, {"referenceID": 6, "context": "Note that there are also other notions of winners, such as the von Neumann winner (Dud\u0131\u0301k et al., 2015) or Random walk winner (Altman & Tennenholtz, 2008) together with their corresponding dueling bandit problems.", "startOffset": 82, "endOffset": 103}, {"referenceID": 1, "context": "Another line of study is on the partial monitoring problem (Bart\u00f3k et al., 2014).", "startOffset": 59, "endOffset": 80}, {"referenceID": 20, "context": "Moreover, existing algorithms for partial monitoring, such as Bayes-update Partial Monitoring (BPM) (Vanchinathan et al., 2014) or Partial Monitoring Deterministic Minimum Empirical Divergence (PMDMED) (Komiyama et al.", "startOffset": 100, "endOffset": 127}, {"referenceID": 19, "context": "There are some algorithms, such as Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al., 2013), Preference-based Racing (PBR) (Busa-Fekete et al.", "startOffset": 102, "endOffset": 122}, {"referenceID": 3, "context": ", 2013), Preference-based Racing (PBR) (Busa-Fekete et al., 2013), and Rank Elicitation (RankEI) (Busa-Fekete et al.", "startOffset": 39, "endOffset": 65}, {"referenceID": 4, "context": ", 2013), and Rank Elicitation (RankEI) (Busa-Fekete et al., 2014), that can deal with general classes of problems that entail solving Copeland dueling bandit problems.", "startOffset": 39, "endOffset": 65}, {"referenceID": 13, "context": "Urvoy et al. (2013) considered a large class of sequential learning problems that includes the dueling bandit problem and introduced the notion of Condorcet, Copeland, and Borda dueling bandit problems.", "startOffset": 0, "endOffset": 20}, {"referenceID": 25, "context": "Note that, we can also consider other definitions of regret; the analysis in this paper is relied on the facts that regret per round ri,j is (i) finite, (ii) determined by the Copeland The constant factor of this definition is different from the one defined in Zoghi et al. (2015a). Our result can be compared with that of Zoghi et al.", "startOffset": 261, "endOffset": 282}, {"referenceID": 25, "context": "Note that, we can also consider other definitions of regret; the analysis in this paper is relied on the facts that regret per round ri,j is (i) finite, (ii) determined by the Copeland The constant factor of this definition is different from the one defined in Zoghi et al. (2015a). Our result can be compared with that of Zoghi et al. (2015a) simply by multiplying a constant.", "startOffset": 261, "endOffset": 344}, {"referenceID": 13, "context": "It is well known that even if there are exponentially many constraints an LP can be solved by using the ellipsoid method (Khachiyan, 1980) in a polynomial time if there exists a polynomial-time oracle that (i) checks whether a point {qij} is feasible or not and (ii) returns a hyperplane such that {qij} and the feasible region are separated if {qij} is infeasible.", "startOffset": 121, "endOffset": 138}, {"referenceID": 25, "context": "Theorem 3 in Zoghi et al. (2015a) showed that CCB has an asymptotic regret bound2 of", "startOffset": 13, "endOffset": 34}, {"referenceID": 25, "context": "Here, we use a \u2206 that is a little bit looser than the one in the original bound of Zoghi et al. (2015a) for the sake of discussion.", "startOffset": 83, "endOffset": 104}, {"referenceID": 18, "context": "(2015b), which is derived from the Microsoft Learning to Rank (MSLR) dataset (Microsoft Research, 2010; Qin et al., 2010) that consists of relevance information between queries and documents with more than 30K queries.", "startOffset": 77, "endOffset": 121}, {"referenceID": 9, "context": "The value \u03bci,j is the probability that the ranker i beats ranker j based on the informational click model (Hofmann et al., 2013).", "startOffset": 106, "endOffset": 128}, {"referenceID": 25, "context": "RUCB (Zoghi et al., 2014) with \u03b1 = 0.", "startOffset": 5, "endOffset": 25}, {"referenceID": 21, "context": "MSLR: We tested submatrices of a 136 \u00d7 136 preference matrix from Zoghi et al. (2015b), which is derived from the Microsoft Learning to Rank (MSLR) dataset (Microsoft Research, 2010; Qin et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 15, "context": "(2015b), which is derived from the Microsoft Learning to Rank (MSLR) dataset (Microsoft Research, 2010; Qin et al., 2010) that consists of relevance information between queries and documents with more than 30K queries. Zoghi et al. (2015b) created a finite set of rankers, each of which corresponds to a ranking feature in the base dataset.", "startOffset": 104, "endOffset": 240}, {"referenceID": 9, "context": "The value \u03bci,j is the probability that the ranker i beats ranker j based on the informational click model (Hofmann et al., 2013). We randomly chose subsets of rankers in our experiments and made sub preference matrices. We excluded cases with extremely small gaps such that |\u03bci,j \u2212 1/2| < 0.005 for K = 16 or |\u03bci,j \u2212 1/2| < 0.0005 for K = 64. Furthermore, we selected the submatrices in which the Condorcet winner exists (Figure 1(a)) and the Condorcet winner does not exist (Figures 1(b) and 1(c)). Sushi: This dataset is based on the sushi preference dataset (Kamishima, 2003) that contains the preferences of 5, 000 Japanese users as regards to 100 types of sushi. We extracted 16 types of sushi and converted them into a preference matrix with \u03bci,j corresponding to the ratio of users who prefer sushi i over j, which is shown in Table 3(a) in Appendix. Gap is the preference matrix of Table 3(b) in Appendix. This matrix is a corner case in which (arg mini1 C E\u2217 i1 ({\u03bci,j}))/(arg mini1 C \u2217 i\u2217({\u03bci,j})) > 100. MultiSol is the preference matrix of Table 3(c) in Appendix. This matrix is an example in which the optimality condition in Theorem 4 is violated. Note that MLSR (Condorcet) and Sushi each have a Condorcet winner, whereas the others do not. The results with smaller preference matrices are shown in Appendix B. Algorithms: We compared the following algorithms: Random is a uniformly random sampling among pairs. Copeland SAVAGE with \u03b4 = 1/T is the algorithm that is general enough to solve the Copeland dueling bandit problems and have O(K log T ) regret bounds. We did not include PBR and RankEI because the two algorithms are reported to be consistently outperformed by other algorithms (Zoghi et al., 2015a). RUCB (Zoghi et al., 2014) with \u03b1 = 0.51 and RMED1 (Komiyama et al., 2015a) are algorithms for solving Condorcet dueling bandit problems. These algorithms are not designed to find all instances of Copeland dueling bandit problems. The values of the hyperparameters of RMED1 are the same as in Komiyama et al. (2015a). CCB (Zoghi et al.", "startOffset": 107, "endOffset": 2043}, {"referenceID": 25, "context": "MSLR Fixed are the two matrices of size 5\u00d7 5 provided by Zoghi et al. (2015a) shown in Table 3(e) and 3(f).", "startOffset": 57, "endOffset": 78}], "year": 2016, "abstractText": "We study theK-armed dueling bandit problem, a variation of the standard stochastic bandit problem where the feedback is limited to relative comparisons of a pair of arms. The hardness of recommending Copeland winners, the arms that beat the greatest number of other arms, is characterized by deriving an asymptotic regret bound. We propose Copeland Winners Relative Minimum Empirical Divergence (CW-RMED) and derive an asymptotically optimal regret bound for it. However, it is not known whether the algorithm can be efficiently computed or not. To address this issue, we devise an efficient version (ECW-RMED) and derive its asymptotic regret bound. Experimental comparisons of dueling bandit algorithms show that ECW-RMED significantly outperforms existing ones.", "creator": "LaTeX with hyperref package"}}}