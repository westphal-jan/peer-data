{"id": "1605.01832", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-May-2016", "title": "Cross-Graph Learning of Multi-Relational Associations", "abstract": "Cross-graph Relational Learning (CGRL) refers to the problem of predicting the strengths or labels of multi-relational tuples of heterogeneous object types, through the joint inference over multiple graphs which specify the internal connections among each type of objects. CGRL is an open challenge in machine learning due to the daunting number of all possible tuples to deal with when the numbers of nodes in multiple graphs are large, and because the labeled training instances are extremely sparse as typical. Existing methods such as tensor factorization or tensor-kernel machines do not work well because of the lack of convex formulation for the optimization of CGRL models, the poor scalability of the algorithms in handling combinatorial numbers of tuples, and/or the non-transductive nature of the learning methods which limits their ability to leverage unlabeled data in training. This paper proposes a novel framework which formulates CGRL as a convex optimization problem, enables transductive learning using both labeled and unlabeled tuples, and offers a scalable algorithm that guarantees the optimal solution and enjoys a linear time complexity with respect to the sizes of input graphs. In our experiments with a subset of DBLP publication records and an Enzyme multi-source dataset, the proposed method successfully scaled to the large cross-graph inference problem, and outperformed other representative approaches significantly.", "histories": [["v1", "Fri, 6 May 2016 06:15:20 GMT  (862kb,D)", "http://arxiv.org/abs/1605.01832v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hanxiao liu", "yiming yang"], "accepted": true, "id": "1605.01832"}, "pdf": {"name": "1605.01832.pdf", "metadata": {"source": "META", "title": "Cross-Graph Learning of Multi-Relational Associations", "authors": ["Hanxiao Liu", "Yiming Yang"], "emails": ["HANXIAOL@CS.CMU.EDU", "YIMING@CS.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive themselves if they do not feel able to survive themselves. Most of them are able to survive themselves, but most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "2. The Proposed Method", "text": "We introduce our notation in 2.1 and the term graph product (GP) in 2.2, then limit ourselves in 2.2 to a specific GP family with desirable computing properties and finally propose our GP-based optimization goal in 2.4."}, {"heading": "2.1. Notations", "text": "We get J heterogeneous graphs in which the j-th graph contains nj vertices and is associated with an adjacence matrix G (j) and Rnj \u00b7 nj. We use ij to index the ij-th vertex of the graph j, and use a tuple (i1,..., iJ) to index each multi-relationship across the J diagrams. System predictions of all possible J = 1 nj multirelations are summarized in an order-J tensor f (Rn1). We use J = 1 xj (or simply j xj) as an abbreviation for the tuple (i1,.., iJ). Let's denote the Kronecker (tensor) product. We use J = 1 xj (or simply j xj) as an abbreviation for the tuple (i1,. \u00b7 xJ)."}, {"heading": "2.2. Graph Product", "text": "In short, graph product (GP) 1 is an illustration of each cross-graph multi-ratio to each vertex in a new graph P. Its edges encode similarities between the multirelations (Fig. 1). A desirable property of GP is to provide a natural reduction from the original multirelational learning problem to heterogeneous information sources (Task 1) to an equivalent graph-based learning problem to a homogeneous graph (Task 2). Task 1. Given the J graphs G (1),..., G (J) with a small set of labeled multirelations O = {(i1,.., iJ)}, predict designations of the unlabeled multirelations. Task 2. Given the product diagram P (G (1),..., G (J)))) with a small set of labeled vertices O = {(i1,.., iJ)}."}, {"heading": "2.3. Spectral Graph Product", "text": "We define a parametric family of GP operators, the spectral graphics product (SGP) J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J"}, {"heading": "2.4. Optimization Objective", "text": "It is often more convenient to write tensor f as a multilinear map. For example, if J = 2, tensor (matrix) f-Rn1 \u00b7 n2 defines a bilinear map from Rn1 \u00b7 Rn2 to R via f (x1, x2): = x > 1 fx2 and we have fi1, i2 = f (ei1, ei2). Such equivalence is analogous to cases of high order where f injects a multilinear map from Rn1 \u00b7 \u00b7 \u00b7 \u00b7 RnJ to R. To perform transductive learning about P (task 2), we inject the structure of the product diagram into f via a Gaussian random field (Zhu et al., 2003). The negative log probability of the previous \u2212 log p (f-Rn) is the same (up to a constant) as the following square semi-standard."}, {"heading": "3. Convex Approximation", "text": "The computational bottleneck for optimization (4) lies in the evaluation of the calculation gap resulting from the extremely large size of P\u0442 using the exact formulation on the basis of which we propose in Section 3.1 our convex approximation scheme in Section 3.2, which reduces the temporal complexity of the evaluation of the semi-standard from O ((\u2211 j nj)) to O (\u0394j nj), with dj nj for j = 1,.., J."}, {"heading": "3.1. Complexity of the Exact Formulation", "text": "The brute force evaluation of the terms \"f\" and \"J\" according to (3) costs \"O\" (\"j\"), because one must evaluate \"O\" (\"j nj\") within the summation, where each term costs \"O\" (\"j nj\"). However, there are redundancies and the minimum complexity for the exact evaluation is stated as follows: \"The exact evaluation of the semi-standard\" f \"i1\" i1 \"i1,\" \"iJ\" (\"j nj\") flops.Proof. Note that the capture of all numbers in (3), namely \"f\" (1) i1, \"\" \"v\" (J) i1, \"\" i1, \"\u00b7\" iJ \"and\" iJ \"is a tensor in\" Rn1, \"\u00b7 \u00b7 \u00b7\" nJ \"above (f\" 1 \"V (1) \u00b7 V\" \u00b7 \u00b7 \u00b7 V \"(J) \u00b7 nJ \u00b7 \u00b7 nj\" and \"n (J) modes\" 1 \"and\" nJ \"(1).\""}, {"heading": "3.2. Approximation via Tucker Form", "text": "Equation (5) implies the reduction of complexity (2) \u00b7 \u00b7 J \u00b7 J \u00b7 \u00b7 J \u00b7 \u00b7 J \u00b7 \u00b7 Self-multiplication (1) \u00b7 J \u00b7 J (1) \u00b7 J \u00b7 J (1) \u00b7 J \u00b7 J (1) \u00b7 J \u00b7 J \u00b7 Property (1) \u00b7 J \u00b7 Property (1) \u00b7 J \u00b7 Property (1) \u00b7 J \u00b7 Self-vectors (1) \u00b7 V \u00b7 (1) \u00b7 J \u00b7 J (1) \u00b7 G (1) \u00b7 J \u00b7 Property (1) \u00b7 J \u00b7 Property (1) \u00b7 J \u00b7 Property (1) \u00b7 G (1) \u00b7 J \u00b7 J \u00b7 Self-vectors (1) \u00b7 \u00b7 G \u00b7 \u00b7 \u00b7 1) \u00b7 G (1) \u00b7 J (J) 1 (J) (J) 1 J 1 (J) (J) (J) 1 J 1 (J) (J) 1 J 1 (J) (J) (J) 1 J (J) (J) 1 (J) (J) 1 (J) (J) (J) 1 (J) (G) 1 (J) (J) 1 (J) (J) (J (J) 1 (J) (J) 1 (J) (J (J) 1 (J) (J) (J (J) 1 (J (J) 1 (J) (J (J) 1 (J) (J (J) 1 (J (J) (J) (J (J) 1 (J (J) 1 (J (J) (J) 1 (J (J) 1 (J) (J (J) (G (J) 1) (G (J) 1) (G (G (1) 1) 1 (J (J (J) 1) (J (J) (J) 1 (J) (J) (G (1) (G (J) 1) (G (1) 1) (G (G (1) (G (1) 1) (G (1) 1) (J (J (J (1) (1) (J (J) (J (1) (J) (J (1) (J (J (1) 1) (J (1) (J (J (1) (J) (J (1"}, {"heading": "4. Optimization", "text": "We define \"O (f) to be the ranking\" 2-hinge loss'O (f) 1-hinge loss'O (f) = 2-hinge loss'O (f) = 1-1,.., iJ) \"O (i)\" (i) \"O\" (i) \"O\" (i) \"J\" (i) \"O\" (i) \"O\" (i) \"O\" (i) \"O\" (i) \"O.\" We use \"O\" (i) all possible multirelations. Eq. (10) encourages the valid tuples in our education O. \""}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Datasets", "text": "We evaluate our method based on real data in two different areas: the enzyme dataset (Yamanishi et al., 2008) for connection with proteins and the DBLP dataset of scientific publications. Fig. 3 illustrates their heterogeneous objects and relationship structures. The enzyme dataset was used to model and predict interactions between drugs and targets, which include a graph of 445 chemical compounds (drugs) and a graph of 664 proteins (targets). The predictive task is to identify the unknown connections between substances and proteins based on the graph structures and a small set of 2,926 known interactions. The graph of the compounds is based on the SIMCOMP Score (Hattori et al., 2003), and the graph of proteins interacting between substances."}, {"heading": "5.2. Methods for Comparison", "text": "We expect to extend the traditional TF by adding graphs to the objective function."}, {"heading": "5.3. Experiment Setups", "text": "For both sets of data, one-third of known training interactions (designated by O), one-third for validation, and the remaining for testing. Known interactions in the test set, designated by T, are treated as positive examples. All tuples that are not in T, designated by T, are treated as negative. Tuples that are already in O are removed from T to avoid misleading results (Bordes et al., 2013). We measure algorithm performance on the basis of enzyme based on the quality of the derived target proteins given to each compound, namely, on the ability to complete (compound,?). For DBLP, performance is measured on the quality of the inferred work indicated by the author and venue, namely, on the ability to finish (author,?, Venue). We use Mean Average Prevision (MAP), Area Under the Curve (AUC), and Hits at Top 5 (Hits @ 5) as our evaluation metrics."}, {"heading": "5.4. Results", "text": "Figure 4 compares the results of TOP with different parameters of the spectral graph product (SGP), among which exponential plot works better on average. Figures 5 and 6 show the main results compared to TOP (with exponential plot) and other representative baselines. TOP clearly outperforms all other methods on both sets of data in all evaluation metrics of MAP 2, AUC and Hit @ 5.Fig.7 shows the power curves from TOP to enzyme across different model sizes (by variing the djs). At a relatively small model size compared to using the entire spectrum, the performance of TOP converges to the optimal point.2MAP values for random rates are 0.014 to enzyme and 0.0072 to DBLP, respectively."}, {"heading": "6. Concluding Remarks", "text": "The paper presents a novel convex optimization framework for transductive CGRL and a scalable algorithmic solution with a guaranteed global optimum and a time complexity that does not depend on the size of the input diagrams. Our experiments with multi-graph datasets provide strong evidence of the superior performance of the proposed approach in cross diagram modeling and large-scale optimization."}, {"heading": "Acknowledgements", "text": "We thank the reviewers for their helpful comments. This work is partially supported by the National Science Foundation (NSF) under grants IIS-1216282, 1350364, 1546329."}], "references": [{"title": "Unifying collaborative and content-based filtering", "author": ["Basilico", "Justin", "Hofmann", "Thomas"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Basilico et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Basilico et al\\.", "year": 2004}, {"title": "Kernel methods for predicting protein\u2013protein interactions", "author": ["Ben-Hur", "Asa", "Noble", "William Stafford"], "venue": "Bioinformatics, 21(suppl 1):i38\u2013i46,", "citeRegEx": "Ben.Hur et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ben.Hur et al\\.", "year": 2005}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Garcia-Duran", "Alberto", "Weston", "Jason", "Yakhnenko", "Oksana"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["Cai", "Deng", "He", "Xiaofei", "Han", "Jiawei", "Huang", "Thomas S"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Cai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Semisupervised learning in gigantic image collections", "author": ["Fergus", "Rob", "Weiss", "Yair", "Torralba", "Antonio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Fergus et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fergus et al\\.", "year": 2009}, {"title": "Heuristics for chemical compound matching", "author": ["Hattori", "Masahiro", "Okuno", "Yasushi", "Goto", "Susumu", "Kanehisa", "Minoru"], "venue": "Genome Informatics,", "citeRegEx": "Hattori et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hattori et al\\.", "year": 2003}, {"title": "Optimizing search engines using clickthrough data", "author": ["Joachims", "Thorsten"], "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Joachims and Thorsten.,? \\Q2002\\E", "shortCiteRegEx": "Joachims and Thorsten.", "year": 2002}, {"title": "Tensor decompositions and applications", "author": ["Kolda", "Tamara G", "Bader", "Brett W"], "venue": "SIAM review,", "citeRegEx": "Kolda et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolda et al\\.", "year": 2009}, {"title": "Bipartite edge prediction via transductive learning over product graphs", "author": ["Liu", "Hanxiao", "Yang", "Yiming"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Tensor factorization using auxiliary information", "author": ["Narita", "Atsuhiro", "Hayashi", "Kohei", "Tomioka", "Ryota", "Kashima", "Hisashi"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Narita et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Narita et al\\.", "year": 2012}, {"title": "Identification of common molecular subsequences", "author": ["Smith", "Temple F", "Waterman", "Michael S"], "venue": "Journal of molecular biology,", "citeRegEx": "Smith et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Smith et al\\.", "year": 1981}, {"title": "Arnetminer: extraction and mining of academic social networks", "author": ["Tang", "Jie", "Zhang", "Jing", "Yao", "Limin", "Li", "Juanzi", "Su", "Zhong"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Tang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2008}, {"title": "Prediction of drug\u2013target interaction networks from the integration of chemical and genomic spaces", "author": ["Yamanishi", "Yoshihiro", "Araki", "Michihiro", "Gutteridge", "Alex", "Honda", "Wataru", "Kanehisa", "Minoru"], "venue": "i232\u2013i240,", "citeRegEx": "Yamanishi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yamanishi et al\\.", "year": 2008}, {"title": "Gaussian process models for link analysis and transfer learning", "author": ["Yu", "Kai", "Chu", "Wei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2008}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["Zhu", "Xiaojin", "Ghahramani", "Zoubin", "Lafferty", "John"], "venue": "In ICML,", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 10, "context": "However, most of the tensor methods do not explicitly model the internal graph structure for each type of objects, although some of those methods implicitly leverage such information via graph-based regularization terms in their objective function that encourage similar objects within each graph to share similar latent factors (Narita et al., 2012; Cai et al., 2011).", "startOffset": 329, "endOffset": 368}, {"referenceID": 3, "context": "However, most of the tensor methods do not explicitly model the internal graph structure for each type of objects, although some of those methods implicitly leverage such information via graph-based regularization terms in their objective function that encourage similar objects within each graph to share similar latent factors (Narita et al., 2012; Cai et al., 2011).", "startOffset": 329, "endOffset": 368}, {"referenceID": 15, "context": "To carry out transductive learning over P\u03ba (Task 2), we inject the structure of the product graph into f via a Gaussian random fields prior (Zhu et al., 2003).", "startOffset": 140, "endOffset": 158}, {"referenceID": 5, "context": "Spectral approximation techniques for graphbased learning has been found successful in standard classification tasks (Fergus et al., 2009), which are special cases under our framework when J = 1.", "startOffset": 117, "endOffset": 138}, {"referenceID": 4, "context": "Following (Duchi et al., 2011), we allow adaptive step sizes for each element in \u03b1.", "startOffset": 10, "endOffset": 30}, {"referenceID": 13, "context": "We evaluate our method on real-world data in two different domains: the Enzyme dataset (Yamanishi et al., 2008) for compound-protein interaction and the DBLP dataset of scientific publication records.", "startOffset": 87, "endOffset": 111}, {"referenceID": 6, "context": "The graph of compounds is constructed based on the SIMCOMP score (Hattori et al., 2003), and the graph of proteins is constructed based on the normalized SmithWaterman score (Smith & Waterman, 1981).", "startOffset": 65, "endOffset": 87}, {"referenceID": 12, "context": "As for the DBLP dataset, we use a subset of 34,340 DBLP publication records in the domain of Artificial Intelligence (Tang et al., 2008), from which 3 graphs are constructed as:", "startOffset": 117, "endOffset": 136}, {"referenceID": 10, "context": "In GRTF, we further enhanced the traditional TF by adding graph regularizations to the objective function, which enforce the model to be aware of the context information inG\u2019s (Narita et al., 2012; Cai et al., 2011);", "startOffset": 176, "endOffset": 215}, {"referenceID": 3, "context": "In GRTF, we further enhanced the traditional TF by adding graph regularizations to the objective function, which enforce the model to be aware of the context information inG\u2019s (Narita et al., 2012; Cai et al., 2011);", "startOffset": 176, "endOffset": 215}, {"referenceID": 2, "context": "Tuples that are already in O are removed from T\u0304 to avoid misleading results (Bordes et al., 2013).", "startOffset": 77, "endOffset": 98}], "year": 2016, "abstractText": "Cross-graph Relational Learning (CGRL) refers to the problem of predicting the strengths or labels of multi-relational tuples of heterogeneous object types, through the joint inference over multiple graphs which specify the internal connections among each type of objects. CGRL is an open challenge in machine learning due to the daunting number of all possible tuples to deal with when the numbers of nodes in multiple graphs are large, and because the labeled training instances are extremely sparse as typical. Existing methods such as tensor factorization or tensor-kernel machines do not work well because of the lack of convex formulation for the optimization of CGRL models, the poor scalability of the algorithms in handling combinatorial numbers of tuples, and/or the non-transductive nature of the learning methods which limits their ability to leverage unlabeled data in training. This paper proposes a novel framework which formulates CGRL as a convex optimization problem, enables transductive learning using both labeled and unlabeled tuples, and offers a scalable algorithm that guarantees the optimal solution and enjoys a linear time complexity with respect to the sizes of input graphs. In our experiments with a subset of DBLP publication records and an Enzyme multi-source dataset, the proposed method successfully scaled to the large cross-graph inference problem, and outperformed other representative approaches significantly.", "creator": "LaTeX with hyperref package"}}}