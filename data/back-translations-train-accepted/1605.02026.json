{"id": "1605.02026", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-May-2016", "title": "Training Neural Networks Without Gradients: A Scalable ADMM Approach", "abstract": "With the growing importance of large network models and enormous training datasets, GPUs have become increasingly necessary to train neural networks. This is largely because conventional optimization algorithms rely on stochastic gradient methods that don't scale well to large numbers of cores in a cluster setting. Furthermore, the convergence of all gradient methods, including batch methods, suffers from common problems like saturation effects, poor conditioning, and saddle points. This paper explores an unconventional training method that uses alternating direction methods and Bregman iteration to train networks without gradient descent steps. The proposed method reduces the network training problem to a sequence of minimization sub-steps that can each be solved globally in closed form. The proposed method is advantageous because it avoids many of the caveats that make gradient methods slow on highly non-convex problems. The method exhibits strong scaling in the distributed setting, yielding linear speedups even when split over thousands of cores.", "histories": [["v1", "Fri, 6 May 2016 18:38:45 GMT  (295kb,D)", "http://arxiv.org/abs/1605.02026v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gavin taylor", "ryan burmeister", "zheng xu 0002", "bharat singh", "ankit patel", "tom goldstein"], "accepted": true, "id": "1605.02026"}, "pdf": {"name": "1605.02026.pdf", "metadata": {"source": "META", "title": "Training Neural Networks Without Gradients:  A Scalable ADMM Approach", "authors": ["Gavin Taylor", "Ryan Burmeister", "Zheng Xu", "Bharat Singh", "Ankit Patel", "Tom Goldstein"], "emails": ["TAYLOR@USNA.EDU", "XUZH@CS.UMD.EDU", "BHARAT@CS.UMD.EDU", "ABP4@RICE.EDU", "TOMG@CS.UMD.EDU"], "sections": [{"heading": "1. Introduction", "text": "As hardware and algorithms move forward, neural network performance continues to improve for many machine learning tasks, particularly in applications where extremely large data sets are available to develop models for the 33rd International Conference on Machine Learning. Since large data sets provide results that exceed previous state-of-the-art in many areas of machine learning, researchers are willing to purchase specialized hardware such as GPUs and spend large amounts of time building models and matching hyperparameters."}, {"heading": "2. Background and notation", "text": "Although there are many variations, a typical neural network consists of L layers, each of which is defined by a linear operator Wl, and a nonlinear neural activation function hl. In a (column) vector of input activations al \u2212 1, a single layer would calculate and output the nonlinear function al = hl (Wlal \u2212 1). A network is created by layering these units together in a nested manner to calculate a composite function; in the 3-layer case, for example, this would be bef (a0; W) = W3 (h2 (W2h1 (W1a0)) (1), where W = {Wl} denotes the total ensemble of weight matrices and a0 contains input activations for each training sample (one sample per column)."}, {"heading": "2.1. What\u2019s wrong with backprop?", "text": "This year, it has come to the point where it is only a matter of time before a solution is found, in which a solution is found."}, {"heading": "2.2. Related work", "text": "A noteworthy example is the Auxiliary Coordinate Method (MAC) (Carreira-Perpina \u0301 n & Wang, 2012), which uses square penalties to approximate equality constraints. Contrary to our method, MAC requires iterative solutions to partial problems, while the method proposed here is designed so that all partial problems have closed form solutions. Likewise, unlike MAC, the method proposed here uses lagrange multipliers to enforce equality constraints exactly as we deem necessary for the formation of deeper networks.Another similar approach is the Expectation Maximization Algorithm (EM) of (Patel et al., 2015), derived from the Deep Rendering Model (DRM), a hierarchical generative model for natural images, showing that forward propagation in a deep revolutionary network corresponds with feedback on their proposed DRM."}, {"heading": "3. Alternating minimization for neural networks", "text": "The idea behind our method is to decouple the weights from the nonlinear link."}, {"heading": "3.1. Minimization sub-steps", "text": "In this section, we look at the updates for each variable in (5). The algorithm proceeds by minimizing the options for Wl, al and zl and then updating the Lagrange multipliers. \u2212 Weight update We first look at the minimization of (4) with respect to {Wl}. For each level l, the optimal solution minimizes the activation matrix (rectangular). \u2212 This is simply a problem with the least squares, and the solution is given by Wl \u00b2 l \u2212 1, where a \u2020 l \u2212 1 represents the pseudo-inverse of the (rectangular) activation matrix (rectangular) activation matrix al \u2212 1. Activation update Minimization for al is a simple leastquares problem similar to the weight update. However, in this case, the matrix al appears in two penalty terms in (4), so we must minimize it."}, {"heading": "4. Lagrange multiplier updates via method of multipliers and Bregman iteration", "text": "Given linear constraints (Yin et al., 2008), the convergence of Bregman iteration is relatively well understood for convex problems with only two separate variable blocks (He & Yuan, 2015).The convergence results also guarantee that local minimums will be achieved for non-convex targets with two blocks under certain assumptions for smoothing (Nocedal & Wright, 2006).Since the proposed scheme includes more than two coupled variable blocks and a non-smooth tightening function, it is outside the scope of known convergence results for ADMM. If ADMM is applied in (3) the conventional way using separate Lagrange multiplier vectors for each constraint, the method is highly unstable due to the destabilizing effect of a large number of coupled, non-convective conditions."}, {"heading": "4.1. Bregman interpretation", "text": "Bregman iteration (also known as the multiplier method) is a general framework for solving limited optimization problems. Methods of this kind have been widely applied in the sparse optimization literature (Yin et al., 2008). Consider the general problem of minimizing u J (u), which is subject to Au = b (9), for some convex functions J and the linear operator A. Bregman iteration repeats solvesuk + 1 \u2190 minDJ (u, uk) + 12% Au \u2212 b (10), where p-D J (uk) is a (sub-) gradient of J at uk and DJ (u, uk) = J (u) \u2212 J (uk) \u2212 uk, p-D > is the so-called Bregman distance. The iterative process (10) can be considered a minimization of the objective J, with the objective J subject to an imprecise penalty that is approximately reached Ax-D."}, {"heading": "4.2. Interpretation as method of multipliers", "text": "In addition to the Bregman interpretation, the proposed method can also be considered an approximation of the method of multipliers, which solves limited problems of formmin u J (u), which Au = b (11) for some convex functions J and (possibly non-linear) operator A. In its most general form (which does not assume linear constraints), the method proceeds with iterative updates {uk + 1 \u2190 min J (u) + < \u03bbk, A (u) \u2212 b > + \u03b22% A (u) \u2212 b \u00b2 2\u03bbk + 1%."}, {"heading": "5. Distributed implementation using data parallelism", "text": "The main advantage of the proposed minimization method is its high degree of scalability. In this section, we will explain how the method is distributed.Consider the distribution of the algorithm acrossN worknodes. The ADMM method is scaled using a data parallelization strategy in which different nodes store activations and outputs corresponding to different subsets of training data.For each level, the activation matrix is broken down into column subsets such as ai = (a1, a2, \u00b7, aN).The output matrix zl and Lagrange multipliers decompose similarly.The optimization steps for updating {al} and {zl} do not require trivial communication and parallelization. The weight matrix update requires the calculation of pseudo-inversions and products concerning the matrices {zl} and {zl}."}, {"heading": "6. Implementation details", "text": "Like many neural network training methods, the ADMM approach requires several tips and tricks to achieve maximum performance. Convergence theory for the multiplier method requires a good minimizer, which must be calculated before updating the Lagrange multipliers. If the method is initialized with random start values, the initial iterations are generally far from optimal. Therefore, we often \"warm up\" the ADMM method by performing multiple iterations without Lagrange multiplier updates, which potentially requires the user to select a large number of parameters {\u03b3i} and {\u03b2i}. We select \u03b3i = 10 and \u03b2i = 1 for all attempts reported here, and we have found that this choice works reliably for a wide range of problems and network architectures. Note that in the classic ADMM method convergence is guaranteed before any choice of square penalty parameters, if the training data is coded with training binaries."}, {"heading": "7. Experiments", "text": "In this section, we present experimental results that compare the performance of the ADMM method with other approaches, including SGD 93, conjugated gradients, and LBFGS on benchmark classification tasks. Comparisons are made on multiple axes. First, we illustrate the scaling of the approach by varying the number of available cores and calculating the computational time needed to achieve an accuracy threshold on the test set of the problem. Second, why test set classification accuracy as a function of time to compare the convergence of optimization methods. Finally, we show these comparisons on two different sets of data, a small and relatively simple, and a large and difficult dataset. The new ADMM approach was implemented in Python on a Cray XC30 supercomputer with Ivy Bridge processors, and the communication between the cores executed via MPI. SGD, conjugated datasets, gradients, B40 and FTesla-FGS are implemented in the BVIUs optimization package."}, {"heading": "7.1. SVHN", "text": "For this dataset, we optimized a network with two hidden layers of 100 and 50 nodes and ReLU activation capabilities. This is a simple problem (test accuracy is increasing fast) that does not require large volumes of data and can be easily handled by gradient-based methods on a GPU. However, Figure 1a shows that ADMM has linear scaling with cores. Although the implementations of gradient-based methods enjoy shared memory communication on the GPU, while ADMM requires CPU-to-CPU communication, strong scaling allows ADMM to CPU cores to compete with gradient-based methods on a GPU. This is clearly illustrated in Figure 1b, which shows the performance of each method on the test set as a function of time. With 1,024 computing cores, ADMM was able to achieve an average of 10 seconds of rapid passes on a GPU, showing the accuracy weakness of 13.3 in the configuration of the GP3."}, {"heading": "7.2. Higgs", "text": "For the much larger and more difficult Higgs dataset, we optimized a simple network with ReLU activation capabilities and a hidden layer of 300 nodes, as proposed in (Baldi et al., 2014).The graph illustrates the time required to optimize the network to a test predistribution accuracy of 64%; this parameter was chosen because all batch methods being tested reliably reach this level of accuracy over numerous experiments.As shown in Figure 2a, parallelization via additional cores dramatically reduces the time required and again shows linear scaling. In this much larger problem, the advantageous scaling allowed ADMM to achieve the 64% benchmark much faster than the other approaches. Figure 2b illustrates this clearly, with ADMM running on 7200 cores and achieving this benchmark in 7.8 seconds. In comparison, L-BFGS took 181 seconds and conjugated 44 minutes."}, {"heading": "8. Discussion & Conclusion", "text": "In addition to avoiding many difficulties with gradient methods (such as saturation and learning rate selection), the proposed method scales linearly up to thousands of cores. This strong scaling allows the proposed approach to trump other methods when dealing with problems with extremely large amounts of data.1 It is worth noting that L-BFGS took considerably more time to reach 64% than ADMM, but the only method was to produce a superior classifier that reached an accuracy of 75% in the test set."}, {"heading": "8.1. Looking forward", "text": "The experiments shown here represent a fairly limited range of classification problems and are not intended to demonstrate the absolute superiority of ADMM as a training method. Rather, this study is intended to be a concept proof that the reservations of gradient-based methods can be avoided by alternative minimization programs. Future work will explore the behavior of changing-direction methods in a broader context. We are particularly interested in focusing future work on recursive networks and Convolutionary Networks. Recursive networks that complicate standard gradient methods (Jaeger, 2002; Lukos Evic Evic Eius, 2012) do not present any difficulties for ADMM systems because they decouple layers by auxiliary variables. Convolutionary networks are also of interest because ADMM can handle them in principle very efficiently. If linear operators (Wl) windings represent and not dense weight matrices}, they are {the least efficient and can be solved} for the adratic problems."}, {"heading": "Acknowledgements", "text": "This work was supported by the National Science Foundation (# 1535902), the Office of Naval Research (# N0001415-1-2676 and # N0001415WX01341) and the DoD High Performance Computing Center."}], "references": [{"title": "Searching for exotic particles in high-energy physics with deep learning", "author": ["Baldi", "Pierre", "Sadowski", "Peter", "Whiteson", "Daniel"], "venue": "Nature communications,", "citeRegEx": "Baldi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Distributed optimization of deeply nested systems", "author": ["Carreira-Perpin\u00e1n", "Miguel A", "Wang", "Weiran"], "venue": "arXiv preprint arXiv:1212.5921,", "citeRegEx": "Carreira.Perpin\u00e1n et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Carreira.Perpin\u00e1n et al\\.", "year": 2012}, {"title": "The loss surfaces of multilayer networks", "author": ["Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Michael", "Arous", "G\u00e9rard Ben", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1412.0233,", "citeRegEx": "Choromanska et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2014}, {"title": "The split bregman method for l1-regularized problems", "author": ["Goldstein", "Tom", "Osher", "Stanley"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Goldstein et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goldstein et al\\.", "year": 2009}, {"title": "On non-ergodic convergence rate of douglas\u2013rachford alternating direction method of multipliers", "author": ["He", "Bingsheng", "Yuan", "Xiaoming"], "venue": "Numerische Mathematik,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the\u201d echo state network", "author": ["Jaeger", "Herbert"], "venue": "approach. GMD-Forschungszentrum Informationstechnik,", "citeRegEx": "Jaeger and Herbert.,? \\Q2002\\E", "shortCiteRegEx": "Jaeger and Herbert.", "year": 2002}, {"title": "A practical guide to applying echo state networks", "author": ["Luko\u0161evi\u010dius", "Mantas"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Luko\u0161evi\u010dius and Mantas.,? \\Q2012\\E", "shortCiteRegEx": "Luko\u0161evi\u010dius and Mantas.", "year": 2012}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["Martens", "James", "Sutskever", "Ilya"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Martens et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2011}, {"title": "A scaled conjugate gradient algorithm for fast supervised learning", "author": ["M\u00f8ller", "Martin Fodslette"], "venue": "Neural networks,", "citeRegEx": "M\u00f8ller and Fodslette.,? \\Q1993\\E", "shortCiteRegEx": "M\u00f8ller and Fodslette.", "year": 1993}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "On optimization methods for deep learning", "author": ["Ngiam", "Jiquan", "Coates", "Adam", "Lahiri", "Ahbik", "Prochnow", "Bobby", "Le", "Quoc V", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "A probabilistic theory of deep learning", "author": ["Patel", "Ankit B", "Nguyen", "Tan", "Baraniuk", "Richard"], "venue": "arXiv preprint arXiv:1504.00641,", "citeRegEx": "Patel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Patel et al\\.", "year": 2015}, {"title": "A direct adaptive method for faster backpropagation learning: The rprop algorithm", "author": ["Riedmiller", "Martin", "Braun", "Heinrich"], "venue": "In Neural Networks,", "citeRegEx": "Riedmiller et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Riedmiller et al\\.", "year": 1993}, {"title": "Accelerating hessian-free optimization for deep neural networks by implicit preconditioning and sampling", "author": ["Sainath", "Tara N", "Horesh", "Lior", "Kingsbury", "Brian", "Aravkin", "Aleksandr Y", "Ramabhadran", "Bhuvana"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Training a neural network with conjugate gradient methods", "author": ["Towsey", "Michael", "Alpsan", "Dogan", "Sztriha", "Laszlo"], "venue": "In Neural Networks,", "citeRegEx": "Towsey et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Towsey et al\\.", "year": 1995}, {"title": "Bregman iterative algorithms for \\ell 1minimization with applications to compressed sensing", "author": ["Yin", "Wotao", "Osher", "Stanley", "Goldfarb", "Donald", "Darbon", "Jerome"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Yin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2008}, {"title": "Deep learning with elastic averaging sgd", "author": ["Zhang", "Sixin", "Choromanska", "Anna E", "LeCun", "Yann"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "A conceptually similar approach is elastic averaging (Zhang et al., 2015), in which different processors simultaneously run SGD using a quadratic penalty term that prevents different processes from drifting too far from the central average.", "startOffset": 53, "endOffset": 73}, {"referenceID": 13, "context": "This approach has been suggested by numerous authors who propose batch computation methods (Ngiam et al., 2011), which compute exact gradients on each iteration using the entire dataset, including conjugate gradients (Towsey et al.", "startOffset": 91, "endOffset": 111}, {"referenceID": 18, "context": ", 2011), which compute exact gradients on each iteration using the entire dataset, including conjugate gradients (Towsey et al., 1995; M\u00f8ller, 1993), BFGS, and Hessian-free (Martens & Sutskever, 2011; Sainath et al.", "startOffset": 113, "endOffset": 148}, {"referenceID": 16, "context": ", 1995; M\u00f8ller, 1993), BFGS, and Hessian-free (Martens & Sutskever, 2011; Sainath et al., 2013) methods.", "startOffset": 46, "endOffset": 95}, {"referenceID": 1, "context": "ers contain little information about the error (Bengio et al., 1994; Riedmiller & Braun, 1993; Hochreiter & Schmidhuber, 1997).", "startOffset": 47, "endOffset": 126}, {"referenceID": 3, "context": "While recent results suggest that local minimizers of SGD are close to global minima (Choromanska et al., 2014), in practice SGD often lingers near saddle points where gradients are small (Dauphin et al.", "startOffset": 85, "endOffset": 111}, {"referenceID": 14, "context": "Another related approach is the expectation-maximization (EM) algorithm of (Patel et al., 2015), which is derived from the Deep Rendering Model (DRM), a hierarchical generative model for natural images.", "startOffset": 75, "endOffset": 95}, {"referenceID": 19, "context": "The convergence of Bregman iteration is fairly well understood in the presence of linear constraints (Yin et al., 2008).", "startOffset": 101, "endOffset": 119}, {"referenceID": 19, "context": "Methods of this type have been used extensively in the sparse optimization literature (Yin et al., 2008).", "startOffset": 86, "endOffset": 104}, {"referenceID": 12, "context": "The first is a subset of the Street View House Numbers (SVHN) dataset (Netzer et al., 2011).", "startOffset": 70, "endOffset": 91}, {"referenceID": 0, "context": "The second dataset is the far more difficult Higgs dataset (Baldi et al., 2014), consisting of a training set of 10,500,000 datapoints of 28 features each, with each datapoint labelled as either a signal process producing a Higgs boson or a background process which does not.", "startOffset": 59, "endOffset": 79}, {"referenceID": 0, "context": "For the much larger and more difficult Higgs dataset, we optimized a simple network with ReLU activation functions and a hidden layer of 300 nodes, as suggested in (Baldi et al., 2014).", "startOffset": 164, "endOffset": 184}, {"referenceID": 17, "context": "These include adding momentum terms to the weight updates and studying different initialization schemes, both of which are known to be important for gradient-based schemes (Sutskever et al., 2013).", "startOffset": 172, "endOffset": 196}], "year": 2016, "abstractText": "With the growing importance of large network models and enormous training datasets, GPUs have become increasingly necessary to train neural networks. This is largely because conventional optimization algorithms rely on stochastic gradient methods that don\u2019t scale well to large numbers of cores in a cluster setting. Furthermore, the convergence of all gradient methods, including batch methods, suffers from common problems like saturation effects, poor conditioning, and saddle points. This paper explores an unconventional training method that uses alternating direction methods and Bregman iteration to train networks without gradient descent steps. The proposed method reduces the network training problem to a sequence of minimization substeps that can each be solved globally in closed form. The proposed method is advantageous because it avoids many of the caveats that make gradient methods slow on highly non-convex problems. The method exhibits strong scaling in the distributed setting, yielding linear speedups even when split over thousands of cores.", "creator": "LaTeX with hyperref package"}}}