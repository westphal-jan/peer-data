{"id": "1605.02677", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2016", "title": "Building a Large Scale Dataset for Image Emotion Recognition: The Fine Print and The Benchmark", "abstract": "Psychological research results have confirmed that people can have different emotional reactions to different visual stimuli. Several papers have been published on the problem of visual emotion analysis. In particular, attempts have been made to analyze and predict people's emotional reaction towards images. To this end, different kinds of hand-tuned features are proposed. The results reported on several carefully selected and labeled small image data sets have confirmed the promise of such features. While the recent successes of many computer vision related tasks are due to the adoption of Convolutional Neural Networks (CNNs), visual emotion analysis has not achieved the same level of success. This may be primarily due to the unavailability of confidently labeled and relatively large image data sets for visual emotion analysis. In this work, we introduce a new data set, which started from 3+ million weakly labeled images of different emotions and ended up 30 times as large as the current largest publicly available visual emotion data set. We hope that this data set encourages further research on visual emotion analysis. We also perform extensive benchmarking analyses on this large data set using the state of the art methods including CNNs.", "histories": [["v1", "Mon, 9 May 2016 18:14:52 GMT  (4063kb,D)", "http://arxiv.org/abs/1605.02677v1", "7 pages, 7 figures, AAAI 2016"]], "COMMENTS": "7 pages, 7 figures, AAAI 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["quanzeng you", "jiebo luo", "hailin jin", "jianchao yang"], "accepted": true, "id": "1605.02677"}, "pdf": {"name": "1605.02677.pdf", "metadata": {"source": "META", "title": "Building a Large Scale Dataset for Image Emotion Recognition: The Fine Print and The Benchmark", "authors": ["Quanzeng You", "Jiebo Luo", "Hailin Jin", "Jianchao Yang"], "emails": ["jluo}@cs.rochester.edu", "hljin@adobe.com", "jianchao.yang@snapchat.com"], "sections": [{"heading": "Introduction", "text": "Psychological studies have shown that human emotions can be aroused by visual content, such as images (Lang 1979; Lang, Bradley and Cuthbert 1998; Joshi et al. 2011). On the basis of these findings, computer scientists have also recently begun to work on this research topic. In fact, unlike psychological studies, which focus mainly on studying the changes between human physiological and psychological activities on visual stimuli, most computer science work has made significant progress in recent years in predicting aroused human emotion in the face of a certain visual content. Indeed, affective computing, which aims to detect, interpret and process human emotions (http: / / en.wikipedia.org / wiki / Affective computing), has become more difficult to predict visual emotions in so far as we try to predict the emotional responses that are granted by copyright c \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aai.org)."}, {"heading": "Amusement Awe Contentment Excitement", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Anger Disgust Fear Sadness", "text": "The increasing popularity of social networks is attracting more and more people to post multimedia content on online platforms. Online users can easily add textual data, such as emotions, emotion, their uploaded images and videos. However, this textual information can only help retrieve visual content on the cognitive level (Machajdik and Hanbury 2010). Meta-text data helps bridge the affective semantic gap between images and human feelings. In (Hanjalic 2006), the authors cite visual emotions inspired by psychology and art theory to examine different groups of craft traits to examine emotional responses to visual content."}, {"heading": "Related Work", "text": "Our work is largely related to visual emotion analysis and Convolutional Neural Networks (CNNs), but recently, deep learning has achieved massive success on a wide range of artificial intelligence tasks. In particular, deep Convolutional Neural Networks have been widely used to solve traditional computer vision related problems. Deep Convolutionary Neural Networks typically consist of multiple Convolutionary Layers and several fully interconnected layers. Between the Convolutional Layers, it may also be possible to bundle layers and normalization layers. In early studies, CNNs (LeCun et al. 1998) were very successful in document recognition, where the inputs are relatively small images. Thanks to the increasing computing power of the GPU, it is now possible to train a deep Convolutionary Neural Network on large collections of images (e.g. (Krizhevsky, Sutskever, and Hinton 2012) to solve other visual image processing problems."}, {"heading": "Visual Emotion Data Sets", "text": "Several small datasets have been used to analyze visual emotions (Yanulevskaya et al. 2008; Machajdik and Hanbury 2010; Zhao et al. 2014), including (1) IAPSSubset: This dataset is a subset of the International Affective Picture System (IAPS) (Lang, Bradley, and Cuthbert 1999), which is divided into eight emotional categories, as shown in Figure 1 of a study conducted in (Mikels et al. 2005). (2) ArtPhoto: Machajdik and Hanbury (2010) have built this dataset, which contains photos of professional artists, obtaining the basic truth from the labels provided by the owner of each image. (3) Abstract Paintings: These are images that consist of both color and texture (Machajdik and Hanbury 2010). They obtain the basic truth of each image by asking people to vote for the emotions of each image. (1) Table shows the previous datasets, each showing very small sets of data, the three that there are only three."}, {"heading": "Building An Image Emotion Dataset from the Wild", "text": "There are many different types of pus bags that are able to hide until they are able to retaliate."}, {"heading": "Fine-tuning Convolutional Neural Network for Visual Emotion Analysis", "text": "Convolutional Neural Networks (CNN) have proven to be effective in image classification, for example, in achieving state-of-the-art performance in ImageNet Challenge (Krizhevsky, Sutskever, and Hinton 2012), and now there are successful applications by fine-tuning the pre-trained ImageNet model, including image style recognition (Karayev et al. 2013) and semantic segmentation (Long, Shelhamer, and Darrell 2014), using the same strategy to fine-tune the pre-trained ImageNet reference network (Jia et al. 2014), using the same neural network architecture. We only change the last layer of the neural network from 1000 to 8. The remaining layers remain the same as in the ImageNet reference network. We randomly divide the collected 23,000 samples into training (80%), testing (15%), and validation sets (5%)."}, {"heading": "Performance of Convolutional Neural Networks on Visual Emotion Analysis", "text": "After fine-tuning the pre-trained CNN model, we receive two new CNN models. In order to compare with the ImageNetCNN, we also show the results of using the SVM trained on features extracted from the second to the last level of the pre-trained ImageNet-CNN model. In particular, we use PCA to reduce the dimensionality of the features. We also try several different numbers of main components. The results are almost identical. To overcome the imbalance problem in the data, we actually adjust the weights of CNN for different classes (in our implementation we use LIBSVM3, which provides such a mechanism). Table 3 summarizes the performance of the three sets of features on the 15% randomly selected test data. The overall accuracy of the Finetuned-CNN is almost 60%. As a baseline, the visual features extracted from ImageNet-CNN only lead to an overall accuracy of about 30%."}, {"heading": "Performance of Convolutional Neural Networks on Public Existing Visual Emotion Data Set", "text": "We have described several existing datasets in section. Table 1 summarizes the statistics of the three datasets Yanaya. To deepen our knowledge, no related studies have been conducted to evaluate the performance of Convolutional Neural Networks on visual emotion analysis. However, in this section we evaluate all three deep neural network analysis models on all three datasets and compare the results with several other state-of-the-art methods to these datasets. Specifically, we extract deep visual characteristics for all images in the three datasets using trained deep neural network analysis models from the second to the last layer. In this way, we obtain a 4096 dimensional representation of the visual characteristics for each image from each deep model. Next, we follow the same evaluation routine described in (Machajdik and Hanbury 2010) and (Zhou et al. 2014). PCA is used to reduce the dimensions of the respective characteristics."}, {"heading": "Conclusions", "text": "In this paper, we present the challenging problem of analyzing visual emotions. Due to the unavailability of a large, well-labeled dataset, little research has been published to date to investigate the effects of Convolutionary Neural Networks on the analysis of visual emotions. In this work, we are introducing such a dataset and intend to make the dataset available to the research community to promote research on visual emotion analysis using Deep Learning and other learning frameworks. Meanwhile, we are also evaluating the deep visual characteristics derived from differently trained models of neural networks. Our experimental results suggest that deep Convolutionary Neural Network Functions outperform modern hand-tuned characteristics for visual emotion analysis. Furthermore, fine-tuned neural networks on emotional datasets can further improve the performance of deep neural networks. Nevertheless, the results obtained in this work are only a start for research on the use of deep learning or other learning frameworks for additional emotional analyses1."}, {"heading": "Acknowledgement", "text": "This work was generously supported by Adobe Research and the New York State CoE IDS. We thank the authors of (Zhao et al. 2014) for providing their algorithms for comparison."}], "references": [{"title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs", "author": ["D. Borth", "R. Ji", "T. Chen", "T. Breuel", "S.-F. Chang"], "venue": "ACM MM, 223\u2013232. ACM.", "citeRegEx": "Borth et al\\.,? 2013", "shortCiteRegEx": "Borth et al\\.", "year": 2013}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["D.C. Cire\u015fan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "IJCAI, 1237\u20131242.", "citeRegEx": "Cire\u015fan et al\\.,? 2011", "shortCiteRegEx": "Cire\u015fan et al\\.", "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. FeiFei"], "venue": "CVPR, 248\u2013255. IEEE.", "citeRegEx": "Deng et al\\.,? 2009", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Deep convolutional networks for scene parsing", "author": ["D. Grangier", "L. Bottou", "R. Collobert"], "venue": "ICML 2009 Deep Learning Workshop, volume 3.", "citeRegEx": "Grangier et al\\.,? 2009", "shortCiteRegEx": "Grangier et al\\.", "year": 2009}, {"title": "Extracting moods from pictures and sounds: Towards truly personalized tv", "author": ["A. Hanjalic"], "venue": "Signal processing magazine, IEEE 23(2):90\u2013100.", "citeRegEx": "Hanjalic,? 2006", "shortCiteRegEx": "Hanjalic", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation 18(7):1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "The art of color: the subjective experience and objective rationale of color", "author": ["J. Itten", "E. Van Haagen"], "venue": "Van Nostrand Reinhold New York, NY, USA.", "citeRegEx": "Itten and Haagen,? 1973", "shortCiteRegEx": "Itten and Haagen", "year": 1973}, {"title": "Can we understand van gogh\u2019s mood?: learning to infer affects from images in social networks", "author": ["J. Jia", "S. Wu", "X. Wang", "P. Hu", "L. Cai", "J. Tang"], "venue": "ACM MM, 857\u2013 860. ACM.", "citeRegEx": "Jia et al\\.,? 2012", "shortCiteRegEx": "Jia et al\\.", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093.", "citeRegEx": "Jia et al\\.,? 2014", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Aesthetics and emotions in images", "author": ["D. Joshi", "R. Datta", "E. Fedorovskaya", "Q.-T. Luong", "J.Z. Wang", "J. Li", "J. Luo"], "venue": "Signal Processing Magazine, IEEE 28(5):94\u2013115.", "citeRegEx": "Joshi et al\\.,? 2011", "shortCiteRegEx": "Joshi et al\\.", "year": 2011}, {"title": "Recognizing image style", "author": ["S. Karayev", "M. Trentacoste", "H. Han", "A. Agarwala", "T. Darrell", "A. Hertzmann", "H. Winnemoeller"], "venue": "arXiv preprint arXiv:1311.3715.", "citeRegEx": "Karayev et al\\.,? 2013", "shortCiteRegEx": "Karayev et al\\.", "year": 2013}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y.-L. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun"], "venue": "NIPS, 5.", "citeRegEx": "Kavukcuoglu et al\\.,? 2010", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Emotion, motivation, and anxiety: brain mechanisms and psychophysiology", "author": ["P.J. Lang", "M.M. Bradley", "B.N. Cuthbert"], "venue": "Biological psychiatry 44(12):1248\u20131263.", "citeRegEx": "Lang et al\\.,? 1998", "shortCiteRegEx": "Lang et al\\.", "year": 1998}, {"title": "International affective picture system (iaps): Technical manual and affective ratings", "author": ["P.J. Lang", "M.M. Bradley", "B.N. Cuthbert"], "venue": null, "citeRegEx": "Lang et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lang et al\\.", "year": 1999}, {"title": "A bio-informational theory of emotional imagery", "author": ["P.J. Lang"], "venue": "Psychophysiology 16(6):495\u2013512.", "citeRegEx": "Lang,? 1979", "shortCiteRegEx": "Lang", "year": 1979}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation 1(4):541\u2013551.", "citeRegEx": "LeCun et al\\.,? 1989", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Convolutional networks and applications in vision", "author": ["Y. LeCun", "K. Kavukcuoglu", "C. Farabet"], "venue": "ISCAS, 253\u2013256. IEEE.", "citeRegEx": "LeCun et al\\.,? 2010", "shortCiteRegEx": "LeCun et al\\.", "year": 2010}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4038.", "citeRegEx": "Long et al\\.,? 2014", "shortCiteRegEx": "Long et al\\.", "year": 2014}, {"title": "On shape and the computability of emotions", "author": ["X. Lu", "P. Suryanarayan", "Adams, Jr., R.B.", "J. Li", "M.G. Newman", "J.Z. Wang"], "venue": "ACM MM, 229\u2013238. ACM.", "citeRegEx": "Lu et al\\.,? 2012", "shortCiteRegEx": "Lu et al\\.", "year": 2012}, {"title": "Rapid: Rating pictorial aesthetics using deep learning", "author": ["X. Lu", "Z. Lin", "H. Jin", "J. Yang", "J.Z. Wang"], "venue": "ACM MM, 457\u2013466. ACM.", "citeRegEx": "Lu et al\\.,? 2014", "shortCiteRegEx": "Lu et al\\.", "year": 2014}, {"title": "Affective image classification using features inspired by psychology and art theory", "author": ["J. Machajdik", "A. Hanbury"], "venue": "ACM MM, 83\u201392. ACM.", "citeRegEx": "Machajdik and Hanbury,? 2010", "shortCiteRegEx": "Machajdik and Hanbury", "year": 2010}, {"title": "Emotional category data on images from the international affective picture system", "author": ["J.A. Mikels", "B.L. Fredrickson", "G.R. Larkin", "C.M. Lindberg", "S.J. Maglio", "P.A. Reuter-Lorenz"], "venue": "Behavior research methods 37(4):626\u2013 630.", "citeRegEx": "Mikels et al\\.,? 2005", "shortCiteRegEx": "Mikels et al\\.", "year": 2005}, {"title": "Ava: A large-scale database for aesthetic visual analysis", "author": ["N. Murray", "L. Marchesotti", "F. Perronnin"], "venue": "CVPR, 2408\u20132415. IEEE.", "citeRegEx": "Murray et al\\.,? 2012", "shortCiteRegEx": "Murray et al\\.", "year": 2012}, {"title": "A multilayer hybrid framework for dimensional emotion classification", "author": ["M.A. Nicolaou", "H. Gunes", "M. Pantic"], "venue": "ACM MM, 933\u2013936. ACM.", "citeRegEx": "Nicolaou et al\\.,? 2011", "shortCiteRegEx": "Nicolaou et al\\.", "year": 2011}, {"title": "Effects of color on emotions", "author": ["P. Valdez", "A. Mehrabian"], "venue": "Journal of Experimental Psychology 123(4):394.", "citeRegEx": "Valdez and Mehrabian,? 1994", "shortCiteRegEx": "Valdez and Mehrabian", "year": 1994}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research 9(25792605):85.", "citeRegEx": "Maaten and Hinton,? 2008", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Image retrieval by emotional semantics: A study of emotional space and feature extraction", "author": ["W. Wei-ning", "Y. Ying-lin", "J. Sheng-ming"], "venue": "Systems, Man and Cybernetics, IEEE International Conference on, volume 4, 3534\u2013 3539. IEEE.", "citeRegEx": "Wei.ning et al\\.,? 2006", "shortCiteRegEx": "Wei.ning et al\\.", "year": 2006}, {"title": "Emotional valence categorization using holistic image features", "author": ["V. Yanulevskaya", "J. Van Gemert", "K. Roth", "A.-K. Herbold", "N. Sebe", "J.-M. Geusebroek"], "venue": "ICIP, 101\u2013 104. IEEE.", "citeRegEx": "Yanulevskaya et al\\.,? 2008", "shortCiteRegEx": "Yanulevskaya et al\\.", "year": 2008}, {"title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks", "author": ["Q. You", "J. Luo", "H. Jin", "J. Yang"], "venue": "AAAI.", "citeRegEx": "You et al\\.,? 2015", "shortCiteRegEx": "You et al\\.", "year": 2015}, {"title": "Exploring principles-of-art features for image emotion recognition", "author": ["S. Zhao", "Y. Gao", "X. Jiang", "H. Yao", "T.-S. Chua", "X. Sun"], "venue": "Proceedings of the ACM International Conference on Multimedia, MM \u201914, 47\u201356. New York, NY, USA: ACM.", "citeRegEx": "Zhao et al\\.,? 2014", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Advances in Neural Information Processing Systems, 487\u2013495.", "citeRegEx": "Zhou et al\\.,? 2014", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "images (Lang 1979; Lang, Bradley, and Cuthbert 1998; Joshi et al. 2011).", "startOffset": 7, "endOffset": 71}, {"referenceID": 9, "context": "images (Lang 1979; Lang, Bradley, and Cuthbert 1998; Joshi et al. 2011).", "startOffset": 7, "endOffset": 71}, {"referenceID": 22, "context": "However, these textual information can only help the retrieval of multimedia content in the cognitive level (Machajdik and Hanbury 2010), i.", "startOffset": 108, "endOffset": 136}, {"referenceID": 4, "context": "In (Hanjalic 2006), the authors call visual emotion prediction affective content analysis.", "startOffset": 3, "endOffset": 18}, {"referenceID": 26, "context": "For example, based on art theory (Itten and Van Haagen 1973; Valdez and Mehrabian 1994), Machajdik and Hanbury (Machajdik and Hanbury 2010) defined eight different kinds of pixel level features (e.", "startOffset": 33, "endOffset": 87}, {"referenceID": 22, "context": "For example, based on art theory (Itten and Van Haagen 1973; Valdez and Mehrabian 1994), Machajdik and Hanbury (Machajdik and Hanbury 2010) defined eight different kinds of pixel level features (e.", "startOffset": 111, "endOffset": 139}, {"referenceID": 31, "context": "In another recent work (Zhao et al. 2014), principles-or-art based features are extracted to classify emotions.", "startOffset": 23, "endOffset": 41}, {"referenceID": 16, "context": "digit recognition (LeCun et al. 1989; Hinton, Osindero, and Teh 2006), image classification (Cire\u015fan et al.", "startOffset": 18, "endOffset": 69}, {"referenceID": 1, "context": "1989; Hinton, Osindero, and Teh 2006), image classification (Cire\u015fan et al. 2011; Krizhevsky, Sutskever, and Hinton 2012), aesthetics estimation (Lu et al.", "startOffset": 60, "endOffset": 121}, {"referenceID": 21, "context": "2011; Krizhevsky, Sutskever, and Hinton 2012), aesthetics estimation (Lu et al. 2014) and scene recognition (Zhou et al.", "startOffset": 69, "endOffset": 85}, {"referenceID": 32, "context": "2014) and scene recognition (Zhou et al. 2014).", "startOffset": 28, "endOffset": 46}, {"referenceID": 2, "context": "From ImageNet (Deng et al. 2009) to AVA dataset (Murray, Marchesotti, and Perronnin 2012) and the very recent Places Database (Zhou et al.", "startOffset": 14, "endOffset": 32}, {"referenceID": 32, "context": "2009) to AVA dataset (Murray, Marchesotti, and Perronnin 2012) and the very recent Places Database (Zhou et al. 2014), the availability of these data sets have significantly promoted the development of new algorithms on these research areas.", "startOffset": 99, "endOffset": 117}, {"referenceID": 30, "context": "(You et al. 2015) employed CNNs to address visual sentiment analysis, which tries to bridge the high-level, abstract sentiments concept and the image pixels.", "startOffset": 0, "endOffset": 17}, {"referenceID": 17, "context": "In early studies, CNNs (LeCun et al. 1998) have been very successful in document recognition, where the inputs are relatively small images.", "startOffset": 23, "endOffset": 42}, {"referenceID": 11, "context": "(Krizhevsky, Sutskever, and Hinton 2012)), to solve other computer vision problems, such as scene parsing (Grangier, Bottou, and Collobert 2009), feature learning (LeCun, Kavukcuoglu, and Farabet 2010), visual recognition (Kavukcuoglu et al. 2010) and image classification (Krizhevsky, Sutskever, and Hinton 2012).", "startOffset": 222, "endOffset": 247}, {"referenceID": 20, "context": "Currently, most of the works on visual emotion analysis can be classified into either dimensional approach (Nicolaou, Gunes, and Pantic 2011; Lu et al. 2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al.", "startOffset": 107, "endOffset": 157}, {"referenceID": 22, "context": "2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al. 2013; Zhao et al. 2014), where the former represents emotion in a continuous two dimensional space and in the later model each emotion is a distinct class.", "startOffset": 30, "endOffset": 95}, {"referenceID": 0, "context": "2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al. 2013; Zhao et al. 2014), where the former represents emotion in a continuous two dimensional space and in the later model each emotion is a distinct class.", "startOffset": 30, "endOffset": 95}, {"referenceID": 31, "context": "2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al. 2013; Zhao et al. 2014), where the former represents emotion in a continuous two dimensional space and in the later model each emotion is a distinct class.", "startOffset": 30, "endOffset": 95}, {"referenceID": 0, "context": "2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al. 2013; Zhao et al. 2014), where the former represents emotion in a continuous two dimensional space and in the later model each emotion is a distinct class. We focus on the categorical approach, which has been studied in several previous works. Jia et al. (2012) extract color features from the images.", "startOffset": 59, "endOffset": 334}, {"referenceID": 0, "context": "2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al. 2013; Zhao et al. 2014), where the former represents emotion in a continuous two dimensional space and in the later model each emotion is a distinct class. We focus on the categorical approach, which has been studied in several previous works. Jia et al. (2012) extract color features from the images. With additional social relationships, they build a factor graph model for the prediction of emotions. Inspired by art and psychology theory, Machajdik and Hanbury (2010) proposed richer hand-tuned features, including color, texture, composition and content features.", "startOffset": 59, "endOffset": 544}, {"referenceID": 0, "context": "2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al. 2013; Zhao et al. 2014), where the former represents emotion in a continuous two dimensional space and in the later model each emotion is a distinct class. We focus on the categorical approach, which has been studied in several previous works. Jia et al. (2012) extract color features from the images. With additional social relationships, they build a factor graph model for the prediction of emotions. Inspired by art and psychology theory, Machajdik and Hanbury (2010) proposed richer hand-tuned features, including color, texture, composition and content features. Furthermore, by exploring the principles of art, Zhao et al. (2014) defined more robust and invariant visual features, such as balance, variety, and gradation.", "startOffset": 59, "endOffset": 709}, {"referenceID": 29, "context": "Visual Emotion Data Sets Several small data sets have been have been used for visual emotion analysis (Yanulevskaya et al. 2008; Machajdik and Hanbury 2010; Zhao et al. 2014), including (1) IAPSSubset: This data set is a subset of the International Affective Picture System (IAPS) (Lang, Bradley, and Cuthbert 1999).", "startOffset": 102, "endOffset": 174}, {"referenceID": 22, "context": "Visual Emotion Data Sets Several small data sets have been have been used for visual emotion analysis (Yanulevskaya et al. 2008; Machajdik and Hanbury 2010; Zhao et al. 2014), including (1) IAPSSubset: This data set is a subset of the International Affective Picture System (IAPS) (Lang, Bradley, and Cuthbert 1999).", "startOffset": 102, "endOffset": 174}, {"referenceID": 31, "context": "Visual Emotion Data Sets Several small data sets have been have been used for visual emotion analysis (Yanulevskaya et al. 2008; Machajdik and Hanbury 2010; Zhao et al. 2014), including (1) IAPSSubset: This data set is a subset of the International Affective Picture System (IAPS) (Lang, Bradley, and Cuthbert 1999).", "startOffset": 102, "endOffset": 174}, {"referenceID": 23, "context": "This data set is categorized into eight emotional categories as shown in Figure 1 in a study conducted in (Mikels et al. 2005).", "startOffset": 106, "endOffset": 126}, {"referenceID": 22, "context": "(3) Abstract Paintings: These are images consisting of both color and texture from (Machajdik and Hanbury 2010).", "startOffset": 83, "endOffset": 111}, {"referenceID": 15, "context": "2014), including (1) IAPSSubset: This data set is a subset of the International Affective Picture System (IAPS) (Lang, Bradley, and Cuthbert 1999). This data set is categorized into eight emotional categories as shown in Figure 1 in a study conducted in (Mikels et al. 2005). (2) ArtPhoto: Machajdik and Hanbury (2010) built this data set, which contains photos by professional artists.", "startOffset": 113, "endOffset": 319}, {"referenceID": 22, "context": "Therefore, if we employ the same methodology (5-fold Cross Validation within each data set) (Machajdik and Hanbury 2010; Zhao et al. 2014), we may have only several images in the training data.", "startOffset": 92, "endOffset": 138}, {"referenceID": 31, "context": "Therefore, if we employ the same methodology (5-fold Cross Validation within each data set) (Machajdik and Hanbury 2010; Zhao et al. 2014), we may have only several images in the training data.", "startOffset": 92, "endOffset": 138}, {"referenceID": 2, "context": "The above results suggest that the previous efforts on visual emotion analysis deal with small emotion-centric data sets compared with other vision data sets, such as ImageNet (Deng et al. 2009) and Places (Zhou et al.", "startOffset": 176, "endOffset": 194}, {"referenceID": 32, "context": "2009) and Places (Zhou et al. 2014).", "startOffset": 17, "endOffset": 35}, {"referenceID": 23, "context": "In this work, we use the same eight emotions defined in Table 1, which is derived from a psychological study in (Mikels et al. 2005).", "startOffset": 112, "endOffset": 132}, {"referenceID": 7, "context": "Using the similar approach in (Jia et al. 2012), we query the image search engines (Flickr and Instagram) using the eight emotions as keywords.", "startOffset": 30, "endOffset": 47}, {"referenceID": 10, "context": "Meanwhile, there are also successful applications by fine-tuning the pre-trained ImageNet model, including recognizing image style (Karayev et al. 2013) and semantic segmentation (Long, Shelhamer, and Darrell 2014).", "startOffset": 131, "endOffset": 152}, {"referenceID": 8, "context": "In this work, we employ the same strategy to fine-tune the pre-trained ImageNet reference network (Jia et al. 2014).", "startOffset": 98, "endOffset": 115}, {"referenceID": 7, "context": "Meanwhile, we also employ the weak labels (Jia et al. 2012) to fine-tune another model as described in (You et al.", "startOffset": 42, "endOffset": 59}, {"referenceID": 30, "context": "2012) to fine-tune another model as described in (You et al. 2015).", "startOffset": 49, "endOffset": 66}, {"referenceID": 22, "context": "Next, we follow the same evaluation routine described in (Machajdik and Hanbury 2010) and (Zhou et al.", "startOffset": 57, "endOffset": 85}, {"referenceID": 32, "context": "Next, we follow the same evaluation routine described in (Machajdik and Hanbury 2010) and (Zhou et al. 2014).", "startOffset": 90, "endOffset": 108}, {"referenceID": 22, "context": "Also, we assign larger penalties to true negative samples in the SVM training stage in order to optimize the per class true positive rate as suggested by both (Machajdik and Hanbury 2010) and (Zhou et al.", "startOffset": 159, "endOffset": 187}, {"referenceID": 32, "context": "Also, we assign larger penalties to true negative samples in the SVM training stage in order to optimize the per class true positive rate as suggested by both (Machajdik and Hanbury 2010) and (Zhou et al. 2014).", "startOffset": 192, "endOffset": 210}, {"referenceID": 29, "context": "(Yanulevskaya et al. 2008), Machajdik and Hanbury (Machajdik and Hanbury 2010) and Zhao et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 22, "context": "2008), Machajdik and Hanbury (Machajdik and Hanbury 2010) and Zhao et al.", "startOffset": 29, "endOffset": 57}, {"referenceID": 32, "context": "(Zhou et al. 2014).", "startOffset": 0, "endOffset": 18}, {"referenceID": 22, "context": "Figure 5: Per-class true positive rates of Machajdik (Machajdik and Hanbury 2010), Yanulevskaya (Yanulevskaya et al.", "startOffset": 53, "endOffset": 81}, {"referenceID": 29, "context": "Figure 5: Per-class true positive rates of Machajdik (Machajdik and Hanbury 2010), Yanulevskaya (Yanulevskaya et al. 2008), Wang (Wei-ning, Ying-lin, and Sheng-ming 2006), Zhao (Zhou et al.", "startOffset": 96, "endOffset": 122}, {"referenceID": 32, "context": "2008), Wang (Wei-ning, Ying-lin, and Sheng-ming 2006), Zhao (Zhou et al. 2014), ImageNet-CNN, Noisy-Fine-tunedCNN and Fine-tuned-CNN on IAPS-Subset data set.", "startOffset": 60, "endOffset": 78}, {"referenceID": 22, "context": "Figure 6: Per-class true positive rates of Machajdik (Machajdik and Hanbury 2010), Yanulevskaya (Yanulevskaya et al.", "startOffset": 53, "endOffset": 81}, {"referenceID": 29, "context": "Figure 6: Per-class true positive rates of Machajdik (Machajdik and Hanbury 2010), Yanulevskaya (Yanulevskaya et al. 2008), Wang (Wei-ning, Ying-lin, and Sheng-ming 2006), Zhao (Zhou et al.", "startOffset": 96, "endOffset": 122}, {"referenceID": 32, "context": "2008), Wang (Wei-ning, Ying-lin, and Sheng-ming 2006), Zhao (Zhou et al. 2014), ImageNet-CNN, Noisy-Fine-tunedCNN and Fine-tuned-CNN on Abstract Paintings data set.", "startOffset": 60, "endOffset": 78}, {"referenceID": 22, "context": "Figure 7: Per-class true positive rates of Machajdik (Machajdik and Hanbury 2010), Yanulevskaya (Yanulevskaya et al.", "startOffset": 53, "endOffset": 81}, {"referenceID": 29, "context": "Figure 7: Per-class true positive rates of Machajdik (Machajdik and Hanbury 2010), Yanulevskaya (Yanulevskaya et al. 2008), Wang (Wei-ning, Ying-lin, and Sheng-ming 2006), Zhao (Zhou et al.", "startOffset": 96, "endOffset": 122}, {"referenceID": 32, "context": "2008), Wang (Wei-ning, Ying-lin, and Sheng-ming 2006), Zhao (Zhou et al. 2014), ImageNet-CNN, Noisy-Fine-tunedCNN and Fine-tuned-CNN on ArtPhoto data set.", "startOffset": 60, "endOffset": 78}, {"referenceID": 31, "context": "We thank the authors of (Zhao et al. 2014) for providing their algorithms for comparison.", "startOffset": 24, "endOffset": 42}], "year": 2016, "abstractText": "Psychological research results have confirmed that people can have different emotional reactions to different visual stimuli. Several papers have been published on the problem of visual emotion analysis. In particular, attempts have been made to analyze and predict people\u2019s emotional reaction towards images. To this end, different kinds of hand-tuned features are proposed. The results reported on several carefully selected and labeled small image data sets have confirmed the promise of such features. While the recent successes of many computer vision related tasks are due to the adoption of Convolutional Neural Networks (CNNs), visual emotion analysis has not achieved the same level of success. This may be primarily due to the unavailability of confidently labeled and relatively large image data sets for visual emotion analysis. In this work, we introduce a new data set, which started from 3+ million weakly labeled images of different emotions and ended up 30 times as large as the current largest publicly available visual emotion data set. We hope that this data set encourages further research on visual emotion analysis. We also perform extensive benchmarking analyses on this large data set using the state of the art methods including CNNs.", "creator": "TeX"}}}