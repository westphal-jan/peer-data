{"id": "1605.03004", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "MUST-CNN: A Multilayer Shift-and-Stitch Deep Convolutional Architecture for Sequence-Based Protein Structure Prediction", "abstract": "Predicting protein properties such as solvent accessibility and secondary structure from its primary amino acid sequence is an important task in bioinformatics. Recently, a few deep learning models have surpassed the traditional window based multilayer perceptron. Taking inspiration from the image classification domain we propose a deep convolutional neural network architecture, MUST-CNN, to predict protein properties. This architecture uses a novel multilayer shift-and-stitch (MUST) technique to generate fully dense per-position predictions on protein sequences. Our model is significantly simpler than the state-of-the-art, yet achieves better results. By combining MUST and the efficient convolution operation, we can consider far more parameters while retaining very fast prediction speeds. We beat the state-of-the-art performance on two large protein property prediction datasets.", "histories": [["v1", "Tue, 10 May 2016 13:31:52 GMT  (171kb,D)", "http://arxiv.org/abs/1605.03004v1", "8 pages ; 3 figures ; deep learning based sequence-sequence prediction. in AAAI 2016"]], "COMMENTS": "8 pages ; 3 figures ; deep learning based sequence-sequence prediction. in AAAI 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zeming lin", "jack lanchantin", "yanjun qi"], "accepted": true, "id": "1605.03004"}, "pdf": {"name": "1605.03004.pdf", "metadata": {"source": "CRF", "title": "MUST-CNN: A Multilayer Shift-and-Stitch Deep Convolutional Architecture for Sequence-based Protein Structure Prediction", "authors": ["Zeming Lin", "Jack Lanchantin", "Yanjun Qi"], "emails": ["zl4ry@virginia.edu", "jjl5sw@virginia.edu", "yq2h@virginia.edu"], "sections": [{"heading": "Introduction", "text": "It is easy to determine the sequence of a protein sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence solvent availability. These properties are hypothetically determined almost exclusively by the primary structure, but it is still difficult to capture them on a large scale. To find a label for each single amino acid in the input protector sequence, the MLP networks must adopt a multi-layered approach whereby a single label is predicted by hurling the amino acid and the surrounding amino acids through the network sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence"}, {"heading": "Related Works", "text": "Two of the most commonly used algorithms in the bioinformatics literature for predicting protein properties are PSIPRED (Jones 1999) and Jpred (Drozdetskiy et al. 2015). PSIPRED 3.2, which uses a two-layer MLP approach, claims a three-layer protein accuracy (Q3) of 81.6%. The Jpred algorithm uses a very similar structure of a two-layer MLP network. However, Jpred considers more properties and Usesar Xiv 1 [cs.L] 10 May 201 6a Jury approach with multiple models (Cuff and Barton 2000)."}, {"heading": "Method: MUST-CNN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Convolutional Neural Networks (CNN)", "text": "Similarly, we use a 1D convolution for the protein sequence labeling problem. A convolution to sequential data tensor X of size T \u00b7 nin with length T, core size k and hidden layer size nin has an output Y of size T \u00b7 nout: Yt, i = n \u00b2 n \u00b2. We try to apply three different nonlinearity functions in our experiments: the hyperbolic tangent, rectified linear units (ReLU) and piecemeal rectified linear units (ReLU). The hyperbolic tangent is a linear unit (ReLU), and the reflected linear unit is the reticulated linear unit (PReLU)."}, {"heading": "Multilayer Shift-and-Stitch (MUST)", "text": "In fact, most of them are able to play by the rules they have set themselves."}, {"heading": "End-to-end Architecture", "text": "In this section we describe the end-to-end model structure of the MUST-CNN and how we are able to train it to make full dense pro-position predictions. Entering into the network is a one-hot encoding of an amino acid pair sequence and the PSI-BLAST position specific scoring matrix (PSSM), which is described in more detail in the Experiments subsection Feature section. Dropout is applied directly to the amino acid input and then fed through a lookup table, similar to (Collobert et al. 2011), to construct an embedding representation for each amino acid. Then, the features from the amino acid embedding are directly connected to the PSSM matrix along the feature dimension and fed into the deep evolutionary mesh. To apply the shift-and-stitch technique, we shift the amino acid sequences according to the height of the pooling layer."}, {"heading": "Connecting to Previous Studies", "text": "MUST-CNN is closely related to three previous models: OverFeat (Sermanet et et al. 2013), Generative Stochastic Networks (GSNs) (Zhou and Troyanskaya 2014), and Conditional Neural Fields (CNFs) (Wang et al. 2011).CNFs are synonymous with a Conditional Random Field (CRF) with a Convolutionary Feature Extractor. As far as we know, the authors are implementing a window-based version with MLP networks. Their model, although able to take into account the entire sequence due to the use of a CRF, is unable to build deeper representations of models. Our model uses multiple convolutionary layers and multitasking to classify each amino acid into one of a few classes across multiple tasks. Our models are much deeper, and can therefore learn more efficient representations for complex dependencies. GSN is similar to a restricted Boltzmann machine with interconnected states."}, {"heading": "Feature", "text": "The characteristics we use are (1) single amino acids and (2) PSI-BLAST information (Altschul et al. 1997) of a protein sequence. Each amino acid a, where A is the dictionary of amino acids, is encoded in R | A | as a uniform vector, that is, the encoding x of the i-th amino acid has xi = 1 and xj 6 = i = 0. PSI-BLAST generates a PSSM of size T x 20 for a T-longitudinal sequence, where a higher value represents a higher probability that the ith amino acid replaces the current amino acid in other species. Generally, two amino acids that are interchangeable in the PSSM indicate that they are also interchangeable in the protein without significantly altering the functionality of the protein."}, {"heading": "Data", "text": "We used two large datasets with protein properties in our experiments. Train, validation and test splits are in Table 1. The two datasets we use are as follows: 4prot Derived from (Qi et al. 2012), we use a trainvalidation test split, in which the model is trained on the training set, selected on validation set results and the best results obtained by testing on the test set. CullPDB Derived from (Zhou and Troyanskaya 2014), we opt for the CullPDB dataset, in which sequences with > 25% identity were removed with the CB513 dataset. Train and validation sets are derived from CullPDB, while the test set is CB513 to compare the results with (Kaae S\u00f8nderby and Winther 2014; Wang et al. 2011)."}, {"heading": "Tasks", "text": "Both datasets have been formatted to have the same multi-task representation. These are the four classification tasks on which we tested our system: dssp The 8-class secondary structure prediction task from the dssp database (Touw et al. 2015). The class names are H = alpha-helix, B = residues in isolated beta bridge, E = elongated strand, G = 3-helix, I = 5-helix, T = hydrogen-bound rotation, S = bending, L = grinding. ssp A broken down version of the 8-class prediction task, since many secondary structure prediction algorithms use a 3-class approach instead of the 8-class approach specified in dssp. {H, G} \u2192 H = helix, {B, E} \u2192 B = beta sheet and {I, S, T, L} \u2022 C = coilsar Relative solvent accessibility to a solvent with a more accessible surface than the AO-mineral."}, {"heading": "Training", "text": "Model Selection (Small Model) We use Bayesian Optimization (Snoek, Larochelle and Adams 2012) to find the optimal model.Model Selection (Large Model) The large model was found using the Spearmint Package (Snoek 2015).The specific architectures we found are detailed in Table 2. Bayesian Optimization could not be used because large models were too slow to train.Model Selection (Large Model) The large model was also achieved by taking into account every single task and starting training from the models learned in the common model.That is, we trained a model whose parameters were the same as the multitask model, but the loss function included only one specific task. The loss function for the task, the sequence s indexed from t = 1, is."}, {"heading": "Results", "text": "Most combinations of parameter optimizations within the optimal learning rate lead to an improvement in average accuracy of less than 1%. By using maxipooling with shift-and-stitch in our model, our average accuracy improved by almost 0.5% with hardly any computational slowdown. Our results on the 4prot datasets are detailed in Table 3. The small model we found through Bayesian Optimization has about as many parameters as previous state-of-the-art models, but we see that it exceeds the network created by (Qi et al. 2012) for all tasks. Fine-tuning individual models is necessary for good performance, which implies that it may be easier to create an MLP subclassifier for each task rather than assuming linearity."}, {"heading": "Discussion", "text": "We have described a multi-layered shift and stitch revolutionary architecture for sequence prediction. We are using ideas from image classification to train a deep revolutionary network for sequence marking. We are the first to use multi-layered shifts and stitches for the protein model Q8 CNF (Wang et al. 2011).649 GSN (Zhou and Troyanskaya 2014).664 LSTM (Kaae S\u00f8nderby and Winther 2014).674 MUST-CNN (Our).684Table 5: Q8 Accuracy training on the CullPDB dataset and testing on the CB513. Testing takes approximately the same time as on the 4prot dataset. We use the same architecture as MUSTCNN large, detailed in Table 2.Quenches, to generate the results per position."}], "references": [{"title": "D", "author": ["S.F. Altschul", "T.L. Madden", "A.A. Sch\u00e4ffer", "J. Zhang", "Z. Zhang", "W. Miller", "Lipman"], "venue": "J.", "citeRegEx": "Altschul et al. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "and Weston", "author": ["R. Collobert"], "venue": "J.", "citeRegEx": "Collobert and Weston 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert"], "venue": "The Journal of Machine Learning Research 12:2493\u20132537", "citeRegEx": "Collobert,? \\Q2011\\E", "shortCiteRegEx": "Collobert", "year": 2011}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Kavukcuoglu Collobert", "R. Farabet 2011] Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "G", "author": ["J.A. Cuff", "Barton"], "venue": "J.", "citeRegEx": "Cuff and Barton 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "G", "author": ["A. Drozdetskiy", "C. Cole", "J. Procter", "Barton"], "venue": "J.", "citeRegEx": "Drozdetskiy et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["Bordes Glorot", "X. Bengio 2011] Glorot", "A. Bordes", "Y. Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["He"], "venue": "arXiv preprint arXiv:1502.01852", "citeRegEx": "He,? \\Q2015\\E", "shortCiteRegEx": "He", "year": 2015}, {"title": "D", "author": ["Jones"], "venue": "T.", "citeRegEx": "Jones 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "and Winther", "author": ["S. Kaae S\u00f8nderby"], "venue": "O.", "citeRegEx": "Kaae S\u00f8nderby and Winther 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Y", "author": ["Kim"], "venue": "2014. Convolutional Neural Networks for Sentence Classification. arXiv:1408.5882 [cs]. arXiv:", "citeRegEx": "Kim 2014", "shortCiteRegEx": null, "year": 1408}, {"title": "P", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "Haffner"], "venue": "1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11):2278\u2013", "citeRegEx": "Lecun et al. 1998", "shortCiteRegEx": null, "year": 2324}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Shelhamer Long", "J. Darrell 2014] Long", "E. Shelhamer", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4038", "citeRegEx": "Long et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Long et al\\.", "year": 2014}, {"title": "and Baldi", "author": ["C.N. Magnan"], "venue": "P.", "citeRegEx": "Magnan and Baldi 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["P.H.O. Pinheiro", "Collobert"], "venue": "2013. Recurrent Convolutional Neural Networks for Scene Parsing. arXiv:1306.2795 [cs]. arXiv:", "citeRegEx": "Pinheiro and Collobert 2013", "shortCiteRegEx": null, "year": 1306}, {"title": "W", "author": ["Y. Qi", "M. Oja", "J. Weston", "Noble"], "venue": "S.", "citeRegEx": "Qi et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "S", "author": ["D. Scherer", "A. M\u00fcller", "Behnke"], "venue": "2010. Evaluation of pooling operations in convolutional architectures for object recognition. In Artificial Neural Networks\u2013ICANN", "citeRegEx": "Scherer. M\u00fcller. and Behnke 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229", "author": ["Sermanet"], "venue": null, "citeRegEx": "Sermanet,? \\Q2013\\E", "shortCiteRegEx": "Sermanet", "year": 2013}, {"title": "R", "author": ["J. Snoek", "H. Larochelle", "Adams"], "venue": "P.", "citeRegEx": "Snoek. Larochelle. and Adams 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Srivastava,? \\Q2014\\E", "shortCiteRegEx": "Srivastava", "year": 2014}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le"], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "Rabinovich"], "venue": "2014. Going Deeper with Convolutions. arXiv:1409.4842 [cs]. arXiv:", "citeRegEx": "Szegedy et al. 2014", "shortCiteRegEx": null, "year": 1409}, {"title": "W", "author": ["Touw"], "venue": "G.; Baakman, C.; Black, J.; te Beek, T. A. H.; Krieger, E.; Joosten, R. P.; and Vriend, G.", "citeRegEx": "Touw et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Protein 8-class secondary structure prediction using conditional neural fields", "author": ["Wang"], "venue": "Proteomics", "citeRegEx": "Wang,? \\Q2011\\E", "shortCiteRegEx": "Wang", "year": 2011}, {"title": "O", "author": ["W. Zaremba", "I. Sutskever", "Vinyals"], "venue": "2014. Recurrent Neural Network Regularization. arXiv:1409.2329 [cs]. arXiv:", "citeRegEx": "Zaremba. Sutskever. and Vinyals 2014", "shortCiteRegEx": null, "year": 1409}, {"title": "O", "author": ["J. Zhou", "Troyanskaya"], "venue": "G.", "citeRegEx": "Zhou and Troyanskaya 2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "Predicting protein properties such as solvent accessibility and secondary structure from its primary amino acid sequence is an important task in bioinformatics. Recently, a few deep learning models have surpassed the traditional window based multilayer perceptron. Taking inspiration from the image classification domain we propose a deep convolutional neural network architecture, MUST-CNN, to predict protein properties. This architecture uses a novel multilayer shift-and-stitch (MUST) technique to generate fully dense per-position predictions on protein sequences. Our model is significantly simpler than the state-of-the-art, yet achieves better results. By combining MUST and the efficient convolution operation, we can consider far more parameters while retaining very fast prediction speeds. We beat the state-of-the-art performance on two large protein property prediction datasets.", "creator": "LaTeX with hyperref package"}}}