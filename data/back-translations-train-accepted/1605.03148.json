{"id": "1605.03148", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "Coverage Embedding Models for Neural Machine Translation", "abstract": "In this paper, we enhance the attention-based neural machine translation by adding an explicit coverage embedding model to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector, and then keeps updating it with a gated recurrent unit as the translation goes. All the initialized coverage embeddings and updating matrix are learned in the training procedure. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.", "histories": [["v1", "Tue, 10 May 2016 18:44:34 GMT  (913kb,D)", "http://arxiv.org/abs/1605.03148v1", "6 pages"], ["v2", "Mon, 29 Aug 2016 15:10:34 GMT  (1106kb,D)", "http://arxiv.org/abs/1605.03148v2", "6 pages; In Proceddings of EMNLP 2016"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haitao mi", "baskaran sankaran", "zhiguo wang", "abe ittycheriah"], "accepted": true, "id": "1605.03148"}, "pdf": {"name": "1605.03148.pdf", "metadata": {"source": "CRF", "title": "A Coverage Embedding Model for Neural Machine Translation", "authors": ["Haitao Mi Baskaran", "Sankaran Zhiguo Wang", "Abe Ittycheriah"], "emails": ["abei}@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Attention in each step, however, only focuses on the previous hidden state and the previous target word; there is no history or coverage information typical of each source word. As a result, this type of model suffers from problems of repetition or falling translations. Traditional statistical machine translation (SMT) considers systems (e.g. Koehn, 2004; Chiang, 2005) to be the answer to the above problems by using a source-side coverage vector to explicitly indicate which words have been translated or are falling."}, {"heading": "2 Neural Machine Translation", "text": "As shown in Figure 1, the attention-based neural machine translation (Bahdanau et al., 2014) is an encoder-decoder network. The encoder uses a bidirectional recursive neural network to encode the source sentence x = (x1,..., xl), where l is the sentence length, into a sequence of hidden states h = (h1,..., hl), each Hi is a concatenation from left to right \u2212 \u2192 hi and from right to left \u2212 hi, hi, hi = [\u2190 \u2212 h i \u2212 \u2192 h i] = [\u2190 \u2212 f (xi, \u2190 \u2212 h i + 1) \u2212 f (xi, \u2212 h \u2212 h \u2212 1), ar Xiv: 160 5.03 148v 1 [cs.C L] May 10, 201 6where the proportions y (f \u2212 f) and \u2212 f are two gated recursive units (GRU). Considering the encoded proportions \u2212 n predicts \u2212 decoded by translating the word \u2212 6xy (8xy), where the target word is \u2212 6xy."}, {"heading": "3 A Coverage Embedding Model", "text": "In traditional statistical translation (e.g. (Koehn, 2004; Chiang, 2005), we use a source-side \"coverage vector\" to explicitly specify which word was translated. A coverage vector begins with all zero, which means that no word was translated. If a source word was translated at position j, the coverage vector will remember position j as 1, and we will not use that source word in future translation. This mechanism avoids the repetition or lowering of translation problems. However, in attention-based NMT systems there is no explicit \"coverage vector,\" since coverage vectors are probabilities in each step. Moreover, the intermediate state Atj is only taken into account if the previous target word yt \u2212 1, and the previous hidden state st \u2212 1, there is no information from history for that particular position j. Thus, it is possible that attention at the j position is repeated, or some positions are never reached."}, {"heading": "4 Related Work", "text": "There are several parallel related works (Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). Tu et al. (2016) is the most relevant work, in which they also use a GRU to model the coverage vector. A major difference is that our model initializes each source word from a specific coverage matrix, in contrast, their work initializes the coverage vector with a uniform distribution. Another difference is in the fertility part, Tu et al. (2016) we add an accumulated operation and a fertility function to simulate the process of one-to-many alignments. In our approach, we add fertility information directly to cover embeddings, since each source word has its own embeddings. In addition, we only feed the previous word, paying attention to the embedding of the cover layer, which is much easier than its last sentence, which consists of us pairing 11 million and clearly millions."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data Preparation", "text": "We conduct our experiments in Chinese to English. We train our machine translation systems on two training sets. The first training corpus consists of about 5 million sentences available within the DARPA BOLT Sino-English task. The corpus comprises a mix of news wire, newscasts, weblog and various sources. The second training corpus comprises HK Law, HK Hansard and UN data, the total number of training sets is 11 million. The Chinese text is segmented with a segment trained on CTB data (CRF). Our development kit is the concatenation of several tuning sets (GALE Dev, P1R6 Dev)."}, {"heading": "5.2 Results", "text": "Table 1 shows the final results of all systems. The traditional hybrid syntax-based system reaches 9.45, 12.90 and 17.72 on MT06, MT08 News, and MT08 Web sets, respectively, averaging 13.36 in terms of (TER- BLEU) / 2. The associated BLEU values are 34.93, 31.12, and 23.45 respectively. The large vocabulary NMT (LVNMT), our baseline, achieves an average (TER- BLEU) / 2 score at 15.74, which is about 2 points worse than the hybrid system. But interestingly, LVNMT shows that the results of the MT08 Web test sets are slightly better. This suggests that NMT has the potential ability to handle informal text better than conventional SMT systems, since NMT represents the source set as dense vectors rather than surface strings."}, {"heading": "6 Conclusion", "text": "In this article, we propose a simple but effective embedding model for embedding cover versions for attention-based NMT. Our model learns a special embedding vector for each initial word. We constantly update these embedding versions as the translation progresses. Experiments on the large-scale task from Chinese to English show significant improvements over the strong LVNMT system."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "ArXiv e-prints, September.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang."], "venue": "Proceedings of ACL.", "citeRegEx": "Chiang.,? 2005", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "CoRR, abs/1409.1259.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Flexible and efficient hypergraph interactions for joint hierarchical and forest-to-string decoding", "author": ["Martin Cmejrek", "Haitao Mi", "Bowen Zhou."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in", "citeRegEx": "Cmejrek et al\\.,? 2013", "shortCiteRegEx": "Cmejrek et al\\.", "year": 2013}, {"title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model", "author": ["T. Cohn", "C.D.V. Hoang", "E. Vymolova", "K. Yao", "C. Dyer", "G. Haffari."], "venue": "ArXiv e-prints, January.", "citeRegEx": "Cohn et al\\.,? 2016", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model", "author": ["S. Feng", "S. Liu", "M. Li", "M. Zhou."], "venue": "ArXiv e-prints, January.", "citeRegEx": "Feng et al\\.,? 2016", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "Tuning as ranking", "author": ["Mark Hopkins", "Jonathan May."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Hopkins and May.,? 2011", "shortCiteRegEx": "Hopkins and May.", "year": 2011}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of ACL, pages 1\u201310, Beijing, China, July.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Pharaoh: a beam search decoder for phrase-based statistical machine translation models", "author": ["Philipp Koehn."], "venue": "Proceedings of AMTA, pages 115\u2013124.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Joint decoding with multiple translation models", "author": ["Yang Liu", "Haitao Mi", "Yang Feng", "Qun Liu."], "venue": "In", "citeRegEx": "Liu et al\\.,? 2009", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412\u20131421, Lisbon, Portu-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Vocabulary manipulation for neural machine translation", "author": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "Proceedings of ACL, Berlin, Germany, August.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Coveragebased Neural Machine Translation", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li."], "venue": "ArXiv e-prints, January.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Generalizing local and non-local word-reordering patterns for syntaxbased machine translation", "author": ["Bing Zhao", "Yaser Al-onaizan."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201908, pages 572\u2013581, Strouds-", "citeRegEx": "Zhao and Al.onaizan.,? 2008", "shortCiteRegEx": "Zhao and Al.onaizan.", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al.", "startOffset": 75, "endOffset": 137}, {"referenceID": 8, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al.", "startOffset": 75, "endOffset": 137}, {"referenceID": 11, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al.", "startOffset": 75, "endOffset": 137}, {"referenceID": 9, "context": "(Koehn, 2004; Chiang, 2005)) address the above issues by employing a source side \u201ccoverage vector\u201d to indicate explicitly which words have been translated, which parts have not yet.", "startOffset": 0, "endOffset": 27}, {"referenceID": 1, "context": "(Koehn, 2004; Chiang, 2005)) address the above issues by employing a source side \u201ccoverage vector\u201d to indicate explicitly which words have been translated, which parts have not yet.", "startOffset": 0, "endOffset": 27}, {"referenceID": 2, "context": "We model this procedure by using a gated recurrent unit (GRU) (Cho et al., 2014).", "startOffset": 62, "endOffset": 80}, {"referenceID": 0, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al. (2014). The attention at each time step shows which source word the model should focus on to predict the next target word.", "startOffset": 76, "endOffset": 207}, {"referenceID": 0, "context": "As shown in Figure 1, attention-based neural machine translation (Bahdanau et al., 2014) is an encoder-decoder network.", "startOffset": 65, "endOffset": 88}, {"referenceID": 9, "context": "(Koehn, 2004; Chiang, 2005)), they employ a source side \u201ccoverage vector\u201d to indicate explicitly which word has been translated.", "startOffset": 0, "endOffset": 27}, {"referenceID": 1, "context": "(Koehn, 2004; Chiang, 2005)), they employ a source side \u201ccoverage vector\u201d to indicate explicitly which word has been translated.", "startOffset": 0, "endOffset": 27}, {"referenceID": 0, "context": "Figure 1: The architecture of attention-based neural machine translation (Bahdanau et al., 2014).", "startOffset": 73, "endOffset": 96}, {"referenceID": 13, "context": "There are several parallel related work (Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016).", "startOffset": 40, "endOffset": 95}, {"referenceID": 6, "context": "There are several parallel related work (Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016).", "startOffset": 40, "endOffset": 95}, {"referenceID": 4, "context": "There are several parallel related work (Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016).", "startOffset": 40, "endOffset": 95}, {"referenceID": 4, "context": ", 2016; Cohn et al., 2016). Tu et al. (2016) is the most relevant one, where they also employ a GRU to model the coverage vector.", "startOffset": 8, "endOffset": 45}, {"referenceID": 4, "context": ", 2016; Cohn et al., 2016). Tu et al. (2016) is the most relevant one, where they also employ a GRU to model the coverage vector. One main difference is that our model initializes each source word from a specific coverage embedding matrix, in contrast, their work initializes the word coverage vector with a uniform distribution. Another difference lays in the fertility part, Tu et al. (2016) add an accumulate operation and a fertility function to simulate the process of one-to-many alignments.", "startOffset": 8, "endOffset": 394}, {"referenceID": 4, "context": ", 2016; Cohn et al., 2016). Tu et al. (2016) is the most relevant one, where they also employ a GRU to model the coverage vector. One main difference is that our model initializes each source word from a specific coverage embedding matrix, in contrast, their work initializes the word coverage vector with a uniform distribution. Another difference lays in the fertility part, Tu et al. (2016) add an accumulate operation and a fertility function to simulate the process of one-to-many alignments. In our approach, we add fertility information directly to coverage embeddings, as each source word has its own embedding. Furthermore, we only feed the previous word, attention to coverage embedding layer, which is much simpler than theirs. The last difference is that we run experiments on 5 million and 11 million sentence pairs, which are significant larger than theirs. Finally, our baseline system is similar to the large vocabulary NMT of Jean et al. (2015) with candidate list decoding and UNK replacement, a much stronger baseline system.", "startOffset": 8, "endOffset": 962}, {"referenceID": 14, "context": "In the training procedure, we use AdaDelta (Zeiler, 2012) to update model parameters with a mini-batch size 80.", "startOffset": 43, "endOffset": 57}, {"referenceID": 5, "context": "For each source sentence, the sentence-level target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from \u2018fast align\u2019 (Dyer et al., 2013).", "startOffset": 210, "endOffset": 229}, {"referenceID": 11, "context": "Following Mi et al. (2016), the output vocabulary for each mini-batch or sentence is a sub-set of the full vocabulary.", "startOffset": 10, "endOffset": 27}, {"referenceID": 8, "context": "Following Jean et al. (2015), We dump the alignments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word.", "startOffset": 10, "endOffset": 29}, {"referenceID": 15, "context": "Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al.", "startOffset": 72, "endOffset": 99}, {"referenceID": 10, "context": "Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013).", "startOffset": 144, "endOffset": 184}, {"referenceID": 3, "context": "Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013).", "startOffset": 144, "endOffset": 184}, {"referenceID": 7, "context": "We tune our system with PRO (Hopkins and May, 2011) to minimize (TER- BLEU)/2 on the development set.", "startOffset": 28, "endOffset": 51}], "year": 2017, "abstractText": "In this paper, we enhance the attention-based neural machine translation by adding an explicit coverage embedding model to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector, and then keeps updating it with a gated recurrent unit as the translation goes. All the initialized coverage embeddings and updating matrix are learned in the training procedure. Experiments on the large-scale Chineseto-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.", "creator": "TeX"}}}