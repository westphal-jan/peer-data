{"id": "1605.03481", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2016", "title": "Tweet2Vec: Character-Based Distributed Representations for Social Media", "abstract": "Text from social media provides a set of challenges that can cause traditional NLP approaches to fail. Informal language, spelling errors, abbreviations, and special characters are all commonplace in these posts, leading to a prohibitively large vocabulary size for word-level approaches. We propose a character composition model, tweet2vec, which finds vector-space representations of whole tweets by learning complex, non-local dependencies in character sequences. The proposed model outperforms a word-level baseline at predicting user-annotated hashtags associated with the posts, doing significantly better when the input contains many out-of-vocabulary words or unusual character sequences. Our tweet2vec encoder is publicly available.", "histories": [["v1", "Wed, 11 May 2016 15:30:09 GMT  (559kb,D)", "https://arxiv.org/abs/1605.03481v1", "6 pages, 2 figures, 4 tables, accepted as conference paper at ACL 2016"], ["v2", "Tue, 17 May 2016 15:00:38 GMT  (559kb,D)", "http://arxiv.org/abs/1605.03481v2", "6 pages, 2 figures, 4 tables, accepted as conference paper at ACL 2016"]], "COMMENTS": "6 pages, 2 figures, 4 tables, accepted as conference paper at ACL 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["bhuwan dhingra", "zhong zhou", "dylan fitzpatrick", "michael muehl", "william w cohen"], "accepted": true, "id": "1605.03481"}, "pdf": {"name": "1605.03481.pdf", "metadata": {"source": "CRF", "title": "Tweet2Vec: Character-Based Distributed Representations for Social Media", "authors": ["Bhuwan Dhingra", "Zhong Zhou", "Dylan Fitzpatrick", "Michael Muehl", "William W. Cohen"], "emails": ["bdhingra@andrew.cmu.edu", "djfitzpa@andrew.cmu.edu", "mmuehl@andrew.cmu.edu", "zhongzhou@cmu.edu", "wcohen@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them are able to surpass themselves by putting themselves in their place. (...) It is the case that they are able to surpass themselves. (...) It is as if they were able to surpass themselves. (...) \"It is the case that they are able to surpass themselves. (...)\" It is the case that they are able to surpass themselves. (...) \"(...)\" (It is.) \"(It is.)\" (It is.) \"(It is. (...)\" (It is.) \"(It is. (...)\" (It is. \"(It is.)\" (It is.) \"(It is.\" (It is.) \"(It is.)\" (It is. (It is.) \"(It is. (It is.)\" (It is. (It is.) \"(It is. (It is.)\" (It is. (It is.) \"(It is. (It.)\" (It is. (It is.) \"(It is. (It is.)\" (It is. (It is.) \"(It is. (It is.)\" (It is. (It is.) \"(It is. (It.)\" (It is. (It is.)"}, {"heading": "2 Related Work", "text": "More recently (Mikolov et al., 2013), word2vec was published - a collection of word vectors formed using a recurring neural network. These word vectors are widely used in the NLP community, and the original work has since expanded to include sentences (Kiros et al., 2015), documents and paragraphs that can be generalized with invisible words at test times (Ling et al., 2015), topics (Niu and Dai, 2015), and queries (Grbovic et al., 2015) All of these methods require the storage of an extremely large table of vectors for all word types and cannot simply be generalized to invisible words at test dates (Ling et al., 2015). They also require pre-processing to find word boundaries that are not trivial to a domain of social networks such as Twitter.In (Ling et al., 2015), the authors are able to create a compositional character model based on biological problems."}, {"heading": "3 Tweet2Vec", "text": "Bi-GRU Encoder: Figure 1 shows our model for encrypting tweets. It uses a structure similar to the C2W model in (Ling et al., 2015), where LSTM units are replaced by GRU units. Input into the network is defined by an alphabet of characters C (this can include the entire Unicode character set), and the input tweet is sent into a stream of characters c1, c2,... cm, each represented by a 1-byte | C encoding, and these one-hot vectors are then projected into a character space by multiplying them by the matrix PC."}, {"heading": "4 Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Word Level Baseline", "text": "Since our goal is to compare character-based and word-based approaches, we have also implemented a simple word-level encoder for tweets: The input tweet is initially divided into tokens along spaces. A more sophisticated tokenizer can be used, but for a fair comparison, we wanted to keep language-specific pre-processing to a minimum. The encoder is essentially the same as tweet2vec, with input occurring as words instead of characters. A lookup table stores word vectors for the most common words (here 20K), the rest is grouped under the \"UNK\" token."}, {"heading": "4.2 Data", "text": "Our record consists of a large collection of global posts from Twitter2 between June 1, 2013 and June 5, 2013. Only English-language posts (as recognized by the long field in the Twitter API) and posts with at least one hashtag are preserved. We removed rare hashtags (< 500 posts) because they do not have enough data for good generalization. We also removed very frequent tags (> 19K posts) that almost always came from auto-generated posts (e.g. # androidgame) that are trivial to predict. The final record contains 2 million tweets for training, 10K for validation and 50K for testing, with a total of 2039 unique hashtags. We use simple regex to pre-process the posttext and remove hashtags (as they are to be predicted) and HTML tags, and replace usernames and URLs with special tokens. We also deleted retweets and converted the text to lowercase https / 2twitter.ps / / /"}, {"heading": "4.3 Implementation Details", "text": "Word vectors and character vectors are each set to the size dL = 150 for their respective models. There were 2829 unique characters in the training set, and we model each independently in a character search table. Embedding sizes were chosen so that each model had approximately the same number of parameters (Table 2). Training is performed using mini-stack gradient descent with Nesterov's impulse. We use a stack size B = 64, initial learning rate \u03b70 = 0.01 and impulse parameter \u00b50 = 0.9. For all models, the L2 regulation was applied with \u03bb = 0.001. Output weights were drawn by 0-average emitters with \u03c3 = 0.1 and initial distortions set to 0. Hyper parameters were set individually to hold others firmly, and values with the lowest validation costs were selected. The resulting combination was used to train the models until the performance of the modelling is increased by less than 0.000percent during the validation process."}, {"heading": "4.4 Results", "text": "We test character and word variations by predicting hashtags for a given set of posts. As there may be more than one correct hashtag per post, we generate a ranking of tags for each post from the output posteriors and report on the average precision @ 1, Recall @ 10, and the mean rank of the correct hashtags. These are listed in Table 3.To see the performance of each model on rare word (RW) and frequent word (FW) posts, we selected two test sets of 2,000 posts each. We populated these sets with posts that had the maximum and minimum number of out-of-vocabulary words, with the vocabulary defined by the 20K most common words. Overall, Tweet2vec outperforms the word model by performing significantly better on RW test set and comparable on FW set. This improved performance comes at the cost of increased training time (see Table 2), as sequences of words lead to longer inputs."}, {"heading": "5 Conclusion", "text": "Our results show that tweet2vec outperforms the word-based approach and performs significantly better when the input text contains many rare words. We have focused only on English-language posts, but the character model does not require language-specific pre-processing and can be extended to other languages. In future work, it would be a natural extension to use a character decoder to predict the hashtags, enabling the generation of hashtags that are not visible in the training dataset. It will also be interesting to see how our Tweet2vec embedding can be used in areas where a semantic understanding of social media is needed, such as tracking infectious diseases (Signorini et al., 2011)."}, {"heading": "Acknowledgments", "text": "We thank Alex Smola, Yun Fu, Hsiao-Yu Fish Tung, Ruslan Salakhutdinov and Barnabas Poczos for useful discussions, J\u00fcrgen Pfeffer for accessing the Twitter data and the reviewers for their comments."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Using topic models for twitter hashtag recommendation", "author": ["Godin et al.2013] Fr\u00e9deric Godin", "Viktor Slavkovikj", "Wesley De Neve", "Benjamin Schrauwen", "Rik Van de Walle"], "venue": "In Proceedings of the 22nd international conference on World Wide Web", "citeRegEx": "Godin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Godin et al\\.", "year": 2013}, {"title": "Context-and contentaware embeddings for query rewriting in sponsored search", "author": ["Nemanja Djuric", "Vladan Radosavljevic", "Fabrizio Silvestri", "Narayan Bhamidipati"], "venue": "In Proceedings of the 38th International", "citeRegEx": "Grbovic et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grbovic et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078", "author": ["Justin Johnson", "Fei-Fei Li"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Tiago Lu\u00eds", "Lu\u00eds Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong et al.2013] Thang Luong", "Richard Socher", "Christopher D Manning"], "venue": "In CoNLL,", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Topic2vec: Learning distributed representations of topics. arXiv preprint arXiv:1506.08422", "author": ["Niu", "Dai2015] Li-Qiang Niu", "Xin-Yu Dai"], "venue": null, "citeRegEx": "Niu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2015}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Santos", "Victor Guimar\u00e3es"], "venue": "arXiv preprint arXiv:1505.05008", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D Santos", "Bianca Zadrozny"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "The use of twitter to track levels of disease activity and public concern in the us during the influenza a h1n1 pandemic", "author": ["Alberto Maria Segre", "Philip M Polgreen"], "venue": "PloS one,", "citeRegEx": "Signorini et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Signorini et al\\.", "year": 2011}, {"title": "tagspace: Semantic embeddings from hashtags", "author": ["Weston et al.2014] Jason Weston", "Sumit Chopra", "Keith Adams"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Weston et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Neural network architectures overcome this problem by defining non-linear compositional models over vector space representations of tokens and hence assign non-zero probability even to sequences not seen during training (Bengio et al., 2003; Kiros et al., 2015).", "startOffset": 220, "endOffset": 261}, {"referenceID": 8, "context": "Recently, (Ling et al., 2015) challenge this assumption and present a bidirectional Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) for composing word vectors from their constituent characters which can memorize the arbitrary aspects of word orthography as well as generalize to rare and out-of-vocabulary words.", "startOffset": 10, "endOffset": 29}, {"referenceID": 10, "context": "Unsupervised approaches attempt to reconstruct the original text from its latent representation (Mikolov et al., 2013; Bengio et al., 2003).", "startOffset": 96, "endOffset": 139}, {"referenceID": 0, "context": "Unsupervised approaches attempt to reconstruct the original text from its latent representation (Mikolov et al., 2013; Bengio et al., 2003).", "startOffset": 96, "endOffset": 139}, {"referenceID": 1, "context": "We propose a Bi-directional Gated Recurrent Unit (Bi-GRU) (Chung et al., 2014) neural network for learning tweet representations.", "startOffset": 58, "endOffset": 78}, {"referenceID": 0, "context": "Using neural networks to learn distributed representations of words dates back to (Bengio et al., 2003).", "startOffset": 82, "endOffset": 103}, {"referenceID": 10, "context": "More recently, (Mikolov et al., 2013) released word2vec - a collection of word vectors trained using a recurrent neural network.", "startOffset": 15, "endOffset": 37}, {"referenceID": 3, "context": ", 2015), documents and paragraphs (Le and Mikolov, 2014), topics (Niu and Dai, 2015) and queries (Grbovic et al., 2015).", "startOffset": 97, "endOffset": 119}, {"referenceID": 8, "context": "All these methods require storing an extremely large table of vectors for all word types and cannot be easily generalized to unseen words at test time (Ling et al., 2015).", "startOffset": 151, "endOffset": 170}, {"referenceID": 8, "context": "In (Ling et al., 2015), the authors present a compositional character model based on bidirectional LSTMs as a potential solution to these problems.", "startOffset": 3, "endOffset": 22}, {"referenceID": 8, "context": "While (Ling et al., 2015) generate word embeddings from character representations, we propose to generate vector representations of entire tweets from characters in our tweet2vec model.", "startOffset": 6, "endOffset": 25}, {"referenceID": 16, "context": "Our work adds to the growing body of work showing the applicability of character models for a variety of NLP tasks such as Named Entity Recognition (Santos and Guimar\u00e3es, 2015), POS tagging (Santos and Zadrozny, 2014), text classification (Zhang et al., 2015) and language modeling (Karpathy et al.", "startOffset": 239, "endOffset": 259}, {"referenceID": 5, "context": ", 2015) and language modeling (Karpathy et al., 2015; Kim et al., 2015).", "startOffset": 30, "endOffset": 71}, {"referenceID": 6, "context": ", 2015) and language modeling (Karpathy et al., 2015; Kim et al., 2015).", "startOffset": 30, "endOffset": 71}, {"referenceID": 9, "context": "Previously, (Luong et al., 2013) dealt with the problem of estimating rare word representations by building them from their constituent morphemes.", "startOffset": 12, "endOffset": 32}, {"referenceID": 15, "context": "Hashtag prediction for social media has been addressed earlier, for example in (Weston et al., 2014; Godin et al., 2013).", "startOffset": 79, "endOffset": 120}, {"referenceID": 2, "context": "Hashtag prediction for social media has been addressed earlier, for example in (Weston et al., 2014; Godin et al., 2013).", "startOffset": 79, "endOffset": 120}, {"referenceID": 15, "context": "(Weston et al., 2014) also use a neural architecture, but compose text embeddings from a lookup table of words.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "It uses a similar structure to the C2W model in (Ling et al., 2015), with LSTM units replaced with GRU units.", "startOffset": 48, "endOffset": 67}, {"referenceID": 14, "context": "Also, it will be interesting to see how our tweet2vec embeddings can be used in domains where there is a need for semantic understanding of social media, such as tracking infectious diseases (Signorini et al., 2011).", "startOffset": 191, "endOffset": 215}], "year": 2016, "abstractText": "Text from social media provides a set of challenges that can cause traditional NLP approaches to fail. Informal language, spelling errors, abbreviations, and special characters are all commonplace in these posts, leading to a prohibitively large vocabulary size for word-level approaches. We propose a character composition model, tweet2vec, which finds vectorspace representations of whole tweets by learning complex, non-local dependencies in character sequences. The proposed model outperforms a word-level baseline at predicting user-annotated hashtags associated with the posts, doing significantly better when the input contains many outof-vocabulary words or unusual character sequences. Our tweet2vec encoder is publicly available1.", "creator": "LaTeX with hyperref package"}}}