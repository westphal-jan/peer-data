{"id": "1605.03661", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2016", "title": "Learning Representations for Counterfactual Inference", "abstract": "Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, \"Would this patient have lower blood sugar had she received a different medication?\". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.", "histories": [["v1", "Thu, 12 May 2016 02:59:40 GMT  (193kb,D)", "http://arxiv.org/abs/1605.03661v1", "To appear in ICML 2016"], ["v2", "Wed, 8 Jun 2016 17:04:07 GMT  (194kb,D)", "http://arxiv.org/abs/1605.03661v2", "Appearing in ICML 2016"]], "COMMENTS": "To appear in ICML 2016", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["fredrik d johansson", "uri shalit", "david sontag"], "accepted": true, "id": "1605.03661"}, "pdf": {"name": "1605.03661.pdf", "metadata": {"source": "META", "title": "Learning Representations for Counterfactual Inference", "authors": ["Fredrik D. Johansson", "Uri Shalit", "David Sontag"], "emails": ["FREJOHK@CHALMERS.SE", "SHALIT@CS.NYU.EDU", "DSONTAG@CS.NYU.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is as if most of us are able to outdo ourselves by taking responsibility for ourselves. (...) In fact, it is as if most of us are able to outlive ourselves. (...) It is not as if they are able to outlive themselves. (...) It is not as if they are able to outlive themselves. (...) It is not as if they are able to outlive themselves. (...) It is as if they are able to outlive themselves. (...) It is as if they are able to outlive themselves. (...) It is as if they are able to outlive themselves. (...) It is as if they are able to outlive themselves. (...) It is as if they are able to outlive themselves. (...) It is as if they are able to outlive themselves."}, {"heading": "2. Problem setup", "text": "The question that arises is to what extent it is actually a \"pure\" \"pure\" \"pure\" \"pure\" \"\" pure \"\" \"\" pure \"\" \"\" \"pure\" \"\" \"\" pure \"\" \"\" \"\" pure \"\" \"\" \"\" pure \"\" \"\" \"\" \"\" \"\" pure \"\" \"\" \"\" \"\" \"\" pure \"\" \"\" \"\" \"\" \"\" \"pure\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "3. Balancing counterfactual regression", "text": "We propose to perform counterfactual conclusions by changing the direct modeling approach, taking into account the fact that the learned estimator h must generalize from the factual distribution to the counterfactual distribution. (1) Our method, presented in Figure 1, involves an objective prediction of the observed results via factual representation, (2) that the unobserved counterfactual results are similar or balanced by taking into account the relevant factual results, and (3) the distribution of treatment and control populations. We achieve low error predictions through the usual Meansof error minimization through observed training and regulation to enable a good objective prediction."}, {"heading": "3.1. Balancing variable selection", "text": "A naive way of balancing the initial distributions of treatment and control groups is to consider only features that are already well balanced, i.e. features that have a similar distribution across both treatment and control groups, while discarding unbalanced features. However, in some cases, unbalanced features can be highly predictable and should not be discarded. A middle level is intended to limit the influence of unbalanced features on the predicted outcome. We build on this idea by learning a sparse reweighting of features that minimizes the boundary in Theorem 1. Reweighting determines the influence of a feature by weighing its predictive abilities and its equilibrium. We implement the reweighting as a diagonal matrix W by forming the representation \u03a6 (x) = Wx, with diag (W) subject to a simple constraint in order to achieve the sparity {= 7 Wdix \u2192 1 (W = W \u2212 1)."}, {"heading": "3.2. Deep neural networks", "text": "It has been shown that deep neural networks successfully learn good representations of high-dimensional data in many different tasks (Bengio et al., 2013). At this point, we show that they can be used for counterfactual conclusions and, crucially, for the consideration of imbalance loads. We propose a modification of the standard feed-forward architecture with fully connected layers, see Figure 2. The first hidden layers are used to learn a representation \u03a6 (x) of input x. The output of the dr: th layer is used to calculate the discrepancy discrepancy H. The Do layers following the first dr layers take as an additional input the treatment allocation ti and generate a prediction h ([xi), ti]) of the result."}, {"heading": "3.3. Non-linear hypotheses and individual effect", "text": "It is important to note that both in the case of variable reweighting and in the case of neural networks with a single linear result layer do = 0, the hypotheses space H includes linear functions of [\u03a6, t]. Therefore, the discrepancy, discH (\u03a6), can be expressed in a closed form. A less desirable consequence of using linear hypotheses is that the models cannot detect the difference in the individual treatment effect, since the predicted result for some vectors \u03b2-Rd and scalars \u03b2t, \u03b20 can be written as \u03b2 > \u03a6 (x) + \u03b20. In order to obtain truly individual estimates of the treatment effect, we must take into account interactions between \u03a6 (x) and t, for example by (polynomic) feature expansion or, in the case of neural networks, by adding nonlinear layers after concatenation."}, {"heading": "4. Theory", "text": "In this section, we derive an upper limit on the relative counterfactual generalization errors of a representation function. The limit uses only quantities that we can measure directly from the available data. In the previous section, we have given several methods for learning representations, of which we derive the factual result yFi = tiY1 (xi) + (1 \u2212 ti) Y0 (xi). Let (x1, t1, y F 1),. Let (xn, tn, y F n) be a sample from the factual result yFi = tiY1 (xi) Y0 (xi)."}, {"heading": "4.1. Linear discrepancy", "text": "A simple calculation shows that for a class Hl linear hypotheses the slice Hl (P, Q) = 1-2 (P) -2 (Q) -2 the spectral standard of A and \u00b52 (P) = Ex-P [xx >] is the moment second order of x-P. In the special case counterfactual inference differ P and Q only in the treatment assignment. Specifically differ slice (P, P, CF) = 1-2 (0d, d vv > 2p \u2212 1): 2 (7) = p \u2212 1 + 2 + 1 (2p \u2212 1) 24 + 1: 22 (8), where v = E (x, t), P, P, F, (x), T, T, T, E (x, t), P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P,"}, {"heading": "5. Related work", "text": "The counterfactual conclusion to determine causal effects in observational studies has been extensively studied in statistics, economics, epidemiology, and sociology (Morgan & Winship, 2014; Robins et al., 2000; Rubin, 2011; Chernozhukov et al., 2013), and in machine learning (Langford et al., 2011; Bottou et al., 2013; Swaminathan & Joachims, 2015).Non-parametric methods do not attempt to model the relationship between context, intervention, and outcome, including the matching between neighbors, the tilt result, and the reweighting of the tilt value (Rosenbaum & Rubin, 1983; Rosenbaum, 2002; Austin, 2011).Parametric methods, on the other hand, attempt to concretely model the relationship between context, intervention, and outcome, covering all types of regression, including linear and logistic regression (Prentice, 1976; Gelman, & Hill)."}, {"heading": "6. Experiments", "text": "In fact, it is not the case that it is merely a scare story, but a pure scare story that sees itself in a position to change the world. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "6.1. Simulation based on real data \u2013 IHDP", "text": "Hill (2011) introduced a semi-simulated dataset based on the Infant Health and Development Program (IHDP). The experiment proposed by Hill (2011) uses covariates from a real randomized experiment and investigates the impact of high-quality childcare and home visits on future cognitive test results. Hill (2011) proposed the experiment using a simulated log-linear result to determine the truth and artificially introduces an imbalance between caregivers and controls by removing a subset of the treatment population. In total, the dataset consists of 747 subjects (139 treated, 608 controlled), each represented by 25 covariants measuring the characteristics of the child and their mother."}, {"heading": "6.2. Simulation based on real data \u2013 News", "text": "We present a new data set that reflects the opinions of a media consumer exposed to multiple messages. (> DA = > Factor) Each item is consumed either on a mobile device or on a desktop. (D) The intervention t {0, 1} represents the viewing device, desktop (t = 0) or mobile phone (t = 1). We assume that the consumer prefers to read certain topics on the mobile phone. (D) To model this, we train a theme model on a large set of documents and let z (x) represent the topic distribution of messages x. We define two centrals in the theme space, zc1 (mobile) and z c 0 (desktop), and let the reader's opinion of messages x on a large set of documents and let's assume the theme model (proci.x-package = htttdor.org / httznz)."}, {"heading": "6.3. Results", "text": "The results of the IHDP experiments and the news experiments are presented in Table 1 and Table 2, respectively. We see that the non-linear methods generally perform better in terms of individual predictions (ITE, PEHE). Furthermore, we see that our proposed balancing neural network, BNN-2-2, performs best in terms of estimating the ITE and PEHE, and is competitive in terms of the average treatment effect, ATE. Particularly noteworthy is the comparison with the non-equilibrium network, NN-4. These results suggest that our proposed regulation may help not to adapt the representation too much to the actual result. Figure 4 shows the performance of BNN-2-2 for various imbalance penalties, LN-2. The valley in the region \u03b1 = 1, and the fact that we do not experience a loss of performance in smaller values of the RIGE-R method that affect ASS 4, which represent a better representation of the BLGE-2-2-2-2-ASS, and ASS-4, ASS-2-2-2-different imbalances, ASS-ASS."}, {"heading": "7. Conclusion", "text": "As machine learning becomes an important tool for researchers and policy makers in various fields such as health care and economics, causal inference becomes a critical issue in machine learning practice. In this paper, we focus on counterfactual inferences, which are a widely applicable special case of causal inferences. We dismiss counterfactual inferences as a kind of domain adaptation problem, and derive a novel method to learn appropriate representations for this problems.Our models are based on a novel type of regulatory criteria: balanced representations learn, representations that exhibit similar distributions among treated and untreated populations, and we show that swapping a compensatory criterion for standard data and regulatory concepts is both practical and theoretically sensible. Open questions that remain are how this method can be generalized for cases where more than one treatment is applicable, and better optimization algorithms can be derived and used."}, {"heading": "Acknowledgements", "text": "DS and US were supported by the NSF CAREER Award # 1350965."}, {"heading": "A. Proof of Theorem 1", "text": "We use an implicit result to prove theory 2 of Cortes & Mohri (2014), in the case where H is the set of linear hypotheses about a fixed representation. \u2212 Fi (2014) give their result in the case of domain adaptation: In our case, the factual distribution is the so-called \"source domain,\" and the counterfactual distribution is the \"target domain.\" \u2212 F. (Cortes & Mohri (2014)] Using the notation and premises of theorem 1, for both Q = PF and Q = PCF: The counterfactual distribution is the \"target domain.\" \u2212 LQ (\u03b2 \u00b2 F) \u2212 LQ (\u03b2 \u00b2 CF)))) 2 \u2264 discHl (P \u00b2 F (P \u00b2 F), the assumptions of theorem 1, for both Q = PF and Q = PCF: xi x \u00b2 r (LQ)."}], "references": [{"title": "An introduction to propensity score methods for reducing the effects of confounding in observational studies", "author": ["Austin", "Peter C"], "venue": "Multivariate behavioral research,", "citeRegEx": "Austin and C.,? \\Q2011\\E", "shortCiteRegEx": "Austin and C.", "year": 2011}, {"title": "Doubly robust estimation in missing data and causal inference models", "author": ["Bang", "Heejung", "Robins", "James M"], "venue": null, "citeRegEx": "Bang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bang et al\\.", "year": 2005}, {"title": "Analysis of representations for domain adaptation", "author": ["nando"], "venue": "Advances in neural information processing systems,", "citeRegEx": "nando,? \\Q2007\\E", "shortCiteRegEx": "nando", "year": 2007}, {"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pierre"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["Beygelzimer", "Alina", "Langford", "John", "Li", "Lihong", "Reyzin", "Lev", "Schapire", "Robert E"], "venue": "arXiv preprint arXiv:1002.4058,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2010}, {"title": "Bayesian additive regression trees", "author": ["Chipman", "Hugh A", "George", "Edward I", "McCulloch", "Robert E. Bart"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Chipman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chipman et al\\.", "year": 2010}, {"title": "Domain adaptation and sample bias correction theory and algorithm for regression", "author": ["Cortes", "Corinna", "Mohri", "Mehryar"], "venue": "Theoretical Computer Science,", "citeRegEx": "Cortes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2014}, {"title": "Domain adaptation for statistical classifiers", "author": ["Daume III", "Hal", "Marcu", "Daniel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "III et al\\.,? \\Q2006\\E", "shortCiteRegEx": "III et al\\.", "year": 2006}, {"title": "Doubly robust policy evaluation and learning", "author": ["Dud\u0131\u0301k", "Miroslav", "Langford", "John", "Li", "Lihong"], "venue": "arXiv preprint arXiv:1103.4601,", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "Domain-adversarial training of neural networks", "author": ["Gani", "Yaroslav", "Ustinova", "Evgeniya", "Ajakan", "Hana", "Germain", "Pascal", "Larochelle", "Hugo", "Laviolette", "Fran\u00e7ois", "Marchand", "Mario", "Lempitsky", "Victor"], "venue": "arXiv preprint arXiv:1505.07818,", "citeRegEx": "Gani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gani et al\\.", "year": 2015}, {"title": "Data analysis using regression and multilevel/hierarchical models", "author": ["Gelman", "Andrew", "Hill", "Jennifer"], "venue": null, "citeRegEx": "Gelman et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gelman et al\\.", "year": 2006}, {"title": "A kernel twosample test", "author": ["Gretton", "Arthur", "Borgwardt", "Karsten M", "Rasch", "Malte J", "Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Bayesian nonparametric modeling for causal inference", "author": ["Hill", "Jennifer L"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Hill and L.,? \\Q2011\\E", "shortCiteRegEx": "Hill and L.", "year": 2011}, {"title": "A literature survey on domain adaptation of statistical classifiers", "author": ["Jiang", "Jing"], "venue": "URL: http://sifaka. cs. uiuc. edu/jiang4/domainadaptation/survey,", "citeRegEx": "Jiang and Jing.,? \\Q2008\\E", "shortCiteRegEx": "Jiang and Jing.", "year": 2008}, {"title": "Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data", "author": ["Kang", "Joseph DY", "Schafer", "Joseph L"], "venue": "Statistical science,", "citeRegEx": "Kang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2007}, {"title": "Doubly robust policy evaluation and learning", "author": ["Langford", "John", "Li", "Lihong", "Dud\u0131\u0301k", "Miroslav"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Langford et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2011}, {"title": "Causation. The journal of philosophy", "author": ["Lewis", "David"], "venue": null, "citeRegEx": "Lewis and David.,? \\Q1973\\E", "shortCiteRegEx": "Lewis and David.", "year": 1973}, {"title": "The variational fair auto encoder", "author": ["Louizos", "Christos", "Swersky", "Kevin", "Li", "Yujia", "Welling", "Max", "Zemel", "Richard"], "venue": "arXiv preprint arXiv:1511.00830,", "citeRegEx": "Louizos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Louizos et al\\.", "year": 2015}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Mansour", "Yishay", "Mohri", "Mehryar", "Rostamizadeh", "Afshin"], "venue": "arXiv preprint arXiv:0902.3430,", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "Counterfactuals and causal inference", "author": ["Morgan", "Stephen L", "Winship", "Christopher"], "venue": null, "citeRegEx": "Morgan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Morgan et al\\.", "year": 2014}, {"title": "Invited commentary: understanding bias amplification", "author": ["Pearl", "Judea"], "venue": "American journal of epidemiology,", "citeRegEx": "Pearl and Judea.,? \\Q2011\\E", "shortCiteRegEx": "Pearl and Judea.", "year": 2011}, {"title": "Use of the logistic model in retrospective studies", "author": ["Prentice", "Ross"], "venue": "Biometrics, pp. 599\u2013606,", "citeRegEx": "Prentice and Ross.,? \\Q1976\\E", "shortCiteRegEx": "Prentice and Ross.", "year": 1976}, {"title": "Marginal structural models and causal inference in epidemiology", "author": ["Robins", "James M", "Hernan", "Miguel Angel", "Brumback", "Babette"], "venue": "Epidemiology, pp", "citeRegEx": "Robins et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Robins et al\\.", "year": 2000}, {"title": "Design of Observational Studies", "author": ["Rosenbaum", "Paul R"], "venue": "Springer Science & Business Media,", "citeRegEx": "Rosenbaum and R.,? \\Q2009\\E", "shortCiteRegEx": "Rosenbaum and R.", "year": 2009}, {"title": "The central role of the propensity score in observational studies for causal effects", "author": ["Rosenbaum", "Paul R", "Rubin", "Donald B"], "venue": null, "citeRegEx": "Rosenbaum et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Rosenbaum et al\\.", "year": 1983}, {"title": "Estimating causal effects of treatments in randomized and nonrandomized studies", "author": ["Rubin", "Donald B"], "venue": "Journal of educational Psychology,", "citeRegEx": "Rubin and B.,? \\Q1974\\E", "shortCiteRegEx": "Rubin and B.", "year": 1974}, {"title": "Causal inference using potential outcomes", "author": ["Rubin", "Donald B"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Rubin and B.,? \\Q2011\\E", "shortCiteRegEx": "Rubin and B.", "year": 2011}, {"title": "On causal and anticausal learning", "author": ["B. Sch\u00f6lkopf", "D. Janzing", "J. Peters", "E. Sgouritsa", "K. Zhang", "J. Mooij"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2012}, {"title": "Learning from logged implicit exploration data", "author": ["Strehl", "Alex", "Langford", "John", "Li", "Lihong", "Kakade", "Sham M"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Strehl et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2010}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Batch learning from logged bandit feedback through counterfactual risk minimization", "author": ["Swaminathan", "Adith", "Joachims", "Thorsten"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Swaminathan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Swaminathan et al\\.", "year": 2015}, {"title": "A simple method for estimating interactions between a treatment and a large number of covariates", "author": ["Tian", "Lu", "Alizadeh", "Ash A", "Gentles", "Andrew J", "Tibshirani", "Robert"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Tibshirani and Robert.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1996}, {"title": "Causal effect models for realistic individualized treatment and intention to treat rules", "author": ["van der Laan", "Mark J", "Petersen", "Maya L"], "venue": "The International Journal of Biostatistics,", "citeRegEx": "Laan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Laan et al\\.", "year": 2007}, {"title": "Estimation and inference of heterogeneous treatment effects using random forests", "author": ["Wager", "Stefan", "Athey", "Susan"], "venue": "arXiv preprint arXiv:1510.04342,", "citeRegEx": "Wager et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2015}, {"title": "Machine learning for treatment assignment: Improving individualized risk attribution", "author": ["Weiss", "Jeremy C", "Kuusisto", "Finn", "Boyd", "Kendrick", "Lui", "Jie", "Page", "David C"], "venue": "American Medical Informatics Association Annual Symposium,", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Learning fair representations", "author": ["Zemel", "Rich", "Wu", "Yu", "Swersky", "Kevin", "Pitassi", "Toni", "Dwork", "Cynthia"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Zemel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zemel et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 36, "context": "Our work has commonalities with recent work on learning fair representations (Zemel et al., 2013; Louizos et al., 2015) and on learning representations suited for transfer learning (Ben-David et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 17, "context": "Our work has commonalities with recent work on learning fair representations (Zemel et al., 2013; Louizos et al., 2015) and on learning representations suited for transfer learning (Ben-David et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 9, "context": ", 2015) and on learning representations suited for transfer learning (Ben-David et al., 2007; Gani et al., 2015).", "startOffset": 69, "endOffset": 112}, {"referenceID": 4, "context": "This is sometimes referred to as bandit feedback (Beygelzimer et al., 2010).", "startOffset": 49, "endOffset": 75}, {"referenceID": 28, "context": "This setup comes up in diverse areas, for example off-policy evaluation in reinforcement learning (Sutton & Barto, 1998), learning from \u201clogged implicit exploration data\u201d (Strehl et al., 2010) or \u201clogged bandit feedback\u201d (Swaminathan & Joachims, 2015), and in understanding and designing complex real world ad-placement systems (Bottou et al.", "startOffset": 171, "endOffset": 192}, {"referenceID": 3, "context": "Second, we derive new families of representation algorithms for counterfactual inference: one family is based on linear models and variable selection, and the other is based on deep learning of representations (Bengio et al., 2013).", "startOffset": 210, "endOffset": 231}, {"referenceID": 8, "context": "Finally, we show that learning representations that encourage similarity (also called balance) between the treatment and control populations leads to better counterfactual inference; this is in contrast to many methods which attempt to create balance by re-weighting samples (e.g., Bang & Robins, 2005; Dud\u0131\u0301k et al., 2011; Austin, 2011; Swaminathan & Joachims, 2015).", "startOffset": 275, "endOffset": 367}, {"referenceID": 35, "context": "In this case the quantity Y1(x) \u2212 Y0(x) is of high interest: it is known as the individualized treatment effect (ITE) for context x (van der Laan & Petersen, 2007; Weiss et al., 2015).", "startOffset": 132, "endOffset": 183}, {"referenceID": 5, "context": "While in principle any function fitting model might be used for estimating the ITE (Prentice, 1976; Gelman & Hill, 2006; Chipman et al., 2010; Wager & Athey, 2015; Weiss et al., 2015), it is important to note how this task differs from standard supervised learning.", "startOffset": 83, "endOffset": 183}, {"referenceID": 35, "context": "While in principle any function fitting model might be used for estimating the ITE (Prentice, 1976; Gelman & Hill, 2006; Chipman et al., 2010; Wager & Athey, 2015; Weiss et al., 2015), it is important to note how this task differs from standard supervised learning.", "startOffset": 83, "endOffset": 183}, {"referenceID": 18, "context": "This is a case of covariate shift, which is a special case of domain adaptation (Daume III & Marcu, 2006; Jiang, 2008; Mansour et al., 2009).", "startOffset": 80, "endOffset": 140}, {"referenceID": 18, "context": "This is a case of covariate shift, which is a special case of domain adaptation (Daume III & Marcu, 2006; Jiang, 2008; Mansour et al., 2009). A somewhat similar connection was noted in Sch\u00f6lkopf et al. (2012) with respect to covariate shift, in the context of a very simple causal model.", "startOffset": 119, "endOffset": 209}, {"referenceID": 11, "context": "Other discrepancy measures such as Maximum Mean Discrepancy (Gretton et al., 2012) could also be used for this purpose.", "startOffset": 60, "endOffset": 82}, {"referenceID": 17, "context": "Finally, we accomplish the third objective by minimizing the so-called discrepancy distance, introduced by Mansour et al. (2009), which is a hypothesis class dependent distance measure specifically tailored for domain adaptation applications.", "startOffset": 107, "endOffset": 129}, {"referenceID": 3, "context": "Deep neural networks have been shown to successfully learn good representations of high-dimensional data in many different tasks (Bengio et al., 2013).", "startOffset": 129, "endOffset": 150}, {"referenceID": 18, "context": "Definition 1 (Mansour et al. 2009).", "startOffset": 13, "endOffset": 34}, {"referenceID": 22, "context": "Counterfactual inference for determining causal effects in observational studies has been studied extensively in statistics, economics, epidemiology and sociology (Morgan & Winship, 2014; Robins et al., 2000; Rubin, 2011; Chernozhukov et al., 2013) as well as in machine learning (Langford et al.", "startOffset": 163, "endOffset": 248}, {"referenceID": 15, "context": ", 2013) as well as in machine learning (Langford et al., 2011; Bottou et al., 2013; Swaminathan & Joachims, 2015).", "startOffset": 39, "endOffset": 113}, {"referenceID": 5, "context": "These methods include any type of regression including linear and logistic regression (Prentice, 1976; Gelman & Hill, 2006), random forests (Wager & Athey, 2015) and regression trees (Chipman et al., 2010).", "startOffset": 183, "endOffset": 205}, {"referenceID": 8, "context": "Doubly robust methods combine aspects of parametric and non-parametric methods, typically by using a propensity score weighted regression (Bang & Robins, 2005; Dud\u0131\u0301k et al., 2011).", "startOffset": 138, "endOffset": 180}, {"referenceID": 31, "context": "Tian et al. (2014) presented one such method, modeling the interactions between treatment and covariates.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Bayesian Additive Regression Trees (BART) (Chipman et al., 2010) is a non-linear regression model which has been used successfully for counterfactual inference in the past (Hill, 2011).", "startOffset": 42, "endOffset": 64}, {"referenceID": 5, "context": "\u2020 (Chipman et al., 2010)", "startOffset": 2, "endOffset": 24}, {"referenceID": 5, "context": "\u2020 (Chipman et al., 2010)", "startOffset": 2, "endOffset": 24}], "year": 2016, "abstractText": "Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, \u201cWould this patient have lower blood sugar had she received a different medication?\u201d. We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.", "creator": "LaTeX with hyperref package"}}}