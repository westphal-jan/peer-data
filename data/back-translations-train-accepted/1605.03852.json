{"id": "1605.03852", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2016", "title": "Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning", "abstract": "We use Bayesian optimization to learn curricula for word representation learning, optimizing performance on downstream tasks that depend on the learned representations as features. The curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus. We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order.", "histories": [["v1", "Thu, 12 May 2016 15:15:58 GMT  (121kb,D)", "http://arxiv.org/abs/1605.03852v1", "ACL 2016 submission, 10 pages"], ["v2", "Tue, 21 Jun 2016 18:35:29 GMT  (124kb,D)", "http://arxiv.org/abs/1605.03852v2", "In proceedings of ACL 2016, 10 pages"]], "COMMENTS": "ACL 2016 submission, 10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yulia tsvetkov", "manaal faruqui", "wang ling", "brian macwhinney", "chris dyer"], "accepted": true, "id": "1605.03852"}, "pdf": {"name": "1605.03852.pdf", "metadata": {"source": "CRF", "title": "Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning", "authors": ["Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Chris Dyer"], "emails": ["ytsvetko@cs.cmu.edu", "mfaruqui@cs.cmu.edu", "lingwang@cs.cmu.edu", "cdyer@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Curriculum Learning Model", "text": "We consider the problem of the maximization of the execution of an NLP task through sequential optimization of the training plan of the training data of word vektor representations that are used as features in the task. Do you leave X = {x1, x2,.., xn} the training corpus with n lines (sets or paragraphs). The curriculum of the word representations is quantified, using each of the paragraphs according to the next linear function is evaluated: w??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "2.1 Bayesian Optimization for Curriculum Learning", "text": "Since no assumptions regarding the form of evaluation (w), gradient-based methods cannot be applied, and conducting a web search using parameterizations of w would require an exponentially growing number of parameterizations to be passed through. Thus, we propose to use Bayesian Optimization (BayesOpt) as a means of maximizing eval (w). BayesOpt is a methodology for costly optimization of multimodal black box functions (Shahriari et al., Bergstra et al., 2011; Snoek et al., 2012) It can be considered as a sequential approach to perform a regression of high-level model parameters to make the number of layers in a neural network, and in our model curriculum weights more different."}, {"heading": "2.2 Distributional and Linguistic Features", "text": "(It. \"It.\" It. \"It.\" (It. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" (It. \"It.\" It. \"\" It. \"\" It. \"\" It. \"\" It. \"\" It. \"\" It. \"\" It. \"\" It. \"\" It. \"\" It. \"\" \"It.\" (\"It.\") \"It.\" (It. \"It.\" \"It.\" (\"It.\" \"\" It. \"\" It. \"\" It. \"\" It. \"\" It. \"(\" It. \"\" It. \"(\" It. \"It.\" (\"It.\" It. \")\" (It. \"It.\" \"It.\" (\"It.\" It. \"(\" It. \"It.\" \"It.\" \"It.\" \"It.\" \"It.\" \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" \"It.\" It. \"It.\" It. \"(\" It. \"It.\" It. \"It.\" \"\" \"It.\") \"(\" It. \"It.\" It. \"It.\" (\"It.\") \"It.\" (It. \"It.\") \"(It.\" It. \"(It.\" It. \")\" It. \"It.\" (It. \"It.\" (\")\" It. \"(It.\" It. \"(It.\" It. \")\" It. \"(It.\" It. \"(\") \"It.\" (It. \"It.\") \"(It.\" (It. \"It.\" (\")\" It. \"(It.\" (It. \"It.\") \"(It.\" (It. \"It.\") \"It.\" It. \"(\" It. \"(\" It. \"(It.\" It. \")\" It. \")\" (\"It.\" (It. \"It.\") \"It.\" It. \"(It.\" It. \"(\" It. \")\" (\"It.\" It. \"(\" It. \"(It.\" It"}, {"heading": "3 Evaluation Benchmarks", "text": "We evaluate the usefulness of predefined word embedding as a feature in downstream NLP tasks. We select the following standard models that use predefined word embedding as a feature: Sentiment Analysis (Senti). Socher et al. (2013) created a tree bank of sentences with fine-grained sentiment labels on phrases and sentences from film reviews. The coarse-grained stem of positive and negative classes was divided into training, development and test data sets containing 6,920, 872 and 1,821 sentences. We use the average of word vectors of a given sentence as a feature vector for classification (Faruqui et al., 2015; Sedocet al., 2016). The \"2-regulated logistic regression classifier is set to the Dev and accuracy is reported in the Testset.Named Entity Recognition (NER)."}, {"heading": "4 Experiments", "text": "This year it is more than ever before."}, {"heading": "5 Analysis", "text": "It is a question of whether and to what extent people in the USA, in what countries, in what countries, in what countries, in what countries, in what countries, in what countries, in what countries, in what countries, in what countries, in what countries, in what countries, in what countries, in what cultures, in what countries, in what countries, in what cultures, in what and in what countries, in what countries, in what countries, in what countries, in what countries, in what countries, in what and in what cultures, in what cultures, in what and in what cultures, in what and in what cultures, in what and in what cultures, in what and in what cultures, in what and in what cultures, in what and in what cultures, in what and in what cultures, in what and in what and in what cultures, in what and in what and in what, in what and in what, in what and in what, in what, in what and in what and in what and in what and in what, and in what and in what and in what and in what and in what and in what and in what and in what and in what and in what"}, {"heading": "6 Related Work", "text": "Two previous studies on curriculum learning in NLP are discussed in the paper (Bengio et al., 2009; Spitkovsky et al., 2010); curriculum learning and related research on self-determined learning have been deepened in computer vision (Bengio et al., 2009; Kumar et al., 2010; Lee and Grauman, 2011) and multimedia analysis (Jiang et al., 2015); little attention has also been paid in NLP to Bayean optimization; GP has been used in evaluating the quality of machine translation (Cohn and Specia, 2013) and in the temporal analysis of social media texts (Preotiuc-Pietro and Cohn, 2013); TPEs have been used by Yogatama et al. (2015) to optimize the selection of feature representations - ngram size, regulatory selection, etc. - in monitored classifiers."}, {"heading": "7 Conclusion", "text": "Our experiments confirmed that better curricula produce stronger models. We also conducted a comprehensive analysis that shows a better understanding of the text properties that are beneficial for model initialization. The proposed novel technique for determining an optimal curriculum is general and can be used with other data sets and models."}], "references": [{"title": "Universal dependencies 1.1. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague", "author": ["Vincze", "Daniel Zeman"], "venue": null, "citeRegEx": "Vincze and Zeman.,? \\Q2015\\E", "shortCiteRegEx": "Vincze and Zeman.", "year": 2015}, {"title": "Algorithms for hyper-parameter optimization", "author": ["R\u00e9mi Bardenet", "Yoshua Bengio", "Bal\u00e1zs K\u00e9gl"], "venue": "In Proc. NIPS,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "Concreteness ratings for 40 thousand generally known english word lemmas", "author": ["Amy Beth Warriner", "Victor Kuperman"], "venue": "Behavior research methods,", "citeRegEx": "Brysbaert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Brysbaert et al\\.", "year": 2014}, {"title": "Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger", "author": ["Ciaramita", "Yasemin Altun"], "venue": "In Proc. EMNLP,", "citeRegEx": "Ciaramita et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ciaramita et al\\.", "year": 2006}, {"title": "Modelling annotator bias with multitask Gaussian processes: An application to machine translation quality estimation", "author": ["Cohn", "Specia2013] Trevor Cohn", "Lucia Specia"], "venue": "In Proc. ACL,", "citeRegEx": "Cohn et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2013}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proc. EMNLP,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proc. ACL", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["Jeffrey L Elman"], "venue": null, "citeRegEx": "Elman.,? \\Q1993\\E", "shortCiteRegEx": "Elman.", "year": 1993}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "venue": "In Proc. NAACL", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "A systematic exploration of diversity in machine translation", "author": ["Gimpel et al.2013] Kevin Gimpel", "Dhruv Batra", "Chris Dyer", "Gregory Shakhnarovich"], "venue": "In Proc. EMNLP,", "citeRegEx": "Gimpel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2013}, {"title": "Combining lexical and grammatical features to improve readability measures for first and second language texts", "author": ["Kevyn Collins-Thompson", "Jamie Callan", "Maxine Eskenazi"], "venue": "In Proc. NAACL,", "citeRegEx": "Heilman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2007}, {"title": "Entropy search for information-efficient global optimization", "author": ["Hennig", "Schuler2012] Philipp Hennig", "Christian J Schuler"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Hennig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hennig et al\\.", "year": 2012}, {"title": "Portfolio allocation for Bayesian optimization", "author": ["Eric Brochu", "Nando de Freitas"], "venue": "In Proc. UAI,", "citeRegEx": "Hoffman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2011}, {"title": "Self-paced learning with diversity", "author": ["Jiang et al.2014] Lu Jiang", "Deyu Meng", "Shoou-I Yu", "Zhenzhong Lan", "Shiguang Shan", "Alexander Hauptmann"], "venue": "In Proc. NIPS,", "citeRegEx": "Jiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2014}, {"title": "Self-paced curriculum learning", "author": ["Jiang et al.2015] Lu Jiang", "Deyu Meng", "Qian Zhao", "Shiguang Shan", "Alexander G Hauptmann"], "venue": "In Proc. AAAI,", "citeRegEx": "Jiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2015}, {"title": "A taxonomy of global optimization methods based on response surfaces", "author": ["Donald R Jones"], "venue": "Journal of global optimization,", "citeRegEx": "Jones.,? \\Q2001\\E", "shortCiteRegEx": "Jones.", "year": 2001}, {"title": "The development of memory in children", "author": ["Robert Kail"], "venue": "W. H. Freeman and Company,", "citeRegEx": "Kail.,? \\Q1990\\E", "shortCiteRegEx": "Kail.", "year": 1990}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "Benjamin Packer", "Daphne Koller"], "venue": "In Proc. NIPS,", "citeRegEx": "Kumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "Age-of-acquisition ratings for 30,000 english words", "author": ["Hans Stadthagen-Gonzalez", "Marc Brysbaert"], "venue": "Behavior Research Methods,", "citeRegEx": "Kuperman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kuperman et al\\.", "year": 2012}, {"title": "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise", "author": ["Harold J Kushner"], "venue": "Journal of Basic Engineering,", "citeRegEx": "Kushner.,? \\Q1964\\E", "shortCiteRegEx": "Kushner.", "year": 1964}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "In Proc. NAACL", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Learning the easy things first: Self-paced visual category discovery", "author": ["Lee", "Grauman2011] Yong Jae Lee", "Kristen Grauman"], "venue": "In Proc. CVPR,", "citeRegEx": "Lee et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "Measuring biological diversity", "author": ["Anne E Magurran"], "venue": null, "citeRegEx": "Magurran.,? \\Q2013\\E", "shortCiteRegEx": "Magurran.", "year": 2013}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proc. ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "On Bayesian methods for seeking the extremum", "author": ["Mo\u010dkus et al.1978] Jonas Mo\u010dkus", "Vytautas Tiesis", "Antanas \u017dilinskas"], "venue": "Towards global optimization,", "citeRegEx": "Mo\u010dkus et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Mo\u010dkus et al\\.", "year": 1978}, {"title": "Revisiting readability: A unified framework for predicting text quality", "author": ["Pitler", "Nenkova2008] Emily Pitler", "Ani Nenkova"], "venue": "In Proc. EMNLP,", "citeRegEx": "Pitler et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pitler et al\\.", "year": 2008}, {"title": "A temporal model of text periodicities using gaussian processes", "author": ["Preotiuc-Pietro", "Cohn2013] Daniel Preotiuc-Pietro", "Trevor Cohn"], "venue": "In Proc. EMNLP,", "citeRegEx": "Preotiuc.Pietro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Preotiuc.Pietro et al\\.", "year": 2013}, {"title": "Diversity and dissimilarity coefficients: a unified approach. Theoretical population biology, 21(1):24\u201343", "author": ["C Radhakrishna Rao"], "venue": null, "citeRegEx": "Rao.,? \\Q1982\\E", "shortCiteRegEx": "Rao.", "year": 1982}, {"title": "Gaussian Processes for machine learning", "author": ["Carl Edward Rasmussen"], "venue": null, "citeRegEx": "Rasmussen.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen.", "year": 2006}, {"title": "Principles of categorization", "author": ["Eleanor Rosch"], "venue": "Cognition and categorization,", "citeRegEx": "Rosch.,? \\Q1978\\E", "shortCiteRegEx": "Rosch.", "year": 1978}, {"title": "Species diversity in space and time", "author": ["Michael L Rosenzweig"], "venue": null, "citeRegEx": "Rosenzweig.,? \\Q1995\\E", "shortCiteRegEx": "Rosenzweig.", "year": 1995}, {"title": "Reading level assessment using support vector machines and statistical language models", "author": ["Schwarm", "Ostendorf2005] Sarah E. Schwarm", "Mari Ostendorf"], "venue": "In Proc. ACL,", "citeRegEx": "Schwarm et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schwarm et al\\.", "year": 2005}, {"title": "Semantic word clusters using signed normalized graph cuts. arXiv preprint arXiv:1601.05403", "author": ["Sedoc et al.2016] Jo\u00e3o Sedoc", "Jean Gallier", "Lyle Ungar", "Dean Foster"], "venue": null, "citeRegEx": "Sedoc et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sedoc et al\\.", "year": 2016}, {"title": "Taking the human out of the loop: A review of Bayesian optimization", "author": ["Kevin Swersky", "Ziyu Wang", "Ryan P Adams", "Nando de Freitas"], "venue": "Proc. IEEE,", "citeRegEx": "Shahriari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shahriari et al\\.", "year": 2016}, {"title": "The behavior of organisms: an experimental analysis", "author": ["Burrhus Frederic Skinner"], "venue": "An Experimental Analysis", "citeRegEx": "Skinner.,? \\Q1938\\E", "shortCiteRegEx": "Skinner.", "year": 1938}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["Snoek et al.2012] Jasper Snoek", "Hugo Larochelle", "Ryan P Adams"], "venue": "In Proc. NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proc. EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "From baby steps to leapfrog: How less is more in unsupervised dependency parsing", "author": ["Hiyan Alshawi", "Dan Jurafsky"], "venue": "In Proc. NAACL,", "citeRegEx": "Spitkovsky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["Andreas Krause", "Sham M Kakade", "Matthias Seeger"], "venue": "In Proc. ICML,", "citeRegEx": "Srinivas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2010}, {"title": "A general framework for analysing diversity in science, technology and society", "author": ["Andy Stirling"], "venue": "Journal of the Royal Society Interface,", "citeRegEx": "Stirling.,? \\Q2007\\E", "shortCiteRegEx": "Stirling.", "year": 2007}, {"title": "An analysis of diversity measures", "author": ["Tang et al.2006] E Ke Tang", "Ponnuthurai N Suganthan", "Xin Yao"], "venue": "Machine Learning,", "citeRegEx": "Tang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2006}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proc. CoNLL", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Metaphor detection with cross-lingual model transfer", "author": ["Leonid Boytsov", "Anatole Gershman", "Eric Nyberg", "Chris Dyer"], "venue": "In Proc. ACL", "citeRegEx": "Tsvetkov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2014}, {"title": "On improving the accuracy of readability classification using insights from second language acquisition", "author": ["Vajjala", "Meurers2012] Sowmya Vajjala", "Detmar Meurers"], "venue": "In Proc. BEA,", "citeRegEx": "Vajjala et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vajjala et al\\.", "year": 2012}, {"title": "MRC psycholinguistic database: Machine-usable", "author": ["Michael Wilson"], "venue": null, "citeRegEx": "Wilson.,? \\Q1988\\E", "shortCiteRegEx": "Wilson.", "year": 1988}, {"title": "Bayesian optimization of text representations", "author": ["Lingpeng Kong", "Noah A Smith"], "venue": "In Proc. EMNLP,", "citeRegEx": "Yogatama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 31, "context": "tend to be learned before abstract ones (Rosch, 1978).", "startOffset": 40, "endOffset": 53}, {"referenceID": 7, "context": "Prior research focusing on curriculum strategies in NLP is scarce, and has conventionally been following a paradigm of \u201cstarting small\u201d (Elman, 1993), i.", "startOffset": 136, "endOffset": 149}, {"referenceID": 39, "context": ", initializing the learner with \u201csimple\u201d examples first, and then gradually increasing data complexity (Bengio et al., 2009; Spitkovsky et al., 2010).", "startOffset": 103, "endOffset": 149}, {"referenceID": 39, "context": "In unsupervised grammar induction, an effective curriculum comes from increasing length of training sentences as training progresses (Spitkovsky et al., 2010).", "startOffset": 133, "endOffset": 158}, {"referenceID": 16, "context": "However, they have relied on heuristics in selecting curricula or have followed the intuitions of human and animal learning (Kail, 1990; Skinner, 1938).", "startOffset": 124, "endOffset": 151}, {"referenceID": 36, "context": "However, they have relied on heuristics in selecting curricula or have followed the intuitions of human and animal learning (Kail, 1990; Skinner, 1938).", "startOffset": 124, "endOffset": 151}, {"referenceID": 35, "context": "tions (Shahriari et al., 2016; Bergstra et al., 2011; Snoek et al., 2012).", "startOffset": 6, "endOffset": 73}, {"referenceID": 1, "context": "tions (Shahriari et al., 2016; Bergstra et al., 2011; Snoek et al., 2012).", "startOffset": 6, "endOffset": 73}, {"referenceID": 37, "context": "tions (Shahriari et al., 2016; Bergstra et al., 2011; Snoek et al., 2012).", "startOffset": 6, "endOffset": 73}, {"referenceID": 19, "context": "Choices of the acquisition functions include probability of improvement (Kushner, 1964), expected improvement (EI) (Mo\u010dkus et al.", "startOffset": 72, "endOffset": 87}, {"referenceID": 26, "context": "Choices of the acquisition functions include probability of improvement (Kushner, 1964), expected improvement (EI) (Mo\u010dkus et al., 1978; Jones, 2001), GP upper confidence bound (Srinivas et al.", "startOffset": 115, "endOffset": 149}, {"referenceID": 15, "context": "Choices of the acquisition functions include probability of improvement (Kushner, 1964), expected improvement (EI) (Mo\u010dkus et al., 1978; Jones, 2001), GP upper confidence bound (Srinivas et al.", "startOffset": 115, "endOffset": 149}, {"referenceID": 40, "context": ", 1978; Jones, 2001), GP upper confidence bound (Srinivas et al., 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 43, "context": ", 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al.", "startOffset": 27, "endOffset": 43}, {"referenceID": 12, "context": ", 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al., 2011); see Shahriari et al.", "startOffset": 136, "endOffset": 158}, {"referenceID": 12, "context": ", 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al., 2011); see Shahriari et al. (2016) for an extensive comparison.", "startOffset": 137, "endOffset": 188}, {"referenceID": 12, "context": ", 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al., 2011); see Shahriari et al. (2016) for an extensive comparison. Yogatama et al. (2015) found that the combination of EI as the acquisition function and TPE as the surrogate model performed favorably in Bayesian optimization of text representations; we follow this choice in our model.", "startOffset": 137, "endOffset": 240}, {"referenceID": 42, "context": "known measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013).", "startOffset": 78, "endOffset": 118}, {"referenceID": 9, "context": "known measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013).", "startOffset": 78, "endOffset": 118}, {"referenceID": 41, "context": "2013), to economics and social studies (Stirling, 2007).", "startOffset": 39, "endOffset": 55}, {"referenceID": 13, "context": "Diversity has been shown effective in related research on curriculum learning in language modeling, vision, and multimedia analysis (Bengio et al., 2009; Jiang et al., 2014).", "startOffset": 132, "endOffset": 173}, {"referenceID": 29, "context": "\u2022 Entropy: \u2212 \u2211 i piln(pi) \u2022 Simpson\u2019s index (Simpson, 1949): \u2211 i pi 2 \u2022 Quadratic entropy (Rao, 1982):1 \u2211 i,j dijpipj", "startOffset": 90, "endOffset": 101}, {"referenceID": 39, "context": "Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings.", "startOffset": 0, "endOffset": 25}, {"referenceID": 10, "context": "spired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Meurers, 2012).", "startOffset": 105, "endOffset": 209}, {"referenceID": 31, "context": "We resort to the Prototype theory (Rosch, 1978), which posits that semantic cat-", "startOffset": 34, "endOffset": 47}, {"referenceID": 18, "context": "\u2022 Age of acquisition (AoA) of words was extracted from the crowd-sourced database, containing over 50 thousand English words (Kuperman et al., 2012).", "startOffset": 125, "endOffset": 148}, {"referenceID": 2, "context": "\u2022 Concreteness ratings on the scale of 1\u20135 (1 is most abstract) for 40 thousand English lemmas (Brysbaert et al., 2014).", "startOffset": 95, "endOffset": 119}, {"referenceID": 47, "context": "\u2022 Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988).", "startOffset": 72, "endOffset": 86}, {"referenceID": 45, "context": "lowing Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word embeddings as features in an `2-regularized logistic regression classifier.", "startOffset": 7, "endOffset": 30}, {"referenceID": 37, "context": "Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts.", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "We use the recently proposed LSTMCRF NER model (Lample et al., 2016) which trains a forward-backward LSTM on a given sequence of words (represented as word vectors), the hidden units of which are then used as (the only) features in a CRF model (Lafferty et al.", "startOffset": 47, "endOffset": 68}, {"referenceID": 20, "context": ", 2016) which trains a forward-backward LSTM on a given sequence of words (represented as word vectors), the hidden units of which are then used as (the only) features in a CRF model (Lafferty et al., 2001) to predict the output label sequence.", "startOffset": 183, "endOffset": 206}, {"referenceID": 5, "context": "1993) training, development and test set splits as described in Collins (2002).", "startOffset": 64, "endOffset": 79}, {"referenceID": 6, "context": "For dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) for English on the universal dependencies v1.", "startOffset": 58, "endOffset": 77}, {"referenceID": 25, "context": "Word embeddings were trained using the cbow model implemented in the word2vec toolkit (Mikolov et al., 2013).", "startOffset": 86, "endOffset": 108}, {"referenceID": 39, "context": "\u2022 Sorted baselines: the curriculum is defined by sorting the training data by sentence length in increasing/decreasing order, similarly to (Spitkovsky et al., 2010).", "startOffset": 139, "endOffset": 164}, {"referenceID": 39, "context": "Two prior studies on curriculum learning in NLP are discussed in the paper (Bengio et al., 2009; Spitkovsky et al., 2010).", "startOffset": 75, "endOffset": 121}, {"referenceID": 17, "context": "related research on self-paced learning has been explored more deeply in computer vision (Bengio et al., 2009; Kumar et al., 2010; Lee and Grauman, 2011) and in multimedia analysis (Jiang et al.", "startOffset": 89, "endOffset": 153}, {"referenceID": 14, "context": ", 2010; Lee and Grauman, 2011) and in multimedia analysis (Jiang et al., 2015).", "startOffset": 58, "endOffset": 78}, {"referenceID": 48, "context": "GPs were used in the task of machine translation quality estimation (Cohn and Specia, 2013) and in temporal analysis of social media texts (Preotiuc-Pietro and Cohn, 2013); TPEs were used by Yogatama et al. (2015) for", "startOffset": 191, "endOffset": 214}], "year": 2017, "abstractText": "We use Bayesian optimization to learn curricula for word representation learning, optimizing performance on downstream tasks that depend on the learned representations as features. The curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus. We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order.", "creator": "LaTeX with hyperref package"}}}