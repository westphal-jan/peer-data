{"id": "1605.04131", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2016", "title": "Barzilai-Borwein Step Size for Stochastic Gradient Descent", "abstract": "One of the major issues in stochastic gradient descent (SGD) methods is how to choose an appropriate step size while running the algorithm. Since the traditional line search technique does not apply for stochastic optimization algorithms, the common practice in SGD is either to use a diminishing step size, or to tune a fixed step size by hand. Apparently, these two approaches can be time consuming in practice. In this paper, we propose to use the Barzilai-Borwein (BB) method to automatically compute step sizes for SGD and its variant: stochastic variance reduced gradient (SVRG) method, which leads to two algorithms: SGD-BB and SVRG-BB. We prove that SVRG-BB converges linearly for strongly convex objective function. As a by-product, we prove the linear convergence result of SVRG with Option I proposed in [10], whose convergence result has been missing in the literature. Numerical experiments on standard data sets show that the performance of SGD-BB and SVRG-BB is comparable to and sometimes even better than SGD and SVRG with best-tuned step sizes, and is superior to some advanced SGD variants.", "histories": [["v1", "Fri, 13 May 2016 11:08:50 GMT  (1103kb,D)", "https://arxiv.org/abs/1605.04131v1", null], ["v2", "Mon, 23 May 2016 02:51:08 GMT  (902kb,D)", "http://arxiv.org/abs/1605.04131v2", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["conghui tan", "shiqian ma", "yu-hong dai", "yuqiu qian"], "accepted": true, "id": "1605.04131"}, "pdf": {"name": "1605.04131.pdf", "metadata": {"source": "CRF", "title": "Barzilai-Borwein Step Size for Stochastic Gradient Descent", "authors": ["Conghui Tan", "Shiqian Ma", "Yu-Hong Dai", "Yuqiu Qian"], "emails": ["chtan@se.cuhk.edu.hk,", "sqma@se.cuhk.edu.hk.", "dyh@lsec.cc.ac.cn.", "qyq79@connect.hku.hk."], "sections": [{"heading": "1 Introduction", "text": "The following optimization problem, which minimizes the sum of the cost reductions in the SGD systems (1,2), usually appears in the areas of machine learning (2,1) and machine learning (2,2). (1) The SGD method is usually very extensive, so it does not take into account the cost function of the i-th sample data collection (2,2). (2) The SGD method is usually very extensive, so it is used for given umpteenth sample data (x). (2) The SGD method and its variants are strongly convex. (1) In the t-th iteration of SGD, a random training sample is chosen. (2). (2) and the iterate xt method is updated. (2)"}, {"heading": "2 The Barzilai-Borwein Step Size", "text": "The BB method, proposed by Barzilai and Borwein in [3], has proved to be very successful in solving nonlinear optimization problems (2,1), where f is differentiable. A typical iteration of the quasi-Newton methods to the solution (2,1) assumes the following form: xt + 1 = xt \u2212 B \u2212 1t = f (xt), (2,2), where Bt is an approximation of the Hessen matrix from f to the current iterate xt. Different options of Bt give different quasi-Newton methods. The most important feature of Bt is that it must fulfill the so-called Secant equation (2,2), where Bt = yt is an approximation of the Hessen matrix from f to the current iterate xt."}, {"heading": "3 Barzilai-Borwein Step Size for SVRG", "text": "We see (2,5) and (2,4) that the BB method does not require any parameters and that the step size is calculated during the execution of the algorithm. This was the main motivation for us to develop a black box method for calculating the stochastic gradient, which can automatically calculate the step size without requiring any parameters. In this section we propose to include the BB step size in the SVRG-BB method, which leads to the SVRG-BB method. In this section, the following assumption is made: (1,3) applies to each xt. We also assume that the objective function F (x) is weakly convex, i.e. F (y) \u2265 F (x) + E (y \u2212 x) > (y \u2212 x) + E (y \u2212 x) + E (y \u2212 Y) 22, x, y, y R. We also assume that the gradient of each component fi (x) x \u2212 L-lip tip is continuous, i.e., that it serves (y \u00b2 Y \u2212 Y \u2212 Y \u2212 L, Y \u2212 2.2 \u2212 Y \u2212 L \u2212 L \u2212 Y \u2212 Y \u2212 Y \u2212 Y \u2212 Y, Y \u2212 Y \u2212 Y \u2212 Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Q = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Q = Y = Q = Y = Q = Q = Y = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Y = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q ="}, {"heading": "3.1 SVRG Method", "text": "The SVRG method proposed by Johnson and Zhang [10] to solve the problem (1,1) is described as in Algorithm 1.Algorithm 1 Stochastic Variance Reduced Gradient (SVRG) method parameters: update frequency m, step size \u03b7, starting point x-0 for k = 0, 1, \u00b7 \u00b7 \u00b7 do gk = 1 n \u00b2 n \u00b2 i = 1 \u00b2 fi (x \u00b2 k) x0 = x \u2212 k \u00b2 convergence for option I: x \u00b2 k + 1 = xm Option II: x-1 doRandomly pick it [1,., n} xt + 1 = xt \u2212 \u03b7k (xt) \u2212 sp (x \u00b2 fit (x \u00b2 k) end for option I: x \u00b2 k + 1 = xm Option II: x \u00b2 k + 1 = xt for randomly chosen t {1,.., m} end forThere are two loops in SVRG (algorithm 1)."}, {"heading": "3.2 SVRG-BB Method", "text": "In this section, we propose the SVRG-BB method, which calculates the step size using the BB method instead of a prefix as in SVRG.Remark 3.3. A few comments are asked for the SVRG-BB algorithm. \u2212 It can be noted that in the latter, the step size is calculated according to the BB formula (2,4) divided by m. This is because in the inner loop to update the xt, m unbiased gradient estimators are added to x0 x gxt. \u2212 If we calculate the step size according to the BB formula (2,4) divided by m, this is possible because in the inner loop to update the xt, m unbiased gradient estimators match x0 x gxt to xm.2. If we always set the step size according to SVRG = 3. BB instead of BB-G (BB step according to SVRG) are not specified according to SVRG level, then we will be specified according to SVRG level."}, {"heading": "3.3 Linear Convergence Analysis", "text": "In this section, we will analyze the linear convergence of SVRG-BB (algorithm 2) for the solution (1.1) with strongly convex objective F (x) and as a by-product, and our analysis will also include the linear convergence of SVRG-I. The following problem, which comes from [16], is useful for our analysis. If f (x): Rd \u2192 R is convex and its gradient is L-Lipschitz continuous, then f (x) \u2212 x f (y) - x (y) - x (x) - x x - x (x) - x - x - x - x - x (x) - x (x) - x - x - x (x) - x - x (x) - x - x (x) - x - x (x) - x (x) - x - x (x) - x (x)."}, {"heading": "4 Barzilai-Borwein Step Size for SGD", "text": "In this section we will use the BB method to SGD (1,2), BB-BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), BB (25), SGD (30), S (30), S (30), S (25), S (30), S (30), S (30), S (30), S (30), S (30), S (30), S (30)."}, {"heading": "4.1 Smoothing Technique for the Step Sizes", "text": "Due to the randomness of the stochastic gradients, the step size calculated in SGD-BB can sometimes vibrate drastically, and this can lead to instability of the algorithm. \u2212 Inspired by [14], we propose the following smoothing technique to stabilize the step size.It is known that to ensure the convergence of SGD, the step sizes must be reduced. \u2212 Similar to [14], we assume that the step sizes are in the form of C / \u03c6 (k), where C > 0 is an unknown constant that needs to be estimated. \u00b7 (k) is a predetermined function that controls the decrease rate of the step size, and is a typical choice of step sizes."}, {"heading": "4.2 Incorporating BB Step Size to SGD Variants", "text": "The BB step size and smoothing technique that we used in SGD-BB (algorithm 3) can also be used in other variants of SGD. In this section, we will use SAG as an example to illustrate how to include the BB step size. SAG with BB step size (referred to as SAG-BB) is used as in Algorithm 4. Since SAG does not need any decreasing step sizes to ensure convergence, we simply choose? (k) in the smoothing technique. 1. In this case, the smoothed step size is equal to the geometric mean of all previous BB step sizes. \u2212 Algorithm 4 SAG with BB step size (SAG-BB) Parameters: update frequency m, initial step sizes 1 and 1 (used only in the first two epochs), weighting parameter \u03b2 (0, 1), starting point x \u00b2 k = 1, k = k = 1, k = 1, k = 1, k = 1 x \u00b2 x = 1, 1 x \u00b2 x = 1."}, {"heading": "5 Numerical Experiments", "text": "In this section, we will perform some numerical experiments to demonstrate the effectiveness of our SVRG-BB (algorithm 2) and SGD-BB (algorithm 3) algorithms. In particular, we will use SVRG-BB and SGD-BB to solve two standard test problems in machine learning: logistic regression with \"2-standard regularization (LR) min xF (x) = 1n n n n \u2211 i = 1 log [1 + exp (\u2212 bia > i x)] + \u03bb 2 x 22, (5,1) and square hinge loss SVM with '2-standard regularization (SVM) min xF (x) = 1n \u00b2 i = 1 ([1 \u2212 bia > i x] +) 2 + \u03bb 2 \u0445 x \u00b2 22, (5,2), where ai-Rd and bi-Rd (\u00b1 1} are the characteristic vector and class name of the i-th sample."}, {"heading": "5.1 Numerical Results of SVRG-BB", "text": "In this section, we compare SVRG-BB (algorithm 2) with SVRG (algorithm 1) with SVRG-BB (algorithm 1) with automatic (5,1) and (5,2). We used the most coordinated step size for SVRG, and three different initial step sizes \u03b70 for SVRG-BB. For SVRG-BB and SVRG, we set m = 2n as proposed in [10]. SVRG-BB and SVRG comparison results are shown in Figure 1. In all six sub-figures, the x-axis denotes the number of epochs k, i.e. the number of outer loops in Figure 2, i.e. the number of outer loops in Figure 1 (a), 1 (b), and 1 (c), the y-axis denotes the sub-optimality F (x) \u2212 F (x) (x), and in Figure 1 (d), 1 (e), 1 (e), and 1 (f), the axis refers to the y-size."}, {"heading": "5.2 Numerical Results of SGD-BB", "text": "In this section, we compare SGD-BB with smoothing technique (algorithm 3) with SGD with solution steps (5.1) and (5.2). In our experiments, we used m = n, \u03b2 = 10 / m and \u03b71 = \u03b70. In applying the smoothing technique, we used \u03c6 (k) = k + 1. Since SGD requires a reduced step size to converge, we tested SGD with decreasing step size in the form \u03b7 / (k + 1) with different constants. The comparison results are in Figure 2. Similar to Figure 1, the dashed line with black color represents SGD with the matched size, and the green and red dashed lines correspond to the other two selection steps. The solid lines with blue, violet and yellow colors in Figure 2 (a) and 2 (d) correspond better than the tuned Figures 0 = 10, 1 and 0.1; the solid lines with blue, and yellow (BB) with only 0.00b and 0.00b."}, {"heading": "5.3 Comparison with Other Methods", "text": "In this section, we mainly compare our SGD-BB (algorithm 3) and SAG-BB (algorithm 4) with three existing methods: AdaGrad [8], SAG with line search (referred to as SAG-L) [22] and a stochastic quasi-Newton method: oLBFGS [23]. For both SGD-BB and SAG-BB, we use m = n and \u03b2 = 10 / m. Since these methods have very different complexity per iteration, we compare their CPU time required to achieve the same sub-optimality. Figures 3 (a), 3 (b) and 3 (c) show the comparison results of SGD-BB and AdaGrad. From these figures, we see that AdaGrad usually has a very fast start, but in many cases convergence becomes slower than in later iterations. In addition, AdaGrad is relatively sensitive to the initial sizes of BGD-BB and BAG."}, {"heading": "6 Conclusion", "text": "In this paper, we proposed to use the BB method to calculate the step sizes for SGD and SVRG, leading to two new stochastic gradient methods: SGD-BB and SVRG-BB. We demonstrated the linear convergence of SVRG-BB for strongly convex function, and as a by-product, we demonstrated the linear convergence of the original SVRG with Option I for strongly convex function. We also proposed a smoothing technique to stabilize the step sizes generated in SGD-BB and demonstrated how to transfer the BB method to other SGD variants such as SAG. We conducted numerical experiments on real data sets to compare the performance of SVRG-BB and SGD-BB with existing methods. Numerical results showed that the performance of our SVRG-BB and SGD-BB variants is comparable and sometimes even better than that of the original SVRG and SGD with striped step sizes and SD variants."}], "references": [{"title": "Variance reduction for faster non-convex optimization", "author": ["Z. Allen-Zhu", "E. Hazan"], "venue": "arXiv preprint arXiv:1603.05643,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Stop wasting my gradients: Practical SVRG", "author": ["R. Babanezhad", "M.O. Ahmed", "A. Virani", "M. Schmidt", "K. Kone\u010dn\u1ef3", "S. Sallinen"], "venue": "Advances in Neural Information Processing Systems, pages 2242\u20132250,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Two-point step size gradient methods", "author": ["J. Barzilai", "J.M. Borwein"], "venue": "IMA Journal of Numerical Analysis, 8(1):141\u2013148,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1988}, {"title": "A new analysis on the Barzilai-Borwein gradient method", "author": ["Y.-H. Dai"], "venue": "Journal of Operations Research Society of China, 1(2):187\u2013198,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Projected Barzilai-Borwein methods for large-scale boxconstrained quadratic programming", "author": ["Y.-H. Dai", "R. Fletcher"], "venue": "Numerische Mathematik, 100(1):21\u201347,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "The cyclic Barzilai-Borwein method for unconstrained optimization", "author": ["Y.-H. Dai", "W.W. Hager", "K. Schittkowski", "H. Zhang"], "venue": "IMA Journal of Numerical Analysis, 26(3):604\u2013 627,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["A. Defazio", "F. Bach", "S. Lacoste-Julien"], "venue": "Advances in Neural Information Processing Systems, pages 1646\u20131654,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "On the Barzilai-Borwein method", "author": ["R. Fletcher"], "venue": "Optimization and control with applications, pages 235\u2013256. Springer,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems, pages 315\u2013323,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Accelerated stochastic approximation", "author": ["H. Kesten"], "venue": "The Annals of Mathematical Statistics, 29(1):41\u201359,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1958}, {"title": "Semi-stochastic gradient descent methods", "author": ["J. Kone\u010dn\u1ef3", "P. Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1312.1666,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic line searches for stochastic optimization", "author": ["M. Mahsereci", "P. Hennig"], "venue": "arXiv preprint arXiv:1502.02846,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Speed learning on the fly", "author": ["P.Y. Mass\u00e9", "Y. Ollivier"], "venue": "arXiv preprint arXiv:1511.02540,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm", "author": ["D. Needell", "N. Srebro", "R. Ward"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Introductory lectures on convex optimization, volume 87", "author": ["Y. Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Stochastic proximal gradient descent with acceleration techniques", "author": ["A. Nitanda"], "venue": "Advances in Neural Information Processing Systems, pages 1574\u20131582,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM J. Control and Optimization, 30:838\u2013855,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1992}, {"title": "On the Barzilai and Borwein choice of steplength for the gradient method", "author": ["M. Raydan"], "venue": "IMA Journal of Numerical Analysis, 13(3):321\u2013326,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1993}, {"title": "The Barzilai and Borwein gradient method for the large scale unconstrained minimization problem", "author": ["M. Raydan"], "venue": "SIAM Journal on Optimization, 7(1):26\u201333,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Stochastic variance reduction for nonconvex optimization", "author": ["S.J. Reddi", "A. Hefny", "S. Sra", "B. Poczos", "A. Smola"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["R.L. Roux", "M. Schmidt", "F. Bach"], "venue": "Advances in Neural Information Processing Systems, pages 2663\u20132671,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["N.N. Schraudolph", "J. Yu", "S. G\u00fcnter"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 436\u2013443,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Jornal of Machine Learning Research, 14:567\u2013599,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic gradient descent with Barzilai-Borwein update step for svm", "author": ["K. Sopy  la", "P. Drozda"], "venue": "Information Sciences,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Projected Barzilai-Borwein methods for large scale nonnegative image restorations", "author": ["Y. Wang", "S. Ma"], "venue": "Inverse Problems in Science and Engineering, 15(6):559\u2013583,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "A fast algorithm for sparse reconstruction based on shrinkage, subspace optimization, and continuation", "author": ["Z. Wen", "W. Yin", "D. Goldfarb", "Y. Zhang"], "venue": "SIAM J. SCI. COMPUT, 32(4):1832\u20131857,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R.D. Nowak", "M.A.T. Figueiredo"], "venue": "IEEE Transactions on Signal Processing, 57(7):2479\u20132493,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["L. Xiao", "T. Zhang"], "venue": "SIAM Journal on Optimization, 24(4):2057\u20132075,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic optimization with importance sampling for regularized loss minimization", "author": ["P. Zhao", "T. Zhang"], "venue": "ICML,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "As a by-product, we prove the linear convergence result of SVRG with Option I proposed in [10], whose convergence result is missing in the literature.", "startOffset": 90, "endOffset": 94}, {"referenceID": 21, "context": "[22] updates the iterates by", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "It is shown in [22] that SAG converges linearly for strongly convex problems.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "[7] is an improved version of SAG, and it does not require the strong convexity assumption.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "The SDCA method proposed by Shalev-Shwartz and Zhang [24] also requires to store all the component gradients.", "startOffset": 53, "endOffset": 57}, {"referenceID": 9, "context": "The stochastic variance reduced gradient (SVRG) method proposed by Johnson and Zhang [10] is now widely used in the machine learning community for solving (1.", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "[22], one important issue regarding to stochastic algorithms (SGD and its variants) that has not been fully addressed in the literature, is how to choose an appropriate step size \u03b7t while running the algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "AdaGrad [8] scales the gradient by the square root of the accumulated magnitudes of the gradients in the past iterations, but it still requires a fixed step size \u03b7.", "startOffset": 8, "endOffset": 11}, {"referenceID": 21, "context": "[22] suggests a line search technique on the component function fik(x) selected in each iteration, to estimate step size for SAG.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] suggests performing line search for an estimated function, which is evaluated by a Gaussian process with samples fit(xt).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] suggests to generate the step sizes by a given function with an unknown parameter, and to use the online SGD to update this unknown parameter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "As a byproduct, we show the linear convergence of SVRG with Option I (SVRG-I) proposed in [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "Note that in [10] only convergence of SVRG with Option II (SVRG-II) was given, and the proof for SVRG-I has been missing in the literature.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "However, SVRG-I is numerically a better choice than SVRG-II, as demonstrated in [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "The BB method, proposed by Barzilai and Borwein in [3], has been proved to be very successful in solving nonlinear optimization problems.", "startOffset": 51, "endOffset": 54}, {"referenceID": 18, "context": "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [19, 20, 9, 5, 6, 4] and references therein.", "startOffset": 108, "endOffset": 128}, {"referenceID": 19, "context": "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [19, 20, 9, 5, 6, 4] and references therein.", "startOffset": 108, "endOffset": 128}, {"referenceID": 8, "context": "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [19, 20, 9, 5, 6, 4] and references therein.", "startOffset": 108, "endOffset": 128}, {"referenceID": 4, "context": "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [19, 20, 9, 5, 6, 4] and references therein.", "startOffset": 108, "endOffset": 128}, {"referenceID": 5, "context": "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [19, 20, 9, 5, 6, 4] and references therein.", "startOffset": 108, "endOffset": 128}, {"referenceID": 3, "context": "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [19, 20, 9, 5, 6, 4] and references therein.", "startOffset": 108, "endOffset": 128}, {"referenceID": 27, "context": "Recently, BB method has been successfully applied for solving problems arising from emerging applications, such as compressed sensing [28], sparse reconstruction [27] and image processing [26].", "startOffset": 134, "endOffset": 138}, {"referenceID": 26, "context": "Recently, BB method has been successfully applied for solving problems arising from emerging applications, such as compressed sensing [28], sparse reconstruction [27] and image processing [26].", "startOffset": 162, "endOffset": 166}, {"referenceID": 25, "context": "Recently, BB method has been successfully applied for solving problems arising from emerging applications, such as compressed sensing [28], sparse reconstruction [27] and image processing [26].", "startOffset": 188, "endOffset": 192}, {"referenceID": 9, "context": "1 SVRG Method The SVRG method proposed by Johnson and Zhang [10] for solving (1.", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "This has been confirmed numerically in [10] where SVRG-I was applied to solve real applications.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": ", [10], [12] and [2]), and the convergence for SVRG-I has been missing in the literature.", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": ", [10], [12] and [2]), and the convergence for SVRG-I has been missing in the literature.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": ", [10], [12] and [2]), and the convergence for SVRG-I has been missing in the literature.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "We now cite the convergence analysis of SVRG-II given in [10] as follows.", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "2 ([10]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "Xiao and Zhang [29] developed a proximal SVRG method for minimizing the finite sum function plus a nonsmooth regularizer.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "[17] applied Nesterov\u2019s acceleration technique to SVRG to improve the convergence rate that depends on the condition number L/\u03bc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] proved if the full gradient computation gk was replaced by a growing-batch estimation, the linear convergence rate can be preserved.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] and [21] showed that SVRG with minor modifications can converge to a stationary point for nonconvex optimization problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[1] and [21] showed that SVRG with minor modifications can converge to a stationary point for nonconvex optimization problems.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "The BB step size can also be naturally incorporated to other SVRG variants, such as SVRG with batching [2].", "startOffset": 103, "endOffset": 106}, {"referenceID": 15, "context": "The following lemma, which is from [16], is useful in our analysis.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "In SGD, \u2207fit(xt) is an unbiased estimation for \u2207F (xt) when it is uniformly sampled (see [15, 30] for studies on importance sampling, which does not sample it uniformly).", "startOffset": 89, "endOffset": 97}, {"referenceID": 29, "context": "In SGD, \u2207fit(xt) is an unbiased estimation for \u2207F (xt) when it is uniformly sampled (see [15, 30] for studies on importance sampling, which does not sample it uniformly).", "startOffset": 89, "endOffset": 97}, {"referenceID": 24, "context": "The recent work by Sopy la and Drozda [25] suggested several variants of this idea to compute an estimated BB step size using the stochastic gradients.", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "However, these ideas lack theoretical justifications and the numerical results in [25] show that these approaches are inferior to existing methods such as averaged SGD [18].", "startOffset": 82, "endOffset": 86}, {"referenceID": 17, "context": "However, these ideas lack theoretical justifications and the numerical results in [25] show that these approaches are inferior to existing methods such as averaged SGD [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 10, "context": "This kind of idea can be traced back to [11].", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "Inspired by [14], we propose the following smoothing technique to stabilize the step size.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "Similar as in [14], we assume the step sizes are in the form of C/\u03c6(k), where C > 0 is an unknown constant that needs to be estimated, \u03c6(k) is a pre-specified function that controls the decreasing rate of the step size, and a typical choice of function \u03c6 is \u03c6(k) = k + 1.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "For both SVRG-BB and SVRG, we set m = 2n as suggested in [10].", "startOffset": 57, "endOffset": 61}, {"referenceID": 7, "context": "3 Comparison with Other Methods In this section, we compare our SGD-BB (Algorithm 3) and SAG-BB (Algorithm 4) with three existing methods: AdaGrad [8], SAG with line search (denoted as SAG-L) [22], and a stochastic quasi-Newton method: oLBFGS [23].", "startOffset": 147, "endOffset": 150}, {"referenceID": 21, "context": "3 Comparison with Other Methods In this section, we compare our SGD-BB (Algorithm 3) and SAG-BB (Algorithm 4) with three existing methods: AdaGrad [8], SAG with line search (denoted as SAG-L) [22], and a stochastic quasi-Newton method: oLBFGS [23].", "startOffset": 192, "endOffset": 196}, {"referenceID": 22, "context": "3 Comparison with Other Methods In this section, we compare our SGD-BB (Algorithm 3) and SAG-BB (Algorithm 4) with three existing methods: AdaGrad [8], SAG with line search (denoted as SAG-L) [22], and a stochastic quasi-Newton method: oLBFGS [23].", "startOffset": 243, "endOffset": 247}], "year": 2016, "abstractText": "One of the major issues in stochastic gradient descent (SGD) methods is how to choose an appropriate step size while running the algorithm. Since the traditional line search technique does not apply for stochastic optimization algorithms, the common practice in SGD is either to use a diminishing step size, or to tune a fixed step size by hand, which can be time consuming in practice. In this paper, we propose to use the Barzilai-Borwein (BB) method to automatically compute step sizes for SGD and its variant: stochastic variance reduced gradient (SVRG) method, which leads to two algorithms: SGD-BB and SVRGBB. We prove that SVRG-BB converges linearly for strongly convex objective functions. As a by-product, we prove the linear convergence result of SVRG with Option I proposed in [10], whose convergence result is missing in the literature. Numerical experiments on standard data sets show that the performance of SGD-BB and SVRG-BB is comparable to and sometimes even better than SGD and SVRG with best-tuned step sizes, and is superior to some advanced SGD variants.", "creator": "LaTeX with hyperref package"}}}