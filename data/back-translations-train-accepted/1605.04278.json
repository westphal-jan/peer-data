{"id": "1605.04278", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2016", "title": "Universal Dependencies for Learner English", "abstract": "We introduce the Treebank of Learner English (TLE), the first publicly available syntactic treebank for English as a Second Language (ESL). The TLE provides manually annotated POS tags and Universal Dependency (UD) trees for 5,124 sentences from the Cambridge First Certificate in English (FCE) corpus. The UD annotations are tied to a pre-existing error annotation of the FCE, whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence. Further on, we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungrammatical English. Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy. We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language.", "histories": [["v1", "Fri, 13 May 2016 18:45:22 GMT  (78kb)", "http://arxiv.org/abs/1605.04278v1", null], ["v2", "Wed, 8 Jun 2016 02:33:34 GMT  (65kb)", "http://arxiv.org/abs/1605.04278v2", "Updated parsing experiments to EWT v1.3, improved grammatical error marking, minor revisions. To appear in ACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yevgeni berzak", "jessica kenney", "carolyn spadine", "jing xian wang", "lucia lam", "keiko sophie mori", "sebastian garza", "boris katz"], "accepted": true, "id": "1605.04278"}, "pdf": {"name": "1605.04278.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jing Xian Wang", "Keiko Sophie Mori"], "emails": ["berzak@mit.edu", "jessk@mit.edu", "cspadine@mit.edu", "jhwang@mit.edu", "lucci@mit.edu", "ksmori@mit.edu", "sjgarza@mit.edu", "boris@mit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.04 278v 1 [cs.C L] 13 M"}, {"heading": "1 Introduction", "text": "This year it has come to be a reactionary, reactionary, reactionary and reactionary party."}, {"heading": "2 Treebank Overview", "text": "The TLE currently contains 5,124 sentences (97,683 characters) with POS tags and dependency notes in the formalism of the English Universal Dependencies (UD) (De Marneffe et al., 2014).The sentences were taken from the FCE corpus (Yannakoudakis et al., 2011), a collection of upper intermediate English learning essays, with error notes in 75 error categories (Nicholls, 2003).Segmentation of the sentence was carried out using an adaptation of the NLTK sentence tokenizer2. Subsegmented sentences were made during the course of the an-2http: / / www.nltk.org / api / nltk.tokenize.htmlnotation."}, {"heading": "3 Annotator Training", "text": "The tree bank was commented on by six students, five of whom are students, one of whom is a doctoral student. Among the students are three major students of linguistics and two major students of engineering with a minor in linguistics. The doctoral student is a linguist specializing in Syntax.shtml8 weeks. Prior to commenting the tree bank sentences, the commenters underwent a training period of about3http: / / nlp.stanford.edu / software / tokenizer.shtml8 weeks. During the training, the commenters attended tutorials on dependency programs and learned the UD Guidelines 4, the Penn Treebank POS Guidelines (Santorini, 1990), the FCE grammatical error listing scheme (Nicholls, 2003), as well as the ESL Guidelines set forth in Section 5.UD. In addition, the commenters completed six commenting OS, were asked to comment on and in English tags."}, {"heading": "4 Annotation Procedure", "text": "The tree bank was formed in four steps: annotation, review, resolution of disagreements and targeted debugging."}, {"heading": "4.1 Annotation", "text": "In the first phase, commentators were given sentences to comment on. We use a CoNLL-style text template in which each word is commented on on a separate line. Each line contains 6 columns, the first of which has the word index (IND) and the second the word itself (WORD). The remaining four columns had to be filled in with a Universal POS tag5 (UPOS), a Penn Treebank POS tag4http: / / universaldependencies.org / en 5http: / universaldependencies.org / en / all.html (POS), a header word index (HIND) and a dependency ratio (REL) according to version 1 of the UD Guidelines. The comment section of the sentence is preceded by a metadata heading."}, {"heading": "4.2 Review", "text": "All annotated sentences were randomly assigned to a second commentator (in the future a reviewer), in a double-blind manner. The reviewer's task was to split the sentences according to the markers in the SEGMENT field if they apply to both the original and the corrected version of the sentence. The resulting segments without grammatical errors in the original version are currently being discarded. Highlight all annotations that they would have commented differently. To help the review process, we have compiled a list of the most common annotation errors available in the published annotation handbook. The annotations were checked using an active editing scheme in which an explicit action was required for all existing annotations. The scheme was introduced to prevent reviewers from overlooking annotation problems due to passive consent. In particular, an additional # character was added to the seventh character of each annotation mark in which an annotation was made."}, {"heading": "4.3 Disagreement Resolution", "text": "In the final stage of the comment process, all disagreements between the commentator and the reviewer were settled by a third commentator (henceforth Judge) whose primary task was to decide in favor of the commentator or the reviewer. Similar to the review process, the review task was performed in a double-blind manner. Judges were allowed to resolve disagreements between the commentator and the reviewer with a third alternative, as well as introduce new corrections for annotation issues that were overlooked by the reviewers. An additional task of the judges was to identify acceptable alternative annotations for ambiguous structures that existed due to disagreements in the review or otherwise in the judgment. These annotations were specified in an additional metadata field called # AMBIGUITY. The ambiguity markers are provided along with the resolved version of the annotations."}, {"heading": "4.4 Final Debugging", "text": "After implementing the decisions made by the judges, we used debugging tests to query the corpus for specific linguistic constructions. This additional testing phase further reduced the number of annotation errors and inconsistencies in the tree bank. Including the familiarization time, the creation of the tree bank took over a year with a total of over 2,000 hours of annotations."}, {"heading": "5 Annotation Scheme for ESL", "text": "Our comments use the existing stock of English UD-POS tags and dependency relationships and follow the standard UD annotation guidelines for English. However, these guidelines were designed for the grammatical use of English and do not cover canonical syntactic structures that occur due to grammatical errors.8 In order to promote the uniform and linguistically motivated annotations of such structures, we formulated a supplementary set of ESL annotation guidelines. Our annotation guidelines for the original sentences follow the general principle of literal reading, which emphasizes syntactical analysis according to observed linguistic usage. This strategy continues a series of work in SLA that advocates centering the analysis of the learning language around morphosyntactic surface evidence (Ragheb and Dickinson, 2012; Dickinson and Ragheb, 2013). Similar to our framework, which includes a parallel annotation of corrected sentences, such anecdotes are often presented in contexts of multiple contexts."}, {"heading": "5.1 Literal Annotation", "text": "< M > nbsp; M > n > n > n & ndW > n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp; n & nbsp;. \""}, {"heading": "5.2 Exceptions to Literal Annotation", "text": "W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W"}, {"heading": "6 Editing Agreement", "text": "We use our two-step review process to estimate the match rates between commentators 9. We measure the match as a fraction of the comments made by the editor. Table 2 shows the match between commentators and reviewers, as well as the match between reviewers and the final version of the comments made by the judges. Match measurements are provided for both the original and the corrected versions of the dataset. Overall, the results indicate a high match rate in the two editing tasks. In addition, the gap between the agreement on the original and corrected sentences is small. Note that despite the introduction of several ESL annotation guidelines during the annotation process, this result is achieved, which has inevitably increased the number of edits related to grammatical errors. We interpret this result as proof of the effectiveness of the ESL annotation scheme to support consistent annotation of all 9consistent language checks."}, {"heading": "7 Parsing Experiments", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "8 Related Work", "text": "In fact, it is that we see ourselves in a position to be in, and that we are in a position to be able to move to another world where we are in a position we are in, \"he said."}, {"heading": "9 Conclusion", "text": "We will present the first large tree bank of learning languages, which will be commented manually and duplicated for POS tags and universal dependencies. The note will be accompanied by a linguistically motivated framework for dealing with syntactic structures associated with grammatical errors. Finally, we will measure the automatic marking and analysis on our corpus and measure the effects of grammatical errors on marking and analysis sequence. The tree bank will support empirical studies of learner syntax in NLP, corpus linguistics and the acquisition of foreign languages."}, {"heading": "10 Acknowledgements", "text": "We thank Anna Korhonen for helpful discussions and insightful comments on this paper. We also thank Dora Alexopoulou, Andrei Barbu, Markus Dickinson, Sue Felshin, Jeroen Geertzen, Yan Huang, Detmar Meurers, Sampo Pyysalo, Roi Reichart and the anonymous critics for valuable feedback on this work. This material is based on work supported by the Center for Brains, Minds, and Machines (CBMM), funded by the NSF STC Prize CCF-1231216."}], "references": [{"title": "Self-training for parsing learner text", "author": ["Aoife Cahill", "Binod Gyawali", "James V Bruno."], "venue": "Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages,", "citeRegEx": "Cahill et al\\.,? 2014", "shortCiteRegEx": "Cahill et al\\.", "year": 2014}, {"title": "A Coefficient of Agreement for Nominal Scales", "author": ["J. Cohen."], "venue": "Educational and Psychological Measurement, 20(1):37.", "citeRegEx": "Cohen.,? 1960", "shortCiteRegEx": "Cohen.", "year": 1960}, {"title": "English as a global language", "author": ["David Crystal."], "venue": "Ernst Klett Sprachen.", "citeRegEx": "Crystal.,? 2003", "shortCiteRegEx": "Crystal.", "year": 2003}, {"title": "Stanford typed dependencies manual", "author": ["Marie-Catherine De Marneffe", "Christopher D Manning."], "venue": "Technical report, Technical report, Stanford University.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Universal stanford dependencies: A cross-linguistic typology", "author": ["Marie-Catherine De Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D Manning."], "venue": "Proceedings of LREC, pages 4585\u20134592.", "citeRegEx": "Marneffe et al\\.,? 2014", "shortCiteRegEx": "Marneffe et al\\.", "year": 2014}, {"title": "Towards interlanguage pos annotation for effective learner corpora in sla and flt", "author": ["Ana D\u0131az-Negrillo", "Detmar Meurers", "Salvador Valera", "Holger Wunsch."], "venue": "Language Forum, 36(1\u20132):139\u2013154.", "citeRegEx": "D\u0131az.Negrillo et al\\.,? 2010", "shortCiteRegEx": "D\u0131az.Negrillo et al\\.", "year": 2010}, {"title": "Dependency annotation for learner corpora", "author": ["Markus Dickinson", "Marwa Ragheb."], "venue": "Proceedings of the Eighth Workshop on Treebanks and Linguistic Theories (TLT-8), pages 59\u201370.", "citeRegEx": "Dickinson and Ragheb.,? 2009", "shortCiteRegEx": "Dickinson and Ragheb.", "year": 2009}, {"title": "Annotation for learner English guidelines, v", "author": ["Markus Dickinson", "Marwa Ragheb."], "venue": "0.1. Technical report, Indiana University, Bloomington, IN, June. June 9, 2013.", "citeRegEx": "Dickinson and Ragheb.,? 2013", "shortCiteRegEx": "Dickinson and Ragheb.", "year": 2013}, {"title": "Adapting a wsj-trained parser to grammatically noisy text", "author": ["Jennifer Foster", "Joachim Wagner", "Josef Van Genabith."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technolo-", "citeRegEx": "Foster et al\\.,? 2008", "shortCiteRegEx": "Foster et al\\.", "year": 2008}, {"title": "Treebanks gone bad", "author": ["Jennifer Foster."], "venue": "International Journal of Document Analysis and Recognition (IJDAR), 10(3-4):129\u2013145.", "citeRegEx": "Foster.,? 2007", "shortCiteRegEx": "Foster.", "year": 2007}, {"title": "Automatic linguistic annotation of large scale l2 databases: The ef-cambridge open language database (efcamdat)", "author": ["Jeroen Geertzen", "Theodora Alexopoulou", "Anna Korhonen."], "venue": "Proceedings of the 31st Second Language Research Forum. Somerville,", "citeRegEx": "Geertzen et al\\.,? 2013", "shortCiteRegEx": "Geertzen et al\\.", "year": 2013}, {"title": "Syntactic annotation of noncanonical linguistic structures", "author": ["Hagen Hirschmann", "Seanna Doolittle", "Anke L\u00fcdeling"], "venue": null, "citeRegEx": "Hirschmann et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hirschmann et al\\.", "year": 2007}, {"title": "Turning on the turbo: Fast third-order non-projective turbo parsers", "author": ["Andr\u00e9 FT Martins", "Miguel Almeida", "Noah A Smith."], "venue": "ACL (2), pages 617\u2013622. Citeseer.", "citeRegEx": "Martins et al\\.,? 2013", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Universal dependency annotation for multilingual parsing", "author": ["Ryan T McDonald", "Joakim Nivre", "Yvonne QuirmbachBrundage", "Yoav Goldberg", "Dipanjan Das", "Kuzman Ganchev", "Keith B Hall", "Slav Petrov", "Hao Zhang", "Oscar T\u00e4ckstr\u00f6m"], "venue": null, "citeRegEx": "McDonald et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2013}, {"title": "Creating a manually error-tagged and shallow-parsed learner corpus", "author": ["Ryo Nagata", "Edward Whittaker", "Vera Sheinman."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Nagata et al\\.,? 2011", "shortCiteRegEx": "Nagata et al\\.", "year": 2011}, {"title": "The conll-2014 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant."], "venue": "CoNLL Shared Task, pages 1\u201314.", "citeRegEx": "Ng et al\\.,? 2014", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "The cambridge learner corpus: Error coding and analysis for lexicography and elt", "author": ["Diane Nicholls."], "venue": "Proceedings of the Corpus Linguistics 2003 conference, pages 572\u2013581.", "citeRegEx": "Nicholls.,? 2003", "shortCiteRegEx": "Nicholls.", "year": 2003}, {"title": "Defining syntax for learner language annotation", "author": ["Marwa Ragheb", "Markus Dickinson."], "venue": "COLING (Posters), pages 965\u2013974.", "citeRegEx": "Ragheb and Dickinson.,? 2012", "shortCiteRegEx": "Ragheb and Dickinson.", "year": 2012}, {"title": "Syntactic annotation of learner corpora", "author": ["Victoria Ros\u00e9n", "Koenraad De Smedt."], "venue": "Systematisk, variert, men ikke tilfeldig, pages 120\u2013132.", "citeRegEx": "Ros\u00e9n and Smedt.,? 2010", "shortCiteRegEx": "Ros\u00e9n and Smedt.", "year": 2010}, {"title": "Evaluating and automating the annotation of a learner corpus", "author": ["Alexandr Rosen", "Jirka Hana", "Barbora \u0160tindlov\u00e1", "Anna Feldman."], "venue": "Language Resources and Evaluation, 48(1):65\u201392.", "citeRegEx": "Rosen et al\\.,? 2014", "shortCiteRegEx": "Rosen et al\\.", "year": 2014}, {"title": "Part-of-speech tagging guidelines for the penn treebank project (3rd revision)", "author": ["Beatrice Santorini."], "venue": "Technical Reports (CIS).", "citeRegEx": "Santorini.,? 1990", "shortCiteRegEx": "Santorini.", "year": 1990}, {"title": "A gold standard dependency corpus for english", "author": ["Natalia Silveira", "Timothy Dozat", "Marie-Catherine de Marneffe", "Samuel R Bowman", "Miriam Connor", "John Bauer", "Christopher D Manning."], "venue": "Proceedings of the Ninth International Conference", "citeRegEx": "Silveira et al\\.,? 2014", "shortCiteRegEx": "Silveira et al\\.", "year": 2014}, {"title": "Using parse features for preposition selection and error detection", "author": ["Joel Tetreault", "Jennifer Foster", "Martin Chodorow."], "venue": "Proceedings of the acl 2010 conference short papers, pages 353\u2013358. Association for Computational Linguistics.", "citeRegEx": "Tetreault et al\\.,? 2010", "shortCiteRegEx": "Tetreault et al\\.", "year": 2010}, {"title": "A new dataset and method for automatically grading ESOL texts", "author": ["Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock."], "venue": "ACL, pages 180\u2013189.", "citeRegEx": "Yannakoudakis et al\\.,? 2011", "shortCiteRegEx": "Yannakoudakis et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 23, "context": "The TLE sentences are drawn from the FCE dataset (Yannakoudakis et al., 2011), and authored by English learners from 10 different native language backgrounds.", "startOffset": 49, "endOffset": 77}, {"referenceID": 13, "context": ", 2014), which provides a unified annotation framework across different languages and is geared towards multilingual NLP (McDonald et al., 2013).", "startOffset": 121, "endOffset": 144}, {"referenceID": 5, "context": "While the annotation inventory and guidelines are defined by the English UD formalism, we build on previous work in learner language analysis (D\u0131az-Negrillo et al., 2010; Dickinson and Ragheb, 2013) to formulate an additional set of annotation conventions aiming at a uniform treatment of ungrammatical learner language.", "startOffset": 142, "endOffset": 198}, {"referenceID": 7, "context": "While the annotation inventory and guidelines are defined by the English UD formalism, we build on previous work in learner language analysis (D\u0131az-Negrillo et al., 2010; Dickinson and Ragheb, 2013) to formulate an additional set of annotation conventions aiming at a uniform treatment of ungrammatical learner language.", "startOffset": 142, "endOffset": 198}, {"referenceID": 10, "context": "In the context of NLP, such studies yield data needed for improving tagging and parsing performance on ungrammatical language (Geertzen et al., 2013), as well as utilizing syntactic structure for grammatical error correction (Tetreault et al.", "startOffset": 126, "endOffset": 149}, {"referenceID": 22, "context": ", 2013), as well as utilizing syntactic structure for grammatical error correction (Tetreault et al., 2010; Ng et al., 2014).", "startOffset": 83, "endOffset": 124}, {"referenceID": 15, "context": ", 2013), as well as utilizing syntactic structure for grammatical error correction (Tetreault et al., 2010; Ng et al., 2014).", "startOffset": 83, "endOffset": 124}, {"referenceID": 23, "context": "The sentences were obtained from the FCE corpus (Yannakoudakis et al., 2011), a collection of upper intermediate English learner essays, containing error annotations with 75 error categories (Nicholls, 2003).", "startOffset": 48, "endOffset": 76}, {"referenceID": 16, "context": ", 2011), a collection of upper intermediate English learner essays, containing error annotations with 75 error categories (Nicholls, 2003).", "startOffset": 122, "endOffset": 138}, {"referenceID": 20, "context": "During the training, the annotators attended tutorials on dependency grammars, and learned the UD guidelines4 , the Penn Treebank POS guidelines (Santorini, 1990), the grammatical error annotation scheme of the FCE (Nicholls, 2003), as well as the ESL guidelines presented in section 5.", "startOffset": 145, "endOffset": 162}, {"referenceID": 16, "context": "During the training, the annotators attended tutorials on dependency grammars, and learned the UD guidelines4 , the Penn Treebank POS guidelines (Santorini, 1990), the grammatical error annotation scheme of the FCE (Nicholls, 2003), as well as the ESL guidelines presented in section 5.", "startOffset": 215, "endOffset": 231}, {"referenceID": 21, "context": "The first three exercises consisted of 20 sentences from the UD gold standard for English, the English Web Treebank (EWT) (Silveira et al., 2014).", "startOffset": 122, "endOffset": 145}, {"referenceID": 17, "context": "This strategy continues a line of work in SLA which advocates for centering analysis of learner language around morpho-syntactic surface evidence (Ragheb and Dickinson, 2012; Dickinson and Ragheb, 2013).", "startOffset": 146, "endOffset": 202}, {"referenceID": 7, "context": "This strategy continues a line of work in SLA which advocates for centering analysis of learner language around morpho-syntactic surface evidence (Ragheb and Dickinson, 2012; Dickinson and Ragheb, 2013).", "startOffset": 146, "endOffset": 202}, {"referenceID": 11, "context": "Similarly to our framework, which includes a parallel annotation of corrected sentences, such strategies are often presented in the context of multi-layer annotation schemes that also account for error corrected sentence forms (Hirschmann et al., 2007; D\u0131az-Negrillo et al., 2010; Rosen et al., 2014).", "startOffset": 227, "endOffset": 300}, {"referenceID": 5, "context": "Similarly to our framework, which includes a parallel annotation of corrected sentences, such strategies are often presented in the context of multi-layer annotation schemes that also account for error corrected sentence forms (Hirschmann et al., 2007; D\u0131az-Negrillo et al., 2010; Rosen et al., 2014).", "startOffset": 227, "endOffset": 300}, {"referenceID": 19, "context": "Similarly to our framework, which includes a parallel annotation of corrected sentences, such strategies are often presented in the context of multi-layer annotation schemes that also account for error corrected sentence forms (Hirschmann et al., 2007; D\u0131az-Negrillo et al., 2010; Rosen et al., 2014).", "startOffset": 227, "endOffset": 300}, {"referenceID": 1, "context": "Cohen\u2019s Kappa scores (Cohen, 1960) for POS tags and dependency labels in all evaluation conditions are above 0.", "startOffset": 21, "endOffset": 34}, {"referenceID": 12, "context": "2 of the Turbo tagger and Turbo parser (Martins et al., 2013), state of the art tools for statistical POS tagging and dependency parsing.", "startOffset": 39, "endOffset": 61}, {"referenceID": 10, "context": "This outcome contrasts a study by Geertzen et al. (2013) which reported a larger performance gap of 7.", "startOffset": 34, "endOffset": 57}, {"referenceID": 9, "context": "For example, in (Foster, 2007; Foster et al., 2008) synthetic learner-like data was created by automatic insertion of grammatical errors to well formed English text.", "startOffset": 16, "endOffset": 51}, {"referenceID": 8, "context": "For example, in (Foster, 2007; Foster et al., 2008) synthetic learner-like data was created by automatic insertion of grammatical errors to well formed English text.", "startOffset": 16, "endOffset": 51}, {"referenceID": 0, "context": "In Cahill et al. (2014) a treebank for secondary level native student texts was used to approximate learner text in order to evaluate a parser that utilizes unlabeled learner data.", "startOffset": 3, "endOffset": 24}, {"referenceID": 14, "context": "Syntactic annotations for ESL were previously developed by Nagata et al. (2011), who annotate an English learner corpus with POS tags and shallow syntactic parses.", "startOffset": 59, "endOffset": 80}, {"referenceID": 10, "context": "Dat dataset (Geertzen et al., 2013), annotated with Stanford dependencies (De Marneffe and Man-", "startOffset": 12, "endOffset": 35}], "year": 2016, "abstractText": "We introduce the Treebank of Learner English (TLE), the first publicly available syntactic treebank for English as a Second Language (ESL). The TLE provides manually annotated POS tags and Universal Dependency (UD) trees for 5,124 sentences from the Cambridge First Certificate in English (FCE) corpus. The UD annotations are tied to a pre-existing error annotation of the FCE, whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence. Further on, we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungrammatical English. Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy. We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language1 .", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}