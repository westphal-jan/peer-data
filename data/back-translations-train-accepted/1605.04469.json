{"id": "1605.04469", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2016", "title": "Rationale-Augmented Convolutional Neural Networks for Text Classification", "abstract": "We present a new Convolutional Neural Network (CNN) model for text classification that jointly exploits labels on documents and their component sentences. Specifically, we consider scenarios in which annotators explicitly mark sentences (or snippets) that support their overall document categorization, i.e., they provide rationales. Our model uses such supervision via a hierarchical approach in which each document is represented by a linear combination of the vector representations of its constituent sentences. We propose a sentence-level convolutional model that estimates the probability that a given sentence is a rationale, and we then scale the contribution of each sentence to the aggregate document representation in proportion to these estimates. Experiments on five classification datasets that have document labels and associated rationales demonstrate that our approach consistently outperforms strong baselines. Moreover, our model naturally provides explanations for its predictions.", "histories": [["v1", "Sat, 14 May 2016 21:30:57 GMT  (149kb,D)", "https://arxiv.org/abs/1605.04469v1", null], ["v2", "Sat, 21 May 2016 01:05:59 GMT  (150kb,D)", "http://arxiv.org/abs/1605.04469v2", null], ["v3", "Sat, 24 Sep 2016 16:35:57 GMT  (131kb,D)", "http://arxiv.org/abs/1605.04469v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ye zhang", "iain james marshall", "byron c wallace"], "accepted": true, "id": "1605.04469"}, "pdf": {"name": "1605.04469.pdf", "metadata": {"source": "CRF", "title": "Rationale-Augmented Convolutional Neural Networks for Text Classification", "authors": ["Ye Zhang", "Iain Marshall", "Byron C. Wallace"], "emails": ["yezhang@cs.utexas.edu,", "iain.marshall@kcl.ac.uk", "byron@ccs.neu.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural models that use word embedding have recently achieved impressive results in text classification tasks (Goldberg, 2015), especially Convolutionary Neural Networks (CNNs) that are automatically connected have emerged as a relatively simple but powerful class of text classification models (Kim, 2014), which tend to assume a standard supervised learning environment in which instance names are provided. Here, we consider an alternative scenario in which we assume that a number of rationalities are provided to us (CNN et al., 2007; Zaidan and Eisner, 2008; McDonnell et al., 2016) in addition to instance names, i.e. sentences or excerpts that support the corresponding document categories. Providing such rationalities during manual classification is a natural interaction for annotators and requires little additional effort (McDonnell, 2011; Settles et)."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Neural models for text classification", "text": "Kim (2014) proposed the basic CNN model that we describe below and on which we build in this thesis. Characteristics of this model were examined empirically in (Zhang and Wallace, 2015). We also note that Zhang et al. (2016) expanded this model to accommodate several sets of pre-trained word embeddings together. Roughly simultaneously with Kim, Johnson and Zhang (2014), they proposed a similar CNN architecture, although they switched to a hot vector instead of (pre-rehearsed) word embeddings. Later, they developed a semi-supervised variant of this approach (Johnson and Zhang, 2015). In related recent work on Recurrent Neural Network (RNN) models for text, Tang et al. (2015) suggested using a long-term short-term memory (LSTM) to represent each sentence and then adopt a different RNN variant over it."}, {"heading": "3 Preliminaries: CNNs for text classification", "text": "We first review the simple single-layer CNN for the sentence modeling (2014) proposed by Kim. Faced with a sentence or document containing n words w1, w2,..., wn, we replace each word with its d-dimensional, pre-formed embedding and stack it in rows, creating an instance matrix A, Rn, x. We then apply folding operations on this matrix by using several linear filters that have the same width d but can vary in height. Thus, each filter effectively takes into account different n-gram characteristics, with n corresponding to the filter height. In practice, we introduce multiple redundant characteristics of each height; therefore, each filter height could have hundreds of corresponding, instantiated filters. Applying i parameterized filters to the instance matrix, where n corresponds to the filter height. In practice, we introduce multiple, redundant characteristics of each height, this process is carried out by shifting the respective height of the filter."}, {"heading": "4 Rationale-Augmented CNN for Document Classification", "text": "We now come to the main contribution of this work: a rationally extended CNN for text classification. We first present a simple variant of the above CNN that models the structure of documents (Section 4.1) and then introduce a means of incorporating the rational level into this model (Section 4.2). In Section 4.3 we discuss links to attention mechanisms and describe a baseline with one, inspired by Yang et al. (2016)."}, {"heading": "4.1 Modeling Document Structure", "text": "We would like to develop a model that can use these annotations during the training to improve the classification. To do this, we develop a hierarchical model that estimates the likelihood that individual sentences are rational sentences, and uses these estimates to inform the classification at the document level.3 As a first step, we expand the CNN model to explicitly include the document structure. Specifically, we apply a CNN to each individual sentence of a document to independently obtain sentence vectors. We then add the respective sentence vectors to create a document vector.3 As before, we add a Softmax layer over the document level vector to perform the classification. We perform the regulation by applying suspensions to both the individual sentence vectors and the final document vector.We will call this model Doc-CNN. Doc-CNN forms the basis for our novel approach described below."}, {"heading": "4.2 RA-CNN", "text": "In this section, we introduce the Rational Augmented CNN (RA-CNN). In short, RA-CNN induces a document-level vector representation by taking a weighted sum of its constituent sentence vectors. Each sentence is set to reflect the estimated probability that it is a rational one in supporting the most likely class. We provide a scheme of this model in Figure 2.RA-CNN by enabling both sentence and document level supervision. So there are two steps in the training phase: sentence-level training and document-level training. For the first, we apply a CNN sentence in each sentence to xijsen sentence vectors. We then add a softmax level used by Wsen as input sentence vectors. We adjust this model to maximize the probabilities of the observed rationalities: 3We also experimented with the average of sentence vectors, but we sum up better."}, {"heading": "4.3 Rationales as \u2018Supervised Attention\u2019", "text": "RA-CNN can be considered a supervised variant of a model equipped with an attention mechanism (Bahdanau et al., 2014). From this point of view, it is obvious that instead of directly capitalizing on rationalities, we could try to let the model learn which sentences are important by using only the document labels. Therefore, we construct an additional baseline that does just this, and allow us to assess the effects of learning directly from supervision on a rational level.Following the recent work of Yang et al. (2016), we first establish a hidden representation uijsen for each sentence vector. We then define a context vector at the sentence level that we multiply with each u ij sen to produce a weight ratio. Finally, the document vector is taken as a weighted sum over sentence vectors that reflect its s. We have: uijsen tanh (Wsj + ibs) (4)."}, {"heading": "5 Datasets", "text": "To evaluate our approach, we used a total of five text classification datasets, four of which are biomedical text classification datasets (5.1) and the last one is a collection of film reviews (5.2). These datasets share the characteristic of having recorded justifications associated with each document categorization. We summarize the attributes of all the datasets used in this paper in Table 1."}, {"heading": "5.1 Risk of Bias (RoB) Datasets", "text": "We used a collection of text classification data sets Risk of Bias (RoB), which have been described in detail elsewhere (Marshall et al., 2016). In short, the task involves assessing the reliability of the evidence presented in full-text articles in biomedical journals describing the behavior and outcomes of randomized controlled trials (RCTs), such as assessing whether patients were properly blinded, whether they received active treatment or a comparator (such as a placebo). If such blinding is not performed correctly, it impairs the study by incorporating statistical bias into the efficacy assessment (s) derived from the study. A formal system for conducting bias prejudgments is codified by the Cochrane Risk of Bias Tool (Higgins et al., 2011). This tool defines the study by assessing the risk that each group may have (several);"}, {"heading": "5.2 Movie Review Dataset", "text": "Pang and Lee (2004) developed and published the original version of this dataset, which contained 1000 positive and 1000 negative movie reviews from the Internet Movie Database (IMDB).5 Zaidan et al. (2007) then5http: / / www.imdb.com / extended this dataset by adding justifications that matched the binary classifications for 1800 documents so that the remaining 200 documents were available for testing purposes. Since 200 documents are a modest size of the test samples, we performed a 9-fold cross-validation of the 1800 commented documents (each fold comprises 200 documents).The justifications originally marked in this dataset were subjective snippets; for the purposes of our model, we considered the entire sets containing the highlighted snippets as substantiations."}, {"heading": "6 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Baselines", "text": "We compare several baselines to assess the benefits of directly incorporating monitoring at the rational level into the proposed CNN architecture. We describe these in the following. SVMs. We evaluated a few variants of linear support vector machines (SVMs) based on sparse text representations. We consider variants that exploit uni and bigrams; we refer to them as uni-SVM or bi-SVM. We also implement the model of extended SVM (RA-SVM) developed specifically for these RoB datasets, that of Zaidan et al. (2007), described in Section 2. For RoB datasets, we also compare the recently proposed multi-task model SVM (MT-SVM) developed specifically for these RoB datasets (Marshall et al., 2015; Marshall et al., 2016). This model takes advantage of the intuition that the risks of distortion in domains are codified across the board."}, {"heading": "6.2 Implementation/Hyper-Parameter Details", "text": "Sentence Splitting. To split the documents from all datasets into sentences for consumption by our DocCNN and RA-CNN models, we used the Natural Language Toolkit (NLTK) 7 sentence splitters. SVM-based models. We kept the 50,000 most common features in each dataset. For estimation, we used SGD. We initialized the C hyperparameter using nested development kits. For the RA-SVM, we also used the \u00b5 and Ccontrast parameters, as per Zaidan et al. (2007).CNN-based models. For all models and datasets, we used word embedding on pre-formed vectors that fit via Word2Vec. For the film ratings, these were 300-dimensional and were posted on Google News.8 For the RoB datasets, these were 200-dimensional and on biomedical texts in PubMed / PubMed Central Pysalyal (2013)."}, {"heading": "7 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Quantitative Results", "text": "For all CNN models, we replicated experiments 5 times, with each replication representing a 5-fold or 9-fold CV or RoB or Movie datasets. We report on the mean and observed accuracy ranges of these 5 replications for these models, because the variance inherent in the estimation method is much smaller for these simpler, linear models (especially CNN dropout) and the estimation method makes the fit of the model stochastical (Zhang and Wallace, 2015). We do not report on the ranges of SVM-based models, because the inherent variance in the estimation method is much smaller for these simpler, linear models. The results of the RoB datasets and Movie datasets are shown in Tables 2 and 3, respectively. RA-CNN consistently exceeds all baseline models, across all five datasets. We also note that CNN / Doc-CNN datasets do not necessarily improve the results of the RoB datasets obtained by SVM models."}, {"heading": "7.2 Qualitative Results: Illustrative Rationales", "text": "RA-CNN delivers not only superior classification performance, but also explainable categorisations; the model can provide the highest scores (in max {ppos, pneg}) for a particular target instance, which in turn - by design - are the ones that most influenced the final classification of the document. For example, a positive example justification supporting a correct classification of a study in terms of the risk of bias in terms of assessing the results simply reads doubly blind. An example justification (correctly) classified as present for a study with a high risk of bias now reads retrospectively like the present study, there is a risk that the woman does not properly remember how and what she experienced... If you turn to the film reviews, an example justification from a glowing review of \"Goodfellas\" (correctly classified as positive) reads this cinematic gem of the 1990s to its rightful place among the best."}, {"heading": "8 Conclusions", "text": "We have developed a new model (RA-CNN) for text classification that expands the CNN architecture to directly exploit rationalities when available. We have shown that this model outperforms several strong, relevant baselines across five sets of data, including vanilla and hierarchical CNN variants, as well as a CNN model equipped with an attention mechanism. In addition, RA-CNN automatically provides explanations for classifications made at the time of testing, providing interpretation options. In the future, we plan to explore additional mechanisms for exploiting lower-level surveillance in neural architectures. Furthermore, we believe that an alternative approach could be a hybrid of the ATCNN and RA-CNN models, where an additional loss could occur if the attention mechanism does not match the available direct monitoring of judgments."}, {"heading": "Acknowledgments", "text": "The research reported in this article was supported by the National Library of Medicine (NLM) of the National Institutes of Health (NIH) under premium number R01LM012086. Content is the sole responsibility of the authors and does not necessarily reflect the official opinion of the National Institutes of Health. This work was also made possible by the support of the Texas Advanced Computer Center (TACC) at UT Austin."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Learning from labeled features using generalized expectation criteria", "author": ["Gregory Druck", "Gideon Mann", "Andrew McCallum."], "venue": "Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Druck et al\\.,? 2008", "shortCiteRegEx": "Druck et al\\.", "year": 2008}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg."], "venue": "arXiv preprint arXiv:1510.00726.", "citeRegEx": "Goldberg.,? 2015", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "The cochrane collaborations tool for assessing risk of bias in randomised", "author": ["Julian PT Higgins", "Douglas G Altman", "Peter C G\u00f8tzsche", "Peter J\u00fcni", "David Moher", "Andrew D Oxman", "Jelena Savovi\u0107", "Kenneth F Schulz", "Laura Weeks", "Jonathan AC Sterne"], "venue": null, "citeRegEx": "Higgins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Higgins et al\\.", "year": 2011}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["Thorsten Joachims."], "venue": "Springer.", "citeRegEx": "Joachims.,? 1998", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["Rie Johnson", "Tong Zhang."], "venue": "arXiv preprint arXiv:1412.1058.", "citeRegEx": "Johnson and Zhang.,? 2014", "shortCiteRegEx": "Johnson and Zhang.", "year": 2014}, {"title": "Semi-supervised convolutional neural networks for text categorization via region embedding", "author": ["Rie Johnson", "Tong Zhang."], "venue": "Advances in Neural Information Processing Systems (NIPs), pages 919\u2013927.", "citeRegEx": "Johnson and Zhang.,? 2015", "shortCiteRegEx": "Johnson and Zhang.", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2342\u20132350.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Automatic text categorization using the importance of sentences", "author": ["Youngjoong Ko", "Jinwoo Park", "Jungyun Seo."], "venue": "Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1\u20137. Association for Computational Linguistics.", "citeRegEx": "Ko et al\\.,? 2002", "shortCiteRegEx": "Ko et al\\.", "year": 2002}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1405.4053.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Generalized expectation criteria for semi-supervised learning with weakly labeled data", "author": ["Gideon S Mann", "Andrew McCallum."], "venue": "The Journal of Machine Learning Research, 11:955\u2013984.", "citeRegEx": "Mann and McCallum.,? 2010", "shortCiteRegEx": "Mann and McCallum.", "year": 2010}, {"title": "Automating risk of bias assessment for clinical trials", "author": ["Iain J Marshall", "Jo\u00ebl Kuiper", "Byron C Wallace."], "venue": "Biomedical and Health Informatics, IEEE Journal of, 19(4):1406\u20131412.", "citeRegEx": "Marshall et al\\.,? 2015", "shortCiteRegEx": "Marshall et al\\.", "year": 2015}, {"title": "Robotreviewer: evaluation of a system for automatically assessing bias in clinical trials", "author": ["Iain J Marshall", "Jo\u00ebl Kuiper", "Byron C Wallace."], "venue": "Journal of the American Medical Informatics Association, 23(1):193\u2013201.", "citeRegEx": "Marshall et al\\.,? 2016", "shortCiteRegEx": "Marshall et al\\.", "year": 2016}, {"title": "Why Is That Relevant? Collecting Annotator Rationales for Relevance Judgments", "author": ["Tyler McDonnell", "Matthew Lease", "Tamer Elsayad", "Mucahid Kutlu."], "venue": "Proceedings of the 4th AAAI Conference on Human Computation and Crowdsourcing (HCOMP).", "citeRegEx": "McDonnell et al\\.,? 2016", "shortCiteRegEx": "McDonnell et al\\.", "year": 2016}, {"title": "Japanese probabilistic information retrieval using location and category information", "author": ["Masaki Murata", "Qing Ma", "Kiyotaka Uchimoto", "Hiromi Ozaku", "Masao Utiyama", "Hitoshi Isahara."], "venue": "Proceedings of the fifth international workshop on on Information re-", "citeRegEx": "Murata et al\\.,? 2000", "shortCiteRegEx": "Murata et al\\.", "year": 2000}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Bo Pang", "Lillian Lee."], "venue": "Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Lin-", "citeRegEx": "Pang and Lee.,? 2004", "shortCiteRegEx": "Pang and Lee.", "year": 2004}, {"title": "Distributional semantics resources for biomedical text processing", "author": ["Sampo Pyysalo", "Filip Ginter", "Hans Moen", "Tapio Salakoski", "Sophia Ananiadou."], "venue": "Proceedings of Languages in Biology and Medicine.", "citeRegEx": "Pyysalo et al\\.,? 2013", "shortCiteRegEx": "Pyysalo et al\\.", "year": 2013}, {"title": "Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances", "author": ["Burr Settles."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1467\u20131478. Association for Computational Lin-", "citeRegEx": "Settles.,? 2011", "shortCiteRegEx": "Settles.", "year": 2011}, {"title": "The constrained weight space svm: learning with ranked features", "author": ["Kevin Small", "Byron Wallace", "Thomas Trikalinos", "Carla E Brodley."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 865\u2013872.", "citeRegEx": "Small et al\\.,? 2011", "shortCiteRegEx": "Small et al\\.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1422\u20131432.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Automatically generating annotator rationales to improve sentiment classification", "author": ["Ainur Yessenalina", "Yejin Choi", "Claire Cardie."], "venue": "Proceedings of the ACL 2010 Conference Short Papers, pages 336\u2013341. Association for Computational Linguistics.", "citeRegEx": "Yessenalina et al\\.,? 2010", "shortCiteRegEx": "Yessenalina et al\\.", "year": 2010}, {"title": "Modeling annotators: A generative approach to learning from annotator rationales", "author": ["Omar F Zaidan", "Jason Eisner."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 31\u201340. Association for Computational Linguis-", "citeRegEx": "Zaidan and Eisner.,? 2008", "shortCiteRegEx": "Zaidan and Eisner.", "year": 2008}, {"title": "Using\u201d annotator rationales\u201d to improve machine learning for text categorization", "author": ["Omar Zaidan", "Jason Eisner", "Christine D Piatko."], "venue": "HLT-NAACL, pages 260\u2013267. Citeseer.", "citeRegEx": "Zaidan et al\\.,? 2007", "shortCiteRegEx": "Zaidan et al\\.", "year": 2007}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Ye Zhang", "Byron C. Wallace."], "venue": "arXiv preprint arXiv:1510.03820.", "citeRegEx": "Zhang and Wallace.,? 2015", "shortCiteRegEx": "Zhang and Wallace.", "year": 2015}, {"title": "Mgnc-cnn: A simple approach to exploiting multiple word embeddings for sentence classification", "author": ["Ye Zhang", "Stephen Roller", "Byron C. Wallace."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Neural models that exploit word embeddings have recently achieved impressive results on text classification tasks (Goldberg, 2015).", "startOffset": 114, "endOffset": 130}, {"referenceID": 9, "context": "Feed-forward Convolutional Neural Networks (CNNs), in particular, have emerged as a relatively simple yet powerful class of models for text classification (Kim, 2014).", "startOffset": 155, "endOffset": 166}, {"referenceID": 27, "context": "Here we consider an alternative scenario in which we assume that we are provided a set of rationales (Zaidan et al., 2007; Zaidan and Eisner, 2008; McDonnell et al., 2016) in addition to instance labels, i.", "startOffset": 101, "endOffset": 171}, {"referenceID": 26, "context": "Here we consider an alternative scenario in which we assume that we are provided a set of rationales (Zaidan et al., 2007; Zaidan and Eisner, 2008; McDonnell et al., 2016) in addition to instance labels, i.", "startOffset": 101, "endOffset": 171}, {"referenceID": 16, "context": "Here we consider an alternative scenario in which we assume that we are provided a set of rationales (Zaidan et al., 2007; Zaidan and Eisner, 2008; McDonnell et al., 2016) in addition to instance labels, i.", "startOffset": 101, "endOffset": 171}, {"referenceID": 20, "context": "Providing such rationales during manual classification is a natural interaction for annotators, and requires little additional effort (Settles, 2011; McDonnell et al., 2016).", "startOffset": 134, "endOffset": 173}, {"referenceID": 16, "context": "Providing such rationales during manual classification is a natural interaction for annotators, and requires little additional effort (Settles, 2011; McDonnell et al., 2016).", "startOffset": 134, "endOffset": 173}, {"referenceID": 5, "context": "Past work (Section 2) has introduced such methods, but these have relied on linear models such as Support Vector Machines (SVMs) (Joachims, 1998), operating over sparse representations of text.", "startOffset": 129, "endOffset": 145}, {"referenceID": 27, "context": "(2) Empirically, we show that the proposed model uniformly outperforms relevant baseline approaches across five datasets, including previously proposed models that capitalize on rationales (Zaidan et al., 2007; Marshall et al., 2016) and multiple baseline CNN variants, including a CNN equipped with an attention mechanism.", "startOffset": 189, "endOffset": 233}, {"referenceID": 15, "context": "(2) Empirically, we show that the proposed model uniformly outperforms relevant baseline approaches across five datasets, including previously proposed models that capitalize on rationales (Zaidan et al., 2007; Marshall et al., 2016) and multiple baseline CNN variants, including a CNN equipped with an attention mechanism.", "startOffset": 189, "endOffset": 233}, {"referenceID": 15, "context": "We also report state-of-the-art results on the important task of automatically assessing the risks of bias in the studies described in full-text biomedical articles (Marshall et al., 2016).", "startOffset": 165, "endOffset": 188}, {"referenceID": 29, "context": "Properties of this model were explored empirically in (Zhang and Wallace, 2015).", "startOffset": 54, "endOffset": 79}, {"referenceID": 7, "context": "They later developed a semi-supervised variant of this approach (Johnson and Zhang, 2015).", "startOffset": 64, "endOffset": 89}, {"referenceID": 6, "context": "Roughly concurrently to Kim, Johnson and Zhang (2014) proposed a similar CNN architecture, although they swapped in one-hot vectors in place of (pre-trained) word embeddings.", "startOffset": 29, "endOffset": 54}, {"referenceID": 23, "context": "In related recent work on Recurrent Neural Network (RNN) models for text, Tang et al. (2015) proposed using a Long Short Term Memory (LSTM) layer to represent each sentence and then passing another RNN variant over these.", "startOffset": 74, "endOffset": 93}, {"referenceID": 23, "context": "In related recent work on Recurrent Neural Network (RNN) models for text, Tang et al. (2015) proposed using a Long Short Term Memory (LSTM) layer to represent each sentence and then passing another RNN variant over these. And Yang et al. (2016) proposed a hierarchical network with two levels of attention mechanisms for document classification.", "startOffset": 74, "endOffset": 245}, {"referenceID": 10, "context": "Prior work has investigated methods to measure the relative importance sentences (Ko et al., 2002; Murata et al., 2000).", "startOffset": 81, "endOffset": 119}, {"referenceID": 17, "context": "Prior work has investigated methods to measure the relative importance sentences (Ko et al., 2002; Murata et al., 2000).", "startOffset": 81, "endOffset": 119}, {"referenceID": 27, "context": "The notion of rationales was first introduced by Zaidan et al. (2007). To harness these for classification, they proposed modifying the Support Vector Machine (SVM) objective function to encode", "startOffset": 49, "endOffset": 70}, {"referenceID": 25, "context": "Yessenalina et al. (2010) later developed an approach to generate rationales.", "startOffset": 0, "endOffset": 26}, {"referenceID": 2, "context": "This work has largely involved inserting constraints into the learning process that favor parameter values that align with a priori featurelabel affinities or rankings (Druck et al., 2008; Mann and McCallum, 2010; Small et al., 2011; Settles, 2011).", "startOffset": 168, "endOffset": 248}, {"referenceID": 13, "context": "This work has largely involved inserting constraints into the learning process that favor parameter values that align with a priori featurelabel affinities or rankings (Druck et al., 2008; Mann and McCallum, 2010; Small et al., 2011; Settles, 2011).", "startOffset": 168, "endOffset": 248}, {"referenceID": 21, "context": "This work has largely involved inserting constraints into the learning process that favor parameter values that align with a priori featurelabel affinities or rankings (Druck et al., 2008; Mann and McCallum, 2010; Small et al., 2011; Settles, 2011).", "startOffset": 168, "endOffset": 248}, {"referenceID": 20, "context": "This work has largely involved inserting constraints into the learning process that favor parameter values that align with a priori featurelabel affinities or rankings (Druck et al., 2008; Mann and McCallum, 2010; Small et al., 2011; Settles, 2011).", "startOffset": 168, "endOffset": 248}, {"referenceID": 9, "context": "We first review the simple one-layer CNN for sentence modeling proposed by Kim (2014). Given a sentence or document comprising n words w1, w2,.", "startOffset": 75, "endOffset": 86}, {"referenceID": 11, "context": "Specifically, we use the Rectified Linear Unit, or ReLU (Krizhevsky et al., 2012).", "startOffset": 56, "endOffset": 81}, {"referenceID": 22, "context": "Dropout (Srivastava et al., 2014) is often applied at this layer as a means of regularization.", "startOffset": 8, "endOffset": 33}, {"referenceID": 29, "context": "For more details, see (Zhang and Wallace, 2015).", "startOffset": 22, "endOffset": 47}, {"referenceID": 9, "context": "This model was originally proposed for sentence classification (Kim, 2014), but we can adapt it for document classification by simply treating the document as one long sentence.", "startOffset": 63, "endOffset": 74}, {"referenceID": 24, "context": "3 we discuss connections to attention mechanisms and describe a baseline equipped with one, inspired by Yang et al. (2016).", "startOffset": 104, "endOffset": 123}, {"referenceID": 1, "context": "Note that this sequential training strategy differs from the alternating training approach commonly used in multi-task learning (Collobert and Weston, 2008).", "startOffset": 128, "endOffset": 156}, {"referenceID": 1, "context": "We found that the latter approach does not work well here, leading us to instead adopt the cascade-like feature learning approach (Collobert and Weston, 2008) just described.", "startOffset": 130, "endOffset": 158}, {"referenceID": 0, "context": "One may view RA-CNN as a supervised variant of a model equipped with an attention mechanism (Bahdanau et al., 2014).", "startOffset": 92, "endOffset": 115}, {"referenceID": 24, "context": "Following the recent work of Yang et al. (2016), we first posit for each sentence vector a hidden representation u sen.", "startOffset": 29, "endOffset": 48}, {"referenceID": 15, "context": "We used a collection Risk of Bias (RoB) text classification datasets, described at length elsewhere (Marshall et al., 2016).", "startOffset": 100, "endOffset": 123}, {"referenceID": 4, "context": "A formal system for making bias assessments is codified by the Cochrane Risk of Bias Tool (Higgins et al., 2011).", "startOffset": 90, "endOffset": 112}, {"referenceID": 18, "context": "Pang and Lee (2004) developed and published the original version of this dataset, which comprises 1000 positive and 1000 negative movie reviews from the Internet Movie Database (IMDB).", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "Pang and Lee (2004) developed and published the original version of this dataset, which comprises 1000 positive and 1000 negative movie reviews from the Internet Movie Database (IMDB).5 Zaidan et al. (2007) then", "startOffset": 0, "endOffset": 207}, {"referenceID": 14, "context": "For the RoB dataset, we also compare to a recently proposed multi-task SVM (MT-SVM) model developed specifically for these RoB datasets (Marshall et al., 2015; Marshall et al., 2016).", "startOffset": 136, "endOffset": 182}, {"referenceID": 15, "context": "For the RoB dataset, we also compare to a recently proposed multi-task SVM (MT-SVM) model developed specifically for these RoB datasets (Marshall et al., 2015; Marshall et al., 2016).", "startOffset": 136, "endOffset": 182}, {"referenceID": 25, "context": "We also re-implemented the rationale augmented SVM (RA-SVM) proposed by Zaidan et al. (2007), described in Section 2.", "startOffset": 72, "endOffset": 93}, {"referenceID": 14, "context": "For the RoB dataset, we also compare to a recently proposed multi-task SVM (MT-SVM) model developed specifically for these RoB datasets (Marshall et al., 2015; Marshall et al., 2016). This model exploits the intuition that the risks of bias across the domains codified in the aforementioned Cochrane RoB tool will likely be correlated. That is, if we know that a study exhibits a high risk of bias for one domain, then it seems reasonable to assume it is at an elevated risk for the remaining domains. Furthermore, Marshall et al. (2016) include rationale-level supervision by first training a (multi-task) sentencelevel model to identify sentences likely to support", "startOffset": 137, "endOffset": 538}, {"referenceID": 12, "context": ", (Le and Mikolov, 2014; Jozefowicz et al., 2015; Tang et al., 2015; Yang et al., 2016).", "startOffset": 2, "endOffset": 87}, {"referenceID": 8, "context": ", (Le and Mikolov, 2014; Jozefowicz et al., 2015; Tang et al., 2015; Yang et al., 2016).", "startOffset": 2, "endOffset": 87}, {"referenceID": 23, "context": ", (Le and Mikolov, 2014; Jozefowicz et al., 2015; Tang et al., 2015; Yang et al., 2016).", "startOffset": 2, "endOffset": 87}, {"referenceID": 24, "context": ", (Le and Mikolov, 2014; Jozefowicz et al., 2015; Tang et al., 2015; Yang et al., 2016).", "startOffset": 2, "endOffset": 87}, {"referenceID": 9, "context": "This informed our choice of baseline CNN variants: standard CNN (Kim, 2014), Doc-CNN (described above) and ATCNN (also described above) that capitalizes on an (unsupervised) attention mechanism at the sentence level, described in Section 4.", "startOffset": 64, "endOffset": 75}, {"referenceID": 27, "context": "For the RA-SVM, we additionally tuned the \u03bc and Ccontrast parameters, as per Zaidan et al. (2007).", "startOffset": 77, "endOffset": 98}, {"referenceID": 19, "context": "8 For the RoB datasets, these were 200-dimensional and trained on biomedical texts in PubMed/PubMed Central (Pyysalo et al., 2013).", "startOffset": 108, "endOffset": 130}, {"referenceID": 29, "context": "5, and we used 3 different filter heights: 3, 4 and 5, following (Zhang and Wallace, 2015).", "startOffset": 65, "endOffset": 90}, {"referenceID": 28, "context": "For parameter estimation we used ADADELTA (Zeiler, 2012), mini-batches of size 50, and an early stopping strategy (using a validation set).", "startOffset": 42, "endOffset": 56}, {"referenceID": 29, "context": "We report the mean and observed ranges in accuracy across these 5 replications for these models, because attributes of the model (notably, dropout) and the estimation procedure render model fitting stochastic (Zhang and Wallace, 2015).", "startOffset": 209, "endOffset": 234}, {"referenceID": 27, "context": "Uni-SVM: unigram SVM, Bi-SVM: Bigram SVM, RA-SVM: Rationale-augmented SVM (Zaidan et al., 2007), MT-SVM: a multi-task SVM model specifically designed for the RoB task, which also exploits the available sentence supervision (Marshall et al.", "startOffset": 74, "endOffset": 95}, {"referenceID": 15, "context": ", 2007), MT-SVM: a multi-task SVM model specifically designed for the RoB task, which also exploits the available sentence supervision (Marshall et al., 2016).", "startOffset": 135, "endOffset": 158}, {"referenceID": 29, "context": "In particular, in previous work (Zhang and Wallace, 2015) we observed that CNN outperforms SVM uniformly on sentence classification tasks (the average sentence-length in these datasets was about 10).", "startOffset": 32, "endOffset": 57}], "year": 2016, "abstractText": "We present a new Convolutional Neural Network (CNN) model for text classification that jointly exploits labels on documents and their constituent sentences. Specifically, we consider scenarios in which annotators explicitly mark sentences (or snippets) that support their overall document categorization, i.e., they provide rationales. Our model exploits such supervision via a hierarchical approach in which each document is represented by a linear combination of the vector representations of its component sentences. We propose a sentence-level convolutional model that estimates the probability that a given sentence is a rationale, and we then scale the contribution of each sentence to the aggregate document representation in proportion to these estimates. Experiments on five classification datasets that have document labels and associated rationales demonstrate that our approach consistently outperforms strong baselines. Moreover, our model naturally provides explanations for its predictions.", "creator": "TeX"}}}