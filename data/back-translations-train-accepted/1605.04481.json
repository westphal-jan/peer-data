{"id": "1605.04481", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2016", "title": "Anchoring and Agreement in Syntactic Annotations", "abstract": "We present a study on two key characteristics of human syntactic annotations: anchoring and agreement. Anchoring is a well known cognitive bias in human decision making, where judgments are drawn towards pre-existing values. We study the influence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output. Our experiments demonstrate a clear anchoring effect and reveal unwanted consequences, including overestimation of parsing performance and lower quality of annotations in comparison with human-based annotations. Using sentences from the Penn Treebank WSJ, we also report the first systematically obtained inter-annotator agreement estimates for English syntactic parsing. Our agreement results control for anchoring bias, and are consequential in that they are \\emph{on par} with state of the art parsing performance for English. We discuss the impact of our findings on strategies for future annotation efforts and parser evaluations.", "histories": [["v1", "Sun, 15 May 2016 00:26:26 GMT  (3453kb)", "https://arxiv.org/abs/1605.04481v1", null], ["v2", "Fri, 3 Jun 2016 16:49:41 GMT  (3763kb)", "http://arxiv.org/abs/1605.04481v2", "Added references, minor revisions"], ["v3", "Wed, 21 Sep 2016 22:34:47 GMT  (3454kb)", "http://arxiv.org/abs/1605.04481v3", "EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yevgeni berzak", "yan huang 0014", "andrei barbu", "anna korhonen", "boris katz"], "accepted": true, "id": "1605.04481"}, "pdf": {"name": "1605.04481.pdf", "metadata": {"source": "CRF", "title": "Anchoring and Agreement in Syntactic Annotations", "authors": ["Yevgeni Berzak"], "emails": ["berzak@mit.edu", "yh358@cam.ac.uk", "andrei@0xab.com", "alk23@cam.ac.uk", "boris@mit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.04 481v 3 [cs.C L] 2"}, {"heading": "1 Introduction", "text": "In fact, most people who fight for the rights of women and men are fighting for equality between men and women."}, {"heading": "2 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Annotation Tasks", "text": "In the POS marking task, each word must be categorized in a sentence with a Penn Treebank POS tag (Santorini, 1990) (hereinafter POS).The task to analyze the dependency is to provide a sentence with a marked dependency tree using Universal Dependencies (UD) formalism (De Marneffe et al., 2014) in accordance with version 1 of the UD Guidelines. We distinguish between three variants of these tasks: annotation, review and ranking. In the annotation variant, participants are asked to do the annotation from scratch. In the review variant, they are asked to provide alternative annotation for all tasks, annotation, review and ranking. In the annotation variant, participants are asked to do the annotation."}, {"heading": "2.2 Annotation Format", "text": "The first two columns of each line contain the word index and the word re-2http: / / universaldependencies.org / # enspectiv. The next three columns are for the annotations of POS, HIND and REL.In the annotation task, these values must be set from scratch by the annotator. In the review task, participants must edit the pre-notated values for a given sentence. The sixth column in the rating template contains an additional # character, the aim of which is to prevent reviewers from overlooking and passively approving existing annotations. Corrections are specified according to this character in a spatially separated format, in which each of the existing three Nexken annotation marks is provided with an alternative annotation value or an * mark."}, {"heading": "2.3 Evaluation Metrics", "text": "We measure both parsing performance and interannotator match using tagging and parsing evaluation metrics. This selection enables direct comparison between parsing and agreement results. In this context, POS refers to tagging accuracy. We use the standard Unlabeled Attachment Score (UAS) and Label Accuracy (LA) metrics to measure the accuracy of head attachments and dependency labels. We also use the standard parsing metric Labeled Attachment Score (LAS), which takes into account both dependency arcs and dependency labels."}, {"heading": "2.4 Corpora", "text": "We use sets of two publicly available data sets covering two different genres: the first corpus used in the experiments in Sections 3 and 4 is the First Certificate in English (FCE) Cambridge Learner Corpus (Yannakoudakis et al., 2011), which contains essays written by upper intermediate English learners. The second corpus is the WSJ part of Penn Treebank (WSJ PTB) (Marcus et al., 1993), which has been the most widely used resource for educating and evaluating English parsers since its publication. Our Interannotator Agreement experiment in Section 5 uses a random subset of sentences in Section 23 of the WSJ PTB, traditionally reserved for tagging and evaluating parsers."}, {"heading": "2.5 Annotators", "text": "We recruited five students at MIT as commentators. Three of the students are major linguistics students and two are major engineering students with minor linguists. Prior to participating in this study, the commentators completed a two-month course. During the course, students completed tutorials and learned the annotation guidelines for PTB POS tags, UD guidelines, and guidelines for annotating sophisticated syntactic structures resulting from grammatical errors. Students also commented individually on sixty3 The annotation distortions and quality results reported in Sections 3 and 4 use the original learning sentences that contain grammatical errors, which were replicated on the corrected versions of the sentences. Practice batches of 20-30 sentences from the English Web Treebank (EWT) (Silveira et al., 2014) and FCE companies resolved the annotation inconsistencies during group sessions."}, {"heading": "3 Parser Bias", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "4 Annotation Quality", "text": "In this section, we expand our investigation to examine the impact of parser bias on the quality of parser-based gold standards. To this end, we perform a manual comparison between the human and parser-based gold standards. Our quality experiment, shown in Figure 1 (b), is a ranking of grades for the human gold standard that is not commented on or reviewed. In all three grading categories, judges tend to favor the parser-based gold standard."}, {"heading": "5 Inter-annotator Agreement", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "6 Related Work", "text": "The term \"anchoring\" was coined in a groundbreaking paper by Tversky and Kahneman (1974), which showed that numerical estimates can be distorted by uninformative pre-information. Subsequent work in various areas of decision-making confirmed the robustness of anchoring using both informative and informative anchors (Furnham and Boo, 2011). In relation to our study, anchoring biases were also shown when participants were domain experts, although they had a lower degree of anchoring than in the early anchoring experiments (Wilson et al., 1996; Mussweiler and Strack, 2000). Prior's work in NLP examined the influence of pre-tagging (Fort and Sagot, 2010) and pre-parsing (Skj\u00e6rholt, 2013) on human annotations. Our work provides a systematic study of this subject using a new experimental framework as well as substantially more sets and attributes."}, {"heading": "7 Discussion", "text": "We present a systematic study of the impact of anchoring on POS and dependency annotations used in NLP, and show that annotations have an anchoring bias effect on the output of automatic annotation tools. This bias leads to an artificial increase in performance for the parsers in question and leads to a lower quality of annotations compared to human annotations. Our analysis shows that despite the negative effects of the annotation principle, predictions shared across different parsers do not significantly reduce the quality of annotations, leading to the following hybrid annotation strategy as a potential future alternative to human-based annotations and parser-based annotations pipelines."}, {"heading": "Acknowledgments", "text": "We thank our great commentators Sebastian Garza, Jessica Kenney, Lucia Lam, Keiko Sophie Mori and Jing Xian Wang. We also thank Karthik Narasimhan and the anonymous critics for valuable feedback on this work. This material is based on work supported by the Center for Brains, Minds, and Machines (CBMM) and awarded the CCF-1231216 by the NSF STC. This work was also supported by AFRL Contract No FA8750-15-C-0010 and the ERC Consolidator Grant LEXICAL (648909)."}], "references": [{"title": "Building a treebank for french", "author": ["Anne Abeill\u00e9", "Lionel Cl\u00e9ment", "Fran\u00e7ois Toussenel."], "venue": "Treebanks, pages 165\u2013187. Springer.", "citeRegEx": "Abeill\u00e9 et al\\.,? 2003", "shortCiteRegEx": "Abeill\u00e9 et al\\.", "year": 2003}, {"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proceedings of ACL, pages 2442\u20132452.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Universal dependencies for learner english", "author": ["Yevgeni Berzak", "Jessica Kenney", "Carolyn Spadine", "Jing Xian Wang", "Lucia Lam", "Keiko Sophie Mori", "Sebastian Garza", "Boris Katz."], "venue": "Proceedings of ACL, pages 737\u2013746.", "citeRegEx": "Berzak et al\\.,? 2016", "shortCiteRegEx": "Berzak et al\\.", "year": 2016}, {"title": "The tiger treebank", "author": ["Sabine Brants", "Stefanie Dipper", "Silvia Hansen", "Wolfgang Lezius", "George Smith."], "venue": "Proceedings of the workshop on treebanks and linguistic theories, volume 168.", "citeRegEx": "Brants et al\\.,? 2002", "shortCiteRegEx": "Brants et al\\.", "year": 2002}, {"title": "Corpus annotation for parser evaluation", "author": ["John Carroll", "Guido Minnen", "Ted Briscoe."], "venue": "arXiv preprint cs/9907013.", "citeRegEx": "Carroll et al\\.,? 1999", "shortCiteRegEx": "Carroll et al\\.", "year": 1999}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine De Marneffe", "Bill MacCartney", "Christopher D Manning"], "venue": "In Proceedings of LREC,", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Universal stanford dependencies: A cross-linguistic typology", "author": ["Marie-Catherine De Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D Manning."], "venue": "Proceedings of LREC, pages 4585\u20134592.", "citeRegEx": "Marneffe et al\\.,? 2014", "shortCiteRegEx": "Marneffe et al\\.", "year": 2014}, {"title": "Influence of preannotation on pos-tagged corpus development", "author": ["Kar\u00ebn Fort", "Beno\u0131\u0302t Sagot"], "venue": "In Proceedings of the fourth linguistic annotation workshop,", "citeRegEx": "Fort and Sagot.,? \\Q2010\\E", "shortCiteRegEx": "Fort and Sagot.", "year": 2010}, {"title": "A literature review of the anchoring effect", "author": ["Adrian Furnham", "Hua Chu Boo."], "venue": "The Journal of SocioEconomics, 40(1):35\u201342.", "citeRegEx": "Furnham and Boo.,? 2011", "shortCiteRegEx": "Furnham and Boo.", "year": 2011}, {"title": "Collaborative dependency annotation", "author": ["Kim Gerdes."], "venue": "DepLing 2013, 88.", "citeRegEx": "Gerdes.,? 2013", "shortCiteRegEx": "Gerdes.", "year": 2013}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of ACL, volume 1, pages 1381\u20131391.", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "The penn arabic treebank: Building a large-scale annotated arabic corpus", "author": ["Mohamed Maamouri", "Ann Bies", "Tim Buckwalter", "Wigdan Mekki."], "venue": "NEMLAR conference on Arabic language resources and tools, volume 27, pages 466\u2013467.", "citeRegEx": "Maamouri et al\\.,? 2004", "shortCiteRegEx": "Maamouri et al\\.", "year": 2004}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Turning on the turbo: Fast third-order nonprojective turbo parsers", "author": ["Andr\u00e9 FT Martins", "Miguel Almeida", "Noah A Smith."], "venue": "Proceedings of ACL, pages 617\u2013622.", "citeRegEx": "Martins et al\\.,? 2013", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Numeric judgments under uncertainty: The role of knowledge in anchoring", "author": ["Thomas Mussweiler", "Fritz Strack."], "venue": "Journal of Experimental Social Psychology, 36(5):495\u2013518.", "citeRegEx": "Mussweiler and Strack.,? 2000", "shortCiteRegEx": "Mussweiler and Strack.", "year": 2000}, {"title": "Linguistically debatable or just plain wrong", "author": ["Barbara Plank", "Dirk Hovy", "Anders S\u00f8gaard"], "venue": "In Proceedings of ACL: Short Papers,", "citeRegEx": "Plank et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2014}, {"title": "Do dependency parsing metrics correlate with human judgments", "author": ["Barbara Plank", "H\u00e9ctor Mart\u0131\u0301nez Alonso", "\u017deljko Agi\u0107", "Danijela Merkler", "Anders S\u00f8gaard"], "venue": "In Proceedings of CoNLL", "citeRegEx": "Plank et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2015}, {"title": "A dependency treebank for english", "author": ["Owen Rambow", "Cassandre Creswell", "Rachel Szekely", "Harriet Taber", "Marilyn A Walker."], "venue": "Proceedings of LREC.", "citeRegEx": "Rambow et al\\.,? 2002", "shortCiteRegEx": "Rambow et al\\.", "year": 2002}, {"title": "Definitional and human constraints on structural annotation of english", "author": ["Geoffrey Sampson", "Anna Babarczy."], "venue": "Natural Language Engineering, 14(04):471\u2013494.", "citeRegEx": "Sampson and Babarczy.,? 2008", "shortCiteRegEx": "Sampson and Babarczy.", "year": 2008}, {"title": "English for the computer: Susanne corpus and analytic scheme", "author": ["Geoffrey Sampson"], "venue": null, "citeRegEx": "Sampson.,? \\Q1995\\E", "shortCiteRegEx": "Sampson.", "year": 1995}, {"title": "Part-of-speech tagging guidelines for the penn treebank project (3rd revision)", "author": ["Beatrice Santorini."], "venue": "Technical Reports (CIS).", "citeRegEx": "Santorini.,? 1990", "shortCiteRegEx": "Santorini.", "year": 1990}, {"title": "Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation", "author": ["Roy Schwartz", "Omri Abend", "Roi Reichart", "Ari Rappoport."], "venue": "Proceedings of ACL, pages 663\u2013672.", "citeRegEx": "Schwartz et al\\.,? 2011", "shortCiteRegEx": "Schwartz et al\\.", "year": 2011}, {"title": "A gold standard dependency corpus for english", "author": ["Natalia Silveira", "Timothy Dozat", "Marie-Catherine De Marneffe", "Samuel R Bowman", "Miriam Connor", "John Bauer", "Christopher D Manning."], "venue": "Proceedings of LREC, pages 2897\u20132904.", "citeRegEx": "Silveira et al\\.,? 2014", "shortCiteRegEx": "Silveira et al\\.", "year": 2014}, {"title": "Influence of preprocessing on dependency syntax annotation: speed and agreement", "author": ["Arne Skj\u00e6rholt."], "venue": "LAW VII & ID.", "citeRegEx": "Skj\u00e6rholt.,? 2013", "shortCiteRegEx": "Skj\u00e6rholt.", "year": 2013}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer."], "venue": "Proceedings of NAACL, pages 173\u2013180.", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Judgment under uncertainty: Heuristics and biases", "author": ["Amos Tversky", "Daniel Kahneman."], "venue": "Science, 185(4157):1124\u20131131.", "citeRegEx": "Tversky and Kahneman.,? 1974", "shortCiteRegEx": "Tversky and Kahneman.", "year": 1974}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of ACL, pages 323\u2013333.", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "A new look at anchoring effects: basic anchoring and its antecedents", "author": ["Timothy D Wilson", "Christopher E Houston", "Kathryn M Etling", "Nancy Brekke."], "venue": "Journal of Experimental Psychology: General, 125(4):387.", "citeRegEx": "Wilson et al\\.,? 1996", "shortCiteRegEx": "Wilson et al\\.", "year": 1996}, {"title": "A new dataset and method for automatically grading ESOL texts", "author": ["Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock."], "venue": "Proceedings of ACL, pages 180\u2013189.", "citeRegEx": "Yannakoudakis et al\\.,? 2011", "shortCiteRegEx": "Yannakoudakis et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 12, "context": "The justification for this annotation methodology was first introduced in a set of experiments on POS tag annotation conducted as part of the Penn Treebank project (Marcus et al., 1993).", "startOffset": 164, "endOffset": 185}, {"referenceID": 3, "context": "Following the Penn Treebank, syntactic annotation projects for various languages, including German (Brants et al., 2002), French (Abeill\u00e9 et al.", "startOffset": 99, "endOffset": 120}, {"referenceID": 0, "context": ", 2002), French (Abeill\u00e9 et al., 2003), Arabic (Maamouri et al.", "startOffset": 16, "endOffset": 38}, {"referenceID": 11, "context": ", 2003), Arabic (Maamouri et al., 2004) and many others, were annotated using automatic tools as a starting point.", "startOffset": 16, "endOffset": 39}, {"referenceID": 25, "context": "subject to the problem of anchoring, a well established and robust cognitive bias in which human decisions are affected by pre-existing values (Tversky and Kahneman, 1974).", "startOffset": 143, "endOffset": 171}, {"referenceID": 12, "context": "Extending the experimental setup of Marcus et al. (1993), we demonstrate that parser bias may lead to lower annotation quality for parser-based annotations compared to humanbased annotations.", "startOffset": 36, "endOffset": 57}, {"referenceID": 20, "context": "In the POS tagging task, each word in a sentence has to be categorized with a Penn Treebank POS tag (Santorini, 1990) (henceforth POS).", "startOffset": 100, "endOffset": 117}, {"referenceID": 28, "context": "The first corpus, used in the experiments in sections 3 and 4, is the First Certificate in English (FCE) Cambridge Learner Corpus (Yannakoudakis et al., 2011).", "startOffset": 130, "endOffset": 158}, {"referenceID": 12, "context": "The second corpus is the WSJ part of the Penn Treebank (WSJ PTB) (Marcus et al., 1993).", "startOffset": 65, "endOffset": 86}, {"referenceID": 22, "context": "practice batches of 20-30 sentences from the English Web Treebank (EWT) (Silveira et al., 2014) and FCE corpora, and resolved annotation disagreements during group meetings.", "startOffset": 72, "endOffset": 95}, {"referenceID": 2, "context": "Following the training period, the students annotated a treebank of learner English (Berzak et al., 2016) over a period of five months, three of which as a full time job.", "startOffset": 84, "endOffset": 105}, {"referenceID": 13, "context": "One participant reviews an annotation generated by the Turbo tagger and parser (Martins et al., 2013).", "startOffset": 79, "endOffset": 101}, {"referenceID": 24, "context": "The other participant reviews the output of the Stanford tagger (Toutanova et al., 2003) and RBG parser (Lei et al.", "startOffset": 64, "endOffset": 88}, {"referenceID": 10, "context": ", 2003) and RBG parser (Lei et al., 2014).", "startOffset": 23, "endOffset": 41}, {"referenceID": 12, "context": "For example, in Marcus et al. (1993) annotators were shown to produce POS tagging agreement of 92.", "startOffset": 16, "endOffset": 37}, {"referenceID": 12, "context": "Our experiment extends the human-based annotation study of Marcus et al. (1993) to include also syntactic trees.", "startOffset": 59, "endOffset": 80}, {"referenceID": 12, "context": "The initial agreement rate on POS annotation from scratch is higher than in (Marcus et al., 1993).", "startOffset": 76, "endOffset": 97}, {"referenceID": 24, "context": "For example, the best model of the Stanford tagger reported in Toutanova et al. (2003) produces an accuracy of 97.", "startOffset": 63, "endOffset": 87}, {"referenceID": 23, "context": "For example, Weiss et al. (2015) report 94.", "startOffset": 13, "endOffset": 33}, {"referenceID": 1, "context": "26 UAS and Andor et al. (2016) report 94.", "startOffset": 11, "endOffset": 31}, {"referenceID": 8, "context": "Subsequent work across various domains of decision making confirmed the robustness of anchoring using both informative and uninformative anchors (Furnham and Boo, 2011).", "startOffset": 145, "endOffset": 168}, {"referenceID": 27, "context": "Pertinent to our study, anchoring biases were also demonstrated when the participants were domain experts, although to a lesser degree than in the early anchoring experiments (Wilson et al., 1996; Mussweiler and Strack, 2000).", "startOffset": 175, "endOffset": 225}, {"referenceID": 14, "context": "Pertinent to our study, anchoring biases were also demonstrated when the participants were domain experts, although to a lesser degree than in the early anchoring experiments (Wilson et al., 1996; Mussweiler and Strack, 2000).", "startOffset": 175, "endOffset": 225}, {"referenceID": 23, "context": "The term \u201canchoring\u201d was coined in a seminal paper by Tversky and Kahneman (1974), which demonstrated that numerical estimation can be biased by uninformative prior information.", "startOffset": 54, "endOffset": 82}, {"referenceID": 7, "context": "Prior work in NLP examined the influence of pre-tagging (Fort and Sagot, 2010) and pre-parsing (Skj\u00e6rholt, 2013) on human annotations.", "startOffset": 56, "endOffset": 78}, {"referenceID": 23, "context": "Prior work in NLP examined the influence of pre-tagging (Fort and Sagot, 2010) and pre-parsing (Skj\u00e6rholt, 2013) on human annotations.", "startOffset": 95, "endOffset": 112}, {"referenceID": 12, "context": "Our study also extends the POS tagging experiments of Marcus et al. (1993), which compared inter-annotator agreement and annotation quality on manual POS tagging in annotation from scratch and tagger-based review conditions.", "startOffset": 54, "endOffset": 75}, {"referenceID": 12, "context": "Our study also extends the POS tagging experiments of Marcus et al. (1993), which compared inter-annotator agreement and annotation quality on manual POS tagging in annotation from scratch and tagger-based review conditions. The first result reported in that study was that tagger-based editing increases inter-annotator agreement compared to annotation from scratch. Our work provides a novel agreement benchmark for POS tagging which reduces annotation errors through a review process while controlling for tagger bias, and obtains agreement measurements for dependency parsing. The second result reported in Marcus et al. (1993) was that tagger-based edits are of higher quality compared to annotations from scratch when evaluated against an additional independent annotation.", "startOffset": 54, "endOffset": 632}, {"referenceID": 4, "context": "However, most such evaluations were conducted using annotation setups that can be affected by an anchoring bias (Carroll et al., 1999; Rambow et al., 2002; Silveira et al., 2014).", "startOffset": 112, "endOffset": 178}, {"referenceID": 17, "context": "However, most such evaluations were conducted using annotation setups that can be affected by an anchoring bias (Carroll et al., 1999; Rambow et al., 2002; Silveira et al., 2014).", "startOffset": 112, "endOffset": 178}, {"referenceID": 22, "context": "However, most such evaluations were conducted using annotation setups that can be affected by an anchoring bias (Carroll et al., 1999; Rambow et al., 2002; Silveira et al., 2014).", "startOffset": 112, "endOffset": 178}, {"referenceID": 19, "context": "A notable exception is the study of Sampson and Babarczy (2008) who measure agreement on annotation from scratch for English parsing in the SUSANNE framework (Sampson, 1995).", "startOffset": 158, "endOffset": 173}, {"referenceID": 9, "context": "Experiments on non-expert dependency annotation from scratch were previously reported for French, suggesting low agreement rates (79%) with an expert annotation benchmark (Gerdes, 2013).", "startOffset": 171, "endOffset": 185}, {"referenceID": 4, "context": "However, most such evaluations were conducted using annotation setups that can be affected by an anchoring bias (Carroll et al., 1999; Rambow et al., 2002; Silveira et al., 2014). A notable exception is the study of Sampson and Babarczy (2008) who measure agreement on annotation from scratch for English parsing in the SUSANNE framework (Sampson, 1995).", "startOffset": 113, "endOffset": 244}, {"referenceID": 15, "context": "Recent work in this area has already proposed an analysis of expert annotator disagreements for POS tagging in the absence of annotation guidelines (Plank et al., 2014).", "startOffset": 148, "endOffset": 168}, {"referenceID": 21, "context": "We believe that better understanding of human disagreements and their relation to disagreements between humans and parsers will also contribute to advancing evaluation methodologies for POS tagging and syntactic parsing in NLP, an important topic that has received only limited attention thus far (Schwartz et al., 2011; Plank et al., 2015).", "startOffset": 297, "endOffset": 340}, {"referenceID": 16, "context": "We believe that better understanding of human disagreements and their relation to disagreements between humans and parsers will also contribute to advancing evaluation methodologies for POS tagging and syntactic parsing in NLP, an important topic that has received only limited attention thus far (Schwartz et al., 2011; Plank et al., 2015).", "startOffset": 297, "endOffset": 340}], "year": 2016, "abstractText": "We present a study on two key characteristics of human syntactic annotations: anchoring and agreement. Anchoring is a well known cognitive bias in human decision making, where judgments are drawn towards preexisting values. We study the influence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output. Our experiments demonstrate a clear anchoring effect and reveal unwanted consequences, including overestimation of parsing performance and lower quality of annotations in comparison with humanbased annotations. Using sentences from the Penn Treebank WSJ, we also report systematically obtained inter-annotator agreement estimates for English dependency parsing. Our agreement results control for parser bias, and are consequential in that they are on par with state of the art parsing performance for English newswire. We discuss the impact of our findings on strategies for future annotation efforts and parser evaluations.1", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}