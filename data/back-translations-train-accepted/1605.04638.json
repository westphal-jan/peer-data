{"id": "1605.04638", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2016", "title": "Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient", "abstract": "This work focuses on dynamic regret of online convex optimization that compares the performance of online learning to a clairvoyant who knows the sequence of loss functions in advance and hence selects the minimizer of the loss function at each step. By assuming that the clairvoyant moves slowly (i.e., the minimizers change slowly), we present several improved variation-based upper bounds of the dynamic regret under the true and noisy gradient feedback, which are {\\it optimal} in light of the presented lower bounds. The key to our analysis is to explore a regularity metric that measures the temporal changes in the clairvoyant's minimizers, to which we refer as {\\it path variation}. Firstly, we present a general lower bound in terms of the path variation, and then show that under full information or gradient feedback we are able to achieve an optimal dynamic regret. Secondly, we present a lower bound with noisy gradient feedback and then show that we can achieve optimal dynamic regrets under a stochastic gradient feedback and two-point bandit feedback. Moreover, for a sequence of smooth loss functions that admit a small variation in the gradients, our dynamic regret under the two-point bandit feedback matches what is achieved with full information.", "histories": [["v1", "Mon, 16 May 2016 03:01:41 GMT  (36kb)", "http://arxiv.org/abs/1605.04638v1", "Accepted by the 33rd International Conference on Machine Learning (ICML 2016)"]], "COMMENTS": "Accepted by the 33rd International Conference on Machine Learning (ICML 2016)", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["tianbao yang", "lijun zhang 0005", "rong jin", "jinfeng yi"], "accepted": true, "id": "1605.04638"}, "pdf": {"name": "1605.04638.pdf", "metadata": {"source": "META", "title": "Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient", "authors": ["Tianbao Yang", "Lijun Zhang", "Rong Jin"], "emails": ["TIANBAO-YANG@UIOWA.EDU", "ZLJZJU@GMAIL.COM", "JINRONG.JR@ALIBABA-INC.COM", "JINFENGY@US.IBM.COM"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.04 638v 1 [cs.L GProceedings of the 33rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W & CP Volume 48. Copyright 2016 by the author (s)."}, {"heading": "1. Introduction", "text": "It is not only the way in which people in the USA, in Europe, in the USA, in Europe, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}, {"heading": "2. Optimal Dynamic Regret with Noiseless Information", "text": "In this section, we present an optimal dynamic regret that depends exclusively on V pT. We first present a lower limit and then present optimal upper limits in two settings: (i) At each step, the complete information about the loss function is revealed; (ii) only the true gradient at the decision vector is revealed for smooth loss functions that exhibit disappearing gradients in the feasible domain."}, {"heading": "2.1. Preliminaries and a Lower Bound", "text": "Since it is impossible to achieve a sublinear dynamic regret for each sequence of loss functions, we consider the following family of functions that allow a deviation limitation: Vp = {1,. \u2212 fT}: V pT \u2264 BT} (6), where BT is the budget. For a (random) policy \u03c0, which uses a sequence of solutions w1,.., wT for a sequence of loss functions f1,.., fT for a sequence of losses f1,., whose dynamic regret is then defined as Rempp ({1,.,.,. fT}), which uses a sequence of solutions w1,. t = 1ft (wt) \u2212 T. The worst dynamic regret about f. Vp is Rempp. (Vp, T) = supf. Vp Rempel."}, {"heading": "2.2. Online Learning with Full Information", "text": "Suppose that at each step the complete information about the loss function ft (w) is disclosed after the decision wt, and each loss function ft (w) is G-Lipschitz continuous. Then, we can update wt + 1 bywt + 1 = min w-ft (w), t-1mit w1 is any point in the calculation. To analyze the dynamic regret, we designate w-0 = w1.T-t = 1ft (wt) \u2212 T-t = 1ft (w-t) = T-t = 1ft (w-t) \u2212 T-T-t = 1ft (w-1) \u2212 T-t = 1ft (w-t) \u2264 T-t = 1G-w-t = 1G-t-t-2 + GT \u2212 1-t = 1-w-t = 1-t-t-t = t-w-t + 1-t-w-w-p-t = 1-t = 1-2-Gr + 1-2 = G-w1 = G-w1-t-2 + GT = 1-t = 1-t-t-t = 1-w-t-t-1-t-t-t = t-w-1-w-w-t = t-w-w-w-w-p-t = 1 ft-t = 1-t = 1-t = 1-t = 1-t-t = 1-t-t = 1-t = 1-Gr + 1-t = 1-2-t = 1-Gr + 1-t = 1-t = 1-Gr + Gr + 1 \u2212 w-2 = G-2 = G-wt = G-w1-t = G-w1-t = G-w1-t = 1-t = G-w1-t-t = 1-t = 1-t-t = T = max."}, {"heading": "2.3. Online Learning with Gradient Feedback", "text": "In this subsection, we assume that only the gradient information [1] is available after the wt decision. In the following, we first present several examples that show that O (V pT) is attainable and we generalize the analysis to a broad family. We consider two gradient functions g1 (w) = max. (0) 2 and g2 (w) = (w \u2212) 2, which are defined in the domain: O (V pT) is attainable and divides all iterations 1,..., T into a number of m batches with each batch size of T. Assume selects opponents g1 (\u00b7) in odd batches and g2 () in even batches and g2 (at each stage of complete gradient feedback, i.e. it is a number of m batches with each batch size of T."}, {"heading": "3. Optimal Dynamic Regret with Noisy Gradient", "text": "In this section we focus on the feedback of the noise gradient, i.e. \u03c6t (wt, ft) is only a noise gradient of ft (w) at wt."}, {"heading": "3.1. A Lower Bound with Noisy Gradient Feedback", "text": "Before we introduce the upper limits of dynamic regret with noise gradient feedback, we will first point out a lower limit. (For setting the lower limit we consider the following class of Policiesp: wt = {\u03c01 (U), t = 1 \u03c01 (BT). (BT). (BT). (BT). (BT). (BT). (BT). (BT). (BT). (BT). (BT). (U). (U). (U). (T). (T). (1). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S. (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S. (S). (S). (S). (S). (S. (S). (S). (S). (S). (S. (S). (S). (S). (S). (S. (S). (S). (S). (S). (S). (S. (S). (S). (S). (S). (S. (S). (S). (S. (S). (S). (S. (S). (S. (S. (S). (S). (S). (S. (S). (S. (S). (S). (S. (S. (S). (S. (S). (S). (S. (S). (S). (S). (S. (S.). (S. (S). (S. (S.). (S). (S). (S). (S.).)."}, {"heading": "3.2. Online Learning with Bounded Stochastic Gradient Feedback", "text": "We accept the policy defined by OGD by using the feedback of the sound gradient, i.e., we may fulfill the expectation of the expectation of expectation. (wt) We adopt the policy defined by OGD by using the feedback of the sound gradient, i.e., we assume that it is a noise difference with a satisfactory assumption. (wt) The upper limit of dynamic regret of OGD with an appropriate step size is presented below. (wt, ft) Adoption 7. (upper limit) Adoption 5. (w) Adoption 5. (w) Adoption, Adoption 2. (w)."}, {"heading": "3.3. Online Learning with Bandit Feedback", "text": "D (D). D (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "4. Conclusions", "text": "We have developed several lower and upper limits of dynamic regret, based on path variation, which measures the temporal changes of the optimal solutions. In view of the presented lower limits, the upper limits reached are optimal for non-strongly convex loss functions when the clairvoyant moves slowly. An interesting question that remains open is what is the optimal dynamic limit for strongly convex loss functions in terms of pastoral variation."}, {"heading": "Acknowledgements", "text": "The authors thank the anonymous reviewers for their helpful comments. T. Yang was partially supported by the NSF (1463988, 1545995)."}, {"heading": "A. Proof of Lemma 4", "text": "So wt + 1 = 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt + 1 wt +"}, {"heading": "B. Proof of Theorem 8", "text": "t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t (t) t) t (t) t (t) t) t (t) t (t) t (t) t) t (t) t (t) t (t) t) t (t) t (t) t (t) t) t (t) t (t) t (t) t (t) t) t (t) t (t) t (t) t) t (t) t (t) t (t) t (t) t) t (t (t) t) t (t) t (t) t (t) t (t) t (t) t (t) t) t (t) t (t (t) t) t (t) t) t (t (t) t) t (t) t (t) t) t (t) t (t) t (t) t) t (t (t) t) t (t) t) t (t) t (t (t) t (t) t (t) t) t (t) t (t) t (t) t) t) t (t (t) t (t) t) t (t (t) t) t (t) t (t) t) t (t) t) t (t (t)"}, {"heading": "C. Proof of Theorem 9", "text": "T (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T (T). (T). (T). (T (T). (T). (T). (T). (T (T). (T). (T). (T (T). (T (T). (T (T). (T (T). (T). (T). (T (T). (T (T). (T). (T (T). (T). (T. (T (T (T (T). (T). (T (T). (T). (T (T). (T (T). (T). (T. (. (T). (. (T). (T. (. (T). (T). (. (.). (. (. (T). (T). (. (. (. (. (T. (T).).). ("}], "references": [{"title": "Optimal algorithms for online convex optimization with multi-point bandit feedback", "author": ["Agarwal", "Alekh", "Dekel", "Ofer", "Xiao", "Lin"], "venue": "In Proceedings of the 23rd Conference on Learning Theory (COLT),", "citeRegEx": "Agarwal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "Unified algorithms for online learning and competitive analysis", "author": ["Buchbinder", "Niv", "Chen", "Shahar", "Naor", "Joseph", "Shamir", "Ohad"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Buchbinder et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Buchbinder et al\\.", "year": 2012}, {"title": "A new look at shifting", "author": ["Cesa-Bianchi", "Nicol", "Gaillard", "Pierre", "Lugosi", "Gbor", "Stoltz", "Gilles"], "venue": "regret. CoRR,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2012}, {"title": "Online optimization with gradual variations", "author": ["Chiang", "Chao-Kai", "Yang", "Tianbao", "Lee", "Chia-Jung", "Mahdavi", "Mehrdad", "Lu", "Chi-Jen", "Jin", "Rong", "Zhu", "Shenghuo"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Chiang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2012}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["Flaxman", "Abraham", "Kalai", "Adam Tauman", "McMahan", "H. Brendan"], "venue": "In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Online optimization in dynamic environments", "author": ["Hall", "Eric C", "Willett", "Rebecca M"], "venue": "CoRR, abs/1307.5944,", "citeRegEx": "Hall et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2013}, {"title": "Tracking the best expert", "author": ["Herbster", "Mark", "Warmuth", "Manfred K"], "venue": "Machine Learning,", "citeRegEx": "Herbster et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Herbster et al\\.", "year": 1998}, {"title": "Online optimization : Competing with dynamic comparators", "author": ["Jadbabaie", "Ali", "Rakhlin", "Alexander", "Shahrampour", "Shahin", "Sridharan", "Karthik"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Jadbabaie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jadbabaie et al\\.", "year": 2015}, {"title": "Regret bounded by gradual variation for online convex optimization", "author": ["Yang", "Tianbao", "Mahdavi", "Mehrdad", "Jin", "Rong", "Zhu", "Shenghuo"], "venue": "Machine Learning,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Recently, there emerges a surge of interest (Besbes et al., 2013; Hall & Willett, 2013; Jadbabaie et al., 2015) in the dynamic regret that compares the performance of online learning to a sequence of optimal solutions.", "startOffset": 44, "endOffset": 111}, {"referenceID": 2, "context": "We note that a regularity metric similar to the path variation (possibly measured in different norms) has been explored in shifting regret analysis (Herbster & Warmuth, 1998) and drifting regret analysis (Cesa-Bianchi et al., 2012; Buchbinder et al., 2012).", "startOffset": 204, "endOffset": 256}, {"referenceID": 1, "context": "We note that a regularity metric similar to the path variation (possibly measured in different norms) has been explored in shifting regret analysis (Herbster & Warmuth, 1998) and drifting regret analysis (Cesa-Bianchi et al., 2012; Buchbinder et al., 2012).", "startOffset": 204, "endOffset": 256}, {"referenceID": 2, "context": "In fact, a similar dynamic regret bound to \u221a V p T T has been established for online convex optimization over the simplex (Cesa-Bianchi et al., 2012), where the path variation is measured in l1 norm.", "startOffset": 122, "endOffset": 149}, {"referenceID": 1, "context": ", 2012; Buchbinder et al., 2012). The regret against the shifting experts was studied in tracking the best expert, where the best sequence of minimizers are assumed to change for a constant number of times. In drifting regret analysis, the constraint is relaxed to that the path variation is small. In fact, a similar dynamic regret bound to \u221a V p T T has been established for online convex optimization over the simplex (Cesa-Bianchi et al., 2012), where the path variation is measured in l1 norm. The present work focuses on OCO in the Euclidean space and considers noisy gradient feedback. A more general variation is considered in (Hall & Willett, 2013), where a sequence of (or a family of) dynamic models \u03c61, . . . , \u03c6T are revealed by the environment for the learner to predict the decision in the next step. Their variation is defined as V \u03c6 T = \u2211T\u22121 t=1 \u2016w\u2217 t+1 \u2212 \u03c6t(w\u2217 t )\u2016 for a sequence of comparators and their dynamic regret scales as V \u03c6 T \u221a T , which is worse than our bounds when \u03c6t(w) = w. There has been a different notion to measure the point-wise changes in the sequence of loss functions that measure the changes of two consecutive functions at any feasible points. For example, Besbes et al. (2013) considered the functional variation defined as", "startOffset": 8, "endOffset": 1222}, {"referenceID": 3, "context": "Another variation that measures point-wise difference between loss functions is the gradient variation introduced in (Chiang et al., 2012), which is defined as", "startOffset": 117, "endOffset": 138}, {"referenceID": 3, "context": "(5) The gradient variation has been explored for bounding the static regret (Chiang et al., 2012; Rakhlin & Sridharan, 2013; Yang et al., 2014).", "startOffset": 76, "endOffset": 143}, {"referenceID": 8, "context": "(5) The gradient variation has been explored for bounding the static regret (Chiang et al., 2012; Rakhlin & Sridharan, 2013; Yang et al., 2014).", "startOffset": 76, "endOffset": 143}, {"referenceID": 3, "context": "(5) The gradient variation has been explored for bounding the static regret (Chiang et al., 2012; Rakhlin & Sridharan, 2013; Yang et al., 2014). Recently, Jadbabaie et al. (2015) used the three variations and developed possibly better dynamic regret than using a single variation measure for nonstrongly convex loss functions.", "startOffset": 77, "endOffset": 179}, {"referenceID": 7, "context": ", 2013) (Jadbabaie et al., 2015) Loss function Feedback path variation functional variation three variations Lipschitz Full Information O(V p T ) O(V f T ) O(min( \u221a V p T V g T , (V g T ) T (V f T ) )) Lipschitz True Gradient N.", "startOffset": 8, "endOffset": 32}, {"referenceID": 7, "context": "ferent from (Jadbabaie et al., 2015), we consider the noisy gradient feedback (including the bandit feedback) and develop both upper bounds and lower bounds.", "startOffset": 12, "endOffset": 36}, {"referenceID": 7, "context": "It is notable that a similar upper bound of O(max(V f T , 1)) with the full information can be achieved (Jadbabaie et al., 2015).", "startOffset": 104, "endOffset": 128}, {"referenceID": 0, "context": "In order to have optimal dynamic regret bounds, we also consider two-point bandit setting and show that the previous algorithms in (Agarwal et al., 2010; Chiang et al., 2013) by adjusting the step size can achieve an O( \u221a V p T T ) dynamic regret for general Lipschitz continuous loss functions and an O(max( \u221a V g T V p T , V p T )) dynamic regret for smooth loss functions.", "startOffset": 131, "endOffset": 174}, {"referenceID": 4, "context": "For any wt \u2208 (1\u2212 \u03be)\u03a9 and any unit vector u, wt + \u03b4u \u2208 \u03a9 (Flaxman et al., 2005).", "startOffset": 56, "endOffset": 78}, {"referenceID": 2, "context": "In particular, using one-point bandit feedback Flaxman et al. (2005) showed an O(T ) static regret bound, while Agarwal et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 0, "context": "(2005) showed an O(T ) static regret bound, while Agarwal et al. (2010) established an optimal static regret bound of O( \u221a T ) using two-point bandit feedback.", "startOffset": 50, "endOffset": 72}, {"referenceID": 0, "context": "(2005) showed an O(T ) static regret bound, while Agarwal et al. (2010) established an optimal static regret bound of O( \u221a T ) using two-point bandit feedback. Recently, Chiang et al. (2013) derived a variational static regret bound in the two-point bandit setting that depends on \u221a V g T where V g T is the gradient variation defined in Section 1.", "startOffset": 50, "endOffset": 191}], "year": 2016, "abstractText": "This work focuses on dynamic regret of online convex optimization that compares the performance of online learning to a clairvoyant who knows the sequence of loss functions in advance and hence selects the minimizer of the loss function at each step. By assuming that the clairvoyant moves slowly (i.e., the minimizers change slowly), we present several improved variationbased upper bounds of the dynamic regret under the true and noisy gradient feedback, which are optimal in light of the presented lower bounds. The key to our analysis is to explore a regularity metric that measures the temporal changes in the clairvoyant\u2019s minimizers, to which we refer as path variation. Firstly, we present a general lower bound in terms of the path variation, and then show that under full information or gradient feedback we are able to achieve an optimal dynamic regret. Secondly, we present a lower bound with noisy gradient feedback and then show that we can achieve optimal dynamic regrets under a stochastic gradient feedback and two-point bandit feedback. Moreover, for a sequence of smooth loss functions that admit a small variation in the gradients, our dynamic regret under the two-point bandit feedback matches what is achieved with full information. Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).", "creator": "LaTeX with hyperref package"}}}