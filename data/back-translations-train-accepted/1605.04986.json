{"id": "1605.04986", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2016", "title": "A Constant-Factor Bi-Criteria Approximation Guarantee for $k$-means++", "abstract": "This paper studies the $k$-means++ algorithm for clustering as well as the class of $D^\\ell$ sampling algorithms to which $k$-means++ belongs. It is shown that for any constant factor $\\beta &gt; 1$, selecting $\\beta k$ cluster centers by $D^\\ell$ sampling yields a constant-factor approximation to the optimal clustering with $k$ centers, in expectation and without conditions on the dataset. This result extends the previously known $O(\\log k)$ guarantee for the case $\\beta = 1$ to the constant-factor bi-criteria regime. It also improves upon an existing constant-factor bi-criteria result that holds only with constant probability.", "histories": [["v1", "Mon, 16 May 2016 23:41:55 GMT  (50kb,D)", "http://arxiv.org/abs/1605.04986v1", "17 pages, 1 figure"]], "COMMENTS": "17 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG cs.CG", "authors": ["dennis wei"], "accepted": true, "id": "1605.04986"}, "pdf": {"name": "1605.04986.pdf", "metadata": {"source": "CRF", "title": "A Constant-Factor Bi-Criteria Approximation Guarantee for k-means++", "authors": ["Dennis Wei"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most of them are able to surpass themselves by putting themselves at the center. (...) In fact, it is so that they are able to surpass themselves. (...) In fact, it is so that most of them are able to surpass themselves. (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"(\") \"(\") \"(\") (\"(\") (\") (\") (\"(\") (\") ((\") (\") ((\") (\") ((\") (\") (\") (\") (\" (\") (\") (\") (\" (\") (\") (\") (\" (\") (\") (\"(\") (\") (\" (\") (\") (\"(\") (\") (\") (\"(\") (\") (\") (\") (\" (\") (\") (\") (\") (\"(\") (\") (\") (\") (\" (\") (\") (\"(\") (\") (\") (\"(\") (\") (\") (\") (\" (\") (\") (\"(\") (\") (\" (\") (\" (\") (\") (\"(\") (\") (\" (\") (\" (\") (\") (\"(\") \"(\") (\"(\" (\")\" (\")\" (\"(\") \"(\") \"(\" (\"(\") \"(\") \"(\" (\"(\") \"(\" (\")\" (\")\" (\"(\") \")\" (\"(\") \"(\" (\")\" (\")\" (\")\" (\"(\") \"(\" (\")\" (\")\" (\")\" (\"(\" (\"(\") \")\" (\"(\""}, {"heading": "1.1 Related Work", "text": "There is considerable literature on approximation algorithms for k means, k medians, and related problems covering a wide range of the goal between narrower approximation factors and lower algorithm complexity. At one end, exact approximation algorithms [Inaba et al., 1994] and several polynomial time approximation schemes (PTAS) [Matous, 2000, Badoiu et al., 2002, de la Vega et al., 2003, Har-Peled and Mazumdar, 2004, Kumar et al., 2010, Chen, 2009, Feldman et al., 2007, Jaiswal et al., 2014] were proposed for k means and k medians. While these polynomial durations have in n, the dependence on k and sometimes on the dimension d is exponential or worse. A simpler local search algorithm was shown to achieve approximation for k means (3 + 2 / p)."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Problem Definition", "text": "We get n points x1,.., xn in a real metric space X with metric D (x, y). The goal is to select t cluster centers c1,.., ct in X and assign points to the closest cluster center in order to minimize the potential functional center = n \u2211 i = 1 min j = 1,..., t D (xi, cj). \"(1) A cluster is thus defined by the points xi assigned to a center cj where connections (several closest centers) are arbitrarily broken up. \u03c6 (S) = \u2211 xi \u0445 S minj = 1,..., tD (xi, cj)\" is considered a contribution to the potential of S. \u03c6 (xi) is the contribution of a single point xi.The exponent \"\u2265 1 in (1) is considered a problem parameter."}, {"heading": "2.2 D` Sampling Algorithm", "text": "The D'Sampling algorithm randomly selects cluster centers using x1,.., xn with probabilities proportional to their current contribution to potential, as described in Algorithm 1. According to Arthur and Vassilvitskii [2007], the case \"= 2 is referred to as k-mean + + algorithm and the probabilities used after the first iteration are referred to as D2 weighting (hence D 'in general). For t Cluster centers, the runtime of D'Sampling O (ntd) is in d dimensions. Algorithm 1 D'Sampling Input: data points x1,..., xn, number of clusters t Select first cluster center c1 uniformly using the random principle of x1,..., xn. Calculate Medicare (xi) for i = 1,.., n. for j = 2 to t doSelect jth center cj = xi with probability (xi) / excellent."}, {"heading": "2.3 Existing Lemmas Regarding D` Sampling", "text": "The following lemmas synthesize the results of Arthur and Vassilvitskii [2007], which define the expected potential within a single optimal cluster by selecting a center from that cluster with equal or D 'weighting, as in Algorithm 1. These lemmas define the constant r (') D, which appears in the main results below, and are also used in their evidences.Lemma 1. [Arthur and Vassilvitskii, 2007, Lemmas 3.1 and 5.1] With an optimal cluster A, the potential resulting from the randomly selected selection of a first cluster center of A with equal weight.Lemma 2. [Arthur and Vassilvitskii, 2007, Lemmas 3.2] for each A results in where (') u = {2,' = 2 and D is euclidean, 2 ', otherwise. Lemma 2. [Arthur and Vassilvitskii, 2007, Lemma 3.2] Given an optimal cluster as an initial A and potential?"}, {"heading": "3 Main Results", "text": "Theorem 1. Let \u03c6 be the potential resulting from the selection of \u03b2k cluster centers according to algorithm 1, where \u03b2 \u2265 1. The expected approximation ratio is then limited as E [\u03c6] \u03c6 \u2264 r (') D (1 + min {\u0432 (k \u2212 2) (\u03b2 \u2212 1) k +, Hk \u2212 1}) \u2212 (1n), with both the golden section and the golden section and Hk = 1 + 12 + \u00b7 \u00b7 \u00b7 \u00b7 + 1 k \u00b2 log k being the shortest number of harmonics. The proof for Theorem 1 in section 4.2 shows that the 1 / n term is in fact not positive and can therefore be omitted as an approximate loss."}, {"heading": "3.1 Comparisons to Existing Results", "text": "A comparison of theorem 1 with the results of Arthur and Vassilvitskii [2007] is implicit in his statement, since the minimum Hk \u2212 1 term is derived directly from Arthur and Vassilvitskii [2007, Theorems 3.1 and 5.1]. For k = 2, 3, the minimum first term is smaller than Hk \u2212 1 for each \u03b2 \u2265 1, and therefore Theorem 1 is always an improvement. For k > 3, theorem 1 improves Arthur and Vassilvitskii [2007] for \u03b2 greater than the critical value 1 + (k \u2212 2 \u2212 Hk \u2212 1) kHk \u2212 1. The numerical evaluation of \u03b2c shows that it reaches a maximum value of 1,204 at k = 22 and then decreases towards 1 approximately as 1 / Hk \u2212 1. It can be concluded that for each k, a maximum of 20% oversampling of theorem 1 is required to achieve a better approximation than Arthur and Vassilvitski.2007 The result is most closely related to factor 1."}, {"heading": "4 Proofs", "text": "The overall strategy for the detection of theorem 1 is similar to that of Arthur and Vassilvitskii [2007]. The most important intermediate result is Lemma 3 below, which relates the potential for later iteration in algorithm 1 to the potential for earlier iteration. Section 4.1 is devoted to the detection of Lemma 3. Subsequently, in Section 4.2 Theorem 1, an application of Lemma 3. Subsequently, we say that an optimal cluster A is covered by a series of cluster centers if at least one of the centers is located in A. Otherwise, A is revealed. Also, define \u03c1 = r (') D \u03c6 as an abbreviation."}, {"heading": "4.1 Proof of Lemma 3", "text": "Lemma 3 has been proved by induction and shows that if it applies to (t, u) and (t, u + 1), it also applies to (t + 1, u + 1), similar to the evidence of Arthur and Vassilvitskii [2007, Lemma 3.3]. The proof is divided into three parts. Section 4.1.1 contains basic cases. In Section 4.1.2, sufficient conditions for the coefficients cV (t, u), cU (t, u) are derived, which make it possible to complete the inductive step. In Section 4.1.3, it is shown that the closed expressions in (2) agree with the basic cases in Section 4.1.1 and meet the sufficient conditions in Section 4.1.2, thereby completing the proof."}, {"heading": "4.1.1 Base cases", "text": "This subsection contains two basic cases of Lemma 3. While the second of these basic cases does not correspond to the functions of (2), later in Section 4.1.3 it is shown that the same base cases also correspond to coefficients of (2). The first case corresponds to u = 0, for which we have? (V) =?. Since the addition of centers cannot increase the potential, i.e., Lema 3 deterministically holds withcV (t, 0) = 1, cU (t, 0) = 0, t \u2265 0. (3) The second base case stands for t = u, u \u2265 1. For this purpose, a slightly reinforced version of Arthur and Vassilvitskii [2007, Lemma 3.3] is used, as stated below. Lemma 4. With the same definitions as in Lemma 3 except for t \u2264 u, u \u00b2, we have a specific version of Arthur and Vassilvitski [2007, Lemma 3.3]."}, {"heading": "4.1.2 Sufficient conditions on coefficients", "text": "In this subsection, it is inductively assumed that Lemma 3 applies to (t, u) and (t, u + 1) cases investigated, and the induction in the case (t + 1, u + 1) is then concluded under the following sufficient conditions: cV (t, u + 1) - cV (t, u + 1) - (t, u + 1) (t, u + 1) - cU (t, u + 1) - cV (t, u + 1) - (t, u + 1) - (t, u - 1) - (t) - (t) - (t) - (t) - (t, u - (t) - (t) - (t, u - (t) (t) - (t), u - (t) (t) (+ -), u - (t) (t, u - (t) (t), u - (t) (t, u - (t) (t, u - (t), u - (t) (t, t - (t), t (+ 1), t (t), t (+ - (t), u - (t), u - (t), u - (t (t), u - (t), t (- (t), t (- (t), t (+ -, t), t (- (t), u - (- (t), u - (t), u - (- (t), u - (t, u - (t), u - (t (t), u - (t (t), u - (t), t (- (t), t (- (t), t), t (t (t - (t), t - (t), t (- (t - (t), t), t (t (- (t -), t), t (t (t), t (t - (t (t), t (t (t), u -), u - (t (t (t), u -), t (t (- (- (t), u - (t), u - (- (t), t (- (t (t), t (t (- ("}, {"heading": "4.1.3 Proof with specific form for coefficients", "text": "These expressions are more general than (2) and are based on the observations shown in Figure 1.cV (t, u) = t + au + bt \u2212 u + b = 1 + (a + 1) u (a + 1) u (b) u (a + 1) u (b) u (a + 1) u (b) u (b) u (b) u + b) u (b) u (b) u (b) u \u2212 u) u (b) u (b) u (b) u (b) u) u (b) u + b) u (b) u (b) u (b) u) u (b) u (b) u (b) u) u (b) u (b) u (u) u (b) u) u (u) u (u) (u) (u) (u) (b) u) (u) (u) (u) (u) (b), u (u) (u) (u) (b)."}, {"heading": "4.2 Proof of Theorem 1", "text": "In the first iteration of the algorithm \u03b2 1, the first cluster center (\u03b2 \u03b2 \u03b2 1) is selected from some A with the probability nA / n. Due to this event, Lemma 3 with the covered quantity V = A, u = k \u2212 1 uncovered clusters and t = n = n \u2212 1 remaining cluster centers are selected. These limits include the final potential target??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "5 Conclusion and Future Work", "text": "This work has shown that simple D'scanning algorithms, including k-mean + +, are guaranteed in the expectation of achieving a constant factor-bi-criterion approach to optimal clustering. As mentioned in Section 3, the constant r (') D in Theorem 1 and Episode 1 provides an opportunity to further improve the approximation boundaries. One possibility is to streamline Lemmas 3.2 and 5.1 in Arthur and Vassilvitskii [2007], which are responsible for the r (') D factor. A more significant improvement could be achieved by considering not only the coverage of optimal clusters by at least one cluster center, but also the effect of selecting more than one center from a single optimal cluster. As the number of selected centers increases, an approximation factor analogous to r (') D may also decrease in the analysis of D algorithms of similar interest."}, {"heading": "A Proof of Lemma 4", "text": "The proof follows the inductive proof of Arthur and Vassilvitskii [2007, Lemma 3,3] is determined by the notation changes of u (u) (u) (u) (u) (u) (u) (u) (u), Xc (V) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u), u (u) (u) (u), u (u) (u), u (u) (u), u (u) (u) (u), u (u) (u), u (u) (u) (u), u (u) (u) (u), u (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u (u) (u) (u) (u) (u) (u (u) (u) (u) (u) (u) (u (u) (u) (u) (u) (u) (u) (u (u) (u) (u) (u (u) (u) (u) (u) (u (u) (u) (u) (u), u (u) (u), u), u (u (u), u (u (u), u (u (u), u), u (u (u (u), u (u), u), u (u), u (u (u (u), u (u), u (u), u (u (u), u (u (u), u), u (u"}, {"heading": "B Proof of Lemma 8", "text": "Set (11) to the leftmost factor in (5b), cV (t, u + 1) \u2212 cU (t, u + 1) = cV (t, u + 1) \u2212 cV (t \u2212 1, u) = (a + 1) (u + 1) t \u2212 u \u2212 1 + b \u2212 (a + 1) u \u2212 1 \u2212 u + b = a + 1t \u2212 u \u2212 1 + b. Similarly on the right side of (5b), cU (t, u + 1) \u2212 cV (t, u) = cV (t \u2212 1, u) \u2212 cV (t, u) = (a + 1) u \u2212 1 \u2212 u + b \u2212 (a + 1) u (t \u2212 u + b + b + b) (t \u2212 u \u2212 1 + 2)."}, {"heading": "C Proof of Lemma 9", "text": "As already mentioned, (11a) has the property that cV (t, u + 1) \u2265 cV (t, u) + b (b) (t, u) for all t, u. Therefore (6a) is equivalent to 2cV (t + 1, u + 1) \u2212 cV (t, u + 1) \u2265 \u221a cV (t, u) 2 + 4 (cV (t, u + 1) \u2212 cV (t, u). (21) \u2212 t \u2212 \u2212 \u2212 t (a + 1) (t + 2) (t + 2) (t, u + 1) \u2212 t (u + 2) \u2212 t (u + 2) \u2212 t (u + 2) \u2212 t (u + 2) \u2212 t \u2212 t (u + 2) \u2212 t \u2212 u \u2212 t (u + 2) \u2212 t \u2212 t \u2212 t (u + 2) \u2212 t \u2212 t (u + 2) t \u2212 t (u + b) \u2212 t (u + 2) \u2212 u (t) \u2212 t \u2212 t (u + b), + 2 \u2212 t (t) \u2212 t \u2212 t."}], "references": [{"title": "Adaptive sampling for k-means clustering", "author": ["A. Aggarwal", "A. Deshpande", "R. Kannan"], "venue": "In Proceedings of the 12th International Workshop and 13th International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques,", "citeRegEx": "Aggarwal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Aggarwal et al\\.", "year": 2009}, {"title": "Streaming k-means approximation", "author": ["N. Ailon", "R. Jaiswal", "C. Monteleoni"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ailon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2009}, {"title": "NP-hardness of Euclidean sum-of-squares clustering", "author": ["D. Aloise", "A. Deshpande", "P. Hansen", "P. Popat"], "venue": "Machine Learning,", "citeRegEx": "Aloise et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Aloise et al\\.", "year": 2009}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In Proceedings of the 18th ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Arthur and Vassilvitskii.,? \\Q2007\\E", "shortCiteRegEx": "Arthur and Vassilvitskii.", "year": 2007}, {"title": "Local search heuristics for k-median and facility location problems", "author": ["V. Arya", "N. Garg", "R. Khandekar", "A. Meyerson", "K. Munagala", "V. Pandit"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Arya et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Arya et al\\.", "year": 2004}, {"title": "The hardness of approximation of Euclidean kmeans", "author": ["P. Awasthi", "M. Charikar", "R. Krishnaswamy", "A.K. Sinop"], "venue": "In Proceedings of the 31st International Symposium on Computational Geometry,", "citeRegEx": "Awasthi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2015}, {"title": "Approximate clustering via core-sets", "author": ["M. Badoiu", "S. Har-Peled", "P. Indyk"], "venue": "In Proceedings of the 34th ACM Symposium on Theory of Computing,", "citeRegEx": "Badoiu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Badoiu et al\\.", "year": 2002}, {"title": "A tight lower bound instance for k-means++ in constant dimension, volume 8402 of Lecture Notes in Computer Science, pages 7\u201322", "author": ["A. Bhattacharya", "R. Jaiswal", "N. Ailon"], "venue": null, "citeRegEx": "Bhattacharya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bhattacharya et al\\.", "year": 2014}, {"title": "A bad instance for k-means++", "author": ["T. Brunsch", "H. R\u00f6glin"], "venue": "Theoretical Computer Science,", "citeRegEx": "Brunsch and R\u00f6glin.,? \\Q2013\\E", "shortCiteRegEx": "Brunsch and R\u00f6glin.", "year": 2013}, {"title": "A constant-factor approximation algorithm for the k-median problem", "author": ["M. Charikar", "S. Guha", "E. Tardos", "D.B. Shmoys"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Charikar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2002}, {"title": "On coresets for k-median and k-means clustering in metric and Euclidean spaces and their applications", "author": ["K. Chen"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Chen.,? \\Q2009\\E", "shortCiteRegEx": "Chen.", "year": 2009}, {"title": "The hardness of k-means clustering", "author": ["S. Dasgupta"], "venue": "Technical Report CS2008-0916, Department of Computer Science and Engineering,", "citeRegEx": "Dasgupta.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta.", "year": 2008}, {"title": "Approximation schemes for clustering problems", "author": ["W.F. de la Vega", "M. Karpinski", "C. Kenyon", "Y. Rabani"], "venue": "In Proceedings of the 35th ACM Symposium on Theory of Computing,", "citeRegEx": "Vega et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Vega et al\\.", "year": 2003}, {"title": "A PTAS for k-means clustering based on weak coresets", "author": ["D. Feldman", "M. Monemizadeh", "C. Sohler"], "venue": "In Proceedings of the 23rd International Symposium on Computational Geometry,", "citeRegEx": "Feldman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2007}, {"title": "Clustering data streams: Theory and practice", "author": ["S. Guha", "A. Meyerson", "N. Mishra", "R. Motwani", "L. O\u2019Callaghan"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Guha et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guha et al\\.", "year": 2003}, {"title": "On coresets for k-means and k-median clustering", "author": ["S. Har-Peled", "S. Mazumdar"], "venue": "In Proceedings of the 36th ACM Symposium on Theory of Computing,", "citeRegEx": "Har.Peled and Mazumdar.,? \\Q2004\\E", "shortCiteRegEx": "Har.Peled and Mazumdar.", "year": 2004}, {"title": "Applications of weighted Voronoi diagrams and randomization to variancebased k-clustering", "author": ["M. Inaba", "N. Katoh", "H. Imai"], "venue": "In Proceedings of the 10th International Symposium on Computational Geometry,", "citeRegEx": "Inaba et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Inaba et al\\.", "year": 1994}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Jain.,? \\Q2010\\E", "shortCiteRegEx": "Jain.", "year": 2010}, {"title": "Approximation algorithms for metric facility location and k-median problems using the primal-dual schema and Lagrangian relaxation", "author": ["K. Jain", "V.V. Vazirani"], "venue": "Journal of the ACM,", "citeRegEx": "Jain and Vazirani.,? \\Q2001\\E", "shortCiteRegEx": "Jain and Vazirani.", "year": 2001}, {"title": "A new greedy approach for facility location problems", "author": ["K. Jain", "M. Mahdian", "A. Saberi"], "venue": "In Proceedings of the 34th ACM Symposium on Theory of Computing,", "citeRegEx": "Jain et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2002}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Computational Geometry,", "citeRegEx": "Kanungo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kanungo et al\\.", "year": 2004}, {"title": "Linear-time approximation schemes for clustering problems in any dimensions", "author": ["A. Kumar", "Y. Sabharwal", "S. Sen"], "venue": "Journal of the ACM,", "citeRegEx": "Kumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "Least squares quantization in PCM", "author": ["S. Lloyd"], "venue": "Technical report, Bell Laboratories,", "citeRegEx": "Lloyd.,? \\Q1957\\E", "shortCiteRegEx": "Lloyd.", "year": 1957}, {"title": "Least squares quantization in PCM", "author": ["S. Lloyd"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Lloyd.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd.", "year": 1982}, {"title": "The planar k-means problem is NP-hard", "author": ["M. Mahajan", "P. Nimbhorkar", "K. Varadarajan"], "venue": "In Proceedings of the 3rd International Workshop on Algorithms and Computation,", "citeRegEx": "Mahajan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mahajan et al\\.", "year": 2009}, {"title": "A bi-criteria approximation algorithm for k means", "author": ["K. Makarychev", "Y. Makarychev", "M. Sviridenko", "J. Ward"], "venue": "Technical Report arXiv:1507.04227,", "citeRegEx": "Makarychev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Makarychev et al\\.", "year": 2015}, {"title": "On approximate geometric k-clustering", "author": ["J. Matou\u0161ek"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "Matou\u0161ek.,? \\Q2000\\E", "shortCiteRegEx": "Matou\u0161ek.", "year": 2000}, {"title": "Optimal time bounds for approximate clustering", "author": ["R.R. Mettu", "C.G. Plaxton"], "venue": "Machine Learning,", "citeRegEx": "Mettu and Plaxton.,? \\Q2004\\E", "shortCiteRegEx": "Mettu and Plaxton.", "year": 2004}, {"title": "The effectiveness of Lloyd-type methods for the k-means problem", "author": ["R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy"], "venue": "Journal of the ACM,", "citeRegEx": "Ostrovsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ostrovsky et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "The k-means problem and its variants constitute one of the most popular paradigms for clustering [Jain, 2010].", "startOffset": 97, "endOffset": 109}, {"referenceID": 19, "context": ", 2015] and k-medians (` = 1) [Jain et al., 2002].", "startOffset": 30, "endOffset": 49}, {"referenceID": 3, "context": "In this paper, we study an enhancement to Lloyd\u2019s algorithm known as k-means++ [Arthur and Vassilvitskii, 2007] and the more general class of D` sampling algorithms to which k-means++ belongs.", "startOffset": 79, "endOffset": 111}, {"referenceID": 3, "context": "D` sampling is attractive for two reasons: First, it is guaranteed to yield an expected O(log k) approximation to the optimal clustering with k centers [Arthur and Vassilvitskii, 2007].", "startOffset": 152, "endOffset": 184}, {"referenceID": 2, "context": "Optimal clustering in this sense is known to be NP-hard, in particular for k-means (` = 2) [Dasgupta, 2008, Aloise et al., 2009, Mahajan et al., 2009, Awasthi et al., 2015] and k-medians (` = 1) [Jain et al., 2002]. In practice, the most widely used algorithm remains Lloyd\u2019s [1957, 1982] (often referred to as the k-means algorithm), which alternates between updating centers given cluster assignments and re-assigning points to clusters. In this paper, we study an enhancement to Lloyd\u2019s algorithm known as k-means++ [Arthur and Vassilvitskii, 2007] and the more general class of D` sampling algorithms to which k-means++ belongs. These algorithms select cluster centers randomly from the given data points with probabilities proportional to their current costs. The clustering can then be refined using Lloyd\u2019s algorithm. D` sampling is attractive for two reasons: First, it is guaranteed to yield an expected O(log k) approximation to the optimal clustering with k centers [Arthur and Vassilvitskii, 2007]. Second, it is as simple as Lloyd\u2019s algorithm, both conceptually as well as computationally with O(nkd) running time in d dimensions. The particular focus of this paper is on the setting where an optimal k-clustering remains the benchmark but more than k cluster centers can be sampled to improve the approximation. Specifically, it is shown (see Theorem 1 and Corollary 1) that for any constant factor \u03b2 > 1, if \u03b2k centers are chosen byD` sampling, then a constant-factor approximation to the optimal k-clustering is obtained. This guarantee holds in expectation and for all datasets, like the one in Arthur and Vassilvitskii [2007], and improves upon the O(log k) factor therein.", "startOffset": 108, "endOffset": 1644}, {"referenceID": 0, "context": "Furthermore, if a solution with a specified number of clusters k is truly required, then linear programming techniques can be used to select a k-subset from the \u03b2k cluster centers while still maintaining a constant-factor approximation [Aggarwal et al., 2009, Charikar et al., 2002]. The main result in this paper differs from the constant-factor bi-criteria approximation established in Aggarwal et al. [2009] in that the latter holds only with constant probability as opposed to in expectation.", "startOffset": 237, "endOffset": 411}, {"referenceID": 0, "context": "Furthermore, if a solution with a specified number of clusters k is truly required, then linear programming techniques can be used to select a k-subset from the \u03b2k cluster centers while still maintaining a constant-factor approximation [Aggarwal et al., 2009, Charikar et al., 2002]. The main result in this paper differs from the constant-factor bi-criteria approximation established in Aggarwal et al. [2009] in that the latter holds only with constant probability as opposed to in expectation. Using Markov\u2019s inequality, a constant-probability corollary can be derived from Theorem 1 herein, and doing so improves upon the approximation factor of Aggarwal et al. [2009] by more than a factor of 2.", "startOffset": 237, "endOffset": 673}, {"referenceID": 0, "context": "Furthermore, if a solution with a specified number of clusters k is truly required, then linear programming techniques can be used to select a k-subset from the \u03b2k cluster centers while still maintaining a constant-factor approximation [Aggarwal et al., 2009, Charikar et al., 2002]. The main result in this paper differs from the constant-factor bi-criteria approximation established in Aggarwal et al. [2009] in that the latter holds only with constant probability as opposed to in expectation. Using Markov\u2019s inequality, a constant-probability corollary can be derived from Theorem 1 herein, and doing so improves upon the approximation factor of Aggarwal et al. [2009] by more than a factor of 2. The present paper also differs from recent work on more general bi-criteria approximation of k-means by Makarychev et al. [2015], which analyzes substantially more complex algorithms.", "startOffset": 237, "endOffset": 830}, {"referenceID": 16, "context": "At one end, exact algorithms [Inaba et al., 1994] and several polynomial-time approximation schemes (PTAS) [Matou\u0161ek, 2000, Badoiu et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 26, "context": "[2004] also rely on a discretization to an -approximate centroid set [Matou\u0161ek, 2000] of size O(n \u2212d log(1/ )).", "startOffset": 69, "endOffset": 85}, {"referenceID": 14, "context": "A constant-factor approximation with slightly superlinear running time has also been obtained in the streaming setting [Guha et al., 2003].", "startOffset": 119, "endOffset": 138}, {"referenceID": 4, "context": ", 1994] and several polynomial-time approximation schemes (PTAS) [Matou\u0161ek, 2000, Badoiu et al., 2002, de la Vega et al., 2003, Har-Peled and Mazumdar, 2004, Kumar et al., 2010, Chen, 2009, Feldman et al., 2007, Jaiswal et al., 2014] have been proposed for k-means and k-medians. While these have polynomial running times in n, the dependence on k and sometimes on the dimension d is exponential or worse. A simpler local search algorithm was shown to yield a ((3+2/p)`+ ) approximation for k-means (` = 2) in Kanungo et al. [2004] and k-medians (` = 1) in Arya et al.", "startOffset": 82, "endOffset": 532}, {"referenceID": 3, "context": "[2004] and k-medians (` = 1) in Arya et al. [2004], the latter under the additional constraint that centers are chosen from a finite set.", "startOffset": 32, "endOffset": 51}, {"referenceID": 3, "context": "[2004] and k-medians (` = 1) in Arya et al. [2004], the latter under the additional constraint that centers are chosen from a finite set. This local search however requires a polynomial number of iterations of complexity nO(p), and Kanungo et al. [2004] also rely on a discretization to an -approximate centroid set [Matou\u0161ek, 2000] of size O(n \u2212d log(1/ )).", "startOffset": 32, "endOffset": 254}, {"referenceID": 3, "context": "In Arthur and Vassilvitskii [2007], it was proved that D` sampling results in an O(log k) approximation, in expectation and for all datasets.", "startOffset": 3, "endOffset": 35}, {"referenceID": 3, "context": "In Arthur and Vassilvitskii [2007], it was proved that D` sampling results in an O(log k) approximation, in expectation and for all datasets. The current work builds upon Arthur and Vassilvitskii [2007] to extend the guarantee to the constant-factor bi-criteria regime.", "startOffset": 3, "endOffset": 203}, {"referenceID": 3, "context": "In Arthur and Vassilvitskii [2007], it was proved that D` sampling results in an O(log k) approximation, in expectation and for all datasets. The current work builds upon Arthur and Vassilvitskii [2007] to extend the guarantee to the constant-factor bi-criteria regime. Arthur and Vassilvitskii [2007] also provided a matching lower bound, exhibiting a dataset on which k-means++ achieves an expected \u03a9(log k) approximation.", "startOffset": 3, "endOffset": 302}, {"referenceID": 3, "context": "In Arthur and Vassilvitskii [2007], it was proved that D` sampling results in an O(log k) approximation, in expectation and for all datasets. The current work builds upon Arthur and Vassilvitskii [2007] to extend the guarantee to the constant-factor bi-criteria regime. Arthur and Vassilvitskii [2007] also provided a matching lower bound, exhibiting a dataset on which k-means++ achieves an expected \u03a9(log k) approximation. Sampling algorithms have been shown to yield improved O(1) approximation factors provided that the dataset satisfies certain conditions. Such a result was established in Ostrovsky et al. [2012] for k-means++ and other variants of Lloyd\u2019s algorithm under the condition that the dataset is well-suited in a sense to partitioning into k clusters.", "startOffset": 3, "endOffset": 619}, {"referenceID": 3, "context": "In Arthur and Vassilvitskii [2007], it was proved that D` sampling results in an O(log k) approximation, in expectation and for all datasets. The current work builds upon Arthur and Vassilvitskii [2007] to extend the guarantee to the constant-factor bi-criteria regime. Arthur and Vassilvitskii [2007] also provided a matching lower bound, exhibiting a dataset on which k-means++ achieves an expected \u03a9(log k) approximation. Sampling algorithms have been shown to yield improved O(1) approximation factors provided that the dataset satisfies certain conditions. Such a result was established in Ostrovsky et al. [2012] for k-means++ and other variants of Lloyd\u2019s algorithm under the condition that the dataset is well-suited in a sense to partitioning into k clusters. In Mettu and Plaxton [2004], anO(1) approximation was shown for a somewhat more complicated algorithm called successive sampling with O(n(k + log n) + k2 log n) running time, subject to a bound on the dispersion of the points.", "startOffset": 3, "endOffset": 797}, {"referenceID": 3, "context": "In Arthur and Vassilvitskii [2007], it was proved that D` sampling results in an O(log k) approximation, in expectation and for all datasets. The current work builds upon Arthur and Vassilvitskii [2007] to extend the guarantee to the constant-factor bi-criteria regime. Arthur and Vassilvitskii [2007] also provided a matching lower bound, exhibiting a dataset on which k-means++ achieves an expected \u03a9(log k) approximation. Sampling algorithms have been shown to yield improved O(1) approximation factors provided that the dataset satisfies certain conditions. Such a result was established in Ostrovsky et al. [2012] for k-means++ and other variants of Lloyd\u2019s algorithm under the condition that the dataset is well-suited in a sense to partitioning into k clusters. In Mettu and Plaxton [2004], anO(1) approximation was shown for a somewhat more complicated algorithm called successive sampling with O(n(k + log n) + k2 log n) running time, subject to a bound on the dispersion of the points. A constant-factor approximation with slightly superlinear running time has also been obtained in the streaming setting [Guha et al., 2003]. For k-means++, the \u03a9(log k) lower bound in Arthur and Vassilvitskii [2007], which holds in expectation, has spurred follow-on works on the question of whether k-means++ might guarantee a constant-factor approximation with reasonably large probability.", "startOffset": 3, "endOffset": 1211}, {"referenceID": 4, "context": "[2013], who showed that an approximation factor better than (2/3) log k cannot be achieved with probability higher than a decaying exponential in k, and Bhattacharya et al. [2014], who showed that a similar statement holds even in 2 dimensions.", "startOffset": 153, "endOffset": 180}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability.", "startOffset": 63, "endOffset": 86}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k).", "startOffset": 63, "endOffset": 522}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation.", "startOffset": 63, "endOffset": 614}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation. Furthermore, Section 3.1 shows that a constant-probability corollary of Theorem 1 improves significantly upon Aggarwal et al. [2009]. Recently, Makarychev et al.", "startOffset": 63, "endOffset": 847}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation. Furthermore, Section 3.1 shows that a constant-probability corollary of Theorem 1 improves significantly upon Aggarwal et al. [2009]. Recently, Makarychev et al. [2015] has also established constant-factor bi-criteria results for k-means.", "startOffset": 63, "endOffset": 883}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation. Furthermore, Section 3.1 shows that a constant-probability corollary of Theorem 1 improves significantly upon Aggarwal et al. [2009]. Recently, Makarychev et al. [2015] has also established constant-factor bi-criteria results for k-means. Their work differs from the present paper in studying more complex algorithms. First, similar to Kanungo et al. [2004], Makarychev et al.", "startOffset": 63, "endOffset": 1072}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation. Furthermore, Section 3.1 shows that a constant-probability corollary of Theorem 1 improves significantly upon Aggarwal et al. [2009]. Recently, Makarychev et al. [2015] has also established constant-factor bi-criteria results for k-means. Their work differs from the present paper in studying more complex algorithms. First, similar to Kanungo et al. [2004], Makarychev et al. [2015] reduce the k-means problem to an -approximate, finite-set instance of k-medians of size nO(log(1/ )/ 2).", "startOffset": 63, "endOffset": 1098}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation. Furthermore, Section 3.1 shows that a constant-probability corollary of Theorem 1 improves significantly upon Aggarwal et al. [2009]. Recently, Makarychev et al. [2015] has also established constant-factor bi-criteria results for k-means. Their work differs from the present paper in studying more complex algorithms. First, similar to Kanungo et al. [2004], Makarychev et al. [2015] reduce the k-means problem to an -approximate, finite-set instance of k-medians of size nO(log(1/ )/ 2). Subsequently, linear programming and local search algorithms are considered, the latter the same as in Kanungo et al. [2004], Arya et al.", "startOffset": 63, "endOffset": 1328}, {"referenceID": 0, "context": "In a similar direction to the one pursued in the present work, Aggarwal et al. [2009] showed that if the number of cluster centers can be increased to a constant factor times k, then a constant-factor approximation can be achieved with constant probability. Specifically, they prove that using d16(k+ \u221a k)e centers gives an approximation factor of 20 with probability 0.03, together with a general bi-criteria guarantee but without explicit constants. An O(1) factor was also obtained independently by Ailon et al. [2009] using more centers, of order O(k log k). As mentioned, the result of Aggarwal et al. [2009] differs from Theorem 1 herein in being true with constant probability as opposed to in expectation. Furthermore, Section 3.1 shows that a constant-probability corollary of Theorem 1 improves significantly upon Aggarwal et al. [2009]. Recently, Makarychev et al. [2015] has also established constant-factor bi-criteria results for k-means. Their work differs from the present paper in studying more complex algorithms. First, similar to Kanungo et al. [2004], Makarychev et al. [2015] reduce the k-means problem to an -approximate, finite-set instance of k-medians of size nO(log(1/ )/ 2). Subsequently, linear programming and local search algorithms are considered, the latter the same as in Kanungo et al. [2004], Arya et al. [2004], and both with polynomial complexity in the size of the k-medians instance.", "startOffset": 63, "endOffset": 1348}, {"referenceID": 3, "context": "Following Arthur and Vassilvitskii [2007], the case ` = 2 is referred to as the k-means++ algorithm and the probabilities used after the first iteration are referred to as D2 weighting (hence D` in general).", "startOffset": 10, "endOffset": 42}, {"referenceID": 3, "context": "3 Existing Lemmas Regarding D Sampling The following lemmas synthesize results from Arthur and Vassilvitskii [2007] that bound the expected potential within a single optimal cluster due to selecting a center from that cluster with uniform or D` weighting, as in Algorithm 1.", "startOffset": 84, "endOffset": 116}, {"referenceID": 3, "context": "[Arthur and Vassilvitskii, 2007, Lemma 3.2] Given an optimal cluster A and an initial potential \u03c6, let \u03c6\u2032 be the potential resulting from adding a cluster center selected randomly fromA withD` weighting. Then E[\u03c6\u2032(A)] \u2264 r D \u03c6\u2217(A) for any A, where r (`) D = 2 `r (`) u . The factor of 2` between r u and r (`) D for general ` is explained just before Theorem 5.1 in Arthur and Vassilvitskii [2007].", "startOffset": 1, "endOffset": 397}, {"referenceID": 3, "context": "1 Comparisons to Existing Results A comparison of Theorem 1 to results in Arthur and Vassilvitskii [2007] is implicit in its statement since the Hk\u22121 term in the minimum comes directly from Arthur and Vassilvitskii [2007, Theorems 3.", "startOffset": 74, "endOffset": 106}, {"referenceID": 3, "context": "1 Comparisons to Existing Results A comparison of Theorem 1 to results in Arthur and Vassilvitskii [2007] is implicit in its statement since the Hk\u22121 term in the minimum comes directly from Arthur and Vassilvitskii [2007, Theorems 3.1 and 5.1]. For k = 2, 3, the first term in the minimum is smaller than Hk\u22121 for any \u03b2 \u2265 1, and hence Theorem 1 is always an improvement. For k > 3, Theorem 1 improves upon Arthur and Vassilvitskii [2007] for \u03b2 greater than the critical value \u03b2c = 1 + \u03c6(k \u2212 2\u2212Hk\u22121) kHk\u22121 .", "startOffset": 74, "endOffset": 438}, {"referenceID": 2, "context": "It can be concluded that for any k, at most 20% oversampling is required for Theorem 1 to guarantee a better approximation than Arthur and Vassilvitskii [2007]. The most closely related result to Theorem 1 and Corollary 1 is found in Aggarwal et al.", "startOffset": 128, "endOffset": 160}, {"referenceID": 0, "context": "The most closely related result to Theorem 1 and Corollary 1 is found in Aggarwal et al. [2009, Theorem 1]. The latter establishes a constant-factor bi-criteria approximation that holds with constant probability, as opposed to in expectation. Since a bound on the expectation implies a bound with constant probability via Markov\u2019s inequality, a direct comparison with Aggarwal et al. [2009] is possible.", "startOffset": 73, "endOffset": 391}, {"referenceID": 0, "context": "The most closely related result to Theorem 1 and Corollary 1 is found in Aggarwal et al. [2009, Theorem 1]. The latter establishes a constant-factor bi-criteria approximation that holds with constant probability, as opposed to in expectation. Since a bound on the expectation implies a bound with constant probability via Markov\u2019s inequality, a direct comparison with Aggarwal et al. [2009] is possible. Specifically, for ` = 2 and the t = d16(k + \u221a k)e cluster centers assumed in Aggarwal et al. [2009], Theorem 1 in the present work implies that E[\u03c6] \u03c6\u2217 \u2264 8 ( 1 + min { \u03c6(k \u2212 2) d15k + 16 \u221a ke+ \u03c6 ,Hk\u22121 })", "startOffset": 73, "endOffset": 504}, {"referenceID": 0, "context": "03 as in Aggarwal et al. [2009]. This 9.", "startOffset": 9, "endOffset": 32}, {"referenceID": 0, "context": "03 as in Aggarwal et al. [2009]. This 9.137 approximation factor is less than half the factor of 20 in Aggarwal et al. [2009].", "startOffset": 9, "endOffset": 126}, {"referenceID": 25, "context": "Corollary 1 may also be compared to the results in Makarychev et al. [2015], although it should be re-emphasized that the latter analyzes different, substantially more complex algorithms, with running time at least nO(log(1/ )/ 2) for reasonably small .", "startOffset": 51, "endOffset": 76}, {"referenceID": 25, "context": "Corollary 1 may also be compared to the results in Makarychev et al. [2015], although it should be re-emphasized that the latter analyzes different, substantially more complex algorithms, with running time at least nO(log(1/ )/ 2) for reasonably small . The main difference between Corollary 1 and the bounds in Makarychev et al. [2015] is the extra factor of r D since the factor of 1 + \u03c6/(\u03b2 \u2212 1) is comparable, at least for moderate values of \u03b2 that are of practical interest.", "startOffset": 51, "endOffset": 337}, {"referenceID": 3, "context": "The overall strategy used to prove Theorem 1 is similar to that in Arthur and Vassilvitskii [2007]. The key intermediate result is Lemma 3 below, which relates the potential at a later iteration in Algorithm 1 to the potential at an earlier iteration.", "startOffset": 67, "endOffset": 99}, {"referenceID": 0, "context": "The contributions herein extend and improve upon previous results concerning D` sampling [Arthur and Vassilvitskii, 2007, Aggarwal et al., 2009]. As noted in Section 3, the constant r D in Theorem 1 and Corollary 1 represents an opportunity to further improve the approximation bounds. One possibility is to tighten Lemmas 3.2 and 5.1 in Arthur and Vassilvitskii [2007], which are the lemmas responsible for the r D factor.", "startOffset": 122, "endOffset": 370}], "year": 2016, "abstractText": "This paper studies the k-means++ algorithm for clustering as well as the class of D sampling algorithms to which k-means++ belongs. It is shown that for any constant factor \u03b2 > 1, selecting \u03b2k cluster centers by D sampling yields a constant-factor approximation to the optimal clustering with k centers, in expectation and without conditions on the dataset. This result extends the previously known O(log k) guarantee for the case \u03b2 = 1 to the constant-factor bi-criteria regime. It also improves upon an existing constant-factor bi-criteria result that holds only with constant probability.", "creator": "LaTeX with hyperref package"}}}