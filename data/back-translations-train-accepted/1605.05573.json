{"id": "1605.05573", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2016", "title": "Modelling Interaction of Sentence Pair with Coupled-LSTMs", "abstract": "Recently, there is rising interest in modelling the interactions of two sentences with deep neural networks. However, most of the existing methods encode two sequences with separate encoders, in which a sentence is encoded with little or no information from the other sentence. In this paper, we propose a deep architecture to model the strong interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architecture and its superiority to state-of-the-art methods.", "histories": [["v1", "Wed, 18 May 2016 13:33:21 GMT  (344kb,D)", "https://arxiv.org/abs/1605.05573v1", null], ["v2", "Fri, 20 May 2016 01:28:43 GMT  (317kb,D)", "http://arxiv.org/abs/1605.05573v2", "Submitted to IJCAI 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["pengfei liu", "xipeng qiu", "yaqian zhou", "jifan chen", "xuanjing huang"], "accepted": true, "id": "1605.05573"}, "pdf": {"name": "1605.05573.pdf", "metadata": {"source": "CRF", "title": "Modelling Interaction of Sentence Pair with Coupled-LSTMs", "authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang"], "emails": ["pfliu14@fudan.edu.cn", "xpqiu@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that we are able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution. \""}, {"heading": "2 Sentence Modelling with LSTM", "text": "Long-term memory network (LSTM) [Hochreiter and Schmidhuber, 1997] is a kind of recurrent neural network (RNN) [Elman, 1990] and deals specifically with the subject of learning long-term dependencies. LSTM maintains a memory cell that only updates and discloses its contents when deemed necessary. Although there are numerous LSTM variants, we are using the LSTM architecture used by [Jozefowicz et al., 2015], which is similar to the architecture of [Graves, 2013] but has no peephole connections. We do not define the LSTM units in each step of time as a collection of vectors in Rd: an entrance gate, a forgotten gate ft, an exit gate ot, a memory cell ct and a hidden state. d is the number of LSTM units that affect the gate vectors they, ft and ot, in [1]."}, {"heading": "3 Coupled-LSTMs for Strong Sentence Interaction", "text": "To deal with two sentences, a simple method is to model them with two separate LSTMs. However, this method is difficult to model local interactions of two sentences. An improved way is to introduce an attention mechanism that has been used for many tasks such as machine translation [Bahdanau et al., 2014] and answering questions [Hermann et al., 2015]. Inspired by the multidimensional recurring neural network [Graves et al., 2007; Graves and Schmidhuber, 2009; Byeon et al., 2015] and the grid LSTM [Kalchbrenner et al., 2015], we propose two models in the computer vision community to capture the interdependencies between two parallel LSTMs, called coupled LSTMs (C-LSTMs). To simplify our models, let us first give some definitions. In view of two sequences X = 1, x2, \u00b7 \u00b7 xtsn, and Y \u00b7 yhi \u00b7 1, let us present one sentence with two."}, {"heading": "3.1 Loosely Coupled-LSTMs (LC-LSTMs)", "text": "To model the local contextual interactions of two sentences, we allow the interdependence of two LSTMs at different positions. Inspired by Grid LSTM [Kalchbrenner et al., 2015] and word-for-word attention LSTMs [Rockta \ufffd schel et al., 2015], we propose a loose coupling model for two interdependent LSTMs. Specifically, we refer to h (1) i, j as the encoding of the sequence x0: i in the first LSTM, influenced by the output of the second LSTM to sequence y0: j. Meanwhile, h (2) i, j is the encoding of the subsequence y0 \u2212 j in the second LSTM, influenced by the output of the first LSTM to sequence x0: i (1) i, j and h (2) i \u2212 h [1 \u2212 j \u2212 H (1) i \u2212 1 (1), c (1) i (i), i (i), i (1), j (1) j (1), j (1), j (i (1) (1) (1), j (i (1)."}, {"heading": "3.2 Tightly Coupled-LSTMs (TC-LSTMs)", "text": "The hidden states of the LC-LSTMs are the combination of the hidden states of two interdependent LSTMs whose memory cells are separated. Inspired by the configuration of the multidimensional LSTM [Byeon et al., 2015], we continue to mix both the hidden states and the memory cells of two LSTMs. We assume that hi, j are directly modelling the interaction of the subsequences x0: i and y0: j, which depends on two previous interactions hi \u2212 1, j \u2212 1, j \u2212 1, where i, j are the positions in sets X and Y. We define a tightly coupled LSTM unit as follows."}, {"heading": "3.3 Analysis of Two Proposed Models", "text": "Our two proposed coupled LSTMs can be formulated as follows: (hi, j, ci, j, j) = C-LSTMs (hi \u2212 1, j, hi, j \u2212 1, ci \u2212 1, yj), (12) where C-LSTMs can be either TC-LSTMs or LC-LSTMs. Input consisted of two types of information in step (i, j) in coupled LSTMs: time dimension hi \u2212 1, j, hi, j \u2212 1, ci, j \u2212 1 and depth dimension xi, yj. The difference between TC-LSTMs and LC-LSTMs consists in the dependence on information from time and depth dimensions. Interaction between time dimensions The TCLSTMs model the interactions in position (i, j) by fixing the internal memory dimensions ci \u2212 1, j \u2212 1 and hidden states hi \u2212 1, j \u2212.STSTSTSTSTSTMs by unifying the interactions in position (j-LC) LSTSTMs."}, {"heading": "4 End-to-End Architecture for Sentence Matching", "text": "In this section, we present an end-to-end architecture to match two sentences, as shown in Figure 2."}, {"heading": "4.1 Embedding Layer", "text": "In order to model the sentences with the neural model, we must first transform the uniform representation of the word into the distributed representation. All words of two sequences X = x1, x2, \u00b7 \u00b7 \u00b7, xn and Y = y1, y2, \u00b7 \u00b7 \u00b7, ym are mapped into low-dimensional vector representations, which are assumed to be the input of the network."}, {"heading": "4.2 Stacked Coupled-LSTMs Layers", "text": "After the embedding layer, we use our proposed coupled-LSTMs to capture the strong interactions between two sets. A base block consists of five levels. To increase the learning ability of the coupled-LSTMs, we stack the base block on top of each other. Four directional coupled-LSTMs Layers The C-LSTMs are defined along a certain predefined direction, and we can extend them to the surrounding context in all directions. Similar to bidirectional LSTMs, there are four directions in coupled-LSTMs. (h1i, j, c 1 i, j) = C-LSTMs (hi \u2212 1, j \u2212 1, ci \u2212 1, j \u2212 1, y \u2212 C, yj)."}, {"heading": "4.3 Pooling Layer", "text": "The output of stacked coupled LSTM layers is a tensor H-Rn-m-d, where n and m are the length of records, and d is the number of hidden neurons. We apply dynamic pooling to automatically extract the Rp-q subsampling matrix in each disc Hi-Rn-m, similar to [Socher et al., 2011].More formally, for each disc matrix Hi, we subdivide the rows and columns of Hi into p-q grids, which are roughly the same. These grids do not overlap. Then, we select the maximum value within each grid. Since each disc Hi consists of the hidden states of a neuron at different positions, the pooling operation can be considered the most informative interaction captured by the neurons. Thus, we obtain a p-q-d tensor, which is further transformed into a vector."}, {"heading": "4.4 Fully-Connected Layer", "text": "The vector obtained by bundling the layer is fed into a complete connecting layer to obtain a final, more abstract representation."}, {"heading": "4.5 Output Layer", "text": "The output layer depends on the types of tasks, we choose the appropriate form of the output layer. There are two popular types of text matching tasks in the NLP. One is the order of the tasks, such as answering community questions. Another is the classification task, such as the text order. The order of precedence of the task is a scalar matching score, which is obtained by a linear transformation after the last fully connected lay.2. In the classification task, the results are the probabilities of the different classes, which are calculated by a Softmax function after the last fully connected layer."}, {"heading": "5 Training", "text": "Max-Margin Loss for Ranking Task Given a positive sentence pair (X, Y) and its corresponding negative pair (X, Y).The matching score s (X, Y) should be larger than s (X, Y, Y).For this task, we use the kontrastive max-margin criterion [Bordes et al., 2013; Socher et al., 2013] to train our models on matching task.The ranking-based loss is defined asL (X, Y, Y) = max (0, 1 \u2212 s (X, Y) + s (X, Y))). (14) where s (X, Y) is predicting matching score for (X, Y).Cross-entropy Loss for Classification Task Given a sentence pair (X, Y) and its label l's (X, Y)."}, {"heading": "6 Experiment", "text": "In this section, we examine the empirical performance of our proposed model using two different text matching tasks: classification task (identification of textual entanglements) and ranking task (matching of question and answer)."}, {"heading": "6.1 Hyperparameters and Training", "text": "Word embedding for all models is initialized with the 100d GloVe vectors (840B token version, [Pennington et al., 2014]) and refined during training to improve performance; the other parameters are initialized by random samples from an even distribution in [\u2212 0.1, 0.1] For each task, we take the hyperparameters that achieve the best performance on the development set via a small grid search using combinations of the initial learning rate [0.05, 0.0005, 0.0001], l2 regulation [0.0, 5E \u2212 5, 1E \u2212 6] and the threshold of the gradient standard [5, 10, 100]. The final hyperparameters are set as table 1."}, {"heading": "6.2 Competitor Methods", "text": "\u2022 Neural bag-of-words (NBOW): Each sequence as the sum of the embedding of the words contained therein, is then concatenated and fed to an MLP. \u2022 Single LSTM: A single LSTM to encode the two sequences used in [Rockta \ufffd schel et al., 2015]. \u2022 Parallel LSTMs: Two sequences are encoded separately by two LSTMs, then linked together and fed to an MLP. \u2022 Attention LSTMs: An attentive LSTM to encode two sentences into a semantic space used in [Rockta \ufffd schel et al., 2015]. \u2022 Word by word Attention LSTMs: Improving attention LSTM by introducing a word-to-word attention mechanism used in [Rockta \ufffd schel et al., 2015]."}, {"heading": "6.3 Experiment-I: Recognizing Textual Entailment", "text": "This year, the time has come for us to put ourselves in a position to take the lead, \"he told the German Press Agency."}, {"heading": "6.4 Experiment-II: Matching Question and Answer", "text": "Matching Question Answering (MQA) is a typical task for semantic matching. Faced with a question, we must select a correct answer from some candidate replies. In this thesis, we use the data set of Yahoo! Answers with the getByCategory function in Yahoo! Answers API, which produces 963, 072 questions and corresponding best answers. We then select the pairs in which the length of the questions and answers are both in the interval [4, 30], whereby 220,000 pairs of answers are obtained to form the positive pairings.For negative pairs, we first use the best answer of each question as a query to retrieve top 1,000 results from the entire answer set with Lucene, where 4 or 9 answers are randomly selected to construct the negative pairings.The entire data set is divided into training, validation and test data with the ratio 20: 1: 1. In addition, we give two test settings: Selecting the best answer of 5 and 10 candidates in relation to the STC results of QA are not significant."}, {"heading": "7 Related Work", "text": "An intuitive paradigm is to calculate similarities between all the words or phrases of the two sentences. Socher et al. [2011] initially used this paradigm for paraphrase recognition; the representation of words or phrases is learned on the basis of recursive autoencoders. Wan et al. [2016] used LSTM to improve the contextual interactions of words or phrases between two sentences. Entering LSTM for a sentence does not include another sentence. A major limitation of this paradigm is the interaction of two sentences by a predefined measure of similarity. Therefore, it is not easy to increase the depth of the network. Compared with this paradigm, we can stack our C-LSTM syndigms to model multiple granularity of the interactions of two sentences."}, {"heading": "8 Conclusion and Future Work", "text": "In this paper, we propose an end-to-end deep architecture to capture the strong interaction information of sentence pairs. Experiments in two large text comparison tasks demonstrate the effectiveness of our proposed model and its superiority over competing models. In addition, our visualization analysis has shown that multiple interpretable neurons in our proposed models can capture the contextual interactions of words or phrases. In future work, we would like to integrate some gating strategies into the depth dimension of our proposed models, such as highways or residual networks, in order to improve the interactions between depth and other dimensions and thus train deeper and more powerful neural networks."}], "references": [{"title": "ArXiv e-prints", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio. Neural machine translation by jointly learning to align", "translate"], "venue": "September", "citeRegEx": "Bahdanau et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "NIPS,", "citeRegEx": "Bordes et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bowman et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "author": ["Wonmin Byeon", "Thomas M Breuel", "Federico Raue", "Marcus Liwicki. Scene labeling with lstm recurrent neural networks"], "venue": "pages 3547\u20133555,", "citeRegEx": "Byeon et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "The Journal of Machine Learning Research", "author": ["John Duchi", "Elad Hazan", "Yoram Singer. Adaptive subgradient methods for online learning", "stochastic optimization"], "venue": "12:2121\u20132159,", "citeRegEx": "Duchi et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Cognitive science", "author": ["Jeffrey L Elman. Finding structure in time"], "venue": "14(2):179\u2013211,", "citeRegEx": "Elman. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Neural Networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber. Framewise phoneme classification with bidirectional lstm", "other neural network architectures"], "venue": "18(5):602\u2013610,", "citeRegEx": "Graves and Schmidhuber. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber. Offline handwriting recognition with multidimensional recurrent neural networks"], "venue": "pages 545\u2013552,", "citeRegEx": "Graves and Schmidhuber. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In Artificial Neural Networks\u2013ICANN 2007", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "J\u00fcrgen Schmidhuber. Multi-dimensional recurrent neural networks"], "venue": "pages 549\u2013558. Springer,", "citeRegEx": "Graves et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "author": ["Hua He", "Kevin Gimpel", "Jimmy Lin. Multiperspective sentence similarity modeling with convolutional neural networks"], "venue": "pages 1576\u20131586,", "citeRegEx": "He et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom. Teaching machines to read", "comprehend"], "venue": "pages 1684\u20131692,", "citeRegEx": "Hermann et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Hu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Jozefowicz et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "Proceedings of ACL,", "citeRegEx": "Kalchbrenner et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12:1532\u20131543,", "citeRegEx": "Pennington et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural tensor network architecture for community-based question answering", "author": ["Xipeng Qiu", "Xuanjing Huang"], "venue": "Proceedings of International Joint Conference on Artificial Intelligence,", "citeRegEx": "Qiu and Huang. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1509.06664,", "citeRegEx": "Rockt\u00e4schel et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on", "author": ["Mike Schuster", "Kuldip K Paliwal. Bidirectional recurrent neural networks. Signal Processing"], "venue": "45(11):2673\u20132681,", "citeRegEx": "Schuster and Paliwal. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng. Reasoning with neural tensor networks for knowledge base completion"], "venue": "pages 926\u2013934,", "citeRegEx": "Socher et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le. Sequence to sequence learning with neural networks"], "venue": "pages 3104\u20133112,", "citeRegEx": "Sutskever et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A deep architecture for semantic matching with multiple positional sentence representations", "author": ["Shengxian Wan", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Liang Pang", "Xueqi Cheng"], "venue": "AAAI,", "citeRegEx": "Wan et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze. Convolutional neural network for paraphrase identification"], "venue": "pages 901\u2013911,", "citeRegEx": "Yin and Sch\u00fctze. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou"], "venue": "arXiv preprint arXiv:1512.05193,", "citeRegEx": "Yin et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Distributed representations of words or sentences have been widely used in many natural language processing (NLP) tasks, such as text classification [Kalchbrenner et al., 2014], question answering and machine translation [Sutskever et al.", "startOffset": 149, "endOffset": 176}, {"referenceID": 23, "context": ", 2014], question answering and machine translation [Sutskever et al., 2014] and so on.", "startOffset": 52, "endOffset": 76}, {"referenceID": 13, "context": "Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses [Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016].", "startOffset": 136, "endOffset": 192}, {"referenceID": 18, "context": "Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses [Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016].", "startOffset": 136, "endOffset": 192}, {"referenceID": 24, "context": "Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses [Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016].", "startOffset": 136, "endOffset": 192}, {"referenceID": 13, "context": "Weak interaction Models Some early works focus on sentence level interactions, such as ARC-I[Hu et al., 2014], CNTN[Qiu and Huang, 2015] and so on.", "startOffset": 92, "endOffset": 109}, {"referenceID": 18, "context": ", 2014], CNTN[Qiu and Huang, 2015] and so on.", "startOffset": 13, "endOffset": 34}, {"referenceID": 25, "context": "Semi-interaction Models Some improved methods focus on utilizing multi-granularity representation (word, phrase and sentence level), such as MultiGranCNN [Yin and Sch\u00fctze, 2015] and Multi-Perspective CNN [He et al.", "startOffset": 154, "endOffset": 177}, {"referenceID": 10, "context": "Semi-interaction Models Some improved methods focus on utilizing multi-granularity representation (word, phrase and sentence level), such as MultiGranCNN [Yin and Sch\u00fctze, 2015] and Multi-Perspective CNN [He et al., 2015].", "startOffset": 204, "endOffset": 221}, {"referenceID": 26, "context": "Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence, such as ABCNN [Yin et al., 2015], Attention LSTM[Rockt\u00e4schel et al.", "startOffset": 163, "endOffset": 181}, {"referenceID": 19, "context": ", 2015], Attention LSTM[Rockt\u00e4schel et al., 2015; Hermann et al., 2015].", "startOffset": 23, "endOffset": 71}, {"referenceID": 11, "context": ", 2015], Attention LSTM[Rockt\u00e4schel et al., 2015; Hermann et al., 2015].", "startOffset": 23, "endOffset": 71}, {"referenceID": 13, "context": "ARC-II [Hu et al., 2014] and MV-LSTM [Wan et al.", "startOffset": 7, "endOffset": 24}, {"referenceID": 24, "context": ", 2014] and MV-LSTM [Wan et al., 2016].", "startOffset": 20, "endOffset": 38}, {"referenceID": 20, "context": "Similar to bidirectional LSTM for single sentence [Schuster and Paliwal, 1997; Graves and Schmidhuber, 2005], there are four directions can be used in coupled-LSTMs.", "startOffset": 50, "endOffset": 108}, {"referenceID": 6, "context": "Similar to bidirectional LSTM for single sentence [Schuster and Paliwal, 1997; Graves and Schmidhuber, 2005], there are four directions can be used in coupled-LSTMs.", "startOffset": 50, "endOffset": 108}, {"referenceID": 12, "context": "Long short-term memory network (LSTM) [Hochreiter and Schmidhuber, 1997] is a type of recurrent neural network (RNN) [Elman, 1990], and specifically addresses the issue of learning long-term dependencies.", "startOffset": 38, "endOffset": 72}, {"referenceID": 5, "context": "Long short-term memory network (LSTM) [Hochreiter and Schmidhuber, 1997] is a type of recurrent neural network (RNN) [Elman, 1990], and specifically addresses the issue of learning long-term dependencies.", "startOffset": 117, "endOffset": 130}, {"referenceID": 14, "context": "While there are numerous LSTM variants, here we use the LSTM architecture used by [Jozefowicz et al., 2015], which is similar to the architecture of [Graves, 2013] but without peep-hole connections.", "startOffset": 82, "endOffset": 107}, {"referenceID": 9, "context": ", 2015], which is similar to the architecture of [Graves, 2013] but without peep-hole connections.", "startOffset": 49, "endOffset": 63}, {"referenceID": 0, "context": "been used in many tasks, such as machine translation [Bahdanau et al., 2014] and question answering [Hermann et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 11, "context": ", 2014] and question answering [Hermann et al., 2015].", "startOffset": 31, "endOffset": 53}, {"referenceID": 8, "context": "Inspired by the multi-dimensional recurrent neural network [Graves et al., 2007; Graves and Schmidhuber, 2009; Byeon et al., 2015] and grid LSTM [Kalchbrenner et al.", "startOffset": 59, "endOffset": 130}, {"referenceID": 7, "context": "Inspired by the multi-dimensional recurrent neural network [Graves et al., 2007; Graves and Schmidhuber, 2009; Byeon et al., 2015] and grid LSTM [Kalchbrenner et al.", "startOffset": 59, "endOffset": 130}, {"referenceID": 3, "context": "Inspired by the multi-dimensional recurrent neural network [Graves et al., 2007; Graves and Schmidhuber, 2009; Byeon et al., 2015] and grid LSTM [Kalchbrenner et al.", "startOffset": 59, "endOffset": 130}, {"referenceID": 16, "context": ", 2015] and grid LSTM [Kalchbrenner et al., 2015] in computer vision community, we propose two models to capture the interdependences between two parallel LSTMs, called coupled-LSTMs (C-LSTMs).", "startOffset": 22, "endOffset": 49}, {"referenceID": 16, "context": "Inspired by Grid LSTM [Kalchbrenner et al., 2015] and word-by-word attention LSTMs [Rockt\u00e4schel et al.", "startOffset": 22, "endOffset": 49}, {"referenceID": 19, "context": ", 2015] and word-by-word attention LSTMs [Rockt\u00e4schel et al., 2015], we propose a loosely coupling model for two interdependent LSTMs.", "startOffset": 41, "endOffset": 67}, {"referenceID": 3, "context": "Inspired by the configuration of the multidimensional LSTM [Byeon et al., 2015], we further conflate both the hidden states and the memory cells of two LSTMs.", "startOffset": 59, "endOffset": 79}, {"referenceID": 21, "context": "We apply dynamic pooling to automatically extract Rp\u00d7q subsampling matrix in each slice Hi \u2208 Rn\u00d7m, similar to [Socher et al., 2011].", "startOffset": 110, "endOffset": 131}, {"referenceID": 1, "context": "For this task, we use the contrastive max-margin criterion [Bordes et al., 2013; Socher et al., 2013] to train our models on matching task.", "startOffset": 59, "endOffset": 101}, {"referenceID": 22, "context": "For this task, we use the contrastive max-margin criterion [Bordes et al., 2013; Socher et al., 2013] to train our models on matching task.", "startOffset": 59, "endOffset": 101}, {"referenceID": 4, "context": "To minimize the objective, we use stochastic gradient descent with the diagonal variant of AdaGrad [Duchi et al., 2011].", "startOffset": 99, "endOffset": 119}, {"referenceID": 9, "context": "To prevent exploding gradients, we perform gradient clipping by scaling the gradient when the norm exceeds a threshold [Graves, 2013].", "startOffset": 119, "endOffset": 133}, {"referenceID": 17, "context": "The word embeddings for all of the models are initialized with the 100d GloVe vectors (840B token version, [Pennington et al., 2014]) and fine-tuned during training to improve the performance.", "startOffset": 107, "endOffset": 132}, {"referenceID": 19, "context": "1 single LSTM [Rockt\u00e4schel et al., 2015] 100 111K 83.", "startOffset": 14, "endOffset": 40}, {"referenceID": 2, "context": "9 parallel LSTMs [Bowman et al., 2015] 100 221K 84.", "startOffset": 17, "endOffset": 38}, {"referenceID": 19, "context": "Attention LSTM [Rockt\u00e4schel et al., 2015] 100 252K 83.", "startOffset": 15, "endOffset": 41}, {"referenceID": 19, "context": "3 Attention(w-by-w) LSTM [Rockt\u00e4schel et al., 2015] 100 252K 83.", "startOffset": 25, "endOffset": 51}, {"referenceID": 19, "context": "\u2022 Single LSTM: A single LSTM to encode the two sequences, which is used in [Rockt\u00e4schel et al., 2015].", "startOffset": 75, "endOffset": 101}, {"referenceID": 19, "context": "\u2022 Attention LSTMs: An attentive LSTM to encode two sentences into a semantic space, which used in [Rockt\u00e4schel et al., 2015].", "startOffset": 98, "endOffset": 124}, {"referenceID": 19, "context": "\u2022 Word-by-word Attention LSTMs: An improvement of attention LSTM by introducing word-by-word attention mechanism, which used in [Rockt\u00e4schel et al., 2015].", "startOffset": 128, "endOffset": 154}, {"referenceID": 2, "context": "We use the Stanford Natural Language Inference Corpus (SNLI) [Bowman et al., 2015].", "startOffset": 61, "endOffset": 82}], "year": 2016, "abstractText": "Recently, there is rising interest in modelling the interactions of two sentences with deep neural networks. However, most of the existing methods encode two sequences with separate encoders, in which a sentence is encoded with little or no information from the other sentence. In this paper, we propose a deep architecture to model the strong interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architecture and its superiority to state-ofthe-art methods.", "creator": "LaTeX with hyperref package"}}}