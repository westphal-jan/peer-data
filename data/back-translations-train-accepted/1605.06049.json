{"id": "1605.06049", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "A Multi-Batch L-BFGS Method for Machine Learning", "abstract": "The question of how to parallelize the stochastic gradient descent (SGD) method has received much attention in the literature. In this paper, we focus instead on batch methods that use a sizeable fraction of the training set at each iteration to facilitate parallelism, and that employ second-order information. In order to improve the learning process, we follow a multi-batch approach in which the batch changes at each iteration. This inherently gives the algorithm a stochastic flavor that can cause instability in L-BFGS, a popular batch method in machine learning. These difficulties arise because L-BFGS employs gradient differences to update the Hessian approximations; when these gradients are computed using different data points the process can be unstable. This paper shows how to perform stable quasi-Newton updating in the multi-batch setting, illustrates the behavior of the algorithm in a distributed computing platform, and studies its convergence properties for both the convex and nonconvex cases.", "histories": [["v1", "Thu, 19 May 2016 16:53:50 GMT  (1382kb,D)", "http://arxiv.org/abs/1605.06049v1", "32 pages, 22 figures"], ["v2", "Sun, 23 Oct 2016 22:48:01 GMT  (1390kb,D)", "http://arxiv.org/abs/1605.06049v2", "NIPS 2016. 31 pages, 22 figures"]], "COMMENTS": "32 pages, 22 figures", "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["albert s berahas", "jorge nocedal", "martin tak\u00e1c"], "accepted": true, "id": "1605.06049"}, "pdf": {"name": "1605.06049.pdf", "metadata": {"source": "CRF", "title": "A Multi-Batch L-BFGS Method for Machine Learning", "authors": ["Albert S. Berahas"], "emails": ["albertberahas@u.northwestern.edu", "j-nocedal@northwestern.edu", "takac.mt@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 The Robust Quasi-Newton Method", "text": "In fact, it is the case that it is a matter of a way in which people in the most different life situations find themselves in the most different life situations. (...) It is as if people in the most different life situations and life situations interact with each other. (...) It is as if people in the most different life situations would live in the most different life situations. (...) It is as if people in the most different life situations would live in the most different life situations. (...) It is as if people in the most different life situations would live in the most different life situations. \"(...) It is as if they live in the most different life situations.\" (...) It is as if people live in the most different life situations in the most different life situations, as if they are in the most different life situations. \"It is as if they live in the most different life situations, if they live in the most different life situations.\""}, {"heading": "2.1 Specification of the Method", "text": "For k-th iteration, the multi-batch BFGS algorithm selects a set Sk-1,.., n and calculates a new iterate according to the formula + 1 = wk \u2212 \u03b1kHkgSkk, (2,4) where \u03b1k is the step length, gSkk is the batch gradient (2,2) and Hk is the inverse BFGS-Hessian matrix approximation, which is the initial memory (2,2) for each iteration using the formula Hk + 1 = V k HkVk + 2. To calculate the correction vectors (sk, yk), we determine the overlap setOk = Sk + 1 consisting of the samples that are common on k-th and k-st iterations."}, {"heading": "2.2 Sample Generation", "text": "This year it is so far that it is not even a year away from being able to find a solution."}, {"heading": "3 Convergence Analysis", "text": "In this section, we analyze the convergence properties of the multi-batch L-BFGS method when applied to minimizing strongly convex and non-convex objective functions, using a fixed step length strategy. We assume that the goal is to minimize the empirical risk F stated in (1.1), but note that a similar analysis could be used to investigate minimizing the expected risk R."}, {"heading": "3.1 Strongly Convex case", "text": "Due to the stochastic nature of the multi-batch approach, each iteration of algorithm 1 uses a gradient that contains errors that do not converge to zero. Therefore, with a fixed step length strategy, one cannot determine convergence to the optimal solution w?, but only convergence to a neighborhood of w? [16]. Nevertheless, this result is of interest as it reflects common practice to use a fixed step length and only reduce it if the desired test error has not been achieved. It also illustrates the compromises that arise between the size of the batch and the step length. In our analysis, we make the following (fairly standard) assumptions about the objective function and algorithm."}, {"heading": "Assumptions A.", "text": "1. F (w) is continuously differentiable twice. 2. There are positive constants. 3. There is a constant. 3. There is a constant. 3. There is a constant. 3. There is a constant. 3. There is a constant. 3. There is a constant. 3. There is a constant. 4. The samples. 4. There is an unbiased estimate. 5. F (w) is an unbiased estimator of the gradients. 6. F (w)."}, {"heading": "3.2 Nonconvex case", "text": "Even with the L-BFGS method, which performs only a small number of updates with each iteration, it is not possible to guarantee that the Hessian approximations have eigenvalues that are uniformly limited above and away from zero. In order to achieve the convergence of the BFGS method in non-convex cases, careful updating procedures have been proposed [12]. Here, we apply a strategy that is well suited to our special algorithm: We skip the updating, i.e. set Hk + 1 = Hk if the curvature condition Tk sk = sk 2 (3.5) is not met, where > 0 is a given constant. Using this mechanism guarantees that the eigenvalues of the Hessian matrix approximations generated by the multi-batch L-BFGS method are limited above and away from zero (Lemma 3.3). The analysis presented in this section is based on the following assumptions."}, {"heading": "Assumptions B.", "text": "1. F (w) is continuously differentiable twice. 2. The gradients of F are increasingly constant and the gradients of FO are inversely O-Lipschitz continuous, for all W-Rd, and all propositions O-Rd, 2,.., n) of cardinality o100 r-n100. 3. The function F (w) is limited below a scalar F-Rd. \u2212 \u2212 4. There are constants nothing. n) of cardinality r-n100. 5. The samples S (w) are drawn independently, and the samples FS (w) is an unbiased estimator of the true gradient F (w). \u2212 2\u00b5m of cardinality r-n100. 5. The samples S (w) are drawn independently, and the samples FS (w) is an unbiased estimator of the actual gradient F (w)."}, {"heading": "4 Numerical Results", "text": "This year it is more than ever before."}, {"heading": "5 Conclusion", "text": "This paper describes a novel variant of the L-BFGS method, which is robust and efficient in two respects: the first occurs when node errors occur in a distributed computer implementation; the second occurs when you want to use a different batch for each iteration to accelerate learning; the proposed method avoids the pitfalls of using volatile gradient differences by performing quasi-Newton updates based on the overlap between successive samples; the numerical results show that the method is efficient in practice; and a convergence analysis demonstrates its theoretical properties."}, {"heading": "A Proofs and Technical Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Assumptions", "text": "We will first repeat the assumptions we use in the Convergence Analysis section (Section 3). Assumptions A and B are used in strongly convex and non-convex cases, respectively."}, {"heading": "Assumptions A", "text": "1. F (w) is to be differentiated twice continuously.2. There are positive constants \u03bb (w) and \u0432 (n) of cardinality o100 r \u00b7 n100. 3. There is a constant \u03b3 (w), (A.8) for all w) Rd and all propositions O (1, 2,..., n) of cardinality o100 r \u00b7 n100. 3. There is a constant \u03b3, so that ES [b) FS (w) 2 \u2264 2, (A.9) for all w) Rd and all batches S (1, 2,..., n) of cardinality r \u00b7 n100.4. Samples S are drawn independently, and \u0445FS (w) is an unbiased estimator of the true gradient. F (w) for all w) Rd, i.e. E [c) FS (w) = F (w). (A.10)."}, {"heading": "Assumptions B", "text": "(W) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n (K) n n (K) n n (K) n (K) n (K) n (K) n n (K) n n n (K) n n n n (K) n n n (K) n n (K) n n n n (K) n n n n n (K) n n n n (K n (n (n) n (K n (n (n) n (K n (n (K n (K n (n) n (K n (n (n) n (K n (K n (n (K n (n) n (K n (n (n) n (K n (n (n) n (K n (n (n (n) n) n (K n (n (n) n (K n (n (n) n (n) n) n (K n (n (n (n) n (n) n) n (K n (n (n (n) n (n (n) n (n) n (n (n (n (n) n (n (K n (n (n) n (n (n (n (n) n) n (n (n) n (n (n) n) n (k n (n (n (n) n (n) n (n) n) n (k n (n (n (n) n) n (k n (n (n) n) n (k n (n) n) n (K n (n) n (K n (n) n n (n (n) n (K n (n (n) n) n) n (K n) n) n) n) n (K n (K n n (n (n) n) n) n (K n) n n) n (K n n n (n (n) n) n) n n (K n n n (K n"}, {"heading": "B Extended Numerical Experiments - Real Datasets", "text": "In this section, we present further numerical results on the data sets listed in Table 1, both in the settings for multiple batches and in the fault tolerant settings. Note that some of the data sets are too small, so there is no reason to run them on a distributed platform; however, we include them because they are part of the standard benchmarking datasets.Note: Let us specify the number of training samples in a given dataset, d the dimension of the parameter vector w, and K the number of MPI processes used. The parameter r denotes the percentage of samples in the dataset that are used to define the gradient, i.e. | S | = r \u00b7 n100. The parameter o denotes the length of overlaps between successive samples and is defined as a percentage of the number of samples in a given lot S, i.e., | O | = o \u00b7 | S | 100. We focus on logistic regression classification; the target function is min."}, {"heading": "B.1 Multi-batch L-BFGS Implementation", "text": "For the experiments in this section, we performed four methods: \u2022 Robust L-BFGS (algorithm 1), \u2022 (L-BFGS) Multi-batch L-BFGS without sample consistency; here, gradient differences are calculated on the basis of different samples, i.e., yk = g Sk + 1 k + 1 \u2212 gSkk, \u2022 (Gradient Descent) Multi-batch gradient descent; which results by specifying Hk = I in algorithm 1, \u2022 (SGD) serial SGD values; one sample is used for each iteration to calculate the gradient. Figures 5-13 show the evolution of Hk = F (w) for different step lengths \u03b1 and for different batches (r%) and overlaps (o%) sizes. For robust L-BFGS, L-BFGS, gradient descent and SD for non-robust BGS sizes:"}, {"heading": "B.2 Fault-tolerant L-BFGS Implementation", "text": "When we execute a distributed algorithm, for example, on a shared computer cluster, we can experience delays. Such delays can be caused by other processes running on the same computing node, node failures, and other reasons. As a result, we perform a simple benchmark MPI code for two different environments: \u2022 Amazon EC2 - Amazon EC2 is a cloud system provided by Amazon. It is expected that if load balancing is done properly, execution time will have little noise; however, the network and communication can still be a problem. (4 MPI processes) Shared Cluster - In our shared cluster, multiple jobs run on each node, with some jobs more demanding than others. Although each node has 16 cores, the amount of resources each job can use over time."}, {"heading": "C Scaling of Robust L-BFGS Implementation", "text": "In this section, we examine the strong and weak scaling properties of the robust L-BFGS method on artificial data. For different values of r and K, we measure the time required to calculate a gradient (gradient) and the time required to calculate and communicate the gradient (gradient + C), as well as the time required to calculate the L-BFGS direction (L-BFGS) and the associated communication effort (L-BFGS + C)."}, {"heading": "C.1 Strong Scaling", "text": "Figure 20 shows the strong scaling properties of our proposed algorithm. We created a dataset with n = 107 and d = 104, each sample having 160 non-zero elements (dataset size 24 GB). We used our code for different values of r (different batch sizes Sk) with K = 1, 2,... 128 number of MPI processes. It can be observed that the calculation time for the gradient and the L-BFGS direction decreases when K is increased. However, taking into account the communication time, the combined costs increase slightly with K. Note that even at r = 10% (i.e. we process 10% of all samples in one iteration) the amount of local work is not sufficient to overcome the communication costs."}, {"heading": "C.2 Weak Scaling - Fixed Problem Dimension, Increasing Data Size", "text": "To illustrate the weak scaling properties of the algorithm, we created a data matrix X-R107 \u00b7 104 and ran it on a common cluster with K = 1, 2, 4, 8,.. 128 MPI processes. For a given number of MPI processes (K), each sample had 10 \u00b7 K non-zero elements. Effectively, the dimension of the problem was determined, but the sparseness of the data increased the more MPI processes were used. Input size was 1.5 \u00b7 K GB (i.e. 1.5 GB per MPI process); for K = 128 we generated 192 GB of data.The computation time for the gradient is almost constant, this is because the workload per MPI process (rank) is almost identical; see Figure 21. On the other hand, because we use a VectorFree L-BFGS implementation."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>The question of how to parallelize the stochastic gradient descent (SGD) method<lb>has received much attention in the literature. In this paper, we focus instead on<lb>batch methods that use a sizeable fraction of the training set at each iteration to<lb>facilitate parallelism, and that employ second-order information. In order to im-<lb>prove the learning process, we follow a multi-batch approach in which the batch<lb>changes at each iteration. This inherently gives the algorithm a stochastic flavor<lb>that can cause instability in L-BFGS, a popular batch method in machine learning.<lb>These difficulties arise because L-BFGS employs gradient differences to update<lb>the Hessian approximations; when these gradients are computed using different<lb>data points the process can be unstable. This paper shows how to perform stable<lb>quasi-Newton updating in the multi-batch setting, illustrates the behavior of the<lb>algorithm in a distributed computing platform, and studies its convergence proper-<lb>ties for both the convex and nonconvex cases.", "creator": "TeX"}}}