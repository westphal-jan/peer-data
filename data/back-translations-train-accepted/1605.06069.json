{"id": "1605.06069", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues", "abstract": "Sequential data often possesses a hierarchical structure with complex dependencies between subsequences, such as found between the utterances in a dialogue. In an effort to model this kind of generative process, we propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with recent neural network architectures. We evaluate the model performance through automatic evaluation metrics and by carrying out a human evaluation. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate the generation of long outputs and maintain the context.", "histories": [["v1", "Thu, 19 May 2016 17:59:02 GMT  (786kb,D)", "http://arxiv.org/abs/1605.06069v1", "14 pages, 5 tables, 3 figures"], ["v2", "Fri, 20 May 2016 16:02:30 GMT  (786kb,D)", "http://arxiv.org/abs/1605.06069v2", "15 pages, 5 tables, 3 figures"], ["v3", "Tue, 14 Jun 2016 02:21:04 GMT  (905kb,D)", "http://arxiv.org/abs/1605.06069v3", "15 pages, 5 tables, 4 figures"]], "COMMENTS": "14 pages, 5 tables, 3 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["iulian vlad serban", "alessandro sordoni", "ryan lowe", "laurent charlin", "joelle pineau", "aaron c courville", "yoshua bengio"], "accepted": true, "id": "1605.06069"}, "pdf": {"name": "1605.06069.pdf", "metadata": {"source": "CRF", "title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues", "authors": ["Iulian V. Serban", "Alessandro Sordoni", "Ryan Lowe", "Yoshua Bengio"], "emails": ["iulian.vlad.serban@umontreal.ca", "alessandro.sordoni@umontreal.ca", "aaron.courville@umontreal.ca", "yoshua.bengio@umontreal.ca", "ryan.lowe@cs.mcgill.ca", "lcharlin@cs.mcgill.ca", "jpineau@cs.mcgill.ca"], "sections": [{"heading": "1 Introduction", "text": "Deep recurrent neural networks (RNNs) have recently shown impressive results on a range of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modeling [10, 18] machine translation [28, 5], dialogue [27, 24], and speech recognition [9]. While these advances are impressive, the underlying RNNs tend to have a relatively simple structure, in the sense that the only variability or stochasticity in the model occurs when an output is sampled. This is often an inappropriate place to inject variability [2, 6, 1] especially for sequential data such as language and natural language, which have a hierarchical generation process with complex intra-sequence dependencies. For example, dialogue in natural language involves at least two levels of structure; within a single expression, the structure is dominated by local statistics of language, while there is a distinct source (uncertainty)."}, {"heading": "2 Technical Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Recurrent Neural Network Language Model", "text": "A recursive neural network (RNN), with parameters \u03b8, models a variable sequence of characters (w1,.., wM) by dividing the probability distribution over outputs: P\u03b8 (w1,..., wM) = M-m = 2 P\u03b8 (wm-w1,..., wm-1) P (w1). (1) The model processes each observation recursively. At each time step, the model observes an element and updates its internal hidden state, hm = f (hm-1, wm), where f is a parameterized nonlinear function, such as the hyperbolic tangent, the LSTM gating unit [12] or the GRU gating unit [5].2 The hidden state behaves like a sufficient statistic summarizing the past sequence and parameterizing the output distribution of the model."}, {"heading": "2.2 Hierarchical Recurrent Encoder-Decoder", "text": "The hierarchically recurring encoder decoder model (HRED) [26, 24] is an extension of the RNNLM. It extends the encoder decoder architecture [5] to the natural dialog setting. The HRED model assumes that each output sequence can be modeled in a two-step hierarchy: sequences of partial sequences and partial sequences of tokens. For example, a dialogue can be modeled as a sequence of utterances (partial sequences), whereby each utterance can be modeled as a sequence of words. Similarly, a natural language document can be modeled as a sequence of sentences (partial sequences), whereby each sentence can be modeled as a sequence of words. The HRED model consists of three RNN modules: an RNN encoder, a RNN context, and a RNN decoder RNN. Each partial sequence of tokens is deterministic into a real vector that encodes the NN context containing the internal NN-N context."}, {"heading": "2.3 A Deficient Generation Process", "text": "In recent literature, it has been observed that the RNNLM and HRED, and similar models based on RNN architectures, have critical problems that lead to meaningful expressions of dialogue [24, 15]. We believe that the root cause of these problems results from the parameterization of production distribution in the RNLM and HRED, which imposes a strong constraint on the generational process: the only source of variation is modelled by the conditional distribution of production, which is harmful from two perspectives: from a probabilistic perspective, with stochastic variations injected only at a low level, the model is encouraged to grasp the local structure in the sequence, rather than the global or long-term structure. This is because random variations at the lower level are severely limited in order to be consistent with the immediate previous observations, but are only weakly able to be consistent with older observations or with future observations."}, {"heading": "3 Latent Variable Hierarchical Recurrent Encoder-Decoder (VHRED)", "text": "This model expands the HRED model with a latent variable on the decoder, which is characterized by maximizing a lower variable limit on the logarithmic probability. This allows to model the hierarchically structured sequences in a two-step generation process, in which the latent variable is first captured and then the output sequence is generated - while maintaining the long-term context relationship. Let w1,.., wN be a sequence consisting of N sub-sequences in which wn = (wn, 1,., Mn) is then'th sub-sequence and wn, m-V is the m'th discrete in this sequence. The VHRED model uses a stochastic latent variable for each sub-sequence."}, {"heading": "4 Experimental Evaluation", "text": "This is an interesting problem for applications in areas such as customer service, technical support, language acquisition and entertainment. [29] It is also a task that requires the creation of sequences with complex structures taking into account the long-term context. We consider two tasks. For each task, the model is given a dialogue context consisting of one or more utterances, and the goal of the model is to generate an appropriate next response to the dialogue. We first conduct experiments on a Twitter dialogue corpus [22]. The task is to generate utterances that are attached to existing Twitter conversations. The data set is extracted using a process similar to Ritter et al. [22] and is divided into training, validation and test sentences, each containing 749, 060, 93, 633 and 10,000 dialogues. Each dialogue contains 6.27 utterances and 94.16 tokens. The dialogs are relatively long compared to the most recent large-scale language models, which include a single word coruscating channel of 927, and a single word coruscating channel of 4.16."}, {"heading": "4.1 Training and Evaluation Procedures", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "4.2 Results of Human Evaluation", "text": "The results (Table 1) show that VHRED is clearly preferred in most experiments. In particular, VHRED is preferred for both short and long contexts over the HRED and TF-IDF baseline models. VHRED is also preferred over the LSTM baseline model for long contexts, but LSTM is preferred over VHRED for short contexts."}, {"heading": "Opponent Wins Losses Ties Wins Losses Ties", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Context Response", "text": "Because it does not model the hierarchical input structure, the LSTM model has a short-term memory and therefore needs to output a response based primarily on the end of the last utterance. Such \"safe\" answers are reasonable for a broader range of contexts, meaning that human evaluators tend to rate them as appropriate. However, we argue that a model that returns only generic answers is undesirable for dialogue, as it leads to less interesting and less engaged conversations. Conversely, the VHRED model is explicitly designed for long contexts and to output a variety of responses based on the sample of latent variables. Therefore, the VHRED model generates longer sentences with more semantic content than the LSTM model (see Tables 3-4). This can be more \"risky\" because longer utterances contain rather small errors that lead to a lower human preference for a single utterance."}, {"heading": "Model Average Greedy Extrema Average Greedy Extrema 1-turn", "text": "These examples show quantitatively that the VHRED model learns to generate longer answers with more information content that are semantically similar to the context and the ground truth response."}, {"heading": "4.3 Results of Metric-based Evaluation", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5 Related Work", "text": "The use of a stochastic latent variable, learned by maximizing a variable lower limit, is inspired by the variational autoencoder (VAE) [14, 21]. Such models were mainly used to generate images in the continuous range [11]. However, there has also been recent work using these architectures to generate sequences, such as the Variational Recurrent Neural Networks (VRNN) [6], which were used for language and handwriting synthesis, and the Stochastic Recurrent Networks (STORN) [1], which were used for music generation and motion capture modeling. Both the VRNN and STORN integrate stochastic latent variables in RNN architectures, but unlike the VHRED they sample a separate latent variable in each time step of the decoder. This makes the hierarchical structure in the data unusable and thus models higher hierarchical ones."}, {"heading": "6 Discussion", "text": "We have applied the proposed model to the difficult task of generating dialog answers and have shown that it is an improvement over previous models in several respects, including the quality of responses as measured in a human study. Empirical results highlight the advantages of the hierarchical generation process for modeling high entropy sequences. Finally, it is not worth noting that the proposed model is very general. It can be applied in principle to any sequential generation task that has a hierarchical structure, such as document-level machine translation, web query prediction, multi-sentence document summary, multi-sentence captions, etc. Our initial experiments confirmed that the deterministic link between the RNN context and the RNN decoder was actually beneficial for reducing variation."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Dataset Details", "text": "Our Twitter Dialogue Corpus was extracted in 2011. We perform minimal pre-processing of the data set to remove irregular punctuation marks and tokenize it with the Moses tokenizer: https: / / github.com / moses-smt / mosesdecoder / blob / master / scripts / tokenizer / tokenizer.perl.We use the Ubuntu Dialogue Corpus v2.0, which was extracted in January 2016 from: http: / / cs.mcgill.ca / ~ jpineau / datasets / ubuntu-corpus-1.0 /. The pre-edited version of the data set will be made available to the public."}, {"heading": "6.2 Model Details", "text": "The implementations of the model will be made available to the public after the adoption of the paper."}, {"heading": "Training and Generation", "text": "We validate each model on the entire validation set every 5000 training parts. As mentioned in the main text, we use the 5-beam beam search at test time to output answers with the RNN decoders [10]. We define the beam cost as the log probability of tokens in the beam divided by the number of tokens it contains. This is a well-known modification that is often used in machine translation models. In principle, we could take samples from the RNN decoders of all models, but it is known that such a sample produces poor results compared to the beam search. It also leads to additional deviations in the evaluation process, which will make the human study very expensive or even impossible within a limited budget."}, {"heading": "Baseline Models", "text": "On Ubuntu, the gating function between the context RNN and the decoder RNN is a single-layer forward-facing neural network with hyperbolic tangent activation. On Twitter, the HRED model encoder RNN is a bidirectional GRU RNN encoder where the forward and backward RNN each have 1000 hidden units, and the context RNN and the decoder RNN each have 1000 hidden units. On Twitter, the decoder RNN calculates a 1000-dimensional real vector for each hidden time step, which is multiplied by the output context RNN. The result is fed by a single-layer forward-facing neural network with hyperbolic tangent activation, which the decoder RNN then uses as input. We found that this worked slightly better for the HRED baseline, but a more careful selection of the hyperbolic parameter probably makes this additional step redundant."}, {"heading": "Context Response", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.3 Model Examples", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.4 Human Study on Amazon Mechanical Turk", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Setup", "text": "We choose to use crowdsourcing platforms such as AMT rather than conducting experiments in the laboratory, even though experiments in the laboratory usually generate less noise and lead to greater agreement between human commentators. We do this because AMT experiments include a larger and more heterogeneous pool of commentators, which implies fewer cultural and geographical distortions, and because such experiments are easier to replicate, which we consider important for benchmarking future research on these tasks. It is important to allow AMT evaluators not to assign preferences for both answers, as there are several reasons why people do not understand the dialogue context, which includes topics they are not familiar with, slang language and non-English language. However, unlike the original Turing test, we only ask human evaluators to consider the next utterance in a given part as \"indeterminate.\" The evaluation structure is similar to the classical Turing test, where human judge-human conversations must be distinguished from human conversations."}, {"heading": "Selection Process", "text": "At the beginning of each experiment, we briefly introduce the human evaluator to the task and show him a simple example of a dialog context and two possible answers. To avoid bias in the presentation, we mix the order of examples and the order of potential answers for each example. During each experiment, we also show four trivial \"attention test examples,\" which any human evaluator who has understood the task should be able to correctly answer. We discard responses from human evaluators who fail more than one of these checks. We select from the test set the examples that human evaluators are randomly shown. We filter out all non-English conversations and conversations with offensive content. We do this automatically by filtering out all conversations with non-English characters and conversations with professors, swear words and otherwise offensive content. This filtering is not perfect, so we manually skim out many conversations and filter out conversations with non-English languages and offensive content."}, {"heading": "Execution", "text": "For each model pair, we perform 3 \u2212 5 Human Intelligence Tests (HITs) on AMT. Each HIT contains 70 \u2212 90 examples (dialogue context and two model reactions) and is evaluated by 3 \u2212 4 unique people. In total, we collect 5363 preferences in 69 HITs. Below are screenshots of an actual Amazon Mechanical Turk (AMT) experiment. These screenshots show the debriefing of the experiment, a sample dialog and a dialogue context with two candidate reactions that human evaluators should choose from. The experiment was conducted with psiturk, which can be downloaded from www.psiturk.org and the source code will be published after publication."}], "references": [{"title": "Learning stochastic recurrent networks", "author": ["J. Bayer", "C. Osendorfer"], "venue": "In NIPS Workshop on Advances in Variational Inference", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Generating sentences from a continuous space. arXiv:1511.06349", "author": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["C. Chelba", "T. Mikolov", "M. Schuster", "Q. Ge", "T. Brants", "P. Koehn", "T. Robinson"], "venue": "In INTERSPEECH", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A recurrent latent variable model for sequential data", "author": ["J. Chung", "K. Kastner", "L. Dinh", "K. Goel", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Variational recurrent auto-encoders", "author": ["O. Fabius", "J.R. van Amersfoort"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["M. Galley", "C. Brockett", "A. Sordoni", "Y. Ji", "M. Auli", "C. Quirk", "M. Mitchell", "J. Gao", "B. Dolan"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Deep Learning", "author": ["I. Goodfellow", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Sequence transduction with recurrent neural networks. In ICML RLW", "author": ["A. Graves"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "In ICLR", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In ICLR", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["J. Li", "M. Galley", "C. Brockett", "J. Gao", "B. Dolan"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Liu", "C.-W", "R. Lowe", "I.V. Serban", "M. Noseworthy", "L. Charlin", "J. Pineau"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems. In SIGDIAL", "author": ["R. Lowe", "N. Pow", "I. Serban", "J. Pineau"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "In ACL,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "A survey on metrics for the evaluation of user simulations", "author": ["O. Pietquin", "H. Hastie"], "venue": "The knowledge engineering review,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Data-driven response generation in social media", "author": ["A. Ritter", "C. Cherry", "W.B. Dolan"], "venue": "In EMNLP,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics", "author": ["V. Rus", "M. Lintean"], "venue": "In Building Educational Applications Workshop,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A.C. Courville", "J. Pineau"], "venue": "In AAAI,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "VCA: an experiment with a multiparty virtual chat agent", "author": ["S. Shaikh", "T. Strzalkowski", "S. Taylor", "N. Webb"], "venue": "In ACL Workshop on Companionable Dialogue Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["A. Sordoni", "Y. Bengio", "H. Vahabi", "C. Lioma", "J.G. Simonsen", "Nie", "J.-Y"], "venue": "In CIKM", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "Nie", "J.-Y", "J. Gao", "B. Dolan"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "POMDP-based statistical spoken dialog systems: A review", "author": ["S. Young", "M. Gasic", "B. Thomson", "J.D. Williams"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 193, "endOffset": 196}, {"referenceID": 9, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 227, "endOffset": 235}, {"referenceID": 17, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 227, "endOffset": 235}, {"referenceID": 27, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 256, "endOffset": 263}, {"referenceID": 4, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 256, "endOffset": 263}, {"referenceID": 26, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 274, "endOffset": 282}, {"referenceID": 23, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 274, "endOffset": 282}, {"referenceID": 8, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 306, "endOffset": 309}, {"referenceID": 1, "context": "This is often an inappropriate place to inject variability [2, 6, 1].", "startOffset": 59, "endOffset": 68}, {"referenceID": 5, "context": "This is often an inappropriate place to inject variability [2, 6, 1].", "startOffset": 59, "endOffset": 68}, {"referenceID": 0, "context": "This is often an inappropriate place to inject variability [2, 6, 1].", "startOffset": 59, "endOffset": 68}, {"referenceID": 11, "context": "At each time step, the model observes an element and updates its internal hidden state, hm = f(hm\u22121, wm), where f is a parametrized non-linear function, such as the hyperbolic tangent, the LSTM gating unit [12] or the GRU gating unit [5].", "startOffset": 206, "endOffset": 210}, {"referenceID": 4, "context": "At each time step, the model observes an element and updates its internal hidden state, hm = f(hm\u22121, wm), where f is a parametrized non-linear function, such as the hyperbolic tangent, the LSTM gating unit [12] or the GRU gating unit [5].", "startOffset": 234, "endOffset": 237}, {"referenceID": 17, "context": "Under this assumption, the RNN Language Model (RNNLM) [18], the simplest possible generative RNN for discrete sequences, parametrizes the output distribution using the softmax function applied to an affine transformation of the hidden state hm.", "startOffset": 54, "endOffset": 58}, {"referenceID": 25, "context": "The hierarchical recurrent encoder-decoder model (HRED) [26, 24] is an extension of the RNNLM.", "startOffset": 56, "endOffset": 64}, {"referenceID": 23, "context": "The hierarchical recurrent encoder-decoder model (HRED) [26, 24] is an extension of the RNNLM.", "startOffset": 56, "endOffset": 64}, {"referenceID": 4, "context": "It extends the encoder-decoder architecture [5] to the natural dialogue setting.", "startOffset": 44, "endOffset": 47}, {"referenceID": 25, "context": "For additional details see [26, 24].", "startOffset": 27, "endOffset": 35}, {"referenceID": 23, "context": "For additional details see [26, 24].", "startOffset": 27, "endOffset": 35}, {"referenceID": 23, "context": "In the recent literature, it has been observed that the RNNLM and HRED, and similar models based on RNN architectures, have critical problems generating meaningful dialogue utterances [24, 15].", "startOffset": 184, "endOffset": 192}, {"referenceID": 14, "context": "In the recent literature, it has been observed that the RNNLM and HRED, and similar models based on RNN architectures, have critical problems generating meaningful dialogue utterances [24, 15].", "startOffset": 184, "endOffset": 192}, {"referenceID": 5, "context": "Similarly, for the diagonal covariance matrix \u03a3prior another matrix multiplication is applied to the net\u2019s output followed by softplus function, to ensure positiveness [6].", "startOffset": 168, "endOffset": 171}, {"referenceID": 28, "context": "This is an interesting problem with applications in areas such as customer service, technical support, language learning and entertainment [29].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "It is also a task domain that requires learning to generate sequences with complex structures while taking into account long-term context [17, 27].", "startOffset": 138, "endOffset": 146}, {"referenceID": 26, "context": "It is also a task domain that requires learning to generate sequences with complex structures while taking into account long-term context [17, 27].", "startOffset": 138, "endOffset": 146}, {"referenceID": 21, "context": "We first perform experiments on a Twitter Dialogue Corpus [22].", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "[22], and is split into training, validation and test sets, containing respectively 749, 060, 93, 633 and 10, 000 dialogues.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "The dialogues are fairly long compared to recent large-scale language modelling corpora, such as the 1 Billion Word Language Model Benchmark [4], which focus on modelling single sentences.", "startOffset": 141, "endOffset": 144}, {"referenceID": 16, "context": "We also experiment on the Ubuntu Dialogue Corpus [17], which contains about 500, 000 dialogues extracted from the #Ubuntu Internet Relayed Chat channel.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "We optimize all models using Adam [13].", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "We choose our hyperparameters and early stop with patience using the variational lower-bound [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "At test time, we use beam search with 5 beams for outputting responses with the RNN decoders [10].", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "For reference, we also include a non-neural network baseline, specifically the TF-IDF retrieval-based model proposed in [17].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "[3]: we drop words in the decoder with a fixed drop rate of 25% and multiply the KL terms in eq.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Evaluation Accurate evaluation of dialogue system responses is a difficult problem [8, 20].", "startOffset": 83, "endOffset": 90}, {"referenceID": 19, "context": "Evaluation Accurate evaluation of dialogue system responses is a difficult problem [8, 20].", "startOffset": 83, "endOffset": 90}, {"referenceID": 15, "context": "[16] show that such metrics have little correlation with human evaluations of response quality.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "This helps compare how well each model can integrate the dialogue context into its response, since it has previously been hypothesized that for long contexts hierarchical RNNs models fare better [24, 26].", "startOffset": 195, "endOffset": 203}, {"referenceID": 25, "context": "This helps compare how well each model can integrate the dialogue context into its response, since it has previously been hypothesized that for long contexts hierarchical RNNs models fare better [24, 26].", "startOffset": 195, "endOffset": 203}, {"referenceID": 24, "context": "However, we believe that response diversity is crucial to maintaining interesting conversations \u2014 in the dialogue literature, generic responses are used primarily as \u2018back-off\u2019 strategies in case the agent has no interesting response that is relevant to the context [25].", "startOffset": 266, "endOffset": 270}, {"referenceID": 18, "context": "The Embedding Average (Average) metric projects the model response and ground truth response into two separate real-valued vectors by taking the mean over the word embeddings in each response, and then computes the cosine similarity between them [19].", "startOffset": 246, "endOffset": 250}, {"referenceID": 22, "context": "Given the (non-exclusive) alignment between words in the two responses, the mean over the cosine similarities is computed for each pair of questions [23].", "startOffset": 149, "endOffset": 153}, {"referenceID": 23, "context": "Thus, VHRED compares favourably to recently proposed models in the literature, which often output extremely low-entropy (generic) responses such as OK and I don\u2019t know [24, 15].", "startOffset": 168, "endOffset": 176}, {"referenceID": 14, "context": "Thus, VHRED compares favourably to recently proposed models in the literature, which often output extremely low-entropy (generic) responses such as OK and I don\u2019t know [24, 15].", "startOffset": 168, "endOffset": 176}, {"referenceID": 13, "context": "The use of a stochastic latent variable learned by maximizing a variational lower bound is inspired by the variational autoencoder (VAE) [14, 21].", "startOffset": 137, "endOffset": 145}, {"referenceID": 20, "context": "The use of a stochastic latent variable learned by maximizing a variational lower bound is inspired by the variational autoencoder (VAE) [14, 21].", "startOffset": 137, "endOffset": 145}, {"referenceID": 10, "context": "Such models have been used predominantly for generating images in the continuous domain [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "However, there has also been recent work applying these architectures for generating sequences, such as the Variational Recurrent Neural Networks (VRNN) [6], which was applied for speech and handwriting synthesis, and Stochastic Recurrent Networks (STORN) [1], which was applied for music generation and motion capture modeling.", "startOffset": 153, "endOffset": 156}, {"referenceID": 0, "context": "However, there has also been recent work applying these architectures for generating sequences, such as the Variational Recurrent Neural Networks (VRNN) [6], which was applied for speech and handwriting synthesis, and Stochastic Recurrent Networks (STORN) [1], which was applied for music generation and motion capture modeling.", "startOffset": 256, "endOffset": 259}, {"referenceID": 6, "context": "Similar to our work is the Variational Recurrent Autoencoder [7] and the Variational Autoencoder Language Model [3], which apply encoder-decoder architectures to generative music modeling and language modeling respectively.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "Similar to our work is the Variational Recurrent Autoencoder [7] and the Variational Autoencoder Language Model [3], which apply encoder-decoder architectures to generative music modeling and language modeling respectively.", "startOffset": 112, "endOffset": 115}], "year": 2017, "abstractText": "Sequential data often possesses a hierarchical structure with complex dependencies between subsequences, such as found between the utterances in a dialogue. In an effort to model this kind of generative process, we propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with recent neural network architectures. We evaluate the model performance through automatic evaluation metrics and by carrying out a human evaluation. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate the generation of long outputs and maintain the context.", "creator": "LaTeX with hyperref package"}}}