{"id": "1605.06203", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Faster Projection-free Convex Optimization over the Spectrahedron", "abstract": "Minimizing a convex function over the spectrahedron, i.e., the set of all positive semidefinite matrices with unit trace, is an important optimization task with many applications in optimization, machine learning, and signal processing. It is also notoriously difficult to solve in large-scale since standard techniques require expensive matrix decompositions. An alternative, is the conditional gradient method (aka Frank-Wolfe algorithm) that regained much interest in recent years, mostly due to its application to this specific setting. The key benefit of the CG method is that it avoids expensive matrix decompositions all together, and simply requires a single eigenvector computation per iteration, which is much more efficient. On the downside, the CG method, in general, converges with an inferior rate. The error for minimizing a $\\beta$-smooth function after $t$ iterations scales like $\\beta/t$. This convergence rate does not improve even if the function is also strongly convex.", "histories": [["v1", "Fri, 20 May 2016 03:07:40 GMT  (110kb)", "http://arxiv.org/abs/1605.06203v1", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG", "authors": ["dan garber"], "accepted": true, "id": "1605.06203"}, "pdf": {"name": "1605.06203.pdf", "metadata": {"source": "CRF", "title": "Faster Projection-free Convex Optimization over the Spectrahedron", "authors": ["Dan Garber"], "emails": ["dgarber@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.06 203v 1 [mat h.O C] 20 May 201 6In this thesis we present a modification of the CG method tailored to the convex optimization via the spectraeder. The complexity per iteration of the method is essentially identical to that of the standard CG method: only a single eigenvector calculation is required. To minimize an \u03b1-strongly convex and \u03b2-smooth function, the expected approximation error of the method according to t-iterations is: O min {\u03b2 t, (\u03b2-rank (X-rank) \u03b11 / 4t) 4 / 3, (\u03b2-point (X-point) 2}, with rank (X-point), 0-point (X-point), 0-point (X-point) representing the rank of the optimal solution and the smallest non-zero eigenvalue."}, {"heading": "1 Introduction", "text": "In fact, it is such that most people who have decided in the past years for a different policy must now orient themselves in a different direction: in the direction in which they have embarked, in the direction in which they are going, in the direction in which they are moving, in the direction in which they are moving, in the direction in which they are moving, in the direction in which they are moving, in the direction in which they are moving, in the direction in which they are moving, in the direction in which they are moving, in which they are moving, in which they are moving, in which they are moving, in which they are moving, in which they are moving, in which they are moving, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they,"}, {"heading": "1.1 Paper organization", "text": "The rest of this paper is structured as follows: In Section 2, we present necessary preparatory work and notations, describe in detail the problem dealt with in this paper, and establish known connections with the popular problem of convex optimization in compliance with a nuclear standard. In Section 3, we briefly describe the conditional gradient and the projected gradient methods for optimization over the spectrahedron, and present our new method, which represents a certain hybridization of the two. We also cite the main theorem of this paper, Theorem 1, which describes the new convergence rate of the proposed method. In Section 4, we analyze our proposed method and prove the main theorem 1. Finally, in Section 5, we present preliminary empirical evidence showing that our method can actually improve in practice over previous conditional gradient methods."}, {"heading": "2 Preliminaries and Notation", "text": "During this work, we use lowercase bold letters to denote vectors in Rd, e.g. V-Face uppercase letters to denote matrices, e.g. X-Face letters to denote scalars. For vectors, we use flowing letters to denote the standard Euclidean norm, while for matrices, we use flowing letters to denote the spectral norm, and for F-Face letters the Frobenius norm, and vice versa. We use Sd to denote the space of d \u00b7 d real symmetric matrices, and Sd to denote the spectrahedron in Sd, i.e., Sd: = {X-Sd | X = 1}. We leave Tr (\u00b7) and Rang (\u00b7) the eigensymmetric matrices, and rank f-filaments in Sd."}, {"heading": "2.1 Problem setting", "text": "The focus of this work is on the following optimization problem: min X-Sd f (X), (2), whereby we assume that f (X) is both \u03b1-strongly convex and \u03b2-smooth."}, {"heading": "2.2 Convex optimization with a nuclear norm constraint", "text": "An important optimization problem strongly related to problem (2) is the problem of minimizing a convex function via the set of d1 \u00b7 d2 really evaluated matrices with a limited core standard, i.e., min Z-NBd1, d2 (\u03b8) f (Z). (3) Here we let NBd1, d2 (\u03b8) specify the nuclear standard sphere of radius \u03b8 in Rd1 \u00b7 d2, i.e., NBd1, d2 (\u03b8): = {Z-Rd1 \u00b7 d2 | \"Z-Z-Z\": = min {d1, d2}. In the direction of this goal, we now consider the following convex optimization problem: minX-Sd1 + d2 f-mizer. (X) Problem (3) could be formulated directly as convex optimization via the spectrahedron (Idorience)."}, {"heading": "3 Our Approach", "text": "In order to better communicate our ideas, we start with a brief description of the method of conditional gradient and projected gradient, pointing out its advantages and disadvantages in solving problem (2) in Section 3.1. Then we present our new method, which represents a particular combination of ideas from both methods in Section 3.2."}, {"heading": "3.1 Conditional gradient and projected gradient descent", "text": "Standard-conditioned gradient algorithm is in algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 2.algorithm 1.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 1.algorithm 2.algorithm 1.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 3.algorith3.algorith3.algorith3.algorith3.algorith3.algorith3.algorith3.algorithm 1.algorith3.algorith3.algorith3.algorith3.algorith3.algorith3.algorithm 1.algorithm 1.algorith3.algorith3.algorith3.algorith3.algorith3.algorith3.algorith3.algorithm 1.algorith3.algorithm 1.algorith3.algorith3.algorith3.algorith3.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorith3.algorith3.algorith3.algorith3.algorith3.algorithm 1.algorith3.algorith3.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorithm 1.algorith3.algorith3.algorith3.algorith3.algorith3.algorith3.algorithm 1.algorithm 1.algorith3.algorithm 1.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm 2.algorithm"}, {"heading": "3.2 A new hybrid approach: rank one-regularized conditional gradient algorithm", "text": "At the heart of our new method is the combination of ideas from the two above approaches: on the one hand, the solution of a particular linear problem to avoid the shortcomings of the CG method, i.e., slow convergence rate, and on the other hand, the maintenance of the simple structure of a leading eigenvalue, which avoids the inadequacy of the calculation method. (a1, a2, ak) It is a probability distribution over [k], and each xi is a unit vector. Note in particular that the standard CG method (algorithm 1) naturally produces such an explicit decomposition of Xt."}, {"heading": "4 Analysis", "text": "In the course of this section, let a matrix Y-Sd, PY, \u03c4-Sd denote the projection matrix on all eigenvectors of Y that correspond to eigenvalues of at least \u03c4. Similarly, let P-Y-Sd indicate the projection matrix on eigenvectors of Y that correspond eigenvalues of the order of magnitude smaller than \u0432 (including eigenvectors that correspond to zero eigenvalues)."}, {"heading": "4.1 A new decomposition for positive semidefinite matrices with locality proprieties", "text": "The analysis of algorithm 2 is strongly based on a new decomposition idea of algorithms in Sd, which suggests that a matrix X in the form of a convex combination of Rank-One matrices (X = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D"}, {"heading": "4.2 Bounding the per-iteration improvement", "text": "We begin with the analysis of a deterministic and much less efficient version that takes into account all aspects of algorithm 2. (8) This is done in Lemma 5. (6) Then we apply Lemma 5 to analyze the random step of Algorithm 2. (2) However, we first need a simple observation with respect to Algorithm 2, which shows that there can always be sufficiently large step sizes, i.e., the step size of the magnitude is at least as high as the number of steps. (2) In the case of the input sequence of the step sizes in Algorithm 2, which shows that there can always be sufficiently large step sizes, i.e., the step sizes of the magnitude in relation to iteration. (2) It follows that the input sequence of the step sizes in Algorithm 1, is monotonically not increasing and not increasing."}, {"heading": "4.3 Proof of Theorem 1", "text": "The proof is derived from the derivation of each of the convergence rates in the theorem independently using the result of Lemma 6. This is shown in the following Lemmas 7,8, 9. We then show that there is a choice of increment and error tolerance limits for eigenvector calculations that satisfy all lemmas at once, and thus the theorem is satisfied at once. Let's be C, t0 non-negative scalars that satisfy: C 18, C 2 \u2212 1 \u2265 t0 \u2265 t0 \u2265 C 6 \u2212 1. Then, if we define for all t more than 1 what we define: C3 (t + t0 \u2264 C), and we set 0 = \u03b2 and t = \u03b2ati t = \u03b2C 6 (t + t0), it follows that all iterates of Algorithm 2 are feasible, and we are unable."}, {"heading": "5 Preliminary Empirical Evaluation", "text": "In this section, we provide a preliminary empirical evaluation of our approach that best fits the given observations. (We evaluate our method, along with other conditional gradient variants, to refer to the task of matrix completion.) The underlying optimization problem for matrix completion is as follows: min Z \"nBd1, d2\" (Z). (Z \"1). (33), where the entry indicator matrix (i\" j \") in R\" d2, and {il \"rl\" nl \"= 1). (d1), [d2],\" Our goal is to find a matrix with a bounded nuclear standard (which serves as a convex surrounding precedence) that corresponds to the best observations (d2)."}], "references": [{"title": "Linearly convergent away-step conditional gradient for non-strongly convex functions", "author": ["Amir Beck", "Shimrit Shtern"], "venue": "arXiv preprint arXiv:1504.05002,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Lifted coordinate descent for learning with trace-norm regularization", "author": ["Miroslav Dud\u0301\u0131k", "Z\u00e4\u0131d Harchaoui", "J\u00e9r\u00f4me Malick"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "An algorithm for quadratic programming", "author": ["M. Frank", "P. Wolfe"], "venue": "Naval Research Logistics Quarterly,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1956}, {"title": "An extended frank-wolfe method with\u201d in-face\u201d directions, and its application to low-rank matrix completion", "author": ["Robert M Freund", "Paul Grigas", "Rahul Mazumder"], "venue": "arXiv preprint arXiv:1511.02204,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": "CoRR, abs/1301.4666,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Playing non-linear games with linear oracles", "author": ["Dan Garber", "Elad Hazan"], "venue": "In 54th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Fast and simple pca via convex optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": "arXiv preprint arXiv:1509.05647,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Faster rates for the frank-wolfe method over strongly-convex sets", "author": ["Dan Garber", "Elad Hazan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Multiple kernel learning algorithms", "author": ["Mehmet G\u00f6nen", "Ethem Alpayd\u0131n"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Large-scale image classification with trace-norm regularization", "author": ["Z\u00e4\u0131d Harchaoui", "Matthijs Douze", "Mattis Paulin", "Miroslav Dud\u0301\u0131k", "J\u00e9r\u00f4me Malick"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Sparse approximate solutions to semidefinite programs", "author": ["Elad Hazan"], "venue": "In 8th Latin American Theoretical Informatics Symposium, LATIN,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Projection-free online learning", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Variance-reduced and projection-free stochastic optimization", "author": ["Elad Hazan", "Haipeng Luo"], "venue": "CoRR, abs/1602.02101,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Convex optimization without projection", "author": ["Martin Jaggi"], "venue": "steps. CoRR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Revisiting frank-wolfe: Projection-free sparse convex optimization", "author": ["Martin Jaggi"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Sulovsk\u00fd. A simple algorithm for nuclear norm regularized problems", "author": ["Martin Jaggi", "Marek"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["Chi Jin", "Sham M Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford"], "venue": "arXiv preprint arXiv:1510.08896,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Estimating the largest eigenvalues by the power and lanczos algorithms with a random start", "author": ["J. Kuczy\u0144ski", "H. Wo\u017aniakowski"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "An affine invariant linear convergence analysis for frank-wolfe algorithms", "author": ["Simon Lacoste-Julien", "Martin Jaggi"], "venue": "CoRR, abs/1312.7864,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "On the global linear convergence of Frank-Wolfe optimization variants", "author": ["Simon Lacoste-Julien", "Martin Jaggi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Conditional gradient sliding for convex optimization", "author": ["Guanghui Lan", "Yi Zhou"], "venue": "Technical report, Technical Report,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Gert RG Lanckriet", "Nello Cristianini", "Peter Bartlett", "Laurent El Ghaoui", "Michael I Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "A hybrid algorithm for convex semidefinite optimization", "author": ["S\u00f6ren Laue"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Constrained minimization methods", "author": ["Evgeny S Levitin", "Boris T Polyak"], "venue": "USSR Computational mathematics and mathematical physics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1966}, {"title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers", "author": ["Sahand Negahban", "Bin Yu", "Martin J Wainwright", "Pradeep K. Ravikumar"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Large-scale convex minimization with a low-rank constraint", "author": ["Shai Shalev-Shwartz", "Alon Gonen", "Ohad Shamir"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["Ohad Shamir"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q Weinberger", "John Blitzer", "Lawrence K Saul"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["Eric P Xing", "Andrew Y Ng", "Michael I Jordan", "Stuart Russell"], "venue": "Advances in neural information processing systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Distance metric learning with eigenvalue optimization", "author": ["Yiming Ying", "Peng Li"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Accelerated training for matrixnorm regularization: A boosting approach", "author": ["Xinhua Zhang", "Dale Schuurmans", "Yao-liang Yu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "1 Introduction Minimizing a convex function over the set of positive semidefinite matrices with unit trace, aka the spectrahedron, is an important optimization task which lies at the heart of many optimization, machine learning, and signal processing tasks such as matrix completion [2, 28, 17], metric learning [32, 31, 33], kernel matrix learning [23, 10], multiclass classification [3, 34, 14], and more.", "startOffset": 283, "endOffset": 294}, {"referenceID": 27, "context": "1 Introduction Minimizing a convex function over the set of positive semidefinite matrices with unit trace, aka the spectrahedron, is an important optimization task which lies at the heart of many optimization, machine learning, and signal processing tasks such as matrix completion [2, 28, 17], metric learning [32, 31, 33], kernel matrix learning [23, 10], multiclass classification [3, 34, 14], and more.", "startOffset": 283, "endOffset": 294}, {"referenceID": 16, "context": "1 Introduction Minimizing a convex function over the set of positive semidefinite matrices with unit trace, aka the spectrahedron, is an important optimization task which lies at the heart of many optimization, machine learning, and signal processing tasks such as matrix completion [2, 28, 17], metric learning [32, 31, 33], kernel matrix learning [23, 10], multiclass classification [3, 34, 14], and more.", "startOffset": 283, "endOffset": 294}, {"referenceID": 31, "context": "1 Introduction Minimizing a convex function over the set of positive semidefinite matrices with unit trace, aka the spectrahedron, is an important optimization task which lies at the heart of many optimization, machine learning, and signal processing tasks such as matrix completion [2, 28, 17], metric learning [32, 31, 33], kernel matrix learning [23, 10], multiclass classification [3, 34, 14], and more.", "startOffset": 312, "endOffset": 324}, {"referenceID": 30, "context": "1 Introduction Minimizing a convex function over the set of positive semidefinite matrices with unit trace, aka the spectrahedron, is an important optimization task which lies at the heart of many optimization, machine learning, and signal processing tasks such as matrix completion [2, 28, 17], metric learning [32, 31, 33], kernel matrix learning [23, 10], multiclass classification [3, 34, 14], and more.", "startOffset": 312, "endOffset": 324}, {"referenceID": 32, "context": "1 Introduction Minimizing a convex function over the set of positive semidefinite matrices with unit trace, aka the spectrahedron, is an important optimization task which lies at the heart of many optimization, machine learning, and signal processing tasks such as matrix completion [2, 28, 17], metric learning [32, 31, 33], kernel matrix learning [23, 10], multiclass classification [3, 34, 14], and more.", "startOffset": 312, "endOffset": 324}, {"referenceID": 22, "context": "1 Introduction Minimizing a convex function over the set of positive semidefinite matrices with unit trace, aka the spectrahedron, is an important optimization task which lies at the heart of many optimization, machine learning, and signal processing tasks such as matrix completion [2, 28, 17], metric learning [32, 31, 33], kernel matrix learning [23, 10], multiclass classification [3, 34, 14], and more.", "startOffset": 349, "endOffset": 357}, {"referenceID": 9, "context": "1 Introduction Minimizing a convex function over the set of positive semidefinite matrices with unit trace, aka the spectrahedron, is an important optimization task which lies at the heart of many optimization, machine learning, and signal processing tasks such as matrix completion [2, 28, 17], metric learning [32, 31, 33], kernel matrix learning [23, 10], multiclass classification [3, 34, 14], and more.", "startOffset": 349, "endOffset": 357}, {"referenceID": 2, "context": "1 Introduction Minimizing a convex function over the set of positive semidefinite matrices with unit trace, aka the spectrahedron, is an important optimization task which lies at the heart of many optimization, machine learning, and signal processing tasks such as matrix completion [2, 28, 17], metric learning [32, 31, 33], kernel matrix learning [23, 10], multiclass classification [3, 34, 14], and more.", "startOffset": 385, "endOffset": 396}, {"referenceID": 33, "context": "1 Introduction Minimizing a convex function over the set of positive semidefinite matrices with unit trace, aka the spectrahedron, is an important optimization task which lies at the heart of many optimization, machine learning, and signal processing tasks such as matrix completion [2, 28, 17], metric learning [32, 31, 33], kernel matrix learning [23, 10], multiclass classification [3, 34, 14], and more.", "startOffset": 385, "endOffset": 396}, {"referenceID": 13, "context": "1 Introduction Minimizing a convex function over the set of positive semidefinite matrices with unit trace, aka the spectrahedron, is an important optimization task which lies at the heart of many optimization, machine learning, and signal processing tasks such as matrix completion [2, 28, 17], metric learning [32, 31, 33], kernel matrix learning [23, 10], multiclass classification [3, 34, 14], and more.", "startOffset": 385, "endOffset": 396}, {"referenceID": 3, "context": "These methods are mostly based on the conditional gradient method, also known as the Frank-Wolfe algorithm [4, 16], which is a generic method for constrained convex optimization given an oracle for minimizing linear functions over the feasible domain.", "startOffset": 107, "endOffset": 114}, {"referenceID": 15, "context": "These methods are mostly based on the conditional gradient method, also known as the Frank-Wolfe algorithm [4, 16], which is a generic method for constrained convex optimization given an oracle for minimizing linear functions over the feasible domain.", "startOffset": 107, "endOffset": 114}, {"referenceID": 3, "context": "While the CG method has been discovered already in the 1950\u2019s [4, 25], it has regained much interest in recent years in the machine learning and optimization communities, in particular due to its applications to semidefinite optimization and convex optimization with a nuclear norm constraint / regularization1, e.", "startOffset": 62, "endOffset": 69}, {"referenceID": 24, "context": "While the CG method has been discovered already in the 1950\u2019s [4, 25], it has regained much interest in recent years in the machine learning and optimization communities, in particular due to its applications to semidefinite optimization and convex optimization with a nuclear norm constraint / regularization1, e.", "startOffset": 62, "endOffset": 69}, {"referenceID": 11, "context": ", [12, 17, 24, 29, 33, 3, 11, 13, 14].", "startOffset": 2, "endOffset": 37}, {"referenceID": 16, "context": ", [12, 17, 24, 29, 33, 3, 11, 13, 14].", "startOffset": 2, "endOffset": 37}, {"referenceID": 23, "context": ", [12, 17, 24, 29, 33, 3, 11, 13, 14].", "startOffset": 2, "endOffset": 37}, {"referenceID": 28, "context": ", [12, 17, 24, 29, 33, 3, 11, 13, 14].", "startOffset": 2, "endOffset": 37}, {"referenceID": 32, "context": ", [12, 17, 24, 29, 33, 3, 11, 13, 14].", "startOffset": 2, "endOffset": 37}, {"referenceID": 2, "context": ", [12, 17, 24, 29, 33, 3, 11, 13, 14].", "startOffset": 2, "endOffset": 37}, {"referenceID": 10, "context": ", [12, 17, 24, 29, 33, 3, 11, 13, 14].", "startOffset": 2, "endOffset": 37}, {"referenceID": 12, "context": ", [12, 17, 24, 29, 33, 3, 11, 13, 14].", "startOffset": 2, "endOffset": 37}, {"referenceID": 13, "context": ", [12, 17, 24, 29, 33, 3, 11, 13, 14].", "startOffset": 2, "endOffset": 37}, {"referenceID": 18, "context": "These running times improve exponentially to only depend on log(1/\u01eb) when the eigenvalues of the input matrix are well distributed [19].", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "Indeed, in several important machine learning applications, such as matrix completion, the CG method requires eigenvector computations of very sparse matrices [17].", "startOffset": 159, "endOffset": 163}, {"referenceID": 7, "context": "Also, very recently, new eigenvector algorithms with significantly improved performance guarantees were introduced which are applicable for matrices with certain popular structure [8, 18, 30].", "startOffset": 180, "endOffset": 191}, {"referenceID": 17, "context": "Also, very recently, new eigenvector algorithms with significantly improved performance guarantees were introduced which are applicable for matrices with certain popular structure [8, 18, 30].", "startOffset": 180, "endOffset": 191}, {"referenceID": 29, "context": "Also, very recently, new eigenvector algorithms with significantly improved performance guarantees were introduced which are applicable for matrices with certain popular structure [8, 18, 30].", "startOffset": 180, "endOffset": 191}, {"referenceID": 12, "context": "In these settings the time required for the optimization method to perform a single update may be a key consideration in its applicability to the problem [13, 7, 6].", "startOffset": 154, "endOffset": 164}, {"referenceID": 6, "context": "In these settings the time required for the optimization method to perform a single update may be a key consideration in its applicability to the problem [13, 7, 6].", "startOffset": 154, "endOffset": 164}, {"referenceID": 5, "context": "In these settings the time required for the optimization method to perform a single update may be a key consideration in its applicability to the problem [13, 7, 6].", "startOffset": 154, "endOffset": 164}, {"referenceID": 26, "context": "On the other hand, the convergence rate of optimal projection-based methods, such as Nesterov\u2019s accelerated gradient method, scales like 1/t2 for smooth functions, and can be improved exponentially to exp(\u2212\u0398(t)) when the objective is also strongly convex [27].", "startOffset": 255, "endOffset": 259}, {"referenceID": 5, "context": "These results exhibit provably-faster rates for optimization over polyhedral sets [6, 20, 1] and stronglyconvex sets [9], but do not apply to the spectrahedron.", "startOffset": 82, "endOffset": 92}, {"referenceID": 19, "context": "These results exhibit provably-faster rates for optimization over polyhedral sets [6, 20, 1] and stronglyconvex sets [9], but do not apply to the spectrahedron.", "startOffset": 82, "endOffset": 92}, {"referenceID": 0, "context": "These results exhibit provably-faster rates for optimization over polyhedral sets [6, 20, 1] and stronglyconvex sets [9], but do not apply to the spectrahedron.", "startOffset": 82, "endOffset": 92}, {"referenceID": 8, "context": "These results exhibit provably-faster rates for optimization over polyhedral sets [6, 20, 1] and stronglyconvex sets [9], but do not apply to the spectrahedron.", "startOffset": 117, "endOffset": 120}, {"referenceID": 28, "context": "For the specific setting considered in this work, several heuristic improvements of the CG method were suggested which show promising empirical evidence, however, non of them provably improve over the rate of the standard CG method [29, 24, 5].", "startOffset": 232, "endOffset": 243}, {"referenceID": 23, "context": "For the specific setting considered in this work, several heuristic improvements of the CG method were suggested which show promising empirical evidence, however, non of them provably improve over the rate of the standard CG method [29, 24, 5].", "startOffset": 232, "endOffset": 243}, {"referenceID": 4, "context": "For the specific setting considered in this work, several heuristic improvements of the CG method were suggested which show promising empirical evidence, however, non of them provably improve over the rate of the standard CG method [29, 24, 5].", "startOffset": 232, "endOffset": 243}, {"referenceID": 18, "context": "The running times for the eigenvector computation are based on the Lanczos method [19].", "startOffset": 82, "endOffset": 86}, {"referenceID": 25, "context": "While the combination of smoothness and strong convexity is a rare commodity, several important problems such as linear regression in the well-conditioned case, and solving undetermined linear systems (such as in the matrix completion problem), under certain conditions (see for instance [26]), exhibit such properties.", "startOffset": 288, "endOffset": 292}, {"referenceID": 5, "context": "This approach has allowed, among other things, to apply CG-based methods to non-smooth problems, for which the standard CG method is not suitable [6], and to strike better trade-offs between the linear optimization oracle complexity and the first-order oracle complexity [22, 14] 1.", "startOffset": 146, "endOffset": 149}, {"referenceID": 21, "context": "This approach has allowed, among other things, to apply CG-based methods to non-smooth problems, for which the standard CG method is not suitable [6], and to strike better trade-offs between the linear optimization oracle complexity and the first-order oracle complexity [22, 14] 1.", "startOffset": 271, "endOffset": 279}, {"referenceID": 13, "context": "This approach has allowed, among other things, to apply CG-based methods to non-smooth problems, for which the standard CG method is not suitable [6], and to strike better trade-offs between the linear optimization oracle complexity and the first-order oracle complexity [22, 14] 1.", "startOffset": 271, "endOffset": 279}, {"referenceID": 16, "context": "The following Lemma, whose proof can be found in [17], shows the equivalence between the two problems.", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "Algorithm 1 Conditional Gradient 1: input: sequence of step-sizes {\u03b7t}t\u22651 \u2282 [0, 1] 2: let X1 be an arbitrary matrix in Sd 3: for t = 1.", "startOffset": 76, "endOffset": 82}, {"referenceID": 15, "context": "(4), in principal, might remain as large as the diameter of Sd, which, given a proper choice of step-size \u03b7t, results in the well-known convergence rate of O(\u03b2/t) [16, 12].", "startOffset": 163, "endOffset": 171}, {"referenceID": 11, "context": "(4), in principal, might remain as large as the diameter of Sd, which, given a proper choice of step-size \u03b7t, results in the well-known convergence rate of O(\u03b2/t) [16, 12].", "startOffset": 163, "endOffset": 171}, {"referenceID": 14, "context": "This consequence holds also in case f(X) is not only smooth, but also strongly-convex, see for instance Lemma 21 in [15].", "startOffset": 116, "endOffset": 120}, {"referenceID": 14, "context": "4 in [15].", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "Let \u03c4, \u03b3 \u2208 [0, 1] be scalars that satisfy \u03b3\u03c4 1\u2212\u03b3 \u2265 \u2016X\u2212Y\u2016F .", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": ", ak) is a distribution over [k], and let \u03c4, \u03b3 \u2208 [0, 1] which satisfy the condition in Lemma 2.", "startOffset": 49, "endOffset": 55}, {"referenceID": 1, "context": "In case the input sequence of step-sizes in Algorithm 2 {\u03b7t}t\u22651, is monotonically non-increasing and \u03b7t \u2208 [0, 2] for all t \u2265 1, it follows that on each iteration t of the algorithm, the iterate Xt admits an explicitly-given factorization into a convex sum of rank-one matrices, as described in the algorithm, such that for every rank-one coefficient ai, it holds that ai \u2265 \u03b7t/2.", "startOffset": 106, "endOffset": 112}, {"referenceID": 1, "context": "Thus, for any \u03b71 \u2208 [0, 2] it indeed follows that a1 \u2265 \u03b71/2.", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": ", (17) where the second inequality follows since min{a, b} \u2264 \u03bba+(1\u2212\u03bb)b for any a, b \u2208 R, \u03bb \u2208 [0, 1],", "startOffset": 93, "endOffset": 99}, {"referenceID": 0, "context": ", (18) where the last inequality follows from plugging the bounds in Lemma 4 and holds for any \u03c4, \u03b3 \u2208 [0, 1] such that \u03c4\u03b3 1\u2212\u03b3 \u2265 \u2016X\u2212X\u2217\u2016F .", "startOffset": 102, "endOffset": 108}, {"referenceID": 1, "context": "\u2200t \u2265 1 : \u03b7t = C 3(t+ t0) \u2208 [0, 2].", "startOffset": 27, "endOffset": 33}, {"referenceID": 1, "context": "(25) In order for our choice of step-sizes to satisfy the conditions of Observation 1, it must hold that {\u03b7t}t\u22651 \u2282 [0, 2].", "startOffset": 115, "endOffset": 121}, {"referenceID": 1, "context": "\u2200t \u2265 1 : \u03b7t = C3/4 3(t+ t0) \u2208 [0, 2].", "startOffset": 30, "endOffset": 36}, {"referenceID": 1, "context": "\u2200t \u2265 1 : \u03b7t = C1/2 3(t+ t0) \u2208 [0, 2].", "startOffset": 30, "endOffset": 36}, {"referenceID": 16, "context": "For a detailed presentation of the setting and the application of the conditional gradient method to this problem, we refer the reader to [17].", "startOffset": 138, "endOffset": 142}, {"referenceID": 16, "context": "(33) is known to have a smoothness parameter \u03b2 with respect to \u2016 \u00b7 \u2016F , which satisfies \u03b2 = O(1), see for instance [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 25, "context": "(1) (which is indeed the only consequence of strong convexity that we use in our analysis), known as restricted strong convexity [26].", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "Second, after computing the eigenvector vt using the choice of \u03b7t prescribed in Theorem 1, we apply a line-search, as detailed in [17], in order to the determine the optimal step-size given the direction vtv \u22a4 t \u2212 xitxit .", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "Baselines As baselines for comparison we used the standard conditional gradient method with exact line-search for setting the step-size (denoted CG in our figures)[17], and the conditional gradient with away-steps variant, recently studied in [20, 1, 21] (denoted Away-CG in our figures).", "startOffset": 163, "endOffset": 167}, {"referenceID": 19, "context": "Baselines As baselines for comparison we used the standard conditional gradient method with exact line-search for setting the step-size (denoted CG in our figures)[17], and the conditional gradient with away-steps variant, recently studied in [20, 1, 21] (denoted Away-CG in our figures).", "startOffset": 243, "endOffset": 254}, {"referenceID": 0, "context": "Baselines As baselines for comparison we used the standard conditional gradient method with exact line-search for setting the step-size (denoted CG in our figures)[17], and the conditional gradient with away-steps variant, recently studied in [20, 1, 21] (denoted Away-CG in our figures).", "startOffset": 243, "endOffset": 254}, {"referenceID": 20, "context": "Baselines As baselines for comparison we used the standard conditional gradient method with exact line-search for setting the step-size (denoted CG in our figures)[17], and the conditional gradient with away-steps variant, recently studied in [20, 1, 21] (denoted Away-CG in our figures).", "startOffset": 243, "endOffset": 254}], "year": 2016, "abstractText": "Minimizing a convex function over the spectrahedron, i.e., the set of all d \u00d7 d positive semidefinite matrices with unit trace, is an important optimization task with many applications in optimization, machine learning, and signal processing, the most notable one probably being matrix completion. Unfortunately, it is also notoriously difficult to solve in large-scale since standard techniques require to compute expensive matrix decompositions on each iteration. An alternative, is the conditional gradient method (aka Frank-Wolfe algorithm) that regained much interest in recent years, mostly due to its application to this specific setting. The key benefit of the CG method is that it avoids expensive matrix decompositions all together, and simply requires a single eigenvector computation per iteration, which is much more efficient. On the downside, the CG method, in general, converges with an inferior rate. The error for minimizing a \u03b2-smooth function after t iterations scales like \u03b2/t. This convergence rate does not improve even if the function is also strongly convex. In this work we present a modification of the CG method tailored for convex optimization over the spectrahedron. The per-iteration complexity of the method is essentially identical to that of the standard CG method: only a single eigenvecor computation is required. For minimizing an \u03b1-strongly convex and \u03b2-smooth function, the expected approximation error of the method after t iterations is:", "creator": "LaTeX with hyperref package"}}}