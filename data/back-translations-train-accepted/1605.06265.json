{"id": "1605.06265", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "End-to-End Kernel Learning with Supervised Convolutional Kernel Networks", "abstract": "In this paper, we propose a new image representation based on a multilayer kernel machine that performs end-to-end learning. Unlike traditional kernel methods, where the kernel is handcrafted or adapted to data in an unsupervised manner, we learn how to shape the kernel for a supervised prediction problem. We proceed by generalizing convolutional kernel networks, which originally provide unsupervised image representations, and we derive backpropagation rules to optimize model parameters. As a result, we obtain a new type of convolutional neural network with the following properties: (i) at each layer, learning filters is equivalent to optimizing a linear subspace in a reproducing kernel Hilbert space (RKHS), where we project data, (ii) the network may be learned with supervision or without, (iii) the model comes with a natural regularization function (the norm in the RKHS). We show that our method achieves reasonably competitive performance on some standard \"deep learning\" image classification datasets such as CIFAR-10 and SVHN, and also state-of-the-art results for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks.", "histories": [["v1", "Fri, 20 May 2016 09:52:14 GMT  (9700kb,D)", "http://arxiv.org/abs/1605.06265v1", null], ["v2", "Tue, 25 Oct 2016 12:52:50 GMT  (9700kb,D)", "http://arxiv.org/abs/1605.06265v2", "to appear in Advances in Neural Information Processing Systems (NIPS)"]], "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG", "authors": ["julien mairal"], "accepted": true, "id": "1605.06265"}, "pdf": {"name": "1605.06265.pdf", "metadata": {"source": "CRF", "title": "End-to-End Kernel Learning with Supervised Convolutional Kernel Networks", "authors": ["Julien Mairal"], "emails": ["julien.mairal@inria.fr"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2 Learning Hierarchies of Subspaces with Convolutional Kernel Networks", "text": "In this section we will introduce the principles of the Convolutionary Core Networks and a few generalizations and improvements of the original approach of [18]. < / p > p > p > p > p > p > p > p (1) p > p (1) p > p (1) p > p > p (1) p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p"}, {"heading": "3 End-to-End Kernel Learning", "text": "In the previous section, we described a variant of Convolutionary Core Networks, in which linear subspaces are learned at each level, without the supervision of a K-Mean algorithm, which leads to small projection residuals. Therefore, it is obvious to introduce a discriminatory approach as well."}, {"heading": "3.1 Backpropagation Rules for Convolutional Kernel Networks", "text": "We are considering a prediction task in which we evaluate the predictive power of the prediction function in relation to the prediction function in relation to the prediction function in relation to the prediction function in relation to the prediction function in relation to the prediction function in relation to the prediction function in relation to the prediction function in relation to the prediction function in relation to the prediction function in relation to the actual prediction function in relation to the prediction function in relation to the prediction function in relation to the prediction function in relation to the prediction function in relation to the prediction function."}, {"heading": "3.2 Optimization and Practical Heuristics", "text": "The retrogressiveness of the previous section has set the stage for the use of a stochastic gradient (SGD). We present some strategies to accelerate it in our context. (S) These methods usually deliver a high velocity over the stochastic gradient without suffering the burden of choosing a learning rate. (S) They rely on the storage capacity, and they require the storage of full training sets. (S) It means storing the n-maps Iik, which is often reasonable when we are not using data. (S) Partly leveraging these fast algorithms for our non-conventional problem, we have adopted a miniature."}, {"heading": "4 Experiments", "text": "We now present two image-related tasks: we start with classification, where we achieve reasonably competitive accuracy, and then move on to super-resolution, where we achieve state-of-the-art results. All experiments were conducted on 8-core and 10-core 2.4GHz Intel CPUs with C + + and Matlab."}, {"heading": "4.1 Image Classification on \u201cDeep Learning\u201d Benchmarks", "text": "We look at the data sets CIFAR-10 [12] and SVHN [19], which contain 32 \u00d7 32 images from 10 classes. CIFAR-10 is medium in size with 50,000 training samples and 10,000 test environments. SVHN is larger with 604 388 training examples and 26 032 test environments. We evaluate the performance of a 9-layer network designed with few hyper parameters: For each layer we learn 512 filters and choose the RBF cores defined in (2) with initial parameters \u03b1j = 1 / (0.52). Layers 1, 3, 5, 7, 9 use 3 \u00d7 3 patches and a subsampling pooling factor of 2 except layer 9, where the factor is 3; layers 2, 4, 6, 8 simply use 1 \u00d7 1 patches and no subsampling values. For CIFAR-10, the parameters are the parameters that are fixed during the training, and are updated in the same way as the SVN for the training, and for the HN."}, {"heading": "4.2 Image Super-Resolution from a Single Image", "text": "Image resolution is a difficult problem where Convolutionary Neural Networks have achieved significant success [7, 8, 27]. Here we follow [8] and replace traditional Convolutionary Neural Networks with our monitored core machine. Specifically, RGB images are converted into the YCbCr color space and the upscaling method is applied to the luminance channel just to allow comparison with previous work. Then, the problem is formulated as multivariate regression. We build a database of 200,000 32 \u00d7 32 patches randomly extracted from the BSD500 dataset [2] after removing the image 302003.jpg that overlaps with one of the test images."}, {"heading": "5 Conclusion", "text": "In this work, we have taken a significant step towards closing the performance gap between deep neural networks and core methods. We have expanded Convolutionary Core Networks to perform end-to-end learning, resulting in better results and truly deep representations, whereas the original Convolutionary Core Networks used no more than three layers. We also believe that the interpretation of our network by learning in the subspace is of interest and opens up new perspectives that we have not yet explored, such as models based on the unification of subspaces, spectral approaches or projections on random subspaces."}, {"heading": "A Orthogonal Projection on the Finite-Dimensional Subspace F1", "text": "First, we find that the kernel K1 is homogeneous, so that for each patch x > > Scalar \u03b3 > 0, 1 (Gonzalez) = Gonzalez 1 (x).So we can assume that x has the unit \"2 norm without loss of universality and perform the projection on F1 = chip (z1),..,.. (zp1) of the normalized patch before applying the reverse recalculation. Then, we use fx to denote the orthogonal projection of a patch x with unit\" 2 norm defines asfx: = arg min f-F1-1 (x) \u2212 f-2H1, which is equivalent to fx: = p1? j1 (zj) with the orthogonal projection of a patch x with unit \"2 norm defines asfx: = arg min-norm defines asfx,\" arg min f-F1 1 (x) \u2212 step1 Z \u2212 j1 (zj) with the orthogonal projection of a patch x with unit \"2 norm defines asfx: = arg min-norm defines asfx,\" arg min f-F1 1 (x) \u2212 jj = 1"}, {"heading": "B Computation of the Gradient with Respect to the Filters", "text": "To calculate the gradient of the loss function, we use term 1 and start by analyzing the effect of the disturbance of each quantity involved in (11), so that we can obtain the desired relations (8) and (9). Before proceeding, we simply remember the definition of the quantity Z + E = > Q (Z1 + \u03b51,., Zk + \u03b5k) and the exact definition of the Landau notation o (E), which we use in (8), whereby it simply means a quantity that is negligible before the standard E (E). (Zk + \u03b5j) Kj (E) Kj (1) F - that is, lim E \u2192 0 (Z + Ej \u2212 IZ \u2212 IZ, Ej \u2212 p F E = 0.Then we start with the initialization of a cell (Z1)."}, {"heading": "C Preconditioning Heuristic on the Sphere", "text": "In this section, we present a pre-conditioning euristics for optimization over the sphere Sp \u2212 1, inspired by second-order optimization techniques (Newton) on smooth manifolds [1]. Following [1], we consider downward steps on the manifold. Therefore, a basic operation is the projection operator Pz on the tangential space at one point. This operator is defined for the sphere byPz [u] = (I \u2212 zz >) u, for any vector u in Rp. Another important operator is the euclidean projection on Sp \u2212 1, which in earlier parts of the paper was called Pz."}, {"heading": "In Rp with no constraint, pre-conditioning is equivalent to performing a change of variable.", "text": "In this case, the conditional gradation of L (z).L (2).L (2).L (2).L (2).W (2).W (1).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W).W (2).W (2).W (2).W (2).W (.W).W (2).W (.2).W (.W).W (2).W (2).W (2).W (2).W (.W).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (2).W (.W).W (.W).W (2).W (2).W (.W).W (2).W (2).W (.W).W (2).W (.W).W (2).W (.W).W (2).W (2).W (.W).W (.W).W (2).W (2).W (2).W (.W).W (.W).W (.W).W ("}, {"heading": "D Additional Results for Image Super-Resolution", "text": "We present a quantitative comparison in Table 3 using the Structural Similarities Index (SSIM), which is known to better reflect human perceived quality than PSNR; it is commonly used to evaluate the quality of superresolution methods, see [8, 26, 27]. Then we present a visual comparison between several approaches in Figures 2, 3 and 4. We focus in particular on the classical Convolutionary Neural Network of [8], as our pipeline differs substantially in the use of our monitored nuclear machine instead of Convolutionary Neural Networks. Subjective evaluation shows that both methods work equally well in textured areas. However, our approach restores better thin radio frequency details, such as the baby's eyelash in the first image. By zooming in on different parts, similar differences are easily noticeable in other images."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we propose a new image representation based on a multilayer kernel<lb>machine that performs end-to-end learning. Unlike traditional kernel methods,<lb>where the kernel is handcrafted or adapted to data in an unsupervised manner, we<lb>learn how to shape the kernel for a supervised prediction problem. We proceed by<lb>generalizing convolutional kernel networks, which originally provide unsupervised<lb>image representations, and we derive backpropagation rules to optimize model<lb>parameters. As a result, we obtain a new type of convolutional neural network with<lb>the following properties: (i) at each layer, learning filters is equivalent to optimizing<lb>a linear subspace in a reproducing kernel Hilbert space (RKHS), where we project<lb>data; (ii) the network may be learned with supervision or without; (iii) the model<lb>comes with a natural regularization function (the norm in the RKHS). We show that<lb>our method achieves reasonably competitive performance on some standard \u201cdeep<lb>learning\u201d image classification datasets such as CIFAR-10 and SVHN, and also<lb>state-of-the-art results for image super-resolution, demonstrating the applicability<lb>of our approach to a large variety of image-related tasks.", "creator": "LaTeX with hyperref package"}}}