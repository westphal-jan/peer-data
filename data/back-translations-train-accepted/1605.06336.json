{"id": "1605.06336", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA", "abstract": "Nonlinear independent component analysis (ICA) provides an appealing framework for unsupervised feature learning, but the models proposed so far are not identifiable. Here, we first propose a new intuitive principle of unsupervised deep learning from time series which uses the nonstationary structure of the data. Our learning principle, time-contrastive learning (TCL), finds a representation which allows optimal discrimination of time segments (windows). Surprisingly, we show how TCL can be related to a nonlinear ICA model, when ICA is redefined to include temporal nonstationarities. In particular, we show that TCL combined with linear ICA estimates the nonlinear ICA model up to point-wise transformations of the sources, and this solution is unique --- thus providing the first identifiability result for nonlinear ICA which is rigorous, constructive, as well as very general.", "histories": [["v1", "Fri, 20 May 2016 12:59:22 GMT  (1287kb,D)", "http://arxiv.org/abs/1605.06336v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["aapo hyv\u00e4rinen", "hiroshi morioka"], "accepted": true, "id": "1605.06336"}, "pdf": {"name": "1605.06336.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA", "authors": ["Aapo Hyv\u00e4rinen"], "emails": [], "sections": [{"heading": null, "text": "Nonlinear Independent Component Analysis (ICA) provides an appealing framework for unattended feature learning, but the models proposed so far are unidentifiable. At this point, we first propose a new intuitive principle of unattended deep learning from time series that utilizes the nonlinear structure of the data. Our learning principle, Time-Contrastive Learning (TCL), finds a representation that allows optimal differentiation of time segments (windows). Surprisingly, we show how TCL can be linked to a nonlinear ICA model when ICA is redefined to include temporal stationaries. Specifically, we show that TCL, in combination with linear ICA, estimates the nonlinear ICA model up to point-to-point transformations of the sources, and this solution is unique - providing the first identification result for nonlinear ICA that is rigorous, constructive, and very general."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "2 Time-contrastive learning", "text": "TCL is a method to train a trait extractor by using a multinomial logistic regression system (MLR), which aims to distinguish all segments (time windows) in a time series, since the segment indices are used as labels of the data points. In detail, TCL proceeds as follows: 1. Divide a multivariate time series xt into segments, i.e. time windows, indexed by \u03c4 = 1.., T. Any time segment method can be used, e.g. simple containers of equal size. 2. Associate each data point with the corresponding segment index, which contains the data point; i.e. the data points in the segment are all provided with the same segment designation. 3. Learn a feature extractor h (xt; \u03b8) together with an MLR with a linear regression function, the MTLD h (xt) + b."}, {"heading": "3 TCL as approximator of log-pdf ratios", "text": "Next, we show how the combination of the optimally discriminating attribute extractor and MLR learns to model the non-stationary log pdf's of the data. the back of the classes for a data point xt in the multinomial logistic regression of TCL is given by the well-known theory asp (Ct = \u03c4 | xt; \u03b8, W, b) = exp (wT\u03c4 h (xt; \u03b8) + b\u03c4) 1 + \u2211 T j = 2 exp (w T j h (xt) + bj) (1) where Ct is a class name of the data at the time t, xt is the n-dimensional data point at the time t, \u03b8 is the parameter vector of the m-dimensional attribute extractor h, W = [w1,.] where Ct is a class name of the data at the time t, xt is the n-dimensional data point at the time t, \u03b8 is the parameter vector of the m-dimensional characteristic h, W = neural."}, {"heading": "4 Nonlinear nonstationary ICA model", "text": "In this section, apparently unrelated to the previous section, we define a probabilistic generative model; the link will be explained in the next section. We assume that the observed multivariate time series xt is a smooth and invertable nonlinear mixture of a vector of source signals st = (s1 (t),.., sn (t)); in other words, xt = f (st). (4) The components si (t) in st are assumed to be mutually independent of i (but not over time t) The crucial question is how to define a suitable model for the sources that is sufficiently general while allowing strong identifiability results. Here, we start with the basic principle that the source signals si (t) in st are not stationary. For example, the variances (or similar scaling coefficients) could be changed as before in the linear case."}, {"heading": "5 Solving nonlinear ICA by TCL", "text": "For the sake of simplicity, we will consider the case qi, 0 = 0, V = 1, i.e. the exponential family has a single modulated function qi, 1 per source, and this function is the same for all sources; we will discuss the general case separately below. The modulated function is simply referred to by q: = qi, 1 in succession. First, we show that the nonlinear functions q (si), i = 1,. n, the sources are unknown linear transformations of the outputs of the feature extractor hi, which is trained by the TCL: Theorem 1. We assume that the data obtained by generative sources according to (5) and the mixing of the topics as in (4) are with a smooth uniqueness."}, {"heading": "6 Simulation on artificial data", "text": "It is not only the way in which the world places the world at the centre, but also the way in which the world places the world at the centre, and the way in which the world places the world at the centre, and the way in which the world places the world at the centre, and how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the world at the centre, how it places the world at the centre, how it places the world at the world at the world at the centre, how it places the world at the centre, how it places the world at the centre, how it places the world at the world places the world at the centre, how it places the world at the world at the centre, how it places the world at the centre, how it places the world places the world at the world at the centre, how it places, how it places the world at the world at the centre, how it places itself, how it places the world places the world at the world at the centre, how it places the world at the world at the world at the centre, how it places, how it places, how it places the world at the world places the world places"}, {"heading": "7 Experiments on real brain imaging data", "text": "In order to verify the vulnerability of the TCL method to real data, we have taken a series of measures aimed at the vulnerabilities of people. (...) We have taken a series of measures aimed at the vulnerabilities of people. (...) We have taken a series of measures aimed at the vulnerabilities of people. (...) We have taken a set of measures aimed at the vulnerabilities of people. (...) \"We have taken a set of measures aimed at the vulnerabilities of people.\" (...) \"We have taken a set of measures aimed at the vulnerabilities of people.\" (...) \"The vulnerabilities.\" (...) We have taken a set of measures aimed at the vulnerabilities. \"(...)\" The vulnerabilities. \"(...) The vulnerabilities.\" The vulnerabilities. \"(...) The vulnerabilities of people. (...) The vulnerabilities. (...) The vulnerabilities of people."}, {"heading": "8 Conclusion", "text": "The resulting \"time-contrasting learning\" is easy to implement because it uses only ordinary neural network training: a multi-layered perceptron with logistic regression. However, we showed that it can estimate independent components in a nonlinear mixing model up to certain uncertainties, provided that the independent components are appropriately non-stationary. The uncertainties include linear mixing (which can be resolved by another linear ICA step) and component-by-component inconsistencies, such as squares or absolute values. TCL also avoids the composition of the jacobic gradient, which is a major problem with maximum probability that ICA variations in the MEZ modality continue. Our developments also provide by far the strongest evidence of non-linear ICA in the literature."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Nonlinear independent component analysis (ICA) provides an appealing framework<lb>for unsupervised feature learning, but the models proposed so far are not identifiable.<lb>Here, we first propose a new intuitive principle of unsupervised deep learning<lb>from time series which uses the nonstationary structure of the data. Our learning<lb>principle, time-contrastive learning (TCL), finds a representation which allows<lb>optimal discrimination of time segments (windows). Surprisingly, we show how<lb>TCL can be related to a nonlinear ICA model, when ICA is redefined to include<lb>temporal nonstationarities. In particular, we show that TCL combined with linear<lb>ICA estimates the nonlinear ICA model up to point-wise transformations of the<lb>sources, and this solution is unique \u2014 thus providing the first identifiability result<lb>for nonlinear ICA which is rigorous, constructive, as well as very general.", "creator": "LaTeX with hyperref package"}}}