{"id": "1605.06353", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction", "abstract": "In this work, we study parameter tuning towards the M$^2$ metric, the standard metric for automatic grammar error correction (GEC) tasks. After implementing M$^2$ as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task. We notice erratic behavior when optimizing sparse feature weights with M$^2$ and offer partial solutions. To our surprise, we find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning outperforms all previously published results for the CoNLL-2014 test set by a large margin (46.37% M$^2$ over previously 40.56%, by a neural encoder-decoder model) while being trained on the same data. Our newly introduced dense and sparse features widen that gap, and we improve the state-of-the-art to 49.49% M$^2$.", "histories": [["v1", "Fri, 20 May 2016 13:43:56 GMT  (141kb,D)", "http://arxiv.org/abs/1605.06353v1", null], ["v2", "Wed, 5 Oct 2016 08:42:23 GMT  (156kb,D)", "http://arxiv.org/abs/1605.06353v2", "Accepted for publication at EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marcin junczys-dowmunt", "roman grundkiewicz"], "accepted": true, "id": "1605.06353"}, "pdf": {"name": "1605.06353.pdf", "metadata": {"source": "META", "title": "Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction", "authors": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz", "Adam Mickiewicz"], "emails": ["junczys@amu.edu.pl", "romang@amu.edu.pl"], "sections": [{"heading": "1 Introduction", "text": "Statistical machine translation (SMT), especially the phrase-based variant, seems to be well established in the field of automatic grammatical error correction (GEC) and systems that are either pure SMT or incorporate SMT as system components. With the recent paradigm shift in machine translation to neural translation models, these models are expected to appear in the field of GEC as well, and the first published results (Xie et al., 2016) already mark the new state of the art for GEC. As is the case in classic bilingual machine translation research, these models should be compared with strong SMT baselines. In this paper, we are trying to identify these baselines. During our initial experiments, we note - to our surprise - that a naked phrase sentence-based system outperforms the best published results on the CoNLL-2014 test by a significant margin."}, {"heading": "2 Previous Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The CoNLL-2014 Shared Task", "text": "While machine translation for GEC has already been used in works by Brockett et al. (2006), we begin our discussion with the CoNLL-2014 shared taskar Xiv: 160 5.06 353v 1 [cs.C L] 20 May 2 (Ng et al., 2014), where for the first time an unlimited number of errors had to be fully corrected. Previous work, especially during the CoNLL Sharedtask 2013 (Ng et al., 2013), focused only on five selected error types, but machine translation approaches (Yoshimoto et al., 2013; Yuan and Felice, 2013) were also used. The goal of the joint task CoNLL-2014 was to evaluate algorithms and systems for automatic correction of grammatical errors in essays written by foreign-language English speakers. Grammar errors of 28 types were targeted. Participating teams were given training data with manual corrections of publicly available grammatical errors."}, {"heading": "2.2 Aftermath", "text": "Shortly after the joint task, Susanto et al. (2014) published a paper on GEC system combinations. They combined the results of a classification-based system with an SMT-based system using MEMT (Heafield and Lavie, 2010) and reported on new state-of-the-art results for the CoNLL-2014 test set. Xie et al. (2016) present a neural network-based approach for GEC. Their method is based on a character encoder decoder recurrent neural network with an attention mechanism. They use data from the public long-8 corpus and combine their model with a n-grammar model trained on web-scaled common crawl data. Adding synthesized erroneous data, they obtain the best published results for the CoNLL-2014 test set with position data so far away. In Figure 1, we give a graphical overview of the published results for the CoNL test set 2014, which are limited to these results we use in comparison with Google's position data."}, {"heading": "3 Dense feature optimization", "text": "Moses has tools that can adjust parameter vectors according to various MT tuning metrics. Prior to the work, Moses was used with default settings: minimal error rate training (Och, 2003) towards BLEU (Papineni et al., 2002). BLEU was never designed for grammatical error correction; we find that direct optimization for M2 works much better."}, {"heading": "3.1 Tuning towards M2", "text": "The M2 measurement metric (Dahlmeier and Ng, 2012) is an FScore based on the machining extracted from a Levenshtein distance matrix. For the joint task CoNLL-2014, the \u03b2 parameter was set to 0.5, giving twice as much weight to precision as the recall. Junczys-Dowmunt and Grundkiewicz (2014) have shown that coordination with BLEU is counterproductive in an environment where M2 is the evaluation metric. In systems that are inherently weak, this can lead to all correction attempts being disabled, but MERT then learns to prevent all changes by reducing the similarity with the reference determined by BLEU. Systems with better training data can be matched with BLEU without suffering this \"deactivating\" effect, but will not achieve optimum performance. Susanto et al. (2014) the CoNT measurement weights of their SMT systems correspond to the two BLT measurement metrics in 2013."}, {"heading": "3.2 Dense features", "text": "In a GEC environment, the most natural units seem to be minimal editing operations that can either be counted or modeled in context with varying degrees of generalization. Thus, the decoder can be informed at multiple levels of abstraction about how the output differs from the input. 1 In this section, we implement several functions that attempt to capture these operations in isolation and context."}, {"heading": "3.2.1 Stateless features", "text": "Our stateless features are calculated when creating translation options before they are decoded, thus modelling relationships between source and target phrases. They are designed to extend the default SMT specification. We believe this is important information that is not yet mastered in neural encoder decoder approaches. MLE-based phrase and word translation probabilities include meaningful phrase-level information about the correction process. Levenshtein removals use Junczys-Dowmunt and Grundkiewicz (2014) as a translation model feature, the word-based Levenshtein distance between source and target phrases, Felice et al. (2014) experiment independently with a character-based version.Edit operation counts Levenshtein removable features with edit operation counts. Based on this blank count, see the number of blank formats in the number of the blank count."}, {"heading": "3.2.2 Stateful features", "text": "Unlike stateless features, stateless features can look at translation hypotheses outside their own span and take advantage of the constructed target context. Typical stateless features are language models. In this section, we discuss LM-like features about editing operations. Operation Sequence Model. Durrani et al. (2013) introduce Operation Sequence Models to Moses. These models are Markov translation models that can be interpreted in our environment as Markov editing models. Translations between identical words are matches, translations with different words on the source and target pages are substitutions; insertions and deletions are interpreted in the same way as SMT. Lapses, jumps, and other operations typical of OSMs do not appear as we have deactivated.Word class language models. The monolingual Wikipedia data was used to create a 9 gram word class model of 200 language levels with 2 word classes (dolov) produced in 2013."}, {"heading": "3.3 Training and Test Data", "text": "The training data provided in both tasks is the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013). NUCLE consists of 1,414 essays written by Singapore students who are not native English speakers, covering a wide range of topics such as pollution, health care, etc. Grammar errors in these essays were corrected by hand by professional English teachers and commented with one of the 28 predefined error types. A further 50 essays, collected and commented on similarly to NUCLE, were used as blind test data in both CoNLL GEC tasks. The CoNLL 2014 test was commented by two human annotators, the CoNLL-2013 by an annotator. Many participants in the CoNLL 2014 joint task used the 2013 test set as a development kit for their systems."}, {"heading": "3.4 Experiments", "text": "Our system is based on the sentence based on the statistical translation system Moses et al., 2007). Only the pure text data is used for the language model and the translation model. External linguistic knowledge is introduced during the coordination process. (Gao and Vogel, 2008). We limit ourselves to the training of models with the standard Moses. No new models are used. (Gao and Vogel, 2008). We limit ourselves to the pun with the model 1 and 5."}, {"heading": "3.5 Larger development sets", "text": "It is no less important than choosing the right tuning metric is a good choice of the development sentence. Among the MT research, there are a number of more or less known truths about suitable development sets for translation-oriented settings: They usually consist of between 2000 and 3000 sentences, they should be a good representation of test data, sparse features require more sentences or more references, and it is unclear how to quantify them in the context of grammar correction. In addition, the error rate in this sentence is 1380 sentences that are hardly sufficient for a translation-related task, and it is unclear how to quantify them in the context of grammar correction."}, {"heading": "4 Sparse Features", "text": "We saw that the introduction of finer-grained machining operations improved performance, and the natural evolution of this idea are features that describe specific correction operations with and without context, which can be achieved with sparse features, but the matching of sparse features according to the M2 metric poses unexpected problems."}, {"heading": "4.1 Optimizing for M2 with PRO and Mira", "text": "The MERT tool included in Moses cannot handle parameter tuning with sparse feature weights, and one of the other optimizers available in Moses only needs to be used for dense features. We initially experimented with PRO (Hopkins and May, 2011) and Batch Mira (Cherry and Foster, 2012) and found PRO and Batch Mira with default settings to either perform significantly worse than MERT or suffer from instability related to different test sets (Table 3). However, experiments with Mira hyper parameters allowed to counteract these effects. We first change the BLEU approximation method in Batch Mira to use model-best hypotheses (--model-bg) that seem to provide more satisfactory results. However, verification of the tuning process also shows problems with this setting. Figure 3 shows how unstable the tuning process with Mira is in order to bridge iterations."}, {"heading": "4.2 Sparse edit operations", "text": "Our sparse editing operations are again based on the Levenshtein removal matrix and count specific edits that are commented on with the source and the target markers involved in the editing. Err: Then, a new problem arises for the following erroneous / corrected sentence pairs. Cor: Therefore, we create sparse features that are edited without context (matches are omitted): subst (Then, Hence) = 1 subst (comes, surfaces) = 1 del (out) = sparse features with one-sided left or right or two-sided editing."}, {"heading": "5 Adding a web-scale language model", "text": "So far, we have limited our experiments to data used by Susanto et al. (2014), but systems from CoNLL-2014 were free to use all publicly available data, e.g. Junczys-Dowmunt and Grundkiewicz (2014) use an n-gram language model trained by Common Crawl. Xie et al. (2016) achieved the best published result for the task (prior to this paper) by integrating a similar n-gram language model with their neural action.Table 4 summarizes the best results reported in this paper for the CoNLL-2014 test theorem (2014 column) before and after the addition of the n-gram language model Common Crawl. Vanille Moses \"baseline with the Common Crawl model can be considered a new simple starting point for unrestricted settings and is ahead of all previously published results. The combination of sparse features and monolingual data on a web scale marks our best result and exceeds the previously published results by 16% (a relative 9% improvement)."}, {"heading": "6 Conclusions", "text": "Despite the fact that statistical machine translation approaches are among the most popular methods of automatic grammatical error correction, few papers that provide results for the CoNLL-2014 test set seem to have reached their full potential. An important aspect of training SMT systems to match model parameters to task evaluation measurement does not seem to have been explored to date. We have shown that a pure SMT system with correct parameter adjustment actually achieves the best results for each paradigm in GEC when performing correct parameter adjustments. This tuning mechanism explores task-specific features that bring further significant improvements, making phrase-based SMT considerably superior to other approaches. None of the features studied requires complicated pipelines or re-ranking mechanisms. Instead, they are a natural part of the log-linear model in phrase-based test systems (hence, our 2014 SMT is right for presenting new SMT results)."}, {"heading": "Acknowledgments", "text": "The authors thank Colin Cherry for his help with Batch Mira hyper-parameters and Kenneth Heafield for many helpful comments and discussions. This work was partly funded by the Polish National Science Center (grant number 2014 / 15 / N / ST6 / 02330)."}], "references": [{"title": "Correcting ESL errors using phrasal SMT techniques", "author": ["Chris Brockett", "William B. Dolan", "Michael Gamon."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational", "citeRegEx": "Brockett et al\\.,? 2006", "shortCiteRegEx": "Brockett et al\\.", "year": 2006}, {"title": "How far are we from fully automatic high quality grammatical error correction", "author": ["Christopher Bryant", "Hwee Tou Ng"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-", "citeRegEx": "Bryant and Ng.,? \\Q2015\\E", "shortCiteRegEx": "Bryant and Ng.", "year": 2015}, {"title": "N-gram counts and language models from the Common Crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas van Ooyen."], "venue": "Proceedings of the Language Resources and Evaluation Conference, pages 3579\u2013 3584, Reykjav\u0131\u0301k, Iceland.", "citeRegEx": "Buck et al\\.,? 2014", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "Methods for smoothing the optimizer instability in SMT", "author": ["Mauro Cettolo", "Nicola Bertoldi", "Marcello Federico."], "venue": "MT Summit XIII: the Thirteenth Machine Translation Summit, pages 32\u201339.", "citeRegEx": "Cettolo et al\\.,? 2011", "shortCiteRegEx": "Cettolo et al\\.", "year": 2011}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Colin Cherry", "George Foster."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages", "citeRegEx": "Cherry and Foster.,? 2012", "shortCiteRegEx": "Cherry and Foster.", "year": 2012}, {"title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "author": ["Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-", "citeRegEx": "Clark et al\\.,? 2011", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Better evaluation for grammatical error correction", "author": ["Daniel Dahlmeier", "Hwee Tou Ng."], "venue": "Proceedings of the 2012 Conference of the North American Chapter", "citeRegEx": "Dahlmeier and Ng.,? 2012", "shortCiteRegEx": "Dahlmeier and Ng.", "year": 2012}, {"title": "Building a large annotated corpus of learner english: The NUS Corpus of Learner English", "author": ["Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu."], "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 22\u2013", "citeRegEx": "Dahlmeier et al\\.,? 2013", "shortCiteRegEx": "Dahlmeier et al\\.", "year": 2013}, {"title": "Can markov models over minimal translation units help phrase-based smt? In ACL (2), pages 399\u2013405", "author": ["Nadir Durrani", "Alexander Fraser", "Helmut Schmid", "Hieu Hoang", "Philipp Koehn."], "venue": "The Association for Computer Linguistics.", "citeRegEx": "Durrani et al\\.,? 2013", "shortCiteRegEx": "Durrani et al\\.", "year": 2013}, {"title": "Grammatical error correction using hybrid systems and type filtering", "author": ["Mariano Felice", "Zheng Yuan", "\u00d8istein E. Andersen", "Helen Yannakoudakis", "Ekaterina Kochmar."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Lan-", "citeRegEx": "Felice et al\\.,? 2014", "shortCiteRegEx": "Felice et al\\.", "year": 2014}, {"title": "Parallel implementations of word alignment tool", "author": ["Qin Gao", "Stephan Vogel."], "venue": "Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 49\u201357. ACL.", "citeRegEx": "Gao and Vogel.,? 2008", "shortCiteRegEx": "Gao and Vogel.", "year": 2008}, {"title": "Combining machine translation output with open source: The Carnegie Mellon multi-engine machine translation scheme", "author": ["Kenneth Heafield", "Alon Lavie."], "venue": "The Prague Bulletin of Mathematical Linguistics, 93:27\u201336.", "citeRegEx": "Heafield and Lavie.,? 2010", "shortCiteRegEx": "Heafield and Lavie.", "year": 2010}, {"title": "Tuning as ranking", "author": ["Mark Hopkins", "Jonathan May."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201911, pages 1352\u20131362, Stroudsburg, USA. Association for Computational Linguistics.", "citeRegEx": "Hopkins and May.,? 2011", "shortCiteRegEx": "Hopkins and May.", "year": 2011}, {"title": "The amu system in the conll-2014 shared task: Grammatical error", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2014", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "The effect of learner corpus size in grammatical error correction of ESL writings", "author": ["Tomoya Mizumoto", "Yuta Hayashibe", "Mamoru Komachi", "Masaaki Nagata", "Yu Matsumoto."], "venue": "Proceedings of COLING 2012, pages 863\u2013872.", "citeRegEx": "Mizumoto et al\\.,? 2012", "shortCiteRegEx": "Mizumoto et al\\.", "year": 2012}, {"title": "The CoNLL2013 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault."], "venue": "Proceedings of the 17th Conference on Computational Natural Language Learning: Shared Task, pages 1\u2013", "citeRegEx": "Ng et al\\.,? 2013", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "The CoNLL-2014 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natu-", "citeRegEx": "Ng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL \u201903, pages 160\u2013167, Stroudsburg, USA. Association for Compu-", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311\u2013318, Stroudsburg, USA.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "The Illinois-Columbia system in the CoNLL-2014 shared task", "author": ["Alla Rozovskaya", "Kai-Wei Chang", "Mark Sammons", "Dan Roth", "Nizar Habash."], "venue": "CoNLL2014, pages 34\u201342.", "citeRegEx": "Rozovskaya et al\\.,? 2014", "shortCiteRegEx": "Rozovskaya et al\\.", "year": 2014}, {"title": "System combination for grammatical error correction", "author": ["Hendy Raymond Susanto", "Peter Phandi", "Tou Hwee Ng."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 951\u2013962. Association for Computa-", "citeRegEx": "Susanto et al\\.,? 2014", "shortCiteRegEx": "Susanto et al\\.", "year": 2014}, {"title": "Neural language correction with character-based attention", "author": ["Ziang Xie", "Anand Avati", "Naveen Arivazhagan", "Dan Jurafsky", "Andrew Y. Ng."], "venue": "CoRR, abs/1603.09727.", "citeRegEx": "Xie et al\\.,? 2016", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "NAIST at 2013 CoNLL grammatical error correction shared task", "author": ["Ippei Yoshimoto", "Tomoya Kose", "Kensuke Mitsuzawa", "Keisuke Sakaguchi", "Tomoya Mizumoto", "Yuta Hayashibe", "Mamoru Komachi", "Yuji Matsumoto."], "venue": "Proceedings of the 17th", "citeRegEx": "Yoshimoto et al\\.,? 2013", "shortCiteRegEx": "Yoshimoto et al\\.", "year": 2013}, {"title": "Constrained grammatical error correction using statistical machine translation", "author": ["Zheng Yuan", "Mariano Felice."], "venue": "Proceedings of the 17th Conference on Computational Natural Language Learning: Shared Task, pages 52\u201361, Sofia, Bulgaria. Association for", "citeRegEx": "Yuan and Felice.,? 2013", "shortCiteRegEx": "Yuan and Felice.", "year": 2013}], "referenceMentions": [{"referenceID": 22, "context": "With the recent paradigm shift in machine translation towards neural translation models, neural encoder-decoder models are expected to appear in the field of GEC as well, and first published results (Xie et al., 2016) already mark the new state-of-theart for GEC.", "startOffset": 199, "endOffset": 217}, {"referenceID": 0, "context": "While machine translation has been used for GEC in works as early as Brockett et al. (2006), we start our discussion with the CoNLL-2014 shared task ar X iv :1 60 5.", "startOffset": 69, "endOffset": 92}, {"referenceID": 17, "context": "(Ng et al., 2014) where for the first time an unrestricted set of errors had to be fully corrected.", "startOffset": 0, "endOffset": 17}, {"referenceID": 16, "context": "Previous work, most notably during the CoNLL sharedtask 2013 (Ng et al., 2013), concentrated only on five selected errors types, but machine translation approaches (Yoshimoto et al.", "startOffset": 61, "endOffset": 78}, {"referenceID": 23, "context": ", 2013), concentrated only on five selected errors types, but machine translation approaches (Yoshimoto et al., 2013; Yuan and Felice, 2013) were used as well.", "startOffset": 93, "endOffset": 140}, {"referenceID": 24, "context": ", 2013), concentrated only on five selected errors types, but machine translation approaches (Yoshimoto et al., 2013; Yuan and Felice, 2013) were used as well.", "startOffset": 93, "endOffset": 140}, {"referenceID": 6, "context": "The corrected system outputs were evaluated blindly using the MaxMatch (M2) metric (Dahlmeier and Ng, 2012).", "startOffset": 83, "endOffset": 107}, {"referenceID": 9, "context": "Among the top-three positioned systems, two submissions \u2014 CAMB (Felice et al., 2014) and AMU (JunczysDowmunt and Grundkiewicz, 2014) \u2014 were partially or fully based on SMT.", "startOffset": 63, "endOffset": 84}, {"referenceID": 20, "context": "The second system, CUUI (Rozovskaya et al., 2014), was a classifierbased approach, another popular paradigm in GEC.", "startOffset": 24, "endOffset": 49}, {"referenceID": 11, "context": "They combined the output from a classificationbased system and a SMT-based system using MEMT (Heafield and Lavie, 2010), reporting new state-of-the-art results for the CoNLL-2014 test set.", "startOffset": 93, "endOffset": 119}, {"referenceID": 20, "context": "Shortly after the shared task, Susanto et al. (2014) published a work on GEC systems combinations.", "startOffset": 31, "endOffset": 53}, {"referenceID": 21, "context": "Positions marked with (r) use only data restricted data set which corresponds to the data set used by Susanto et al. (2014). Positions with (u) Co NL L 20 14 (u)", "startOffset": 102, "endOffset": 124}, {"referenceID": 22, "context": "make use of web-scale data, this corresponds to the data used in Xie et al. (2016). We marked the participants of the CoNLL-2014 shared task as unrestricted as some participants made use of Common Crawl data or Google n-grams.", "startOffset": 65, "endOffset": 83}, {"referenceID": 18, "context": "Prior work used Moses with default settings: minimum error rate training (Och, 2003) towards BLEU (Papineni et al.", "startOffset": 73, "endOffset": 84}, {"referenceID": 19, "context": "Prior work used Moses with default settings: minimum error rate training (Och, 2003) towards BLEU (Papineni et al., 2002).", "startOffset": 98, "endOffset": 121}, {"referenceID": 6, "context": "The M2 metric (Dahlmeier and Ng, 2012) is an FScore, based on the edits extracted from a Levenshtein distance matrix.", "startOffset": 14, "endOffset": 38}, {"referenceID": 4, "context": "Based on Clark et al. (2011) concerning the effects of optimizer instability, we report results averaged over five tuning runs.", "startOffset": 9, "endOffset": 29}, {"referenceID": 3, "context": "Additionally, we compute parameter weight vector centroids as suggested by Cettolo et al. (2011). They showed that parameter vector centroids averaged over several tuning runs yield similar to or better than average results and reduce variance.", "startOffset": 75, "endOffset": 97}, {"referenceID": 12, "context": "Junczys-Dowmunt and Grundkiewicz (2014) use word-based Levenshtein distance between source and target phrases as a translation model feature, Felice et al.", "startOffset": 0, "endOffset": 40}, {"referenceID": 9, "context": "Junczys-Dowmunt and Grundkiewicz (2014) use word-based Levenshtein distance between source and target phrases as a translation model feature, Felice et al. (2014) independently experiment with a character-based version.", "startOffset": 142, "endOffset": 163}, {"referenceID": 8, "context": "Durrani et al. (2013) introduce Operation Sequence Models in Moses.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "The monolingual Wikipedia data has been used create a 9-gram wordclass language model with 200 word-classes produced by word2vec (Mikolov et al., 2013).", "startOffset": 129, "endOffset": 151}, {"referenceID": 7, "context": "The training data provided in both shared tasks is the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013).", "startOffset": 93, "endOffset": 117}, {"referenceID": 15, "context": "Parallel data for translation model training is adapted from the above mentioned NUCLE corpus and the publicly available Lang-8 corpus (Mizumoto et al., 2012).", "startOffset": 135, "endOffset": 158}, {"referenceID": 20, "context": "As mentioned before, in order to make our results comparable to previous work, we report main results using similar training data as Susanto et al. (2014). We refer to this setting that as the \u201cresticted-data setting\u201d (r).", "startOffset": 133, "endOffset": 155}, {"referenceID": 12, "context": "As mentioned above, to demonstrate the ease of data integration we propose an \u201cunrestricted setting\u201d (u) based on the data used in Junczys-Dowmunt and Grundkiewicz (2014), one of the shared task submissions, and later in Xie et al.", "startOffset": 131, "endOffset": 171}, {"referenceID": 12, "context": "As mentioned above, to demonstrate the ease of data integration we propose an \u201cunrestricted setting\u201d (u) based on the data used in Junczys-Dowmunt and Grundkiewicz (2014), one of the shared task submissions, and later in Xie et al. (2016). We use Common Crawl data made-available by Buck et al.", "startOffset": 131, "endOffset": 239}, {"referenceID": 2, "context": "We use Common Crawl data made-available by Buck et al. (2014).", "startOffset": 43, "endOffset": 62}, {"referenceID": 10, "context": "The translation model is built with the standard Moses training script, word-alignment models are produced with MGIZA++ (Gao and Vogel, 2008), we restrict the word alignment training to 5 iterations of Model 1 and 5 iterations of the HMM-Model.", "startOffset": 120, "endOffset": 141}, {"referenceID": 21, "context": "We first successfully reproduce results from Susanto et al. (2014) for BLEU-based tuning on the CoNLL-2013 test set as the development set (Fig.", "startOffset": 45, "endOffset": 67}, {"referenceID": 21, "context": "We first successfully reproduce results from Susanto et al. (2014) for BLEU-based tuning on the CoNLL-2013 test set as the development set (Fig. 2a) using similar training data. Repeated tuning places the scores reported by Susanto et al. (2014) for their SMT-ML combinations (37.", "startOffset": 45, "endOffset": 246}, {"referenceID": 21, "context": "Since Susanto et al. (2014) do not report results for multiple tuning steps, the extend of influence of optimizer instability on their experiments remains unclear.", "startOffset": 6, "endOffset": 28}, {"referenceID": 21, "context": "Since Susanto et al. (2014) do not report results for multiple tuning steps, the extend of influence of optimizer instability on their experiments remains unclear. Even with BLEU-based tuning, we can see significant improvements when replacing Levenshtein distance with the finer-grained edit operations, and another performance jump with additional stateful features. The value range of the different tuning runs for the last feature set includes the currently bestperforming system (Xie et al. (2016) with 40.", "startOffset": 6, "endOffset": 503}, {"referenceID": 21, "context": "Until now, we followed the seemingly obvious approach from Susanto et al. (2014) to tune on the CoNLL-2013 test set.", "startOffset": 59, "endOffset": 81}, {"referenceID": 12, "context": "We first experimented with both, PRO (Hopkins and May, 2011) and Batch Mira (Cherry and Foster, 2012), for the dense features only, and found PRO and Batch Mira with standard settings to either severely underperform in comparison to MERT or to suffer from instability with regard to different test sets (Table 3).", "startOffset": 37, "endOffset": 60}, {"referenceID": 4, "context": "We first experimented with both, PRO (Hopkins and May, 2011) and Batch Mira (Cherry and Foster, 2012), for the dense features only, and found PRO and Batch Mira with standard settings to either severely underperform in comparison to MERT or to suffer from instability with regard to different test sets (Table 3).", "startOffset": 76, "endOffset": 101}, {"referenceID": 20, "context": "Until now we restricted our experiments to data used by Susanto et al. (2014). However, systems from the CoNLL-2014 were free to use any publicly available data, for instance Junczys-Dowmunt and Grundkiewicz (2014) make use of an n-gram language model trained from Common Crawl.", "startOffset": 56, "endOffset": 78}, {"referenceID": 13, "context": "However, systems from the CoNLL-2014 were free to use any publicly available data, for instance Junczys-Dowmunt and Grundkiewicz (2014) make use of an n-gram language model trained from Common Crawl.", "startOffset": 96, "endOffset": 136}, {"referenceID": 13, "context": "However, systems from the CoNLL-2014 were free to use any publicly available data, for instance Junczys-Dowmunt and Grundkiewicz (2014) make use of an n-gram language model trained from Common Crawl. Xie et al. (2016) reach the best published result for the task (before this work) by integrating a similar n-gram language model with their neural approach.", "startOffset": 96, "endOffset": 218}, {"referenceID": 1, "context": "2 According to the Bryant and Ng (2015), human annotators seem to reach on average 72.", "startOffset": 19, "endOffset": 40}, {"referenceID": 1, "context": "See Bryant and Ng (2015) for a re-assessment of the CoNLL-2014 systems with this extended test set.", "startOffset": 4, "endOffset": 25}], "year": 2017, "abstractText": "In this work, we study parameter tuning towards the M2 metric, the standard metric for automatic grammar error correction (GEC) tasks. After implementing M2 as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task. We notice erratic behavior when optimizing sparse feature weights with M2 and offer partial solutions. To our surprise, we find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning outperforms all previously published results for the CoNLL-2014 test set by a large margin (46.37% M2 over previously 40.56%, by a neural encoder-decoder model) while being trained on the same data. Our newly introduced dense and sparse features widen that gap, and we improve the state-ofthe-art to 49.49% M2.", "creator": "LaTeX with hyperref package"}}}