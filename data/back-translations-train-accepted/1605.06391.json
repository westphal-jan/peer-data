{"id": "1605.06391", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "histories": [["v1", "Fri, 20 May 2016 15:05:58 GMT  (273kb,D)", "http://arxiv.org/abs/1605.06391v1", "Technical report, 14 pages, 6 figures"], ["v2", "Thu, 16 Feb 2017 20:40:41 GMT  (252kb,D)", "http://arxiv.org/abs/1605.06391v2", "9 pages, Accepted to ICLR 2017 Conference Track. This is a conference version of the paper. For the multi-domain learning part (not in this version), please refer tothis https URL"]], "COMMENTS": "Technical report, 14 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yongxin yang", "timothy hospedales"], "accepted": true, "id": "1605.06391"}, "pdf": {"name": "1605.06391.pdf", "metadata": {"source": "CRF", "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "authors": ["Yongxin Yang", "Timothy M. Hospedales"], "emails": ["t.hospedales}@qmul.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "The paradigm of multi-task learning consists in learning several related tasks at the same time, so that the knowledge gained from each task can be reused by the others. Early work in this area focused on neural network models [Caruana, 1997], while newer methods shifted the focus to core methods, thrift and low-dimensional task representations of linear models [Evgeniou and Pontil, 2004, Argyriou et al., 2008, Kumar and Daume III, 2012]. Nevertheless, given the impressive practical effectiveness of contemporary neural networks (DNNs) in many important applications, we are motivated to reconsider MTL from a deep learning perspective. While the machine learning community has focused on flat linear models, applications have recently continued to share the neural networks MTL [Zhang et al., 2014, Liu et al., 2015]. The typical design pattern dates back at least 20 years to 1997, when Caruana shared structures with a lower representation."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Multi-Task Learning", "text": "Most contemporary MTL algorithms assume that both input and model are D-dimensional vectors, and the models of T tasks can then be stacked into a D x T matrix W. Despite different motivations and implementations, many matrix-based MTL methods work by imposing constraints on W. For example, a \"2.1 standard for W\" is established to encourage low-level W [Argyriou et al., 2008]. Similarly, in [Kumar and Daume \"III, 2012] W is classified as W = LS, i.e. a lower rank is assigned as hyperparameter. An earlier paper [Evgeniou and Pontil, 2004] suggests that the linear model can be written for each task t as wt = w = t + w = 0. This is the factorization L = [w-0, w-1,]."}, {"heading": "2.2 Tensor Factorisation", "text": "In deep learning, tensor factorisation was used to take advantage of the fewer parameters of factored tensors than the original (e.g. 4-way convolutionary core) tensor, thus compressing and / or accelerating the model, e.g. [Lebedev et al., 2015, Novikov et al., 2015]. For flat linear MTL, tensor factorisation was used to address problems where tasks are described by multiple independent factors, rather than simply indexed by a single factor [Yang and Hospedales, 2015]. Here, the D-dimensional linear models for all unique tasks pile up to a tensor W, e.g. D \u00d7 T1 \u00d7 T2 in the case of two task factors. Knowledge transfer is then achieved by imposing tensor standards onW [Romera-Paredes et al., 2013, Wimalawarne et al., 2014], the tensor factors \u00b7 DDN for different reasons."}, {"heading": "2.3 Heterogeneous MTL and DNNs", "text": "Some studies consider heterogeneous MTL, where tasks may have a different number of output layers [Caruana, 1997], which differs from the previous studies [Evgeniou and Pontil, 2004, Argyriou et al., 2008, Bonilla et al., 2007, Jacob et al., 2009, Kumar and Daume \u0301 III, 2012, Romera-paredes et al., 2013, Wimalawarne et al., 2014], which implicitly assume that each task has a single output. Heterogeneous MTL typically uses neural networks with multiple outputs and losses. For example, [Zhang et al., 2014] uses a DNN to find facial features (regression) and recognize facial features (classification); while [Liu et al., 2015] suggests a DNN for query classification and information retrieval (ranking for web search). A central common feature of these studies is that they all require user-specific parameterization."}, {"heading": "2.4 Multi-Domain Learning", "text": "Multi-Domain Learning [Dredze et al., 2010, Joshi et al., 2012] is a relatively independent research direction in the field of multi-task learning. It is closely related to Domain Adaptation (DA), in particular supervises DA, where all domains have some marked data, e.g. [Saenko et al., 2010, Duan et al., 2012]. DA and MDL differ in their objectives: DA focus on improving performance in a particular target area compared to a model trained in another source, and MDL focus on simultaneously improving performance in all areas analogous to MTL. Although some MTL methods have been applied to MDL scenarios [Argyriou et al., 2008, Kumar and Daume III, 2012], they are limited to single output problems. For example, school data sets [Argyriou et al., 2007] MDL are used as a target for predicting exam outcomes of the One IS student (Output Students 2004-Assignment), where E.gyriou and M3 students are assigned to the domain three times."}, {"heading": "3 Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Preliminaries", "text": "In fact, it is the case that most of us are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that most of us are able to abide by the rules. (...) It is not the case that they abide by the rules. (...) It is not the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) (...) () () (...) () () () (...) () () () () () ()) () () ()) () () ()) () () ()) () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () ()) () () () () ()) () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () (() (() () () (() (() (() () (() () () () (() () () (() () () (() (() () () () (((() () () (() () () ((() (() () () ("}, {"heading": "3.2 Tensor Factorisation for Knowledge Sharing", "text": "Tucker Decomposition Given an N-path tensor of size D1 \u00b7 D2 \u00b7 \u00b7 \u00b7 DN, Tucker Decomposition results in a core tensor S of size K1 \u00b7 K2 \u00b7 \u00b7 \u00b7 KN and N matrices U (n) of size Dn \u00b7 \u00b7 Kn, so that Wd1, d2,..., dN = K1 \u2211 k1 = 1 K2 \u2211 k2 = 1 \u00b7 \u00b7 \u00b7 KN \u2211 kN = 1 Sk1, k2,..., kNU (1) d1, k1 U (2) d2, k2 \u00b7 dN (N), kN (3), kN (1) W = S \u00b7 U (1,2) \u00b7 \u00b7 \u00b7 \u00b7 U (N) (1,2) (4) Tucker Decomposition is usually implemented by an alternating composition of the smallest squares (ALS) or U."}, {"heading": "3.3 Deep Multi-Task Representation Learning", "text": "In fact, the fact is that most of them will be able to show themselves that they are able to move, that they are able to move."}, {"heading": "4 Experiments", "text": "Implementation Details Our method is implemented with TensorFlow [Abadi et al., 2015] The code is released on GitHub3. For DMTRL-Tucker, DMTRL-TT and DMTRLSVD, we have to rank each weight sensor. DNN architecture itself may be complicated and can therefore benefit from different ranks at different levels, but finding grids is not practical. However, since all three tensor splitting methods have SVD-based solutions, we can initialize the model and set the ranks as follows: First, we train the DNNs independently in the learning mode of a single task, then we pack the layered parameters as input for decomposition of the tensor. When SVD is applied, we set a threshold for relative errors so3https: / / github.com / wOOL / DMTRLSVD."}, {"heading": "4.1 Homogeneous MTL", "text": "The task is to recognize numeric images from zero to nine. If this data set is used for evaluation of MTL methods, each example is a monochrome image of size 28 x 28 x 1. We use a modified LeNet [Kumar et al., 1998] as the CNN architecture. The first revolutionary layer has 32 filters of size 5 x 5, followed by 2 x 2 maximum pooling. The second revolutionary layer has 64 filters of size 4 x 4, and again a 2 x 2 maximum pooling. After these two revolutionary layers we are placed two fully connected layers with 512 and 1 output (s)."}, {"heading": "4.2 Heterogeneous MTL: Face Analysis", "text": "Dataset, Settings and Baselines The AdienceFaces [Eidinger et al., 2014] is a large-format facial image dataset with the names of the gender and age group of each person. We use this dataset to evaluate heterogeneous MTL with two tasks: (i) gender classification (two classes) and (ii) age group classification (eight classes) Two independent CNN models for this benchmark are introduced in [Levi and Hassncer, 2015] The two CNNs have the same architecture except for the last fully connected layer because the heterogeneous tasks have different number of results (two / eight). We take these CNNs [Levi and Hassncer, 2015] as an STL baseline. We are again looking for the best possible user-defined MTL architecture as a strong competitor: The proposed CNN has six layers - three revolutionary and three fully connected layers."}, {"heading": "4.3 Heterogeneous MTL: Multi-Alphabet Recognition", "text": "\"I'm not going to say that I'm not going to be able to do it, but I'm going to be able to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it, and I'm going to do it.\""}, {"heading": "4.4 Multi-Domain Learning", "text": "This year it is so far that it will be able to achieve the mentioned goals, \"he said.\" We have never achieved so much as this year, \"he said.\" But we are not yet so far in a position that we will be able to achieve these goals. \""}, {"heading": "5 Conclusion", "text": "In this paper, we propose a novel framework for end-to-end multi-task representation learning in contemporary deep neural networks, the key idea being to generalize matrix factoring ideas for tensor factorization to flexibly share knowledge in fully interconnected and revolutionary DNN layers. Our methodology consistently provides better performance than learning individual tasks and comparable or better performance than the best results from exhaustive search of custom MTL architectures, reducing design choice and architectural search space that need to be studied in the workflow of deep MTL architectural design [Caruana, 1997, Zhang et al., 2014, Liu et al., 2015], thereby freeing researchers from the need to decide how to structure layer / segregation sharing on a layer-by-layer basis that also enables smooth interpolation between layers and deeper parts."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorflow.org", "author": ["Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "Yu and Zheng.,? \\Q2015\\E", "shortCiteRegEx": "Yu and Zheng.", "year": 2015}, {"title": "A spectral regularization framework for multi-task structure learning", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil", "Y. Ying"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Argyriou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "Multi-task gaussian process prediction", "author": ["E.V. Bonilla", "K.M. Chai", "C. Williams"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Bonilla et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bonilla et al\\.", "year": 2007}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Frustratingly easy domain adaptation", "author": ["H. Daum\u00e9 III"], "venue": "In ACL,", "citeRegEx": "III.,? \\Q2007\\E", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Multi-domain learning by confidence-weighted parameter combination", "author": ["M. Dredze", "A. Kulesza", "K. Crammer"], "venue": "Machine Learning,", "citeRegEx": "Dredze et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dredze et al\\.", "year": 2010}, {"title": "Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach", "author": ["L. Duan", "D. Xu", "S.-F. Chang"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Duan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2012}, {"title": "Age and gender estimation of unfiltered faces", "author": ["E. Eidinger", "R. Enbar", "T. Hassner"], "venue": "IEEE Transactions on Information Forensics and Security,", "citeRegEx": "Eidinger et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Eidinger et al\\.", "year": 2014}, {"title": "Regularized multi\u2013task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "In Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Evgeniou and Pontil.,? \\Q2004\\E", "shortCiteRegEx": "Evgeniou and Pontil.", "year": 2004}, {"title": "The indian buffet process: An introduction and review", "author": ["T.L. Griffiths", "Z. Ghahramani"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Griffiths and Ghahramani.,? \\Q2011\\E", "shortCiteRegEx": "Griffiths and Ghahramani.", "year": 2011}, {"title": "Clustered multi-task learning: A convex formulation", "author": ["L. Jacob", "J.-p. Vert", "F.R. Bach"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Jacob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2009}, {"title": "Multi-domain learning: When do domains matter", "author": ["M. Joshi", "M. Dredze", "W.W. Cohen", "C.P. Ros\u00e9"], "venue": "In Empirical Methods on Natural Language Processing (EMNLP),", "citeRegEx": "Joshi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2012}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM Review,", "citeRegEx": "Kolda and Bader.,? \\Q2009\\E", "shortCiteRegEx": "Kolda and Bader.", "year": 2009}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daum\u00e9 III"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kumar and III.,? \\Q2012\\E", "shortCiteRegEx": "Kumar and III.", "year": 2012}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "A multilinear singular value decomposition", "author": ["L.D. Lathauwer", "B.D. Moor", "J. Vandewalle"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Lathauwer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2000}, {"title": "Speedingup convolutional neural networks using fine-tuned cp-decomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I.V. Oseledets", "V.S. Lempitsky"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Lebedev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebedev et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Age and gender classification using convolutional neural networks", "author": ["G. Levi", "T. Hassncer"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "Levi and Hassncer.,? \\Q2015\\E", "shortCiteRegEx": "Levi and Hassncer.", "year": 2015}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information", "author": ["X. Liu", "J. Gao", "X. He", "L. Deng", "K. Duh", "Y.-Y. Wang"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Tensorizing neural networks", "author": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D. Vetrov"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Novikov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novikov et al\\.", "year": 2015}, {"title": "Tensor-train decomposition", "author": ["I.V. Oseledets"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Oseledets.,? \\Q2011\\E", "shortCiteRegEx": "Oseledets.", "year": 2011}, {"title": "Flexible modeling of latent task structures in multitask learning", "author": ["A. Passos", "P. Rai", "J. Wainer", "H. Daum\u00e9 III"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Passos et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2012}, {"title": "Multilinear multitask learning", "author": ["B. Romera-paredes", "H. Aung", "N. Bianchi-berthouze", "M. Pontil"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Romera.paredes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Romera.paredes et al\\.", "year": 2013}, {"title": "To transfer or not to transfer", "author": ["M.T. Rosenstein", "Z. Marx", "L.P. Kaelbling", "T.G. Dietterich"], "venue": "NIPS Workshop, Inductive Transfer: 10 Years Later,", "citeRegEx": "Rosenstein et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rosenstein et al\\.", "year": 2005}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Saenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saenko et al\\.", "year": 2010}, {"title": "Data-effiicient temporal regression with multitask recurrent neural networks", "author": ["S. Spieckermann", "S. Udluft", "T. Runkler"], "venue": "In NIPS Workshop on Transfer and Multi-Task Learning,", "citeRegEx": "Spieckermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Spieckermann et al\\.", "year": 2014}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["L.R. Tucker"], "venue": null, "citeRegEx": "Tucker.,? \\Q1966\\E", "shortCiteRegEx": "Tucker.", "year": 1966}, {"title": "Multitask learning meets tensor factorization: task imputation via convex optimization", "author": ["K. Wimalawarne", "M. Sugiyama", "R. Tomioka"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Wimalawarne et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wimalawarne et al\\.", "year": 2014}, {"title": "Multi-task learning for classification with dirichlet process priors", "author": ["Y. Xue", "X. Liao", "L. Carin", "B. Krishnapuram"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Xue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 4, "context": "Early work in this area focused on neural network models [Caruana, 1997], while more recent methods have shifted focus to kernel methods, sparsity and low-dimensional task representations of linear models [Evgeniou and Pontil, 2004, Argyriou et al.", "startOffset": 57, "endOffset": 72}, {"referenceID": 4, "context": "The typical design pattern dates back at least 20 years [Caruana, 1997]: define a DNN with shared lower representation layers, which then forks into separate layers and losses for each task.", "startOffset": 56, "endOffset": 71}, {"referenceID": 2, "context": "For example, posing an `2,1 norm on W to encourage low-rank W [Argyriou et al., 2008].", "startOffset": 62, "endOffset": 85}, {"referenceID": 9, "context": "An earlier work [Evgeniou and Pontil, 2004] proposes that the linear model for each task t can be written as wt = \u0175t + \u01750.", "startOffset": 16, "endOffset": 43}, {"referenceID": 32, "context": ", [Xue et al., 2007] assumes S\u00b7,i (the ith column of S) is a unit vector generated by a Dirichlet Process and [Passos et al.", "startOffset": 2, "endOffset": 20}, {"referenceID": 25, "context": ", 2007] assumes S\u00b7,i (the ith column of S) is a unit vector generated by a Dirichlet Process and [Passos et al., 2012] models W using linear factor analysis with Indian Buffet Process [Griffiths and Ghahramani, 2011] prior on S.", "startOffset": 97, "endOffset": 118}, {"referenceID": 10, "context": ", 2012] models W using linear factor analysis with Indian Buffet Process [Griffiths and Ghahramani, 2011] prior on S.", "startOffset": 73, "endOffset": 105}, {"referenceID": 4, "context": "3 Heterogeneous MTL and DNNs Some studies consider heterogeneous MTL, where tasks may have different numbers of outputs [Caruana, 1997].", "startOffset": 120, "endOffset": 135}, {"referenceID": 21, "context": ", 2014] uses a DNN to find facial landmarks (regression) as well as recognise facial attributes (classification); while [Liu et al., 2015] proposes a DNN for query classification and information retrieval (ranking for web search).", "startOffset": 120, "endOffset": 138}, {"referenceID": 1, "context": "For example, School dataset [Argyriou et al., 2007] is MDL as the target is to predict students\u2019 exam score (one single-output task) where students are grouped by school (multiple domains).", "startOffset": 28, "endOffset": 51}, {"referenceID": 28, "context": ", Office object recognition dataset [Saenko et al., 2010] has three domains due to three different capture devices.", "startOffset": 36, "endOffset": 57}, {"referenceID": 30, "context": "Unlike matrix factorisation, there are multiple definitions of tensor factorisation, and we use Tucker [Tucker, 1966] and Tensor Train (TT) [Oseledets, 2011] decompositions.", "startOffset": 103, "endOffset": 117}, {"referenceID": 24, "context": "Unlike matrix factorisation, there are multiple definitions of tensor factorisation, and we use Tucker [Tucker, 1966] and Tensor Train (TT) [Oseledets, 2011] decompositions.", "startOffset": 140, "endOffset": 157}, {"referenceID": 14, "context": "Tucker decomposition is usually implemented by an alternating least squares (ALS) method [Kolda and Bader, 2009].", "startOffset": 89, "endOffset": 112}, {"referenceID": 17, "context": "However [Lathauwer et al., 2000] treat it as a higher-order singular value decomposition (HOSVD), which is more efficient to solve: U (n) is exactly the U matrix from the SVD of mode-n flattening W(n) of W, and the core tensor S is obtained by, 1We slightly abuse \u2018-1\u2019 referring to the last axis of the tensor.", "startOffset": 8, "endOffset": 32}, {"referenceID": 24, "context": "The TT decomposition is typically realised with a recursive SVD-based solution [Oseledets, 2011].", "startOffset": 79, "endOffset": 96}, {"referenceID": 19, "context": "We use a modified LeNet [LeCun et al., 1998] as the CNN architecture.", "startOffset": 24, "endOffset": 44}, {"referenceID": 8, "context": "2 Heterogeneous MTL: Face Analysis Dataset, Settings and Baselines The AdienceFaces [Eidinger et al., 2014] is a largescale face images dataset with the labels of each person\u2019s gender and age group.", "startOffset": 84, "endOffset": 107}, {"referenceID": 20, "context": "Two independent CNN models for this benchmark are introduced in [Levi and Hassncer, 2015].", "startOffset": 64, "endOffset": 89}, {"referenceID": 20, "context": "We take these CNNs from [Levi and Hassncer, 2015] as the STL baseline.", "startOffset": 24, "endOffset": 49}, {"referenceID": 27, "context": "This is the negative transfer phenomenon [Rosenstein et al., 2005], where using a transfer learning algorithm is worse than not using it.", "startOffset": 41, "endOffset": 66}, {"referenceID": 16, "context": "3 Heterogeneous MTL: Multi-Alphabet Recognition Dataset, Settings and Baselines We next consider the task of learning to recognise handwritten letters in multiple languages using the Omniglot [Lake et al., 2015] dataset.", "startOffset": 192, "endOffset": 211}, {"referenceID": 22, "context": "The Street View House Numbers (SVHN) [Netzer et al., 2011] is an image dataset that contains photos of door numbers.", "startOffset": 37, "endOffset": 58}, {"referenceID": 28, "context": "Directly applying a model trained on one of these tasks to another would not yield good performance due to domain shift [Saenko et al., 2010].", "startOffset": 120, "endOffset": 141}, {"referenceID": 28, "context": "The multi-domain learning problem setting considered in this section is related to supervised domain adaptation (where all domains have labels) [Saenko et al., 2010], however the objective is to perform well in all domains rather than only on one special target domain.", "startOffset": 144, "endOffset": 165}], "year": 2016, "abstractText": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL, as well as multi-domain learning (MDL). Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "creator": "LaTeX with hyperref package"}}}