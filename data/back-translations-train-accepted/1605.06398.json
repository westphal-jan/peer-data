{"id": "1605.06398", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Stochastic Variance Reduction Methods for Saddle-Point Problems", "abstract": "We consider convex-concave saddle-point problems where the objective functions may be split in many components, and extend recent stochastic variance reduction methods (such as SVRG or SAGA) to provide the first large-scale linearly convergent algorithms for this class of problems which is common in machine learning. While the algorithmic extension is straightforward, it comes with challenges and opportunities: (a) the convex minimization analysis does not apply and we use the notion of monotone operators to prove convergence, showing in particular that the same algorithm applies to a larger class of problems, such as variational inequalities, (b) there are two notions of splits, in terms of functions, or in terms of partial derivatives, (c) the split does need to be done with convex-concave terms, (d) non-uniform sampling is key to an efficient algorithm, both in theory and practice, and (e) these incremental algorithms can be easily accelerated using a simple extension of the \"catalyst\" framework, leading to an algorithm which is always superior to accelerated batch algorithms.", "histories": [["v1", "Fri, 20 May 2016 15:16:29 GMT  (207kb)", "http://arxiv.org/abs/1605.06398v1", null], ["v2", "Thu, 3 Nov 2016 10:24:55 GMT  (211kb)", "http://arxiv.org/abs/1605.06398v2", "Neural Information Processing Systems (NIPS), 2016, Barcelona, Spain"]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["balamurugan palaniappan", "francis r bach"], "accepted": true, "id": "1605.06398"}, "pdf": {"name": "1605.06398.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 5.06 398v 1 [cs.L G] 2"}, {"heading": "1 Introduction", "text": "In this context, it should be noted that this project is a project which is, first and foremost, a project."}, {"heading": "2 Composite Decomposable Saddle-Point Problems", "text": "We consider the saddle point problem defined in Eq. (1), with the following assumptions: (A) M is strong (\u03bb, \u03b3) -convex concave, that is, the function (x, y), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), K (K), K (K, K, K, K, K (K, K, K, K, K, K, K, K (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K (K), (K), (K), (K (K), (K), (K (K), (K), (K (K), (K), (K), (K (K), (K (K), (K), (K (K), K (K (K), K (K, K, K, K, K, K, K, K, K, K, K, K, K, K, K (, K, K), K (K, K, K, K, K, K, K (K, K, K, K, K), K (K, K, K, K, K (K, K, K), K (K (K), K (K, K), K (K), K (K (K, K), K (K, K (K, K, K), K (K (K), K (K, K), K (K (K), K (K (K), K (K"}, {"heading": "2.1 Examples in machine learning", "text": "In fact, the fact is that most people are able to decide whether they want to be able or not, will be able to play by the rules."}, {"heading": "2.2 Existing batch algorithms", "text": "In this section, we review existing algorithms that aim to solve the compound saddle point problem in equation. (1), without using the sum structure. Note that it is often possible to apply batch algorithms to the associated primary or dual problems \u2212 \u2212 convergence (which are generally inseparable). Forward-backward (FB) algorithm (1). The most important iteration is (xt, yt) = prox algorithm M [(xt \u2212 1, yt \u2212 1) \u2212 convergence (1, yt \u2212 1) \u2212 B (xt \u2212 1) B (xt \u2212 1) = prox\u03c3M (xt \u2212 1 \u2212 1) xK (xt \u2212 xK (xt \u2212 1, yt \u2212 1 + \u03c31) -proxy)."}, {"heading": "2.3 Existing stochastic algorithms", "text": "Forward-backward algorithms have been investigated with additional noise [25], which results in a convergence rate in O (1 / t) after t-iterations for strongly convex-concave problems. In our setting, we replace B (x, y) in our algorithm with 1\u03c0iBi (x, y), where i-I is from the probability vector (\u03c0i) i. We have EBi (x, y) = B (x, y); the main titeration is then (xt, yt) = prox \u03c3t M [(xt \u2212 1, yt \u2212 1) \u2212 \u03c3t (1 / \u03bb 0 1 / \u03b3) 1 \u03c0it bit (xt \u2212 1, yt \u2212 1)], where it is selected independently in I with probability vector \u03c0. In Appendix C, we show that the use of \u03c3t = 2 / (t + 1 + 8L (\u03c0) 2) results in a convergence rate in O (1, yt \u2212 1), whereby below the uniformity as shown in the miniature scheme (1)."}, {"heading": "2.4 Sampling probabilities, convergence rates and running-time complexities", "text": "To characterize runtimes, we specify the complexity of the calculations A (x, y) for each operator A (x, y), while from Tprox (M) we specify the complexity of the calculations K (x, y). In our motivational example for bilinear functions K (x, y) we assume that Tprox (M) takes the time proportional to n + d and a single element of K is O (1). To characterize the convergence rate, we need the Lipschitz constant L, which was previously defined, and a smoothing constant, which is adapted to our sampling schemes: L 2 = sup (x, y, y)."}, {"heading": "3 SVRG: Stochastic Variance Reduction for Saddle Points", "text": "Following [3, 27], we will consider a stochastic variance reduced estimate of the finite sum B (x, y) = [1, i] Bi (x, y). This is achieved by assuming that we have an iterate (x, y) with a known value of B (x, y), and we will consider the estimate of B (x, y): B (x, y) + 1\u03c0iBi (x, y) \u2212 1 \u03c0i Bi (x, y), which has the correct expectation when i is sampled by I with the probability \u03c0 but with a reduced variance. Since we regularly need to reform the (x, y) proportions, the algorithm works in epochs (we allow to project m elements per update, i.e., a mini-batch of size m), with an algorithm that shares the same structure as SVRG for convex minimization."}, {"heading": "4 SAGA: Online Stochastic Variance Reduction for Saddle", "text": "\"We need to adjust that we do the values for epochs v, number of updates per iteration (mini-batch size) mSet = [L2 + 3L / m] \u2212 1 for u = 1 to v we do initialize (x, y).i\" We need to adjust to the (x, y) number of epochs per iteration (mini-batch size), we need to adjust the (x, y) number of epochs per iteration (mini-batch size) mSet = [L2 + 3L / m] \u2212 1 for u = 1 to v we do (x, y) and the (x, y) number of epochs per iteration (mini-batch size).we need to adjust to the (L2 + 3L / m) \u2212 1 for u = 1 to v we do (x, y) and the (x, y) number of epochs per iteration (x, y)."}, {"heading": "5 Acceleration", "text": "Following the \"catalyst\" of [8] we consider a sequence of saddle point problems with additional regulation; namely, given (x, y), we use SVRG to solve approximate problems (x, y) + M (x, y) + empirical (x, y) + empirical (x, y) + empirical (x, x) -empirical (x) -empirical) solutions for Saddle PointsInput: Functions (Ki) i, M, probabilities (2) i, smoothness L, iterate (x, y) number of iterations t, number of updates per iteration (mini-batch size) mSet."}, {"heading": "6 Extension to Monotone Operators", "text": "It turns out, however, that our algorithm and, more importantly, our analysis extend to all fixed monotonous operators [9, 31]. Therefore, we consider a maximum strongly monotonous operator A on a Euclidean space E as well as a finite family of Lipschitz continuous (not necessarily monotonous) operators Bi, i, I, with B = [I] Bi, monotonous. Our algorithm then finds the zeros of A + [I] Bi = A + B, from the knowledge of the resolving (\"backward\") operator Bi, i, i, I, Bi, Bi, monoton. Our algorithm then finds the zeros of A + [I, Bi] Bi, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, the zeros of Bi, i, [I, I] the zeros of Bi, i, i, [I, I] the zeros of the resolving (\"backward\") operator Bi, i, I, I, which are not applicable to the resolving (\"backward\") operators Bi, \u2212 1 (for a well-selected algorithm, i, i, >, i, and i, which are interesting)."}, {"heading": "7 Experiments", "text": "We consider binary classification problems with the design matrix K and the label vector in {\u2212 1, 1} n = 49.0, an inseparable strongly convex regulator with an efficient proximal operator (the sum of the square norm f \u2212 x \u00b2 2 / 2 and the clustering-inducing term i6 = j | xi \u2212 xj |, for which the proximal operator in O (n log \u2212 n) can be calculated by isotonic regression \u2212 n [33]) and an inseparable smooth loss (a surrogate to the area below the ROC curve, defined as proportional to the methods used i +.I \u2212 i + yj) 2, where I + / I \u2212 are sentences with positive / negative labels, for a vector of the prediction y, for which an efficient proximal operator can also be compressed, see Appendix E).Our upper bounds depend on the blebleed relation, where the K and the regularity is erected in K."}, {"heading": "8 Conclusion", "text": "We proposed the first linearly convergent incremental gradient algorithms for saddle point problems, which improve both in theory and practice over existing batch or stochastic algorithms. While we currently need to know the strong convex concavity constants, we plan to study the adaptability to these constants in the future, as already achieved in convexminimization [4], paving the way for analysis without strong convexity concavity."}, {"heading": "Acknowledgements", "text": "We would like to thank Simon Lacoste-Julien and Jalal Fadili for fruitful discussions regarding saddle point problems and monotonous operators."}, {"heading": "A Formalization through Monotone Operators", "text": "In the proofs, we will consider only maximum monotonous operators in a Euclidean space (2x). Furthermore, it is assumed that A is a \"strongly monotonous\" (equivalent to M for saddle points) and potentially fixed operators (moreover, B is monotonous and L-Lipschitz-continuous in relation to the Euclidean norm (and therefore evaluated individually). For an introduction to monotonous operators, see [9, 31]. For simplicity, we will consider only a single operator A in this appendix (the proof extends to each fixed operator A), and we will focus largely on monotonous operators (noting that the \"maximum\" operators can be strictly treated, especially to ensure that the resolute operator is defined everywhere). An operator is monotonous if and only if for all (z, z), (z \u00b2), (z \u00b2)."}, {"heading": "B Proof for Deterministic Algorithms", "text": "All the proofs in this section follow the same principle, showing that at each step of our algorithms, a certain function (a \"Lyapunov\" function) is contracted by a factor less than one. \u2212 \u2212 For the forward-backward algorithm, this is the distance to optimum (zt \u2212 1), with B being considered as a monotonous L-lipice continuous and A as a strongly monotonous. \u2212 For the forward-backward algorithm, we consider the iteration zt = (I + \u03c3A) \u2212 1 (zt \u2212 1) B (zt \u2212 1), with B being considered as a monotonous L-lipice continuous and A as strongly monotonous. \u2212 The optimal z solution (i.e.) is the zero of A + B) invariant by this iteration. Note that this is the analysis of [20] and that we could improve by putting some of the strong monotonicity in operator B rather than in A.We have:"}, {"heading": "C Proof for Existing Stochastic Algorithms", "text": "We follow [25], but with a specific step size that results in a simple result that also applies to non-uniform samples from a finite pool. (We consider the iteration zt = (I + \u03c3tA) \u2212 1 (zt \u2212 1 \u2212 0) so that ECt (z) = 0 for all z (z). We assume that all random operators Ct (1) are independent, and we rename the iteration of C1,.. (Ct), i.e., the information up to the time. (z) We assume that all random operators Ct (z) are independent, and we rename the one of C1,. (z) t-z-t-field that is by C1. (z) t-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z (z-z-z-z-z-z-z-z-z-z-z-z-t) t (z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-t) (z-z-z-z-z-z-z-z-z-z-z-t) (z-z-z-z-z-z-z-z-z-z-z-t) (z-z-z-z-z-z-z-z-z-z-z-z-t) (z-z-z-z-z-z-z-z-z-z-z-z-t) (z-z-z-z-z-z-z-z-z-z-z-z-z-t) (z-z-z-z-z-z-z-z-z-z-z-z-t) (z-z-z-z-z-z-z-z-z-z-z-z-z-t) (z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-t) t (z-z-z-z-z-z-z-z-z-z-z"}, {"heading": "D Proof for New Stochastic Algorithms", "text": "This is the only assumption we use. (b) We follow the evidence of the corresponding convex minimization algorithms, with key differences that we have highlighted below. (a) We do not use functional values, and (b) we use shorter increments to show the absence of co-coercivity.D.1 SVRG: Stochastic variance reduced saddle point problems (theorem 1) We analyze only a single epoch starting from the reference estimate z, and show that the expected square distance to the optimum is shrunk by a factor of 3 / 4 if the number of iterations per epoch is well selected. The epoch is started with z0 = z. We denote it with Ft \u2212 1 of the information until the time t \u2212 1. We consider it as sampling it1,."}, {"heading": "E Surrogate to Area Under the ROC Curve", "text": "We consider the following loss function on Rn, with a vector of positive and negative names corresponding to a convex substitute for the number of incorrectly classified pairs. (13, 14): (u) \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 n \u2212 \u2212 n \u2212 n + a = a + a = a (1 \u2212 ui \u2212 u + u + a) 2 + a (u + a) 2 + a (u + n \u2212 a + a) 2 + a (a + a) (v + a) 2 + a (u + a) 2 + a (i + \u2212 n \u2212 a) 1 \u2212 a (u + a) 2 + a (u + a) 2 + a (a + a) 12 + 1n (i + \u2212 1n \u2212 a) 1 \u2212 a (i + \u2212 n \u2212 a) 1 \u2212 u \u2212 n \u2212 u \u2212 n \u2212 a + a (u + a) 2 + a (u + a) 2 + a)."}, {"heading": "F Additional Experimental Results", "text": "We supplement the results of the main analysis in several ways: (a) by looking at all test losses, the distance to the optimal prediction method (x-x, y-y) on the log scale, and the primary-dual gaps on the log scale, as a function of the number of passes on the data. We look at the three machine learning settings: - Figure 1: sido-norm, AUC loss, and cluster standard (plus squared-norm) regularizer (both non-divisible). - Figure 2: sido dataset, quadratic loss, and AI-norma-norm (plus squared-norm) regularizer (both divisible). - Figure 3: rcv1 dataset, quadratic loss, and quadratic loss (plus squared-norm) regularizer (both divisible). We look at the following methods in all cases (all methods are performed with the respective convergence analysis): pref2 - accelerated sab:"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We consider convex-concave saddle-point problems where the objective functions may be split<lb>in many components, and extend recent stochastic variance reduction methods (such as SVRG<lb>or SAGA) to provide the first large-scale linearly convergent algorithms for this class of problems<lb>which is common in machine learning. While the algorithmic extension is straightforward, it<lb>comes with challenges and opportunities: (a) the convex minimization analysis does not apply<lb>and we use the notion of monotone operators to prove convergence, showing in particular that<lb>the same algorithm applies to a larger class of problems, such as variational inequalities, (b)<lb>there are two notions of splits, in terms of functions, or in terms of partial derivatives, (c) the<lb>split does need to be done with convex-concave terms, (d) non-uniform sampling is key to an<lb>efficient algorithm, both in theory and practice, and (e) these incremental algorithms can be<lb>easily accelerated using a simple extension of the \u201ccatalyst\u201d framework, leading to an algorithm<lb>which is always superior to accelerated batch algorithms.", "creator": "LaTeX with hyperref package"}}}