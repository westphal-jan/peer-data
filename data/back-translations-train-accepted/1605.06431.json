{"id": "1605.06431", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks", "abstract": "In this work, we introduce a novel interpretation of residual networks showing they are exponential ensembles. This observation is supported by a large-scale lesion study that demonstrates they behave just like ensembles at test time. Subsequently, we perform an analysis showing these ensembles mostly consist of networks that are each relatively shallow. For example, contrary to our expectations, most of the gradient in a residual network with 110 layers comes from an ensemble of very short networks, i.e., only 10-34 layers deep. This suggests that in addition to describing neural networks in terms of width and depth, there is a third dimension: multiplicity, the size of the implicit ensemble. Ultimately, residual networks do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network - rather, they avoid the problem simply by ensembling many short networks together. This insight reveals that depth is still an open research question and invites the exploration of the related notion of multiplicity.", "histories": [["v1", "Fri, 20 May 2016 16:44:03 GMT  (139kb,D)", "http://arxiv.org/abs/1605.06431v1", null], ["v2", "Thu, 27 Oct 2016 00:43:58 GMT  (348kb,D)", "http://arxiv.org/abs/1605.06431v2", "NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG cs.NE", "authors": ["andreas veit", "michael j wilber", "serge j belongie"], "accepted": true, "id": "1605.06431"}, "pdf": {"name": "1605.06431.pdf", "metadata": {"source": "CRF", "title": "Residual Networks are Exponential Ensembles of Relatively Shallow Networks", "authors": ["Andreas Veit", "Michael Wilber", "Serge Belongie"], "emails": ["sjb344}@cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "Most of the time it was that you were able to go to another city where you had gone to another city in order to stay there."}, {"heading": "2 Related Work", "text": "The sequential and hierarchical computer vision pipeline, which regulates its expressiveness and performance, has long been a hierarchical process from the analysis of simple to complex networks; this formalism is based on the discovery of the receptive field [10], which characterizes the visual system as a hierarchical and forward-looking system, widely used in early visual domains and sensitive to basic visual features such as edges and bars. Neurons in the deeper layers of the hierarchy capture basic shapes, and even deeper neurons react to complete objects. This organization has been widely adopted in computer vision and machine learning literature, from early neural networks such as the Neocognitron [4] and the traditional hand-made characteristics of the Malik and Perona pipeline [15] to revolutionary neural networks [14, 13]. Recent strong results from very deep neural networks [18, 20] have led to the general perception that it is about the depth of neural expression and their networks."}, {"heading": "3 The unraveled view of residual networks", "text": "To better understand residual meshes, we present a formulation that makes it easier to think about their recursive nature. Consider a residual network with three blocks from input y0 to output y3. Equation (1) gives a recursive definition of residual meshes. The output of each stage is based on the combination of two subterms. We can reveal the split structure of the residual mesh by unfolding the recursion into an exponential number of nested terms that extend a layer in each substitution step: y3 + f3 (y2) = y1 (y1) + f3 (y1) + f2 (y1)."}, {"heading": "4 Lesion study", "text": "In this section, we will use three lesion studies to show that residual networks behave like ensembles. All experiments are conducted at test time on CIFAR-10 [12]. Experiments on ImageNet [2] show similar results. We train residual networks according to the standard procedure with the same training strategy, data set enlargement and learning rate policy as [6]. For our experiments, we train a 110-layer (54-module) residual network with pre-activation modules that include batch normalization as a first step. It should be noted that we did not apply a specific training strategy to adjust the network. Specifically, we did not use disturbances such as stochastic depth during training."}, {"heading": "4.1 Experiment: Deleting individual layers from neural networks at test time", "text": "As a motivating experiment, we will show that not all transformations within a residual network are necessary by deleting individual modules from the neural network after it has been fully trained, removing the residual module from a single module, leaving the skip link (or downsampling projection, if any) untouched. That is, we change yi = yi \u2212 1 + fi (yi \u2212 1) to y \u2032 i = yi \u2212 1. We can measure the importance of each module by varying which residual module we remove. Compared to conventional conventional conventional conventional neural networks, we train a 15-layer VGG network by setting the number of channels to 128 for all layers to allow the removal of all layers. It is unclear whether any neural network can withstand such a drastic change in the model structure. We expect them to break up because dropping any layer dramatically alters the input distribution of all subsequent layers."}, {"heading": "4.2 Experiment: Deleting many modules from residual networks at test-time", "text": "One of the characteristics of ensembles is that their performance smoothly depends on the number of members. Consequently, we expect the test time performance of residual networks to correlate with the number of valid paths in the ensemble, which means that errors should decrease smoothly as we delete more residual modules. In fact, we observe that if we delete an increasing number of residual models, the error increases smoothly, as shown in Figure 5 (a). Residual networks share this property with ensembles, which strengthens our hypotheses. If you delete k residual modules from a network that originally had length n, the number of valid paths in the ensemble decreases to O (2n \u2212 k). For example, the original network started with 54 blocks, deleting 10 levels leaves 244 paths in the ensemble. Although the ensemble is now a factor of about 10 \u2212 6 of its original size, there are still many valid paths and errors by 0.2."}, {"heading": "4.3 Experiment: Reordering modules in residual networks at test-time", "text": "In our previous experiments, it was all about dropping layers that have the effect of removing paths from the exponential ensemble. In this experiment, we look at changing the structure of the network by rearranging the blocks, removing some paths and inserting new paths into the ensemble that the network has never seen during training. In particular, it shifts transformations at the high level ahead of transformations at the low level. To reorder the network, we swap randomly sampled pairs of blocks with compatible dimensions and ignore modules that perform downsampling. We draw errors in relation to the Kendall-tau rank coefficient, which measures the extent of corruption. The results are shown in Figure 5 (b). As corruption increases, the error also increases smoothly. This result is surprising because it suggests that residual networks can be reconfigured to a limited extent at runtime. All of these experiments strongly suggest that residual networks share features with ensembles."}, {"heading": "5 The ensemble of relatively shallow networks", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "6 Discussion", "text": "The removal of residual modules from a network of length n, but the proportion of the remaining paths per path length x is illustrated by the fraction of the remaining paths of length x = (n \u2212 d x) (8), the proportion of the remaining paths after the deletion of 1, 10 and 20 modules from a 54 module network. It turns out that the deletion of residual modules mostly affects the long paths. Even after the deletion of 10 residual modules, many of the effective paths between 5 and 17 modules are still valid."}, {"heading": "7 Conclusion", "text": "In the latest iteration of residual networks, He et al. [6] claims: \"We get these results from a simple but fundamental concept - which goes deeper, and these results show the potential to push the boundaries of depth.\" We now know that this is not entirely true. Residual networks push the boundaries of network diversity, not network depth. Our proposed unraveled view and lesion study show that residual networks are an implicit interaction of exponentially many networks. Moreover, the paths through the network that contribute to a gradient are shorter than expected because deep paths do not show gradients during training due to dwindling gradients. If most paths that contribute to a gradient are very short compared to the overall depth of the network, increased depth alone cannot be the key feature of residual networks. We now believe that the multiplicity, the expressive capacity of the network, plays a key role in terms of the number of paths."}, {"heading": "Acknowledgements", "text": "We would like to thank Sam Kwak and Theofanis Karaletsos for their helpful feedback, which is partly funded by AOL through the Connected Experiences Laboratory (Author 1), an NSF Graduate Research Fellowship (NSF DGE-1144153, Author 2) and a Google Focused Research Award (Author 3)."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Boosting and other ensemble methods", "author": ["Harris Drucker", "Corinna Cortes", "Lawrence D. Jackel", "Yann LeCun", "Vladimir Vapnik"], "venue": "Neural Computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["Kunihiko Fukushima"], "venue": "Biological Cybernetics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1980}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["Sepp Hochreiter"], "venue": "Master\u2019s thesis, Institut fur Informatik, Technische Universitat,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["David H Hubel", "Torsten N Wiesel"], "venue": "The Journal of Physiology,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1962}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Preattentive texture discrimination with early vision mechanisms", "author": ["Jitendra Malik", "Pietro Perona"], "venue": "Journal of the Optical Society of America,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "The strength of weak learnability", "author": ["Robert E Schapire"], "venue": "Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "A feedforward architecture accounts for rapid categorization", "author": ["Thomas Serre", "Aude Oliva", "Tomaso Poggio"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Recently proposed residual networks [5, 6] challenge this conventional view in three ways.", "startOffset": 36, "endOffset": 42}, {"referenceID": 5, "context": "Recently proposed residual networks [5, 6] challenge this conventional view in three ways.", "startOffset": 36, "endOffset": 42}, {"referenceID": 12, "context": "This is contrary to architectures like AlexNet [13] and even biological systems [17] that can capture complex concepts within half a dozen layers.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "This is contrary to architectures like AlexNet [13] and even biological systems [17] that can capture complex concepts within half a dozen layers.", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "This is surprising because removing a layer from a traditional architecture such as VGG [18] leads to a dramatic loss in performance.", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "On the one hand, residual network performance improves with adding more and more layers [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "This formalism is based on the discovery of the receptive field [10], which characterizes the visual system as a hierarchical and feedforward system.", "startOffset": 64, "endOffset": 68}, {"referenceID": 3, "context": "This organization has been widely adopted in the computer vision and machine learning literature, from early neural networks such as the Neocognitron [4] and the traditional hand-crafted feature pipeline of Malik and Perona [15] to convolutional neural networks [14, 13].", "startOffset": 150, "endOffset": 153}, {"referenceID": 14, "context": "This organization has been widely adopted in the computer vision and machine learning literature, from early neural networks such as the Neocognitron [4] and the traditional hand-crafted feature pipeline of Malik and Perona [15] to convolutional neural networks [14, 13].", "startOffset": 224, "endOffset": 228}, {"referenceID": 13, "context": "This organization has been widely adopted in the computer vision and machine learning literature, from early neural networks such as the Neocognitron [4] and the traditional hand-crafted feature pipeline of Malik and Perona [15] to convolutional neural networks [14, 13].", "startOffset": 262, "endOffset": 270}, {"referenceID": 12, "context": "This organization has been widely adopted in the computer vision and machine learning literature, from early neural networks such as the Neocognitron [4] and the traditional hand-crafted feature pipeline of Malik and Perona [15] to convolutional neural networks [14, 13].", "startOffset": 262, "endOffset": 270}, {"referenceID": 17, "context": "The recent strong results of very deep neural networks [18, 20] led to the general perception that it is the depth of neural networks that govern their expressive power and performance.", "startOffset": 55, "endOffset": 63}, {"referenceID": 18, "context": "The recent strong results of very deep neural networks [18, 20] led to the general perception that it is the depth of neural networks that govern their expressive power and performance.", "startOffset": 55, "endOffset": 63}, {"referenceID": 4, "context": "Residual networks [5, 6] are neural networks in which each layer consists of a residual module fi and a shortcut connection.", "startOffset": 18, "endOffset": 24}, {"referenceID": 5, "context": "Residual networks [5, 6] are neural networks in which each layer consists of a residual module fi and a shortcut connection.", "startOffset": 18, "endOffset": 24}, {"referenceID": 10, "context": "where fi(x) is some sequence of convolutions, batch normalization [11], and Rectified Linear Units (ReLU) as nonlinearities.", "startOffset": 66, "endOffset": 70}, {"referenceID": 5, "context": "In the most recent formulation of residual networks [6], fi(x) is defined by fi(x) \u2261 Wi \u00b7 \u03c3(B(W \u2032 i \u00b7 \u03c3(B(x)))), (2)", "startOffset": 52, "endOffset": 55}, {"referenceID": 21, "context": "For example, Zeiler and Fergus [23] visualize convolutional filters to unveil the concepts learned by individual neurons.", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": "[21] investigate the function learned by neural networks and how small changes in the input called adversarial examples can lead to large changes in the output.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22], which performs lesion studies on AlexNet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Though boosting has been used in the past [16], one simple approach is to arrange a committee [3] of neural networks in a simple voting scheme, where the final output predictions are averaged.", "startOffset": 42, "endOffset": 46}, {"referenceID": 2, "context": "Though boosting has been used in the past [16], one simple approach is to arrange a committee [3] of neural networks in a simple voting scheme, where the final output predictions are averaged.", "startOffset": 94, "endOffset": 97}, {"referenceID": 12, "context": "Top performers in several competitions use this technique almost as an afterthought [13, 6, 18], but it is rarely thought of as an explicit part of the model structure.", "startOffset": 84, "endOffset": 95}, {"referenceID": 5, "context": "Top performers in several competitions use this technique almost as an afterthought [13, 6, 18], but it is rarely thought of as an explicit part of the model structure.", "startOffset": 84, "endOffset": 95}, {"referenceID": 17, "context": "Top performers in several competitions use this technique almost as an afterthought [13, 6, 18], but it is rarely thought of as an explicit part of the model structure.", "startOffset": 84, "endOffset": 95}, {"referenceID": 6, "context": "[7] show that dropping out individual neurons during training leads to a network which is equivalent to averaging over an ensemble of exponentially many networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Similar in spirit, stochastic depth [9] trains an ensemble of networks by dropping out entire layers during training.", "startOffset": 36, "endOffset": 39}, {"referenceID": 11, "context": "All experiments are performed at test time on CIFAR-10 [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 1, "context": "Experiments on ImageNet [2] show comparable results.", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "We train residual networks according to the standard procedure with the same training strategy, dataset augmentation, and learning rate policy as [6].", "startOffset": 146, "endOffset": 149}, {"referenceID": 7, "context": "In particular, the length of the paths through the network affects the gradients that can get propagated though them [8, 1].", "startOffset": 117, "endOffset": 123}, {"referenceID": 0, "context": "In particular, the length of the paths through the network affects the gradients that can get propagated though them [8, 1].", "startOffset": 117, "endOffset": 123}, {"referenceID": 8, "context": "Effect of stochastic depth training procedure Recently, an alternative training procedure for residual networks has been proposed, referred to as stochastic depth [9].", "startOffset": 163, "endOffset": 166}, {"referenceID": 5, "context": "[6] claim \u201cWe obtain these results via a simple but essential concept\u2014 going deeper.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "In this work, we introduce a novel interpretation of residual networks showing they are exponential ensembles. This observation is supported by a large-scale lesion study that demonstrates they behave just like ensembles at test time. Subsequently, we perform an analysis showing these ensembles mostly consist of networks that are each relatively shallow. For example, contrary to our expectations, most of the gradient in a residual network with 110 layers comes from an ensemble of very short networks, i.e., only 10-34 layers deep. This suggests that in addition to describing neural networks in terms of width and depth, there is a third dimension: multiplicity, the size of the implicit ensemble. Ultimately, residual networks do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network \u2013 rather, they avoid the problem simply by ensembling many short networks together. This insight reveals that depth is still an open research question and invites the exploration of the related notion of multiplicity.", "creator": "LaTeX with hyperref package"}}}