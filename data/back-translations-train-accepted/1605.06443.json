{"id": "1605.06443", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Structured Prediction Theory Based on Factor Graph Complexity", "abstract": "We present a general theoretical analysis of structured prediction. By introducing a new complexity measure that explicitly factors in the structure of the output space and the loss function, we are able to derive new data-dependent learning guarantees for a broad family of losses and for hypothesis sets with an arbitrary factor graph decomposition. We extend this theory by leveraging the principle of Voted Risk Minimization (VRM) and showing that learning is possible with complex factor graphs. We both present new learning bounds in this advanced setting as well as derive two new families of algorithms, \\emph{Voted Conditional Random Fields} and \\emph{Voted Structured Boosting}, which can make use of very complex features and factor graphs without overfitting. Finally, we also validate our theory through experiments on several datasets.", "histories": [["v1", "Fri, 20 May 2016 17:21:17 GMT  (1461kb,D)", "http://arxiv.org/abs/1605.06443v1", null], ["v2", "Thu, 1 Dec 2016 17:02:48 GMT  (1485kb,D)", "http://arxiv.org/abs/1605.06443v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["corinna cortes", "vitaly kuznetsov", "mehryar mohri", "scott yang"], "accepted": true, "id": "1605.06443"}, "pdf": {"name": "1605.06443.pdf", "metadata": {"source": "CRF", "title": "Structured Prediction Theory Based on Factor Graph Complexity", "authors": ["Corinna Cortes", "Vitaly Kuznetsov"], "emails": ["corinna@google.com", "vitaly@cims.nyu.edu", "mohri@cims.nyu.edu", "yangs@cims.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2 Preliminaries", "text": "It is the question whether it is a way and way, in which a certain way and way, a certain way and way, a different way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, another way and way, one way and another way, one way and one way, one way and one way, one way and one way, one way and one way and one way, one way and one way and one way, one way and one way and one way, one way and one way and one way and one way and one way, one way and one way and one way and one way and one way, one way and one way and one way and one way and one way, one way and one way and one way and one way and one way and one way, one way and one way and one way and one way and one way and one way and one way and one way, one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way, one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way, one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one way and one"}, {"heading": "3 General learning bounds for structured prediction", "text": "In this section, we present new learning guarantees for structured predictions. Our analysis is general and refers to the broad family of definitive and limited loss functions introduced in the previous section. It is also general in the sense that it applies to general hypotheses and not just to subfamilies of linear functions. For linear hypotheses, we will give a more refined analysis that applies to arbitrary norm-p regulated hypotheses. Our learning guarantees are based on a new idea of Rademacher's complexity, and they are somewhat finer than the previously known limits of Taskar et al. [2003], which were given for the specific case of hamming losses with Norm-2 regulation using coverage numbers. Our limits are data-dependent, allow a favorable dependence on scarcity and effective factor curve, and apply to general loss functions and regulations. For the same norm-2 regulation, they also allow for a logical improvement."}, {"heading": "3.1 Complexity measure", "text": "A key component of our analysis is a new data-dependent concept of complexity, which extends the classic Rademacher complexity. We define the empirical factor graph Rademacher complexity R-GS (H) of a hypothesis setH for a sample S = (x1,.., xm) and the factor graph G as follows: R-GS (H) = 1 m E [sup h-H m-Y] i = 1 x f-Fi-Y-Y-Y-Y-Y-Y-Y (xi, y)], where = (i, f, y) i-H, f-Fi, y-Yf with i, f, ys independent Rademacher random variables are uniformly distributed over {\u00b1 1}. The factor graph Rademacher complexity of H is defined for a factor graph G as expectation: RGm (H) = ES-Dm [R-GS (H)]."}, {"heading": "3.2 Generalization bounds", "text": "In this section, we introduce new margin limits for structured predictions based on the Rademacher factor chart."}, {"heading": "3.3 Special cases and comparisons", "text": "Here we discuss some specific cases of interest, for which we compare our learning guarantees with those given by previous work. Markov networks with a fixed number of substructures l, by Taskar et al. [2003], our equivalent factor allows l-nodes, and the maximum size of Yf is di = k2, if each substructure of a pair can be assigned to one of the k classes. [2003], if we apply the corollary 8 with our loss function and divide the loss by l to normalize the loss of interval [0, 1] as in [Taskar et al., 2003], we get the following explicit form of our guarantee for an additive empirical margin loss, for all h-H2: R (h), the R-addS, (h) + 4k2. \""}, {"heading": "4 Voted Risk Minimization", "text": "In many structured prediction applications, such as natural language processing and computer vision, one may wish to take advantage of very rich properties. (However, the use of rich families of hypotheses could lead to over-adaptation. (In this section, we show that it may be possible to use rich families in conjunction with simpler families, provided that less complex hypotheses are used (or that they are used with a lower mixing weight).We achieve this goal by deriving learning guarantees for ensembles of structure prediction rules that explicitly take into account the different complexities between families, which motivates the algorithms we have assumed in Section 5.Assuming that we have p families H1,...., Hp of function mapping from X \u00d7 Y to R. Define of the ensemble family F = conv (most likely pk = 1Hk), which is the function family f of the form f = implicit."}, {"heading": "5 Algorithms", "text": "In this section, we derive several structured prediction algorithms based on the VRM principle discussed in Section 4. First, we specify general convex upper limits (Section 5.1) for structured prediction losses, which, as special cases, restore the loss functions used in StructSVM [Tsochantaridis et al., 2005], Max-Margin Markov Networks [Taskar et al., 2003] and Conditional Random Field (CRF) [Lafferty et al., 2001]. Next, we present a new algorithm, Voted Conditional Random Field (VCRF) Section 5.2, with accompanying experiments as evidence for the concept. We also present another algorithm, Voted StructBoost (VStructBoost), in Appendix C."}, {"heading": "5.1 General framework for convex surrogate losses", "text": "Considering a given example (x, y) \u0394X \u00b7 Y, Figure h 7 \u2192 L (h (x), y) is typically not a convex function of h, resulting in mathematically hard optimization problems, which motivates the use of convex replacement losses. We first introduce a general formulation of replacement losses for structured prediction problems. Lemma 4. For each u \u00b2 R +, we leave \u03a6u: R \u2212 R a decreasing upper limit for v 7 \u2192 u1v \u2264 0, so u 7 \u2192 \u03a6u (v) for a fixed v. Then the following upper limit applies for each h \u00b2 H and (x, y) for each h \u2212 H \u2212 Y, L (h \u2212 x), y \u2212 additive upper limit for v \u00b2 loss."}, {"heading": "5.2 Voted Conditional Random Field (VCRF)", "text": "First, we consider the convex replacement loss on the basis of (1 + eu \u2212 v), which corresponds to the loss defined by CRF models. Using the monotonicity of the logarithm and the upper limit of the maximum by a sum results in the following upper limit of the replacement loss: max y \u2032 6 = ylog (1 + eL (y, y \") \u2212 w \u00b7 \u0441\u0442 (x, y\") = log (x, y eL (\") w \u00b7 eL, y\"), which, in combination with the VRM principle, leads to the following optimization problem: min w1mm \u0445i = 1log (\"Y eL, y\") \u2212 w \u00b7 eL \u00b7 eL (\"), which, in combination with the VRM principle, leads to the following optimization problem: min w1mm \u0445i = 1log (\") \u2212 w \u00b7 eL (\")"}, {"heading": "6 Experiments", "text": "In Appendix B, we confirm our theory by reporting experimental results that indicate that the VCRF algorithm can outperform the CRF algorithm in a number of part-of-speech (POS) datasets."}, {"heading": "7 Conclusion", "text": "We presented a general theoretical analysis of structured predictions. Data-dependent margin guarantees for structured predictions can be used to guide the design of algorithms or to determine the generalization guarantees of existing algorithms, and their explicit dependence on the properties of the factor graph used to define model decomposition and feature sparseness can help shed new light on the role that the graph and features play in generalization. Our extension of VRM theory to structured predictions provides a new analysis of generalization using a very rich set of features common in many applications, such as natural language processing, leading to new algorithms, VCRF and VStructBoost. Our preliminary experimental results for VCRF serve as concept proof and motivate for more comprehensive empirical and algorithmic studies of these two algorithm families."}, {"heading": "A Proofs", "text": "At (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um) h (um)"}, {"heading": "B Experiments", "text": "The experiments in this section are meant to serve as proof of the advantages of VRM type regulation as proposed by the theory developed in this thesis. We leave an extensive experimental study on other aspects of our theory, including general loss functions, convex surrogates and p-standards, for future work results. For our experiments, we chose the part-of-speech task (POS), which consists of labeling each word with its correct part-of-speech tag. We used 10 POS datasets: Basque, Chinese, Dutch, English, Finnish FTB, Hindi, Tamil, Turkish and Twitter. The detailed description of these datasets is in Appendix B.1. Our VCRF algorithms can be applied with a variety of different families of feature functions."}, {"heading": "D Optimization solutions", "text": "(0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0).). (0). (0). (0). (0). (0). (0). (0). (0).). (0). (0). (0). (0). (0). (0). (0).). (0).). (0). (0).). (0). (0). (0).). (0). (0).). (0). (0).). (0).). (0). (0).). (0).). (0). (0).). (0). (0).). (0). (0). (0)."}, {"heading": "Appendix References", "text": "N. B. Atalay, K. Oflazer, and B. Say. The annotation process in the turkish treebank. In LINC, 2003.K. Gimpel, N. Schneider, B. O'Connor, D. Das, D. Mills, J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan, and N. A. Smith. Part-of-speech tagging for twitter: annotation, features, and experiments. In ACL, 2011.V. Koltchinskii and D. Panchenko. Empirical Margin Distributions and bounding the generalization error of combined classifiers. Annals of Statistics, 30, 2002.V. Kuznetsov, M. Mohri, and U. Syed. Multi-class deep boosting. In Proceedings of NIPS, 2014.M. Ledoux and M. Talagrand."}], "references": [{"title": "The annotation process in the turkish treebank", "author": ["N.B. Atalay", "K. Oflazer", "B. Say"], "venue": "In LINC,", "citeRegEx": "Atalay et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Atalay et al\\.", "year": 2003}, {"title": "Part-of-speech tagging for twitter: annotation, features, and experiments", "author": ["K. Gimpel", "N. Schneider", "B. O\u2019Connor", "D. Das", "D. Mills", "J. Eisenstein", "M. Heilman", "D. Yogatama", "J. Flanigan", "N.A. Smith"], "venue": "In ACL,", "citeRegEx": "Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2011}, {"title": "Empirical margin distributions and bounding the generalization error of combined classifiers", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": "Annals of Statistics,", "citeRegEx": "Koltchinskii and Panchenko.,? \\Q2002\\E", "shortCiteRegEx": "Koltchinskii and Panchenko.", "year": 2002}, {"title": "Multi-class deep boosting", "author": ["V. Kuznetsov", "M. Mohri", "U. Syed"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Kuznetsov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kuznetsov et al\\.", "year": 2014}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes", "author": ["M. Ledoux", "M. Talagrand"], "venue": null, "citeRegEx": "Ledoux and Talagrand.,? \\Q1991\\E", "shortCiteRegEx": "Ledoux and Talagrand.", "year": 1991}, {"title": "Foundations of Machine Learning", "author": ["M. Mohri", "A. Rostamizadeh", "A. Talwalkar"], "venue": null, "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "Building a turkish treebank", "author": ["K. Oflazer", "B. Say", "D.Z. Hakkani-T\u00fcr", "G. T\u00fcr"], "venue": "In Text, Speech and Language Technology,", "citeRegEx": "Oflazer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Oflazer et al\\.", "year": 2003}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["O. Owoputi", "B. O\u2019Connor", "C. Dyer", "K. Gimpel", "N. Schneider", "N.A. Smith"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Owoputi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Owoputi et al\\.", "year": 2013}, {"title": "Chinese treebank 6.0 LDC2007T36", "author": ["M. Palmer", "N. Xue", "F. Xia", "F.-D. Chiou", "Z. Jiang", "M. Chang"], "venue": "Web Download,", "citeRegEx": "Palmer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Palmer et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": "All of these margin bounds can be extended to hold uniformly over \u03c1 \u2208 (0, 1] at the price of an additional term of the form \u221a (log log2 2 \u03c1 )/m in the bound, using known techniques (see for example [Mohri et al., 2012]).", "startOffset": 198, "endOffset": 218}, {"referenceID": 3, "context": "In that case, for linear hypotheses, the complexity term of our bound varies as O(\u039b2r2 \u221a c/\u03c12m) for example for norm-2 regularization (Corollary 9), which improves upon the best known general margin bounds of Kuznetsov et al. [2014], who provide a guarantee that scales linearly with the number of classes instead.", "startOffset": 209, "endOffset": 233}, {"referenceID": 3, "context": "In that case, for linear hypotheses, the complexity term of our bound varies as O(\u039b2r2 \u221a c/\u03c12m) for example for norm-2 regularization (Corollary 9), which improves upon the best known general margin bounds of Kuznetsov et al. [2014], who provide a guarantee that scales linearly with the number of classes instead. Moreover, in the special case where an individual wy is learned for each class y \u2208 [c], we retrieve the recent favorable bounds given by Lei et al. [2015], albeit with a somewhat simpler formulation.", "startOffset": 209, "endOffset": 470}, {"referenceID": 3, "context": "In that case, for linear hypotheses, the complexity term of our bound varies as O(\u039b2r2 \u221a c/\u03c12m) for example for norm-2 regularization (Corollary 9), which improves upon the best known general margin bounds of Kuznetsov et al. [2014], who provide a guarantee that scales linearly with the number of classes instead. Moreover, in the special case where an individual wy is learned for each class y \u2208 [c], we retrieve the recent favorable bounds given by Lei et al. [2015], albeit with a somewhat simpler formulation. In that case, for any (x, y), all components of the feature vector \u03a8(x, y) are zero, except (perhaps) for the N components corresponding to class y, where N is the dimension of wy. In view of that, for example for a group-norm \u2016 \u00b7 \u20162,1regularization, the complexity term of our bound varies as O(\u039br \u221a (log c)/\u03c12m), which matches the results of Lei et al. [2015] with a logarithmic dependency on c (ignoring some complex exponents of log c in their case).", "startOffset": 209, "endOffset": 877}, {"referenceID": 2, "context": "By standard Rademacher complexity bounds (Koltchinskii and Panchenko [2002]), for any \u03b4 > 0, with probability at least 1\u2212 \u03b4, the following inequality holds for all h \u2208 H: Radd \u03c1 (h) \u2264 R\u0302add S,\u03c1(h) + 2Rm(H0) + \u221a log 1\u03b4 2m ,", "startOffset": 42, "endOffset": 76}, {"referenceID": 4, "context": "Since \u03a6\u2217 is 1-Lipschitz, by Talagrand\u2019s contraction lemma (Ledoux and Talagrand [1991], Mohri et al.", "startOffset": 59, "endOffset": 87}, {"referenceID": 4, "context": "Since \u03a6\u2217 is 1-Lipschitz, by Talagrand\u2019s contraction lemma (Ledoux and Talagrand [1991], Mohri et al. [2012]), we have Rm(H0) \u2264 Rm(H1).", "startOffset": 59, "endOffset": 108}, {"referenceID": 3, "context": "The proof makes use of Theorem 1 and the proof techniques of Kuznetsov et al. [2014][Theorem 1] but requires a finer analysis both because of the general loss functions used here and because of the more complex structure of the hypothesis set.", "startOffset": 61, "endOffset": 85}, {"referenceID": 4, "context": "Most are annotated under the Universal Dependencies (UD) annotation system, with the exception of the Chinese (Palmer et al. [2007]), Turkish (Oflazer et al.", "startOffset": 111, "endOffset": 132}, {"referenceID": 4, "context": "[2007]), Turkish (Oflazer et al. [2003], Atalay et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 0, "context": "[2003], Atalay et al. [2003]), and Twitter (Gimpel et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "[2003], Atalay et al. [2003]), and Twitter (Gimpel et al. [2011], Owoputi et al.", "startOffset": 8, "endOffset": 65}, {"referenceID": 0, "context": "[2003], Atalay et al. [2003]), and Twitter (Gimpel et al. [2011], Owoputi et al. [2013]) datasets.", "startOffset": 8, "endOffset": 88}], "year": 2016, "abstractText": "We present a general theoretical analysis of structured prediction. By introducing a new complexity measure that explicitly factors in the structure of the output space and the loss function, we are able to derive new data-dependent learning guarantees for a broad family of losses and for hypothesis sets with an arbitrary factor graph decomposition. We extend this theory by leveraging the principle of Voted Risk Minimization (VRM) and showing that learning is possible with complex factor graphs. We both present new learning bounds in this advanced setting as well as derive two new families of algorithms, Voted Conditional Random Fields and Voted Structured Boosting, which can make use of very complex features and factor graphs without overfitting. Finally, we also validate our theory through experiments on several datasets.", "creator": "LaTeX with hyperref package"}}}