{"id": "1605.06465", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Swapout: Learning an ensemble of deep architectures", "abstract": "We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR-100. Swapout samples from a rich set of architectures including dropout, stochastic depth and residual architectures as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model.", "histories": [["v1", "Fri, 20 May 2016 18:39:33 GMT  (206kb,D)", "http://arxiv.org/abs/1605.06465v1", "Submitted to NIPS 2016"]], "COMMENTS": "Submitted to NIPS 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["saurabh singh", "derek hoiem", "david a forsyth"], "accepted": true, "id": "1605.06465"}, "pdf": {"name": "1605.06465.pdf", "metadata": {"source": "CRF", "title": "Swapout: Learning an ensemble of deep architectures", "authors": ["Saurabh Singh", "Derek Hoiem", "David Forsyth"], "emails": ["daf}@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "This paper describes Swapout, a stochastic training method for general deep networks. Swapout is a generalization of dropouts [17] and stochastic depths [6] methods. Dropout randomly zeroes out the results of individual units during training, while stochastic depth randomly skips entire layers. In comparison, the general swap network produces the value of each individual unit of production independently by specifying the sum of a randomly selected subset of current and all previous layers for that unit. As a result, some units in one shift can act like normal feedback-forward units, others can produce skip connections, and still others can produce a sum of several previous outputs."}, {"heading": "2 Related Work", "text": "In fact, it is the case that we will be able to look for a solution that is capable, that we are able, that we are able to find a solution that is capable of us, that we are able, that we are able to find a solution that we are able to find, that we are able to find a solution that we are able to find, that we are able to find a solution that we are able to find, that we are able to find, that we are able to find a solution."}, {"heading": "3 Swapout", "text": "It is a question of whether it is a complete network followed by a ReLU or a residual network. Swap allows individual units to be connected in the form of a controlled accyclic diagram to form the complete network model.Dropout randomly kills individual units; stochastic depth skips entire blocks of units at random. Swap-out allows individual units to be dropped or to skip individual blocks at random. Swap-out is a direct generalization of dropouts. Swap-X is the input to some network blocks that F (u'th unit) produces F (u) as output. Swap-out is a tensor of i.d Bernoulli random generalization of dropouts. Swap-X is the input to some network blocks that F (X).The u'th unit produces F (u) as output."}, {"heading": "3.1 Inference in Stochastic Networks", "text": "A swap-out-trained model represents an entire family of networks with bound parameters, where members of the family were randomly selected during the training. There are two options for conclusions: we could either replace random variables with their expected values, as in Srivastava et al. [17] (deterministic inference); alternatively, we could randomly select several members of the family and their predictions (stochastic inference); there is an important difference between swap-out and dropout; in a dropout network, one can accurately estimate expectations (as long as the network is not trained with batch normalization, below); this is because E [ReLU [\u0445F (X)]] = Reference F (X)] (callback is a tenor of Bernoulli random variables that are not trained with batch normalization); in a swap-out network, one cannot accurately estimate expectations."}, {"heading": "3.2 Baseline comparison methods", "text": "ResNets: We compare with ResNet architectures as described in [4] (referred to as v1) and in [5] (referred to as v2).Dropout: We use standard dropout (replacing Equation 3 with Equation 1).Layer Dropout: We replace Equation 3 with Y = X + 1 (1 \u00d7 1) F (X).This is a single Bernoulli random variable divided across all units. SkipForward: Equation 3 introduces two stochastic parameters: 1 and 2. We also examine and compare a simpler architecture, SkipForward, which contains only one parameter but samples from a smaller group F (u) = {X (u), F (u) (X) (X)} as shown below. Y = EX + (1 \u2212 BA) F (X) (4)"}, {"heading": "4 Experiments", "text": "In fact, it is the case that most of them are able to survive themselves, and that they are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \"\" I don't think we are able to change the world, \"he said in an interview with\" Welt am Sonntag. \"\" I don't think we are able to put the world in order, \"he said.\" I don't think we are able to put the world in order. \""}, {"heading": "5 Discussion and future work", "text": "Swap-out is a stochastic training method that shows reliable improvements in performance and leads to networks that use parameters efficiently. Relatively flat swap networks provide comparable performance for extremely deep residual networks. We have shown that different stochastic training plans induce different behaviors but have not systematically looked for the best schedule. It may be possible to achieve improvements by doing so. We have described an extremely general swap mechanism. It is easy to use Equation 2 to apply swap-out to receiving networks [19] (using several different input functions and a sufficiently general form of evolution)."}], "references": [{"title": "On the expressive power of deep architectures", "author": ["Y. Bengio", "O. Delalleau"], "venue": "Proceedings of the 22nd International Conference on Algorithmic Learning Theory,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "AISTATS,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["M. Hardt", "B. Recht", "Y. Singer"], "venue": "CoRR, abs/1509.01240,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1603.05027,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"], "venue": "CoRR, abs/1603.09382,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "CoRR, abs/1502.03167,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Deeply-supervised nets", "author": ["C.-Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "AISTATS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "CoRR, abs/1312.4400,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrent convolutional neural networks for scene parsing", "author": ["P.H. Pinheiro", "R. Collobert"], "venue": "arXiv preprint arXiv:1306.2795,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "ICLR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "NIPS,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S.E. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CoRR, abs/1409.4842,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "ICML, pages 1058\u20131066,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1301.3557,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 16, "context": "Swapout samples from a rich set of architectures including dropout [17], stochastic depth [6] and residual architectures [4, 5] as special cases.", "startOffset": 67, "endOffset": 71}, {"referenceID": 5, "context": "Swapout samples from a rich set of architectures including dropout [17], stochastic depth [6] and residual architectures [4, 5] as special cases.", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "Swapout samples from a rich set of architectures including dropout [17], stochastic depth [6] and residual architectures [4, 5] as special cases.", "startOffset": 121, "endOffset": 127}, {"referenceID": 4, "context": "Swapout samples from a rich set of architectures including dropout [17], stochastic depth [6] and residual architectures [4, 5] as special cases.", "startOffset": 121, "endOffset": 127}, {"referenceID": 16, "context": "Swapout is a generalization of dropout [17] and stochastic depth [6] methods.", "startOffset": 39, "endOffset": 43}, {"referenceID": 5, "context": "Swapout is a generalization of dropout [17] and stochastic depth [6] methods.", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "Our experimental work focuses on a version of swapout which is a natural generalization of the residual network [4, 5].", "startOffset": 112, "endOffset": 118}, {"referenceID": 4, "context": "Our experimental work focuses on a version of swapout which is a natural generalization of the residual network [4, 5].", "startOffset": 112, "endOffset": 118}, {"referenceID": 15, "context": "The number of parameters rises sharply, although recent works such as [16, 19] have addressed this by reducing the filter size [16, 19].", "startOffset": 70, "endOffset": 78}, {"referenceID": 18, "context": "The number of parameters rises sharply, although recent works such as [16, 19] have addressed this by reducing the filter size [16, 19].", "startOffset": 70, "endOffset": 78}, {"referenceID": 15, "context": "The number of parameters rises sharply, although recent works such as [16, 19] have addressed this by reducing the filter size [16, 19].", "startOffset": 127, "endOffset": 135}, {"referenceID": 18, "context": "The number of parameters rises sharply, although recent works such as [16, 19] have addressed this by reducing the filter size [16, 19].", "startOffset": 127, "endOffset": 135}, {"referenceID": 18, "context": "Such difficulties have been addressed by architectural innovations that introduce shorter paths from input to loss either directly [19, 18, 4] or with additional losses applied to intermediate layers [19, 10].", "startOffset": 131, "endOffset": 142}, {"referenceID": 17, "context": "Such difficulties have been addressed by architectural innovations that introduce shorter paths from input to loss either directly [19, 18, 4] or with additional losses applied to intermediate layers [19, 10].", "startOffset": 131, "endOffset": 142}, {"referenceID": 3, "context": "Such difficulties have been addressed by architectural innovations that introduce shorter paths from input to loss either directly [19, 18, 4] or with additional losses applied to intermediate layers [19, 10].", "startOffset": 131, "endOffset": 142}, {"referenceID": 18, "context": "Such difficulties have been addressed by architectural innovations that introduce shorter paths from input to loss either directly [19, 18, 4] or with additional losses applied to intermediate layers [19, 10].", "startOffset": 200, "endOffset": 208}, {"referenceID": 9, "context": "Such difficulties have been addressed by architectural innovations that introduce shorter paths from input to loss either directly [19, 18, 4] or with additional losses applied to intermediate layers [19, 10].", "startOffset": 200, "endOffset": 208}, {"referenceID": 4, "context": "At the time of writing, the deepest networks that have been successfully trained are residual networks (1001 layers [5]).", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "Bengio and Dellaleau argue that circuit efficiency constraints suggest increasing depth is important, because there are functions that require exponentially large shallow networks to compute [1].", "startOffset": 191, "endOffset": 194}, {"referenceID": 8, "context": "Convolutional neural networks have a long history (see the introduction of [9]).", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "[8]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "Increasing the number of layers in a network improves performance [16, 19] if the network can be trained.", "startOffset": 66, "endOffset": 74}, {"referenceID": 18, "context": "Increasing the number of layers in a network improves performance [16, 19] if the network can be trained.", "startOffset": 66, "endOffset": 74}, {"referenceID": 11, "context": "A variety of significant architectural innovations improve trainability, including: the ReLU [12, 2]; batch normalization [7]; and allowing signals to skip layers.", "startOffset": 93, "endOffset": 100}, {"referenceID": 1, "context": "A variety of significant architectural innovations improve trainability, including: the ReLU [12, 2]; batch normalization [7]; and allowing signals to skip layers.", "startOffset": 93, "endOffset": 100}, {"referenceID": 6, "context": "A variety of significant architectural innovations improve trainability, including: the ReLU [12, 2]; batch normalization [7]; and allowing signals to skip layers.", "startOffset": 122, "endOffset": 125}, {"referenceID": 17, "context": "Highway networks use gated skip connections to allow information and gradients to pass unimpeded across several layers [18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 3, "context": "Residual networks use identity skip connections to further improve training [4]; extremely deep residual networks can be trained, and perform well [5].", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "Residual networks use identity skip connections to further improve training [4]; extremely deep residual networks can be trained, and perform well [5].", "startOffset": 147, "endOffset": 150}, {"referenceID": 13, "context": "For a review of the history of random methods, see the introduction of [14], which shows that entirely randomly chosen features can produce an SVM that generalizes well.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "Randomly dropping out unit values (dropout [17]) discourages coadaptation between units.", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "Randomly skipping layers (stochastic depth) [6] during training reliably leads to improvements at test time, likely because doing so regularizes the network.", "startOffset": 44, "endOffset": 47}, {"referenceID": 19, "context": "Other successful recent randomized methods include dropconnect [20] which generalizes dropout by dropping individual connections instead of units (so dropping several connections together), and stochastic pooling [21] (which regularizes by replacing the deterministic pooling by randomized pooling).", "startOffset": 63, "endOffset": 67}, {"referenceID": 20, "context": "Other successful recent randomized methods include dropconnect [20] which generalizes dropout by dropping individual connections instead of units (so dropping several connections together), and stochastic pooling [21] (which regularizes by replacing the deterministic pooling by randomized pooling).", "startOffset": 213, "endOffset": 217}, {"referenceID": 2, "context": "Recent results show that (a) stochastic gradient descent with sufficiently few steps is stable (in the sense that changes to training data do not unreasonably disrupt predictions) and (b) dropout enhances that property, by reducing the value of a Lipschitz constant ([3], Lemma 4.", "startOffset": 267, "endOffset": 270}, {"referenceID": 18, "context": "Reliable improvements in accuracy are achievable by training distinct models (which have distinct sets of parameters), then averaging predictions [19], thereby forming an explicit ensemble.", "startOffset": 146, "endOffset": 150}, {"referenceID": 16, "context": "argue that, at test time, random values in a dropout network should be replaced with expectations, rather than taking an average over multiple instances [17] (though they use explicit ensembles, increasing the computational cost).", "startOffset": 153, "endOffset": 157}, {"referenceID": 3, "context": "a convolution followed by a ReLU or a residual network block [4].", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "Dropout has been shown to enhance the stability of stochastic gradient descent ([3], lemma 4.", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": "First, a Lipschitz constant applies to this network; second, E[||\u2207S [f ] (v) ||] \u2264 E[||\u2207G [f ] (v) ||] \u2264 L, so swapout makes stability no worse; third, we speculate light conditions on f should provide E[||\u2207S [f ] (v) ||] < E[||\u2207G [f ] (v) ||] \u2264 L, improving stability ([3] Section 4).", "startOffset": 270, "endOffset": 273}, {"referenceID": 16, "context": "[17] (deterministic inference).", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "may have overestimated how many samples are required for an accurate average, because they use distinct dropout networks in the average (Figure 11 in [17]).", "startOffset": 150, "endOffset": 154}, {"referenceID": 5, "context": "This may explain why [6] reports that dropout doesn\u2019t lead to any improvement when used in residual networks with batch normalization.", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "ResNets: We compare with ResNet architectures as described in [4](referred to as v1) and in [5](referred to as v2).", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "ResNets: We compare with ResNet architectures as described in [4](referred to as v1) and in [5](referred to as v2).", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "Model: We experiment with ResNet architectures as described in [4](referred to as v1) and in [5](referred to as v2).", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "Model: We experiment with ResNet architectures as described in [4](referred to as v1) and in [5](referred to as v2).", "startOffset": 93, "endOffset": 96}, {"referenceID": 10, "context": "For final prediction we follow a scheme similar to Network in Network [11].", "startOffset": 70, "endOffset": 74}, {"referenceID": 3, "context": "We represent the number of filters in each group as a tuple with the smallest size as (16, 32, 64) (as used in [4]for CIFAR-10).", "startOffset": 111, "endOffset": 114}, {"referenceID": 16, "context": "Note that dropout slows convergence ([17], A.", "startOffset": 37, "endOffset": 41}, {"referenceID": 3, "context": "v1 and v2 represent residual block architectures in [4] and [5] respectively.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "v1 and v2 represent residual block architectures in [4] and [5] respectively.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "Method Width #Params Error(%) ResNet v1 [4] W \u00d7 1 0.", "startOffset": 40, "endOffset": 43}, {"referenceID": 5, "context": "Linear(a, b) refers to linear interpolation from a to b from the first block to the last (see [6]).", "startOffset": 94, "endOffset": 97}, {"referenceID": 16, "context": "Compare Figure-11 of [17], which implies \u223c 50 samples are required.", "startOffset": 21, "endOffset": 25}, {"referenceID": 3, "context": "ResNets [4] contain three groups of layers with all convolutional layers in a group containing equal number of filters.", "startOffset": 8, "endOffset": 11}, {"referenceID": 19, "context": "DropConnect [20] 9.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "32 NIN [11] 8.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "81 FitNet(19) [15] 8.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "39 DSN [10] 7.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "97 Highway[18] 7.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "60 ResNet v1(110) [4] 1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 5, "context": "41 Stochastic Depth v1(1202) [6] 19.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "58 ResNet v2 (1001) [5] 10.", "startOffset": 20, "endOffset": 23}, {"referenceID": 18, "context": "It is straightforward using equation 2 to apply swapout to inception networks [19] (by using several different functions of the input and a sufficiently general form of convolution); to recurrent convolutional networks [13] (by", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "It is straightforward using equation 2 to apply swapout to inception networks [19] (by using several different functions of the input and a sufficiently general form of convolution); to recurrent convolutional networks [13] (by", "startOffset": 219, "endOffset": 223}, {"referenceID": 10, "context": "NIN [11] 35.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "68 DSN [10] 34.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "57 FitNet [15] 35.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "04 Highway [18] 32.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "39 ResNet v1 (110) [4] 1.", "startOffset": 19, "endOffset": 22}, {"referenceID": 5, "context": "22 Stochastic Depth v1 (110) [6] 1.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "58 ResNet v2 (164) [5] 1.", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "33 ResNet v2 (1001) [5] 10.", "startOffset": 20, "endOffset": 23}], "year": 2016, "abstractText": "We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR100. Swapout samples from a rich set of architectures including dropout [17], stochastic depth [6] and residual architectures [4, 5] as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model.", "creator": "LaTeX with hyperref package"}}}