{"id": "1605.06492", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Linear-Memory and Decomposition-Invariant Linearly Convergent Conditional Gradient Algorithm for Structured Polytopes", "abstract": "Recently, several works have shown that natural modifications of the classical conditional gradient method (aka Frank-Wolfe algorithm) for constrained convex optimization, provably converge with a linear rate when: i) the feasible set is a polytope, and ii) the objective is smooth and strongly-convex. However, all of these results suffer from two significant shortcomings: large memory requirement due to the need to store an explicit convex decomposition of the current iterate, and as a consequence, large running-time overhead per iteration, and worst case convergence rate that depends unfavorably on the dimension.", "histories": [["v1", "Fri, 20 May 2016 19:55:48 GMT  (275kb,D)", "http://arxiv.org/abs/1605.06492v1", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG", "authors": ["dan garber", "ofer meshi"], "accepted": true, "id": "1605.06492"}, "pdf": {"name": "1605.06492.pdf", "metadata": {"source": "CRF", "title": "Linear-memory and Decomposition-invariant Linearly Convergent Conditional Gradient Algorithm for Structured Polytopes", "authors": ["Dan Garber", "Ofer Meshi"], "emails": ["meshi}@ttic.edu"], "sections": [{"heading": null, "text": "1. Large memory requirements due to the need to store an explicit convex decomposition of the current iteration, and consequently large runtime expenditures per iteration. the convergence rate in the worst-case scenario depends unfavorably on the dimensionIn this paper, we present a new conditional gradient variant and a corresponding analysis that improves the two above-mentioned shortcomings. Specifically: 1. Memory and computing effort are only linear in dimension. if the optimal solution is sparse, the new convergence rate replaces a factor that is at least linear in dimension in previous work with a linear dependence on the number of non-zeros in the optimal solution. at the core of our method and the corresponding analysis is a novel method for calculating decomposition invariant pathways. While our theoretical guarantees are not applicable to polyopes, they apply to several important structured polyopes that capture key concepts in graphs, perfectly structured in mathematical tasks structured by the formations."}, {"heading": "1 Introduction", "text": "The reason for this is that most people who are in the United States are not in a position to abide by the rules. (...) It is not the case that they are able to understand the rules. (...) It is not the case that they abide by the rules. (...) It is not the case that they abide by the rules. (...) It is not the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) (...) (...) () (...) () () (() () (()) () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () (() () () (() (() (() () () (() () () (() () ((() (() (()) (() (() (() ((())) (((() ()) (((()) ((()) ()) ((((())) (((()) ((((())) ((((()))) (((((((((())))) ((("}, {"heading": "1.1 Organization of the paper", "text": "The rest of this paper is organized as follows: In Section 2, we will give preliminary work and notation and present the exact setting that is taken into account in this paper. In Section 3, we will briefly introduce the conditional gradient method and its previous path-based variants, and introduce our new method: a composition-invariant pair-conditioned gradient algorithm. In this section, we will also present our main theorem, which describes in detail the novel convergence rate of our method. In Section 4, we will briefly describe some important polytopes that fall within our assumptions, and detail the application of our method for optimizing over these polytopes. In Section 5, we will give a complete analysis of our method and prove the main theorem. In Section 6, we will describe in detail how we can apply our approach to a broader class of polytopes, although in this case we will not supplement our algorithm with a convergence rate result. We will also show that our requirement that the objective function be strongly convective, and that our results may be based on a broader class of 7, and that we will apply our method to a more or less optimal one in a broader range of functions."}, {"heading": "2 Preliminaries", "text": "Definition 1. We say that a function f (x): Rn \u2192 R \u03b1-strong convex (x) + \u03b1-strong convex w.r.t. is a norm if it applies to all x, y-Rn. Definition 2. We say that a function f (x): Rn \u2192 R \u03b2-smooth w.r.t. is a norm if it applies to all x, y-Rn. Definition 2. We say that a function f (x): Rn \u2192 R \u03b2-smooth w.r.t. is a norm if x, y-Rn applies to all."}, {"heading": "2.1 Setting", "text": "In this thesis, we consider the following optimization problem: min x-P f (x).We make the following assumptions about f and P: \u2022 f (x) is \u03b1-strongly convex and \u03b2-smooth in relation to the \"2 standard.\" \u2022 P is a polytopic that fulfills the following two properties: 1. P can be algebraically described as P = {x-Rn | x-0, Ax = b}. 2. All vertices of P lie on the hypercube {0, 1}.We let x-x denote the (unique) minimizer of f over P, and we let D denote the euclidean diameter of P, namely D = maxx, y-P-x-y-y-y. We let V denote the theorem of P where, according to our assumptions, it denotes the minimizer of f over P, and we note that V-nodes in relation to the polymetry [0, 1} nodes of P, namely, maxx-y-y-P."}, {"heading": "3 Our Approach", "text": "In order to better communicate our ideas, we first briefly present the standard conditional gradient method and its accelerated path-based variants. In Section 3.1, we discuss both the advantages and disadvantages of these path-based variants. Then, in Section 3.2, we present our new method, a decomposition invariant path-based conditional gradient algorithm, and discuss how it remedies the major deficiencies of earlier path-based variants."}, {"heading": "3.1 The conditional gradient method and acceleration via away-steps", "text": "In fact, most of them will be able to play by the rules they have established over the last five years."}, {"heading": "3.2 A new decomposition-invariant pairwise conditional gradient method", "text": "Our most important observation is that in many cases of interest, if you specify a practicable iteration xt = > iteration xt (=), you can actually calculate an optimal step away from xt without relying on any single specific decomposition. \u2212 This observation allows us to overcome both of the main drawbacks of previous path-step-based CG variants. Our algorithm, which we call decomposition invariant pair conditional gradients (DICG), is below in algorithm 3.Algorithm 3 decomposition invariant pair conditional gradient1: Input: Step sequence: Step width (sequence) not defined. \u2212 t 1: Let x0 be an arbitrary point in P 3. \u2212 x1: arg minv-invariant conditional gradient1: the step width."}, {"heading": "1 for the step-size, and suppose that", "text": "The following sequence of theorem 1 shows that the so-called duality gap, defined as gt: = (xt \u2212 v + t) \u00b7 f (xt), which serves as a certificate for the sub-optimality of the iterations of algorithm 3, also corresponds to a linear rate. Consequence 1 is that the iteration t of algorithm 3 converts the dual gap: = (xt \u2212 v + t) \u00b7 f (xt) \u00b7 f (xt) \u00b7 f (xt) \u00b7 f (xt) \u00b7 f (xt) \u00b7 f (x) converts the convergence of the iterations of algorithm 3."}, {"heading": "4 Examples of Polytopes", "text": "In this section we will address several important examples of structured polytopes, which encompass the assumptions of two different levels. (That is, we extend the application practice from algorithm 3 up to optimization of algorithms 3 up to these polytopes.S The application practice from algorithm 3 up to the individual elements in the linear objective of all standard base vectors in Rn is easy to verify that D = 2.Linear minimization above the simplex level is trivial and can be performed from a single pass over the non-null elements in the linear objective.) In particular, the calculation v \u2212 t in algorithm 3 amounts simply to searching for the largest (signed) entry (xt), which corresponds to a non-zero entry in xt, and is thus even more efficient than the calculation of the standard CG direction +.Flow"}, {"heading": "5 Analysis", "text": "In this section, we will analyze the performance of algorithm 3 and prove theorem 1. During this section, we will have ht denote the approximation error of algorithm 3 to iteration t for each t \u2265 1, i.e. ht = f (xt) \u2212 f (x \u0445)."}, {"heading": "5.1 Feasibility of the iterates generated by Algorithm 3", "text": "We start by proving that the iteration of algorithm 3 is always feasible. While feasibility is simple if we use the line search option to specify the step size (option 2), it is less obvious if we use the first option. Suppose that on any iteration t of algorithm 3 the iteration xt is feasible, and that the step size is selected using option 1. Suppose that for all i [n] for which xt (i) 6 = 0 it applies that xt (i), then the following iteration xt + 1 is also feasible, and that the step size is selected using option 1. Based on the optimality of v \u2212 t, it follows that for all i), if xt (i) 6 = 0 (note in particular that any vertex with positive weighting in any convex must meet this condition."}, {"heading": "5.2 Bounding the per-iteration error-reduction of Algorithm 3", "text": "The following technical problem is the derivation of linear convergence rates in our method and in particular the derivation of improved dependence on the performance of x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "5.3 Proof of Theorem 1", "text": "We will first prove the convergence rate given in the theorem, assuming that all iterations are practicable, and then we will show that the iterations are practicable for our choice of step variables. We will now prove by induction that there is c0, c1 such that for all t \u00b2 -1 it applies that ht \u2264 c0 (1 \u2212 c1) t \u2212 1. (5) Using Lemma 3 and the production hypothesis, we must now determine that the induction holds for some t \u00b2 -1. Let us know that the induction for some t \u00b2 -1 (1 \u2212 c1) t \u00b2 -0 \u00b2 -1 \u2212 c1 \u00b7 t \u2212 2 \u00b7 is the production hypothesis we have."}, {"heading": "6 Extensions", "text": "In this section we explain two extensions of our result: i) loosening the specific structure of the polytopic P considered in Section 2.1 and ii) loosening the strong convexity requirement for the objective function f (x)."}, {"heading": "6.1 Extension of Algorithm 3 to arbitrary polytopes", "text": "In this subsection, we assume that we have an efficient way to evaluate the A2x vector, which is indeed the case for most structured polytopes."}, {"heading": "6.2 Relaxing the strong convexity of the objective function", "text": "So far, we have assumed that the objective function f is strongly convex. However, as can be seen from our analysis, the only consequence of the strong convexity we have relied on in our analysis is the equation. (1) In fact, there are functions that are not strongly convex, i.e. still fulfill equation under certain conditions. (1) and are therefore compatible with our method and analysis. Following the work of Beck and Shtern [2], we can look at a broader class of objective functions, namely functions that take the following form: f (x) = g (Ax) + b \u00b7 x, (8), where A (Rm) \u00b7 n and g: Rm \u2192 R are smooth and strongly convex. In [2] (Lemma 2.5), an application of Hoffman's Lemma has shown that there is a constant equation that depends both on the conditional number g and on the parameters A, b, so that it extends for every possible point."}, {"heading": "7 Lower Bound for Problems with a Sparse Solution", "text": "In this section, we present a simple lower limit for the approximation error of, informally speaking, any natural conditional gradient variant, which, when initialized with a vertex of the realizable set, after t-iterations permits a convex decomposition to a maximum of t + 1 vertex of the polyp. That is, at most a single new vertex is added to the decomposition on each iteration. The lower limit indicates that there is a 1-smooth and 1-strongly convex function f, for which any such CG variant applied to the minimization of f via the unit simplex must take steps before entering the linear convergence regime. To date, none of the previous analyses of linear convergence CG variants corresponds to this lower limit, since in this exact setting they require all the steps that x x require before entering the linear convergence regime."}, {"heading": "8 Experiments", "text": "In this context, it should be noted that this is a very complex and lengthy matter."}], "references": [{"title": "Linear convergence of a modified frank-wolfe algorithm for computing minimum-volume enclosing ellipsoids", "author": ["S. Damla Ahipasaoglu", "Peng Sun", "Michael J. Todd"], "venue": "Optimization Methods and Software,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Linearly convergent away-step conditional gradient for non-strongly convex functions", "author": ["Amir Beck", "Shimrit Shtern"], "venue": "arXiv preprint arXiv:1504.05002,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A conditional gradient method with linear rate of convergence for solving convex linear systems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "Math. Meth. of OR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Lifted coordinate descent for learning with trace-norm regularization", "author": ["Miroslav Dud\u0301\u0131k", "Z\u00e4\u0131d Harchaoui", "J\u00e9r\u00f4me Malick"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "An algorithm for quadratic programming", "author": ["M. Frank", "P. Wolfe"], "venue": "Naval Research Logistics Quarterly,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1956}, {"title": "A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": "CoRR, abs/1301.4666,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Faster rates for the frank-wolfe method over strongly-convex sets", "author": ["Dan Garber", "Elad Hazan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Some comments on Wolfe\u2019s \u2018away step", "author": ["Jacques Gu\u00e9Lat", "Patrice Marcotte"], "venue": "Mathematical Programming,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1986}, {"title": "Large-scale image classification with trace-norm regularization", "author": ["Z\u00e4\u0131d Harchaoui", "Matthijs Douze", "Mattis Paulin", "Miroslav Dud\u0301\u0131k", "J\u00e9r\u00f4me Malick"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Projection-free online learning", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Variance-reduced and projection-free stochastic optimization", "author": ["Elad Hazan", "Haipeng Luo"], "venue": "CoRR, abs/1602.02101,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Revisiting frank-wolfe: Projection-free sparse convex optimization", "author": ["Martin Jaggi"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Sulovsk\u00fd. A simple algorithm for nuclear norm regularized problems", "author": ["Martin Jaggi", "Marek"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Efficient image and video co-localization with Frank-Wolfe algorithm", "author": ["Armand Joulin", "Kevin Tang", "Li Fei-Fei"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "An affine invariant linear convergence analysis for frank-wolfe algorithms", "author": ["Simon Lacoste-Julien", "Martin Jaggi"], "venue": "CoRR, abs/1312.7864,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "On the global linear convergence of Frank-Wolfe optimization variants", "author": ["Simon Lacoste-Julien", "Martin Jaggi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Blockcoordinate frank-wolfe optimization for structural svms", "author": ["Simon Lacoste-Julien", "Martin Jaggi", "Mark W. Schmidt", "Patrick Pletscher"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "The complexity of large-scale convex programming under a linear optimization oracle", "author": ["Guanghui Lan"], "venue": "CoRR, abs/1309.5550,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A hybrid algorithm for convex semidefinite optimization", "author": ["S\u00f6ren Laue"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Constrained minimization methods", "author": ["Evgeny S Levitin", "Boris T Polyak"], "venue": "USSR Computational mathematics and mathematical physics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1966}, {"title": "Minding the gaps for block frank-wolfe optimization of structured svm", "author": ["Anton Osokin", "Jean-Baptiste Alayrac", "Puneet K. Dokania", "Simon Lacoste-Julien"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Combinatorial Optimization - Polyhedra and Efficiency", "author": ["A. Schrijver"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Large-scale convex minimization with a low-rank constraint", "author": ["Shai Shalev-Shwartz", "Alon Gonen", "Ohad Shamir"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Max-margin Markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In Advances in Neural Information Processing Systems. MIT Press,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Graphical Models, Exponential Families, and Variational Inference", "author": ["M. Wainwright", "M.I. Jordan"], "venue": "Now Publishers Inc.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Distance metric learning with eigenvalue optimization", "author": ["Yiming Ying", "Peng Li"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "Prime examples for this phenomenon include various structured polytopes that arise in combinatorial optimization, such as the path polytope of a graph (aka the unit flow polytope), the perfect matching polytope of a bipartite graph, and the base polyhedron of a matroid, for which we have highly efficient combinatorial algorithms for linear minimization that rely heavily on the specific rich structure of the polytope [22].", "startOffset": 420, "endOffset": 424}, {"referenceID": 12, "context": "It has been recently shown that the method delivers state-of-the-art performance on many problems of interest, see for instance [13, 17, 4, 9, 10, 23, 19, 26, 11, 14].", "startOffset": 128, "endOffset": 166}, {"referenceID": 16, "context": "It has been recently shown that the method delivers state-of-the-art performance on many problems of interest, see for instance [13, 17, 4, 9, 10, 23, 19, 26, 11, 14].", "startOffset": 128, "endOffset": 166}, {"referenceID": 3, "context": "It has been recently shown that the method delivers state-of-the-art performance on many problems of interest, see for instance [13, 17, 4, 9, 10, 23, 19, 26, 11, 14].", "startOffset": 128, "endOffset": 166}, {"referenceID": 8, "context": "It has been recently shown that the method delivers state-of-the-art performance on many problems of interest, see for instance [13, 17, 4, 9, 10, 23, 19, 26, 11, 14].", "startOffset": 128, "endOffset": 166}, {"referenceID": 9, "context": "It has been recently shown that the method delivers state-of-the-art performance on many problems of interest, see for instance [13, 17, 4, 9, 10, 23, 19, 26, 11, 14].", "startOffset": 128, "endOffset": 166}, {"referenceID": 22, "context": "It has been recently shown that the method delivers state-of-the-art performance on many problems of interest, see for instance [13, 17, 4, 9, 10, 23, 19, 26, 11, 14].", "startOffset": 128, "endOffset": 166}, {"referenceID": 18, "context": "It has been recently shown that the method delivers state-of-the-art performance on many problems of interest, see for instance [13, 17, 4, 9, 10, 23, 19, 26, 11, 14].", "startOffset": 128, "endOffset": 166}, {"referenceID": 25, "context": "It has been recently shown that the method delivers state-of-the-art performance on many problems of interest, see for instance [13, 17, 4, 9, 10, 23, 19, 26, 11, 14].", "startOffset": 128, "endOffset": 166}, {"referenceID": 10, "context": "It has been recently shown that the method delivers state-of-the-art performance on many problems of interest, see for instance [13, 17, 4, 9, 10, 23, 19, 26, 11, 14].", "startOffset": 128, "endOffset": 166}, {"referenceID": 13, "context": "It has been recently shown that the method delivers state-of-the-art performance on many problems of interest, see for instance [13, 17, 4, 9, 10, 23, 19, 26, 11, 14].", "startOffset": 128, "endOffset": 166}, {"referenceID": 4, "context": "It is known, already from the first introduction of the method by Frank and Wolfe in the 1950\u2019s [5], and the somewhat later work of Polyak and Levitin [20], that the method converges with a rate of roughly O(1/t) for minimizing a smooth convex function over a convex and compact set, which matches the rate of the standard projected gradient method for the same setting.", "startOffset": 96, "endOffset": 99}, {"referenceID": 19, "context": "It is known, already from the first introduction of the method by Frank and Wolfe in the 1950\u2019s [5], and the somewhat later work of Polyak and Levitin [20], that the method converges with a rate of roughly O(1/t) for minimizing a smooth convex function over a convex and compact set, which matches the rate of the standard projected gradient method for the same setting.", "startOffset": 151, "endOffset": 155}, {"referenceID": 17, "context": "In fact, certain lower bounds, such as in [18, 6], suggest that such improvement, even if possible, should come with a worse dependence on the problem\u2019s parameters (e.", "startOffset": 42, "endOffset": 49}, {"referenceID": 5, "context": "In fact, certain lower bounds, such as in [18, 6], suggest that such improvement, even if possible, should come with a worse dependence on the problem\u2019s parameters (e.", "startOffset": 42, "endOffset": 49}, {"referenceID": 7, "context": "For instance, Gu\u00e9Lat and Marcotte [8] showed that a CG variant which uses the concept of away-steps converges exponentially fast in case the objective function is strongly convex, the feasible set is a polytope, and the optimal solution is located in the interior of the set.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "A similar result was presented by Beck and Teboulle [3] who considered a specific problem they refer to a the convex feasibility problem over an arbitrary convex set.", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "Later, Ahipasaoglu, Sun and Todd [1] showed that in the specific case of minimizing a smooth and strongly convex function over the unit simplex, a variant of the CG method which also uses away-steps, converges with a linear rate.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "The exponent in their convergence rate depends on various geometric parameters of the polytope [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 14, "context": "In a follow-up work, Lacoste-Julien and Jaggi [15, 16] gave a refined affine-invariant analysis of an algorithm presented in [8] which also uses away steps, and showed that it also converges exponentially fast in the same setting as the Garber-Hazan", "startOffset": 46, "endOffset": 54}, {"referenceID": 15, "context": "In a follow-up work, Lacoste-Julien and Jaggi [15, 16] gave a refined affine-invariant analysis of an algorithm presented in [8] which also uses away steps, and showed that it also converges exponentially fast in the same setting as the Garber-Hazan", "startOffset": 46, "endOffset": 54}, {"referenceID": 7, "context": "In a follow-up work, Lacoste-Julien and Jaggi [15, 16] gave a refined affine-invariant analysis of an algorithm presented in [8] which also uses away steps, and showed that it also converges exponentially fast in the same setting as the Garber-Hazan", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "In a later work, Beck and Shtern [2] gave a different, duality-based, analysis for the algorithm of [8], and showed that it can be applied to a wider class of functions than purely strongly convex functions.", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "In a later work, Beck and Shtern [2] gave a different, duality-based, analysis for the algorithm of [8], and showed that it can be applied to a wider class of functions than purely strongly convex functions.", "startOffset": 100, "endOffset": 103}, {"referenceID": 5, "context": "However, the explicit dependency of their convergence rate on the dimension is suboptimal, compared to [6, 16].", "startOffset": 103, "endOffset": 110}, {"referenceID": 15, "context": "However, the explicit dependency of their convergence rate on the dimension is suboptimal, compared to [6, 16].", "startOffset": 103, "endOffset": 110}, {"referenceID": 6, "context": "Aside from the polytope case, Garber and Hazan have shown recently that in case the feasible set is strongly-convex and the objective function satisfies certain strong convexity-like proprieties, then the standard CG method converges with an accelerated rate of O(1/t2) [7].", "startOffset": 270, "endOffset": 273}, {"referenceID": 1, "context": "Maintaining such a decomposition and computing the away-steps, even with efficient implementations of incremental decomposition procedures, such as suggested in [2], require both memory and per-iteration runtime overheads that are at least quadratic in the dimension.", "startOffset": 161, "endOffset": 164}, {"referenceID": 5, "context": ", when the optimal solution is, informally speaking, dense (see for instance the lower bound in [6]), it is not clear that such an unfavorable dependence is mandatory when the optimum is sparse.", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "#LOO calls runtime memory Frank & Wolfe [5] \u03b2D 2 1 n n Garber & Hazan [6] n\u03b2D 2 \u03b1 log(1/ ) 1 n 2 n2 Lacoste-Julien & Jaggi [16] n\u03b2D 2 \u03b1 log(1/ ) 1 n 2 n2 Beck & Shtern [2] n 2\u03b2D2 \u03b1 log(1/ ) 1 n 2 n2 This paper card(x \u2217)\u03b2D2 \u03b1 log(1/ ) 2 n n", "startOffset": 40, "endOffset": 43}, {"referenceID": 5, "context": "#LOO calls runtime memory Frank & Wolfe [5] \u03b2D 2 1 n n Garber & Hazan [6] n\u03b2D 2 \u03b1 log(1/ ) 1 n 2 n2 Lacoste-Julien & Jaggi [16] n\u03b2D 2 \u03b1 log(1/ ) 1 n 2 n2 Beck & Shtern [2] n 2\u03b2D2 \u03b1 log(1/ ) 1 n 2 n2 This paper card(x \u2217)\u03b2D2 \u03b1 log(1/ ) 2 n n", "startOffset": 70, "endOffset": 73}, {"referenceID": 15, "context": "#LOO calls runtime memory Frank & Wolfe [5] \u03b2D 2 1 n n Garber & Hazan [6] n\u03b2D 2 \u03b1 log(1/ ) 1 n 2 n2 Lacoste-Julien & Jaggi [16] n\u03b2D 2 \u03b1 log(1/ ) 1 n 2 n2 Beck & Shtern [2] n 2\u03b2D2 \u03b1 log(1/ ) 1 n 2 n2 This paper card(x \u2217)\u03b2D2 \u03b1 log(1/ ) 2 n n", "startOffset": 123, "endOffset": 127}, {"referenceID": 1, "context": "#LOO calls runtime memory Frank & Wolfe [5] \u03b2D 2 1 n n Garber & Hazan [6] n\u03b2D 2 \u03b1 log(1/ ) 1 n 2 n2 Lacoste-Julien & Jaggi [16] n\u03b2D 2 \u03b1 log(1/ ) 1 n 2 n2 Beck & Shtern [2] n 2\u03b2D2 \u03b1 log(1/ ) 1 n 2 n2 This paper card(x \u2217)\u03b2D2 \u03b1 log(1/ ) 2 n n", "startOffset": 168, "endOffset": 171}, {"referenceID": 5, "context": "To get lower complexity and memory requirements for the algorithms in [6, 16, 2], we assume they all employ an algorithmic version of Carathodory\u2019s theorem to maintain a convex decomposition of the iterate to at most n + 1 vertices, as fully detailed in [2].", "startOffset": 70, "endOffset": 80}, {"referenceID": 15, "context": "To get lower complexity and memory requirements for the algorithms in [6, 16, 2], we assume they all employ an algorithmic version of Carathodory\u2019s theorem to maintain a convex decomposition of the iterate to at most n + 1 vertices, as fully detailed in [2].", "startOffset": 70, "endOffset": 80}, {"referenceID": 1, "context": "To get lower complexity and memory requirements for the algorithms in [6, 16, 2], we assume they all employ an algorithmic version of Carathodory\u2019s theorem to maintain a convex decomposition of the iterate to at most n + 1 vertices, as fully detailed in [2].", "startOffset": 70, "endOffset": 80}, {"referenceID": 1, "context": "To get lower complexity and memory requirements for the algorithms in [6, 16, 2], we assume they all employ an algorithmic version of Carathodory\u2019s theorem to maintain a convex decomposition of the iterate to at most n + 1 vertices, as fully detailed in [2].", "startOffset": 254, "endOffset": 257}, {"referenceID": 15, "context": "We note that the bound on number of iterations in the analysis of [16] does not depend explicitly on the dimension n, but on the squared inverse pyramidal width of P, which is difficult to evaluate.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": ", [12, 14, 16]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 13, "context": ", [12, 14, 16]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 15, "context": ", [12, 14, 16]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 5, "context": "Importantly, the above assumptions allow us to get rid of the dependency of the convergence rate on certain geometric parameters (such as \u03c8, \u03be in [6] or the pyramidal width in [15, 16]), which can be polynomial in the dimension, and hence result in an impractical convergence rate.", "startOffset": 146, "endOffset": 149}, {"referenceID": 14, "context": "Importantly, the above assumptions allow us to get rid of the dependency of the convergence rate on certain geometric parameters (such as \u03c8, \u03be in [6] or the pyramidal width in [15, 16]), which can be polynomial in the dimension, and hence result in an impractical convergence rate.", "startOffset": 176, "endOffset": 184}, {"referenceID": 15, "context": "Importantly, the above assumptions allow us to get rid of the dependency of the convergence rate on certain geometric parameters (such as \u03c8, \u03be in [6] or the pyramidal width in [15, 16]), which can be polynomial in the dimension, and hence result in an impractical convergence rate.", "startOffset": 176, "endOffset": 184}, {"referenceID": 11, "context": "It is well known that when setting the step-size \u03b7t in an appropriate way, the worst case convergence rate of the method is O(\u03b2D2/t) [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "This convergence rate is tight for the method in general, see for instance [18].", "startOffset": 75, "endOffset": 79}, {"referenceID": 5, "context": "This key principle proves to be crucial to breaking the 1/t rate of the standard method, and to achieve a linear convergence rate under certain strong-convexity assumptions, as described in the recent works [6, 16, 2].", "startOffset": 207, "endOffset": 217}, {"referenceID": 15, "context": "This key principle proves to be crucial to breaking the 1/t rate of the standard method, and to achieve a linear convergence rate under certain strong-convexity assumptions, as described in the recent works [6, 16, 2].", "startOffset": 207, "endOffset": 217}, {"referenceID": 1, "context": "This key principle proves to be crucial to breaking the 1/t rate of the standard method, and to achieve a linear convergence rate under certain strong-convexity assumptions, as described in the recent works [6, 16, 2].", "startOffset": 207, "endOffset": 217}, {"referenceID": 5, "context": "For instance, in [6] it was shown, via the introduction of the concept of a Local Linear Optimization Oracle, that using such a non-uniform reweighing rule, in fact approximates a certain proximal problem, that together with the shrinking effect of strong convexity, as captured by Eq.", "startOffset": 17, "endOffset": 20}, {"referenceID": 15, "context": "As a concrete example, which will also serve as a basis for our new method, we bring the pairwise variant recently studied in [16], which applies this principle in Algorithm 2, given below 1.", "startOffset": 126, "endOffset": 130}, {"referenceID": 5, "context": "While the away-step-based variants increase the size of the decomposition by at most a single vertex per iteration, they also typically exhibit linear convergence after performing at least O(n) steps [6, 16, 2], and thus, this O(n2) estimate still holds.", "startOffset": 200, "endOffset": 210}, {"referenceID": 15, "context": "While the away-step-based variants increase the size of the decomposition by at most a single vertex per iteration, they also typically exhibit linear convergence after performing at least O(n) steps [6, 16, 2], and thus, this O(n2) estimate still holds.", "startOffset": 200, "endOffset": 210}, {"referenceID": 1, "context": "While the away-step-based variants increase the size of the decomposition by at most a single vertex per iteration, they also typically exhibit linear convergence after performing at least O(n) steps [6, 16, 2], and thus, this O(n2) estimate still holds.", "startOffset": 200, "endOffset": 210}, {"referenceID": 1, "context": "Moreover, since these methods require i) to find the worse vertex in the decomposition, in terms of dot-product with current gradient direction, and ii) to update this decomposition on each iteration (even when using sophisticated update techniques such as in [2]), the per-iteration over-head in terms of computation time of these methods is also at least O(n2).", "startOffset": 260, "endOffset": 263}, {"referenceID": 15, "context": "As observable in Table 1, for certain problems in which the optimal solution is sparse, all analyses of previous away-steps-based variants are significantly suboptimal, since they all depend explicitly on the dimension, which seems to While the convergence rate of this pairwise variant, established in [16], despite being linear, is significantly worse than other away-step-based variants, here we show on the contrary, that a proper analysis yields state-ofthe-art performance guarantees.", "startOffset": 303, "endOffset": 307}, {"referenceID": 0, "context": "Option 2: line-search \u03b3t \u2190 max\u03b3\u2208[0,1]{xt + \u03b3(v t \u2212 v \u2212 t ) \u2265 0}, \u03b7\u0303t \u2190 min\u03b7\u2208(0,\u03b3t] f(xt + \u03b7(v + t \u2212 v \u2212 t ))", "startOffset": 32, "endOffset": 37}, {"referenceID": 5, "context": "This is much more efficient than previous linearly convergent CG variant, such as those in [6, 16, 2], which typically require at least additional O(n2) time and space per iteration, since they require to maintain an explicit convex decomposition of the iterates.", "startOffset": 91, "endOffset": 101}, {"referenceID": 15, "context": "This is much more efficient than previous linearly convergent CG variant, such as those in [6, 16, 2], which typically require at least additional O(n2) time and space per iteration, since they require to maintain an explicit convex decomposition of the iterates.", "startOffset": 91, "endOffset": 101}, {"referenceID": 1, "context": "This is much more efficient than previous linearly convergent CG variant, such as those in [6, 16, 2], which typically require at least additional O(n2) time and space per iteration, since they require to maintain an explicit convex decomposition of the iterates.", "startOffset": 91, "endOffset": 101}, {"referenceID": 15, "context": "\u2022 Theorem 1 improves significantly over the convergence rate established for the pairwise conditional gradient variant in [16].", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "In particular, the number of iterations to reach an error in the analysis of [16] depends linearly on |V|!, where |V| is the number of vertices of P.", "startOffset": 77, "endOffset": 81}, {"referenceID": 21, "context": "Since the graph G is a DAG, this could be carried out in O(m) time [22].", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "This could be carried out via combinatorial algorithms in min{\u00d5( \u221a nm), O(n3)} time [22].", "startOffset": 84, "endOffset": 88}, {"referenceID": 24, "context": "There exists a set of linear constraints, known as the marginal polytope, which guarantees that these variables are legal marginals of some global distribution [25].", "startOffset": 160, "endOffset": 164}, {"referenceID": 23, "context": "For example, the learning problem in Max-Margin Markov Networks is defined as a quadratic program over the marginal polytope [24].", "startOffset": 125, "endOffset": 129}, {"referenceID": 24, "context": "Fortunately, for some models, such as tree-structured graphs, the polytope can be characterized by a polynomial number of local consistency constraints, known as the local marginal polytope [25].", "startOffset": 190, "endOffset": 194}, {"referenceID": 24, "context": "For tree-structured graphsML is known to have only integral vertices [25], so it has the desired form assumed in Section 2.", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "Suppose that the sequence of step-sizes {\u03b7t}t\u22651 is monotonically non-increasing, and contained in the interval [0, 1].", "startOffset": 111, "endOffset": 117}, {"referenceID": 5, "context": "from [6], it follows that we can write x as", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "ci := { 0 if A2(i) \u00b7 xt < b2(i) \u221e if A2(i) \u00b7 xt = b2(i) 6: v\u2212 t \u2190 arg minv\u2208V (\u2212\u2207f(xt)) \u00b7 v + cA2v 7: \u03b3t \u2190 max{\u03b3 \u2208 [0, 1] |A2(xt + \u03b3(v t \u2212 v \u2212 t )) \u2264 b2} 8: \u03b7t \u2190 arg min\u03b7\u2208[0,\u03b3t] f(xt + \u03b7(v + t \u2212 v \u2212 t )) 9: xt+1 \u2190 xt + \u03b7t(v t \u2212 v \u2212 t ) 10: end for", "startOffset": 114, "endOffset": 120}, {"referenceID": 1, "context": "Following the work of Beck and Shtern [2], we can consider a broader class of objective functions, namely functions that take the following form:", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "In [2] (Lemma 2.", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "We emphasize that the idea behind the construction of this lower bound is well known and follows almost immediately from previous constructions, such as those in [12, 6].", "startOffset": 162, "endOffset": 169}, {"referenceID": 5, "context": "We emphasize that the idea behind the construction of this lower bound is well known and follows almost immediately from previous constructions, such as those in [12, 6].", "startOffset": 162, "endOffset": 169}, {"referenceID": 15, "context": "We use the two experimental settings from [16], which include a constrained Lasso problem and a video co-localization problem.", "startOffset": 42, "endOffset": 46}, {"referenceID": 23, "context": "In addition, we test our algorithm on a learning problem related to an optical character recognition (OCR) task from [24].", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "In each setting we compare the performance of our algorithm (DICG) to standard conditional gradient (CG), as well as to the fast away (ACG) and pairwise (PCG) variants [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 15, "context": "For the baselines in the first two settings we use the publicly available code from [16], to which we add our own implementation of Algorithm 3.", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "Similarly, for the OCR problem we extend code from [21], kindly provided by the authors.", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "We generate the random matrix \u0100 and vector b\u0304 as in [16].", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "Video co-localization The second example is a quadratic program over the flow polytope, originally proposed in [14].", "startOffset": 111, "endOffset": 115}, {"referenceID": 23, "context": "The constraints in this setting are the marginal polytope corresponding to a chain graph over the letters of a word (see [24]), and the objective function is quadratic.", "startOffset": 121, "endOffset": 125}, {"referenceID": 16, "context": "For this problem we actually run Algorithm 3 in a block-coordinate fashion, where blocks correspond to training examples in the dual SVM formulation [17, 21].", "startOffset": 149, "endOffset": 157}, {"referenceID": 20, "context": "For this problem we actually run Algorithm 3 in a block-coordinate fashion, where blocks correspond to training examples in the dual SVM formulation [17, 21].", "startOffset": 149, "endOffset": 157}], "year": 2016, "abstractText": "Recently, several works have shown that natural modifications of the classical conditional gradient method (aka Frank-Wolfe algorithm) for constrained convex optimization, provably converge with a linear rate when: i) the feasible set is a polytope, and ii) the objective is smooth and strongly-convex. However, all of these results suffer from two significant shortcomings: 1. large memory requirement due to the need to store an explicit convex decomposition of the current iterate, and as a consequence, large running-time overhead per iteration 2. the worst case convergence rate depends unfavorably on the dimension In this work we present a new conditional gradient variant and a corresponding analysis that improves on both of the above shortcomings. In particular: 1. both memory and computation overheads are only linear in the dimension 2. in case the optimal solution is sparse, the new convergence rate replaces a factor which is at least linear in the dimension in previous works, with a linear dependence on the number of non-zeros in the optimal solution At the heart of our method, and corresponding analysis, is a novel way to compute decomposition-invariant away-steps. While our theoretical guarantees do not apply to any polytope, they apply to several important structured polytopes that capture central concepts such as paths in graphs, perfect matchings in bipartite graphs, marginal distributions that arise in structured prediction tasks, and more. Our theoretical findings are complemented by empirical evidence which shows that our method delivers state-of-the-art performance.", "creator": "LaTeX with hyperref package"}}}