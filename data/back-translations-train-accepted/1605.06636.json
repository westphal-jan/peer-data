{"id": "1605.06636", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2016", "title": "Deep Transfer Learning with Joint Adaptation Networks", "abstract": "Deep networks rely on massive amounts of labeled data to learn powerful models. For a target task short of labeled data, transfer learning enables model adaptation from a different source domain. This paper addresses deep transfer learning under a more general scenario that the joint distributions of features and labels may change substantially across domains. Based on the theory of Hilbert space embedding of distributions, a novel joint distribution discrepancy is proposed to directly compare joint distributions across domains, eliminating the need of marginal-conditional factorization. Transfer learning is enabled in deep convolutional networks, where the dataset shifts may linger in multiple task-specific feature layers and the classifier layer. A set of joint adaptation networks are crafted to match the joint distributions of these layers across domains by minimizing the joint distribution discrepancy, which can be trained efficiently using back-propagation. Experiments show that the new approach yields state of the art results on standard domain adaptation datasets.", "histories": [["v1", "Sat, 21 May 2016 12:56:14 GMT  (112kb,D)", "http://arxiv.org/abs/1605.06636v1", null], ["v2", "Thu, 17 Aug 2017 07:35:59 GMT  (428kb,D)", "http://arxiv.org/abs/1605.06636v2", "34th International Conference on Machine Learning"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["mingsheng long", "han zhu", "jianmin wang 0001", "michael i jordan"], "accepted": true, "id": "1605.06636"}, "pdf": {"name": "1605.06636.pdf", "metadata": {"source": "CRF", "title": "Deep Transfer Learning with Joint Adaptation Networks", "authors": ["Mingsheng Long", "Jianmin Wang", "Michael I. Jordan"], "emails": ["mingsheng@tsinghua.edu.cn,", "jimwang@tsinghua.edu.cn,", "jordan@berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "2 Related Work", "text": "Transfer Learning aims to build learning machines that are generalizable in different domains with different probability distributions [1, 2, 3, 4, 5] that allow learning in new domains without labeled data and have broad applications in computer vision [13, 14, 15, 16] and natural language processing [17]. The main technical difficulty of transfer learning is to reduce shifts in data distributions between domains. Most existing methods learn a flat representation model that minimizes domain discrepancy but cannot suppress domain-specific exploration factors of variation. Deep networks learn abstract representations that unravel the explanatory factors of variations in the data [18] and extract transferable factors underlying different populations, which can only reduce but not eliminate the cross-domain discrepancy."}, {"heading": "3 Joint Adaptation Networks", "text": "In domain adaptation problems, we are given a source domain D = {(xsi, ysi)} ns i = 1 of ns-designated examples and a target domain D = {xti} nt i = 1 of nt-designated examples. Source domain and target domain are scanned from the P (X, Y) and Q (X, Y) shared distributions and P 6 = Q. The aim of this essay is to create a deep neural network y = f (x) that formally reduces shifts in common distributions between domains and allows transferable features and classifiers to minimize the target risk Rt (f) = Pr (x, y) Q [kf (x) 6 = y] via the source domain supervision. Transfer learning is a challenging paradigm shift of machine learning due to the scarcity of designated data in the target domain, and the.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k.k...k.k......k........k.....k..............k............k..............k.............k............k................k....................................................................................................................................."}, {"heading": "3.1 Joint Distribution Discrepancy", "text": "Many existing methods mainly address the transmission problem by eliminating the target error with the source and showing a discrepancy between the marginal distributions P (X) and Q (X) of the source and target domains. (D) Two classes of statistics have been studied for two-digit tests that accept or reject the null hypothesis P (X) = Q (X) (X) based on the two examples of energy distance (ED) and maximum Mean Discrepancy (MMD) respectively generated from P (X) and Q (X). [23] Since these statistics are defined for marginal distributions, they are applied only to covariate shifts that include the state of the type of deep fit methods [8, 9]. In this paper, we propose to measure the discrepancy between the common distributions P (X, Y) and Q (X)."}, {"heading": "3.2 Joint Adaptation Networks", "text": "This paper addresses common distributional adjustments within deep networks, which differ in the manner shown in Figure 1 (1) as shown in Figure 1. (7) The abbreviation between individual networks and the empirical errors of CNN on the source data ismin f1ns, i = 1 J (f xsi), y s i), where J (7) is the cross-entropy loss function. The deep features we have learned from CNNs can be the exploratory factors of variations in data distribution and the promotion of knowledge adjustment [20, 18].However, the latest literature results show that the deep features can be reduced but not removed."}, {"heading": "4 Experiments", "text": "We evaluate the common adaptation networks on the basis of current transfer learning and deep learning methods based on standard domain adaptation benchmarks. The codes and data sets will be available online."}, {"heading": "4.1 Setup", "text": "This year, it has come to the point where it is only a matter of time before a solution is found, in which a solution is found."}, {"heading": "4.2 Results", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "4.3 Discussion", "text": "Feature Visualization: We visualize in Figures 2 (a) -2 (d) the activation of task A \u2192 W learned from DAN or JAN using the t-SNE embeddings [27]. Compared to the activations given by DAN in Figure 2 (a) -2 (b), the activations given by JAN in Figure 2 (c) -2 (d) show that the target categories are much more clearly discriminated against by the JAN source classifier, which indicates that the common adaptation of deep features and designations is a powerful approach to effective domain adaptation. Distribution differences: The theory of domain adaptation [22, 28] suggests the A distance as a measure of cross-domain discrepancy, which, together with the source risk, limits the target risk. The proxy A distance is defined as dA = 2 (1 \u2212 2), where \u2192 is the generalization error of a kernel classification (e.g. M) of the domain fixer (SVW)."}, {"heading": "5 Conclusion", "text": "Unlike previous methods, we eliminate the need for separate adjustments of marginal and conditional distributions, which are often subject to quite strong assumptions of independence. The discrepancy between common distributions of multiple characteristics and labels can be calculated by embedding the common distributions in a Hilbert space for tensor products, which can, of course, be implemented by most deep networks. As future work, we will present the further evaluations of larger datasets and the semi-monitored adjustment scenarios for domains."}], "references": [{"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "TKDE, 22(10):1345\u20131359,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Direct importance estimation with model selection and its application to covariate shift adaptation", "author": ["M. Sugiyama", "S. Nakajima", "H. Kashima", "P.V. Buenau", "M. Kawanabe"], "venue": "NIPS,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "TNNLS, 22(2):199\u2013210,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Domain adaptation under target and conditional shift", "author": ["K. Zhang", "B. Sch\u00f6lkopf", "K. Muandet", "Z. Wang"], "venue": "ICML,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Flexible transfer learning under support and model shift", "author": ["X. Wang", "J. Schneider"], "venue": "NIPS,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation with conditional transferable components", "author": ["M. Gong", "K. Zhang", "T. Liu", "D. Tao", "C. Glymour", "B. Sch\u00f6lkopf"], "venue": "ICML,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["E. Tzeng", "J. Hoffman", "N. Zhang", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1412.3474,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning transferable features with deep adaptation networks", "author": ["M. Long", "Y. Cao", "J. Wang", "M.I. Jordan"], "venue": "ICML,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Y. Ganin", "V. Lempitsky"], "venue": "ICML,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Simultaneous deep transfer across domains and tasks", "author": ["E. Tzeng", "J. Hoffman", "N. Zhang", "K. Saenko", "T. Darrell"], "venue": "ICCV,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems", "author": ["L. Song", "J. Huang", "A. Smola", "K. Fukumizu"], "venue": "ICML,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "ECCV,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "ICCV,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "LSDA: Large scale detection through adaptation", "author": ["J. Hoffman", "S. Guadarrama", "E. Tzeng", "R. Hu", "J. Donahue", "R. Girshick", "T. Darrell", "K. Saenko"], "venue": "NIPS,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR, 12:2493\u20132537,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "TPAMI, 35(8):1798\u20131828,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "CVPR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Dataset shift in machine learning", "author": ["J. Quionero-Candela", "M. Sugiyama", "A. Schwaighofer", "N.D. Lawrence"], "venue": "The MIT Press,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "MLJ, 79(1-2):151\u2013175,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Equivalence of distance-based and rkhs-based statistics in hypothesis testing", "author": ["D. Sejdinovic", "B. Sriperumbudur", "A. Gretton", "K. Fukumizu"], "venue": "The Annals of Statistics, 41(5):2263\u20132291,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "A kernel two-sample test", "author": ["A. Gretton", "K. Borgwardt", "M. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "JMLR, 13:723\u2013773,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "ICML,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "COLT,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "However, this learning paradigm suffers from the shift in data distributions across different domains, which poses a major obstacle in adapting predictive models for the target task [1].", "startOffset": 182, "endOffset": 185}, {"referenceID": 0, "context": "Learning a discriminative model in the presence of the shift between training and test distributions is known as transfer learning or domain adaptation [1].", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "Previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels [1, 2, 3, 4, 5, 6].", "startOffset": 192, "endOffset": 210}, {"referenceID": 1, "context": "Previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels [1, 2, 3, 4, 5, 6].", "startOffset": 192, "endOffset": 210}, {"referenceID": 2, "context": "Previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels [1, 2, 3, 4, 5, 6].", "startOffset": 192, "endOffset": 210}, {"referenceID": 3, "context": "Previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels [1, 2, 3, 4, 5, 6].", "startOffset": 192, "endOffset": 210}, {"referenceID": 4, "context": "Previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels [1, 2, 3, 4, 5, 6].", "startOffset": 192, "endOffset": 210}, {"referenceID": 5, "context": "Previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels [1, 2, 3, 4, 5, 6].", "startOffset": 192, "endOffset": 210}, {"referenceID": 6, "context": "Recent deep transfer learning methods leverage deep networks to learn more transferable feature representations by embedding domain adaptation in the pipeline of deep learning, which can simultaneously disentangle the explanatory factors of variations behind data and match the marginal distributions across domains [7, 8, 9, 10].", "startOffset": 316, "endOffset": 329}, {"referenceID": 7, "context": "Recent deep transfer learning methods leverage deep networks to learn more transferable feature representations by embedding domain adaptation in the pipeline of deep learning, which can simultaneously disentangle the explanatory factors of variations behind data and match the marginal distributions across domains [7, 8, 9, 10].", "startOffset": 316, "endOffset": 329}, {"referenceID": 8, "context": "Recent deep transfer learning methods leverage deep networks to learn more transferable feature representations by embedding domain adaptation in the pipeline of deep learning, which can simultaneously disentangle the explanatory factors of variations behind data and match the marginal distributions across domains [7, 8, 9, 10].", "startOffset": 316, "endOffset": 329}, {"referenceID": 9, "context": "Recent deep transfer learning methods leverage deep networks to learn more transferable feature representations by embedding domain adaptation in the pipeline of deep learning, which can simultaneously disentangle the explanatory factors of variations behind data and match the marginal distributions across domains [7, 8, 9, 10].", "startOffset": 316, "endOffset": 329}, {"referenceID": 3, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11].", "startOffset": 21, "endOffset": 30}, {"referenceID": 4, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11].", "startOffset": 21, "endOffset": 30}, {"referenceID": 5, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11].", "startOffset": 21, "endOffset": 30}, {"referenceID": 10, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11].", "startOffset": 177, "endOffset": 181}, {"referenceID": 3, "context": "marginal and conditional distributions which often require strong independence and/or smoothness assumptions on the factorized distributions [4], we propose a novel joint distribution discrepancy that can directly compare joint distributions by embedding them into reproducing kernel Hilbert spaces, which eliminates the need of marginal-conditional factorization for separate adaptation.", "startOffset": 141, "endOffset": 144}, {"referenceID": 11, "context": "Transfer learning is enabled in deep convolutional networks, where the dataset shifts may linger in multiple task-specific feature layers and the classifier layer, because deep features eventually transition from general to specific along the network and the transferability of features and classifiers decreases when the domain discrepancy increases [12].", "startOffset": 351, "endOffset": 355}, {"referenceID": 0, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 137, "endOffset": 152}, {"referenceID": 1, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 137, "endOffset": 152}, {"referenceID": 2, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 137, "endOffset": 152}, {"referenceID": 3, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 137, "endOffset": 152}, {"referenceID": 4, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 137, "endOffset": 152}, {"referenceID": 12, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 262, "endOffset": 278}, {"referenceID": 13, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 262, "endOffset": 278}, {"referenceID": 14, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 262, "endOffset": 278}, {"referenceID": 15, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 262, "endOffset": 278}, {"referenceID": 16, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 311, "endOffset": 315}, {"referenceID": 17, "context": "Deep networks learn abstract representations that disentangle the explanatory factors of variations in data [18] and extract transferable factors underlying different populations [19, 20], which can only reduce, but not remove, the cross-domain discrepancy [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 18, "context": "Deep networks learn abstract representations that disentangle the explanatory factors of variations in data [18] and extract transferable factors underlying different populations [19, 20], which can only reduce, but not remove, the cross-domain discrepancy [12].", "startOffset": 179, "endOffset": 187}, {"referenceID": 19, "context": "Deep networks learn abstract representations that disentangle the explanatory factors of variations in data [18] and extract transferable factors underlying different populations [19, 20], which can only reduce, but not remove, the cross-domain discrepancy [12].", "startOffset": 179, "endOffset": 187}, {"referenceID": 11, "context": "Deep networks learn abstract representations that disentangle the explanatory factors of variations in data [18] and extract transferable factors underlying different populations [19, 20], which can only reduce, but not remove, the cross-domain discrepancy [12].", "startOffset": 257, "endOffset": 261}, {"referenceID": 6, "context": "The state of the art work on deep domain adaptation bridges deep learning and transfer learning [7, 8, 9, 10] by integrating domain-adaptation modules into deep network architectures to boost transfer performance.", "startOffset": 96, "endOffset": 109}, {"referenceID": 7, "context": "The state of the art work on deep domain adaptation bridges deep learning and transfer learning [7, 8, 9, 10] by integrating domain-adaptation modules into deep network architectures to boost transfer performance.", "startOffset": 96, "endOffset": 109}, {"referenceID": 8, "context": "The state of the art work on deep domain adaptation bridges deep learning and transfer learning [7, 8, 9, 10] by integrating domain-adaptation modules into deep network architectures to boost transfer performance.", "startOffset": 96, "endOffset": 109}, {"referenceID": 9, "context": "The state of the art work on deep domain adaptation bridges deep learning and transfer learning [7, 8, 9, 10] by integrating domain-adaptation modules into deep network architectures to boost transfer performance.", "startOffset": 96, "endOffset": 109}, {"referenceID": 6, "context": "Previous deep transfer learning methods [7, 8, 9, 10] mainly correct the shifts in the marginal distributions.", "startOffset": 40, "endOffset": 53}, {"referenceID": 7, "context": "Previous deep transfer learning methods [7, 8, 9, 10] mainly correct the shifts in the marginal distributions.", "startOffset": 40, "endOffset": 53}, {"referenceID": 8, "context": "Previous deep transfer learning methods [7, 8, 9, 10] mainly correct the shifts in the marginal distributions.", "startOffset": 40, "endOffset": 53}, {"referenceID": 9, "context": "Previous deep transfer learning methods [7, 8, 9, 10] mainly correct the shifts in the marginal distributions.", "startOffset": 40, "endOffset": 53}, {"referenceID": 7, "context": "Note that [8, 10] partially align the source and target classes based on pseudo labels, but both rely on a rather strong assumption that the adaptations of marginal distributions P (X) and target distributions P (Y) can be independent.", "startOffset": 10, "endOffset": 17}, {"referenceID": 9, "context": "Note that [8, 10] partially align the source and target classes based on pseudo labels, but both rely on a rather strong assumption that the adaptations of marginal distributions P (X) and target distributions P (Y) can be independent.", "startOffset": 10, "endOffset": 17}, {"referenceID": 3, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11], which is a remarkable step towards full-transfer of joint distributions.", "startOffset": 21, "endOffset": 30}, {"referenceID": 4, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11], which is a remarkable step towards full-transfer of joint distributions.", "startOffset": 21, "endOffset": 30}, {"referenceID": 5, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11], which is a remarkable step towards full-transfer of joint distributions.", "startOffset": 21, "endOffset": 30}, {"referenceID": 10, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11], which is a remarkable step towards full-transfer of joint distributions.", "startOffset": 177, "endOffset": 181}, {"referenceID": 1, "context": "covariate shift [2]), the conditional distributions P (Y|X) 6= Q(Y|X) (a.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "conditional shift [4]), or both (a.", "startOffset": 18, "endOffset": 21}, {"referenceID": 20, "context": "dataset shift [21]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "In general, the presence of conditional shift leads to an ill-posed problem, and the additional assumption that the conditional distribution changes only under location-scale transformations on X is commonly imposed to make the problem tractable [4, 5, 6].", "startOffset": 246, "endOffset": 255}, {"referenceID": 4, "context": "In general, the presence of conditional shift leads to an ill-posed problem, and the additional assumption that the conditional distribution changes only under location-scale transformations on X is commonly imposed to make the problem tractable [4, 5, 6].", "startOffset": 246, "endOffset": 255}, {"referenceID": 5, "context": "In general, the presence of conditional shift leads to an ill-posed problem, and the additional assumption that the conditional distribution changes only under location-scale transformations on X is commonly imposed to make the problem tractable [4, 5, 6].", "startOffset": 246, "endOffset": 255}, {"referenceID": 21, "context": "Many existing methods address the transfer learning problem mainly by bounding the target error with the source error plus a discrepancy metric between the marginal distributions P (X) and Q(X) of the source and target domains [22].", "startOffset": 227, "endOffset": 231}, {"referenceID": 22, "context": "Two classes of statistics have been explored for two-sample testing, which accepts or rejects the null hypothesis P (X) = Q(X) based on the two samples respectively generated from P (X) and Q(X): Energy Distance (ED) and Maximum Mean Discrepancy (MMD) [23].", "startOffset": 252, "endOffset": 256}, {"referenceID": 1, "context": "As these statistics are defined for the marginal distributions, they are only applied to covariate shift adaptation problems [2, 3, 8], which include state of the art deep adaptation methods [8, 9, 10].", "startOffset": 125, "endOffset": 134}, {"referenceID": 2, "context": "As these statistics are defined for the marginal distributions, they are only applied to covariate shift adaptation problems [2, 3, 8], which include state of the art deep adaptation methods [8, 9, 10].", "startOffset": 125, "endOffset": 134}, {"referenceID": 7, "context": "As these statistics are defined for the marginal distributions, they are only applied to covariate shift adaptation problems [2, 3, 8], which include state of the art deep adaptation methods [8, 9, 10].", "startOffset": 125, "endOffset": 134}, {"referenceID": 7, "context": "As these statistics are defined for the marginal distributions, they are only applied to covariate shift adaptation problems [2, 3, 8], which include state of the art deep adaptation methods [8, 9, 10].", "startOffset": 191, "endOffset": 201}, {"referenceID": 8, "context": "As these statistics are defined for the marginal distributions, they are only applied to covariate shift adaptation problems [2, 3, 8], which include state of the art deep adaptation methods [8, 9, 10].", "startOffset": 191, "endOffset": 201}, {"referenceID": 9, "context": "As these statistics are defined for the marginal distributions, they are only applied to covariate shift adaptation problems [2, 3, 8], which include state of the art deep adaptation methods [8, 9, 10].", "startOffset": 191, "endOffset": 201}, {"referenceID": 10, "context": "In this paper, we propose to directly measure the discrepancy between joint distributions P (X,Y) andQ(X,Y) (denoted by P andQ for short) based on the theory of kernel embedding of distributions [11], in which a probability distribution is represented as an element of a reproducing kernel Hilbert space (RKHS).", "startOffset": 195, "endOffset": 199}, {"referenceID": 23, "context": "We extend the Maximum Mean Discrepancy (MMD) [24] to measure the discrepancy between joint distributions.", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "Based on the theory of kernel two-sample testing [24], we have P = Q if and only if D(P,Q) = 0.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "Since deep features eventually transition from general to specific along the network [12, 8], transferability of features at different layers may be task-dependent and it is safer to model the joint distribution discrepancy based on multiple task-specific layers.", "startOffset": 85, "endOffset": 92}, {"referenceID": 7, "context": "Since deep features eventually transition from general to specific along the network [12, 8], transferability of features at different layers may be task-dependent and it is safer to model the joint distribution discrepancy based on multiple task-specific layers.", "startOffset": 85, "endOffset": 92}, {"referenceID": 17, "context": "Deep networks [18] can learn distributed, compositional, and abstract representations for natural data such as image and text.", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "AlexNet [25] and GoogLeNet [26], to novel joint adaptation networks (JANs) as shown in Figure 1.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "AlexNet [25] and GoogLeNet [26], to novel joint adaptation networks (JANs) as shown in Figure 1.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "The deep features learned by CNNs can disentangle the exploratory factors of variations in data distributions and promote knowledge adaptation [20, 18].", "startOffset": 143, "endOffset": 151}, {"referenceID": 17, "context": "The deep features learned by CNNs can disentangle the exploratory factors of variations in data distributions and promote knowledge adaptation [20, 18].", "startOffset": 143, "endOffset": 151}, {"referenceID": 11, "context": "However, the latest literature findings also reveal that the deep features can reduce, but not remove, the cross-domain distribution discrepancy [12, 8].", "startOffset": 145, "endOffset": 152}, {"referenceID": 7, "context": "However, the latest literature findings also reveal that the deep features can reduce, but not remove, the cross-domain distribution discrepancy [12, 8].", "startOffset": 145, "endOffset": 152}, {"referenceID": 11, "context": "transition from general to specific along the network, and the transferability of features and classifiers decreases when the cross-domain discrepancy increases [12].", "startOffset": 161, "endOffset": 165}, {"referenceID": 23, "context": "Motivated by the unbiased estimate of MMD [24], we devise a similar linear-time estimate of JDD as", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "This linear-time estimate nicely fits into the mini-batch stochastic gradient descent (SGD) algorithm implemented in [8], based on which the training of the JAN models can scale linearly to large samples.", "startOffset": 117, "endOffset": 120}, {"referenceID": 12, "context": "Office-31 [13] is a standard benchmark for domain adaptation in computer vision, comprising 4,652 images and 31 categories collected from three distinct domains: Amazon (A), which contains images downloaded from amazon.", "startOffset": 10, "endOffset": 14}, {"referenceID": 26, "context": "We evaluate all methods across three transfer tasks A\u2192W, D\u2192W and W\u2192 D, which are widely adopted by prior deep transfer learning methods [27, 7, 9], and another three transfer tasks A\u2192 D, D\u2192 A and W\u2192 A as in [8, 10].", "startOffset": 136, "endOffset": 146}, {"referenceID": 6, "context": "We evaluate all methods across three transfer tasks A\u2192W, D\u2192W and W\u2192 D, which are widely adopted by prior deep transfer learning methods [27, 7, 9], and another three transfer tasks A\u2192 D, D\u2192 A and W\u2192 A as in [8, 10].", "startOffset": 136, "endOffset": 146}, {"referenceID": 8, "context": "We evaluate all methods across three transfer tasks A\u2192W, D\u2192W and W\u2192 D, which are widely adopted by prior deep transfer learning methods [27, 7, 9], and another three transfer tasks A\u2192 D, D\u2192 A and W\u2192 A as in [8, 10].", "startOffset": 136, "endOffset": 146}, {"referenceID": 7, "context": "We evaluate all methods across three transfer tasks A\u2192W, D\u2192W and W\u2192 D, which are widely adopted by prior deep transfer learning methods [27, 7, 9], and another three transfer tasks A\u2192 D, D\u2192 A and W\u2192 A as in [8, 10].", "startOffset": 207, "endOffset": 214}, {"referenceID": 9, "context": "We evaluate all methods across three transfer tasks A\u2192W, D\u2192W and W\u2192 D, which are widely adopted by prior deep transfer learning methods [27, 7, 9], and another three transfer tasks A\u2192 D, D\u2192 A and W\u2192 A as in [8, 10].", "startOffset": 207, "endOffset": 214}, {"referenceID": 2, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 138, "endOffset": 141}, {"referenceID": 14, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 170, "endOffset": 174}, {"referenceID": 24, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 214, "endOffset": 218}, {"referenceID": 25, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 233, "endOffset": 237}, {"referenceID": 6, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 268, "endOffset": 271}, {"referenceID": 7, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 303, "endOffset": 306}, {"referenceID": 8, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 339, "endOffset": 342}, {"referenceID": 23, "context": "DDC is the first method that maximizes domain invariance by adding to AlexNet an adaptation layer using linear-kernel MMD [24].", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "We examine the influence of deep representations for domain adaptation by employing the breakthrough AlexNet [25] and the state of the art GoogLeNet [26] for learning deep representations.", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": "We examine the influence of deep representations for domain adaptation by employing the breakthrough AlexNet [25] and the state of the art GoogLeNet [26] for learning deep representations.", "startOffset": 149, "endOffset": 153}, {"referenceID": 26, "context": "For AlexNet, we follow DeCAF [27] and use the activations of the fc7 layer as image representation.", "startOffset": 29, "endOffset": 33}, {"referenceID": 7, "context": "We follow standard evaluation protocols for unsupervised domain adaptation [8, 9].", "startOffset": 75, "endOffset": 81}, {"referenceID": 8, "context": "We follow standard evaluation protocols for unsupervised domain adaptation [8, 9].", "startOffset": 75, "endOffset": 81}, {"referenceID": 23, "context": "median trick [24].", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "We implement all deep methods based on the Caffe framework, and fine-tune from Caffe-trained models of AlexNet [25] and GoogLeNet [26], both are pre-trained on the ImageNet dataset.", "startOffset": 111, "endOffset": 115}, {"referenceID": 25, "context": "We implement all deep methods based on the Caffe framework, and fine-tune from Caffe-trained models of AlexNet [25] and GoogLeNet [26], both are pre-trained on the ImageNet dataset.", "startOffset": 130, "endOffset": 134}, {"referenceID": 8, "context": "9 and the learning rate annealing strategy implemented in RevGrad [9]: the learning rate is not selected through a grid search due to high computational cost\u2014it is adjusted during SGD using the following formula: \u03b7p = \u03b70 (1+\u03b1p) , where p is the training progress linearly changing from 0 to 1, \u03b70 = 0.", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "Note that for fair comparison, the results of the state of the art methods, DAN [8] and RevGrad [9], are directly reported from their original papers.", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "Note that for fair comparison, the results of the state of the art methods, DAN [8] and RevGrad [9], are directly reported from their original papers.", "startOffset": 96, "endOffset": 99}, {"referenceID": 12, "context": "A\u2192W and A\u2192 D, where the source and target domains are substantially different, and produce comparable classification accuracy on easy transfer tasks, D \u2192W and W\u2192 D, where the source and target domains are similar [13].", "startOffset": 213, "endOffset": 217}, {"referenceID": 2, "context": "Method A\u2192W D\u2192W W\u2192 D A\u2192 D D\u2192 A W\u2192 A Avg TCA [3] 61.", "startOffset": 43, "endOffset": 46}, {"referenceID": 14, "context": "8 GFK [15] 60.", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": "7 AlexNet [25] 61.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "1 DDC [7] 61.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "6 RevGrad [9] 73.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "3 DAN [8] 68.", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "Method A\u2192W D\u2192W W\u2192 D A\u2192 D D\u2192 A W\u2192 A Avg TCA [3] 68.", "startOffset": 43, "endOffset": 46}, {"referenceID": 14, "context": "4 GFK [15] 71.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "7 GoogLeNet [26] 71.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "5 DDC [7] 72.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "1 DAN [8] 76.", "startOffset": 6, "endOffset": 9}, {"referenceID": 11, "context": "These results confirm the current practice that deep networks learn abstract feature representations, which can only reduce, but not remove, the domain discrepancy [12].", "startOffset": 164, "endOffset": 168}, {"referenceID": 11, "context": "Since deep features eventually transition from general to specific along the network, there may be multiple layers where the features are not safely transferable [12, 8], hence it is safer to adapt the joint distributions based on multi-layer features instead of single-layer features.", "startOffset": 162, "endOffset": 169}, {"referenceID": 7, "context": "Since deep features eventually transition from general to specific along the network, there may be multiple layers where the features are not safely transferable [12, 8], hence it is safer to adapt the joint distributions based on multi-layer features instead of single-layer features.", "startOffset": 162, "endOffset": 169}, {"referenceID": 10, "context": "It is noteworthy that this paper initiates a principled way to model multi-layer features in a kernel embedding framework [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 2, "context": "Method C\u2192I C\u2192P C\u2192B I\u2192C I\u2192P I\u2192B P\u2192C P\u2192I P\u2192B B\u2192C B\u2192I B\u2192P Avg TCA [3] 83.", "startOffset": 63, "endOffset": 66}, {"referenceID": 14, "context": "3 GFK [15] 84.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "0 GoogLeNet [26] 85.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "4 DDC [7] 86.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "1 DAN [8] 90.", "startOffset": 6, "endOffset": 9}, {"referenceID": 26, "context": "Feature Visualization: We visualize in Figures 2(a)\u20132(d) the activations of task A\u2192W learned by DAN and JAN respectively using the t-SNE embeddings [27].", "startOffset": 148, "endOffset": 152}, {"referenceID": 21, "context": "Distribution Discrepancy: The theory of domain adaptation [22, 28] suggests the A-distance as a measure of cross-domain discrepancy, which, together with the source risk, will bound the target risk.", "startOffset": 58, "endOffset": 66}, {"referenceID": 27, "context": "Distribution Discrepancy: The theory of domain adaptation [22, 28] suggests the A-distance as a measure of cross-domain discrepancy, which, together with the source risk, will bound the target risk.", "startOffset": 58, "endOffset": 66}], "year": 2016, "abstractText": "Deep networks rely on massive amounts of labeled data to learn powerful models. For a target task short of labeled data, transfer learning enables model adaptation from a different source domain. This paper addresses deep transfer learning under a more general scenario that the joint distributions of features and labels may change substantially across domains. Based on the theory of Hilbert space embedding of distributions, a novel joint distribution discrepancy is proposed to directly compare joint distributions across domains, eliminating the need of marginal-conditional factorization. Transfer learning is enabled in deep convolutional networks, where the dataset shifts may linger in multiple task-specific feature layers and the classifier layer. A set of joint adaptation networks are crafted to match the joint distributions of these layers across domains by minimizing the joint distribution discrepancy, which can be trained efficiently using back-propagation. Experiments show that the new approach yields state of the art results on standard domain adaptation datasets.", "creator": "LaTeX with hyperref package"}}}