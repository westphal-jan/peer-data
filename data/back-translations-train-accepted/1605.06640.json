{"id": "1605.06640", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2016", "title": "Programming with a Differentiable Forth Interpreter", "abstract": "There are families of neural networks that can learn to compute any function, provided sufficient training data. However, given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. Here we consider the case of prior procedural knowledge such as knowing the overall recursive structure of a sequence transduction program or the fact that a program will likely use arithmetic operations on real numbers to solve a task. To this end we present a differentiable interpreter for the programming language Forth. Through a neural implementation of the dual stack machine that underlies Forth, programmers can write program sketches with slots that can be filled with learnable behaviour. As the program interpreter is end-to-end differentiable, we can optimize this behaviour directly through gradient descent techniques on user specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex transduction tasks such as sequence sorting or addition with substantially less data and better generalisation over problem sizes. In addition, we introduce neural program optimisations based on symbolic computation and parallel branching that lead to significant speed improvements.", "histories": [["v1", "Sat, 21 May 2016 13:24:14 GMT  (119kb,D)", "http://arxiv.org/abs/1605.06640v1", null], ["v2", "Sat, 5 Nov 2016 19:15:44 GMT  (188kb,D)", "http://arxiv.org/abs/1605.06640v2", null], ["v3", "Sun, 23 Jul 2017 09:20:48 GMT  (225kb,D)", "http://arxiv.org/abs/1605.06640v3", "34th International Conference on Machine Learning (ICML 2017)"]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["matko bosnjak", "tim rockt\u00e4schel", "jason naradowsky", "sebastian riedel"], "accepted": true, "id": "1605.06640"}, "pdf": {"name": "1605.06640.pdf", "metadata": {"source": "CRF", "title": "Programming with a Differentiable Forth Interpreter", "authors": ["Sebastian Riedel", "Matko Bo\u0161njak", "Tim Rockt\u00e4schel"], "emails": ["t.rocktaschel}@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "A central goal of Artificial Intelligence is machines that we can not only program, but also teach to answer a larger question: neural architectures that can learn to perform algorithms similar to traditional computers, using primitives such as memory access and stack manipulation. In this context, the role of the human programmer is often limited to providing training data. However, for many tasks, training data is scarce. In these cases, the programmer may have partial processional background knowledge: she may know the rough structure of the program, or how to implement several sub-routines that are likely to be necessary to complete the task. In visual programming [7], a user often knows a rough sketch of what she wants to do, and must fit into the specific components."}, {"heading": "2 The Forth Programming Language", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "3 Differentiable Forth", "text": "When a programmer writes a Forth program, she defines a sequence of Forth words and thus of known state transition functions. In other words, she knows exactly how the calculation should proceed. To account for cases where the developer's procedural background knowledge is incomplete, we expand Forth to support the definition of a program sketch. Just like Forth programs, sketches are lists of transition functions, but the behavior of some of these transition functions must be learned from data. To learn the behavior of transition functions within a program, we want machine output to be differentiated in terms of these functions (and possibly representations of program inputs), which allows us to select transition functions such as neural networks and train their parameters efficiently through backpropagation and gradient algorithms. To this end, we first provide a continuous representation of the state of a Forth abstract machine. Then we present an RN, the network of the program bifurcated in parallel to the program execution by this machine, and finally we discuss the symbolic modeling of the transition program on each of that machine."}, {"heading": "3.1 Machine State Encoding", "text": "We map the symbolic state of the machine S = (D, R, H, c) to a continuous representation S = (D, R, H, c) in the following way. Since the data D and the return stack R are both represented by the same mechanism, we will only describe D. Its continuous representation D is a tuple (DB, dt), where DB-Rl \u00b7 v is a matrix that serves as a buffer for length l and value width v and dt-Rl points to the current uppermost element in the stack as it is contained in the buffer. To access the uppermost element of the stack, we use the product DBdt. Pushing and packing is easily implemented by writing into the buffer matrix and incrementing and decreasing dt-Rl. The stack representation H-Rh \u00b7 v is a matrix that serves as a random memory buffer for length h and width. The same widths of H and allow us to move values directly from the stack to the stack and vice versa."}, {"heading": "3.2 The Execution RNN", "text": "We define a differentiable program P as a sequence of continuous transition functions P = w1... wn with wi \u0441S \u2192 S. If each of these functions corresponds to a Forth word, we call it a differentiable Forth program. Our goal is to define a neural version of the Forth machine that can execute such programs and guarantees that the execution of differentiable Forth programs leads to a final state that corresponds to the final state of the corresponding symbolic execution. We model the execution using an RNN that generates a state Si + 1 conditioned on a previous state Si. This is done by first passing the current state to each function wi in the program and then weighing each of the next states produced against the component of the program countervector ci that corresponds to the program index i, effectively using c as an attention vector over the code. Formally, we have: Si + 1 = wi cii (Si).Clearly, this is the definitive state for the program, and the recurrence is distinct."}, {"heading": "3.3 Forth Sketches", "text": "A Forth slot F is a differentiated program in which for some program indexes i function wi is learnable. We will call these learnable functions slots because they correspond to sub-specific \"slots\" in the program code that must be filled in through learning. We allow users to define a slot w by specifying a pair of a state encoder wenc that produces a latent representation h with a multi-layer perceptron \u2212 and a decoder wdec that consumes this representation to produce the next machine state. So we have w = wdec. To inject slots into Forth program code, we introduce a notation that reflects this decomposition. Specifically, slots are defined with the {Encoder - > Decoder} syntax, where the encoder and decoder are specifications of the corresponding slot elements. Encoder We offer the following options for encoder: \u2022 static representation, regardless of the actual state."}, {"heading": "3.4 Program Code Optimizations", "text": "In return, a new machine state is calculated by executing all the words in the program, and then the result states are weighted by activating the program counter for the given word. This parallel execution of all words is expensive, and we therefore want to avoid full RNN steps wherever possible. We use two strategies to significantly speed up the execution of Byrds 4. Symbolic execution Whenever we have a sequence of forth words that contains no entry or exit points, we collapse this sequence into a single transition. We do this using symbolic execution [11]: We first fill the stacks and stack a standard forth abstract machine with symbols representing arbitrary values (e.g. D = d1... dl and R = r1.), and then execute the sequence of forth words on the machine."}, {"heading": "3.5 Training", "text": "When we train the program, we assume that we have a sequence of input-output pairs of machine states (xi, yi). In addition, we can have a mask mi that specifies which components of the machine states should be evaluated and which should be ignored. For example, we do not care about values in the stack buffer above the target stack depth. With ST (xi) to specify the final state of the execution state RNN after T steps, we can use the output state xi backpropagation and any variant of SGD to optimize a loss function L (ST (xi), yi, mi)."}, {"heading": "4 Related Work", "text": "The idea of program synthesis is as old as artificial intelligence, and therefore has a long history in the computer world."}, {"heading": "5 Experiments", "text": "We test \u2202 4 for sorting and addition tasks, which are represented in [19] with different levels of the program structure."}, {"heading": "5.1 Sorting", "text": "Sorting sequences of digits is a tough task for RNNs like LSTMs, as they are not able to generalize sequences that are slightly longer than those on which they were trained. We examine several strong priorities based on BubbleSort for this transduction task and present two examples that allow us to learn to sort only a few training examples, which is achieved by defining sufficient structures so that the behavior of the network is invariably related to the input sequence length. For example, we can assume that to sort a list of numbers we need to use a function repeatedly in order to gradually apply shorter sub-lists, this outer loop can be specified in the next line 4 of listing 1, which repeatedly calls a function BUBBLE. Listing 2 shows a sketch for BUBBLE in which we only specify that on the top two elements of the stack, these top two elements of the stack, and the top two of the return elements should be based."}, {"heading": "5.2 Addition", "text": "The addition of numbers with n digits is another problem to which we apply \u2202 4. The algorithm on which we rely is the standard addition of the primary school, where the goal is to iterate over the aligned digits and to carry and calculate their resulting sum. In this case, the sketch we have chosen is in listing 3. As input, it requires aligned pairs of numbers, a carry for the least significant digit and the length of the respective numbers. The sketch essentially defines the hold state of the recursion (no remaining digits, line 1-2) and manipulates two digits and a carry into a new digit (pushed onto the return stack) and a new carry. It then reduces the problem size by one and returns the solution by jumping it out of the return stack. The sketch presented, when it is trained on single digit addition examples, successfully learns the return stack (insertion)."}, {"heading": "6 Conclusion", "text": "We presented \u2202 4, a differentiable abstract machine for the Forth language, and demonstrated how it can be used to fill in missing behavior in Forth program sketches. \u2202 4 has successfully learned to sort and add, using only program sketches and input pairs as input. We believe that the \u2202 4 will be useful to solve complex problems, such as machine reading and conclusions in knowledge bases, where we could inject the recursive structure of the theorem evidence, but leave the unification open to be learned. We also believe that it can be used as a testbed to understand how much previous structural bias requires a problem. In the future, we plan to focus on more user-friendly guest languages, further scaling learning and execution, and integrating non-differentiable transitions (as they arise when interacting with a real environment)."}, {"heading": "Acknowledgments", "text": "We thank Dirk Weissenborn and Guillaume Bouchard for their comments on this draft. This work was supported by Microsoft Research through its doctoral fellowship program, an Allen Distinguished Investigator Award and a Marie Curie Career Integration Award."}, {"heading": "A. \u22024 Words", "text": "We implemented a small subset of available Forth words in \u2202 4. The table of these words together with their descriptions is in Table 1. The commands are roughly divided into 6 groups, separated in the table line by line, are: Data Stack Operations 1 +, 1-, DUP, SWAP, OVER, DROP Heap Operations @,! Comparators >, <, = Return Stack Operations > R, R >, @ R Control Statements IF.. THEN.. ELSE, BEGIN.. WHILE.. REPEAT, DO.. LOOP Subroutine Control:,;"}, {"heading": "B. Forth Adder", "text": "0: DIGIT + 1 DUP 1 = IF > R 2 DUP 9 = IF 3 DROP 0 R > ELSE 1 + R > 1 - 4 THEN 5 THEN 6 > R 7 BEGIN DUP WHILE 8 1 - SWAP DUP 9 = IF 9 R > 1 + > R10 DROP 0 11 ELSE 12 1 + SWAP 13 THEN 14 REPEAT 15 DROP 16 R > 17; Listing 4: Complete code for adding two digits and one carry."}], "references": [{"title": "Recursive Program Synthesis", "author": ["A. Albarghouthi", "S. Gulwani", "Z. Kincaid"], "venue": "In Computer Aided Verification,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Programming Languages - Forth. American National Standard for Information Systems, ANSI X3.215-1994", "author": ["A.J.T. Committee"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Church: a language for generative models", "author": ["N. Goodman", "V. Mansinghka", "D.M. Roy", "K. Bonawitz", "J.B. Tenenbaum"], "venue": "In Proc. UAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Learning to Transduce with Unbounded Memory", "author": ["E. Grefenstette", "K.M. Hermann", "M. Suleyman", "P. Blunsom"], "venue": "In Proc. NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Ontology-Based Meta- Mining of Knowledge Discovery Workflows", "author": ["M. Hilario", "P. Nguyen", "H. Do", "A. Woznica", "A. Kalousis"], "venue": "In Meta-Learning in Computational Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "author": ["A. Joulin", "T. Mikolov"], "venue": "In Proc. NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Neural GPUs Learn Algorithms", "author": ["\u0141. Kaiser", "I. Sutskever"], "venue": "In Proc. ICLR", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Symbolic Execution and Program Testing", "author": ["J.C. King"], "venue": "Commun. ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1976}, {"title": "Inductive Programming: A Survey of Program Synthesis Techniques", "author": ["E. Kitzelmann"], "venue": "In Approaches and Applications of Inductive Programming,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Genetic Programming: On the Programming of Computers by means of Natural Selection, volume 1", "author": ["J.R. Koza"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1992}, {"title": "Neural Random-Access Machines", "author": ["K. Kurach", "M. Andrychowicz", "I. Sutskever"], "venue": "In Proc. ICLR", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Learning repetitive text-editing procedures with smartedit", "author": ["T. Lau", "S.A. Wolfman", "P. Domingos", "D.S. Weld"], "venue": "Your Wish Is My Command: Giving Users the Power to Instruct Their Software,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Toward automatic program synthesis", "author": ["Z. Manna", "R.J. Waldinger"], "venue": "Communications of the ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1971}, {"title": "Neural Programmer: Inducing latent programs with gradient descent", "author": ["A. Neelakantan", "Q.V. Le", "I. Sutskever"], "venue": "In Proc. ICLR", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Evolutionary Program Induction of Binary Machine Code and Its Applications", "author": ["P. Nordin"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Programming by Sketching for Bit-streaming Programs", "author": ["A. Solar-Lezama", "R. Rabbah", "R. Bod\u0131\u0301k", "K. Ebcio\u011flu"], "venue": "In Proc. PLDI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Combinatorial Sketching for Finite Programs", "author": ["A. Solar-Lezama", "L. Tancau", "R. Bodik", "S. Seshia", "V. Saraswat"], "venue": "In Proc. ASPLOS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}], "referenceMentions": [{"referenceID": 6, "context": "A recent and important step towards this goal are neural architectures that can learn to perform algorithms akin to traditional computers, using primitives such as memory access and stack manipulation [5, 9, 10, 14, 19].", "startOffset": 201, "endOffset": 219}, {"referenceID": 7, "context": "A recent and important step towards this goal are neural architectures that can learn to perform algorithms akin to traditional computers, using primitives such as memory access and stack manipulation [5, 9, 10, 14, 19].", "startOffset": 201, "endOffset": 219}, {"referenceID": 11, "context": "A recent and important step towards this goal are neural architectures that can learn to perform algorithms akin to traditional computers, using primitives such as memory access and stack manipulation [5, 9, 10, 14, 19].", "startOffset": 201, "endOffset": 219}, {"referenceID": 4, "context": "For example, in visual programming [7], a user often knows a rough sketch of what she wants to do, and needs to fit in the specific components.", "startOffset": 35, "endOffset": 38}, {"referenceID": 12, "context": "In programming by demonstration [15] and programming with query languages [17] a user conforms to a larger set of conditions on the data, and needs to settle details.", "startOffset": 32, "endOffset": 36}, {"referenceID": 14, "context": "In programming by demonstration [15] and programming with query languages [17] a user conforms to a larger set of conditions on the data, and needs to settle details.", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "In this approach the programmer specifies a program sketch [20] in a traditional programming language.", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "For example, the input heap of the program could be filled with the output of an LSTM [8], and its output could be connected to the neural network of an reinforcement learning agent.", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Forth is a simple but Turing-complete stack-based programming language [3, 2].", "startOffset": 71, "endOffset": 77}, {"referenceID": 2, "context": "Line 11 first puts the sequence [2 4 2 7] on the data stack D, followed by the sequence length 4, and then calls the SORT word.", "startOffset": 32, "endOffset": 41}, {"referenceID": 4, "context": "Line 11 first puts the sequence [2 4 2 7] on the data stack D, followed by the sequence length 4, and then calls the SORT word.", "startOffset": 32, "endOffset": 41}, {"referenceID": 4, "context": "At this point the stack should contain the ordered sequence [7 4 2 2].", "startOffset": 60, "endOffset": 69}, {"referenceID": 2, "context": "At this point the stack should contain the ordered sequence [7 4 2 2].", "startOffset": 60, "endOffset": 69}, {"referenceID": 8, "context": "We do this using symbolic execution [11]: we first fill the stacks and heap of a standard Forth abstract machine with symbols representing arbitrary values (e.", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "Program Synthesis The idea of program synthesis is as old as Artificial Intelligence, and thus has a long history in computer science [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 10, "context": "A large body of work has focused on using genetic programming [13] to induce programs from the given input-output specification [18].", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": "A large body of work has focused on using genetic programming [13] to induce programs from the given input-output specification [18].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "Inductive Programming approaches [12] aimed at inducing programs from incomplete specifications of the code to be implemented.", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "Heuristic search in special graph structures induced recursive programs that satisfy the input-output specifications [1].", "startOffset": 117, "endOffset": 120}, {"referenceID": 17, "context": "Lastly, SAT-based program synthesisers [21] successfully filled in simple underspecified bit-stream programmes.", "startOffset": 39, "endOffset": 43}, {"referenceID": 2, "context": "Probabilistic and Bayesian Programming Our work is closely related to probabilistic programming languages such as Church [4].", "startOffset": 121, "endOffset": 124}, {"referenceID": 7, "context": "Neural GPUs [10], on the other hand, approximated Turing completeness with a comparatively shallow model.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "Neural stacks aided in learning binary addition and recognizing context-free languages [9], and were a crucial component in language transduction experiments, together with the neural queues, and deques [6, 14].", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "Neural stacks aided in learning binary addition and recognizing context-free languages [9], and were a crucial component in language transduction experiments, together with the neural queues, and deques [6, 14].", "startOffset": 203, "endOffset": 210}, {"referenceID": 11, "context": "Neural stacks aided in learning binary addition and recognizing context-free languages [9], and were a crucial component in language transduction experiments, together with the neural queues, and deques [6, 14].", "startOffset": 203, "endOffset": 210}, {"referenceID": 5, "context": "In other work, LSTM [8] controllers controlled connections between computational primitives to learn referencing and dereferencing pointers in various array and list access problems.", "startOffset": 20, "endOffset": 23}, {"referenceID": 14, "context": "[17] learned SQLlike behavior\u2014querying tables from natural language with simple arithmetic operations.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "There are families of neural networks that can learn to compute any function, provided sufficient training data. However, given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. Here we consider the case of prior procedural knowledge such as knowing the overall recursive structure of a sequence transduction program or the fact that a program will likely use arithmetic operations on real numbers to solve a task. To this end we present a differentiable interpreter for the programming language Forth. Through a neural implementation of the dual stack machine that underlies Forth, programmers can write program sketches with slots that can be filled with learnable behaviour. As the program interpreter is end-to-end differentiable, we can optimize this behaviour directly through gradient descent techniques on user specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex transduction tasks such as sequence sorting or addition with substantially less data and better generalisation over problem sizes. In addition, we introduce neural program optimisations based on symbolic computation and parallel branching that lead to significant speed improvements.", "creator": "LaTeX with hyperref package"}}}