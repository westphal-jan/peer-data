{"id": "1605.06676", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2016", "title": "Learning to Communicate with Deep Multi-Agent Reinforcement Learning", "abstract": "We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can propagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.", "histories": [["v1", "Sat, 21 May 2016 17:20:04 GMT  (1052kb,D)", "https://arxiv.org/abs/1605.06676v1", null], ["v2", "Tue, 24 May 2016 18:16:56 GMT  (3387kb,D)", "http://arxiv.org/abs/1605.06676v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.MA", "authors": ["jakob n foerster", "yannis m assael", "nando de freitas", "shimon whiteson"], "accepted": true, "id": "1605.06676"}, "pdf": {"name": "1605.06676.pdf", "metadata": {"source": "CRF", "title": "Learning to Communicate with Deep Multi-Agent Reinforcement Learning", "authors": ["Jakob N. Foerster", "Yannis M. Assael", "Shimon Whiteson"], "emails": ["jakob.foerster@cs.ox.ac.uk", "yannis.assael@cs.ox.ac.uk", "nandodefreitas@google.com", "shimon.whiteson@cs.ox.ac.uk"], "sections": [{"heading": null, "text": "In these environments, agents must learn communication protocols in order to exchange information needed to solve the tasks. By using deep neural networks, we are able to demonstrate the endless learning of protocols in complex environments inspired by communication puzzles and multi-agent computer education problems with partial observation. We propose two approaches to learning in these areas: enhanced inter-agent learning (RIAL) and differentiated inter-agent learning (DIAL). The former uses deep Q-learning, while the latter takes advantage of the fact that agents can re-propagate error derivatives through (loud) communication channels while learning. Therefore, this approach uses centralized learning but decentralized execution. Our experiments introduce new environments to study communication protocol learning and represent a number of technical innovations that are essential for success in these areas."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Work", "text": "Communication research spans many areas, such as linguistics, psychology, evolution and AI. In AI, it is divided along several axes: a) predefined or learned communication protocols, b) planning or learning methods, c) evolution or RL, and d) cooperative or competitive settings. Given the topic of our work, we focus on related work dealing with cooperative learning of communication protocols, with the exception of Kasai et al. [8], where tabular Q-Learning Agents have to learn the content of a message in order to solve a predator prey task with communication. Most start from a predefined communication protocol rather than trying to learn protocols."}, {"heading": "3 Background", "text": "In a single agent, fully observable Q, RL setting Q [13], an agent observes the current state st-S in each discrete time step t, selects an action ut-U according to a potentially stochastic policy Q, observes a reward signal rt, and transitions to a new state st + 1. Its goal is to maximize an expectation about the discounted return, Rt = rt + \u03b3rt + 1 + \u03b32rt + 2 + \u00b7 \u00b7, where rt is the reward given time t and \u03b3 [0, 1] is a discount factor. The Q function of a policy is to maximize the ability to learn. The Q function of a policy area is the ability to learn. (s, u) = E [Rt agent] is an optimal action value function Q. (s, u) The optimal action value function Q agent."}, {"heading": "4 Setting", "text": "While no agent can observe the underlying Markov state, each agent receives a private observation oligarch, which is correlated with the most. At each step in time, agents select an environmental action that affects the environment and a communication action that is observed by other agents but has no direct impact on the environment or reward. We are interested in such constellations because only when multiple agents and partial observation exist side by side, agents have the incentive to communicate. Since no communication protocol is given a priori, agents must develop and agree such a protocol in order to complete the task. Since logs are mappings of action observation histories to sequences of messages, the space of logs is extremely high dimensional. Automatic detection of effective logs in this space remains a difficult challenge to master."}, {"heading": "5 Methods", "text": "In this section, we present two approaches to learning communication protocols."}, {"heading": "5.1 Reinforced Inter-Agent Learning", "text": "The simplest approach, which we call enhanced inter-agent learning (RIAL), is to combine DRQN with independent Q-learning for action and communication selection.The Q network of each agent represents Qa (oat, m a \u2032 t \u2212 1, h a \u2212 1, among others), the conditions for the individual hidden state of that agent and observation. Here and around the world, the agent index is a.To avoid needing a network of | U | M | outputs, we share the network in Qau and Qam, the Q values for the environment and communication actions, respectively. Similar to [19], the action selector separately chooses uat uat and m a t of Qu and Qm, using a greedy policy, the network requires only in Qau and Qam."}, {"heading": "5.2 Differentiable Inter-Agent Learning", "text": "While RIAL can share parameters between agents, it still does not take full advantage of centralized Q-learning processes. Specifically, agents do not provide feedback about their communication actions. Contrast this with human communication, which is rich in tight feedback loops. To address this limitation, we naturally suggest discreet interaction errors, with listeners sending fast nonverbal queues to the speaker that point to the level of understanding and interest. The key finding behind DIAL is that the combination of centralized learning and Q networks makes it possible not only to share parameters, but to push gradients from one agent to another through the communication channel. While RIAL is end-to-end trainable within each agent, DIAL is an end-to-end trainable via agents."}, {"heading": "6 Experiments", "text": "In fact, it is a question of a way in which the people of a country in which they are able to govern and govern themselves are not a country, but a country in which they are able to live, but a country in which they live, a country in which they live, a country in which they live, a country in which they live, a country in which they live, and a country in which they live, a country in which they live, a country in which they live, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "6.3 MNIST Games", "text": "In fact, the fact is that most of them are able to put themselves in another world, in which they put themselves in another world, in which they find themselves in another world, in which they find themselves in another world, in which they do not find themselves again, in which they do not find themselves in another world, in which they do not find themselves again, in which they do not find themselves again, in which they do not find themselves, in which they find themselves in another world, in which they do not find themselves again, in which they live in another world, in which they do not find themselves again, in which they do not find themselves again, in which they do not find themselves again, in which they live themselves, in which they live themselves, in which they live themselves, in which they live, in which they live themselves, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "7 Conclusions", "text": "This paper developed novel environments and successful techniques for learning communication protocols. It presented a detailed comparative analysis covering key factors in learning communication protocols with deep networks, including differentiated communication, neural network architecture design, channel noise, bound parameters, and other methodological aspects. This paper should be considered as a first attempt at learning communication and language with deep learning approaches. The daunting task of understanding communication and language in its full glory includes compositionality, concept cancellation, conversation agents, and many other important issues that remain to be addressed. However, we are optimistic that the approaches proposed in this paper can play a significant role in addressing these challenges."}, {"heading": "Acknowledgements", "text": "This work was supported by the Oxford-Google DeepMind Graduate Scholarship and the EPSRC. We thank Brendan Shillingford, Serkan Cabi and Christoph Aymanns for their helpful comments."}, {"heading": "A DIAL Details", "text": "At each time step, we will select an action for each agent - \"Q\" - \"Q\" -1, \"A\" -1, \"A\" -1, \"A-1\" -1, \"A-1\" -1, \"A-2\" -1, \"A-2\" -1, \"A-1\" -1, \"A-2\" -1, \"A-1\" -1, \"\" A-2 \"-1,\" A-2 \"-1,\" \"A-2,\" -1, \"\" \"-1,\" \"\" A-1-1 \"-1,\" \"\" A-2-2 \"-1,\" -, \"\" \"- 1-2,\" \"-,\" \"\" - 1-2, \"\" - 1-2, \"-,\" \"- 1-2,\" -, \"- 1-2,\" -, \"\" - 1-2, \"-,\" - 1-2, \"-,\" - 1-2."}, {"heading": "B MNIST Games: Further Analysis", "text": "Our results show that DIAL deals more effectively with stochastic rewards in the colored MNIST game than RIAL. To better understand why, we consider a simpler two-agent problem with a structurally similar reward function r = (\u2212 1) (s1 + s2 + a2), which is antisymmetrical in the observations and actions of the agents. Here, random digits s1, s2, 0, 1 are entered to Agent 1 and Agent 2 and u2, 1, 2 with equal probability as binary action. Agent 1 can send a single message, m1. Until a protocol has been learned, the average reward for each action of the agent 2 is 0, since the reward is averaged over s1, with equal probability + 1 or \u2212 1. Similarly, the TD error for Agent 1, the sender, zero for each message m: E [\u0394 Q (s1, m1)] = (s1), the agent, the agent, the agent, the agent, the agent (m1), the (1), the message (\u2212 1), the message, the agent, the agent, the agent, the agent, the message (1), the message (\u2212 1), the message, the agent, the agent, the agent, the message (1), the message (1), the message, the agent, the agent, the agent, the agent, the agent, the message (\u2212 1), the message (1)."}, {"heading": "C Effect of Noise: Further Analysis", "text": "(...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) () (...) (...) () (...) () () (...) (...) (...) (...) (...) () () () (...) () () () (...) () () (...) () (... (... () () (...) () () (...) () () (...) (...) () (...) () () (...) () () (...) () () (...) () () (...) () (...) (...) (...) () () (...) () (...) ("}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We consider the problem of multiple agents sensing and acting in environments<lb>with the goal of maximising their shared utility. In these environments, agents must<lb>learn communication protocols in order to share information that is needed to solve<lb>the tasks. By embracing deep neural networks, we are able to demonstrate end-<lb>to-end learning of protocols in complex environments inspired by communication<lb>riddles and multi-agent computer vision problems with partial observability. We<lb>propose two approaches for learning in these domains: Reinforced Inter-Agent<lb>Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses<lb>deep Q-learning, while the latter exploits the fact that, during learning, agents can<lb>backpropagate error derivatives through (noisy) communication channels. Hence,<lb>this approach uses centralised learning but decentralised execution. Our experi-<lb>ments introduce new environments for studying the learning of communication<lb>protocols and present a set of engineering innovations that are essential for success<lb>in these domains.", "creator": "LaTeX with hyperref package"}}}