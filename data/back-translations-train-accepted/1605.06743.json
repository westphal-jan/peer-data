{"id": "1605.06743", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2016", "title": "Inductive Bias of Deep Convolutional Networks through Pooling Geometry", "abstract": "Our formal understanding of the inductive bias that drives the success of convolutional networks on computer vision tasks is limited. In particular, it is unclear what makes hypotheses spaces born from convolution and pooling operations so suitable for natural images. In this paper we study the ability of convolutional arithmetic circuits to model correlations among regions of their input. Correlations are formalized through the notion of separation rank, which for a given input partition, measures how far a function is from being separable. We show that a polynomially sized deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial separation ranks for others. The network's pooling geometry effectively determines which input partitions are favored, thus serves as a means for controlling the inductive bias. Contiguous pooling windows as commonly employed in practice favor interleaved partitions over coarse ones, orienting the inductive bias towards the statistics of natural images. In addition to analyzing deep networks, we show that shallow ones support only linear separation ranks, and by this gain insight into the benefit of functions brought forth by depth - they are able to efficiently model strong correlation under favored partitions of the input.", "histories": [["v1", "Sun, 22 May 2016 06:15:31 GMT  (295kb,D)", "http://arxiv.org/abs/1605.06743v1", null], ["v2", "Fri, 4 Nov 2016 16:06:20 GMT  (606kb,D)", "http://arxiv.org/abs/1605.06743v2", null], ["v3", "Wed, 14 Dec 2016 10:29:18 GMT  (606kb,D)", "http://arxiv.org/abs/1605.06743v3", null], ["v4", "Mon, 17 Apr 2017 18:36:08 GMT  (606kb,D)", "http://arxiv.org/abs/1605.06743v4", "Published as a conference paper at ICLR 2017"]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["nadav cohen", "amnon shashua"], "accepted": true, "id": "1605.06743"}, "pdf": {"name": "1605.06743.pdf", "metadata": {"source": "CRF", "title": "Inductive Bias of Deep Convolutional Networks through Pooling Geometry", "authors": ["Nadav Cohen", "Amnon Shashua"], "emails": ["cohennadav@cs.huji.ac.il", "shashua@cs.huji.ac.il"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before an agreement is reached."}, {"heading": "2 Preliminaries", "text": "The analyses performed in this thesis are based on concepts and results from the field of tensor analysis. (J) In this section, we determine the minimum background required to follow our arguments. (D) We limit the discussion to these specific cases, as they are sufficient for our needs and easier to capture. (D) The core concept of tensor analysis is a tensor that can simply be thought of as a multi-dimensional array for our purposes. (D) The order of a tensor is defined to be the number of indexed entries in the array called modes. (D) The dimension of a tensor in a given mode is defined as the number of values that can be taken from the index in this mode. (For example, a matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matx matrix matrix matrix matrix matrix matrix matrix"}, {"heading": "3 Convolutional arithmetic circuits", "text": "The revolutionary arithmetic circuit architecture we focus on in this paper is the one considered in [8] (in Fig. 1 (a). Instances processed by a network are presented as N-length sequences of s-dimensional vectors. They are generally considered images, with the s-dimensional vectors corresponding to the local fields being considered. Instances could be presented as 32-by-32 RGB images, with local fields crossing the three color bands. In this case, a patch is taken around each pixel in an image (boundaries padded), we have N = 1024 and s = 75. Throughout the paper, we denote a general instance of X = 5 regions crossing the three color ribbons. xN) with xi-Rs representing their patches. The first layer processing an instance is referred to as representation. It consists of the application of M representation functions."}, {"heading": "4 Separation rank", "text": "In this section we define the term separator rank for functions realized by revolutionary arithmetic circuits. (J = = J) | J (sec. 3), i.e. functions that are input X = (x1,.., xN). However, the separator rank serves as a measure of the correlations that such functions produce between different groups of input fields, i.e. different subsets of the variable set {x1,., xN}. Let (I, J) be a division of the input indices, i.e. I and J are fragmented subsets of [N] whose composition [N] exists. We can write that I = {i1,.,.,.,."}, {"heading": "5 Correlation analysis", "text": "In this section, we analyze revolutionary arithmetic circuits (paragraph 3) with respect to the correlations they model between the pages of different input partitions, i.e., with respect to the separation lines (paragraph 4) that they support under different partitions (I, J) of [N]. We begin in paragraph 5.1 by establishing a correspondence between separation lines and coefficient tensor matrix lines, which correspondence is then used in paragraphs 5.2 and 5.3 to analyze the deep and flat networks represented in paragraph 3, respectively. We note that we are focusing on these particular networks purely for the sake of simplicity of presentation - the analysis can easily be adapted to alternative networks with different depths and pooling schemes."}, {"heading": "5.1 From separation rank to matricization rank", "text": "Let hy be a function realized by a revolutionary arithmetic circuit, with corresponding coefficients tensor J (eq. 2). Denote by (I, J) an arbitrary division of [N], i.e. I \u00b7 J = [N]. We are interested in studying sep (hy; I, J) - the separation rank of hy w.r.t. (I, J) (eq. 5). As claim 1 below states, the representation functions {f\u03b8d} d (M] is linearly independent (there is no reason to choose them differently 3), this separation rank is equal to the rank of JAyKI, J - the matrification of the tensor Ay w.r.t. The partition (I, J) thus translates to the ranking of matricized tensors Jy. I maintain hy is a function realized by a revolutionary arithmetic circuit."}, {"heading": "5.2 Deep network", "text": "In this subsection, we examine correlations modeled by the deep network represented in paragraph 3 (Figure 1 (a) of size 4 related windows and hidden layers (L = 4N). \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 In accordance with paragraph 5.1, we represent this by characterizing the maximum ranks of this tensor among various divisions of [N]. Let us (I, J) make any division, i.e. I \u00b7 J = [N]. Matrifying the last row of eq. 3 w.r.t. (I, J) while applying the relation in eq. 1, we obtain: JAyKI = empirical partition, i.e. I. L = [N]. The last row of eq. 3 w.r.t. (I, J), while applying the relation in eq. 1, results in: JAyKempi = I."}, {"heading": "5.3 Shallow network", "text": "(...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (..., (...), (...), (...), (..., (...), (..., (...), (...), (..., (...), (..., (...), (...), (..., (...), (..., (...), (...), (..., (...), (...), (..., (...), (...), (..., (...), (...), (...), (..., (...), (...), (...), (...), (...), (..., (...), (...), (...), (..., (), (...), (...), (...), (...), (..., (...), (..., (), (...), (...), (..., (), (...), (), (...), (...), (..., (), (...), (...), (...), (..., (), (...), (...), (...), (...), (...), (..., (), (...), (...), (), (..., (...), (...), (), (...), (...), (...), (...), (..., (...), (...), (...), (...), (...), (...), (...), (...), (...), (...),"}, {"heading": "6 Inductive bias through pooling geometry", "text": "In fact, it is such that most of us will be in a position to be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "7 Discussion", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "Acknowledgments", "text": "This work is partly funded by the Intel Grant ICRI-CI No. 9-2012-6133 and the ISF Center 1790 / 12. Nadav Cohen is supported by a Google Fellowship in Machine Learning."}, {"heading": "A Deferred proofs", "text": "The first part of the proof is elementary and does not make use of the representation functions' (f-J) of linear independence. The second part is based on this assumption and employs a slightly more advanced mathematical machinery. In the course of the proof, we assume that the partition (I, J) of the [N] function is such that I take smaller values while J takes larger ones. That is, we assume that I = 1, and J = 1, without loss of generality. 6To prove that the sep (hy; I, J) is equal."}], "references": [{"title": "Introduction to matrix analysis, volume 960", "author": ["Richard Bellman", "Richard Ernest Bellman"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1970}, {"title": "Numerical operator calculus in higher dimensions", "author": ["Gregory Beylkin", "Martin J Mohlenkamp"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Multivariate regression and machine learning with sums of separable functions", "author": ["Gregory Beylkin", "Jochen Garcke", "Martin J Mohlenkamp"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "The zero set of a polynomial", "author": ["Richard Caron", "Tim Traynor"], "venue": "WSMR Report 05-02,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Simnets: A generalization of convolutional networks", "author": ["Nadav Cohen", "Amnon Shashua"], "venue": "Advances in Neural Information Processing Systems (NIPS), Deep Learning Workshop,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Convolutional rectifier networks as generalized tensor decompositions", "author": ["Nadav Cohen", "Amnon Shashua"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Deep simnets", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "On the expressive power of deep learning: A tensor analysis", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": "Conference On Learning Theory (COLT),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Shallow vs. deep sum-product networks", "author": ["Olivier Delalleau", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1512.03965,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "On the efficient evaluation of coalescence integrals in population balance models", "author": ["Wolfgang Hackbusch"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Tensor Spaces and Numerical Tensor Calculus, volume 42 of Springer Series in Computational Mathematics", "author": ["Wolfgang Hackbusch"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Multiresolution quantum chemistry in multiwavelet bases", "author": ["Robert J Harrison", "George I Fann", "Takeshi Yanai", "Gregory Beylkin"], "venue": "In Computational Science-ICCS", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Tensor Decompositions and Applications", "author": ["Tamara G Kolda", "Brett W Bader"], "venue": "SIAM Review (),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1995}, {"title": "Learning real and boolean functions: When is deep better than shallow", "author": ["Hrushikesh Mhaskar", "Qianli Liao", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1603.00988,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "author": ["Razvan Pascanu", "Guido Montufar", "Yoshua Bengio"], "venue": "arXiv preprint arXiv,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "I-theory on depth vs width: hierarchical function composition", "author": ["Tomaso Poggio", "Fabio Anselmi", "Lorenzo Rosasco"], "venue": "Technical report, Center for Brains, Minds and Machines (CBMM),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Functional analysis. international series in pure and applied mathematics", "author": ["Walter Rudin"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1991}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Going Deeper with Convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "Perhaps the most successful exemplar of inductive bias to date manifests itself in the use of convolutional networks ([18]) for computer vision tasks.", "startOffset": 118, "endOffset": 122}, {"referenceID": 15, "context": "[17, 27, 26, 14]), largely responsible for the resurgence of deep learning ([19]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 24, "context": "[17, 27, 26, 14]), largely responsible for the resurgence of deep learning ([19]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 23, "context": "[17, 27, 26, 14]), largely responsible for the resurgence of deep learning ([19]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 13, "context": "[17, 27, 26, 14]), largely responsible for the resurgence of deep learning ([19]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 8, "context": "In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example [9, 23, 21, 28, 10, 24, 20]).", "startOffset": 151, "endOffset": 178}, {"referenceID": 20, "context": "In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example [9, 23, 21, 28, 10, 24, 20]).", "startOffset": 151, "endOffset": 178}, {"referenceID": 18, "context": "In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example [9, 23, 21, 28, 10, 24, 20]).", "startOffset": 151, "endOffset": 178}, {"referenceID": 9, "context": "In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example [9, 23, 21, 28, 10, 24, 20]).", "startOffset": 151, "endOffset": 178}, {"referenceID": 21, "context": "In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example [9, 23, 21, 28, 10, 24, 20]).", "startOffset": 151, "endOffset": 178}, {"referenceID": 17, "context": "In recent years, a large body of research was devoted to proving existence of depth efficiency under different types of architectures (see for example [9, 23, 21, 28, 10, 24, 20]).", "startOffset": 151, "endOffset": 178}, {"referenceID": 7, "context": "Recently, [8] analyzed the depth efficiency of convolutional arithmetic circuits, showing that besides a negligible set, all functions realizable by a deep network require exponential size in order to be realized (or approximated) by a shallow one.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "In subsequent work, [6] proved that for the popular convolutional rectifier networks, i.", "startOffset": 20, "endOffset": 23}, {"referenceID": 19, "context": "convolutional networks with rectified linear (ReLU, [22]) activation and max or average pooling, depth efficiency exists but it is not complete, meaning such networks are inferior to convolutional arithmetic circuits in terms of depth efficiency.", "startOffset": 52, "endOffset": 56}, {"referenceID": 4, "context": "Motivated by these results, and by the fact that convolutional arithmetic circuits are equivalent to SimNets, a new deep learning architecture that has recently demonstrated promising empirical performance ([5, 7]), we focus in this paper on convolutional arithmetic circuits as representatives of the class of convolutional networks.", "startOffset": 207, "endOffset": 213}, {"referenceID": 6, "context": "Motivated by these results, and by the fact that convolutional arithmetic circuits are equivalent to SimNets, a new deep learning architecture that has recently demonstrated promising empirical performance ([5, 7]), we focus in this paper on convolutional arithmetic circuits as representatives of the class of convolutional networks.", "startOffset": 207, "endOffset": 213}, {"referenceID": 1, "context": "a partition of its input is measured through the notion of separation rank ([2]), which can be viewed as quantifying how far the function is from being equal to a product of factors, each depending on input regions from only one side of the partition.", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "By this we return to the complete depth efficiency result of [8], but with an added important insight into the benefit of functions brought forth by depth \u2013 they are able to efficiently model strong correlation under favored partitions of the input.", "startOffset": 61, "endOffset": 64}, {"referenceID": 11, "context": "In this section we establish the minimal background required in order to follow our arguments 1 , referring the interested reader to [12] for a broad and comprehensive introduction to the field.", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "1 The definitions we give are actually concrete special cases of more abstract algebraic definitions as given in [12].", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "The convolutional arithmetic circuit architecture on which we focus in this paper is the one considered in [8], portrayed in fig.", "startOffset": 107, "endOffset": 110}, {"referenceID": 11, "context": "3 describes what is called a hierarchical tensor decomposition (see chapter 11 in [12]), with underlying tree over modes being a full quad-tree (corresponding to the fact that the network\u2019s pooling windows cover four entries each).", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "4 is an instance of the classic CP decomposition, also known as rank-1 decomposition (see [16] for a historic survey).", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "To conclude this section, we relate the background material above, as well as our contribution described in the upcoming sections, to the work of [8].", "startOffset": 146, "endOffset": 149}, {"referenceID": 7, "context": "The central result in [8] relates to inductive bias through the notion of depth efficiency \u2013 it is shown that in the parameter space of a deep network, all weight settings but a set of measure zero give rise to functions that can only be realized (or approximated) by a shallow network if the latter has exponential size.", "startOffset": 22, "endOffset": 25}, {"referenceID": 11, "context": "From the theory of tensor products between L spaces (see [12] for a comprehensive coverage), we know that any h\u2208L((R) ), i.", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "The concept of separation rank was introduced in [2] for numerical treatment of high-dimensional functions.", "startOffset": 49, "endOffset": 52}, {"referenceID": 12, "context": "Since then, it has been employed for various applications, including quantum chemistry ([13]), particle engineering ([11]), machine learning ([3]), and more.", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "Since then, it has been employed for various applications, including quantum chemistry ([13]), particle engineering ([11]), machine learning ([3]), and more.", "startOffset": 117, "endOffset": 121}, {"referenceID": 2, "context": "Since then, it has been employed for various applications, including quantum chemistry ([13]), particle engineering ([11]), machine learning ([3]), and more.", "startOffset": 142, "endOffset": 145}, {"referenceID": 3, "context": "\u03b1=1 a \u03b1 \u00b7 q \u03c6L\u22121,\u03b1 y I\u2229[4L\u22121],J\u2229[4L\u22121] q \u03c6L\u22121,\u03b1 y (I\u22124L\u22121)\u2229[4L\u22121],(J\u22124L\u22121)\u2229[4L\u22121] q \u03c6L\u22121,\u03b1 y (I\u22122\u00b74L\u22121)\u2229[4L\u22121],(J\u22122\u00b74L\u22121)\u2229[4L\u22121] q \u03c6L\u22121,\u03b1 y (I\u22123\u00b74L\u22121)\u2229[4L\u22121],(J\u22123\u00b74L\u22121)\u2229[4L\u22121] For every k \u2208 [4] define IL\u22121,k := (I \u2212 (k \u2212 1) \u00b7 4L\u22121) \u2229 [4L\u22121] and JL\u22121,k := (J \u2212 (k \u2212 1) \u00b7 4L\u22121) \u2229 [4L\u22121].", "startOffset": 190, "endOffset": 193}, {"referenceID": 3, "context": "L\u22121}, k \u2208 [N/4]: Il,k := (I \u2212 (k \u2212 1) \u00b7 4) \u2229 [4] Jl,k := (J \u2212 (k \u2212 1) \u00b7 4) \u2229 [4] (6) That is to say, (Il,k, Jl,k) represents the partition induced by (I, J) on the set of indexes {(k\u22121) \u00b74+ 1, .", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "L\u22121}, k \u2208 [N/4]: Il,k := (I \u2212 (k \u2212 1) \u00b7 4) \u2229 [4] Jl,k := (J \u2212 (k \u2212 1) \u00b7 4) \u2229 [4] (6) That is to say, (Il,k, Jl,k) represents the partition induced by (I, J) on the set of indexes {(k\u22121) \u00b74+ 1, .", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": ", N} (9) Under (I , J), all partitions induced on size-4L\u22121 patch groups (quadrants of [N ]) are completely one-sided (min{|IL\u22121,k|, |JL\u22121,k|} = 0 for all k \u2208 [4]), resulting in the upper bound being no greater than rL\u22121 \u2013 linear in network size.", "startOffset": 159, "endOffset": 162}, {"referenceID": 7, "context": "Since all but a negligible set of the functions realizable by the deep network give rise to maximal separation ranks (claim 2), we obtain the complete depth efficiency result of [8].", "startOffset": 178, "endOffset": 181}, {"referenceID": 7, "context": "However, unlike [8], which did not provide any explanation for the usefulness of functions brought forth by depth, we obtain an insight into their utility \u2013 they are able to efficiently model strong correlation between favored partitions of the input.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "By this we derive the depth efficiency result of [8], but in addition, provide an insight into the benefit of functions brought forth by depth \u2013 they are able to efficiently model strong correlation under favored partitions of the input.", "startOffset": 49, "endOffset": 52}], "year": 2016, "abstractText": "Our formal understanding of the inductive bias that drives the success of convolutional networks on computer vision tasks is limited. In particular, it is unclear what makes hypotheses spaces born from convolution and pooling operations so suitable for natural images. In this paper we study the ability of convolutional arithmetic circuits to model correlations among regions of their input. Correlations are formalized through the notion of separation rank, which for a given input partition, measures how far a function is from being separable. We show that a polynomially sized deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial separation ranks for others. The network\u2019s pooling geometry effectively determines which input partitions are favored, thus serves as a means for controlling the inductive bias. Contiguous pooling windows as commonly employed in practice favor interleaved partitions over coarse ones, orienting the inductive bias towards the statistics of natural images. In addition to analyzing deep networks, we show that shallow ones support only linear separation ranks, and by this gain insight into the benefit of functions brought forth by depth \u2013 they are able to efficiently model strong correlation under favored partitions of the input.", "creator": "LaTeX with hyperref package"}}}