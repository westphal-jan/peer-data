{"id": "1605.06792", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2016", "title": "Active Nearest-Neighbor Learning in Metric Spaces", "abstract": "We propose a pool-based non-parametric active learning algorithm for general metric spaces, called called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. Our algorithm is based on a generalized sample compression scheme and a new label-efficient active model-selection procedure.", "histories": [["v1", "Sun, 22 May 2016 14:00:27 GMT  (23kb)", "https://arxiv.org/abs/1605.06792v1", null], ["v2", "Sun, 16 Oct 2016 08:23:18 GMT  (37kb)", "http://arxiv.org/abs/1605.06792v2", null]], "reviews": [], "SUBJECTS": "cs.LG math.ST stat.TH", "authors": ["aryeh kontorovich", "sivan sabato", "ruth urner"], "accepted": true, "id": "1605.06792"}, "pdf": {"name": "1605.06792.pdf", "metadata": {"source": "CRF", "title": "Active Nearest-Neighbor Learning in Metric Spaces", "authors": ["Aryeh Kontorovich", "Sivan Sabato", "Ruth Urner"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 5.06 792v 2 [cs.L G] 1"}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "1.1 Related work", "text": "The theory of active learning has attracted a lot of attention over the last ten years. (...) It has been shown that most of them are people who are able to identify themselves. (...) It has been shown that they are people who are able to identify themselves. (...) It has been shown that they are people who are able to identify themselves. (...) It has been shown that they are people who are able to identify themselves. (...) It has been shown that they are people who are able to identify themselves. (...) There are people who are able to identify themselves. (...) There are people who are able to identify themselves. (...) There are people who are able to identify themselves. (...) There are people who are able to identify themselves. (...) There are people who are able to identify themselves. (...) There are people who are able to identify themselves. (...) There are people who are able to identify themselves. (...) There are people who are able to identify themselves."}, {"heading": "2 Preliminaries", "text": "In this section we define the necessary preparatory work. In Section 2.1 we formally define the setting and the necessary notation. In Section 2.2 we discuss networks in metric spaces and in Section 2.3 we present the guarantees of the compression-based passive learner from Gottlieb et al. [2016b]."}, {"heading": "2.1 Setting and notation", "text": "Let us consider learning in a general metric space (X, R), where X is a set and R is the metric to X. (Our problem is to classify the instance space X into a finite term group Y. Let us suppose that there is a distribution D over X \u00b7 Y, and let S \u0445 Dm be a labeled sample of the size m, where m is an integer. Call the sequence of unlabeled points in S by U (S). We sometimes treat S and U (S) as multisets, because the order is unimportant. For a labeled plural S X \u00b7 Y and y Y, denote Sy: = {x | (x, y) \u0445S}; in particular U (S) = Y (S) as multisets, because the order is unimportant. The error of a classifier h: X \u2192 Y on D is denotederr (h, D): x x (h) 6 = Y], where (X).The empirical error on a labeled sample."}, {"heading": "2.2 Nets", "text": "One-set A-X is not separate if infa, a-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A"}, {"heading": "2.3 Passive compression-based nearest-neighbors", "text": "The original margin-based methods of Luxburg and Bousquet [2004] and Gottlieb et al. [2010] analyzed the generalization performance via the technique of the Lipschitz extension. Later, it was noted in Gottlieb et al. [2014b] that the presence of a margin allows compression - almost optimal indeed. We say that a labeled multiset S (2011) is separated, for a definition [0, 1] and t > 0 (represents a margin t with noise), if one can remove a fracture of the points in S, and in the resulting multiset S is at least t-far apart from each other. Formally, we have the following definition 2.2. S is (ordered, t) -separated if there is a fracture of the points in S, and in the resulting multiset points with different designations are at least t-far apart."}, {"heading": "3 Main results", "text": "This approach, which is described in detail in the following sections, does not require locating and removing all obstructive points in Sin, and can be implemented in an active environment where a small number of labels are used. The resulting active learning algorithm method 1 MARMANN, has an error guarantee depending on the passivity of the passive learner, and a label complexity that can be significantly lower. We understand the subset used by the next neighbor rule as a compression set.Algorithm 1 MARMANN: MArgin Regularized Metric Active Nearest Neighbor input Unlabeled sample Uin size m, 1). # SelectScale is given in Section 5, Alg. 4. SelectScale is given in Section 5, SelectScale."}, {"heading": "4 Active nearest-neighbor at a given scale", "text": "The main challenges for active learning in our non-parametric environment are the selection of models, that is, the selection of a good scale that we are similar to the passive learners of Gottlieb et al. (\"There are only a few examples that we are able to identify ourselves\"). (\"There are only a few examples that are able to identify themselves.\" (\"There are only a few examples that are able to identify themselves.\") The passive algorithms that are able to determine a compression of Sin the first cognition and removal of Sin all points that interfere with each other (. \") -separation in a particular scale t > 0. (\" There are no significant differences, but no significant differences \")."}, {"heading": "5 Model Selection", "text": "We will now show you how to select the scale t that will be used to generate the closest neighboring rule. The biggest challenge is to do this with low label complexity: Creating the full scale t classification rule requires a number of labels that depend on N (t), which can be very large. We want MARMANN's label complexity to depend only on N (t) (where t) is the selected scale that is in the order mG. Therefore, we can only invest a limited number of labels in each scale tested when selecting models. To keep label complexity low, we also want to avoid testing all scales. In Section 5.1, we will describe how we estimate the error on a particular scale. In Section 5.2, we offer a search method similar to binary search that uses the estimation method to select a single scale."}, {"heading": "5.1 Estimating the error at a given scale", "text": "In the event that the selected scale is set to t, our selection procedure performs a search, similar to binary search, of the possible scales. For each scale tested, the procedure estimates the empirical error rate (t): = err (hnnS) within a certain accuracy, using an estimation method given below. EstimateErr gives an estimate (t), up to a certain threshold. > 0, using that of Sin. To estimate the error, we must estimate random examples from Sin and check the prediction of hnnS (t) on these examples. The prediction error of each fixed hypothesis hon a random example from Sin is an independent Bernoulli variable with expectation (h, Sin)."}, {"heading": "5.2 Selecting a scale", "text": "The model procedure Selectal, specified in Alg. 4, implements its search on the basis of the guarantees in Cor. 5.2. First, we present some notations. We want MARMANN to obtain a generalization guarantee that competes with Gmin (m, \u03b4). Denote\u03c6 (t): (N (t) + 1) log (m) + log (1) log (1) m, (4) and letG (\u2264): = 23 (t) + 3 \u221a (t). Note: for all others (t), GB (n), ltm, 1) = m \u2212 N (t) G (0). If we choose G (t), G (t), t (t), or G (t), t) that we (t) that we omit (t) the second t for brevity. Instead of a direct optimization of G (t), we will select a scale based on our estimate (t)."}, {"heading": "By Cor. 5.2, \u01eb\u0302(t) \u2264 max(\u03c6(t), 4\u01eb(t)/3), therefore \u03c8\u0302t \u2264 43\u03c8t. Therefore G(\u01eb(t\u0302)) \u2264 37\u03c8min.", "text": "Therefore, the idea of the evidence is as follows: First, we show that the aggregate number of evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the basis of the evidence requests on the evidence requests on the basis of the evidence requests on the evidence requests on the basis of the evidence requests on the evidence requests on the evidence requests on the basis of the evidence requests on the evidence requests on the evidence requests on the evidence requests on the evidence requests on the evidence requests on the evidence requests on the basis of the evidence requests on the evidence requests on the basis of the evidence requests on the evidence requests on the evidence requests on the evidence requests on the"}, {"heading": "6.1 Sample compression with side information", "text": "It seems as if the generalization of the boundaries of S (I) by a defined sample of S (I) is possible. (I) It seems as if the generalization of S (I) is possible. (I) It seems as if the generalization of S (I) and the generalization of S (I) is possible. (I) It seems as if the generalization of S (I) is possible. (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I. (I). (I). (I). (I). (I. (I). (I). (I). (I. (I). (I). (I.). (I. (I). (I.). (I.). (I.). (I.). (I. (I.). (I.). (I.). (I. (I.). (I.). (I.). (I. (I.). (I.). (I. (I.). (I.). (I.).). (I. (I.). (I.). (I. (I. (I.).). (I.). (I. (I.). (I. (I. (I.). (I. (I. (I.).). (I.). (I. (I. (I.). (I.). (I.). (I. (I. (I. (I.). I. (I.). (I.). (I. I. (I.). (I.). I. (I. I. (I.). (I. (I.). I. (I.). I. (I"}, {"heading": "6.2 Proof of Theorem 3.1", "text": "The proof of the main theorem, Theorem 3.1, which provides the guarantee for MARMAN, is almost directly derived from Theorem 6.2, Theorem 4.3, Theorem 5.6 and Theorem 5.5 of Theorem 3.1. Within the framework of these events, we have through Theorem 5.6 that if Gmin (m) and V (t) of Theorem 4.3 and Cor. 5.2 for all t (t) Ttest Distmon with a probability of at least 1 \u2212 \u03b4 / 2. Within the framework of these events, we have through Theorem 5.6 that if Gmin (m) and V (t) of Theorem 4.3, GB (t), N (t), m, 1) \u2264 O (min tGB (t), N (t), N (t), Q (m, 1), 3) the explanation of Theorem 6.2, with a probability of Theorem 6.2, GB (t)."}, {"heading": "7 Passive learning lower bounds", "text": "Theorem 3.2 subdivides the performance of a passive learner who observes a limited number of random labels from Sin. Theorem 3.2 is chosen to correspond to the same order as the number of labels observed by MARMANN for the case analysed in Section 3.2. We derive theorem 3.2 from a more general result relating to the sample complexity of passive learning; the general result is given as Theorem 7.1 in Section 7.1. The proof for Theorem 3.2 is provided in Section 7.2. We note that while the lower boundaries assume that the passive learner observes only the randomly labelled sample of the size, in fact their proofs apply even if the algorithm has access to the complete unlabelled sample of size m that is scanned by Sin. This is because the lower boundary is based on the learner having to distinguish between distributions that all have the same boundaries. In this scenario, the access to unlabelled algorithm does not provide additional examples of information."}, {"heading": "7.1 A general lower bound", "text": "In this section, we show a general example of passive learning complexity that may be of independent interest. (< b) We are aware of two existing lower limits for agnostic limited noise PAC (< b). (http: / / p). (http: / / p). (http: / / p). (http: / / p). (http: / / p). (http: / / p). (http: / / p). (http: / / p). (http: / / p). (http: / / p). (http: / / p). (http: / / p). (http: / / p). (http: / / p). (http: / p). (p.) (http: / p.). (http: / p.). (http: / p.). (http: / p.) (http: / p.). (http: / p.) (http: / p.). (http: / p.). (http: / p.) (http: / p.) (http: / p.) (http: / p.) (http: / p.) (http: / p.) (http: / p.) (http: / / p.) (http: / p.) (http: / p.) (http: / p.) (http: / / p.) (http: / p.) (http: / / p.) (http: / p.) (http: / p.) (http: / / p.) (http: / p. (http: / p.) (http: / p.) (http: / p.) (http: /. (http: / p.) (http: / p. (http: / p.) (http: / p."}, {"heading": "7.2 Proof of Theorem 3.2", "text": "We divide the proof into several stages. (i) Define a family of opposing distributions. (i) Define a group of opposing distributions. (i) Define a group of opposing distributions. (i) Define a group of opposing distributions. (i) Define a group of opposing distributions. (i) Define a group of opposing distributions. (i) Define a group of opposing distributions. (i) Define this by the construction described in the proof of the theorem. (i) Define a group of opposing distributions. (i) Define a group of opposing distributions. (i) The marginal distribution via T = x1,.) Define a mass from 1 \u2212 p to xd and spread the remaining mass uniformly over the other points, as in (16). The \"heavy\" point has a deterministic label and the \"remaining points.\""}, {"heading": "8 Active learning lower bound", "text": "To prove the theory, we must first prove a result similar to the classical no-free lunch theorem (2014, Theorem 5.1), unless it applies to active learning algorithms. The proof closely follows the proof of the classical no-free lunch theorem given in Shalev-Shwartz and Ben-David (2014, Theorem 5.1), with appropriate modifications. Allow an active learning algorithm to be given over a finite domain X, which as input is a random sample S labeled with hidden labels (with hidden labels) and outputs h. If a query consists of less than X / 2 labels of S, then there is a distribution D over X (0, 1), so that \u2022 its marginality is uniform on X, and for each x."}, {"heading": "9 Discussion", "text": "Our approach provides competitive error guarantees for general distributions in a general metric space, while the complexity of the labels is significantly lower than that of any passive learner with the same guarantees. MARMANN provides fully empirical error estimates that can easily be calculated from finite samples, in contrast to classical techniques that represent limits and rates that depend on unknown distribution-dependent quantities. An interesting question is whether the guarantees can be related to the Bayes error of distribution, and our error guarantees give a constant factor above the error guarantees of Gottlieb et al. [2016b]. A variant of this approach [Gottlieb et al., 2010] has proven to be consistent with Bayes [Kontorovich and Wei\u00df, 2015], and we suspect that this also applies to the algorithm of Gottlieb et al. [2016b]. Since in our analysis MARMANN establishes a constant factor over this problem, we cannot derive the error technics from the present."}, {"heading": "Acknowledgements", "text": "Sivan Sabato was partially supported by the Israel Science Foundation (Scholarship No. 555 / 15), Aryeh Kontorovich partially supported by the Israel Science Foundation (Scholarships No. 1141 / 12 and 755 / 15) and a Yahoo Faculty Award. We thank Lee-Ad Gottlieb and Dana Ron for the helpful conversations."}], "references": [{"title": "Neural Network Learning: Theoretical Foundations", "author": ["M. Anthony", "P.L. Bartlett"], "venue": null, "citeRegEx": "Anthony and Bartlett.,? \\Q1999\\E", "shortCiteRegEx": "Anthony and Bartlett.", "year": 1999}, {"title": "Fast learning rates in statistical inference through aggregation", "author": ["J.-Y. Audibert"], "venue": "Ann. Statist., 37(4):1591\u20131646,", "citeRegEx": "Audibert.,? \\Q2009\\E", "shortCiteRegEx": "Audibert.", "year": 2009}, {"title": "The power of localization for efficiently learning linear separators with noise", "author": ["P. Awasthi", "M. Balcan", "P.M. Long"], "venue": "In Symposium on Theory of Computing,", "citeRegEx": "Awasthi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2014}, {"title": "Margin based active learning", "author": ["M. Balcan", "A.Z. Broder", "T. Zhang"], "venue": "In Proceedings of the 20th Annual Conference on Learning Theory, COLT", "citeRegEx": "Balcan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2007}, {"title": "The true sample complexity of active learning", "author": ["M. Balcan", "S. Hanneke", "J.W. Vaughan"], "venue": "Machine Learning,", "citeRegEx": "Balcan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2010}, {"title": "Agnostic active learning", "author": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Balcan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2009}, {"title": "A finite sample analysis of the naive bayes classifier", "author": ["D. Berend", "A. Kontorovich"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Berend and Kontorovich.,? \\Q2015\\E", "shortCiteRegEx": "Berend and Kontorovich.", "year": 2015}, {"title": "Active nearest neighbors in changing environments", "author": ["C. Berlind", "R. Urner"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Berlind and Urner.,? \\Q2015\\E", "shortCiteRegEx": "Berlind and Urner.", "year": 2015}, {"title": "In defense of nearest-neighbor based image classification", "author": ["O. Boiman", "E. Shechtman", "M. Irani"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Boiman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Boiman et al\\.", "year": 2008}, {"title": "Using the doubling dimension to analyze the generalization of learning algorithms", "author": ["N.H. Bshouty", "Y. Li", "P.M. Long"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Bshouty et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bshouty et al\\.", "year": 2009}, {"title": "Minimax bounds for active learning", "author": ["R.M. Castro", "R.D. Nowak"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Castro and Nowak.,? \\Q2008\\E", "shortCiteRegEx": "Castro and Nowak.", "year": 2008}, {"title": "Faster rates in regression via active learning", "author": ["R.M. Castro", "R. Willett", "R.D. Nowak"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Castro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Castro et al\\.", "year": 2005}, {"title": "Active learning on trees and graphs", "author": ["N. Cesa-Bianchi", "C. Gentile", "F. Vitale", "G. Zappella"], "venue": "In Proceedings of rhe 23rd Conference on Learning Theory, COLT", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2010}, {"title": "Rates of convergence for nearest neighbor classification", "author": ["K. Chaudhuri", "S. Dasgupta"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chaudhuri and Dasgupta.,? \\Q2014\\E", "shortCiteRegEx": "Chaudhuri and Dasgupta.", "year": 2014}, {"title": "Nearest neighbor pattern classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cover and Hart.,? \\Q1967\\E", "shortCiteRegEx": "Cover and Hart.", "year": 1967}, {"title": "S2: an efficient graph based active learning algorithm with application to nonparametric classification", "author": ["G. Dasarathy", "R.D. Nowak", "X. Zhu"], "venue": "In Proceedings of the 28th Annual Conference on Learning Theory,", "citeRegEx": "Dasarathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dasarathy et al\\.", "year": 2015}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dasgupta.,? \\Q2004\\E", "shortCiteRegEx": "Dasgupta.", "year": 2004}, {"title": "Consistency of nearest neighbor classification under selective sampling", "author": ["S. Dasgupta"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory, COLT", "citeRegEx": "Dasgupta.,? \\Q2012\\E", "shortCiteRegEx": "Dasgupta.", "year": 2012}, {"title": "Hierarchical sampling for active learning", "author": ["S. Dasgupta", "D. Hsu"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Dasgupta and Hsu.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta and Hsu.", "year": 2008}, {"title": "Nonparametric density estimation: the L1 view. Wiley Series in Probability and Mathematical Statistics: Tracts on Probability and Statistics", "author": ["L. Devroye", "L. Gy\u00f6rfi"], "venue": null, "citeRegEx": "Devroye and Gy\u00f6rfi.,? \\Q1985\\E", "shortCiteRegEx": "Devroye and Gy\u00f6rfi.", "year": 1985}, {"title": "A probabilistic theory of pattern recognition, volume 31 of Applications of Mathematics (New York)", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": null, "citeRegEx": "Devroye et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 1996}, {"title": "A (1 + \u01eb)-embedding of low highway dimension graphs into bounded treewidth graphs", "author": ["A.E. Feldmann", "W.S. Fung", "J. K\u00f6nemann", "I. Post"], "venue": "CoRR, abs/1502.04588,", "citeRegEx": "Feldmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feldmann et al\\.", "year": 2015}, {"title": "Report Number 4, Project Number 21-49-004", "author": ["E. Fix", "J.L.J. Hodges"], "venue": "USAF School of Aviation", "citeRegEx": "Fix and Hodges,? \\Q1951\\E", "shortCiteRegEx": "Fix and Hodges", "year": 1951}, {"title": "Discriminatory analysis. nonparametric discrimination: Consistency properties", "author": ["E. Fix", "J.L.J. Hodges"], "venue": "International Statistical Review / Revue Internationale de Statistique,", "citeRegEx": "Fix and Hodges,? \\Q1989\\E", "shortCiteRegEx": "Fix and Hodges", "year": 1989}, {"title": "Sample compression, learnability, and the vapnik-chervonenkis dimension", "author": ["S. Floyd", "M.K. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Floyd and Warmuth.,? \\Q1995\\E", "shortCiteRegEx": "Floyd and Warmuth.", "year": 1995}, {"title": "Efficient active learning of halfspaces: an aggressive approach", "author": ["A. Gonen", "S. Sabato", "S. Shalev-Shwartz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gonen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gonen et al\\.", "year": 2013}, {"title": "Efficient active learning of halfspaces: an aggressive approach", "author": ["A. Gonen", "S. Sabato", "S. Shalev-Shwartz"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Gonen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gonen et al\\.", "year": 2013}, {"title": "Efficient classification for metric data", "author": ["L. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Gottlieb et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2014}, {"title": "Near-optimal sample compression for nearest neighbors", "author": ["L. Gottlieb", "A. Kontorovich", "P. Nisnevitch"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gottlieb et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2014}, {"title": "Proximity algorithms for nearly-doubling spaces", "author": ["L.-A. Gottlieb", "R. Krauthgamer"], "venue": "In APPROX-RANDOM, pages 192\u2013204,", "citeRegEx": "Gottlieb and Krauthgamer.,? \\Q2010\\E", "shortCiteRegEx": "Gottlieb and Krauthgamer.", "year": 2010}, {"title": "Proximity algorithms for nearly doubling spaces", "author": ["L.-A. Gottlieb", "R. Krauthgamer"], "venue": "SIAM J. Discrete Math.,", "citeRegEx": "Gottlieb and Krauthgamer.,? \\Q2013\\E", "shortCiteRegEx": "Gottlieb and Krauthgamer.", "year": 2013}, {"title": "Efficient classification for metric data", "author": ["L.-A. Gottlieb", "L. Kontorovich", "R. Krauthgamer"], "venue": "In Proceedings of the 23rd Annual Conference on Learning Theory, COLT", "citeRegEx": "Gottlieb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2010}, {"title": "Adaptive metric dimensionality reduction", "author": ["L.-A. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "In Proceedings of the 24th International Conference on Algorithmic Learning Theory, ALT", "citeRegEx": "Gottlieb et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2013}, {"title": "Adaptive metric dimensionality reduction", "author": ["L.-A. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "Theoretical Computer Science,", "citeRegEx": "Gottlieb et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2016}, {"title": "Nearly optimal classification for semimetrics", "author": ["L.-A. Gottlieb", "A. Kontorovich", "P. Nisnevitch"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Gottlieb et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2016}, {"title": "Pac-bayesian compression bounds on the prediction error of learning algorithms for classification", "author": ["T. Graepel", "R. Herbrich", "J. Shawe-Taylor"], "venue": "Machine Learning,", "citeRegEx": "Graepel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graepel et al\\.", "year": 2005}, {"title": "Rates of convergence in active learning", "author": ["S. Hanneke"], "venue": "The Annals of Statistics,", "citeRegEx": "Hanneke.,? \\Q2011\\E", "shortCiteRegEx": "Hanneke.", "year": 2011}, {"title": "Minimax analysis of active learning", "author": ["S. Hanneke", "L. Yang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hanneke and Yang.,? \\Q2015\\E", "shortCiteRegEx": "Hanneke and Yang.", "year": 2015}, {"title": "Exact lower bounds for the agnostic probably-approximatelycorrect (PAC) machine learning model", "author": ["A. Kontorovich", "I. Pinelis"], "venue": null, "citeRegEx": "Kontorovich and Pinelis.,? \\Q2016\\E", "shortCiteRegEx": "Kontorovich and Pinelis.", "year": 2016}, {"title": "Maximum margin multiclass nearest neighbors", "author": ["A. Kontorovich", "R. Weiss"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Kontorovich and Weiss.,? \\Q2014\\E", "shortCiteRegEx": "Kontorovich and Weiss.", "year": 2014}, {"title": "A bayes consistent 1-nn classifier", "author": ["A. Kontorovich", "R. Weiss"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Kontorovich and Weiss.,? \\Q2015\\E", "shortCiteRegEx": "Kontorovich and Weiss.", "year": 2015}, {"title": "k-nn regression adapts to local intrinsic dimension", "author": ["S. Kpotufe"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kpotufe.,? \\Q2011\\E", "shortCiteRegEx": "Kpotufe.", "year": 2011}, {"title": "Hierarchical label queries with data-dependent partitions", "author": ["S. Kpotufe", "R. Urner", "S. Ben-David"], "venue": "In Proceedings of the 28th Annual Conference on Learning Theory, COLT", "citeRegEx": "Kpotufe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kpotufe et al\\.", "year": 2015}, {"title": "Navigating nets: Simple algorithms for proximity search", "author": ["R. Krauthgamer", "J.R. Lee"], "venue": "In 15th Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Krauthgamer and Lee.,? \\Q2004\\E", "shortCiteRegEx": "Krauthgamer and Lee.", "year": 2004}, {"title": "Rates of convergence of nearest neighbor estimation under arbitrary sampling", "author": ["S.R. Kulkarni", "S.E. Posner"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Kulkarni and Posner.,? \\Q1995\\E", "shortCiteRegEx": "Kulkarni and Posner.", "year": 1995}, {"title": "Relating data compression and learnability, unpublished", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": null, "citeRegEx": "Littlestone and Warmuth.,? \\Q1986\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1986}, {"title": "Empirical Bernstein bounds and sample-variance penalization", "author": ["A. Maurer", "M. Pontil"], "venue": "In Proceedings of the 22nd Annual Conference on Learning", "citeRegEx": "Maurer and Pontil.,? \\Q2009\\E", "shortCiteRegEx": "Maurer and Pontil.", "year": 2009}, {"title": "Employing EM and pool-based active learning for text classification", "author": ["A. McCallum", "K. Nigam"], "venue": "In Proceedings of the 15th International Conference on Machine Learning,", "citeRegEx": "McCallum and Nigam.,? \\Q1998\\E", "shortCiteRegEx": "McCallum and Nigam.", "year": 1998}, {"title": "On the method of bounded differences", "author": ["C. McDiarmid"], "venue": null, "citeRegEx": "McDiarmid.,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid.", "year": 1989}, {"title": "Active regression by stratification", "author": ["S. Sabato", "R. Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sabato and Munos.,? \\Q2014\\E", "shortCiteRegEx": "Sabato and Munos.", "year": 2014}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["S. Shalev-Shwartz", "S. Ben-David"], "venue": null, "citeRegEx": "Shalev.Shwartz and Ben.David.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Ben.David.", "year": 2014}, {"title": "Structural risk minimization over data-dependent hierarchies", "author": ["J. Shawe-Taylor", "P.L. Bartlett", "R.C. Williamson", "M. Anthony"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Shawe.Taylor et al\\.,? \\Q1926\\E", "shortCiteRegEx": "Shawe.Taylor et al\\.", "year": 1926}, {"title": "Consistent nonparametric regression", "author": ["C.J. Stone"], "venue": "The Annals of Statistics,", "citeRegEx": "Stone.,? \\Q1977\\E", "shortCiteRegEx": "Stone.", "year": 1977}, {"title": "PLAL: cluster-based active learning", "author": ["R. Urner", "S. Wulff", "S. Ben-David"], "venue": "In Proceedings of the 26th Annual Conference on Learning Theory, COLT", "citeRegEx": "Urner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Urner et al\\.", "year": 2013}, {"title": "Distance-based classification with Lipschitz functions", "author": ["U. von Luxburg", "O. Bousquet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Luxburg and Bousquet.,? \\Q2004\\E", "shortCiteRegEx": "Luxburg and Bousquet.", "year": 2004}, {"title": "Submodularity in data subset selection and active learning", "author": ["K. Wei", "R.K. Iyer", "J.A. Bilmes"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Wei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2015}, {"title": "Exponential bounds of mean error for the nearest neighbor estimates of regression functions", "author": ["L.C. Zhao"], "venue": "J. Multivariate Anal.,", "citeRegEx": "Zhao.,? \\Q1987\\E", "shortCiteRegEx": "Zhao.", "year": 1987}, {"title": "Combining active learning and semi-supervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "J. Lafferty", "Z. Ghahramani"], "venue": "In ICML 2003 workshop,", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 47, "context": "In pool-based active learning [McCallum and Nigam, 1998], a collection of random examples is provided, and the algorithm can interactively query an oracle to label some of the examples.", "startOffset": 30, "endOffset": 56}, {"referenceID": 35, "context": ", 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005].", "startOffset": 79, "endOffset": 101}, {"referenceID": 40, "context": "[2014a], has been shown to be Bayes consistent [Kontorovich and Weiss, 2015].", "startOffset": 47, "endOffset": 76}, {"referenceID": 27, "context": "Previous passive learning approaches to classification using nearestneighbor rules under noisy-margin assumptions [Gottlieb et al., 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005]. Their finite-sample guarantees depend on the number of noisy labels relative to an optimal margin scale. A central challenge in the active setting is performing model selection to select a margin scale with a low label complexity. A key insight that we exploit in this work is that by designing a new labeling scheme for the compression set, we can construct the compression set and estimate its error with label-efficient procedures. We obtain statistical guarantees for this approach using generalization bounds for sample compression with side information. We derive a label-efficient, as well as computationally efficient, active model-selection procedure. This procedure finds a good scale by estimating the sample error for some scales, using a small number of active querying rounds. Crucially, unlike cross-validation, our model-selection procedure does not require a number of labels that depends on the worst possible scale, nor does it test many scales. This allows our label complexity bounds to be low, and to depend only on the final scale selected by the algorithm. Our error guarantee is a constant factor over the error guarantee of the passive learner of Gottlieb et al. [2016b]. An approach similar to Gottlieb et al.", "startOffset": 115, "endOffset": 1430}, {"referenceID": 27, "context": "Previous passive learning approaches to classification using nearestneighbor rules under noisy-margin assumptions [Gottlieb et al., 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005]. Their finite-sample guarantees depend on the number of noisy labels relative to an optimal margin scale. A central challenge in the active setting is performing model selection to select a margin scale with a low label complexity. A key insight that we exploit in this work is that by designing a new labeling scheme for the compression set, we can construct the compression set and estimate its error with label-efficient procedures. We obtain statistical guarantees for this approach using generalization bounds for sample compression with side information. We derive a label-efficient, as well as computationally efficient, active model-selection procedure. This procedure finds a good scale by estimating the sample error for some scales, using a small number of active querying rounds. Crucially, unlike cross-validation, our model-selection procedure does not require a number of labels that depends on the worst possible scale, nor does it test many scales. This allows our label complexity bounds to be low, and to depend only on the final scale selected by the algorithm. Our error guarantee is a constant factor over the error guarantee of the passive learner of Gottlieb et al. [2016b]. An approach similar to Gottlieb et al. [2016b], proposed in Gottlieb et al.", "startOffset": 115, "endOffset": 1478}, {"referenceID": 27, "context": "Previous passive learning approaches to classification using nearestneighbor rules under noisy-margin assumptions [Gottlieb et al., 2014b, 2016b] provide statistical guarantees using sample compression bounds [Graepel et al., 2005]. Their finite-sample guarantees depend on the number of noisy labels relative to an optimal margin scale. A central challenge in the active setting is performing model selection to select a margin scale with a low label complexity. A key insight that we exploit in this work is that by designing a new labeling scheme for the compression set, we can construct the compression set and estimate its error with label-efficient procedures. We obtain statistical guarantees for this approach using generalization bounds for sample compression with side information. We derive a label-efficient, as well as computationally efficient, active model-selection procedure. This procedure finds a good scale by estimating the sample error for some scales, using a small number of active querying rounds. Crucially, unlike cross-validation, our model-selection procedure does not require a number of labels that depends on the worst possible scale, nor does it test many scales. This allows our label complexity bounds to be low, and to depend only on the final scale selected by the algorithm. Our error guarantee is a constant factor over the error guarantee of the passive learner of Gottlieb et al. [2016b]. An approach similar to Gottlieb et al. [2016b], proposed in Gottlieb et al. [2014a], has been shown to be Bayes consistent [Kontorovich and Weiss, 2015].", "startOffset": 115, "endOffset": 1515}, {"referenceID": 49, "context": "Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014].", "startOffset": 92, "endOffset": 116}, {"referenceID": 4, "context": "An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010].", "startOffset": 87, "endOffset": 108}, {"referenceID": 18, "context": "The paradigm of cluster-based active learning [Dasgupta and Hsu, 2008] has been shown to provide label savings under some distributional clusterability assumptions [Urner et al.", "startOffset": 46, "endOffset": 70}, {"referenceID": 2, "context": ", 2009, Hanneke, 2011, Awasthi et al., 2014]. Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014]. An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010]. The potential benefits of active learning for non-parametric settings are less well understood. Practical Bayesian graph-based active learning methods [Zhu et al., 2003, Wei et al., 2015] rely on generative model assumptions, and therefore come without distribution-free performance guarantees. From a theoretical perspective, the label complexity of graph based active learning has mostly been analyzed in terms of combinatorial graph parameters [Cesa-Bianchi et al., 2010, Dasarathy et al., 2015], which also do not yield statistical performance guarantees. Castro et al. [2005], Castro and Nowak [2008] analyze minimax rates for non-parametric regression and classification respectively, for a class of distributions in Euclidean space, characterized by decision boundary regularity and noise conditions with uniform marginals.", "startOffset": 23, "endOffset": 855}, {"referenceID": 2, "context": ", 2009, Hanneke, 2011, Awasthi et al., 2014]. Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014]. An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010]. The potential benefits of active learning for non-parametric settings are less well understood. Practical Bayesian graph-based active learning methods [Zhu et al., 2003, Wei et al., 2015] rely on generative model assumptions, and therefore come without distribution-free performance guarantees. From a theoretical perspective, the label complexity of graph based active learning has mostly been analyzed in terms of combinatorial graph parameters [Cesa-Bianchi et al., 2010, Dasarathy et al., 2015], which also do not yield statistical performance guarantees. Castro et al. [2005], Castro and Nowak [2008] analyze minimax rates for non-parametric regression and classification respectively, for a class of distributions in Euclidean space, characterized by decision boundary regularity and noise conditions with uniform marginals.", "startOffset": 23, "endOffset": 880}, {"referenceID": 2, "context": ", 2009, Hanneke, 2011, Awasthi et al., 2014]. Recently, it has been shown that active queries can also be beneficial for regression tasks [Sabato and Munos, 2014]. An active model selection procedure has also been developed for the parametric setting [Balcan et al., 2010]. The potential benefits of active learning for non-parametric settings are less well understood. Practical Bayesian graph-based active learning methods [Zhu et al., 2003, Wei et al., 2015] rely on generative model assumptions, and therefore come without distribution-free performance guarantees. From a theoretical perspective, the label complexity of graph based active learning has mostly been analyzed in terms of combinatorial graph parameters [Cesa-Bianchi et al., 2010, Dasarathy et al., 2015], which also do not yield statistical performance guarantees. Castro et al. [2005], Castro and Nowak [2008] analyze minimax rates for non-parametric regression and classification respectively, for a class of distributions in Euclidean space, characterized by decision boundary regularity and noise conditions with uniform marginals. The paradigm of cluster-based active learning [Dasgupta and Hsu, 2008] has been shown to provide label savings under some distributional clusterability assumptions [Urner et al., 2013, Kpotufe et al., 2015]. Dasgupta and Hsu [2008] showed that a suitable cluster-tree can yield label savings in this framework, and papers following up quantified the label savings under distributional clusterability assumptions.", "startOffset": 23, "endOffset": 1337}, {"referenceID": 17, "context": "to a consistent algorithm [Dasgupta, 2012].", "startOffset": 26, "endOffset": 42}, {"referenceID": 7, "context": "A selective querying strategy has been shown to be beneficial for nearest neighbors under covariate shift [Berlind and Urner, 2015], where one needs to adapt to a change in the data generating process.", "startOffset": 106, "endOffset": 131}, {"referenceID": 27, "context": "2, and present the guarantees of the compression-based passive learner of Gottlieb et al. [2016b] in Section 2.", "startOffset": 74, "endOffset": 98}, {"referenceID": 29, "context": "Constructing a minimum size t-net for a general set B is NP-hard [Gottlieb and Krauthgamer, 2010].", "startOffset": 65, "endOffset": 97}, {"referenceID": 43, "context": "The size of any t-net of a metric space A \u2286 X is at most \u2308diam(A)/t\u2309ddim(X )+1 [Krauthgamer and Lee, 2004].", "startOffset": 79, "endOffset": 106}, {"referenceID": 9, "context": "Generalization bounds in terms of the doubling dimension of the hypothesis space were established in Bshouty et al. [2009], while runtime and generalization errors in terms of ddim(X ) were given in Gottlieb et al.", "startOffset": 101, "endOffset": 123}, {"referenceID": 9, "context": "Generalization bounds in terms of the doubling dimension of the hypothesis space were established in Bshouty et al. [2009], while runtime and generalization errors in terms of ddim(X ) were given in Gottlieb et al. [2014a]. Constructing a minimum size t-net for a general set B is NP-hard [Gottlieb and Krauthgamer, 2010].", "startOffset": 101, "endOffset": 223}, {"referenceID": 26, "context": "As shown in Gottlieb and Krauthgamer [2013], the doubling dimension is \u201calmost hereditary\u201d in the sense that forA \u2282 X , we have ddim(A) \u2264 cddim(X ) for some universal constant c \u2264 2 [Feldmann et al.", "startOffset": 12, "endOffset": 44}, {"referenceID": 21, "context": "As shown in Gottlieb and Krauthgamer [2013], the doubling dimension is \u201calmost hereditary\u201d in the sense that forA \u2282 X , we have ddim(A) \u2264 cddim(X ) for some universal constant c \u2264 2 [Feldmann et al., 2015, Lemma 6.6]. For simplicity, the bounds above are presented in terms of ddim(X ), the doubling dimension of the ambient space. It should be noted that one can obtain tighter bounds in terms of ddim(U(S)) when the latter is substantially lower than that of the ambient space, and it is also possible to perform metric dimensionality reduction, as in Gottlieb et al. [2013].", "startOffset": 183, "endOffset": 577}, {"referenceID": 27, "context": "3 Passive compression-based nearest-neighbors Non-parameteric binary classification admits performance guarantees that scale with the sample\u2019s noisy-margin [von Luxburg and Bousquet, 2004, Gottlieb et al., 2010, 2016b]. The original margin-based methods of von Luxburg and Bousquet [2004] and Gottlieb et al.", "startOffset": 189, "endOffset": 289}, {"referenceID": 27, "context": "3 Passive compression-based nearest-neighbors Non-parameteric binary classification admits performance guarantees that scale with the sample\u2019s noisy-margin [von Luxburg and Bousquet, 2004, Gottlieb et al., 2010, 2016b]. The original margin-based methods of von Luxburg and Bousquet [2004] and Gottlieb et al. [2010] analyzed the generalization performance via the technique of Lipschitz extension.", "startOffset": 189, "endOffset": 316}, {"referenceID": 27, "context": "3 Passive compression-based nearest-neighbors Non-parameteric binary classification admits performance guarantees that scale with the sample\u2019s noisy-margin [von Luxburg and Bousquet, 2004, Gottlieb et al., 2010, 2016b]. The original margin-based methods of von Luxburg and Bousquet [2004] and Gottlieb et al. [2010] analyzed the generalization performance via the technique of Lipschitz extension. Later, it was noticed in Gottlieb et al. [2014b] that the presence of a margin allows for compression \u2014 in fact, nearly optimally so.", "startOffset": 189, "endOffset": 447}, {"referenceID": 27, "context": "Gottlieb et al. [2016b] propose a passive learner with the following guarantees1 as a function of the separation of S.", "startOffset": 0, "endOffset": 24}, {"referenceID": 27, "context": "3 (Gottlieb et al. [2016b]).", "startOffset": 3, "endOffset": 27}, {"referenceID": 27, "context": "3 (Gottlieb et al. [2016b]). Let m be an integer, \u03b4 \u2208 (0, 1). There exists a passive learning algorithm that returns a nearest-neighbor classifier h Spas , where Spas \u2286 Sin, such that, with probability 1\u2212 \u03b4, err(h Spas ,D) \u2264 Gmin(m, \u03b4). The passive algorithm of Gottlieb et al. [2016b] generates Spas of size approximately N (t) for the optimal scale t > 0 (found by searching over all scales), by removing the |Sin|\u03bd(t) points that obstruct the t-separation between different labels in Sin, and then selecting a subset of the remaining labeled examples to form Spas, so that the examples are a t-net for Sin (not including the obstructing points).", "startOffset": 3, "endOffset": 286}, {"referenceID": 27, "context": "3 (Gottlieb et al. [2016b]). Let m be an integer, \u03b4 \u2208 (0, 1). There exists a passive learning algorithm that returns a nearest-neighbor classifier h Spas , where Spas \u2286 Sin, such that, with probability 1\u2212 \u03b4, err(h Spas ,D) \u2264 Gmin(m, \u03b4). The passive algorithm of Gottlieb et al. [2016b] generates Spas of size approximately N (t) for the optimal scale t > 0 (found by searching over all scales), by removing the |Sin|\u03bd(t) points that obstruct the t-separation between different labels in Sin, and then selecting a subset of the remaining labeled examples to form Spas, so that the examples are a t-net for Sin (not including the obstructing points). For the binary classification case (|Y| = 2) an efficient algorithm is shown in Gottlieb et al. [2016b]. However, in the general multiclass case, it is not known how to find a minimal t-separation efficiently \u2014 a naive approach requires solving the NP-hard problem of vertex cover.", "startOffset": 3, "endOffset": 753}, {"referenceID": 27, "context": "A main challenge for active learning in our non-parametric setting is performing model selection, that is, selecting a good scale t similarly to the passive learner of Gottlieb et al. [2016b]. In the passive supervised setting, the approach developed in several previous works [Gottlieb et al.", "startOffset": 168, "endOffset": 192}, {"referenceID": 36, "context": "2In the case of binary labels (|Y| = 2), the problem of estimating Sa(t) can be formulated as a special case of the benign noise setting for parametric active learning, for which tight lower and upper bounds are provided in Hanneke and Yang [2015]. However, our case is both more general (as we allow multiclass labels) and more specific (as we are dealing with a specific \u201chypothesis class\u201d).", "startOffset": 224, "endOffset": 248}, {"referenceID": 46, "context": "3This follows from Theorem 4 of Maurer and Pontil [2009] since 7 3(n\u22121) \u2264 8 3n for n \u2265 8.", "startOffset": 32, "endOffset": 57}, {"referenceID": 43, "context": "1 Sample compression with side information It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al.", "startOffset": 132, "endOffset": 163}, {"referenceID": 20, "context": "1 Sample compression with side information It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al. [1996]; some background is given in Floyd and Warmuth [1995].", "startOffset": 167, "endOffset": 189}, {"referenceID": 20, "context": "1 Sample compression with side information It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al. [1996]; some background is given in Floyd and Warmuth [1995]. As noted in Section 4, our algorithm relies on a generalized sample compression scheme, which requires side information.", "startOffset": 167, "endOffset": 243}, {"referenceID": 20, "context": "1 Sample compression with side information It appears that compression-based generalization bounds were independently discovered by Littlestone and Warmuth [1986] and Devroye et al. [1996]; some background is given in Floyd and Warmuth [1995]. As noted in Section 4, our algorithm relies on a generalized sample compression scheme, which requires side information. This side information is used to represent the labels of the sample points in the compression set. A similar idea appears in Floyd and Warmuth [1995] for hypotheses with short description length.", "startOffset": 167, "endOffset": 515}, {"referenceID": 27, "context": "If the compression set includes only the original labels, the compression analysis of Gottlieb et al. [2016b] gives the bound GB(\u01eb,N, \u03b4,m, 1).", "startOffset": 86, "endOffset": 110}, {"referenceID": 6, "context": "This expression previously appeared in Berend and Kontorovich [2015, Equation (25)] in the context of information-theoretic lower bounds; the current terminology was motivated in Kontorovich and Pinelis [2016], where various precise estimates on bayes(\u00b7) were provided.", "startOffset": 39, "endOffset": 210}, {"referenceID": 0, "context": ", Anthony and Bartlett [1999] p.", "startOffset": 2, "endOffset": 30}, {"referenceID": 50, "context": "4) in Shalev-Shwartz and Ben-David [2014]), for some j,X,B it holds that E[err(\u0125j ,Dj)] \u2265 1 T T", "startOffset": 6, "endOffset": 42}, {"referenceID": 48, "context": "Hence, by McDiarmid\u2019s inequality [McDiarmid, 1989], with probability at least 1 \u2212 1 28 , |\u03bd(2 ) \u2212 E[\u03bd(12 )]| \u2264 \u221a", "startOffset": 33, "endOffset": 50}, {"referenceID": 31, "context": "A variant of this approach [Gottlieb et al., 2010] was shown to be Bayes-consistent [Kontorovich and Weiss, 2015], and we conjecture that this holds also for", "startOffset": 27, "endOffset": 50}, {"referenceID": 40, "context": ", 2010] was shown to be Bayes-consistent [Kontorovich and Weiss, 2015], and we conjecture that this holds also for", "startOffset": 41, "endOffset": 70}, {"referenceID": 27, "context": "Our error guarantees give a constant factor over the error guarantees of Gottlieb et al. [2016b]. A variant of this approach [Gottlieb et al.", "startOffset": 73, "endOffset": 97}, {"referenceID": 27, "context": "the algorithm of Gottlieb et al. [2016b]. Since in our analysis MARMANN obtains a constant factor over the error of the passive learner, Bayes-consistency cannot be inferred from our present techniques; we leave this problem open for future research.", "startOffset": 17, "endOffset": 41}], "year": 2016, "abstractText": "We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. MARMANN is based on a generalized sample compression scheme, and a new label-efficient active model-selection procedure.", "creator": "LaTeX with hyperref package"}}}