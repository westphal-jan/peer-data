{"id": "1605.06955", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data", "abstract": "Semi-supervised learning based on the low-density separation principle such as the cluster and manifold assumptions has been extensively studied in the last decades. However, such semi-supervised learning methods do not always perform well due to violation of the cluster and manifold assumptions. In this paper, we propose a novel approach to semi-supervised learning that does not require such restrictive assumptions. Our key idea is to combine learning from positive and negative data (standard supervised learning) and learning from positive and unlabeled data (PU learning), the latter is guaranteed to be able to utilize unlabeled data without the cluster and manifold assumptions. We theoretically and experimentally show the usefulness of our approach.", "histories": [["v1", "Mon, 23 May 2016 09:37:48 GMT  (153kb,D)", "http://arxiv.org/abs/1605.06955v1", null], ["v2", "Fri, 14 Oct 2016 14:04:24 GMT  (261kb,D)", "http://arxiv.org/abs/1605.06955v2", null], ["v3", "Wed, 1 Mar 2017 11:39:31 GMT  (406kb,D)", "http://arxiv.org/abs/1605.06955v3", null], ["v4", "Fri, 16 Jun 2017 11:14:36 GMT  (1750kb,D)", "http://arxiv.org/abs/1605.06955v4", "Accepted to the 34th International Conference on Machine Learning (ICML 2017)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tomoya sakai", "marthinus christoffel du plessis", "gang niu", "masashi sugiyama"], "accepted": true, "id": "1605.06955"}, "pdf": {"name": "1605.06955.pdf", "metadata": {"source": "CRF", "title": "Beyond the Low-density Separation Principle: A Novel Approach to Semi-supervised Learning", "authors": ["Tomoya Sakai", "Masashi Sugiyama"], "emails": ["sakai@ms.k.u-tokyo.ac.jp", "christo@ms.k.u-tokyo.ac.jp", "gang@ms.k.u-tokyo.ac.jp", "sugi@k.u-tokyo.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "In recent decades, there has been a glaring problem."}, {"heading": "2 Background", "text": "In this section we will first formulate our goal classification problem and then briefly discuss PN learning and PU (NU) learning methods that will be used in the following sections to develop a novel semi-supervised learning method."}, {"heading": "2.1 Problem Settings", "text": "Suppose we do not get data from p (x, y), but instead observe the following three data sets: X + = {x + i} n + i = 1 economic p + (x), X \u2212 = {x \u2212 i} n + (x), X \u2212 = {x \u2212 i} n \u2212 i = 1 economic p \u2212 (x), andXu = {xui} nu i = 1 economic pu (x). Each of the three data sets contains independent and identically distributed data from the corresponding limit density. We call X +, X \u2212 and Xu positive (P), negative (N) and unlabeled (U) pu (x). Each of the three data sets contains independent and identically distributed data from the corresponding limit density. We call X +, X \u2212 and Xu positive (P) and a learning problem using P and N data PU or learning and learning using PU."}, {"heading": "2.2 PN Learning", "text": "Let g: Rd 7 \u2192 R be any real decision function for binary classification and \": R 7 \u2192 R be a limited Lipschitz continuous loss function. Letter R + (g) = E + ['(g (X)], R \u2212 (g) = E \u2212 [' (\u2212 g (X))], where E \u00b1 [\u00b7] = EX \u0445 p \u00b1 [\u00b7]. Then the risk for g w.r.t. is given under p (x, y) by R (g) = E (X, Y) ['(Y g (X))] = \u03c0R + (g) + (1 \u2212 \u03c0) R \u2212 (g). (1). If we approximate R (g) on the basis of Equation (1), we get an unbiased estimate of the risk in PN learning: R \u0445pn (g) = \u03c0n + n + \u2211 i = 1' (g + i) + 1 \u2212 n \u2212 b \u2212 i = 1 \u00b7 g (whose probability is 1 \u2212 p)."}, {"heading": "2.3 PU Learning", "text": "Let us take Ru, \u2212 (g) = EX ['(\u2212 g (X))], where EX = EX \u0445 pu [\u00b7]. du Plessis et al. [7] has shown that we have the following symmetrical condition' (t) + '(\u2212 t) = 1, (2) Ru, \u2212 (g) = \u03c0 (1 \u2212 R + (g) + (1 \u2212 \u03c0) R \u2212 (g) and hence R (g) = 2\u03c0R + (g) + Ru, \u2212 (g) \u2212 \u03c0. (3) If we approximate R (g) on the basis of Equation (3), we obtain an unbiased estimate of the risk of PU learning: R-pu (g) = \u2212 \u03c0 + 2\u03c0n + n + \u2211 i = 1' (g (x + i) + 1 nu-i = 1 '(\u2212 g (xui)), whose convergence rate is Op (1 / 4 + 1 / 7)."}, {"heading": "2.4 NU Learning", "text": "Similarly, R (g) could be estimated using NU data: R (g) = Ru, + (g) + 2 (1 \u2212 \u03c0) R \u2212 (g) \u2212 (1 \u2212 \u03c0), (4) where Ru, + (g) = EX ['(g (X)]. Approaching R (g) on the basis of Equation (4), we obtain an unbiased estimate of the risk in NU learning: R-nu (g) = 1nu nu \u2211 i = 1' (g (xui))) + 2 (1 \u2212 \u03c0) n \u2212 \u2211 i = 1 '(\u2212 g (x \u2212 i) \u2212 (1 \u2212 \u03c0), whose convergence rate is Op (1 / \u221a n \u2212 + 1 / \u221a nu)."}, {"heading": "3 Learning from PNU Data", "text": "In this section we describe our semi-supervised learning methods, which combine PN learning and PU learning (or NU learning)."}, {"heading": "3.1 PNPU and PNNU Estimators", "text": "Let us leave Rpn (g) for R (g) for Eq. (1), Rpu (g) for R (g) for Eq. (3) and \u03b3 for a real scalar, so that 0 \u2264 \u03b3 \u2264 1. Then the convex combination of Rpn (g) and Rpu (g) R (g) for Ru, \u2212 (g) for (1 \u2212 \u03b3) Rpu (g) = (2\u03c0 \u2212 \u03b3) R + (g) for R \u2212 (g) for Rpn (g) + (1 \u2212 \u03b3) Ru, \u2212 (g) for Rpu (5), we get an unbiased estimate of the risk: R (g) for Pnpu (g) = Prognos (g) = Prognos (g), R (pn) for the (g) + Prognos (g), R (p) for the (g) + Prognos (n), R (g) for Rpu (g) = 1 \u2212 g (s)."}, {"heading": "3.2 Algorithm", "text": "Here is an algorithm to minimize empirical loss (6) for the PNPU learning procedure. PNNU learning can be done the same way. (For simplicity, we introduce a series of triple samples: \"The cost of each sample is, i.e., ci = c + for positive, ci = c \u2212 for negative, and ci = cu for unlabeled sample. We use the ramp loss that meets the symmetrical condition (2):\" R \"max (0, min (2, 1 \u2212 z). Since the ramp loss can be divided into convex and convex parts, we use the ramp loss that meets the symmetrical condition (2)."}, {"heading": "4 Theoretical Analyses", "text": "In this section, we will theoretically analyze the behavior of the proposed method."}, {"heading": "4.1 Variance Analysis", "text": "All estimators of R (g) are so far unbiased. A natural question would be raised: Could R (g) and R (g) have smaller deviations than R (g)? To answer this question, let g be corrected and assume that n + nu, n \u2212 nu, what should be natural in PU and NU learning processes. Then, we have the following deviations (their evidence is given in Appendix A and Appendix B): Lemma 1. For all fixed g, if1n + assumed deviations 2 + (g) < 1n \u2212 (1 \u2212) assumed deviations 2 \u2212 (8) is true, the combination R (g) assumed deviations pnpu (g) assumed deviations pnpu = 2 (1 \u2212 n) assumed deviations (1 \u2212 n assumed deviations from the learning process."}, {"heading": "4.2 Generalization Error Bounds", "text": "Next, we analyze the generalization error of PNPU learning and PNNU learning. Let G be a function class of hyperplains with bounded norms and character cards: G = {g (x) = < w, \u03c6 (x) > | 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0 = 0 = 0 ="}, {"heading": "5 Experiments", "text": "In this section we evaluate experimentally the performance of the proposed method."}, {"heading": "5.1 Numerical illustration", "text": "The PNU data is generated from the following distributions: p + (x) = N (x; (1, 0) >, I2), p \u2212 (x) = N (x; (\u2212 1, 0) >, I2), and I2 is the identity matrix. We generate 6 positive, 2 negative samples and 30 blank samples using class priorities of \u03c0 = 0.2, 0.5. Class boundaries before this and subsequent experiments are considered known, but in practice they can be accurately estimated with the methods proposed in [6, 16, 17]. A linear model g (x) = w > x + b and a fixed regulation parameter of \u03bb = 0.001 is used for learning."}, {"heading": "5.2 Benchmark data sets", "text": "We compare our method against two semi-monitored (i.e. PNU) learning methods, Laplacian SVM (LapSVM) [3] and logistic regression with entropy regularization (ER) [18], on different benchmarks. 1 In the experiments, we fix n + and n \u2212 and vary \u03c0. Since the class may differ from the target class before the marked data, we are reweighted SVM for LapSVM and reweighted logistic regression for ER.However, to ensure a fair comparison, all methods are used which include a Gaussian kernel model (x) = n designated data iK (x, xi), where K (x \u2032) = exp \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 2).The bandwidth of the kernel was selected from {10 \u2212 3, 10 \u2212 2, 101}. The hyper parameters in all methods were selected via 5-fold cross-validation."}, {"heading": "6 Conclusions", "text": "In this paper, we proposed a novel approach to semi-supervised learning, which is not based on the principle of low-density separation. Our basic idea was the combination of PN learning and PU learning or NU learning. Theoretically, we demonstrated that PNPU learning and PNNU learning almost always improve variance compared to PN learning without the low-density separation principle. Furthermore, we demonstrated that PNPU learning and PNNU learning achieve the optimal parametric convergence rate with respect to the generalization error. Numerical experiments demonstrated the effectiveness of the proposed methods."}, {"heading": "A Proof of Lemma 1", "text": "Proven. Due to the independence of X +, X \u2212 and Xu and the assumptions n + nu and n \u2212 nu, we have Var [R \u03b3pnpu (g)] = (2\u03c0 \u2212 \u03b3\u03c0) 2n + \u03c32 + (g) + \u03b32 (1 \u2212 \u03c0) 2n \u2212 \u03c32 \u2212 (g) (13) as nu \u2192, where \u03c32 + (g) = VarX \u0445 p + (x) ['(g (X))) [' (g) = VarX \u0445 p \u2212 (x)] ['(g) = (x) [' (\u2212 g (X))) ['(\u2212 g (X) sp \u2212 p \u2212 (x)) and (x) p \u2212 (x) [' (\u2212 g) sp \u2212 (\u2212 g) p \u2212 (x)) and (g) p \u2212 (g) p \u2212 (g) p \u2212 (\u00b2) and (\u00b2) p \u2212 (\u00b2) only (l \u00b2) and g (\u00b2) p (\u00b2)."}, {"heading": "B Proof of Lemma 2", "text": "Proof. With the assumptions n + nu and n \u2212 nu we have Var [R] \u03b3pnnu (g)] \u2248 \u03b32\u03c02n + \u03c32 + (g) + (2 \u2212 \u03b3) 2 (1 \u2212 \u03c0) 2n \u2212 \u03c32 \u2212 (g) (14) as nu \u2192 \u221e, and thereafter \u03b3 pnnu = argmin \u04212\u03c02n + \u03c32 + (g) + (2 \u2212 \u03b3) 2 (1 \u2212 \u03c0) 2n \u2212 \u03c32 + (g) = 2 (1 + \u03c02\u03c32 + (g) / n + (1 \u2212 \u03c0) 2\u03c32 \u2212 (g) / n \u2212 1. It is clear that 0 < \u03b3 \u2012 pnnu < 1 whether and only if1n + \u03c02\u03c32 + (g) > 1n \u2212 (1 \u2212 \u03c0) 2\u04452 \u2212 (g).Equation (14) is also square, and therefore we have Lemma 2."}, {"heading": "C Proof of Theorem 4", "text": "-, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, - (g), R \u2212 (g), R \u2212 u \u2212 (g) and Ru \u2212 (g) of the sizes n +, n \u2212, nu \u2212 and nu \u2212, respectively, empirical approximations of R + (g), R \u2212 (g) and Ru \u2212, (g) of the sizes n +, n \u2212, nu and nu \u2212. Then we have the following concentration results, which are used, um theorem 4: Lemma 5. For each category > 0, with a probability of at least 1, or 3, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -,"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Semi-supervised learning based on the low-density separation principle such as<lb>the cluster and manifold assumptions has been extensively studied in the last<lb>decades. However, such semi-supervised learning methods do not always per-<lb>form well due to violation of the cluster and manifold assumptions. In this paper,<lb>we propose a novel approach to semi-supervised learning that does not require<lb>such restrictive assumptions. Our key idea is to combine learning from positive<lb>and negative data (standard supervised learning) and learning from positive and<lb>unlabeled data (PU learning), the latter is guaranteed to be able to utilize unla-<lb>beled data without the cluster and manifold assumptions. We theoretically and<lb>experimentally show the usefulness of our approach.", "creator": "LaTeX with hyperref package"}}}