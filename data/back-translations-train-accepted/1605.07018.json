{"id": "1605.07018", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Online Learning with Feedback Graphs Without the Graphs", "abstract": "We study an online learning framework introduced by Mannor and Shamir (2011) in which the feedback is specified by a graph, in a setting where the graph may vary from round to round and is \\emph{never fully revealed} to the learner. We show a large gap between the adversarial and the stochastic cases. In the adversarial case, we prove that even for dense feedback graphs, the learner cannot improve upon a trivial regret bound obtained by ignoring any additional feedback besides her own loss. In contrast, in the stochastic case we give an algorithm that achieves $\\widetilde \\Theta(\\sqrt{\\alpha T})$ regret over $T$ rounds, provided that the independence numbers of the hidden feedback graphs are at most $\\alpha$. We also extend our results to a more general feedback model, in which the learner does not necessarily observe her own loss, and show that, even in simple cases, concealing the feedback graphs might render a learnable problem unlearnable.", "histories": [["v1", "Mon, 23 May 2016 14:07:43 GMT  (42kb)", "http://arxiv.org/abs/1605.07018v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alon cohen", "tamir hazan", "tomer koren"], "accepted": true, "id": "1605.07018"}, "pdf": {"name": "1605.07018.pdf", "metadata": {"source": "CRF", "title": "Online Learning with Feedback Graphs Without the Graphs", "authors": ["Alon Cohen", "Tamir Hazan", "Tomer Koren"], "emails": ["alon.cohen@technion.ac.il", "tamir.hazan@technion.ac.il", "tomerk@technion.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.07 018v 1 [cs.L G] 23 May 201 6? \u03b1T q Regret for T-rounds, provided that the dependence numbers of the hidden feedback diagrams are at most \u03b1. We also extend our results to a more general feedback model, where the learner does not necessarily observe his own loss, and show that even in simple cases, hiding the feedback diagrams could make a learnable problem unlearnable."}, {"heading": "1 Introduction", "text": "In its most basic form, it can be described as follows: A learner must select an action from a range of available actions and suffer a loss associated with that action. However, the losses of actions in each round are assigned in advance by an arbitrary, possibly contradictory, environment. The goal of learners is to minimize their regret over the T-rounds of the game, which is the difference between their cumulative loss and that of the best fixed action in Hindsight. After each decision is made, the learner receives some form of feedback about the losses. Traditionally, literature considers two types of feedback: full feedback and warmth, 1990; Cesa-Bianchi et al., 1997, where the learner observes the losses associated with all possible actions, and Feedback Auer al. (2002b), where the learner observes only the loss of the general framework of action."}, {"heading": "1.1 Our contributions", "text": "In this thesis, we study online learning with feedback charts in an environment where the feedback charts are never disclosed to the learner in their entirety. That is, in this setting, the only feedback available to the learner at the end of the round is still insufficient. Is it possible to achieve any non-trivial regret guarantee in this environment, together with the loss associated with each of the actions in this neighborhood and the loss of the action that it has chosen? KT has raised the following questions: how this lack of full disclosure affects the learners \"regret? Is it possible to achieve any non-trivial regret guarantees in this environment, i.e., one that improves the trivial op? KT q is limited to the following measures? In particular, can we come up with the independence numbers of the feedback charts, which do not know that the overall feedback charts can have a significant impact on the learners\" achievable regret."}, {"heading": "1.2 Additional related work", "text": "Online learning with feedback graphs was previously considered in the stochastic environment of Caron et al. (2012), which yielded results depending on the graph's clique structure, but their analysis is only valid if the feedback graph is fixed throughout the game and can bind regret only in terms of a quantity similar to the graph's clique partition number, which is always larger than its number of independence (the gap between the two can be very large; see Alon et al., 2014). More recently, Wu et al. (2015) and Koca \u0301 k et al. (2016) investigated a noisy version of the feedback graph model, in which feedback is specified by a weighted directed graph with edge weights that specify the quality (e.g. the noise level or variance) of the feedback received on adjacent vertices. Wu al. (2015) provided end-time algorithms specializing in this coefficient and generalized sub-time setting (the coefficient)."}, {"heading": "2 Setup and Main Results", "text": "We will consider a general online learning model with graphically structured feedback, which can be described as a game q q q between a learner and an environment progressing for T rounds. Before the game begins, the environment privately determines a sequence of loss functions. \u2022 In the adversarial setting, the environment becomes a sequence of directed graphs G1,..., Ku of K actions, which we consider a sequence of loss vectors. \u2022 The environment fixes a sequence of directed graphs G1,..., GT over V as vertics.We will consider two different cases, which we call the adversarial sequence and the stochastic sequence of losses. \u2022 In the adversarial sequence, the loss vectors 1,..., the T and the feedback graphs G1,..., GT are chosen by the environment in an arbitrary manner. \u2022 In the stochastic setting, the environment selects a loss."}, {"heading": "2.1 Main results", "text": "Our first result deals with the opposing case and shows that if the feedback diagrams are not disclosed to the learner at the end of each round, their regret may be very great, even if the independence numbers of the diagrams are small - they are all limited by a constant. Theorem 1: In the opposing environment, any online learning algorithm must suffer at least one damage. (e.g. Exp3, Auer et al., 2002b), even if all feedback diagrams have G1,.., GT independence numbers. The lower limit in the theorem is narrow: it can be compensated by simply executing a standard bandit algorithm (e.g. Exp3, Auer et al., 2002b), ignoring all observed feedback diagrams except the loss of field of action. Our next result shows that in the stochastic case the learner is still able to achieve non-trivial regret."}, {"heading": "2.2 Discussion of the results", "text": "Our results show that there is a large gap between the attainable regret rates in the opposing and stochastic settings in terms of dependence on the properties of the feedback graphs. In the opposing case, the environment is free to choose the sequences of loss values and feedback graphs in conjunction with each other simultaneously; they can, for example, be derived from a common distribution of loss values and sequences of directed diagrams; the environment can use this freedom to manipulate the feedback observed by the learner and maliciously distort their observations. In the stochastic environment, on the other hand, the loss values are only drawn from the underlying distribution after the environment has settled on an arbitrary sequence of diagrams, so that the feedback graphs are likely to be independent of the findings of the losses. In fact, as our arguments in Section 3 show, there is a randomized construction of loss vectors and feedback graphs that cause the graphs."}, {"heading": "3 Lower Bound for Adversarial Losses", "text": "In this section, we deal with the opposing setting and prove Theorem 1: We show a deviation q? KT q lower limit on the performance of any online learning algorithm, where both the losses of actions and the feedback graphs can be arbitrarily selected. Let's sketch the idea behind the lower limit and move the formal details to Section 6. According to Yao's Minimax principle, in order to prove a lower limit on the learner's regret, it is sufficient to point out a randomized strategy for the environment that forces every deterministic learner to regret? KT q Regret. We construct the strategy of our environment as following."}, {"heading": "4 Algorithms for Stochastic Losses", "text": "In this section, we introduce ourselves and analyze our algorithms for stochastic setting nr. \"The algorithms that we acquire are exhausted.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\" \"The algorithms that we apply.\""}, {"heading": "4.1 Gap-based analysis", "text": "\"We can leave the intuition behind us.\" Every call to most r.P. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"\" A. \"\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A. \"A.\" A \"A.\" A \"A.\" A \"A.\" A \"A.\" A \"A.\" A \"A.\" A \"A\" A. \""}, {"heading": "4.2 Efficient sampling scheme", "text": "In this section we discuss the AlphaSample randomized sampling procedure. This procedure allows us to collect a sample of the loss for each action while we only perform rOp\u03b1q rounds in anticipation. AlphaSample is in algorithm 2.Let us now explain the intuition behind the procedure. The actions, the algorithm 2 AlphaSample input set of actions initializes U \u00b2 V, while | U \u00b2 s observes an action u P U uniformly at random, and let W puq observe the series of actions, the losses of each P W puq in S update UzW puqend, while the return shave is observed until the process then continues recursively until U is empty."}, {"heading": "5 Beyond Bandit Feedback", "text": "In this section, we expand our results to a more general class of feedback graphs. In particular, we no longer assume that the learner can automatically observe the loss of the action he chooses. Instead, we allow the graph self-loops, namely edges of the form v \u00d1 v. The absence of self-loops in individual actions allows feedback models that are not necessarily more informative than the bandit model. Recently, Alon et al. (2015) have studied this more general feedback model and divided feedback graphs into three categories: unobservable graphs for which the induced problem is not learnable; poorly observable graphs for which r\u0439pT 2 {3q repentance is attainable; and highly observable graphs for which it is possible to achieve r\u00eap? T? T? T. Their results assume that the feedback graphs are available to the learner, at least after any prediction. Here, we assume that the graphs are proactive."}, {"heading": "5.1 Strongly observable graphs", "text": "In the opposing environment, we show that the problem could be unlearnable even with highly observable graphs; formally, we prove (see Section 6.6): Theorem 15. Therefore, in the opposing environment, every algorithm must suffer at least T {16 remorse at worst, even if G1,.., GT are all highly observable. Consider a problem about two actions, u and v. The environment chooses one of two distributions about the choice of loss of action v, edge u, and self-loop v, which are summarized in Figure 2. Each cell in the table is divided into two, where the left half is for the first distribution, and the right half is for the second distribution. The two most right columns indicate the marginal distributions between the loss of action v and either the edgeu or the self-loop v."}, {"heading": "5.2 Observable graphs", "text": "For the opposing environment, the problem is unlearnable, since it is already unlearnable for highly observable graphs according to Theorem 15. On the other hand, we have the following result in the stochastic environment.Theorem 17. In the stochastic environment, there exists an online learning algorithm that achieves rOpK1 {3T 2 {3q repentance, provided that G1,..., GT are all observable. This regret is closely tied to logarithmic factors for weakly observable G1,..., GT, since the r\u0439pK1 {3T 2 {3q lower limit proved by Alon et al. (2015), in the simpler environment in which the graphs are revealed after each decision, is applicable in our stochastic environment.Evidence for sketch. The algorithm is carried out in two phases. In the exploration phase, the learner estimates the means of the loss of all actions to accuracy. The learner simply performs random actions according to a problem similar to the discovery phase."}, {"heading": "6 Additional proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Proof of Lemma 8", "text": "Proof. AlphaSample observed the loss of each action in Vr for nr times. Note that the losses of actions are distributed independently of the feedback diagrams. Therefore, by Hoeffdings inequality and the composite bond we have at least 1 \"T\" 1, @ v P Vr, | \u00b5pvq \"mrpvq\" mrpvq. \"Identify by v\" an action so that mrpv \"m.\" Note that we are by inducing v \"P Vr\" 1, because if v \"P Vr\" then mrpv \"q\" m \"q.\""}, {"heading": "6.2 Proof of Lemma 7", "text": "Proof. Note that algorithm 2 passes through K rounds at most with probability 1. With theorem 12 with \u03b4 \"\u03b1 {K we get that the expected number of rounds to be completed by the algorithm is at most 4\u03b1 logpK2 {\u03b1q'p\u03b1 {Kq \u00a8 K \u0445 10\u03b1 logK, since K \u011b 2 by acceptance and \u03b1 \u011b 1."}, {"heading": "6.3 Proof of Lemma 5", "text": "Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q Q"}, {"heading": "6.4 Proof of Theorem 4", "text": "\"In order to prove the probability and expectation of the marginal distribution, we need a few definitions in which there is a limit.\" Let's take the PvP, Q, QrEs, QrEs, QrEs, QrEs, etc. \"If the PvP and Q sum up the PvP support, and where the sum up the PvP support, and where the sum up the PvP support is taken.\" We can now turn to the proof of theorem.Proof Theoremp 4. Let's imagine the random variables Tv, whose value is the number of learners. \"We also present the notations Pv and Ev, which represent probability and expectation in relation to the marginal distributions.\""}, {"heading": "6.5 Proof of Theorem 1", "text": "We will construct an environment whose distribution by diagrams coincides with that described in section 3, due to the case that the independence numbers of all the diagrams by 9.We now claim that regret against this environment is not too far removed from regret against the original environment, because the expected regret against the original environment is at best the expected regret against this environment plus p\u0442 {8qT, by term 5 and since regret against this environment is at most T (probability 1).According to Theorem 4, regret against this environment is at least RT '8 T \u011b? KT32' p1 {8qa K {T8 T? KT64 and thus holds the lower limit."}, {"heading": "6.6 Proof of Theorem 15", "text": "Suffice it to point out a randomized strategy for the environment that forces each deterministic learner to impose a lower limit on the learner's regret. We will construct the strategy of our environment as follows. Let's consider a learning problem about two actions, u and v. Before the game begins, the environment is sampled randomly for an index of the ability to learn. If the environment plays a distribution; if it plays a different distribution, then it plays a different distribution. Below the first distribution, the loss of v is distributed Bernoulli (3 {8); below the second distribution, it is distributed as Bernoulli (5 {8). In both cases, the action u always has a self-loop and its loss is constant 1 {2.The feedback graphs G1,..., GT are i.d. chosen and depend on the loss of the action v. Below the first distribution."}, {"heading": "Acknowledgements", "text": "The TK would like to thank Nicolo Cesa-Bianchi and Ofer Dekel for the stimulating discussions in the early stages of this research."}], "references": [{"title": "The Probabilistic Method", "author": ["N. Alon", "J.H. Spencer"], "venue": null, "citeRegEx": "Alon and Spencer.,? \\Q2008\\E", "shortCiteRegEx": "Alon and Spencer.", "year": 2008}, {"title": "From bandits to experts: A tale of domination and independence", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Alon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2013}, {"title": "Nonstochastic multi-armed bandits with graph-structured feedback", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "S. Mannor", "Y. Mansour", "O. Shamir"], "venue": "CoRR, abs/1409.8428,", "citeRegEx": "Alon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2014}, {"title": "Online learning with feedback graphs: Beyond bandits", "author": ["N. Alon", "N. Cesa-Bianchi", "O. Dekel", "T. Koren"], "venue": "In Proceedings of The 28th Conference on Learning", "citeRegEx": "Alon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2015}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["J. Audibert", "S. Bubeck"], "venue": "In Proceedings of the 22nd Conference on Learning Theory (COLT),", "citeRegEx": "Audibert and Bubeck.,? \\Q2009\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2009}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Leveraging side observations in stochastic bandits", "author": ["S. Caron", "B. Kveton", "M. Lelarge", "S. Bhagat"], "venue": "In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, Catalina Island, CA,", "citeRegEx": "Caron et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Caron et al\\.", "year": 2012}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Helmbold", "R.E. Schapire", "M.K. Warmuth"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "Pac bounds for multi-armed bandit and markov decision processes", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "In Computational Learning Theory,", "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of computer and system sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Almost optimal exploration in multi-armed bandits", "author": ["Z. Karnin", "T. Koren", "O. Somekh"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Karnin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karnin et al\\.", "year": 2013}, {"title": "Efficient learning by implicit exploration in bandit problems with side observations", "author": ["T. Koc\u00e1k", "G. Neu", "M. Valko", "R. Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2014}, {"title": "Online learning with noisy side observations", "author": ["T. Koc\u00e1k", "G. Neu", "M. Valko"], "venue": "In Proceedings of the Nineteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2016}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "From bandits to experts: On the value of side-observations", "author": ["S. Mannor", "O. Shamir"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mannor and Shamir.,? \\Q2011\\E", "shortCiteRegEx": "Mannor and Shamir.", "year": 2011}, {"title": "Aggregating strategies", "author": ["V.G. Vovk"], "venue": "In Proc. Third Workshop on Computational Learning Theory,", "citeRegEx": "Vovk.,? \\Q1990\\E", "shortCiteRegEx": "Vovk.", "year": 1990}, {"title": "Online learning with gaussian payoffs and side observations", "author": ["Y. Wu", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "We study an online learning framework introduced by Mannor and Shamir (2011) in which the feedback is specified by a graph, in a setting where the graph may vary from round to round and is never fully revealed to the learner.", "startOffset": 52, "endOffset": 77}, {"referenceID": 14, "context": "Traditionally, the literature considers two types of feedback: full feedback (Littlestone and Warmuth, 1994; Vovk, 1990; Cesa-Bianchi et al., 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al.", "startOffset": 77, "endOffset": 147}, {"referenceID": 16, "context": "Traditionally, the literature considers two types of feedback: full feedback (Littlestone and Warmuth, 1994; Vovk, 1990; Cesa-Bianchi et al., 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al.", "startOffset": 77, "endOffset": 147}, {"referenceID": 8, "context": "Traditionally, the literature considers two types of feedback: full feedback (Littlestone and Warmuth, 1994; Vovk, 1990; Cesa-Bianchi et al., 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al.", "startOffset": 77, "endOffset": 147}, {"referenceID": 1, "context": ", 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al. (2002b), where the learner only observes the loss of the action she has actually taken.", "startOffset": 112, "endOffset": 132}, {"referenceID": 1, "context": ", 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al. (2002b), where the learner only observes the loss of the action she has actually taken. Full feedback and bandit feedback are special cases of a general framework introduced by Mannor and Shamir (2011), in which the feedback model is specified by a sequence G1, .", "startOffset": 112, "endOffset": 326}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al. (2014); Alon et al.", "startOffset": 0, "endOffset": 57}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? \u03b1T q regret, where \u03b1 is a bound on the independence numbers of the graphs G1, .", "startOffset": 0, "endOffset": 77}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? \u03b1T q regret, where \u03b1 is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al.", "startOffset": 0, "endOffset": 379}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? \u03b1T q regret, where \u03b1 is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al. (2002b); Audibert and Bubeck (2009).", "startOffset": 0, "endOffset": 434}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? \u03b1T q regret, where \u03b1 is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al. (2002b); Audibert and Bubeck (2009). The r Op ? \u03b1T q bound turns out to be tight for any feedback graph (when it is fixed throughout the game and known in advance), in light of a matching lower bound due to Mannor and Shamir (2011).", "startOffset": 0, "endOffset": 462}, {"referenceID": 1, "context": "Alon et al. (2013), and subsequently Koc\u00e1k et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? \u03b1T q regret, where \u03b1 is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al. (2002b); Audibert and Bubeck (2009). The r Op ? \u03b1T q bound turns out to be tight for any feedback graph (when it is fixed throughout the game and known in advance), in light of a matching lower bound due to Mannor and Shamir (2011).", "startOffset": 0, "endOffset": 658}, {"referenceID": 12, "context": "While some require the entire graph Gt for performing their updates only at the end of round t (e.g., Alon et al., 2013; Koc\u00e1k et al., 2014; Alon et al., 2015), others actually need the description of Gt at the beginning of the round before making their decision (e.", "startOffset": 95, "endOffset": 159}, {"referenceID": 3, "context": "While some require the entire graph Gt for performing their updates only at the end of round t (e.g., Alon et al., 2013; Koc\u00e1k et al., 2014; Alon et al., 2015), others actually need the description of Gt at the beginning of the round before making their decision (e.", "startOffset": 95, "endOffset": 159}, {"referenceID": 5, "context": "\u201d In other words, the side observations received by the learner are effectively useless; she may as well ignore them and use a standard bandit algorithm such as Exp3 Auer et al. (2002b) to perform optimally.", "startOffset": 166, "endOffset": 186}, {"referenceID": 10, "context": "This result is optimal up to logarithmic factors, even when the feedback graph is fixed throughout the game and known in advance, due to a lower bound of Mannor and Shamir (2011). For our algorithm in the stochastic case, we also prove a distribution-dependent regret bound that scales logarithmically with T .", "startOffset": 154, "endOffset": 179}, {"referenceID": 2, "context": "This bound is a substantial improvement over standard regret bounds of stochastic multi-armed bandit algorithms such as UCB Auer et al. (2002a): whereas the regret of the latter algorithms is typically bounded by a sum \u0159 vPV p1{\u2206vq taken over all K actions, the sum in our bound is taken only over the subset of r Op\u03b1q actions with the smallest gaps.", "startOffset": 124, "endOffset": 144}, {"referenceID": 1, "context": "Again, this result cannot be improved even when the feedback graph is fixed throughout the game, and has an optimal dependence on \u03b1 as well as on the gaps \u2206v, thus resolving an open question of Alon et al. (2014). Finally, we extend our results to a more general feedback model recently studied by Alon et al.", "startOffset": 194, "endOffset": 213}, {"referenceID": 1, "context": "Again, this result cannot be improved even when the feedback graph is fixed throughout the game, and has an optimal dependence on \u03b1 as well as on the gaps \u2206v, thus resolving an open question of Alon et al. (2014). Finally, we extend our results to a more general feedback model recently studied by Alon et al. (2015), in which the learner does not necessarily observe her own loss after making predictions (namely, each action may or may not have a self-loop in each feedback graph).", "startOffset": 194, "endOffset": 317}, {"referenceID": 1, "context": "Again, this result cannot be improved even when the feedback graph is fixed throughout the game, and has an optimal dependence on \u03b1 as well as on the gaps \u2206v, thus resolving an open question of Alon et al. (2014). Finally, we extend our results to a more general feedback model recently studied by Alon et al. (2015), in which the learner does not necessarily observe her own loss after making predictions (namely, each action may or may not have a self-loop in each feedback graph). Alon et al. (2015) gave a necessary and sufficient condition for attaining \u0398p ? T q regret in this more general model\u2014a graph-theoretic condition they call strong observability.", "startOffset": 194, "endOffset": 503}, {"referenceID": 4, "context": "Online learning with feedback graphs was previously considered in the stochastic setting by Caron et al. (2012), who gave results depending on the graph clique structure.", "startOffset": 92, "endOffset": 112}, {"referenceID": 1, "context": "Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014). More recently, Wu et al. (2015) and Koc\u00e1k et al.", "startOffset": 296, "endOffset": 348}, {"referenceID": 1, "context": "Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014). More recently, Wu et al. (2015) and Koc\u00e1k et al. (2016) have investigated a noisy version of the feedback graph model, where feedback is specified by a weighted directed graph with edge weights indicating the quality (e.", "startOffset": 296, "endOffset": 372}, {"referenceID": 1, "context": "Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014). More recently, Wu et al. (2015) and Koc\u00e1k et al. (2016) have investigated a noisy version of the feedback graph model, where feedback is specified by a weighted directed graph with edge weights indicating the quality (e.g., the noise level or variance) of the feedback received on adjacent vertices. Wu et al. (2015) provided finite-time problem-dependent lower bounds for this setting; Koc\u00e1k et al.", "startOffset": 296, "endOffset": 633}, {"referenceID": 1, "context": "Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014). More recently, Wu et al. (2015) and Koc\u00e1k et al. (2016) have investigated a noisy version of the feedback graph model, where feedback is specified by a weighted directed graph with edge weights indicating the quality (e.g., the noise level or variance) of the feedback received on adjacent vertices. Wu et al. (2015) provided finite-time problem-dependent lower bounds for this setting; Koc\u00e1k et al. (2016) generalized the notion of independence number to the noisy case and gave new efficient algorithms in this setting.", "startOffset": 296, "endOffset": 723}, {"referenceID": 15, "context": "This regret bound is optimal up to logarithmic factors, since the lower bound of \u03a9p ? \u03b1T q found in Mannor and Shamir (2011) applies in our stochastic setting.", "startOffset": 100, "endOffset": 125}, {"referenceID": 11, "context": "The algorithm, given in Algorithm 1, is reminiscent of elimination-based algorithms for the stochastic multiarmed bandit problem (e.g., Even-Dar et al., 2002; Karnin et al., 2013).", "startOffset": 129, "endOffset": 179}, {"referenceID": 1, "context": "Recently, Alon et al. (2015) have studied this more general feedback model and divided feedback graphs into three categories: unobservable graphs, for which the induced problem is not learnable; weakly observable graphs, for which r \u0398pT 2{3q regret is achievable; and strongly observable graphs, for which it is possible to attain r \u0398p ? T q regret.", "startOffset": 10, "endOffset": 29}, {"referenceID": 1, "context": "Recently, Alon et al. (2015) have studied this more general feedback model and divided feedback graphs into three categories: unobservable graphs, for which the induced problem is not learnable; weakly observable graphs, for which r \u0398pT 2{3q regret is achievable; and strongly observable graphs, for which it is possible to attain r \u0398p ? T q regret. Their results assume that the feedback graphs are available to the learner, at least after making each prediction. Here, we revisit their results assuming that the graphs are never fully revealed to the learner. We begin by recalling the definitions of observability of Alon et al. (2015). A vertex in a directed graph is observable if it has at least one incoming edge.", "startOffset": 10, "endOffset": 639}, {"referenceID": 1, "context": ", GT , since the r \u03a9pK1{3T 2{3q lower bound proved by Alon et al. (2015), in the easier setting where the graphs are revealed following each decision, applies in our stochastic setting.", "startOffset": 54, "endOffset": 73}], "year": 2016, "abstractText": "We study an online learning framework introduced by Mannor and Shamir (2011) in which the feedback is specified by a graph, in a setting where the graph may vary from round to round and is never fully revealed to the learner. We show a large gap between the adversarial and the stochastic cases. In the adversarial case, we prove that even for dense feedback graphs, the learner cannot improve upon a trivial regret bound obtained by ignoring any additional feedback besides her own loss. In contrast, in the stochastic case we give an algorithm that achieves r \u0398p ? \u03b1T q regret over T rounds, provided that the independence numbers of the hidden feedback graphs are at most \u03b1. We also extend our results to a more general feedback model, in which the learner does not necessarily observe her own loss, and show that, even in simple cases, concealing the feedback graphs might render a learnable problem unlearnable.", "creator": "LaTeX with hyperref package"}}}