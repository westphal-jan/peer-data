{"id": "1605.07078", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Learning Sensor Multiplexing Design through Back-propagation", "abstract": "Recent progress on many imaging and vision tasks has been driven by the use of deep feed-forward neural networks, which are trained by propagating gradients of a loss defined on the final output, back through the network up to the first layer that operates directly on the image. We propose back-propagating one step further---to learn camera sensor designs jointly with networks that carry out inference on the images they capture. In this paper, we specifically consider the design and inference problems in a typical color camera---where the sensor is able to measure only one color channel at each pixel location, and computational inference is required to reconstruct a full color image. We learn the camera sensor's color multiplexing pattern by encoding it as layer whose learnable weights determine which color channel, from among a fixed set, will be measured at each location. These weights are jointly trained with those of a reconstruction network that operates on the corresponding sensor measurements to produce a full color image. Our network achieves significant improvements in accuracy over the traditional Bayer pattern used in most color cameras. It automatically learns to employ a sparse color measurement approach similar to that of a recent design, and moreover, improves upon that design by learning an optimal layout for these measurements.", "histories": [["v1", "Mon, 23 May 2016 16:26:59 GMT  (876kb,D)", "https://arxiv.org/abs/1605.07078v1", "Project page atthis http URL"], ["v2", "Wed, 2 Nov 2016 18:30:57 GMT  (5465kb,D)", "http://arxiv.org/abs/1605.07078v2", "NIPS 2016. Project page atthis http URL"]], "COMMENTS": "Project page atthis http URL", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ayan chakrabarti"], "accepted": true, "id": "1605.07078"}, "pdf": {"name": "1605.07078.pdf", "metadata": {"source": "CRF", "title": "Learning Sensor Multiplexing Design through Back-propagation", "authors": ["Ayan Chakrabarti"], "emails": ["ayanc@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this project is a project, which is primarily a project."}, {"heading": "2 Background", "text": "In fact, most of them will be able to move to a different world in which they are able to live and live than the world in which they live."}, {"heading": "3 Jointly Learning Measurement and Reconstruction", "text": "We formulate our task as the reconstruction of an RGB image y (n) R3, where n (n) Z2 indexes the pixel position from a measured sensor image s (n) R. In addition to this reconstruction task, we must also choose a multiplex pattern that determines the color channel corresponding to each of the s (n). We allow this choice between one of the C channels - a parameterization that takes into account which spectral filters can be physically synthesized. We use x (n) RC to denote the intensity measurements corresponding to each of these color channels, and a zero-one selection card I (n) {0, 1} C, | I (n) | = 1 to encode the multiplex pattern so that the corresponding sensor measurements are given by s (n) = I (n) Tx (n) Tx (n).In addition, we assume that I (n) repeat all P pixels and therefore have only 2 unique values."}, {"heading": "3.1 Learning the Multiplexing Pattern", "text": "The key challenge to our common learning problem lies in restoring the optimal multiplexing pattern I (n), since it requires a slow increase and must be learned to make a hard, non-differentiable decision between the C-options. To address this, we also rely on the default Soft Max pattern traditionally used in our multi-level classification. However, unlike in classification tasks where the proper labels are the final output, and where the training target prefers the hard assignment to a single label, our formulation I (n) is used to generate sensor measurements that are then processed by a reconstruction network. In fact, when we use a straight Soft Max network, we find that the reconstruction networks convert to real-rated cards that match the measurement of different weighted combinations of the input channels even if we continue to train the significant decrease in tholdering."}, {"heading": "3.2 Reconstruction Network Architecture", "text": "Traditional demosaicking algorithms [15] produce a full color image by interpolating the missing color values from adjacent measurement sites (e.g. in [4]) and exploiting cross-channel dependencies; this interpolation is often linear, but in some cases it occurs in the form of transferring chromaticity or color ratios. Furthermore, most demosaicking algorithms establish the image textures and edges to avoid cross-border smoothing or the creation of aliasing artifacts. We use a simple, split network architecture that uses these intuitions. As shown in Fig. 1, our network reconstructs each P-P patch in y (n) from a receptive field centered on this patch in the measured image (s), and three times as large in each dimension. The network has two paths that generate both on the entire input and both outputs (P-P-P-values-3K) for each dimension."}, {"heading": "4 Experiments", "text": "We follow a similar approach to [4] training and evaluation of our method. As [4], we use the Gehler-Shi database [6, 21], which consists of 568 color images of indoor and outdoor scenes taken under different lighting fixtures. Therefore, these images were taken at half the resolution of the original sensor image from a camera that uses the Bayer pattern with an anti-aliasing filter, using the different color measurements in each Bayer block to construct a single RGB pixel. Therefore, these images are half the resolution of the original sensor image, but have statistics representative of the aliasing-free full-color images of typical natural scenes. Unlike [4], which only use 10 images for evaluation, we use the entire data set - using 56 images for testing, 461 images for training, and the remaining 51 images as a validation set to fix hyperparameters. We treat the images in the data sets (the RGB Truth Edition)."}, {"heading": "4.1 Evaluating the Reconstruction Network", "text": "We start by comparing the performance of our learned reconstruction networks with traditional demosaicking algorithms for the standard Bayer pattern and the pattern of [4]. Note that our goal is not to propose a new demosaicking method for existing sensors. However, since our sensor pattern is learned together with our proposed reconstruction architecture, it is important to determine whether this architecture can learn to effectively argue with different types of sensor patterns, which is necessary to effectively cover the common sensor inference design space. We compare our learned networks with Zhang and Wu's method [27] for the Bayer pattern and Chakrabarti et al.'s method [4] for their own pattern. We measure the performance in terms of rebuilding all non-overlapping 64 \u00d7 64 patches from all test images (approximately 40,000 patches). Table 1 compares the mean PSNR values for reconstructions across all patches 0- one of the better patterns - two of the same noise patterns - one of 0 - 1."}, {"heading": "4.2 Visualizing Sensor Pattern Training", "text": "In Fig. 2, we illustrate the evolution of our sensor pattern during the training process as it is learned along with the reconstruction network. In the first iterations, the sensor layers show a preference for dense scanning of the RGB channels with very few panchromatic measurements - in fact, in the first row of Fig. 2, we see panchromatic pixels switching to color measurements. This is likely because the reconstruction network has not yet learned to take advantage of the cross-channel correlations at the beginning of the training process, and therefore needs to measure the output channels directly. However, as the training progresses, the reconstruction network becomes more complex, and we see the number of color measurements more sparse and in favor of panchromatic pixels, which offer the advantage of higher SNR correlations. Essentially, the sensor layer is beginning to assume one of the design principles of [4]. However, it distributes the color corners to the pattern of 500, so that they fix the pattern as most of K."}, {"heading": "4.3 Evaluating Learned Pattern", "text": "Finally, we evaluate the performance of neural network-based reconstruction using our learned pattern up to those with the Bayer pattern and pattern of [4]. Table 2 shows different quantities of PSNR reconstruction for different sound levels, with STD values ranging from 0 to 0.04. Although our sensor pattern was trained at the sound level of STD = 0.01, we find that it achieves the highest reconstruction quality over a wide range of sound levels. Specifically, it always exceeds the Bayer pattern by fairly significant margins at higher noise levels. The improvement in performance over the pattern of [4] is less pronounced, although we consistently achieve higher PSNR values for all quantities at most noise levels. Figure 3 shows examples of color spots reconstructed by our learned sensor, and compares them with those of the Bayer pattern and [4].We see that the reconstructions of the Bayer pattern are significantly worse."}, {"heading": "5 Conclusion", "text": "We learned this pattern through collaborative training with a neural network to reconstruct full-color images from the multiplexed measurements. We used a soft-max operation with a rising temperature parameter to model the non-differentiable color channel selection at each point, which enabled us to effectively train the pattern. Finally, we showed that our learned pattern enabled better reconstructions than previous designs. Implementation of our method along with trained models, data and results is available on our project page at http: / / www.ttic.edu / chakrabarti / learncfa /. Our results suggest that learning measurement strategies along with computational conclusions is both useful and possible. Specifically, our approach can be used to learn other forms of optimized multiplexing patterns - such as spatial measurement strategies for light field acquisition, multiplexing, etc."}, {"heading": "Acknowledgments", "text": "We thank NVIDIA Corporation for donating a Titan X GPU used for this research."}], "references": [{"title": "Compressive sensing", "author": ["R.G. Baraniuk"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Color imaging array", "author": ["B.E. Bayer"], "venue": "US Patent 3971065,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1976}, {"title": "Image denoising: Can plain neural networks compete with BM3D? In Proc", "author": ["H.C. Burger", "C.J. Schuler", "S. Harmeling"], "venue": "CVPR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Rethinking color cameras", "author": ["A. Chakrabarti", "W.T. Freeman", "T. Zickler"], "venue": "Proc. ICCP,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimized projections for compressed sensing", "author": ["M. Elad"], "venue": "IEEE Trans. Sig. Proc.,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian color constancy revisited", "author": ["P.V. Gehler", "C. Rother", "A. Blake", "T. Minka", "T. Sharp"], "venue": "Proc. CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "arXiv:1510.00149,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Flutter shutter video camera for compressive sensing of videos", "author": ["J. Holloway", "A.C. Sankaranarayanan", "A. Veeraraghavan", "S. Tambe"], "venue": "Proc. ICCP,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning joint demosaicing and denoising based on sequential energy minimization", "author": ["T. Kaltzer", "K. Hammernik", "P. Knobelreiter", "T. Pock"], "venue": "Proc. ICCP,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Demosaicking using artificial neural networks", "author": ["O. Kapah", "H.Z. Hel-Or"], "venue": "Electronic Imaging,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Joint demosaicing and denoising via learned nonparametric random fields", "author": ["D. Khashabi", "S. Nowozin", "J. Jancsary", "A.W. Fitzgibbon"], "venue": "IEEE Trans. Imag. Proc.,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Image and depth from a conventional camera with a coded aperture", "author": ["A. Levin", "R. Fergus", "F. Durand", "W.T. Freeman"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Image demosaicing: A systematic survey", "author": ["X. Li", "B. Gunturk", "L. Zhang"], "venue": "Proc. SPIE,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proc. CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Non-local sparse models for image restoration", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "Proc. ICCV,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Coded exposure photography: motion deblurring using fluttered shutter", "author": ["R. Raskar", "A. Agrawal", "J. Tumblin"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "A machine learning approach for non-blind image deconvolution", "author": ["C.J. Schuler", "H.C. Burger", "S. Harmeling", "B. Scholkopf"], "venue": "Proc. CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "arXiv:1312.6229,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Re-processed version of the Gehler color constancy dataset", "author": ["L. Shi", "B. Funt"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Separable markov random field model and its applications in low level vision", "author": ["J. Sun", "M.F. Tappen"], "venue": "IEEE Trans. Imag. Proc.,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Dappled photography: Mask enhanced cameras for heterodyned light fields and coded aperture refocusing", "author": ["A. Veeraraghavan", "R. Raskar", "A. Agrawal", "A. Mohan", "J. Tumblin"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Designing deep networks for surface normal estimation", "author": ["X. Wang", "D. Fouhey", "A. Gupta"], "venue": "Proc. CVPR,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparcs: Recovering low-rank and sparse matrices from compressive measurements", "author": ["A.E. Waters", "A.C. Sankaranarayanan", "R. Baraniuk"], "venue": "NIPS,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep convolutional neural network for image deconvolution", "author": ["L. Xu", "J.S. Ren", "C. Liu", "J. Jia"], "venue": "NIPS,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Color demosaicking via directional linear minimum mean square-error estimation", "author": ["L. Zhang", "X. Wu"], "venue": "IEEE Trans. Imag. Proc.,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "From learning models of natural image patches to whole image restoration", "author": ["D. Zoran", "Y. Weiss"], "venue": "Proc. ICCV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "Sophisticated techniques, such as those for denoising [3, 28], deblurring [19, 26], etc.", "startOffset": 54, "endOffset": 61}, {"referenceID": 27, "context": "Sophisticated techniques, such as those for denoising [3, 28], deblurring [19, 26], etc.", "startOffset": 54, "endOffset": 61}, {"referenceID": 18, "context": "Sophisticated techniques, such as those for denoising [3, 28], deblurring [19, 26], etc.", "startOffset": 74, "endOffset": 82}, {"referenceID": 25, "context": "Sophisticated techniques, such as those for denoising [3, 28], deblurring [19, 26], etc.", "startOffset": 74, "endOffset": 82}, {"referenceID": 17, "context": "For example, coded exposure imaging [18] allows better inversion of motion blur, coded apertures [14, 23] allow passive measurement of scene depth from a single shot, and compressive measurement strategies [1, 8, 25] combined with sparse reconstruction algorithms allow the recovery of visual measurements with higher spatial, spectral, and temporal resolutions.", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "For example, coded exposure imaging [18] allows better inversion of motion blur, coded apertures [14, 23] allow passive measurement of scene depth from a single shot, and compressive measurement strategies [1, 8, 25] combined with sparse reconstruction algorithms allow the recovery of visual measurements with higher spatial, spectral, and temporal resolutions.", "startOffset": 97, "endOffset": 105}, {"referenceID": 22, "context": "For example, coded exposure imaging [18] allows better inversion of motion blur, coded apertures [14, 23] allow passive measurement of scene depth from a single shot, and compressive measurement strategies [1, 8, 25] combined with sparse reconstruction algorithms allow the recovery of visual measurements with higher spatial, spectral, and temporal resolutions.", "startOffset": 97, "endOffset": 105}, {"referenceID": 0, "context": "For example, coded exposure imaging [18] allows better inversion of motion blur, coded apertures [14, 23] allow passive measurement of scene depth from a single shot, and compressive measurement strategies [1, 8, 25] combined with sparse reconstruction algorithms allow the recovery of visual measurements with higher spatial, spectral, and temporal resolutions.", "startOffset": 206, "endOffset": 216}, {"referenceID": 7, "context": "For example, coded exposure imaging [18] allows better inversion of motion blur, coded apertures [14, 23] allow passive measurement of scene depth from a single shot, and compressive measurement strategies [1, 8, 25] combined with sparse reconstruction algorithms allow the recovery of visual measurements with higher spatial, spectral, and temporal resolutions.", "startOffset": 206, "endOffset": 216}, {"referenceID": 24, "context": "For example, coded exposure imaging [18] allows better inversion of motion blur, coded apertures [14, 23] allow passive measurement of scene depth from a single shot, and compressive measurement strategies [1, 8, 25] combined with sparse reconstruction algorithms allow the recovery of visual measurements with higher spatial, spectral, and temporal resolutions.", "startOffset": 206, "endOffset": 216}, {"referenceID": 3, "context": ", [4]), or based on the decision to use a specific image model or inference strategy\u2014e.", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": ", measurements corresponding to random [1], or dictionary-specific [5], projections are a common choice for sparsity-based reconstruction methods.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": ", measurements corresponding to random [1], or dictionary-specific [5], projections are a common choice for sparsity-based reconstruction methods.", "startOffset": 67, "endOffset": 70}, {"referenceID": 12, "context": "We leverage the successful use of back-propagation and stochastic gradient descent (SGD) [13] in learning deep neural networks for various tasks [12, 16, 20, 24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "We leverage the successful use of back-propagation and stochastic gradient descent (SGD) [13] in learning deep neural networks for various tasks [12, 16, 20, 24].", "startOffset": 145, "endOffset": 161}, {"referenceID": 15, "context": "We leverage the successful use of back-propagation and stochastic gradient descent (SGD) [13] in learning deep neural networks for various tasks [12, 16, 20, 24].", "startOffset": 145, "endOffset": 161}, {"referenceID": 19, "context": "We leverage the successful use of back-propagation and stochastic gradient descent (SGD) [13] in learning deep neural networks for various tasks [12, 16, 20, 24].", "startOffset": 145, "endOffset": 161}, {"referenceID": 23, "context": "We leverage the successful use of back-propagation and stochastic gradient descent (SGD) [13] in learning deep neural networks for various tasks [12, 16, 20, 24].", "startOffset": 145, "endOffset": 161}, {"referenceID": 1, "context": "We find that our approach significantly outperforms the traditional Bayer pattern [2] used in most color cameras.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "We also compare it to a recently introduced design [4] based on making sparse color measurements, that has superior noise performance and fewer aliasing artifacts.", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "The CFA pattern determines which color channel is measured at which pixel, with the most commonly pattern used in RGB color cameras being the Bayer mosaic [2] introduced in 1976.", "startOffset": 155, "endOffset": 158}, {"referenceID": 14, "context": "These algorithms [15] typically rely on the assumption that different color channels are correlated and piecewise smooth, and reason about locations of edges and other high-frequency image content to avoid creating aliasing artifacts.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "[4] recently proposed the use of an alternative CFA pattern in which a majority of the pixels measure the total unfiltered visible light intensity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The reconstruction algorithm in [4] is significantly different from traditional demosaicking, and involves first recovering missing luminance values by hole-filling (which is computationally easier than up-sampling since there is more context around the missing intensities), and then propagating chromaticities from the color measurement sites to the remaining pixels using edges in the luminance image as a guide.", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "That [4]\u2019s CFA pattern required a very different reconstruction algorithm illustrates the fact that both the sensor and inference method need to be modified together to achieve gains in performance.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "In [4]\u2019s case, this was achieved by applying an intuitive design principles\u2014of making high SNR non-aliased measurements of one color channel.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "While learning-based methods have been proposed for demosaicking [10, 17, 22] (as well as for joint demosaicking and denoising [9, 11]), these work with a pre-determined CFA pattern and training is used only to tune the reconstruction algorithm.", "startOffset": 65, "endOffset": 77}, {"referenceID": 16, "context": "While learning-based methods have been proposed for demosaicking [10, 17, 22] (as well as for joint demosaicking and denoising [9, 11]), these work with a pre-determined CFA pattern and training is used only to tune the reconstruction algorithm.", "startOffset": 65, "endOffset": 77}, {"referenceID": 21, "context": "While learning-based methods have been proposed for demosaicking [10, 17, 22] (as well as for joint demosaicking and denoising [9, 11]), these work with a pre-determined CFA pattern and training is used only to tune the reconstruction algorithm.", "startOffset": 65, "endOffset": 77}, {"referenceID": 8, "context": "While learning-based methods have been proposed for demosaicking [10, 17, 22] (as well as for joint demosaicking and denoising [9, 11]), these work with a pre-determined CFA pattern and training is used only to tune the reconstruction algorithm.", "startOffset": 127, "endOffset": 134}, {"referenceID": 10, "context": "While learning-based methods have been proposed for demosaicking [10, 17, 22] (as well as for joint demosaicking and denoising [9, 11]), these work with a pre-determined CFA pattern and training is used only to tune the reconstruction algorithm.", "startOffset": 127, "endOffset": 134}, {"referenceID": 6, "context": "\u2019s [7] approach to network compression.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "Traditional demosaicking algorithms [15] produce a full color image by interpolating the missing color values from neighboring measurement sites, and by exploiting cross-channel dependencies.", "startOffset": 36, "endOffset": 40}, {"referenceID": 3, "context": ", in [4]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "We follow a similar approach to [4] for training and evaluating our method.", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "Like [4], we use the Gehler-Shi database [6, 21] that consists of 568 color images of indoor and outdoor scenes, captured under various illuminants.", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "Like [4], we use the Gehler-Shi database [6, 21] that consists of 568 color images of indoor and outdoor scenes, captured under various illuminants.", "startOffset": 41, "endOffset": 48}, {"referenceID": 20, "context": "Like [4], we use the Gehler-Shi database [6, 21] that consists of 568 color images of indoor and outdoor scenes, captured under various illuminants.", "startOffset": 41, "endOffset": 48}, {"referenceID": 3, "context": "Unlike [4] who only used 10 images for evaluation, we use the entire dataset\u2014using 56 images for testing, 461 images for training, and the remaining 51 images as a validation set to fix hyper-parameters.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "Like [4], we choose the fourth channel to be white or panchromatic, and construct it as the sum of the RGB measurements.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "As mentioned in [4], this corresponds to a conservative estimate of the light-efficiency of an unfiltered channel.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "We also train reconstruction networks at all noise levels in a similar way for the Bayer pattern, as well the pattern of [4] (with a color sampling rate of 4).", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "We begin by comparing the performance of our learned reconstruction networks to traditional demosaicking algorithms for the standard Bayer pattern, and the pattern of [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "Bayer CFZ [4]", "startOffset": 10, "endOffset": 13}, {"referenceID": 26, "context": "We compare our learned networks to Zhang and Wu\u2019s method [27] for the Bayer pattern, and Chakrabarti et al.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "\u2019s method [4] for their own pattern.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "For the pattern of [4], we find that our network performs similar to their reconstruction method at the low noise level, and significantly better at the higher noise level.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "In comparison, [4] and [27]\u2019s reconstruction methods take 20s and 1 min.", "startOffset": 15, "endOffset": 18}, {"referenceID": 26, "context": "In comparison, [4] and [27]\u2019s reconstruction methods take 20s and 1 min.", "startOffset": 23, "endOffset": 27}, {"referenceID": 3, "context": "Essentially, the sensor layer begins to adopt one of the design principles of [4].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "However, it distributes the color measurement sites across the pattern, instead of concentrating them into separated blocks like [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "Finally, we evaluate the performance of neural network-based reconstruction from measurements with our learned pattern, to those with the Bayer pattern and the pattern of [4].", "startOffset": 171, "endOffset": 174}, {"referenceID": 3, "context": "The improvement in performance over [4]\u2019s pattern is less pronounced, although we do achieve consistently higher PSNR values for all quantiles at most noise levels.", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "Figure 3 shows examples of color patches reconstructed from our learned sensor, and compare these to those from the Bayer pattern and [4].", "startOffset": 134, "endOffset": 137}, {"referenceID": 3, "context": "Both [4] and our pattern yield significantly better reconstructions.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "Indeed, most of our gains over the Bayer pattern come from choosing to make most measurements panchromatic, a design principle shared by [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 3, "context": "Moreover, we see that [4]\u2019s reconstructions tend to have a few more instances of \u201cchromaticity noise\u201d, in the form of contiguous regions with incorrect hues, which explain its slightly lower PSNR values in Table 2.", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "Noise STD Percentile Bayer [2] CFZ [4] Learned", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "Noise STD Percentile Bayer [2] CFZ [4] Learned", "startOffset": 35, "endOffset": 38}], "year": 2016, "abstractText": "Recent progress on many imaging and vision tasks has been driven by the use of deep feed-forward neural networks, which are trained by propagating gradients of a loss defined on the final output, back through the network up to the first layer that operates directly on the image. We propose back-propagating one step further\u2014to learn camera sensor designs jointly with networks that carry out inference on the images they capture. In this paper, we specifically consider the design and inference problems in a typical color camera\u2014where the sensor is able to measure only one color channel at each pixel location, and computational inference is required to reconstruct a full color image. We learn the camera sensor\u2019s color multiplexing pattern by encoding it as layer whose learnable weights determine which color channel, from among a fixed set, will be measured at each location. These weights are jointly trained with those of a reconstruction network that operates on the corresponding sensor measurements to produce a full color image. Our network achieves significant improvements in accuracy over the traditional Bayer pattern used in most color cameras. It automatically learns to employ a sparse color measurement approach similar to that of a recent design, and moreover, improves upon that design by learning an optimal layout for these measurements.", "creator": "LaTeX with hyperref package"}}}