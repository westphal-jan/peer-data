{"id": "1605.07110", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Deep Learning without Poor Local Minima", "abstract": "In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the independence assumption adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs for shallow networks (with three layers) and deeper networks (with more than three layers). Moreover, we prove that the same four statements hold for deep linear neural networks with any depth, any widths and no unrealistic assumptions. As a result, we present an instance, for which we can answer to the following question: how difficult to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima and the property of the saddle points). We note that even though we have advanced the theoretical foundations of deep learning, there is still a gap between theory and practice.", "histories": [["v1", "Mon, 23 May 2016 17:34:20 GMT  (33kb,D)", "http://arxiv.org/abs/1605.07110v1", null], ["v2", "Mon, 22 Aug 2016 14:26:22 GMT  (39kb,D)", "http://arxiv.org/abs/1605.07110v2", "In NIPS 2016. Selected for NIPS oral presentation (top 2% submissions). ---- This accepted version's contents remain the same as v1 (presentation was improved by following the reviewers' suggestions)"], ["v3", "Tue, 27 Dec 2016 22:47:50 GMT  (39kb)", "http://arxiv.org/abs/1605.07110v3", "In NIPS 2016. Selected for NIPS oral presentation (top 2% submissions). ---- The final NIPS 2016 version: the results remain the same"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["kenji kawaguchi"], "accepted": true, "id": "1605.07110"}, "pdf": {"name": "1605.07110.pdf", "metadata": {"source": "CRF", "title": "Deep Learning without Poor Local Minima", "authors": ["Kenji Kawaguchi"], "emails": ["kawaguch@mit.edu"], "sections": [{"heading": null, "text": "In this paper, we substantiate a conjecture published in 1989 and in part also address an open problem that was announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the assumption of independence adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs between flat networks (with three layers) and deeper networks (with more than three layers). Furthermore, we prove that the same four statements apply to deep linear neural networks with arbitrary depth, arbitrary width, and without unrealistic assumptions. As a result, we represent an instance for which we can answer the following question: How difficult is it to learn a deep model directly in theory? It is still more difficult than the gap between conventional learning machine (because of the non-conventional learning theory) and the non-conventional learning machine (because)."}, {"heading": "1 Introduction", "text": "In addition to their practical success, the theoretical results have shown that deep learning is attractive in terms of its generalization properties (Livni et al., 2014; Mhaskar et al., 2016), which means that deep learning introduces good functional classes that have a low capacity in the sense of the VC, while being able to represent target functions of interest well. However, deep learning requires that we have to deal with seemingly intractable optimization problems. Typically, training a deep model is done via non-convex optimization. Since finding a global minimum of general non-convex function is a NP-complete problem (Murty & Kabadi, 1987), it is hoped that a function induced by a deep model has a structure that makes non-convex optimization feasible. Unfortunately, in 1992 it was shown that the formation of a very simple neural network is actually NP-hard (Blum & Rivest, 1992)."}, {"heading": "2 Deep linear neural networks", "text": "In view of the lack of a theoretical understanding of deep nonlinear neural networks, Goodfellow et al. (2016) found that it is beneficial to theoretically analyze the loss functions of simpler models, i.e. linear neural networks. However, the functional class of a linear neural network contains only functions that are linear in terms of input, but their loss functions are not convex in the weight parameters and therefore not trivial. Saxe et al. (2014) showed empirically that the optimization of deep linear models exhibits similar properties to the optimization of deep nonlinear models. Ultimately, it is natural for theoretical development to start with linear models before working with nonlinear models (Baldi & Lu, 2012), and yet even with linear models, understanding is scarce when models become deep."}, {"heading": "2.1 Model and notation", "text": "Let H be the number of hidden layers, and let Y (X, Y) be the number of training data, where Y (Rdy) \u00b7 m and X (Rdx) \u00b7 m are the number of data points, where m is the number of data points. Here, dy \u2265 1 and dx \u2265 1 are the number of components (or dimensions) of the output and input data. We denote the model (weight) parameters by W, which consists of parameter matrices corresponding to each layer: WH + 1 (Rdy) \u00b7 dH,.., Wk (Rdk) -th layer is the output layer (i.e., d0 = Rd1 \u00b7 dx). Here, dk represents the width of the k-th layer, where the 0-th layer is the input layer, and the (H + 1) -th layer is the output layer (i.e., + ddy = 1 and dx)."}, {"heading": "2.2 Background", "text": "Recently Goodfellow et al. (2016) noted that when Baldi & Hornik (1989: flat linear network) claimed and proved that Proposition 2.1 for flat linear networks also represented Conjecture 2.2 for deep linear networks.Proposition 2.1 (Baldi & Hornik, 1989: flat linear network) Suppose that H = 1 (i.e. Y (W, X) = W2W1X), assuming that XXT and XY T are invertible, and assuming that p < dx, p < dy and dy = dx (e.g. an autoencoder), then the loss function L (W) requires the following properties: (i) It is convex convex in each matrix W1 (or W2), and a global minimum is set in each other W2 (or W1). (ii) Each local minimum is a global minimum (Baldi & Hornik, 1989: deep network)."}, {"heading": "2.3 Results", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to outdo themselves. (...) In fact, it is the case that they are able to outdo themselves. (...) In fact, it is the case that they are able to outdo themselves. (...) In fact, it is the case that they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...)"}, {"heading": "3 Deep nonlinear neural networks", "text": "Given this understanding of the loss area of deep linear models, we are discussing deep nonlinear models."}, {"heading": "3.1 Model", "text": "We use the same notation as for the deep linear models defined at the beginning of Section 2.1. the output of the deep nonlinear neural mesh Y (W, X), Y (W, X), Y (W, X) = q\u03c3H + 1 (WH + 1\u03c3H (WH\u03c3H \u2212 1 (WH \u2212 1 \u00b7 \u00b7 \u03c32 (W2\u03c31 (W1X) \u00b7 \u00b7 \u00b7))))))), 1If H = 1 is defined, to be brief, we define WH \u00b7 \u00b7 W2 = W1 \u00b7 \u00b7 W2, Id1, with slight notation misuse. 2Typically, we do this by assuming a smoothness of the values of the loss function. 3Other problems, such as poor conditioning, can make it difficult to achieve a fast convergence rate.where q \u043eR is simply a normalization factor whose value is specified later."}, {"heading": "3.2 Background", "text": "Following the work of Dauphin et al. (2014), Choromanska et al. (2015a), we examined the link between the loss functions of deep nonlinear networks and a function based on random matrix theory (i.e., we discuss the most important assumptions that discard these assumptions is an important open problem. The assumptions were made as A1p, A2p, A3p, A5u and A7p. Here we discuss the most relevant assumptions: A1p, A5u and A6u. We refer to the part of the assumption A1p (resp. A5u) that corresponds only to the model assumptions."}, {"heading": "3.3 Results", "text": "Contrary to previous work, we do not assume that we can take the expectation by random variables. In addition, we consider loss functions for all data points and all possible output dimensions (i.e., the vectored-rated results). We also look at the square loss of the expected square error, i.e., we look at the expected square error losses, i.e., we look at the expected square error losses, i.e., we look at the square error compensation, i.e., we look at the square error compensation, i.e., we look at the square error compensation, i.e. (W, X) \u2212 Y \u2212 2 F. Theorem 3.2 (Loss surface of deep nonlinear networks) Assume A1p-m and A5u-m. Furthermore, we assume that the square error squares, i.e."}, {"heading": "4 Important lemmas", "text": "In this section, we provide further theoretical results leading to further generalized insights. (...) Let us include the generalizations of lemmas in the appendix (...) Let us include the generalizations of lemmas in the appendix (...) Let us include the generalizations of lemmas in the appendix of WTk. (...) Let us include the generalizations of WTk (...), the generalizations of WTk (...), the generalizations of WTk (...), the generalizations of WTk (...), the generalizations of WTk (...), the generalizations of WTk (...), the generalizations of Wk (...), the generalizations of Wk (...), the Wk (...), the generalizations of Wk (...), the Wk (...), the Wk (...), the generalizations of the Wk (...)."}, {"heading": "5 Proof sketches of theorems", "text": "In fact, we have a different approach to deal with the \"bad\" points that appear when the model gets deeper (see Section 2.3), as well as to get more comprehensive characteristics of the critical points that are associated with more generality. Whereas the previous proofs are heavily based on the first parts of our proofs, we use the information of the second order."}, {"heading": "5.3 Proof sketch of Theorem 3.2", "text": "Similar to the previous paper (Choromanska et al., 2015a, b), we correlate our loss function with another function under the assumed assumptions. More specifically, we show that all theoretical results developed to date apply to the loss function of the deep linear models, L-W, also to the loss function of the deep nonlinear models, EZ [L (W)] and LEZ [Y] (W)."}, {"heading": "6 Conclusion", "text": "In this paper, we addressed some outstanding issues and presented the theoretical foundations of deep learning and nonconvex optimization. With respect to deep linear neural networks, we proved the above assumption and more detailed statements with more universality. With respect to deep nonlinear neural networks with corrected linear activation, we demonstrated a narrower statement with more universality (dy may vary) and strictly attenuated model assumptions (only two out of seven assumptions) compared to the previous work. However, our theory does not yet directly apply to the practical situation. To fill the gap between theory and practice, future work would further discard the remaining two of the seven assumptions made in previous work."}, {"heading": "Acknowledgments", "text": "The author would like to thank Prof. Leslie Kaelbling for her thoughtful comments on the paper. We are grateful for the support of NSF-Funding 1420927, ONR-Funding N00014-14-1-0486 and ARO-Funding W911NF1410433. All opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of our sponsors."}, {"heading": "A Proofs of lemmas and corollary in Section 4", "text": "We complete the collections of evidence of the lemmas and corollary in section 4.A.1 = WTC (WTC = WTC = WTC = WTC = WTC = WTC = WTC = WTC = WTC (WTC = WTC = WTC = WTC = WTC = WTC) L (WTK = WTK) L (WTK = WTK) L (WTC) L (WTK = WTC) L (WTC = WTC) L (WTK = WTK) L (WTK = WTK) L (WTC = WTC = WTC = WTC) L (WTC = WTC (WTK = WTC = WTC) L (WTC = WTC = WTC) L (WTK = WTC = WTC = WTC = WTC) L (WTK = WTC = WTC = WTK = WTC = WTC) L (WTK = WTC = WTC = WTK = WTC = WTC = WTC)"}, {"heading": "B Proof of Theorems 2.3 and 3.2", "text": "We complete the proofs of theorems 2.3 and 3.2.B.1 Proof of theorems 2.3 (ii) Proof by case analysis, we show that any point that meets the necessary conditions and the definition of a local minimum is a global minimum.) When we write a statement in proof, we often mean that a necessary condition of local minima implies the statement as it should be clear (i.e., we do not claim that the statement must be true unless the point is the candidate of local minima.) The case in which rank (WH \u00b7 W2) = p and dy \u2264 p: Assume that rank (WH \u00b7 W2) = p: We first get a necessary condition of Hessian that is semidefinitly positive, Xr = 0, and then interpret the condition true. If we < p, Corollary 4.5 with k = H supplies the necessary condition that Xr = 0. This is because the other condition > (Wp) is 1."}, {"heading": "C Proofs of Corollaries 2.4 and 3.3", "text": "In Theorem 2.3 (iv), \"if rank (W1 \u00b7 \u00b7 W2) = rank (Id1) = d1 = p,\" which is always true, is because p is the smallest width of hidden layers and there is only one hidden layer whose width is t1. Therefore, Theorem 2.3 (iv) immediately implies the statement of Korollary 2.4 with H \u2265 2, it is sufficient to show the existence of a simple layer containing t1 points of which the Hessian has no negative eigenvalue. Suppose that WH = WH \u2212 1 = \u00b7 \u00b7 W2 = W1 = 0. Then, from Lemma 4.1, we define a set of critical points where WH + 1 can vary in Rdy \u00d7 dH."}, {"heading": "D Discussion of the 1989 conjecture", "text": "The idea behind this is that the people who stay in the city also stay in the city in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which, in which they, in which, in which they, in which they, in which they, in which, in which they, in which they, in which, in which, in which they, in which, they, in which, they, in which, live, in which they, in which they, in which they, live, in which, in which they, in which they, in which they, live, live, in which they, in which, in which they, in which they, in which, in which, in which, they, in which, in which, they, in which, they, live, in which, in which, they, in which"}], "references": [{"title": "Linear learning: Landscapes and algorithms", "author": ["Baldi", "Pierre."], "venue": "Advances in neural information processing systems. pp. 65\u201372.", "citeRegEx": "Baldi and Pierre.,? 1989", "shortCiteRegEx": "Baldi and Pierre.", "year": 1989}, {"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["Baldi", "Pierre", "Hornik", "Kurt"], "venue": "Neural networks,", "citeRegEx": "Baldi et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 1989}, {"title": "Training a 3-node neural network is NP-complete", "author": ["Blum", "Avrim L", "Rivest", "Ronald L"], "venue": "Neural Networks,", "citeRegEx": "Blum et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1992}, {"title": "The Loss Surfaces of Multilayer Networks", "author": ["Choromanska", "Anna", "Henaff", "MIkael", "Mathieu", "Michael", "Ben Arous", "Gerard", "LeCun", "Yann"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Open Problem: The landscape of the loss surfaces of multilayer networks", "author": ["Choromanska", "Anna", "LeCun", "Yann", "Arous", "G\u00e9rard Ben"], "venue": "In Proceedings of The 28th Conference on Learning Theory", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Dauphin", "Yann N", "Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Ganguli", "Surya", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Escaping From Saddle Points\u2014Online Stochastic Gradient for Tensor Decomposition", "author": ["Ge", "Rong", "Huang", "Furong", "Jin", "Chi", "Yuan", "Yang"], "venue": "In Proceedings of The 28th Conference on Learning Theory", "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Deep Learning. Book in preparation for MIT Press. http://www.deeplearningbook.org", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "On the computational efficiency of training neural networks", "author": ["Livni", "Roi", "Shalev-Shwartz", "Shai", "Shamir", "Ohad"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Livni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Livni et al\\.", "year": 2014}, {"title": "Learning Real and Boolean Functions: When Is Deep Better Than Shallow", "author": ["Mhaskar", "Hrushikesh", "Liao", "Qianli", "Poggio", "Tomaso"], "venue": "Massachusetts Institute of Technology CBMM Memo No", "citeRegEx": "Mhaskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mhaskar et al\\.", "year": 2016}, {"title": "Some NP-complete problems in quadratic and nonlinear programming", "author": ["Murty", "Katta G", "Kabadi", "Santosh N"], "venue": "Mathematical programming,", "citeRegEx": "Murty et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Murty et al\\.", "year": 1987}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "The Schur complement and its applications", "author": ["Zhang", "Fuzhen."], "venue": "Vol. 4. Springer Science & Business Media.", "citeRegEx": "Zhang and Fuzhen.,? 2006", "shortCiteRegEx": "Zhang and Fuzhen.", "year": 2006}, {"title": "By solving for W1, W1 = (C TC)\u2212CTY X (XX )\u22121 + (I \u2212 (CTC)\u2212CTC)L, for an arbitrary matrix L. Due to the property of any generalized inverse (Zhang", "author": ["C. CC \u2212XY"], "venue": null, "citeRegEx": "\u2212XY,? \\Q2006\\E", "shortCiteRegEx": "\u2212XY", "year": 2006}, {"title": "Here, the first implication follows the necessary condition with any principal submatrix and the second implication follows the necessary condition with the Schur complement (Zhang, 2006, theorem 1.20", "author": [], "venue": "(I \u2212MM\u2212)M \u2032 = 0 (Zhang,", "citeRegEx": "T,? \\Q2006\\E", "shortCiteRegEx": "T", "year": 2006}, {"title": "R(CC) = R(C", "author": [], "venue": null, "citeRegEx": "\u2286,? \\Q1989\\E", "shortCiteRegEx": "\u2286", "year": 1989}], "referenceMentions": [{"referenceID": 8, "context": "In addition to its practical success, theoretical results have shown that deep learning is attractive in terms of its generalization properties (Livni et al., 2014; Mhaskar et al., 2016).", "startOffset": 144, "endOffset": 186}, {"referenceID": 9, "context": "In addition to its practical success, theoretical results have shown that deep learning is attractive in terms of its generalization properties (Livni et al., 2014; Mhaskar et al., 2016).", "startOffset": 144, "endOffset": 186}, {"referenceID": 7, "context": "In this paper, as a step toward establishing the optimization theory for deep learning, we prove a conjecture noted in (Goodfellow et al., 2016) for deep linear networks, and also address an open problem announced in (Choromanska et al.", "startOffset": 119, "endOffset": 144}, {"referenceID": 7, "context": "Given the absence of a theoretical understanding of deep nonlinear neural networks, Goodfellow et al. (2016) noted that it is beneficial to theoretically analyze the loss functions of simpler models, i.", "startOffset": 84, "endOffset": 109}, {"referenceID": 7, "context": "Given the absence of a theoretical understanding of deep nonlinear neural networks, Goodfellow et al. (2016) noted that it is beneficial to theoretically analyze the loss functions of simpler models, i.e., linear neural networks. The function class of a linear neural network only contains functions that are linear with respect to inputs. However, their loss functions are non-convex in the weight parameters and thus nontrivial. Saxe et al. (2014) empirically showed that the optimization of deep linear models exhibits similar properties to those of the optimization of deep nonlinear models.", "startOffset": 84, "endOffset": 450}, {"referenceID": 7, "context": "Recently, Goodfellow et al. (2016) remarked that when Baldi & Hornik (1989) stated and proved Proposition 2.", "startOffset": 10, "endOffset": 35}, {"referenceID": 7, "context": "Recently, Goodfellow et al. (2016) remarked that when Baldi & Hornik (1989) stated and proved Proposition 2.", "startOffset": 10, "endOffset": 76}, {"referenceID": 6, "context": "4 suggest that for 1-hidden layer networks, training can be done in polynomial time with a second order method or even with a modified stochastic gradient decent method, as discussed in (Ge et al., 2015).", "startOffset": 186, "endOffset": 203}, {"referenceID": 3, "context": "Following the work by Dauphin et al. (2014), Choromanska et al.", "startOffset": 22, "endOffset": 44}, {"referenceID": 3, "context": "(2014), Choromanska et al. (2015a) investigated the connection between the loss functions of deep nonlinear networks and a function well-studied via random matrix theory (i.", "startOffset": 8, "endOffset": 35}, {"referenceID": 3, "context": "(2014), Choromanska et al. (2015a) investigated the connection between the loss functions of deep nonlinear networks and a function well-studied via random matrix theory (i.e., the Hamiltonian of the spherical spin-glass model). They explained that their theoretical results relied on several unrealistic assumptions. Later, Choromanska et al. (2015b) suggested at the Conference on Learning Theory (COLT) 2015 that discarding these assumptions is an important open problem.", "startOffset": 8, "endOffset": 352}], "year": 2016, "abstractText": "In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the independence assumption adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs for shallow networks (with three layers) and deeper networks (with more than three layers). Moreover, we prove that the same four statements hold for deep linear neural networks with any depth, any widths and no unrealistic assumptions. As a result, we present an instance, for which we can answer to the following question: how difficult to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima and the property of the saddle points). We note that even though we have advanced the theoretical foundations of deep learning, there is still a gap between theory and practice.", "creator": "LaTeX with hyperref package"}}}