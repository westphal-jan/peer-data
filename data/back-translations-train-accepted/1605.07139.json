{"id": "1605.07139", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Fairness in Learning: Classic and Contextual Bandits", "abstract": "We introduce the study of fairness in multi-armed bandit problems. Our fairness definition can be interpreted as demanding that given a pool of applicants (say, for college admission or mortgages), a worse applicant is never favored over a better one, despite a learning algorithm's uncertainty over the true payoffs. We prove results of two types.", "histories": [["v1", "Mon, 23 May 2016 18:58:24 GMT  (511kb,D)", "http://arxiv.org/abs/1605.07139v1", null], ["v2", "Mon, 7 Nov 2016 15:49:05 GMT  (150kb,D)", "http://arxiv.org/abs/1605.07139v2", "A condensed version of this work appears in the 30th Annual Conference on Neural Information Processing Systems (NIPS), 2016"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["matthew joseph", "michael kearns", "jamie h morgenstern", "aaron roth"], "accepted": true, "id": "1605.07139"}, "pdf": {"name": "1605.07139.pdf", "metadata": {"source": "CRF", "title": "Fairness in Learning: Classic and Contextual Bandits\u2217", "authors": ["Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Aaron Roth"], "emails": ["majos@cis.upenn.edu.", "mkearns@cis.upenn.edu.", "jamiemor@cis.upenn.edu.", "aaroth@cis.upenn.edu."], "sections": [{"heading": null, "text": "First, in the important special case of the classic stochastic bandit problem (i.e., where there are no contexts), we provide a demonstrably fair algorithm based on chained confidence intervals and demonstrate a cumulative regret associated with a cubic dependence on the number of weapons. We also show that any fair algorithm must have such dependence. Combined with regret limits for non-fair standard algorithms such as UCB, this demonstrates a strong separation between fair and unfair learning that extends to the general context case. In the general context case, we demonstrate a close link between fairness and the KWIK learning model (Knows What It Knows): a KWIK algorithm for a class of functions can be transformed into a demonstrably fair contextual bandit algorithm, and vice versa, any fair contextual bandit algorithm can provide us with a KWIK learning algorithm that demonstrably enables us to connect a bandit to a contextual problem."}, {"heading": "1 Introduction 3", "text": "1.1 Fairness and learning................................................................................................................................................................."}, {"heading": "2 Preliminaries 6", "text": "2.1 Specialisation in classical stochastic bandits............................. 7"}, {"heading": "3 Fair Classic Stochastic Bandits: An Algorithm 8", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Fair Classic Stochastic Bandits: A Lower Bound 11", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 KWIK Learnability Implies Fair Bandit Learnability 15", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Fair Bandit Learnability Implies KWIK Learnability 18", "text": "6.1 An exponential separation between fair and unfair learning........ 20"}, {"heading": "A Missing Proofs for the Classic Stochastic Bandits Upper Bound 23", "text": "A.1 Missing derivative of R (T) for theorem 2........................... 24"}, {"heading": "B Missing Proofs for the Classic Stochastic Bandits Lower Bound 24", "text": "C Lack of Context Bandit Setting 26"}, {"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "1.1 Fairness and Learning", "text": "Our idea of individual fairness is very simple: it says that it is unfair to give preference to an individual (e.g. for a loan, a job, admission to college, etc.) when he is not as qualified as the other individual. This definition of fairness is appropriate for our attitude, since in contextual learning the quality of an arm is perfectly clear: its expected reward. We see different types of armament, even if there are no discriminatory intentions. [Podesta et al., 2014] notes that \"the increasing use of algorithms to make eligibility decisions needs to be carefully monitored for potentially discriminatory outcomes for disadvantaged groups, even without discriminatory intentions. additional research is needed in measuring negative outcomes due to the use of scores or algorithms to understand the impact of these tools in both the private and public sectors.\""}, {"heading": "1.2 Our Results", "text": "In fact, it is the case that most of them are able to abide by the rules which they have imposed on themselves, and that they are able to abide by the rules which they have imposed on themselves. (...) It is not the case that they abide by the rules. (...) It is not the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they do not abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) (...) () (...) () (() (()) ((()) ((())) ((())) (()) () ()) (()) (()) (())) (()) (()) ()) (() () () ()) () () () () () () () () () () ()) () () () () () ()) () () () () ()) () () () () () () () ()) () () () () ()) () () () () ()) () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() (() () () (() () (() () () () (() () (() (() (() () ((() () ((() (() () () (() () (() () () (((() () (((() () () (() () () ((((() () (((() (() (() () ((() ("}, {"heading": "1.3 Other Related Work", "text": "One line of work aims to find algorithms for group justice, otherwise known as equality of outcomes, or algorithms that avoid disparate effects (see e.g. Calders and Verwer [2010], Kamishima et al. [2011], Feldman et al. [2015], Fish et al.] and Adler et al. [2016] for a study of testing existing algorithms for disparate effects. While statistical parity is sometimes a desirable goal, it is sometimes required by law - as by Dwork et al."}, {"heading": "2 Preliminaries", "text": "(k): = {1,., k} and a class C of functions of the form f: X, 1). For each arm j there is a specific function fj, which is unknown to the learner. In rounds t = 1,.., T, an opponent discloses to the algorithm a context xtj for each arm6. An algorithm A then selects an arm and observes reward rtit for the arm he has chosen. We assume that r t j, o [rtj] = fj (x j], for some distribution dtj over [0, 1].Let's leave the sets of policy mapping contexts to distributions over arms Xk, and vice versa the optimal policy selects a distribution over arms as a function of the maximized reward of these arms."}, {"heading": "2.1 Specializing to Classic Stochastic Bandits", "text": "In sections 3 and 4 we examine the classical stochastic bandit problem, an important special case of the contextual bandit situation described above. Here, we specialize our spelling in this situation, in which there are no contexts. For each arm j [k] there is an unknown distribution Dj over [0, 1] with unknown mean \u00b5j. A learning algorithm A selects an arm, which it selects in round t, and considers the reward Dit for the arm it has chosen. Let i [k] be the arm with the highest expected reward: i [arg maxi] \u00b5i. The pseudo-regret of an algorithm A on D1,... Dk is now only: T \u00b7 ui. \u2212 It is 0 \u2264 t \u2264 T \u00b5it = Regret (T, D1,..)."}, {"heading": "3 Fair Classic Stochastic Bandits: An Algorithm", "text": "In this section, we describe a simple and intuitive modification of the standard UCB algorithm [Auer et al., 2002], called FairBandits, proving that it is fair, and analyzing its regret. The algorithm and its analysis highlight a key idea that is important for designing fair algorithms in this context: that of concatenating trust intervals. Intuitively, this is done by maintaining empirical estimates of the means of both arms, along with equal probability, until there is sufficient data to deduce from it, either that \u00b5j2 or vice versa. FairBandits do this by maintaining empirical estimates of the means of both arms, along with trust intervals. To be sure, the algorithm must play the arms with equal probability, while their trust intervals overlap."}, {"heading": "4 Fair Classic Stochastic Bandits: A Lower Bound", "text": "We show that regret for FairBandits has an optimal dependence on k: no fair algorithm has decreasing regret before T = (k3) rounds. All the missing evidence is in Appendix B. The main result of this section is therefore the following reward. There is a distribution P over k-arm instances of the stochastic multi-armed bandit problem, so any fair algorithm on P experiences constant per-round remorse for at least leastT = (k3 ln 1\u03b4) distributions. Despite the fact that regret is defined in a previous free manner, the proof for Theorem 3 goes beyond Bayesian reasoning. We construct a family of lower bound instances so that poor have denounced payouts from Bernoulli distributions B (\u00b5) for medium \u00b5. Thus, to specify a problem instance, it is sufficient to specify an average for each of the arms:"}, {"heading": "5 KWIK Learnability Implies Fair Bandit Learnability", "text": "In this section, we show that if a class of functions is learnable, then there is a fair algorithm for learning the same class of functions in the contextual bandit environment, with a regret related to the function class \"KWIK.\" Intuitively, KWIK learning ability has guaranteed a class of functions that we can learn the behavior of the function to a high degree of accuracy with a high degree of confidence. As the fairness limitations of an algorithm that most determines the behavior of the payout functions before the algorithm, this guarantee allows us to learn fairly without causing much additional regret. Formally, we prove the following polynomial relationships. Theorem 4: For an instance of the contextual multi-arm bandit problem in which fj-C is intended for all j functions."}, {"heading": "6 Fair Bandit Learnability Implies KWIK Learnability", "text": "In this section, we will show how to use a fair, non-regrettable context algorithm to construct a KWIK learning algorithm whose KWIK boundary has logarithmic dependence on the number of laps. [1] Any fair algorithm that achieves a low regret must be able to both find and exploit an optimal arm (since the algorithm has no regrets) -to find and exploit confidence about the reward of each arm. [1 \u2212 2] Theorem 6 is a fair understanding of the qualities of all arms (because the algorithm is fair). [2] Therefore, any fair algorithm will ultimately be narrow (1 \u2212 2) -to have confidence in the reward of each arm. [2] Theorem 6 is a fair algorithm for the qualities of all arms (because the algorithm is fair)."}, {"heading": "6.1 An Exponential Separation Between Fair and Unfair Learning", "text": "In this section, we first use a simple but unfair problem that we have tested between fair contextual bandit algorithms and KWIK learning algorithms to give a simple contextual bandit problem for which fairness imposes an exponential cost factor in its regret, in contrast to the case where the underlying function class is linear, for which we give fair contextual bandit algorithms with regret within a polynomial factor of their unrestricted counterparts. In this case, the context domain of the d-dimensional Boolean hypercube is: X = {0, 1} d - i.e. the context of each turn for each individual consists of d Boolean attributes. Our class of C functions is the class of Boolean conjunctions: C = {f | f (x) = xi1)."}, {"heading": "A Missing Proofs for the Classic Stochastic Bandits Upper Bound", "text": "We begin with the examination of Lemma 1, used in Section 3 to prove the fairness of FairBandits algorithms = k = k = k. Proof of Lemma 1. Select any arm and define the indicator variablesX1,.., Xni (t), where Xn takes the reward for pulling the arm i. From a Chernoff bound, for any arm i and round t and define the indicator variablesX1,., Xni (t), where Xn takes the reward for pulling the arm i. From a Chernoff bound, for any case, it is the case that P [p] 6 (p)."}, {"heading": "B Missing Proofs for the Classic Stochastic Bandits Lower Bound", "text": "All lemmas in this section are used in section 4 to prove the fair lower limit in theorem 3 = = ht = ht. The first, lemmas 4, lets us analyze the distributions on pay. Proof of lemmas 4: Let Ri represent the common distribution on rewards for both experiments: in both cases, the common distribution on rewards is identical, since the process that generates them is the same. We will use the notation m, d1,., \"in order to show some fixed realizations of the random variables \u00b5i, r 1 i,., r t i and \u00b5, i, r 1, r, i. In particular, it will be sufficient to show that thatP (\u00b5i, r1\u00ba), the equality of W [\u00b5i, r 1 i, i,."}, {"heading": "C Missing Proofs for the Contextual Bandit Setting", "text": "We begin by examining two results related to KWIKToFair. The first, Lemma 8, was used in Section 5 to prove that KWIKToFair is fair in all cases. (D) We point to either (a) or (b) as a failure of the learner Li. (D) For each of these violations, a number of questions are asked. (D) So, for most T contexts and most T histories, Li is queried for a fixed run of our algorithms. (D) Prefixes of Li's final history. (D) So, for most T-2 queries, Li has."}], "references": [{"title": "Auditing black-box models by obscuring features", "author": ["Philip Adler", "Casey Falk", "Sorelle A. Friedler", "Gabriel Rybeck", "Carlos Scheidegger", "Brandon Smith", "Suresh Venkatasubramanian"], "venue": "CoRR, abs/1602.07043,", "citeRegEx": "Adler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Adler et al\\.", "year": 2016}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel J. Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Graphical models for bandit problems", "author": ["Kareem Amin", "Michael Kearns", "Umar Syed"], "venue": "arXiv preprint arXiv:1202.3782,", "citeRegEx": "Amin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Amin et al\\.", "year": 2012}, {"title": "Large-scale bandit problems and kwik learning", "author": ["Kareem Amin", "Michael Kearns", "Moez Draief", "Jacob D Abernethy"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Amin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Amin et al\\.", "year": 2013}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Big data\u2019s disparate impact", "author": ["Solon Barocas", "Andrew D. Selbst"], "venue": "California Law Review,", "citeRegEx": "Barocas and Selbst.,? \\Q2016\\E", "shortCiteRegEx": "Barocas and Selbst.", "year": 2016}, {"title": "The new science of sentencing", "author": ["Anna Maria Barry-Jester", "Ben Casselman", "Dana Goldstein"], "venue": "The Marshall Project, August", "citeRegEx": "Barry.Jester et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barry.Jester et al\\.", "year": 2015}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["Alina Beygelzimer", "John Langford", "Lihong Li", "Lev Reyzin", "Robert E. Schapire"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2011}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Artificial intolerance. MIT Technology Review, March 28 2016", "author": ["Nanette Byrnes"], "venue": "URL https://www. technologyreview.com/s/600996/artificial-intolerance/. Retrieved 4/28/2016", "citeRegEx": "Byrnes.,? \\Q2016\\E", "shortCiteRegEx": "Byrnes.", "year": 2016}, {"title": "Three naive bayes approaches for discrimination-free classification", "author": ["Toon Calders", "Sicco Verwer"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Calders and Verwer.,? \\Q2010\\E", "shortCiteRegEx": "Calders and Verwer.", "year": 2010}, {"title": "Contextual bandits with linear payoff functions", "author": ["Wei Chu", "Lihong Li", "Lev Reyzin", "Robert E. Schapire"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Chu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2011}, {"title": "Regulating by robot: Administrative decision-making in the machine-learning era", "author": ["Cary Coglianese", "David Lehr"], "venue": "Georgetown Law Journal,", "citeRegEx": "Coglianese and Lehr.,? \\Q2016\\E", "shortCiteRegEx": "Coglianese and Lehr.", "year": 2016}, {"title": "Fairness through awareness", "author": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel"], "venue": "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,", "citeRegEx": "Dwork et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2012}, {"title": "Certifying and removing disparate impact", "author": ["Michael Feldman", "Sorelle A. Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "Feldman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2015}, {"title": "A confidence-based approach for balancing fairness and accuracy", "author": ["Benjamin Fish", "Jeremy Kun", "\u00c1d\u00e1m D Lelkes"], "venue": "SIAM International Symposium on Data Mining,", "citeRegEx": "Fish et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fish et al\\.", "year": 2016}, {"title": "Navigating the \u201ctrackless ocean\u201d: Fairness in big data research and decision making", "author": ["FTC Commisioner Julie Brill"], "venue": "Keynote Address at the Columbia", "citeRegEx": "Brill.,? \\Q2015\\E", "shortCiteRegEx": "Brill.", "year": 2015}, {"title": "Fairness-aware learning through regularization approach", "author": ["Toshihiro Kamishima", "Shotaro Akaho", "Jun Sakuma"], "venue": "In Data Mining Workshops (ICDMW),", "citeRegEx": "Kamishima et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kamishima et al\\.", "year": 2011}, {"title": "Sequential choice from several populations", "author": ["Michael N Katehakis", "Herbert Robbins"], "venue": "PROCEEDINGS-NATIONAL ACADEMY OF SCIENCES USA,", "citeRegEx": "Katehakis and Robbins.,? \\Q1995\\E", "shortCiteRegEx": "Katehakis and Robbins.", "year": 1995}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "A unifying framework for computational reinforcement learning theory", "author": ["Lihong Li"], "venue": "PhD thesis, Rutgers, The State University of New Jersey,", "citeRegEx": "Li.,? \\Q2009\\E", "shortCiteRegEx": "Li.", "year": 2009}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["Lihong Li", "Michael L Littman", "Thomas J Walsh", "Alexander L Strehl"], "venue": "Machine learning,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "k-nn as an implementation of situation testing for discrimination discovery and prevention", "author": ["Binh Thanh Luong", "Salvatore Ruggieri", "Franco Turini"], "venue": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Luong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2011}, {"title": "Can an algorithm hire better than a human", "author": ["Clair C Miller"], "venue": "The New York Times, June", "citeRegEx": "Miller.,? \\Q2015\\E", "shortCiteRegEx": "Miller.", "year": 2015}, {"title": "Big data: A report on algorithmic systems, opportunity, and civil rights", "author": ["Cecilia Munoz", "Megan Smith", "DJ Patil"], "venue": "Technical report, Executive Office of the President,", "citeRegEx": "Munoz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munoz et al\\.", "year": 2016}, {"title": "Big data: Seizing opportunities, protecting values. Technical report, Executive Office of the President", "author": ["John Podesta", "Penny Pritzker", "Ernest J. Moniz", "John Holdern", "Jeffrey Zients"], "venue": null, "citeRegEx": "Podesta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Podesta et al\\.", "year": 2014}, {"title": "Online linear regression and its application to model-based reinforcement", "author": ["Michael L Littman"], "venue": null, "citeRegEx": "Strehl and Littman.,? \\Q2016\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2016}, {"title": "Discrimination in online ad delivery", "author": ["Rich Zemel", "Yu Wu", "Kevin Swersky", "Toni Pitassi", "Cynthia Dwork"], "venue": "Neural Information Processing Systems, pages 1417\u20131424,", "citeRegEx": "Zemel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zemel et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 23, "context": "Automated techniques from statistics and machine learning are increasingly being used to make decisions that have important consequences on people\u2019s lives, including hiring [Miller, 2015], lending [Byrnes, 2016], policing [Rudin, 2013], and even criminal sentencing [Barry-Jester et al.", "startOffset": 173, "endOffset": 187}, {"referenceID": 9, "context": "Automated techniques from statistics and machine learning are increasingly being used to make decisions that have important consequences on people\u2019s lives, including hiring [Miller, 2015], lending [Byrnes, 2016], policing [Rudin, 2013], and even criminal sentencing [Barry-Jester et al.", "startOffset": 197, "endOffset": 211}, {"referenceID": 6, "context": "Automated techniques from statistics and machine learning are increasingly being used to make decisions that have important consequences on people\u2019s lives, including hiring [Miller, 2015], lending [Byrnes, 2016], policing [Rudin, 2013], and even criminal sentencing [Barry-Jester et al., 2015].", "startOffset": 266, "endOffset": 293}, {"referenceID": 5, "context": "These high stakes uses of machine learning have led to increasing concern in law and policy circles about the potential for (often opaque) machine learning techniques to be discriminatory or unfair [Coglianese and Lehr, 2016, Barocas and Selbst, 2016]. Moreover, these concerns are not merely hypothetical: Sweeney [2013] observed that contextual ads for public record services shown in response to Google searches for stereotypically African American names were more likely to contain text referring to arrest records, compared to comparable ads shown in response to searches for stereotypically Caucasian names, which showed more neutral text.", "startOffset": 226, "endOffset": 322}, {"referenceID": 25, "context": "1 For example, a 2014 White House report [Podesta et al., 2014] notes that \u201c[t]he increasing use of algorithms to make eligibility decisions must be carefully monitored for potential discriminatory outcomes for disadvantaged groups, even absent discriminatory intent.", "startOffset": 41, "endOffset": 63}, {"referenceID": 24, "context": "\u201d Along the same lines, a 2016 White House report [Munoz et al., 2016] observes that \u201c[a]s improvements in the uses of big data and machine learning continue, it will remain important not to place too much reliance on these new systems without questioning and continuously testing the inputs and mechanics behind them and the results they produce.", "startOffset": 50, "endOffset": 70}, {"referenceID": 4, "context": "Without a fairness constraint, it is known that it is possible to guarantee non-trivial regret to the optimal policy after only T = O(k) many rounds [Auer et al., 2002].", "startOffset": 149, "endOffset": 168}, {"referenceID": 4, "context": "Without a fairness constraint, it is known that it is possible to guarantee non-trivial regret to the optimal policy after only T = O(k) many rounds [Auer et al., 2002]. In Section 3, we give an algorithm that satisfies our fairness constraint and is able to guarantee non-trivial regret after T = O(k3) rounds. We then show in Section 4 that it is not possible to do better \u2013 any It is natural that different populations should have different underlying functions \u2013 for example, in a college admissions setting, the function mapping applications to college success probability might weight SAT scores less in a wealthy population that employs SAT tutors, and more in a working-class population that does not \u2013 see Dwork et al. [2012] for more discussion of this issue and Munoz et al.", "startOffset": 150, "endOffset": 735}, {"referenceID": 4, "context": "Without a fairness constraint, it is known that it is possible to guarantee non-trivial regret to the optimal policy after only T = O(k) many rounds [Auer et al., 2002]. In Section 3, we give an algorithm that satisfies our fairness constraint and is able to guarantee non-trivial regret after T = O(k3) rounds. We then show in Section 4 that it is not possible to do better \u2013 any It is natural that different populations should have different underlying functions \u2013 for example, in a college admissions setting, the function mapping applications to college success probability might weight SAT scores less in a wealthy population that employs SAT tutors, and more in a working-class population that does not \u2013 see Dwork et al. [2012] for more discussion of this issue and Munoz et al. [2016] for examples.", "startOffset": 150, "endOffset": 793}, {"referenceID": 21, "context": "We then move on to the general contextual bandit setting and prove a broad characterization result, relating fair contextual bandit learning to KWIK learning [Li et al., 2011].", "startOffset": 158, "endOffset": 175}, {"referenceID": 21, "context": "This general connection has immediate implications, because it allows us to import known results for KWIK learning [Li et al., 2011].", "startOffset": 115, "endOffset": 132}, {"referenceID": 7, "context": "Calders and Verwer [2010], Luong et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 7, "context": "Calders and Verwer [2010], Luong et al. [2011], Kamishima et al.", "startOffset": 0, "endOffset": 47}, {"referenceID": 7, "context": "Calders and Verwer [2010], Luong et al. [2011], Kamishima et al. [2011], Feldman et al.", "startOffset": 0, "endOffset": 72}, {"referenceID": 7, "context": "Calders and Verwer [2010], Luong et al. [2011], Kamishima et al. [2011], Feldman et al. [2015], Fish et al.", "startOffset": 0, "endOffset": 95}, {"referenceID": 7, "context": "Calders and Verwer [2010], Luong et al. [2011], Kamishima et al. [2011], Feldman et al. [2015], Fish et al. [2016] and Adler et al.", "startOffset": 0, "endOffset": 115}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact).", "startOffset": 11, "endOffset": 31}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems.", "startOffset": 11, "endOffset": 232}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness.", "startOffset": 11, "endOffset": 577}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d.", "startOffset": 11, "endOffset": 850}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer.", "startOffset": 11, "endOffset": 1423}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer. The most challenging aspect of this approach (as they acknowledge) is that it requires that some third party design a \u201cfair\u201d metric on individuals, which in a sense encodes much of the relevant challenge. The question of how to design such a metric was considered by Zemel et al. [2013], who study methods to learn representations that encode the data, while obscuring protected attributes.", "startOffset": 11, "endOffset": 1955}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer. The most challenging aspect of this approach (as they acknowledge) is that it requires that some third party design a \u201cfair\u201d metric on individuals, which in a sense encodes much of the relevant challenge. The question of how to design such a metric was considered by Zemel et al. [2013], who study methods to learn representations that encode the data, while obscuring protected attributes. Our fairness constraint, conversely, is entirely aligned with the goal of the algorithm designer in that it is satisfied by the optimal policy; nevertheless, it affects the space of feasible learning algorithms, because it interferes with learning an optimal policy, which depends on the unknown reward functions. At a technical level, our work is related to Amin et al. [2012] and Amin et al.", "startOffset": 11, "endOffset": 2437}, {"referenceID": 0, "context": "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal \u2013 indeed, it is sometimes required by law \u2013 as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer. The most challenging aspect of this approach (as they acknowledge) is that it requires that some third party design a \u201cfair\u201d metric on individuals, which in a sense encodes much of the relevant challenge. The question of how to design such a metric was considered by Zemel et al. [2013], who study methods to learn representations that encode the data, while obscuring protected attributes. Our fairness constraint, conversely, is entirely aligned with the goal of the algorithm designer in that it is satisfied by the optimal policy; nevertheless, it affects the space of feasible learning algorithms, because it interferes with learning an optimal policy, which depends on the unknown reward functions. At a technical level, our work is related to Amin et al. [2012] and Amin et al. [2013], which also relate KWIK learning to bandit learning in a different context, unrelated to fairness (when the arm space is very large).", "startOffset": 11, "endOffset": 2460}, {"referenceID": 13, "context": "This relaxation is a special case of Dwork et al. [2012]\u2019s proposed family of definitions, which require that \u201csimilar individuals be treated similarly\u201d.", "startOffset": 37, "endOffset": 57}, {"referenceID": 4, "context": "In this section, we describe a simple and intuitive modification of the standard UCB algorithm [Auer et al., 2002], called FairBandits, prove that it is fair, and analyze its regret bound.", "startOffset": 95, "endOffset": 114}, {"referenceID": 4, "context": "This is reflected in its regret bound, which is only non-trivial after T k3, whereas the UCB algorithm [Auer et al., 2002] achieves non-trivial regret after T = O(k) rounds.", "startOffset": 103, "endOffset": 122}, {"referenceID": 8, "context": "However, it is known that \u03a9 (\u221a kT ) regret is necessary even in the unrestricted setting (without fairness) if one does not make data-specific assumptions on an instance [Bubeck and Cesa-Bianchi, 2012] (e.", "startOffset": 170, "endOffset": 201}, {"referenceID": 20, "context": "This relies upon a known lower bound for KWIK learning conjunctions [Li, 2009]:", "startOffset": 68, "endOffset": 78}], "year": 2017, "abstractText": "We introduce the study of fairness in multi-armed bandit problems. Our fairness definition can be interpreted as demanding that given a pool of applicants (say, for college admission or mortgages), a worse applicant is never favored over a better one, despite a learning algorithm\u2019s uncertainty over the true payoffs. We prove results of two types: First, in the important special case of the classic stochastic bandits problem (i.e. in which there are no contexts), we provide a provably fair algorithm based on chained confidence intervals, and prove a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence. When combined with regret bounds for standard non-fair algorithms such as UCB, this proves a strong separation between fair and unfair learning, which extends to the general contextual case. In the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm, and conversely any fair contextual bandit algorithm can be transformed into a KWIK learning algorithm. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms. \u2217Department of Computer and Information Sciences, University of Pennsylvania. {majos,mkearns,jamiemor,aaroth}@cis.upenn.edu. AR is supported in part by an NSF CAREER award, a Sloan Foundation Fellowship, and a Google Faculty Research Award. 1 ar X iv :1 60 5. 07 13 9v 1 [ cs .L G ] 2 3 M ay 2 01 6", "creator": "LaTeX with hyperref package"}}}