{"id": "1605.07144", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Actively Learning Hemimetrics with Applications to Eliciting User Preferences", "abstract": "Motivated by an application of eliciting users' preferences, we investigate the problem of learning hemimetrics, i.e., pairwise distances among a set of $n$ items that satisfy triangle inequalities and non-negativity constraints. In our application, the (asymmetric) distances quantify private costs a user incurs when substituting one item by another. We aim to learn these distances (costs) by asking the users whether they are willing to switch from one item to another for a given incentive offer. Without exploiting structural constraints of the hemimetric polytope, learning the distances between each pair of items requires $\\Theta(n^2)$ queries. We propose an active learning algorithm that substantially reduces this sample complexity by exploiting the structural constraints on the version space of hemimetrics. Our proposed algorithm achieves provably-optimal sample complexity for various instances of the task. For example, when the items are embedded into $K$ tight clusters, the sample complexity of our algorithm reduces to $O(nK)$. Extensive experiments on a restaurant recommendation data set support the conclusions of our theoretical analysis.", "histories": [["v1", "Mon, 23 May 2016 19:21:35 GMT  (1280kb,D)", "https://arxiv.org/abs/1605.07144v1", "Extended version of ICML'16 paper"], ["v2", "Fri, 27 May 2016 17:45:26 GMT  (2585kb,D)", "http://arxiv.org/abs/1605.07144v2", "Extended version of ICML'16 paper"]], "COMMENTS": "Extended version of ICML'16 paper", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["adish singla", "sebastian tschiatschek", "andreas krause 0001"], "accepted": true, "id": "1605.07144"}, "pdf": {"name": "1605.07144.pdf", "metadata": {"source": "META", "title": "Actively Learning Hemimetrics  with Applications to Eliciting User Preferences", "authors": ["Adish Singla", "Sebastian Tschiatschek", "Andreas Krause"], "emails": ["ADISH.SINGLA@INF.ETHZ.CH", "SEBASTIAN.TSCHIATSCHEK@INF.ETHZ.CH", "KRAUSEA@ETHZ.CH"], "sections": [{"heading": "1 Introduction", "text": "The question that arises in this context is whether this is a form in which people are able to put themselves and themselves in the center, or whether it is a form in which people are able to put themselves and themselves in the role, in which they are able to slip into the role of the individual, to slip into the role of the individual, to slip into the role of the individual, to slip into the role of the individual, to slip into the role of the individual, to slip into the role of the individual, to slip into the role of the individual, to slip into the role of the individual, to slip into the role of the individual, to slip into the role of the individual, to slip into the role of the individual, to slip into the role of the individual, to slip into the role of the single, to slip into the role of the single, to slip into the role of the single, to slip into the role of the role of the single, to slip into the role of the role of the single, to slip into the role of the role of the single, to slip into the role of the role of the single, to slip into the role of the role of the single, to slip into the role of the role of the single, to slip into the role of the role of the single, to slip into the role of the role of the role of the single, to slip into the role of the role of the single, to slip into the role of the role of the role of the single, to slip into the role of the role of the single, to slip into the role of the role of the role of the single, to slip into the role of the role of the role of the single, to slip into the role of the role of the role of the single, to slip into the role of the role of the"}, {"heading": "1.1 Our Approach and Main Contributions", "text": "A naive approach to this problem is to learn each of the n2 distances independently of each other. However, the main contributions of this paper are the following: Novel Metric Learning Framework. We propose a new learning problem within the framework of metric learning, motivated by the application to find out the preferences of the users. The main characteristics of our attitude are: (i) the specific modality of user queries (with natural motivation from economics) and (ii) the asymmetry of the learned distances. Exploitation of structural constraints. We develop a novel active learning algorithm LEARNHM, which significantly reduces the sample complexity by taking advantage of the structural limitations of the hemimetric polyp. We offer tight theoretical guarantees for the sample complexity of the proposed algorithm for several natural distances, which are not strictly necessary by reflecting the practical settings."}, {"heading": "2 Problem Statement", "text": "We now formalize the problem addressed in this work. Items A. There are n items (or types of items) designated by the set A = {1, 2,.., n}. For example, in a restaurant a recommendation system like Yelp, A could consist of types of restaurants characterized by high-level attributes such as cuisine, locality, ratings and so on. (1) HemimetricD is the set of limited hemimetrics, i.e., matricesD, characterized by high-level attributes such as cuisine, locality, ratings and so on. (1) Di, j \u2265 0, i [n], j \u2264 r, i [n], and (3) Di, j \u2264 r, i) Di, j \u2264 Di, Dk + Dk, j, i, j, k [n], where [n], and mm."}, {"heading": "3 Warmup: Overview of our Approach", "text": "We now present the high-level ideas that underlie our approach."}, {"heading": "3.1 Independent Learning: INDGREEDY", "text": "One way to tackle our learning problem is to learn each of the n2 pairs of distances independently of each other. Let's tackle a specific item pair (i, j) 2. Given the query modality considered in our framework, the most effective way to learn the distance is to perform a binary search across the range [0, r]. Formally, in the iteration t = 0, we initiate a lower limit of the E question i, j \u2212 i, j = 0 and an upper limit to U ti, j = r. In each t > 0, we choose a value c t = 12 (L t \u2212 1, j \u2212 1) and in the iteration t (I \u2212 1, j \u2212 2)."}, {"heading": "3.2 Exploiting Structural Constraints: LEARNHM", "text": "Our algorithm is based on three functions LU-PROJ, QCLIQUE and GETUSERRESPONSE. An overall description of these functions is given as follows: LU-PROJ reduces the search space of hemimetrics by determining the lower and upper limits of L-t, U-t to Lt, U-t. Details are given in Section 4. QCLIQUE is the query policy that determines the next query xt = (it, jt, ct) at iteration t taking into account the current state of the learning process according to Lt \u2212 1 and U t \u2212 1. Details are given in Section 5. GETUSERRESPONSE provides the yt label for query xt. In the deterministic noise-free setting, this label is determined by Equation 5. We are also developing a robust noise-tolerant variant of GETUSERRESPONSE for those in chastic setting A."}, {"heading": "4 LU-PROJ: Updating Bounds", "text": "We now present the details of our function LU-PROJ. The proof for theorem 1 is given in appendix F and the proof for theorem 2 is reduced to INDGREEDY in appendix G.2Algorithm 1 if QCLIQUE is replaced by QGREEDY and LU-PROJ (L, U, T). Our algorithm: LEARNHM 1: Input: SetA of n items, value range r, error parameter (, T) 2: Output: hemimetric D, 3: Initialization: iteration t = 0; labeled dataZt = \"lower limits: Lti, j = 0, j [n] upper limits: U ti, j = r, error parameter (, T) 2: Output: hemimetric D, 3: Initialization: iteration t = 0; labeled dataZt =\" lower limits: Lti, j = 0, j [n] upper limits: U ti, j = 0 [je] 2: Output: hemimetric D, 3: initialization: hemimetric D, 3: initialization: iteration t = 0; labeled dataZt = \"lower limits: Lti, j = 0, j [n] upper limits: U, then Z: 1, Z: 1, then Z: [jj: je] upper limits (jj: jxt, j: 1, j: 1, j: jxt [j: j: [t], n: 1, then Z: 1 upper limits: 1, j: 1, n: 1]"}, {"heading": "4.1 Valid Bounds", "text": "Let us start with the definition of the minimum conditions for the lower and upper limits returned by LU-PROJ that are valid with regard to the version space. First, we formally define the version space for our setting. In algorithm 1, the labeled data is given in the iteration t by Zt = {z1,.., zt}, where zl = (xl, yl) stands for l [t]. Then, in the iteration t, the version space is defined as Dt: = {D, D,... l = yl}, (9) where D (xl) = 1 (cl, yl) = 1 (cl, jl); here, 1 (\u00b7) stands for the indicator function. That is, Dt D is the set of half metries in the iteration t that are consistent with the designated DataZt. Also, for the given lower limits L and upper limits U, we define the set of half metrics that meet these limits as L, U, Li, D and D (L, Li D)."}, {"heading": "4.2 Updating Bounds via Projection", "text": "We formalize the problem of obtaining the limits Lt, U t as a solution to the following optimization problem: min U, L-U-L-1 (P1) s.t. D (L, U) Dt, where the initial \"1 standard of a matrix\" is defined as follows: min U, L-U-L-1 = \u2211 i, j-Mi, j |. The intuitive idea behind this problem is to reduce the gap between upper and lower limits as much as possible and at the same time ensure that the resulting limits are valid. It turns out that problem P1 can be solved in a two-step process by solving the following two problems: U-T = arg min U-D s.t. U-U-T-1 (P2) L-T = arg min L-L (U-T) s.t."}, {"heading": "4.3 Function LU-PROJ: Tightening Bounds", "text": "We now present an efficient solver for the optimization problem P1, which is solved by the function LU-PROJ in Algo-1: Input: L-t, U t; Output: Lt 2: Initialize: Lt = L-t 3: for k = 1 to n do 4: for i = 1 to n do 5: for j = 1 to n do 6: Lti, j = max (Lti, j, L t i, k \u2212 U tj, k, Ltk, j \u2212 U tk, i) 7: Return: Ltrithm 2. The algorithm is called with the inputs L-t, U-t - its optimality is ensured by the following theorem.Theorem2: The lower and upper limit Lt, U-tk, i) 7: Return: Ltrithm 2. The algorithm is called with the inputs L-t, U-t - its optimality is guaranteed by the following theorem.Theorem2: The lower and upper limit Lt, U-tk, T-tk, T-t: T, T-m, T-t)."}, {"heading": "4.4 Geometric Interpretation ofL\u2217t andU\u2217t", "text": "s define \u03c00 as the set of inequalities {Di, i = 0, 0 \u2264 Di, j \u2264 r, Di, j \u2264 Di, k + Dk, j \u0441i, j, k [n]. Thus, the subset of Rn2 described by \u03c00 corresponds to the set of bound hemimetries. With iteration t we get a new designation for the datapoint zt = ((it, jt, ct), yt) and update the set of inequalities as follows: \u03c0t = {t \u2212 1 \u0445 {ct Dit, jt} if yt = 1, \u0432t \u2212 1 \u0432lt; Dit, jt} if yt = 0.Now we look at the set of inequalities defined by \u03c0t in Rn2. Furthermore, we look at the hypercube in Rn2 defined by some upper and upper limit < Dit, jt} if yt = 0.Now we look at the set of inequalities defined by \u03c0t in Rn2."}, {"heading": "5 QCLIQUE: Proposing Queries", "text": "We first show the limits of a greedy short-sighted policy QGREEDY for proposing queries and then design our query policy QCLIQUE that overcomes these limitations."}, {"heading": "5.1 Myopic Policy QGREEDY", "text": "The policy QGREEDY is inspired by the idea of reducing the gap (U t \u2212 1i, j \u2212 L t \u2212 1 i, j) of pairs (it, jt) with maximum uncertainty. As already mentioned, this policy can be considered short-sighted (greedy) with regard to minimizing the goal \"D\" -D. \"However, this policy may prove to be suboptimal with regard to exploiting the structural limitations of hemimetry. In particular, let us consider a simple example with n items belonging to a single narrow cluster, namely\" i, \"\" j \"[n]:\" i, \"j\" = 0. In view of the 2n distances \"I,\" \"j\", \"\" j, \"\" \"1\" for \"j\" n, \"due to the density of triangular inequalities, it is possible to conclude all other distances. In this example, INDGREEDY becomes\" t \"the largest upper limit maxi\" U \"\" \"J\" in each iteration."}, {"heading": "5.2 Non-myopic Policy QCLIQUE", "text": "It is not the first time that the EU Commission and the EU Commission have set out to find a solution. (...) It is the first time that the EU Commission is in a position to find a solution. (...) It is the second time that the EU Commission is in a position to find a solution. (...) It is the second time that the EU Commission is in a position to find a solution. (...) It is the third time that the EU Commission is in a position to find a common solution. (...) It is the first time that the EU Commission is in a position to find a common solution. (...) It is the second time that the EU Commission is in a position to find a common solution. (...) It is the second time that the EU Commission is in a position to find a common solution. (...) It is the second time that the EU Commission, the EU Commission, the EU Commission, the EU and the EU. (...) It is the EU Commission, the EU Commission, the EU and the EU. (...) It is the third time that the EU Commission is in a position to find a common solution. (...) It is the second time that the EU Commission, the EU Commission, the Commission, the Commission, the EU and the EU Commission, the Commission, the Commission, the Commission, the Commission and the Commission are in a common solution. (...)"}, {"heading": "6 Performance Analysis", "text": "In this section, we analyze the sample complexity and runtime of our proposed algorithm LEARNHM. All the evidence can be found in Appendix D."}, {"heading": "6.1 Sample Complexity", "text": "In real applications where the diametric complexity of LEARNHM (r) and LEARNHM (r) is applicable in the real world, we could make the complexity of LEARNHM (r) and LEARNHM (r) applicable in the real world. (r) In real applications where the real complexity of LEARNHM (r) and the real complexity of LEARNHM (r) is the real complexity of LEARNHM (r), the real complexity of LEARNHM (r) is the real complexity of LEARNHM (r) of LEARNHM (r) and the real complexity of LEARNHM (r) is the real complexity of LEARNHM (r) of LEARNHM (r)."}, {"heading": "6.2 Runtime Analysis and Speeding Up LEARNHM", "text": "The LEARNHM algorithm calls LU-PROJ after each query, resulting in a total runtime that is unaffordable for most realistic problem cases. The key idea for accelerating LEARNHM is that we can select the constraints that should be exploited rather than exploiting all the constraints after each query (line 6 in U-PROJ & L-PROJ). If the constraints that are to be exploited are carefully selected, we can still take reasonable advantage of tightening the lower and upper limits rather than taking into account all the constraints after each query (line 6 in U-PROJ & L-PROJ)."}, {"heading": "7 Experimental Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Benchmarks", "text": "We compare the performance of the accelerated LEARN with the baseline INDGREEDY. We also compare with a second baseline INDGREEDY-SIT (INDGREEDY with page information from triplet comparisons), which uses a low-dimensional embedding of items as a pre-processing step. Following the work of Jamieson & Nowak (2011a), a can3The-O-D notation is used to exclude logarithmic factors from the factors present, i.e. for a triplet (i, j, k) such a query yields 1 (D \u0445 i, j \u0445i, k)."}, {"heading": "7.2 Experimental Setup", "text": "Yelp Dataset. We use the recently proposed Yelp Dataset Challenge (round 7) data for our experiments. This data contains information about 77K companies in 10 cities around the world. We examined companies that belong to the category of restaurants and are located in the city of Pittsburgh, PA. Specifically, we extracted information for all 290 restaurants that offer food from the Mexican (50), Thai (26), Chinese (53), Mediterranean (75), Italian (86). For each of these restaurants, we also collected the rating number and coordinates (length and width). We decreed the rating in High (popular, 166 restaurants) when there were more than 25 reviews and in Low (unpopular, 124 restaurants) otherwise. The collected data is visualized in Figure 3. User Preference Models. We simulate user preference models from this data by creating the underlying hemimetricD models as follows."}, {"heading": "8 Related Work", "text": "8.1 Metric Learning & Low-dimensional Embeddings Learning Distances to capture ideas of similarity or dissimilarity plays a central role in many machine learning applications. We refer the reader to the detailed surveys by Yang & Jin (2006); van der Maaten et al. (2009); Bellet et al. (2013). Some of the most important distinctions of our formulation from existing research are described below: Supervised metric learning. In their groundbreaking work, Xing et al. (2002) introduced the supervised metric learning framework for learning Mahalanobis distance functions via a convex formulation. Contrary to our setting, they assume that the algorithm has access to the characteristic space of the input data. Furthermore, this framework and its variants are limited to the restoration of symmetric distance functions."}, {"heading": "8.2 Exploiting Structural Constraints", "text": "Our approach to exploiting the structural limitations of hemimetric polytopy is partly inspired by Elkan (2003), which accelerates the problem of projecting a nonmetric matrix onto a metric matrix by maintaining boundaries at distances, efficiently reducing the number of distance calculations. Brickell et al. (2008) investigate the problem of projecting a nonmetric matrix onto a metric matrix and consider a specific class of pure decrease projections. Our approach to updating upper boundaries by means of mere decrease projections is similar in spirit, but the greatest technical difficulties arise in maintaining and updating the lower boundaries for which we are developing new techniques. The active learning approach advocated by Jamieson & Nowak (2011 1a) utilizes the geometry of the embedding space to minimize the complexity of the sample through triplet-based queries."}, {"heading": "8.3 Learning User Preferences", "text": "Another relevant area of research is the identification of user preferences. Specifically, we try to determine a user's private cost of switching from their default choice for article i to article j. Such preferences can be used in marketing applications, for example, to induce them to change their choices (Kamenica & Gentzkow, 2009). Singla et al. (2015) looked at similar preferences in the context of balancing a bike-sharing system by encouraging users to visit alternative stations to pick up or unload bicycles. Abernethy et al. (2015) looked at the use of users \"purchase data and quantified the prices that should be offered to them. A major difference in our approach is that we are interested in learning preferences between articles together, i.e. n2 learning problems together."}, {"heading": "8.4 Active Learning", "text": "Our problem formulation shares the goal of reducing sample complexity with other examples of active learning (Settles, 2012).Our goal of specifically exploiting the structural limitations of the hemimetric polytopic lies in the line of research in the field of structured active learning, where the hypotheses or output label space have an inherent structure that can be exploited, e.g. the structure of the partial linguistic marker of the sentence (Roth & Small, 2006)."}, {"heading": "9 Conclusions", "text": "The two key techniques used in the construction of our LEARNHM algorithm are novel projection techniques to streamline the lower and upper limits of the solution space and a non-myopic (non-greedy) interrogation policy. Our algorithm can easily be applied to the online setting, allowing the hemimetric solution to expand over time. We provided a thorough analysis of the sample complexity and runtime of our algorithm. Our experiments with Yelp data showed significant improvements over the basic algorithms in line with our theoretical findings. This research is supported in part by SNF funding 200020 159557 and the Nano-Tera.ch program under the Opensense II project."}, {"heading": "A Robust Noise-tolerant Algorithms", "text": "In this section we present our noise model and the associated stochastic acceptance function. In addition, we present a robust noise tolerant variant of GETUSERRESPONSE for this stochastic adjustment.A.1 Noise model Our noise model is parameterized by the variance matrix. The acceptance function P (Y (((((i, j, c)) = 1)) is parameterized by the CDF of a normal distribution N (D)."}, {"heading": "B Speeding up LEARNHM", "text": "The key idea for accelerating LEARNHM is that we can select the constraints that should be exploited rather than exploiting all constraints (line 6 in U-PROJ & L-PROJ). (line 6 in U-PROJ). (line 6 in U-PROJ). (line 6 in U-PROJ). (line 6 in U-PROJ). (line 6 in U-PROJ). (line 6 in U-PROJ). (line 6 in U-PROJ). (line 6 in U-PROJ). (line 6 in U-PROJ). (line 6 in U-PROJ.). (line 6 in U-PROJ.). (line 6 in U-PROJ.). (line 6 in U-PROJ.). (line 6 in U-PROJ. (line 6 in U-PROJ.). (line 6 in U-PROJ.). (line 6 in U-PROJ.). (line 6 in U-PROJ.)."}, {"heading": "C Experimental Results for Stochastic Settings", "text": "We perform two sets of experiments for stochastic setting. In stochastic setting with limited noise rate (i.e., the algorithm knows that the noise rate is limited by 0.5) LEARNHM uses the function GETUSERRESPONSE for limited noise in algorithm 6. In our noise model (i.e., the algorithm does not accept assumptions about the limited noise rate) LEARNHM uses the function GETUSERRESPONSE for unlimited noise in algorithm 7. Figures 4 (a-c) show results for stochastic setting with limited noise rate defined by (B) parameters for the user. Then we present the results for our noise model in Figure 4 (d-f) parameterized by \u03c3. We used the same value of variance for all pairs, i.e. we used Figure i, j [n]: \u03c3i, j = Manfred."}, {"heading": "D Sample Complexity Analysis", "text": "To do this, politics must learn all the distances between a and all items in C. During this learning process, we will say that LEARNHM / QCLIQUE has already learned all the distances between a and items C and C and has now learned the distance between a and b in C and C. That is, LEARNHM / QCLIQUE grows C \"to C,\" and then we will consider the following cases: C1 b is the first item in its cluster to be added to C. C2 Another item e from the same cluster is already part of C. \"C2.1 b belongs to the same cluster as a C2.2 b.\""}, {"heading": "F Proof of Theorem 1", "text": "In this section we provide the proof for theorems 1. (D) D (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n n (D) n n (D) n (D) n (D) n (D) n (D) n (D) n n (D) n n (D) n n (D) n n n n n n (D) n n n n (D) n n n n (D) n n (D n (D n (D n (D n (D n (D n (D n (D n (D n (D n (D n (D n (D n (D n) n (D n (D n (D n (D n (D n n (D n (D n n n n n (D n n n n (D n n n n (D n n n n n n (D n n n n n (D n n n n (D n n n n n (D n n n n n (D n n n n (D n D n n n (D n D n n n n n n n n (D n D n n n n n n n n n n (D n n n n n D n n n n n n n n n (D n n n n n (D n n n n n n n n n D n n n n n n n n n n n n n n n (D n n n n n n n n n n n n n n n n n n n (D n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n D n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n D n n n n n n n n n n n n n n n n n n n n n n n n n n n n"}, {"heading": "G Proof of Theorem 2", "text": "The proof of theorem 2: \"k.\" \"k.\" \"k.\" \"k.\" \"b.\" \"b.\" \"b.\" \"b.\" \"b.\" \"b.\" \"b.\" \"b.\" \"b.\" \"b.\" \"b.\" \"b.\" \"b.\" \"b.\" \"b.\" \"b.\" \"\" b. \"\" \"b.\" \"\" b. \"\" b. \"\" b. \"\" b. \"\" \"b.\" \"c.\" \"b.\" \"c.\" \".\" \"b.\". \"\" c. \"\" \"c.\" \"\". \"\" b. \"\" \".\" \"b.\" \"\" c. \"\" \"c.\" \"\" c. \"\" c. \"\" \"c.\". \"\" c."}], "references": [{"title": "Low-cost learning via active data procurement", "author": ["J. Abernethy", "Y. Chen", "C. Ho", "B. Waggoner"], "venue": "In Proceedings of the Sixteenth ACM Conference on Economics and Computation,", "citeRegEx": "Abernethy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2015}, {"title": "Multiview triplet embedding: Learning attributes in multiple maps", "author": ["E. Amid", "A. Ukkonen"], "venue": "In ICML, pp", "citeRegEx": "Amid and Ukkonen,? \\Q2015\\E", "shortCiteRegEx": "Amid and Ukkonen", "year": 2015}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "arXiv preprint arXiv:1306.6709,", "citeRegEx": "Bellet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellet et al\\.", "year": 2013}, {"title": "Learning what\u2019s going on: Reconstructing preferences and priorities from opaque transactions", "author": ["A. Blum", "Y. Mansour", "J. Morgenstern"], "venue": "In Proceedings of the Sixteenth ACM Conference on Economics and Computation,", "citeRegEx": "Blum et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2015}, {"title": "The metric nearness problem", "author": ["J. Brickell", "I.S. Dhillon", "S. Sra", "J.A. Tropp"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Brickell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Brickell et al\\.", "year": 2008}, {"title": "Upper and lower error bounds for active learning", "author": ["Castro", "Rui M", "Nowak", "Robert D"], "venue": "In Allerton,", "citeRegEx": "Castro et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Castro et al\\.", "year": 2006}, {"title": "Learning user preferences for sets of objects", "author": ["M. Desjardins", "E. Eaton", "K.L. Wagstaff"], "venue": "In ICML, pp", "citeRegEx": "Desjardins et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2006}, {"title": "Using the triangle inequality to accelerate k-means", "author": ["C. Elkan"], "venue": "In ICML,", "citeRegEx": "Elkan,? \\Q2003\\E", "shortCiteRegEx": "Elkan", "year": 2003}, {"title": "Learning local invariant mahalanobis distances", "author": ["E. Fetaya", "S. Ullman"], "venue": "In ICML,", "citeRegEx": "Fetaya and Ullman,? \\Q2015\\E", "shortCiteRegEx": "Fetaya and Ullman", "year": 2015}, {"title": "Algorithm 97: shortest path", "author": ["Floyd", "Robert W"], "venue": "Communications of the ACM,", "citeRegEx": "Floyd and W.,? \\Q1962\\E", "shortCiteRegEx": "Floyd and W.", "year": 1962}, {"title": "Manifoldranking based image retrieval", "author": ["J. He", "M. Li", "H. Zhang", "H. Tong", "C. Zhang"], "venue": "In Proceedings of the 12th annual ACM International Conference on Multimedia,", "citeRegEx": "He et al\\.,? \\Q2004\\E", "shortCiteRegEx": "He et al\\.", "year": 2004}, {"title": "At what quality and what price?: Eliciting buyer preferences as a market design problem", "author": ["J.J. Horton", "R. Johari"], "venue": "In Proceedings of the Sixteenth ACM Conference on Economics and Computation,", "citeRegEx": "Horton and Johari,? \\Q2015\\E", "shortCiteRegEx": "Horton and Johari", "year": 2015}, {"title": "Logeuclidean metric learning on symmetric positive definite manifold with application to image set classification", "author": ["Z. Huang", "R. Wang", "S. Shan", "X. Li", "X. Chen"], "venue": "In ICML,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Low-dimensional embedding using adaptively selected ordinal data", "author": ["K.G. Jamieson", "R.D. Nowak"], "venue": "In Allerton,", "citeRegEx": "Jamieson and Nowak,? \\Q2011\\E", "shortCiteRegEx": "Jamieson and Nowak", "year": 2011}, {"title": "Active ranking using pairwise comparisons", "author": ["Jamieson", "Kevin G", "Nowak", "Robert"], "venue": "In NIPS, pp", "citeRegEx": "Jamieson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2011}, {"title": "Active learning in the non-realizable case", "author": ["K\u00e4\u00e4ri\u00e4inen", "Matti"], "venue": "In ALT, pp", "citeRegEx": "K\u00e4\u00e4ri\u00e4inen and Matti.,? \\Q2006\\E", "shortCiteRegEx": "K\u00e4\u00e4ri\u00e4inen and Matti.", "year": 2006}, {"title": "Bayesian persuasion", "author": ["E. Kamenica", "M. Gentzkow"], "venue": "Technical report, National Bureau of Economic Research,", "citeRegEx": "Kamenica and Gentzkow,? \\Q2009\\E", "shortCiteRegEx": "Kamenica and Gentzkow", "year": 2009}, {"title": "Efficient learning of mahalanobis metrics for ranking", "author": ["D. Lim", "G. Lanckriet"], "venue": "In ICML, pp. 1980\u20131988,", "citeRegEx": "Lim and Lanckriet,? \\Q2014\\E", "shortCiteRegEx": "Lim and Lanckriet", "year": 2014}, {"title": "Similarity learning for high-dimensional sparse data", "author": ["Liu", "Kuan", "Bellet", "Aur\u00e9lien", "Sha", "Fei"], "venue": "In AISTATS, pp", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Margin-based active learning for structured output spaces", "author": ["Roth", "Dan", "Small", "Kevin"], "venue": "In ECML,", "citeRegEx": "Roth et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2006}, {"title": "Active learning", "author": ["B. Settles"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning,", "citeRegEx": "Settles,? \\Q2012\\E", "shortCiteRegEx": "Settles", "year": 2012}, {"title": "Truthful incentives in crowdsourcing tasks using regret minimization mechanisms", "author": ["A. Singla", "A. Krause"], "venue": "In WWW, pp", "citeRegEx": "Singla and Krause,? \\Q2013\\E", "shortCiteRegEx": "Singla and Krause", "year": 2013}, {"title": "Incentivizing users for balancing bike sharing systems", "author": ["A. Singla", "M. Santoni", "G. Bart\u00f3k", "P. Mukerji", "M. Meenen", "A. Krause"], "venue": "In AAAI,", "citeRegEx": "Singla et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Singla et al\\.", "year": 2015}, {"title": "Adaptively learning the crowd kernel", "author": ["O. Tamuz", "C. Liu", "S. Belongie", "O. Shamir", "A.T. Kalai"], "venue": "In ICML,", "citeRegEx": "Tamuz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tamuz et al\\.", "year": 2011}, {"title": "Dimensionality reduction: A comparative review", "author": ["van der Maaten", "Laurens JP", "Postma", "Eric O", "van den Herik", "H Jaap"], "venue": null, "citeRegEx": "Maaten et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2009}, {"title": "Active learning and dynamic pricing policies", "author": ["M. V\u00e1zquez-Gallo", "M. Est\u00e9vez", "S. Egido"], "venue": "American Journal of Operations Research,", "citeRegEx": "V\u00e1zquez.Gallo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "V\u00e1zquez.Gallo et al\\.", "year": 2014}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "M.I. Jordan", "S. Russell", "A.Y. Ng"], "venue": "In NIPS, pp", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Distance metric learning: A comprehensive survey", "author": ["Yang", "Liu", "Jin", "Rong"], "venue": "Michigan State Universiy,", "citeRegEx": "Yang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 26, "context": "In machine learning algorithms, the distances serve as a notion of similarity (or dissimilarity) between data points and are important for various tasks such as clustering (Xing et al., 2002), object ranking (Lim & Lanckriet, 2014), image retrieval / classification (He et al.", "startOffset": 172, "endOffset": 191}, {"referenceID": 10, "context": ", 2002), object ranking (Lim & Lanckriet, 2014), image retrieval / classification (He et al., 2004; Huang et al., 2015), etc.", "startOffset": 82, "endOffset": 119}, {"referenceID": 12, "context": ", 2002), object ranking (Lim & Lanckriet, 2014), image retrieval / classification (He et al., 2004; Huang et al., 2015), etc.", "startOffset": 82, "endOffset": 119}, {"referenceID": 6, "context": ", from a catalogue of products) to improve product recommendation and dynamic pricing of goods (Desjardins et al., 2006; Horton & Johari, 2015; Blum et al., 2015; V\u00e1zquez-Gallo et al., 2014).", "startOffset": 95, "endOffset": 190}, {"referenceID": 3, "context": ", from a catalogue of products) to improve product recommendation and dynamic pricing of goods (Desjardins et al., 2006; Horton & Johari, 2015; Blum et al., 2015; V\u00e1zquez-Gallo et al., 2014).", "startOffset": 95, "endOffset": 190}, {"referenceID": 25, "context": ", from a catalogue of products) to improve product recommendation and dynamic pricing of goods (Desjardins et al., 2006; Horton & Johari, 2015; Blum et al., 2015; V\u00e1zquez-Gallo et al., 2014).", "startOffset": 95, "endOffset": 190}, {"referenceID": 2, "context": "1 In economics, the distance function can encode the We refer the interested reader to the survey by Bellet et al. (2013) for a detailed discussion of various applications.", "startOffset": 101, "endOffset": 122}, {"referenceID": 0, "context": "This query is motivated by the posted-price model used in marketplaces (Abernethy et al., 2015; Singla & Krause, 2013), where users are offered a take-it-or-leave-it price by the system, and they can accept by providing a positive response, or reject by providing a negative response.", "startOffset": 71, "endOffset": 118}, {"referenceID": 20, "context": "old function in the active-learning setting (Castro & Nowak, 2006; Settles, 2012).", "startOffset": 44, "endOffset": 81}, {"referenceID": 20, "context": "One policy inspired by uncertainty sampling (Settles, 2012) is to pick the pair (i, j) with maximum uncertainty quantified by (U t\u22121 i,j \u2212L t\u22121 i,j ).", "startOffset": 44, "endOffset": 59}, {"referenceID": 4, "context": "Similar equivalence has been shown by Brickell et al. (2008) while studying the problem of projecting a non-metric matrix to a metric via decrease-only projections.", "startOffset": 38, "endOffset": 61}, {"referenceID": 4, "context": "We compute D\u2217 as the closest metric toW according to Brickell et al. (2008). For different weights w1, .", "startOffset": 53, "endOffset": 76}, {"referenceID": 18, "context": "Another line of research considers learning asymmetric distances, for instance by learning local invariant Mahalanobis distances (Fetaya & Ullman, 2015) or by learning general bilinear similarity matrices (Liu et al., 2015).", "startOffset": 205, "endOffset": 223}, {"referenceID": 23, "context": "Another line of research is that of learning low-dimensional embeddings for a set of items respecting the observed geometric relations between these items (Amid & Ukkonen, 2015; Tamuz et al., 2011; Cox & Cox, 2000; Jamieson & Nowak, 2011a).", "startOffset": 155, "endOffset": 239}, {"referenceID": 20, "context": "4 Active Learning Our problem formulation shares the goal of reducing sample complexity with other instances of active learning (Settles, 2012).", "startOffset": 128, "endOffset": 143}, {"referenceID": 16, "context": "We refer the reader to the detailed surveys by Yang & Jin (2006); van der Maaten et al. (2009); Bellet et al.", "startOffset": 74, "endOffset": 95}, {"referenceID": 1, "context": "(2009); Bellet et al. (2013). Some of the key distinctions of our formulation from the existing research are described in the following.", "startOffset": 8, "endOffset": 29}, {"referenceID": 1, "context": "(2009); Bellet et al. (2013). Some of the key distinctions of our formulation from the existing research are described in the following. Supervised metric learning. In their seminal work, Xing et al. (2002) introduced the supervised metric learning framework for learning Mahalanobis distance functions via a convex formulation.", "startOffset": 8, "endOffset": 207}, {"referenceID": 1, "context": "(2009); Bellet et al. (2013). Some of the key distinctions of our formulation from the existing research are described in the following. Supervised metric learning. In their seminal work, Xing et al. (2002) introduced the supervised metric learning framework for learning Mahalanobis distance functions via a convex formulation. In contrast to our setting, they assume that the algorithm has access to the feature space of the input data. Furthermore, this framework and its variants are restricted to recover symmetric distance functions. Another line of research considers learning asymmetric distances, for instance by learning local invariant Mahalanobis distances (Fetaya & Ullman, 2015) or by learning general bilinear similarity matrices (Liu et al., 2015). However, this line of work is not directly applicable to our setting because it also requires access to the feature space of the input data. Learning embeddings. Another line of research is that of learning low-dimensional embeddings for a set of items respecting the observed geometric relations between these items (Amid & Ukkonen, 2015; Tamuz et al., 2011; Cox & Cox, 2000; Jamieson & Nowak, 2011a). For applications involving human subjects, a triplet-based queries framework \u2014 for a triplet (i, j, k) it queries1(Di,j \u2264 Di,k) \u2014 has been employed. However the distances recovered from these approaches are merely optimized to respect the observed relations seen in the data from query responses \u2014 they are symmetric and importantly do not have an actual quantitative (economic) interpretation as we seek in our formulation. 8.2 Exploiting Structural Constraints Our approach of exploiting the structural constraints of the hemimetrics polytope is in part inspired by Elkan (2003), who acceleratesK-means by exploiting the triangle inequality.", "startOffset": 8, "endOffset": 1749}, {"referenceID": 1, "context": "(2009); Bellet et al. (2013). Some of the key distinctions of our formulation from the existing research are described in the following. Supervised metric learning. In their seminal work, Xing et al. (2002) introduced the supervised metric learning framework for learning Mahalanobis distance functions via a convex formulation. In contrast to our setting, they assume that the algorithm has access to the feature space of the input data. Furthermore, this framework and its variants are restricted to recover symmetric distance functions. Another line of research considers learning asymmetric distances, for instance by learning local invariant Mahalanobis distances (Fetaya & Ullman, 2015) or by learning general bilinear similarity matrices (Liu et al., 2015). However, this line of work is not directly applicable to our setting because it also requires access to the feature space of the input data. Learning embeddings. Another line of research is that of learning low-dimensional embeddings for a set of items respecting the observed geometric relations between these items (Amid & Ukkonen, 2015; Tamuz et al., 2011; Cox & Cox, 2000; Jamieson & Nowak, 2011a). For applications involving human subjects, a triplet-based queries framework \u2014 for a triplet (i, j, k) it queries1(Di,j \u2264 Di,k) \u2014 has been employed. However the distances recovered from these approaches are merely optimized to respect the observed relations seen in the data from query responses \u2014 they are symmetric and importantly do not have an actual quantitative (economic) interpretation as we seek in our formulation. 8.2 Exploiting Structural Constraints Our approach of exploiting the structural constraints of the hemimetrics polytope is in part inspired by Elkan (2003), who acceleratesK-means by exploiting the triangle inequality. By maintaining bounds on the distances, he efficiently reduces the number of distance computations. Brickell et al. (2008) study the problem of projecting a non-metric matrix to a metric matrix, and consider a specific class of decrease-only projections.", "startOffset": 8, "endOffset": 1935}, {"referenceID": 1, "context": "(2009); Bellet et al. (2013). Some of the key distinctions of our formulation from the existing research are described in the following. Supervised metric learning. In their seminal work, Xing et al. (2002) introduced the supervised metric learning framework for learning Mahalanobis distance functions via a convex formulation. In contrast to our setting, they assume that the algorithm has access to the feature space of the input data. Furthermore, this framework and its variants are restricted to recover symmetric distance functions. Another line of research considers learning asymmetric distances, for instance by learning local invariant Mahalanobis distances (Fetaya & Ullman, 2015) or by learning general bilinear similarity matrices (Liu et al., 2015). However, this line of work is not directly applicable to our setting because it also requires access to the feature space of the input data. Learning embeddings. Another line of research is that of learning low-dimensional embeddings for a set of items respecting the observed geometric relations between these items (Amid & Ukkonen, 2015; Tamuz et al., 2011; Cox & Cox, 2000; Jamieson & Nowak, 2011a). For applications involving human subjects, a triplet-based queries framework \u2014 for a triplet (i, j, k) it queries1(Di,j \u2264 Di,k) \u2014 has been employed. However the distances recovered from these approaches are merely optimized to respect the observed relations seen in the data from query responses \u2014 they are symmetric and importantly do not have an actual quantitative (economic) interpretation as we seek in our formulation. 8.2 Exploiting Structural Constraints Our approach of exploiting the structural constraints of the hemimetrics polytope is in part inspired by Elkan (2003), who acceleratesK-means by exploiting the triangle inequality. By maintaining bounds on the distances, he efficiently reduces the number of distance computations. Brickell et al. (2008) study the problem of projecting a non-metric matrix to a metric matrix, and consider a specific class of decrease-only projections. Our approach towards updating upper bounds via decrease-only projections is similar in spirit. However, the main technical difficulties arise in maintaining and updating the lower bounds, for which we develop novel techniques. The active-learning approach proposed by Jamieson & Nowak (2011a) exploits the geometry of the embedding space to minimize the sample complexity using triplet-based queries.", "startOffset": 8, "endOffset": 2360}, {"referenceID": 1, "context": "(2009); Bellet et al. (2013). Some of the key distinctions of our formulation from the existing research are described in the following. Supervised metric learning. In their seminal work, Xing et al. (2002) introduced the supervised metric learning framework for learning Mahalanobis distance functions via a convex formulation. In contrast to our setting, they assume that the algorithm has access to the feature space of the input data. Furthermore, this framework and its variants are restricted to recover symmetric distance functions. Another line of research considers learning asymmetric distances, for instance by learning local invariant Mahalanobis distances (Fetaya & Ullman, 2015) or by learning general bilinear similarity matrices (Liu et al., 2015). However, this line of work is not directly applicable to our setting because it also requires access to the feature space of the input data. Learning embeddings. Another line of research is that of learning low-dimensional embeddings for a set of items respecting the observed geometric relations between these items (Amid & Ukkonen, 2015; Tamuz et al., 2011; Cox & Cox, 2000; Jamieson & Nowak, 2011a). For applications involving human subjects, a triplet-based queries framework \u2014 for a triplet (i, j, k) it queries1(Di,j \u2264 Di,k) \u2014 has been employed. However the distances recovered from these approaches are merely optimized to respect the observed relations seen in the data from query responses \u2014 they are symmetric and importantly do not have an actual quantitative (economic) interpretation as we seek in our formulation. 8.2 Exploiting Structural Constraints Our approach of exploiting the structural constraints of the hemimetrics polytope is in part inspired by Elkan (2003), who acceleratesK-means by exploiting the triangle inequality. By maintaining bounds on the distances, he efficiently reduces the number of distance computations. Brickell et al. (2008) study the problem of projecting a non-metric matrix to a metric matrix, and consider a specific class of decrease-only projections. Our approach towards updating upper bounds via decrease-only projections is similar in spirit. However, the main technical difficulties arise in maintaining and updating the lower bounds, for which we develop novel techniques. The active-learning approach proposed by Jamieson & Nowak (2011a) exploits the geometry of the embedding space to minimize the sample complexity using triplet-based queries. While similar in spirit, their approach is based on triplet-based queries, and differs from our methodology of exploiting the structural constraints. 8.3 Learning User Preferences Another relevant line of research is concerned with eliciting user preferences. Specifically, we seek to learn private costs of a user for switching from her default choice of item i to instead choose item j. This type of preferences can be used in marketing applications, e.g., for persuading users to change their decisions (Kamenica & Gentzkow, 2009). Singla et al. (2015) considered similar preferences in the context of balancing a bike-sharing system by incentivizing users to go to alternate stations for pickup or dropoff of bikes.", "startOffset": 8, "endOffset": 3024}, {"referenceID": 0, "context": "Abernethy et al. (2015) considered the application of purchasing data from users, and quantified the prices that should be offered to them.", "startOffset": 0, "endOffset": 24}, {"referenceID": 4, "context": "Similar equivalence has been shown by Brickell et al. (2008) while studying the problem of projecting a non-metric matrix to a metric via decrease-only projections.", "startOffset": 38, "endOffset": 61}, {"referenceID": 4, "context": "Similar equivalence has been shown by Brickell et al. (2008) while studying the problem of projecting a non-metric matrix to a metric via decrease-only projections. Brickell et al. (2008) used a similar result as Lemma 4, however they directly used the interpretation of shortest-path problem.", "startOffset": 38, "endOffset": 188}, {"referenceID": 4, "context": "The approach to prove optimality of upper bounds U t is similar in spirit to that of Brickell et al. (2008) who showed the optimality of downward-only projection for the metric-nearness problem.", "startOffset": 85, "endOffset": 108}], "year": 2016, "abstractText": "Motivated by an application of eliciting users\u2019 preferences, we investigate the problem of learning hemimetrics, i.e., pairwise distances among a set of n items that satisfy triangle inequalities and non-negativity constraints. In our application, the (asymmetric) distances quantify private costs a user incurs when substituting one item by another. We aim to learn these distances (costs) by asking the users whether they are willing to switch from one item to another for a given incentive offer. Without exploiting structural constraints of the hemimetric polytope, learning the distances between each pair of items requires \u0398(n) queries. We propose an active learning algorithm that substantially reduces this sample complexity by exploiting the structural constraints on the version space of hemimetrics. Our proposed algorithm achieves provably-optimal sample complexity for various instances of the task. For example, when the items are embedded into K tight clusters, the sample complexity of our algorithm reduces to O(nK). Extensive experiments on a restaurant recommendation data set support the conclusions of our theoretical analysis.", "creator": "LaTeX with hyperref package"}}}