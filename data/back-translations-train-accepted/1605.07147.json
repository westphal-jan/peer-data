{"id": "1605.07147", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Riemannian SVRG: Fast Stochastic Optimization on Riemannian Manifolds", "abstract": "We study optimization of finite sums of \\emph{geodesically} smooth functions on Riemannian manifolds. Although variance reduction techniques for optimizing finite-sum problems have witnessed a huge surge of interest in recent years, all existing work is limited to vector space problems. We introduce \\emph{Riemannian SVRG}, a new variance reduced Riemannian optimization method. We analyze this method for both geodesically smooth \\emph{convex} and \\emph{nonconvex} functions. Our analysis reveals that Riemannian SVRG comes with advantages of the usual SVRG method, but with factors depending on manifold curvature that influence its convergence. To the best of our knowledge, ours is the first \\emph{fast} stochastic Riemannian method. Moreover, our work offers the first non-asymptotic complexity analysis for nonconvex Riemannian optimization (even for the batch setting). Our results have several implications; for instance, they offer a Riemannian perspective on variance reduced PCA, which promises a short, transparent convergence analysis.", "histories": [["v1", "Mon, 23 May 2016 19:28:05 GMT  (104kb,D)", "http://arxiv.org/abs/1605.07147v1", null], ["v2", "Fri, 7 Apr 2017 18:13:53 GMT  (104kb,D)", "http://arxiv.org/abs/1605.07147v2", "This is the final version that appeared in NIPS 2016. Our proof of Lemma 2 was incorrect in the previous arXiv version. (9 pages paper + 6 pages appendix)"]], "reviews": [], "SUBJECTS": "math.OC cs.LG", "authors": ["hongyi zhang", "sashank j reddi", "suvrit sra"], "accepted": true, "id": "1605.07147"}, "pdf": {"name": "1605.07147.pdf", "metadata": {"source": "CRF", "title": "Fast stochastic optimization on Riemannian manifolds", "authors": ["Hongyi Zhang", "Sashank J. Reddi"], "emails": ["hongyiz@mit.edu", "sjakkamr@cs.cmu.edu", "suvrit@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "It is only a matter of time before it will happen, until it will happen."}, {"heading": "2 Preliminaries", "text": "Before the formal discussion of Riemannian's optimization, let us cite some basic concepts of Riemannian geometry: \"We assume that Riemannian's geometry is an arbitrary way.\" \"We can relate in a different way in a different way in a different way in a different way in a different way in a different way in a different way in a different way in a different way in a different way in a different way in a different way in a different world.\" \"We,\" Riemannian said, \"so Riemannius,\" so Riemannius, \"\" so Riemannius, \"\" so Riemannius, \"\" so Riemannix, \"\" so different way in a different way in a different way in a different way, so different way and so and so another way in a different way. \""}, {"heading": "3 Riemannian SVRG", "text": "In this section, we formally present Rsvrg. We proceed from the following assumptions: (a) f reaches its optimum at x-X; (b) X is compact, and the diameter of X is limited by D, i.e. maxx, y-X d (x, y) \u2264 D; (c) the cross-section in X is limited above \u03bamax, and within X the exponential map is invertable; and (d) the cross-section curvature in X is lower than \u0445min. We define the following two most important geometric constants that capture the effects of multiple curvature: \u0430 = \u0432min | D tanh (\u0432min < 0.1, if \u043fmin \u2265 0, and \u03c3 = {\u0445maxDtan (\u0432\u0430\u0441maxD), if \u0441max > 0, 1, if \u0442max > 0 \u2212 We find that most (if not all) practical optimization problems can satisfy these assumptions."}, {"heading": "3.1 Convergence analysis for geodesically convex (g-convex) functions", "text": "In this section we will analyze the global complexity of Rsvrg for solution (1), where each fi (i [n]) is g-convex and f-convex is strong."}, {"heading": "3.2 Convergence analysis for geodesically nonconvex functions", "text": "In this section, we analyze the global complexity of Rsvrg for the solution (1), where each fi is only required to be L-g-smooth, and neither fi nor f must be g-smooth. We first measure that Riemannian SGD considers nonconvex L-g-smooth problems attains O (1 / 2) as SGD [10]; we banish the results to the appendix.Recently, two groups independently of each other, that the variance reduction also benefits from stochastic gradients for nonconvex-sum optimization problems, with different analyses [2; 21]. Our analysis for nonconvex Rsvrg is inspired by [21]."}, {"heading": "4 Applications", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Computing the leading eigenvector", "text": "In this section, we apply our analysis of Rsvrg for gradient-dominated functions (theorem 4) to fast eigenvector calculation, a fundamental problem that is still actively explored in the big data environment [9; 13; 25]. For the problem of calculating the leading eigenvector, i.e., min x > x = 1 \u2212 x > (\u2211 ni = 1 zz >) x, \u2212 x > Ax = f (x), (8) existing analyses for state-of-the-art algorithms typically result in O (1 / 2) dependence on the eigengap of A, as opposed to the presumed ability of O (1 / 3), and dependence of O (1 / 3) on power iteration. Here, we provide new support for the O (1 / 3) conjecture. Our central observation is that although problem (8) is seen as one in Rd."}, {"heading": "4.2 Computing the Riemannian centroid", "text": "In this subsection we confirm that Rsvrg converges linearly to determine PSD matrices on average, which is a geographically strongly convex problem but is not convex in Euclidean space. This problem has been investigated both in the matrix calculation and in various applications [4; 12]. We use the same experimental setting as in [32] and compare Rsvrg with Rieman's full gradient (RGD) and stochastic gradient (RSGD) algorithms (Figure 3). Note that the target is the sum of square belt distances in a non-positively curved space, i.e. (2N) -strong g-convex and (2N\u0442) -g-smooth. In a reasonable initialization, the conditional number \u0432 is under control, in this case we choose m = n and the optimal step size for Rsvrg is O (1 / (N3 / 2)."}, {"heading": "5 Discussion", "text": "We present the Rieman SVRG, the first (to the best of our knowledge) reduced-variance stochastic gradient algorithm for belt optimization. Furthermore, we analyze its global complexity to optimize geographically strongly convex, convex, and nonconvex functions, and explicitly show its dependence on the cross-section curvature. Our experiments confirm our analysis that the Rieman SVRG is much faster than methods for solving finite-sum optimization problems on belt manifolds. Our analysis of the calculation of the leading eigenvector as a riemanic optimization problem is also worth mentioning: A nonconvex problem with nonpositive Hessian and nonlinear constraints in ambient space turns out to be dominated by gradients on the multiplicity. We believe that this holds the promise of a theoretical study of belt optimization and geometric optimization in general, and we hope that other researchers will join us in this endeavor."}, {"heading": "Appendix: Fast Stochastic Optimization on Riemannian Manifolds", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Proofs for Section 3.1", "text": "(1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (x) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x x (1) x x (1) x x x (1) x x) x (1) x (1) x) x (1) x (1) x) x (1) x (1) x) x (1) x (1) x) x (1) x (1) x (1) x) x (1) x (1) x (1) x (1) x) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (x) x (1) x (x) x (1) x (1) x (x) x (1) x (1) x (1) x x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x x (1) x (1) x (1) x (1) x (1) x (1) x (1) x x (1) x (1) x (1) x x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x x x (1) x (1) x (1) x x (1) x (1) x (1) x x x (1) x (1) x x x (1) x"}, {"heading": "B Proofs for Section 3.2", "text": "Theorem 6 (xx2) + > exponential map is set to X, f = 1 (x2), f = 1 (x2), f = 1 (x2), f = 1 (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (f), f (f), f (x2), f (f), f (f), f (f), f (f), f (x), f (x2), f (2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2), f (x2"}, {"heading": "C Proof for Section 4.1", "text": "Theorem 5: Suppose A has eigenvalues \u03bb1 > \u03bb2 \u2265 \u00b7 > \u03bbd and \u03b4 = \u03bb1 \u2212 \u03bb2. With probability 1 \u2212 p, the random initialization x0 falls into a belt-like sphere of a global optimum of objective function within which the objective function O (dp2\u03b4) gradient dominates. Proof: We write x on the basis of eigenvectors of A {vi} di = 1 with corresponding eigenvalues \u03bb1 > \u03bb2 \u2265 \u00b7 \u00b7 \u0432\u0438\u0441\u0438\u043d\u0438\u0441\u04422, i.e. x = \u2211 d i = 1 \u03b1ivi. Thus, Ax = \u2211 d i = 1 \u03b1ivi and f (x) = \u2212 \u0445d i = 1 \u03b1 \u2212 2 i\u03bbi. The Riemannian gradient off (x) is Px (x) f (I \u2212 xx >) Ax (f) f (x) x) x) x."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>We study optimization of finite sums of geodesically smooth functions on Riemannian manifolds.<lb>Although variance reduction techniques for optimizing finite-sum problems have witnessed a huge surge of<lb>interest in recent years, all existing work is limited to vector space problems. We introduce Riemannian<lb>SVRG, a new variance reduced Riemannian optimization method. We analyze this method for both<lb>geodesically smooth convex and nonconvex functions. Our analysis reveals that Riemannian SVRG comes<lb>with advantages of the usual SVRG method, but with factors depending on manifold curvature that<lb>influence its convergence. To the best of our knowledge, ours is the first fast stochastic Riemannian<lb>method. Moreover, our work offers the first non-asymptotic complexity analysis for nonconvex Riemannian<lb>optimization (even for the batch setting). Our results have several implications; for instance, they offer<lb>a Riemannian perspective on variance reduced PCA, which promises a short, transparent convergence<lb>analysis.", "creator": "LaTeX with hyperref package"}}}