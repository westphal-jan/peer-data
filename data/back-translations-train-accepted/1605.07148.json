{"id": "1605.07148", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Backprop KF: Learning Discriminative Deterministic State Estimators", "abstract": "Generative state estimators based on probabilistic filters and smoothers are one of the most popular classes of state estimators for robots and autonomous vehicles. However, generative models have limited capacity to handle rich sensory observations, such as camera images, since they must model the entire distribution over sensor readings. Discriminative models do not suffer from this limitation, but are typically more complex to train as latent variable models for state estimation. We present an alternative approach where the parameters of the latent state distribution are directly optimized as a deterministic computation graph, resulting in a simple and effective gradient descent algorithm for training discriminative state estimators. We show that this procedure can be used to train state estimators that use complex input, such as raw camera images, which must be processed using expressive nonlinear function approximators such as convolutional neural networks. Our model can be viewed as a type of recurrent neural network, and the connection to probabilistic filtering allows us to design a network architecture that is particularly well suited for state estimation. We evaluate our approach on tracking task with raw image inputs. The results show significant improvement over both standard generative approaches and regular recurrent neural networks.", "histories": [["v1", "Mon, 23 May 2016 19:28:21 GMT  (137kb,D)", "http://arxiv.org/abs/1605.07148v1", null], ["v2", "Sun, 17 Jul 2016 01:43:34 GMT  (135kb,D)", "http://arxiv.org/abs/1605.07148v2", null], ["v3", "Sun, 30 Oct 2016 05:15:47 GMT  (2110kb,D)", "http://arxiv.org/abs/1605.07148v3", "Accepted to NIPS 2016"], ["v4", "Sun, 1 Oct 2017 00:57:20 GMT  (2239kb,D)", "http://arxiv.org/abs/1605.07148v4", "NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["tuomas haarnoja", "anurag ajay", "sergey levine", "pieter abbeel"], "accepted": true, "id": "1605.07148"}, "pdf": {"name": "1605.07148.pdf", "metadata": {"source": "CRF", "title": "Backprop KF: Learning Discriminative Deterministic State Estimators", "authors": ["Tuomas Haarnoja"], "emails": ["haarnoja@berkeley.edu", "anuragajay@berkeley.edu", "svlevine@cs.washington.edu", "pabbeel@cs.berkeley.edu"], "sections": [{"heading": null, "text": "Generative state estimators, based on probabilistic filters and smoothing methods, are one of the most popular classes of state estimators for robots and autonomous vehicles. However, generative models are limited in their ability to process rich sensory observations such as camera images, as they have to model the entire distribution via sensor measurements. Discriminatory models do not suffer from this limitation, but are typically more complex than latent variable models for state estimations. We show that this method can be used to train state estimators that use complex inputs, such as raw camera images that need to be processed using expressive nonlinear functional approximators such as Convolutionary Neural Networks."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "3 Preliminaries", "text": "If we want to perform state estimates with high-dimensional observations such as camera images, the use of a generative model is directly very difficult, since the observations are generated by a complex and highly nonlinear process. In practice, however, the underlying state of the system xt could mainly depend on a low-dimensional vector that can be extracted from ot, which we call pt. In this case, we can first train a discriminatory model g\u03b8 (ot) to predict from ot in a forward-looking manner, and then filter the predictions to output the desired state designations y1: t. In this case, a Kalman filter with hidden state text could be trained to use both yt and the predicted point as observations, and then perform conclusions about yt at the test date. This standard approach for state estimates with high-dimensional observations is illustrated in Figure 2b. While this method is an active interpretation method, it may not be suitable to observe the entire variation method."}, {"heading": "4 Discriminative Deterministic State Estimation", "text": "Our contribution is based on a generalized view of the state assessment, which summarizes the models discussed in the previous section and allows them to be trained end-to-end with simple and scalable stochastic gradient methods. In the naive approach, the observation function g\u03b8 (ot) is trained to predict directly pt, since a standardized generative filter model does not provide an easy way to optimize g\u03b8 (ot) with respect to the accuracy of the filter on the y1: T labels. However, the filter can be viewed as a calculation graph unwanted by time, as shown in Figure 2b. In this diagram, the filter has an internal state defined by the posterior via xt. For example, in a Kalman filter with Gaussian posterior, we can represent the internal state with the tuple st = {\u00b5xt}. Generally, we will use st to refer to the state of any filter."}, {"heading": "5 Experimental Evaluation", "text": "In this section, we compare our deterministic, discriminatory trained state estimator with a number of alternative methods, including simple feedback-forward convolution networks, standard Kalman filter variants, and fully generic LSTM models. We evaluate these models using a task that requires tracking a red disk in disorder and strong occlusion, which requires the state estimator to process raw image input, bypass longer occlusion periods, and locate the object in the presence of a large number of distractors."}, {"heading": "5.1 State Estimation Models", "text": "Our proposed model, which we call the \"Backprop-Kalman Filter\" (BKF), is a computational graph consisting of a Kalman filter (KF) and a feedback-convolutionary neural network that distils the observation ot into a low-dimensional signal pt, which serves as an observation for the CF. The neural network issues both a mean for pt and an observation covariance matrix Rt. As the network is trained together with the filter, it can learn to use the covariance matrix to communicate the desired degree of uncertainty about the observation in order to maximize the accuracy of the final filter prediction. We compare the Backprop-KF with three alternative state estimators: the \"Piecewise-KF\" and the \"LSTM model.\" The simplest of the models, the feedback-forward model, which does not take the time structure into account at all in the task."}, {"heading": "5.2 Backprop KF Computation Graph Architecture", "text": "The architecture of the calculation graph, which corresponds to the BKF model, is shown in Figure 3 >. It consists of a feedback neural network and a recursive part based on the Kalman filter. The neural network, shown in Figure 4, has two revolutionary layers and two hidden, fully connected layers. A special aspect of our network design is a novel reaction normalization layer that is applied to the revolutionary activations before nonlinearity is applied. Reaction normalization transforms the activations such that the activation of the layer i always means \u00b5i and variance \u03c32i, regardless of the input image. The parameters \u00b5i and \u03c32i are learned together with other parameters. \u2212 This normalization is used in all revolutionary networks in our evaluation and is similar to the batch normalization [9] in its behavior. However, we have found that this approach is much more effective for recursive models that require retrogression over time."}, {"heading": "5.3 Visual State Estimation Task Setup", "text": "Our task for state assessment is to reflect the typical challenges of visual state assessment: the need for long-term occlusion tracking, the presence of noise, and the need to process raw pixel data. It requires the follow-up of a red disc from image observations, as shown in Figure 5. Distractor discs with random colors and radii are added to the scene to cover the red disc, and the trajectories of all discs follow linear-Gaussian dynamics, with a linear spring force pulling the discs toward the center of the frame, and a traction force that prevents high velocities. Disks may leave the frame temporarily because the contacts are not modeled. Gaussian noise is added to disrupt motion. The difficulty of the task can be adjusted by increasing or decreasing the number of distractor discs, which affects the frequency of events."}, {"heading": "5.3.1 Comparing the State Estimation Models", "text": "The results in Table 1 show that BKF exceeds both the standard probable KF-based estimators and the more powerful and expressive LSTM estimators.The tracking error of simpleTable 1: Benchmark ResultsState Estimation Model # Parameters RMS test errorfeedforward model 7394 0.2322 \u00b1 0.1316 piecewise KF 7397 0,1160 \u00b1 0.0330 LSTM model (64 units) 33506 0,1407 \u00b1 0.1154 LSTM model (128 units) 92423 \u00b1 0.1352 BKF model (ours) 7493 0.0537 \u00b1 0.1235feedforward model is significantly larger due to the occlusions, and the model tends to predict the mean coordinates when the target piecewise model is performs better, but because the observation covariance is not conditional on ot, the KF learner, the KF learning to use."}, {"heading": "6 Discussion", "text": "In fact, we are in a position to go in search of a solution that enables us, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in the position we are in."}], "references": [{"title": "Discriminative training of kalman filters", "author": ["P. Abbeel", "A. Coates", "M. Montemerlo", "A.Y. Ng", "S. Thrun"], "venue": "Robotics: Science and Systems (R:SS)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Sequential deep learning for human action recognition", "author": ["M. Baccouche", "F. Mamalet", "C. Wolf", "C. Garcia", "A. Baskurt"], "venue": "Second International Conference on Human Behavior Unterstanding, pages 29\u201339, Berlin, Heidelberg", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "A neural network implementing optimal state estimation based on dynamic spike train decoding", "author": ["O. Bobrowski", "R. Meir", "S. Shoham", "Y.C. Eldar"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "B", "author": ["K. Cho"], "venue": "van Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30\u201342", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["T. Do", "T. Arti"], "venue": "Neural conditional random fields. In International Conference on Artificial Intelligence and Statistics, pages 177\u2013184", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Discriminatively trained particle filters for complex multi-object tracking", "author": ["R. Hess", "A. Fern"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 240\u2013247. IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8): 1735\u20131780", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional state space models for discriminative motion estimation", "author": ["M. Kim", "V. Pavlovic"], "venue": "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1\u20138. IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "GP-BayesFilters: Bayesian filtering using Gaussian process prediction and observation models", "author": ["J. Ko", "D. Fox"], "venue": "Autonomous Robots, 27(1):75\u201390", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F.C. Pereira"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "CRF-filters: Discriminative particle filters for sequential state estimation", "author": ["B. Limketkai", "D. Fox", "L. Liao"], "venue": "International Conference on Robotics and Automation (ICRA)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent-dynamic discriminative models for continuous gesture recognition", "author": ["L.-P. Morency", "A. Quattoni", "T. Darrell"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR\u201907. IEEE Conference on, pages 1\u20138. IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Combining discriminative features to infer complex trajectories", "author": ["D.A. Ross", "S. Osindero", "R.S. Zemel"], "venue": "Proceedings of the 23rd international conference on Machine learning, pages 761\u2013768. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Discriminative density propagation for 3d human motion estimation", "author": ["C. Sminchisescu", "A. Kanaujia", "Z. Li", "D. Metaxas"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 390\u2013397. IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Probabilistic Robotics", "author": ["S. Thrun", "W. Burgard", "D. Fox"], "venue": "The MIT Press", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural network based state estimation of dynamical systems", "author": ["N. Yadaiah", "G. Sowmya"], "venue": "International Joint Conference on Neural Networks (IJCNN)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 16, "context": "State estimation is an important component of mobile robotic applications, including autonomous driving and flight [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "Instead, the most popular methods for vision-based state estimation (such as SLAM [17]) are based on domain knowledge and geometric principles.", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "Discriminative models such as CRFs [12] typically do not use latent variables, which means that training data must contain full state observations.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "While discriminative models can be augmented with latent state [14], this typically makes them harder to train.", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "For a more complete review of state estimation, we refer the reader to standard references on this topic [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "The challenges of generative state space estimation can be mitigated by using complex observation models [11], but building effective generative models of images remains a challenging open problem.", "startOffset": 105, "endOffset": 109}, {"referenceID": 11, "context": "As an alternative to generative models, discriminative models such as conditional random fields (CRFs) can directly estimate p(xt|o1:t) [12].", "startOffset": 136, "endOffset": 140}, {"referenceID": 15, "context": "A number of CRFs and conditional state space models (CSSMs) have been applied to state estimation [16, 15, 10, 13, 7], typically using a log-linear representation.", "startOffset": 98, "endOffset": 117}, {"referenceID": 14, "context": "A number of CRFs and conditional state space models (CSSMs) have been applied to state estimation [16, 15, 10, 13, 7], typically using a log-linear representation.", "startOffset": 98, "endOffset": 117}, {"referenceID": 9, "context": "A number of CRFs and conditional state space models (CSSMs) have been applied to state estimation [16, 15, 10, 13, 7], typically using a log-linear representation.", "startOffset": 98, "endOffset": 117}, {"referenceID": 12, "context": "A number of CRFs and conditional state space models (CSSMs) have been applied to state estimation [16, 15, 10, 13, 7], typically using a log-linear representation.", "startOffset": 98, "endOffset": 117}, {"referenceID": 6, "context": "A number of CRFs and conditional state space models (CSSMs) have been applied to state estimation [16, 15, 10, 13, 7], typically using a log-linear representation.", "startOffset": 98, "endOffset": 117}, {"referenceID": 4, "context": "More recently, discriminative finetuning of generative models with nonlinear neural network observations [5], as well as direct training of CRFs with neural network factors [6], have allowed for training of nonlinear discriminative models.", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "More recently, discriminative finetuning of generative models with nonlinear neural network observations [5], as well as direct training of CRFs with neural network factors [6], have allowed for training of nonlinear discriminative models.", "startOffset": 173, "endOffset": 176}, {"referenceID": 13, "context": "Although CRFs have also been combined with latent states [14], the difficulty of CRF inference makes latent state CRF models difficult to train.", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "Prior work has also proposed to optimize SSM parameters with respect to a discriminative loss [1].", "startOffset": 94, "endOffset": 97}, {"referenceID": 17, "context": "The use of recurrent neural networks (RNNs) for state estimation has been explored in several prior works [18, 3], but has generally been limited to simple tasks without complex sensory inputs such as images.", "startOffset": 106, "endOffset": 113}, {"referenceID": 2, "context": "The use of recurrent neural networks (RNNs) for state estimation has been explored in several prior works [18, 3], but has generally been limited to simple tasks without complex sensory inputs such as images.", "startOffset": 106, "endOffset": 113}, {"referenceID": 7, "context": "Recently, innovative RNN architectures have been successful at mitigating this problem, through models such as the long short-term memory (LSTM) [8] and the gated recurrent unit (GRU) [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 3, "context": "Recently, innovative RNN architectures have been successful at mitigating this problem, through models such as the long short-term memory (LSTM) [8] and the gated recurrent unit (GRU) [4].", "startOffset": 184, "endOffset": 187}, {"referenceID": 1, "context": "LSTMs have been combined with vision for perception tasks such as activity recognition [2].", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "Finally, we compare to a recurrent neural network based on LSTM hidden units [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 8, "context": "normalization [9] in its behavior.", "startOffset": 14, "endOffset": 17}], "year": 2017, "abstractText": "Generative state estimators based on probabilistic filters and smoothers are one of the most popular classes of state estimators for robots and autonomous vehicles. However, generative models have limited capacity to handle rich sensory observations, such as camera images, since they must model the entire distribution over sensor readings. Discriminative models do not suffer from this limitation, but are typically more complex to train as latent variable models for state estimation. We present an alternative approach where the parameters of the latent state distribution are directly optimized as a deterministic computation graph, resulting in a simple and effective gradient descent algorithm for training discriminative state estimators. We show that this procedure can be used to train state estimators that use complex input, such as raw camera images, which must be processed using expressive nonlinear function approximators such as convolutional neural networks. Our model can be viewed as a type of recurrent neural network, and the connection to probabilistic filtering allows us to design a network architecture that is particularly well suited for state estimation. We evaluate our approach on tracking task with raw image inputs. The results show significant improvement over both standard generative approaches and regular recurrent neural networks.", "creator": "LaTeX with hyperref package"}}}