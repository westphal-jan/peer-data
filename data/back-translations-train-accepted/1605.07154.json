{"id": "1605.07154", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations", "abstract": "We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes.", "histories": [["v1", "Mon, 23 May 2016 19:40:50 GMT  (119kb,D)", "http://arxiv.org/abs/1605.07154v1", "15 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["behnam neyshabur", "yuhuai wu", "ruslan salakhutdinov", "nati srebro"], "accepted": true, "id": "1605.07154"}, "pdf": {"name": "1605.07154.pdf", "metadata": {"source": "CRF", "title": "Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations", "authors": ["Behnam Neyshabur", "Yuhuai Wu", "Ruslan Salakhutdinov", "Nathan Srebro"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this paper, our goal is to better understand the geometry of plain RNNs and to develop better methods aimed directly at activating networks [4, 3, 9], including those that involve long-term dependencies (e.g., [1, 23]). However, most empirical success has not been achieved with \"simple\" RNNs, but with alternative, more complex structures, such as Long Short-Term Memory (LSTM) networks [7] or Gated Recurrent Units (GRUs). Much of the motivation for these more complex models is not so much because of their modelling of wealth, but perhaps more because they seem to be easier to optimize."}, {"heading": "2 Recurrent Neural Nets as Feedforward Nets with Shared Weights", "text": "We see that it is an imperfect and imperfect solution, which is based on an imperfect and imperfect solution. We see that it is an imperfect solution, which is based on an imperfect and imperfect solution. We see that it is an imperfect solution, which amounts to an imperfect and imperfect solution. We point to the set of edges that share the parameters. We point to the parameters that map the parameters of Ei = {e} the parameters in the parameters of the indices. That is, for every edge that exists. We point to the set of edges that share the parameters of Ei = {e} (e).That is, for each e2).That is, for each egg (e1).We point to the parameters that we share the parameters of Ei = e (e).That is, for each e2, e2 and e2."}, {"heading": "3 Non-Saturating Activation Functions", "text": "The choice of activation function for neural networks is large (whether the unit is very \"active\" or very active \") in terms of optimization. We are particularly concerned with the distinction between\" saturation \"and\" no-start, \"while we are fully focused on the distinction between\" saturation \"and\" no-start \"of activation functions. We only look at whether the function values of finite values are converted at negative and positive infinity, and thus asymptomatic horizontal lines on both sides. This is the derivation of activation functions to zero, like input to both \u2212 and +. Networks therefore have a big deficit: the vanishing gradient problem [6] is that the gradient disappears when the activation is large (whether the unit is very\" active \"or very active\")."}, {"heading": "4 Path-SGD for Networks with Shared Weights", "text": "As we have discussed, optimization is inherently tied to a choice of geometry, represented here by a choice of the measure of complexity or the \"standard\" 2. Furthermore, we prefer the use of an invariant measure, which could then lead to an invariant optimization method. In Section 4.1, we introduce the path regulator, and in Section 4.2, the derived path SGD optimization algorithm for standard forward networks. In Section 4.3, we then extend these ideas to common-weighted networks, including RNNs, and present two invariant optimization algorithms based on them. In Section 4.4, we show how these can be efficiently implemented by forward and backward propagations."}, {"heading": "4.1 Path-regularizer", "text": "The path regulator is the sum of all paths from the input nodes to the output nodes of the product of the square weights along the path. To formally define it, P is the set of directional paths from the input nodes to the output nodes, so that for each path we cross we have a series of longitudinal lines, and for each 0 \u2264 i \u2264 len (\u0442 0,....) \u2212 1, (\u0445i > i + 1). We also abuse the notation and name e \u0432 if for some i, e = (\u0441i, \u0412i + 1). Then the path regulator can be written recursively as follows: \u03b32net (w) = \u04212v (u \u00b2 w) = 0 w2\u0445i \u2192 i + 1 (2)."}, {"heading": "4.2 Path-SGD for Feedforward Networks", "text": "Formally we consider for a network without common weights, where the parameters are the weights themselves, the diagonal quadratic approach of the path regulator to the current iteration w (t): \u03b3-2net (w (t) + 0-2net (w (t)) + 0-2net (w (t)), 0-2net (w (t) + 0-2net (w (t))), 0-2net (w (w (t) + 0-2), 0-2net (w (w (t) + 0-2), 0-2net (w) > diag (w (w (t))), 0-2net (w (w) + 0-2), 0-2net (we \u2212 w), 0-2net (we \u2212 2), 0-2b (we), 0-2w (we), 2w-2w (we), 2w-2w (we), 2w-2w (we), 2w-2w (we), 2w-0, 2w (we), 2w-2w, 2w (we), 2w-2w (we), 2w-2w (we), 2w-2w, 2w-0, 2w (we), 2w-2w, 2w (we), 2w-2w, 2w-2w, 2w (we), 2w-2w (we, 2w-2w, 2w-0, 2w, 2w-0, 2w-0, 2w-0, 2w-0, 2w (we), 2w-2w (we), 2w-2w-2w-2w, 2w-2w, 2w (we, 2w-2, 2w-2, 2w, 2w, 2w-2w-2, 2w-0, 2w-2w (we)."}, {"heading": "4.3 Extending to Networks with Shared Weights", "text": "If the networks have common weights, the path regulator is a function of the p parameters and therefore the square approximation should also occur with respect to iterate p (t) instead of w (t), which results in the following update rule: p (t + 1) = min p (t), p \u2212 p (t), p \u2212 p (t), p \u2212 p (t), p (t), p (t), p (p), p (p), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), c (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t), p (t, p (t), p (t), p (t, p, p (t), p, p (t), p (t, p, p (t), p, p, p (t, p (t), p, p (t), p (t, p, p, p (t), p (t, p (t), p (t, p, p (t), p (t, p, p, p (t), p (t), p, p (t, p (t), p (t), p (t, p (t), p (t, p, p (t, p (t), p, p (t, p (t), p (t, p (t), p, p (t), p (t, p (t, p (t), p (t), p, p (t, p (t, p (t), p (t, p (t), p (t), p (t, p (t), p (t, p (t), p"}, {"heading": "4.4 Simple and Efficient Computations for RNNs", "text": "We show how to use a mesh with the same architecture, but with square weights, to calculate the cost: Theorem 3. For each NetworkN (G, \u03c0, p), considerN (G, \u03c0, p), where for each i, p, i = p2i. Then we can define the function g: R | Vin | \u2192 R to calculate the sum of the returns of this mesh to be: g (x) = Vout | i = 1 fp (x) [i]. Then we can define the function g: R | Vin | R to calculate the sum of the returns of this mesh, p \u00b2, p \u00b2, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 The Contribution of the Second Term", "text": "In this section, we will examine the meaning of the second term and show that, at least in our experiments, the contribution of the second term is negligible. To compare the two terms, we can train a single-layer RNN with H = 200 hidden units for the task of word-level modeling on Penn Treebank (PTB) Corpus [13]. Figure 2 compares the performance of the first term with / without Path-SGD. We can clearly see that both versions of the Path-SGD perform very similarly and both perform significantly better than SGD. The result is that the first term is more significant and we can therefore ignore the second term c. To better understand the meaning of the two terms, we have compared the ratio of the two terms."}, {"heading": "5.2 Synthetic Problems with Long-term Dependencies", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5.3 Language Modeling Tasks", "text": "We are looking at two sets of data, Penn Treebank (PTB-c) and text8 5. PTB-c: We have conducted experiments on a tokenized Penn Treebank Corpus, following the experimental protocol of [11]. The training, validations and test data contain 5017k, 393k and 442k characters. The alphabet size is 50, and each training sequence is of the length 50th text8: The text8 dataset contains 100M characters from Wikipedia with an alphabet size of 27. We are following the data partition of [14], where each training sequence has a length of 180. Performance is evaluated metrically using bits-per-character (BPC), the log2 of the perplexity. Similar to the experiments on the synthetic datasets, for both tasks, we are training a single-layer RNN consisting of 2048 hidden units with path-Gath-N size."}, {"heading": "6 Conclusion", "text": "We studied the geometry of RNNs in a broader class of shared-weight feedback networks and demonstrated how understanding geometry can lead to significant improvements in various learning tasks. By developing an optimization algorithm with a geometry that is well suited for RNNs, we closed over half the performance gap between vanilla RNNNs and LSTMs. This is especially useful for applications where we seek compressed models with fast prediction time that require minimal memory; and also a step toward bridging the gap between LSTMs and RNNNNs."}, {"heading": "Acknowledgments", "text": "This research was partially supported by an NSF RI-AF Award and Intel ICRI-CI. We thank Saizheng Zhang for sharing a base code for RNNs."}, {"heading": "A Proofs", "text": "The proof of theorem 1We first show that each RNN by induction on layers and time steps is invariant to T1. Specifically, we prove that for each layer (1) of each layer (1) of each layer (1) of each layer (1) of each layer (1) of each layer (1) of each layer (1) of each layer (1) of each layer (1) of each layer (1) of each layer (1) of each layer (1) of each layer (1) of each layer (1). We show that for i = 1) of each layer (1), if we assume that the statement for t = t (1) is true, then it also applies for each layer (1)."}], "references": [{"title": "Unitary evolution recurrent neural networks", "author": ["Martin Arjovsky", "Amar Shah", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1511.06464,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceeding of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In Proceeding of the International Conference on Machine Learning (ICML),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "In Proceeding of the International Conference on Learning Representations,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Sepp Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In Proceeding of the International Conference on Learning Representations,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks. In Advances in neural information processing systems (NIPS), pages", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Regularizing RNNs by stabilizing activations", "author": ["David Krueger", "Roland Memisevic"], "venue": "In Proceeding of the International Conference on Learning Representations,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "Subword language modeling with neural networks", "author": ["Tom\u00e1\u0161 Mikolov", "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink", "J Cernocky"], "venue": "(http://www.fit.vutbr.cz/ imikolov/rnnlm/char.pdf),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Path-SGD: Path-normalized optimization in deep neural networks", "author": ["Behnam Neyshabur", "Ruslan Salakhutdinov", "Nathan Srebro"], "venue": "In Advanced in Neural Information Processsing Systems (NIPS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Data-dependent path normalization in neural networks", "author": ["Behnam Neyshabur", "Ryota Tomioka", "Ruslan Salakhutdinov", "Nathan Srebro"], "venue": "In the International Conference on Learning Representations,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Norm-based capacity control in neural networks", "author": ["Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro"], "venue": "In Proceeding of the 28th Conference on Learning Theory (COLT),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Riemannian metrics for neural networks ii: recurrent networks and learning symbolic data sequences", "author": ["Yann Ollivier"], "venue": "Information and Inference, page iav007,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Regularization and nonlinearities for neural language models: when are they needed", "author": ["Marius Pachitariu", "Maneesh Sahani"], "venue": "arXiv preprint arXiv:1301.5650,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Improving performance of recurrent neural network with relu nonlinearity", "author": ["Sachin S. Talathi", "Aniket Vartak"], "venue": "In the International Conference on Learning Representations workshop track,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "Recurrent Neural Networks (RNNs) have been found to be successful in a variety of sequence learning problems [4, 3, 9], including those involving long term dependencies (e.", "startOffset": 109, "endOffset": 118}, {"referenceID": 2, "context": "Recurrent Neural Networks (RNNs) have been found to be successful in a variety of sequence learning problems [4, 3, 9], including those involving long term dependencies (e.", "startOffset": 109, "endOffset": 118}, {"referenceID": 8, "context": "Recurrent Neural Networks (RNNs) have been found to be successful in a variety of sequence learning problems [4, 3, 9], including those involving long term dependencies (e.", "startOffset": 109, "endOffset": 118}, {"referenceID": 0, "context": ", [1, 23]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 6, "context": "However, most of the empirical success has not been with \u201cplain\u201d RNNs but rather with alternate, more complex structures, such as Long Short-Term Memory (LSTM) networks [7] or Gated Recurrent Units (GRUs) [3].", "startOffset": 169, "endOffset": 172}, {"referenceID": 2, "context": "However, most of the empirical success has not been with \u201cplain\u201d RNNs but rather with alternate, more complex structures, such as Long Short-Term Memory (LSTM) networks [7] or Gated Recurrent Units (GRUs) [3].", "startOffset": 205, "endOffset": 208}, {"referenceID": 21, "context": "One motivation for insisting on plain RNNs, as opposed to LSTMs or GRUs, is because they are simpler and might be more appropriate for applications that require low-complexity design such as in mobile computing platforms [22, 5].", "startOffset": 221, "endOffset": 228}, {"referenceID": 4, "context": "One motivation for insisting on plain RNNs, as opposed to LSTMs or GRUs, is because they are simpler and might be more appropriate for applications that require low-complexity design such as in mobile computing platforms [22, 5].", "startOffset": 221, "endOffset": 228}, {"referenceID": 11, "context": "Improving training RNNs with ReLU activations has been the subject of some recent attention, with most research focusing on different initialization strategies [12, 22].", "startOffset": 160, "endOffset": 168}, {"referenceID": 21, "context": "Improving training RNNs with ReLU activations has been the subject of some recent attention, with most research focusing on different initialization strategies [12, 22].", "startOffset": 160, "endOffset": 168}, {"referenceID": 15, "context": "We build on prior work on the geometry and optimization in feed-forward networks, which uses the path-norm [16] (defined in Section 4) to determine a geometry leading to the path-SGD optimization method.", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "Networks with saturating activations therefore have a major shortcoming: the vanishing gradient problem [6].", "startOffset": 104, "endOffset": 107}, {"referenceID": 14, "context": "While sigmoid and hyperbolic tangent have historically been popular choices for fully connected feedforward and convolutional neural networks, more recent works have shown undeniable advantages of non-saturating activations such as ReLU, which is now the standard choice for fully connected and Convolutional networks [15, 10].", "startOffset": 318, "endOffset": 326}, {"referenceID": 9, "context": "While sigmoid and hyperbolic tangent have historically been popular choices for fully connected feedforward and convolutional neural networks, more recent works have shown undeniable advantages of non-saturating activations such as ReLU, which is now the standard choice for fully connected and Convolutional networks [15, 10].", "startOffset": 318, "endOffset": 326}, {"referenceID": 0, "context": "However, for recurrent neural networks, using ReLU activations is challenging in a different way, as even a small change in the direction of the leading eigenvector of the recurrent weights could get amplified and potentially lead to the explosion in forward or backward propagation [1].", "startOffset": 283, "endOffset": 286}, {"referenceID": 18, "context": "Invariances have also been studied as different mappings from the parameter space to the same function space [19] while we define the transformation as a mapping inside a fixed parameter space.", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "A very important invariance in feedforward networks is node-wise rescaling [17].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "The stochastic version that uses a subset of training examples to estimate \u2202L \u2202wu\u2192v (w (t)) is called Path-SGD [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "The path-norm which we define is a norm on functions, not on weights, but as we prefer not getting into this technical discussion here, we use the term \u201cnorm\u201d very loosely to indicate some measure of magnitude [18].", "startOffset": 210, "endOffset": 214}, {"referenceID": 12, "context": "To compare the two terms \u03ba(1) and \u03ba(2), we train a single layer RNN with H = 200 hidden units for the task of word-level language modeling on Penn Treebank (PTB) Corpus [13].", "startOffset": 169, "endOffset": 173}, {"referenceID": 5, "context": "2 Synthetic Problems with Long-term Dependencies Training Recurrent Neural Networks is known to be hard for modeling long-term dependencies due to the gradient vanishing/exploding problem [6, 2].", "startOffset": 188, "endOffset": 194}, {"referenceID": 1, "context": "2 Synthetic Problems with Long-term Dependencies Training Recurrent Neural Networks is known to be hard for modeling long-term dependencies due to the gradient vanishing/exploding problem [6, 2].", "startOffset": 188, "endOffset": 194}, {"referenceID": 6, "context": "Addition problem: The addition problem was introduced in [7].", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "Here, each input consists of two sequences of length T , one of which includes numbers sampled from the uniform distribution with range [0, 1] and the other sequence serves as a mask which is filled with zeros except for two entries.", "startOffset": 136, "endOffset": 142}, {"referenceID": 11, "context": "Sequential MNIST: In sequential MNIST, each digit image is reshaped into a sequence of length 784, turning the digit classification task into sequence classification with long-term dependencies [12, 1].", "startOffset": 194, "endOffset": 201}, {"referenceID": 0, "context": "Sequential MNIST: In sequential MNIST, each digit image is reshaped into a sequence of length 784, turning the digit classification task into sequence classification with long-term dependencies [12, 1].", "startOffset": 194, "endOffset": 201}, {"referenceID": 11, "context": "For both tasks, we closely follow the experimental protocol in [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "We also train an RNN of the same size with identity initialization, as was proposed in [12], using SGD as our baseline model, referred to as IRNN.", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "Similar to [1], we found the IRNN to be fairly unstable (with SGD optimization typically diverging).", "startOffset": 11, "endOffset": 14}, {"referenceID": 11, "context": "We also compare RNN-Path with the previously published results, including identity initialized RNN [12] (IRNN), unitary RNN [1] (uRNN), and np-RNN4 introduced by [22].", "startOffset": 99, "endOffset": 103}, {"referenceID": 0, "context": "We also compare RNN-Path with the previously published results, including identity initialized RNN [12] (IRNN), unitary RNN [1] (uRNN), and np-RNN4 introduced by [22].", "startOffset": 124, "endOffset": 127}, {"referenceID": 21, "context": "We also compare RNN-Path with the previously published results, including identity initialized RNN [12] (IRNN), unitary RNN [1] (uRNN), and np-RNN4 introduced by [22].", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "Adding Adding Adding 100 400 750 sMNIST IRNN [12] 0 16.", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "0 uRNN [1] 0 3 16.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "9 LSTM [1] 0 2 16.", "startOffset": 7, "endOffset": 10}, {"referenceID": 21, "context": "8 np-RNN[22] 0 2 >2 3.", "startOffset": 8, "endOffset": 12}, {"referenceID": 19, "context": "PTB text8 RNN+smoothReLU [20] - 1.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "55 HF-MRNN [14] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "54 RNN-ReLU[11] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "65 RNN-tanh[11] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "55 TRec,\u03b2 = 500[11] 1.", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "PTB-c: We performed experiments on a tokenized Penn Treebank Corpus, following the experimental protocol of [11].", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "We follow the data partition of [14], where each training sequence has a length of 180.", "startOffset": 32, "endOffset": 36}, {"referenceID": 7, "context": "Instead, we use Adam optimizer [8] to help speed up the training, where we simply use the path-SGD gradient as input to the Adam optimizer.", "startOffset": 31, "endOffset": 34}, {"referenceID": 20, "context": "For LSTMs, we use orthogonal initialization [21] for the recurrent matrices and uniform[\u22120.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "We also compare our results to an RNN that uses hidden activation regularizer [11] (TRec,\u03b2 = 500), Multiplicative RNNs trained by Hessian Free methods [14] (HF-MRNN), and an RNN with smooth version of ReLU [20].", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "We also compare our results to an RNN that uses hidden activation regularizer [11] (TRec,\u03b2 = 500), Multiplicative RNNs trained by Hessian Free methods [14] (HF-MRNN), and an RNN with smooth version of ReLU [20].", "startOffset": 151, "endOffset": 155}, {"referenceID": 19, "context": "We also compare our results to an RNN that uses hidden activation regularizer [11] (TRec,\u03b2 = 500), Multiplicative RNNs trained by Hessian Free methods [14] (HF-MRNN), and an RNN with smooth version of ReLU [20].", "startOffset": 206, "endOffset": 210}], "year": 2016, "abstractText": "We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes.", "creator": "LaTeX with hyperref package"}}}