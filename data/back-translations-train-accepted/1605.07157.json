{"id": "1605.07157", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Unsupervised Learning for Physical Interaction through Video Prediction", "abstract": "A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 50,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a \"visual imagination\" of different futures based on different courses of action. Our experiments show that our proposed method not only produces more accurate video predictions, but also more accurately predicts object motion, when compared to prior methods.", "histories": [["v1", "Mon, 23 May 2016 19:45:55 GMT  (6109kb,D)", "http://arxiv.org/abs/1605.07157v1", null], ["v2", "Tue, 24 May 2016 19:33:23 GMT  (6109kb,D)", "http://arxiv.org/abs/1605.07157v2", "Fixing typo in supplementary materials URL. Correct URL is: sites.google.com/site/robotprediction"], ["v3", "Thu, 9 Jun 2016 00:29:37 GMT  (9318kb,D)", "http://arxiv.org/abs/1605.07157v3", "Fixing typo in supplementary materials URL. Correct URL is:this http URL"], ["v4", "Mon, 17 Oct 2016 20:09:56 GMT  (9116kb,D)", "http://arxiv.org/abs/1605.07157v4", "To appear in NIPS '16; Video results, code, and data available at:this http URL"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.RO", "authors": ["chelsea finn", "ian j goodfellow", "sergey levine"], "accepted": true, "id": "1605.07157"}, "pdf": {"name": "1605.07157.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning for Physical Interaction through Video Prediction", "authors": ["Chelsea Finn", "Ian Goodfellow"], "emails": ["cbfinn@eecs.berkeley.edu", "ian@openai.com", "slevine@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance,"}, {"heading": "2 Related Work", "text": "The results of this study show that most people who are able are able to determine for themselves what they want and what they don't want. (...) Most of them are not able to move. (...) Most of them are not able to move. (...) Most of them are not able to move. (...) Most of them are not able to move. (...) Most of them are not able to move. (...) Most of them are not able to move. (...) Most of them are not able to move. (...) Most of them are not able to move. (...) Most of them are not able to move. (...) Most of them are not able to move. (...) Most of them are not able to see the world. (...) Most of them are not able to be able to stay. (...) Most of them are able to stay. (...) Most of them are able to stay. (...) Most of them are not able to see the world. (...) Most of them are not able to see the world. (...) Most of them are not able to be able to understand the world. (...) Most of them are not able to understand the world. (... are not able to be able to understand the things. (...) Most of them are not able to understand the world. (...) Most of them are not able to understand the world. (...) Most of them are not able to understand the world. (... are not able to be able to see the world."}, {"heading": "3 Motion-Focused Predictive Models", "text": "To learn about the motion of objects while remaining invariant of appearance, we introduce a class of video prediction models that directly use information about the appearance of previous frames to create pixel predictions. Our model calculates the next frame by first predicting the motion of objects in the pixel space and then masking these predictions. In this section, we discuss our novel pixel transformation models and suggest how to effectively merge predicted movements of multiple objects into a single next image prediction. The architecture of the CDNA model is illustrated in Figure 1."}, {"heading": "3.1 Pixel Transformations for Future Video Prediction", "text": "The core of our models is a motion prediction module that predicts the motion of objects without reconstructing their appearance. (This module is therefore partially invariant to appearance and can effectively be generalized to previously invisible objects.) We have three motion prediction modules: Dynamic Neural Advection (DNA): In this approach, we predict distribution across the locations of individual pixels in the new environment. (This keeps the dimensionality of the prediction low.) This approach is the most flexible approach of the proposed approach. (We limit pixel movement to a local region where pixel's great distances are not moved. (This keeps the dimensionality of the prediction low.) Formally, we apply the predicted motion transformation M to the predicted image transformation M, to which the previous pixel rotation is applied.) I think that pixel's great distances will not be shifted."}, {"heading": "3.2 Composing Object Motion Predictions", "text": "CDNA and STP provide predictions of the motion of multiple objects that need to be combined into a single image. To do this, our model also predicts a series of masks that are applied to the transformations, which show how much each transformed image affects each pixel. A softmax over the mask channels ensures that it comes down to one. In practice, our experiments show that the model learns to mask objects that move in consistent directions. The benefit of this approach is twofold: first, predicted motion transformations are reused for multiple pixels in the image, and second, of course, the model extracts a more object-centric representation in an unattended manner, a desirable property for an agent that learns to interact with objects. For each model, including DNA, we also include a \"background mask,\" in which we allow the models to copy pixels directly from the previous frame, so that we may better represent that previously located regions in the background, and, in addition, we may better represent that last step."}, {"heading": "3.3 Action-conditioned Convolutional LSTMs", "text": "Most existing physics and video prediction models use feedback architectures [14, 13] or feedback encodings of the image [17]. To generate the motion predictions mentioned above, we use stacked Convolutionary LSTMs [24]. Repetition by coils is a natural requirement for multi-level video predictions, since it exploits the spatial invariance of image representations, since the laws of physics are largely consistent in space. As a result, models with revolutionary repetition require significantly fewer parameters and use these parameters more efficiently. Model architecture is presented in Figure 1 and detailed in Appendix B. In an interactive environment, the actions of the agent and the internal state (such as the pose of the robot grab) are also influencing nextimage, and we integrate both into our model by setting the vector of the linked state and action based on spatial conditions."}, {"heading": "5 Experiments", "text": "We evaluate our method using the data set in Section 4 as well as the data set Human3.6M [8], which consists of videos of human movements. In both settings, we evaluate our three models described in Section 3, as well as previous models [14, 17]. For CDNA and STP, we used 10 transformers. For all three models, in addition to the described transformations, we allow the network to copy background pixels directly from the previous frame by including the previous image in the masked compositing operation, with an additional channel in the mask for the background. While we show stills from the predicted videos in the numbers, the qualitative results are easiest to compare when the predicted videos can be viewed side by side. For this reason, we encourage the reader to examine the video results on the complementary website: sites.google.com / robotctionTraining details: We trained all models with the TensorFlow library by not scheduling the conversions with the suggested use of AD11."}, {"heading": "5.1 Action-conditioned prediction for robotic pushing", "text": "Our primary assessment is based on video predictions using our robotic interaction data sets, conditioned by the future actions of the robot. In this context, we pass in two initial images, as well as the initial robot arm state and actions, and then roll out sequentially by going into the future actions and the state of prediction from the previous time step. We have trained for 8 future time steps for all recurring models, and test for up to 18 time steps. We considered 5% of the training for validation. To quantify the predictions, we measure average PSNR and SSIM, as proposed in [14]. To explicitly evaluate whether the model predicts the correct movement of objects, we report the above described optical flows. In contrast, we measure all metrics on the entire image."}, {"heading": "5.2 Human motion prediction", "text": "In addition to the action-based prediction, we also evaluate our model for predicting future videos without action. We selected the Human3.6M dataset, which consists of human actors performing various actions in a room. We trained all models on 5 human subjects, provided one subject for validation, and provided another subject for the assessments presented here. Therefore, the models did not see either this specific human subject or any subject in the same clothing. We examined the video at 10 frames per second, so that within a reasonable time frame, there was noticeable movement in the videos. As the model is no longer tied to actions, we fed in 10 video images and trained the network to produce the next 10 images, each corresponding to one second. Our evaluation measures performance up to 20 time steps into the future.The results in Figure 7 show that our motion-based models exceed earlier methods in quantitative terms and we started generating plausible images after at least 10 time periods of prediction and at least 10 times of prediction."}, {"heading": "6 Conclusion & Future Directions", "text": "In this thesis, we are developing an action-based video prediction model for interaction that combines information about the appearance of previous images with movements predicted by the model. To investigate unattended learning for interaction, we are also presenting a new video dataset with 50,000 real robot interactions and more than 1 million video images. Our experiments show that by learning to transform pixels in the initial image, our model can produce plausible video sequences with more than 10 time steps into the future, which is equivalent to about one second. Compared to previous methods, our method achieves the best results on a number of previously proposed metrics. Predicting future object movements in the context of physical interaction is a key building block of an intelligent interactive system. The nature of action-based prediction of future video frames that we demonstrate can allow an interactive agent, such as a robot, to self-detect different futures based on the available actions, to put together an interesting mechanism that can be used to clarify an object."}, {"heading": "A Data Collection Details", "text": "In this section we will discuss further details of the data acquisition process. Data was collected using 10 robotic arms with a degree of freedom of 7 degrees and included the joint angle, gripper posture, gripper posture, gripper commanded posture, measured torque and 640 x 512 RGB images taken by the robot camera. Images were cut in half and scanned to 64 x 64. Sample images are shown in Figure 2. Images and measurements of the robot were recorded at 10 Hz, and the robot was controlled by impedance control in the task room. In our experiments, the internal state of the robot was the gripper's posture, and the action was the gripper's posture. Two test sets of 1,500 recorded movements each were also collected, both using the robot control method described above. The first test set used two different subsets of objects that were moved during the training, and the second test set consisted of two subsets of objects each of 1,500 recorded movements, each of which were placed in front of twenty objects during the training."}, {"heading": "B Model Details", "text": "In this case, it is as if it were a purely reactionary project."}, {"heading": "C Additional Experimental Results", "text": "To evaluate the method of [14], we trained the multi-scale feedback architecture on four of the proposed targets. For the robot motion dataset, we added the actions of the architecture on each scale through tiles. We failed to successfully train the model with a reverse loss, and as shown in Figure 10, the model that trained GDL + l1 performed best on the robot dataset. Therefore, we presented the results of this model in the main evaluation."}], "references": [{"title": "Simulation as an engine of physical scene understanding", "author": ["P.W. Battaglia", "J.B. Hamrick", "J.B. Tenenbaum"], "venue": "Proceedings of the National Academy of Sciences, 110(45)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "Advances in Neural Information Processing Systems, pages 1171\u20131179", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning predictive models of a depth camera & manipulator from raw execution traces", "author": ["B. Boots", "A. Byravan", "D. Fox"], "venue": "International Conference on Robotics and Automation (ICRA)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimating contact dynamics", "author": ["M.A. Brubaker", "L. Sigal", "D.J. Fleet"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Attend", "author": ["S. Eslami", "N. Heess", "T. Weber", "Y. Tassa", "K. Kavukcuoglu", "G.E. Hinton"], "venue": "infer, repeat: Fast scene understanding with generative models. arXiv preprint arXiv:1603.08575", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Vision meets robotics: The KITTI dataset", "author": ["A. Geiger", "P. Lenz", "C. Stiller", "R. Urtasun"], "venue": "International Journal of Robotics Research (IJRR)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Action-reaction: Forecasting the dynamics of human interaction", "author": ["D.-A. Huang", "K.M. Kitani"], "venue": "European Conference on Computer Vision (ECCV). Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "and C", "author": ["C. Ionescu", "D. Papava", "V. Olaru"], "venue": "Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. PAMI, 36(7)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "Spatial transformer networks. In Neural Information Processing Systems (NIPS)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "Compuer Vision and Pattern Recognition (CVPR)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kinga", "J. Ba"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Autonomous reinforcement learning on raw visual input data in a real world application", "author": ["S. Lange", "M. Riedmiller", "A. Voigtlander"], "venue": "International Joint Conference on Neural Networks (IJCNN)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning physical intuition of block towers by example", "author": ["A. Lerer", "S. Gross", "R. Fergus"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["M. Mathieu", "C. Couprie", "Y. LeCun"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Newtonian image understanding: Unfolding the dynamics of objects in static images", "author": ["R. Mottaghi", "H. Bagherinezhad", "M. Rastegari", "A. Farhadi"], "venue": "arXiv preprint arXiv:1511.04048", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "and A", "author": ["R. Mottaghi", "M. Rastegari", "A. Gupta"], "venue": "Farhadi. \"What happens if...\" learning to predict the effect of forces in images. arXiv preprint arXiv:1603.05600", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "Neural Information Processing Systems (NIPS)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["M. Ranzato", "A. Szlam", "J. Bruna", "M. Mathieu", "R. Collobert", "S. Chopra"], "venue": "arXiv preprint arXiv:1412.6604", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Anticipating the future by watching unlabeled video", "author": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "CoRR, abs/1504.08023", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Patch to the future: Unsupervised visual prediction", "author": ["J. Walker", "A. Gupta", "M. Hebert"], "venue": "Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Dense optical flow prediction from a static image", "author": ["J. Walker", "A. Gupta", "M. Hebert"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["M. Watter", "J. Springenberg", "J. Boedecker", "M. Riedmiller"], "venue": "Neural Information Processing Systems (NIPS)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional LSTM network: A machine learning approach for precipitation nowcasting", "author": ["S. Xingjian", "Z. Chen", "H. Wang", "D. Yeung", "W. Wong", "W. Woo"], "venue": "Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "A data-driven approach for event prediction", "author": ["J. Yuen", "A. Torralba"], "venue": "European Conference on Computer Vision ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Prior video prediction methods have typically considered short-range prediction [14], small image patches [19], or synthethic images [17].", "startOffset": 80, "endOffset": 84}, {"referenceID": 18, "context": "Prior video prediction methods have typically considered short-range prediction [14], small image patches [19], or synthethic images [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 16, "context": "Prior video prediction methods have typically considered short-range prediction [14], small image patches [19], or synthethic images [17].", "startOffset": 133, "endOffset": 137}, {"referenceID": 8, "context": "The last approach, which we call spatial transformer predictors (STP), outputs the parameters of multiple affine transformations to apply to the previous image, akin to the spatial transfomer network previously proposed for supervised learning [9].", "startOffset": 244, "endOffset": 247}, {"referenceID": 7, "context": "Our experiments using this new robotic pushing dataset, and using a human motion video dataset [8], show that models that explicitly transform pixels from previous frames better capture object motion and produce more accurate video predictions compared to prior state-of-the-art methods based on pixel reconstruction.", "startOffset": 95, "endOffset": 98}, {"referenceID": 24, "context": "[25] constructed predictions from similar videos in a dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "proposed a baseline for video prediction inspired by language models [18].", "startOffset": 69, "endOffset": 73}, {"referenceID": 18, "context": "LSTM models have been adapted for video prediction on patches [19], action-conditioned Atari frame predictions [17], and precipitation nowcasting [24].", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "LSTM models have been adapted for video prediction on patches [19], action-conditioned Atari frame predictions [17], and precipitation nowcasting [24].", "startOffset": 111, "endOffset": 115}, {"referenceID": 23, "context": "LSTM models have been adapted for video prediction on patches [19], action-conditioned Atari frame predictions [17], and precipitation nowcasting [24].", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "proposed new loss functions for sharper frame predictions [14].", "startOffset": 58, "endOffset": 62}, {"referenceID": 19, "context": "Prior methods generally reconstruct frames from the internal state of the model, and some predict the internal state directly, without producing images [20].", "startOffset": 152, "endOffset": 156}, {"referenceID": 21, "context": "This approach differs from recent work on optic flow prediction [22, 14], which predicts where pixels will move to.", "startOffset": 64, "endOffset": 72}, {"referenceID": 13, "context": "This approach differs from recent work on optic flow prediction [22, 14], which predicts where pixels will move to.", "startOffset": 64, "endOffset": 72}, {"referenceID": 2, "context": "predict future images of a robot arm using nonparametric kernel-based methods [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "Learning physics: Several works have explicitly addressed prediction of physical interactions, including predicting ball motion [4], block falling [1], the effects of forces [16, 15], future human interactions [7], and future car trajectories [21].", "startOffset": 128, "endOffset": 131}, {"referenceID": 0, "context": "Learning physics: Several works have explicitly addressed prediction of physical interactions, including predicting ball motion [4], block falling [1], the effects of forces [16, 15], future human interactions [7], and future car trajectories [21].", "startOffset": 147, "endOffset": 150}, {"referenceID": 15, "context": "Learning physics: Several works have explicitly addressed prediction of physical interactions, including predicting ball motion [4], block falling [1], the effects of forces [16, 15], future human interactions [7], and future car trajectories [21].", "startOffset": 174, "endOffset": 182}, {"referenceID": 14, "context": "Learning physics: Several works have explicitly addressed prediction of physical interactions, including predicting ball motion [4], block falling [1], the effects of forces [16, 15], future human interactions [7], and future car trajectories [21].", "startOffset": 174, "endOffset": 182}, {"referenceID": 6, "context": "Learning physics: Several works have explicitly addressed prediction of physical interactions, including predicting ball motion [4], block falling [1], the effects of forces [16, 15], future human interactions [7], and future car trajectories [21].", "startOffset": 210, "endOffset": 213}, {"referenceID": 20, "context": "Learning physics: Several works have explicitly addressed prediction of physical interactions, including predicting ball motion [4], block falling [1], the effects of forces [16, 15], future human interactions [7], and future car trajectories [21].", "startOffset": 243, "endOffset": 247}, {"referenceID": 11, "context": "In the domain of reinforcement learning, model-based methods have been proposed that learn prediction on images [12, 23], but they have either used synthetic images or instance-level models, and have not demonstrated generalization to novel objects nor accurate prediction on real-world videos.", "startOffset": 112, "endOffset": 120}, {"referenceID": 22, "context": "In the domain of reinforcement learning, model-based methods have been proposed that learn prediction on images [12, 23], but they have either used synthetic images or instance-level models, and have not demonstrated generalization to novel objects nor accurate prediction on real-world videos.", "startOffset": 112, "endOffset": 120}, {"referenceID": 16, "context": "As shown by our comparison to LSTM-based prediction designed for Atari frames [17], models that work well on synthetic domains do not necessarily succeed on real images.", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "Video datasets: Existing video datasets capture YouTube clips [10], human motion [8], synthetic video game frames [17], and driving [6].", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "Video datasets: Existing video datasets capture YouTube clips [10], human motion [8], synthetic video game frames [17], and driving [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 16, "context": "Video datasets: Existing video datasets capture YouTube clips [10], human motion [8], synthetic video game frames [17], and driving [6].", "startOffset": 114, "endOffset": 118}, {"referenceID": 5, "context": "Video datasets: Existing video datasets capture YouTube clips [10], human motion [8], synthetic video game frames [17], and driving [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 8, "context": "Spatial Transformer Predictors (STP): In this approach, the model produces multiple sets of parameters for 2D affine image transformations, and applies the transformations using a bilinear sampling kernel [9].", "startOffset": 205, "endOffset": 208}, {"referenceID": 13, "context": "Most existing physics and video prediction models use feedforward architectures [14, 13] or feedforward encodings of the image [17].", "startOffset": 80, "endOffset": 88}, {"referenceID": 12, "context": "Most existing physics and video prediction models use feedforward architectures [14, 13] or feedforward encodings of the image [17].", "startOffset": 80, "endOffset": 88}, {"referenceID": 16, "context": "Most existing physics and video prediction models use feedforward architectures [14, 13] or feedforward encodings of the image [17].", "startOffset": 127, "endOffset": 131}, {"referenceID": 23, "context": "To generate the motion predictions discussed above, we employ stacked convolutional LSTMs [24].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "Alternative losses, such as those presented in [14] could complement this method.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "6M dataset [8], which consists of videos of human motion.", "startOffset": 11, "endOffset": 14}, {"referenceID": 13, "context": "In both settings, we evaluate our three models described in Section 3, as well as prior models [14, 17].", "startOffset": 95, "endOffset": 103}, {"referenceID": 16, "context": "In both settings, we evaluate our three models described in Section 3, as well as prior models [14, 17].", "startOffset": 95, "endOffset": 103}, {"referenceID": 10, "context": "Training details: We trained all models using the TensorFlow library, optimizing to convergence using ADAM [11] with the suggested hyperparameters.", "startOffset": 107, "endOffset": 111}, {"referenceID": 1, "context": "For all recurrent models, we trained with and without scheduled sampling [2] and report the performance of the model with the best validation error.", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": "Evaluation: Prior work has proposed to evaluate video prediction using peak signal to noise ratio (PSNR) and structured similarity index (SSIM) [14].", "startOffset": 144, "endOffset": 148}, {"referenceID": 16, "context": "Figure 3: Qualitative and quantitative reconstruction performance of our models, compared with [17, 14].", "startOffset": 95, "endOffset": 103}, {"referenceID": 13, "context": "Figure 3: Qualitative and quantitative reconstruction performance of our models, compared with [17, 14].", "startOffset": 95, "endOffset": 103}, {"referenceID": 13, "context": "All models were trained for 8-step prediction, except [14], which is trained for 1-step prediction.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "To quantitatively evaluate the predictions, we measure average PSNR and SSIM, as proposed in [14].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Unlike [14], we measure all metrics on the entire image.", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "We report the performance of the feedforward multiscale model of [14] using an l1+GDL loss, which was the best performing model in our experiments \u2013 full results of the multi-scale models are in Appendix C.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "The FC LSTM model [17] reconstructs the background, lacking the representational power to reconstruct the objects in the bin.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "as in [5]).", "startOffset": 6, "endOffset": 9}], "year": 2016, "abstractText": "A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 50, 000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot\u2019s future actions amounts to learning a \u201cvisual imagination\u201d of different futures based on different courses of action. Our experiments show that our proposed method not only produces more accurate video predictions, but also more accurately predicts object motion, when compared to prior methods.", "creator": "LaTeX with hyperref package"}}}