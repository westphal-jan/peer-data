{"id": "1605.07221", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Global Optimality of Local Search for Low Rank Matrix Recovery", "abstract": "We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements. With noisy measurements we show all local minima are very close to a global optimum. Together with a curvature bound at saddle points, this yields a polynomial time global convergence guarantee for stochastic gradient descent {\\em from random initialization}.", "histories": [["v1", "Mon, 23 May 2016 22:05:42 GMT  (1060kb,D)", "https://arxiv.org/abs/1605.07221v1", "21 pages, 3 figures"], ["v2", "Fri, 27 May 2016 00:54:17 GMT  (1061kb,D)", "http://arxiv.org/abs/1605.07221v2", "21 pages, 3 figures"]], "COMMENTS": "21 pages, 3 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["srinadh bhojanapalli", "behnam neyshabur", "nati srebro"], "accepted": true, "id": "1605.07221"}, "pdf": {"name": "1605.07221.pdf", "metadata": {"source": "CRF", "title": "Global Optimality of Local Search for Low Rank Matrix Recovery", "authors": ["Srinadh Bhojanapalli", "Behnam Neyshabur", "Nathan Srebro"], "emails": ["srinadh@ttic.edu", "bneyshabur@ttic.edu", "nati@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "It is not only the way in which people in the United States and other countries react to the question of how they have behaved, but also the way in which they have behaved, how they have behaved, and how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved, how they have behaved in the world, how they have behaved in the world, how they have behaved in the world, how they have behaved, how they have behaved in the world, how they have behaved, how they have behaved in the world, how they have behaved in the world, how they have behaved, how they have behaved in the world, how they have behaved, how they have behaved in the world, how they have behaved, how they have behaved in the world, how they have behaved in the world, how they have behaved, how they have behaved in the world, how they have behaved, how they have behaved in the world,"}, {"heading": "2 Formulation and Assumptions", "text": "We write the linear measuring operator A: Rn \u00b7 n \u2192 Rm as A (X) i = < Ai, X > where Ai Rn \u00b7 n, yi = < Ai, X \u0445 > + wi, i = 1, \u00b7 \u00b7 \u00b7, m. We assume that the value N (0, \u03c32w) i.i.d is Gaussian noise. In general, we are interested in the high-dimensional system, in which the number of measurements m is usually much smaller than the dimension n2.Even if we know that rank (X) \u2264 r, many measurements are not sufficient if they are not sufficiently \"spread.\" For example, if all measurements comprise only the first n / 2 rows and columns, we would never have any information about the lower right block. A sufficient condition for the identification of a low-level measurement X (ltr) is the accuracy of linear measurements according to right et al. [20] based on restricted isometry."}, {"heading": "3 Main Results", "text": "We are ready to present our main results (U = 1). (U = 1). (U). (U = 1). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U). (U.). (U.). (U. (U.). (U.). (U.). (U.). (U. (U.). (U.). (U.). (U.).). (U. (U.). (U.). (U.). (U.). (U.). (U.). (U. (U.).). (U. (U.). (U.).). (U.).). (U. (U.).). (U. (U.).). (U.).). (U..). (U.).). (U.). (U.). (U.).). (U.). (U.).). (U."}, {"heading": "4 Proof for the Noiseless Case", "text": "In this section, we will present the evidence that characterizes the local minimum of the problem (2). > > For simplicity of presentation, we will first present the results for the silent case (w = 0). \u2212 Evidence for the general case can be found in the appendix. \u2212 Theorem 4.1. Consider the optimization problem (2), where y = A (X), A satisfies (2r, \u03b42r) -RIP with \u03b42r < 15, and rank (X) \u2264 r. Then, for each local minimum of f (U): UU > = X. To prove this theorem, we will first discuss the implications of the first and second order of optimal conditions and then show how they are combined to produce the result.Our evidentiary techniques differ from the existing results describing local minima in the dictionary."}, {"heading": "4.1 First order optimality", "text": "First we consider the condition of the first order, after which f (U) = 0. For each stationary point of Ai results in the following result.Lemma 4.2. [Condition of the first order] For each stationary point of the first order of f (U), and A, which fulfils the (2r) -RIP (3), the following applies: \"(UU)\" QQ \"(U)\" Q \"(F).\" \"Q\" (U) \"Q\" (U). \"\" Q \"(U)\" Q \"(U).\" Q \"(U).\" \"Q\" \"\" (U). \"Q\" (U). \"Q\" (U). \"Q\" (U). \"Q\" (U). \"(U)\" Q \"(U).\" (U) \"Q.\" (U). \"(U).\" (U). \"(F). (F\" Q \"(U).\" Q \"(U). (U)."}, {"heading": "4.2 Second order optimality", "text": "We consider the second condition to indicate that the error rate along the Q-Q-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U-U"}, {"heading": "5 Necessity of RIP", "text": "We have shown that there are no false local minimums, only under a limited isometric assumption. A natural question is whether this is necessary, or whether the problem (2) never has any false local minima, perhaps similar to the non-convex PCA problem minU-rank A \u2212 UU >. \"A good indicator that this is not the case is that (2) NP is hard, even in the silent case when y = A (X-point) for rank (X-point) k [20] (if we do not need RIP, we can consider any Ai to be non-zero on a single entry, where (2) becomes a matrix completion problem for which the hardness has been shown even under fairly favorable conditions [13]. This is unlikely that we have a poly-time algorithm that is successful for any linear measurement operator. Although this formally rules out the possibility that there are no false local minima, but it takes a very long time for us to find such a scenario."}, {"heading": "6 Conclusion", "text": "We have found that the non-convex formulation of matrix sensors (2) does not have false local minimums (or, in the noisy and approximate environments, at least not outside a small radius around a global minimum), and we can obtain theoretical guarantees of the success of their optimization by random initialization, which is consistent with the methods commonly used in practice and can explain their success. This guarantee is very different in nature from other recent work on nonconvex optimization of low-ranking problems, which relies heavily on initialization to come close to the global optimum, and on the local search for final local convergence with the global optimum. We believe that this is the first result, along with the parallel work of Ge et al. [11], on the global convergence of the local search for common border-related problems that are the worst."}, {"heading": "Acknowledgements", "text": "The authors thank Afonso Bandeira for the discussions, Jason Lee and Tengyu Ma for sharing and discussing their work. This research was supported in part by an NSF RI-AF Award and Intel ICRI-CI."}, {"heading": "A Numerical Simulations", "text": "In this section, we present simulation results for performing the gradient descent over f (U). We consider measurements yi = < Ai, X \u0445 > in which Ai is i.i.d Gaussian, with each entry distributed as N (0, 1 / m). X * is a 100 x 100 rank r random p.s.d matrix with the series \"X\" and \"F = 1. r varied in the experiments from 1 to 20. We consider both standard gradient descent and noisy gradient descent (4) with step size 1\" U. \"We add noise of strength 1e \u2212 4 for the noisy gradient updates. Each method is executed until convergence (max. 200 iterations). Let us leave the output of the gradient descent as U.\" A run of this experiment is considered successful if the final error \"U,\" U. \""}, {"heading": "B Proof for the Noisy Case", "text": "In this section we present evidence that the first order of the problem (2). (2) > Q = > Q = > Q (2). (2) Q (2) Q (2) Q (2) Q (2) Q (2) Q (2) Q (2). (3) Q (2) Q (2) Q (2). (3) Q (3) Q (3) Q (3) Q (2). (3) Q (2). (3) Q (2) Q (2). (3) Q (2). (3) F). (3) F.). (4). (4). (4) F.). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5)."}, {"heading": "C Proof for the High Rank Case", "text": "In this section, we will provide proof of the inexact case in which we are followed by precedence (X)."}, {"heading": "D Proofs for Section 3", "text": "In this section we present the evidence for the strict saddle theory (> > Q) (> Q) (> Q) and the convergence guarantees (theorem 3,3), using the Lemmas developed in Section 4 and the supporting Lemmas from Section E.Proof Theorem 3,2. From Lemma 4,3 we know that the convergence guarantees (theorem 3,3) > [1 m \u00b2 f (U) \u2212 2 m \u00b2 f (U) vec (1 m \u00b2) = 1 m \u00b2 m \u00b2 i = 1 (R \u00b2) 4 Ai, U \u00b2 m \u00b2 (UU \u00b2)."}, {"heading": "E Supporting Lemmas", "text": "In this section we will present the results of the SVD > U. Afterwards we will expand the QS (QS) on both sides in QS and QS (QS). QS (QS) - QS (QS) - QS (QS) - QS (QS) - QS (QS) - QS (QS) - QS (QS) - QS (QS) - QS (QS) - QS (QS) - QS (QS) - QS (QS) - QS (QS) - QS (QS)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix<lb>recovery from incoherent linear measurements. With noisy measurements we show all local minima are very close to a<lb>global optimum. Together with a curvature bound at saddle points, this yields a polynomial time global convergence<lb>guarantee for stochastic gradient descent from random initialization.", "creator": "LaTeX with hyperref package"}}}