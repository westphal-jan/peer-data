{"id": "1605.07252", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Interaction Screening: Efficient and Sample-Optimal Learning of Ising Models", "abstract": "We consider the problem of learning the underlying graph of an unknown Ising model on p spins from a collection of i.i.d. samples generated from the model. We suggest a new estimator that is computationally efficient and requires a number of samples that is near-optimal with respect to previously established information-theoretic lower-bound. Our statistical estimator has a physical interpretation in terms of \"interaction screening\". The estimator is consistent and is efficiently implemented using convex optimization. We prove that with appropriate regularization, the estimator recovers the underlying graph using a number of samples that is logarithmic in the system size p and exponential in the maximum coupling-intensity and maximum node-degree.", "histories": [["v1", "Tue, 24 May 2016 01:36:48 GMT  (81kb,D)", "http://arxiv.org/abs/1605.07252v1", null], ["v2", "Mon, 14 Nov 2016 03:00:29 GMT  (81kb,D)", "http://arxiv.org/abs/1605.07252v2", "To be published in Advances in Neural Information Processing Systems 30"], ["v3", "Mon, 19 Dec 2016 13:32:25 GMT  (81kb,D)", "http://arxiv.org/abs/1605.07252v3", "To be published in Advances in Neural Information Processing Systems 30"]], "reviews": [], "SUBJECTS": "cs.LG cond-mat.stat-mech cs.IT math.IT math.ST stat.ML stat.TH", "authors": ["marc vuffray", "sidhant misra", "andrey y lokhov", "michael chertkov"], "accepted": true, "id": "1605.07252"}, "pdf": {"name": "1605.07252.pdf", "metadata": {"source": "CRF", "title": "Interaction Screening: Efficient and Sample-Optimal Learning of Ising Models", "authors": ["Marc Vuffray", "Sidhant Misra", "Andrey Lokhov", "Michael Chertkov"], "emails": ["chertkov}@lanl.gov"], "sections": [{"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Main Results", "text": "Consider a diagram G = (V, E) with p-vertexes, where V = {1,.., p} is the vertext set and E = V \u00b7 V is the undirected edge set. Vertexes i = V are associated with binary random variables \u03c3i = {\u2212 1, + 1}, which are called spins. Edges (i, j) and E are associated with unequal real parameters, which are called couplings. An output model is a probability distribution \u00b5 over spin configurations, which reads as follows: \u00b5 (i, j) = 1Z exp \u0445 (i, j)."}, {"heading": "2.1 Structure-Learning of Ising Models", "text": "Let us assume that n sequences / samples of p-spins {\u03c3 (k)} k = 1,..., n are observed. Let us assume that each observed spin configuration \u03c3 (k) = {\u03c3 (k) 1,.., \u03c3 (k) p} i.i.d. (1). Based on these measurements / samples, we aim to construct a marginal quantity estimator that reconstructs the structure with high probability, i.e. P [E] = 1 \u2212, (3), where \"0, 12\" is a prescribed reconstruction error. We are interested in learning structures of the issuing models in a high-dimensional system in which the number of observations / samples is in the order n = O (ln p). A necessary condition for the number of samples is given in [21, Thm. 1], this condition explicitly depends on the smallest and greatest coupling intensity d: min = (j)."}, {"heading": "2.2 Regulrized Interaction Screening Estimator", "text": "The main contribution of this thesis is to present a structure-learning algorithm that is of low complexity and almost optimal in terms of boundaries (6) and (7). Our algorithm reconstructs the structure of the issuing model exactly as indicated in Eq. (3) Our algorithm consists of two steps: first, we estimate the number of samples that lie near each node, and second, we assume that the estimated target values are sufficiently small to zero, resulting in the absence of the corresponding edge."}, {"heading": "3 Analysis", "text": "The Regularized Interaction Screening Estimator (9) belongs to the class of so-called regulated M-estimators. Naghaband et al. have proposed in [22] a framework for analyzing the square error of such estimators. Condition 1 is that the enforcement of only two conditions for the loss function is sufficient to get a handle on the square error of a \"1-regulated M-estimator.\" The first condition links the choice of the penalty parameter with the gradient of the objective function. (13) Condition 1 guarantees that the \"1-penalty parameter forces a regulation strongly if it is greater than all subderivatives of the objective function at that time."}, {"heading": "3.1 Gradient Concentration", "text": "Like the ISO model (8), its gradient is in each component l = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0"}, {"heading": "3.2 Restricted Strong-Convexity", "text": "The remainder of the first order Taylor expansion of the ISO, defined in Eq. (15) reads\u03b4Sn (3): 1 + 1 (3), 1 (3), 1 (4), 1 (4), 1 (4), 2 (4), 2 (4), 3 (4), 4 (5), 4 (5), 5 (5), 5 (5), 5 (5), 5 (5), 5 (5), 5 (5), 6 (5), 6 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5 (5), 7 (5), 7 (5), 7 (5 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5, 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5), 7 (5)"}, {"heading": "3.3 Proof of the main Theorems", "text": "The proof of theorem 1 (square error of RISE). We try to apply theorem 1 to the Regulated Interaction Screening Estimator (9), using 3 = 2 13 in Lemma 4 and taking into account the fact that condition 1 is met with a probability greater than 1 \u2212 2 1 / 3 whenever n \u2265 (9d) 5 e6\u03b2d ln 3p 1. Using 4 = 1 / 3 in Lemma 7 and observing that 3d\u03bb (e \u2212 3\u03b2d4 (d + 1)) \u2212 1 \u2264 1, for n \u2265 (9d) 5 e6\u03b2d ln p 1, we conclude that condition 2 follows with a probability greater than 1 \u2212 13. Theorem 1 follows by using a composite bond and then the sentence 1. The proof of theorem 2 becomes an immediate application of theorem 1. Evidence of theorem 2 (structural threshold of the issing of models). Theorem 1 follows by using a composite bond and then the probability of 2 and then the probability of 1 \u00b0 at most (1 and 1)."}, {"heading": "4 Numerical Results", "text": "We test the performance of the Struct-RISE, with the strength of the l1 regularization parameterization of \u03bb = 4 \u221a ln (3p2 /) n, on Ising models over a two-dimensional grid with periodic boundary conditions (i.e. degree of each node in the diagram is 4).We are interested in finding the minimum number of samples, nmin, so that the graph is perfectly reconstructed with the probability of 1 \u2212 \u2265 0.95. In our numerical experiments, we recover the value of nmin as the minimum n, for which Struct-RISE returns the perfect structure 45 times out of 45 different experiments with n samples, guaranteeing that the probability of a perfect reconstruction is greater than 0.95 with a statistical certainty of about 90%. We first check the logarithmic scaling of nmin with respect to the number of spins p. The coupling parametry risk becomes consistent with the magnetic structure p."}, {"heading": "5 Conclusions and Path Forward", "text": "In this paper, we construct and analyze the Regularized Interaction Screening Estimator (9). We show that the estimator is computationally efficient and requires an optimal number of samples to learn Ising Models.The RISE estimator does not require prior knowledge of the model parameters for implementation and is based on minimizing the loss function (8), which we call the Interaction Screening Objective. The \"Interaction Screening\" approach presented here is an empirical average (across samples) of an object designed to filter a single spin / variable from its factor diagram neighborhoods. Although we focus in this paper exclusively on learning pairs of binary models, the \"Interaction Screening\" approach presented here extends to learning other graphical models, including those beyond higher (discrete, continuous, or mixed) alphabets and the inclusion of highly ordered (above-general) interactions, which we have gone beyond the actual interpairing idea to determine."}, {"heading": "Acknowledgment", "text": "We thank Guy Bresler and Andrea Montanari for valuable discussions, comments and insights. This work was conducted under the auspices of the National Nuclear Security Administration of the U.S. Department of Energy at the Los Alamos National Laboratory under contract number DE-AC52-06NA25396 and was partially supported by the DTRA Basic Research Project # 10027-13399 and the Advanced Grid Modeling Program of the U.S. Department of Energy."}], "references": [{"title": "Wisdom of crowds for robust gene network inference", "author": ["D. Marbach", "J.C. Costello", "R. Kuffner", "N.M. Vega", "R.J. Prill", "D.M. Camacho", "K.R. Allison", "M. Kellis", "J.J. Collins", "G. Stolovitzky"], "venue": "Nat Meth, vol. 9, pp. 796\u2013804, Aug 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Direct-coupling analysis of residue coevolution captures native contacts across many protein families", "author": ["F. Morcos", "A. Pagnani", "B. Lunt", "A. Bertolino", "D.S. Marks", "C. Sander", "R. Zecchina", "J.N. Onuchic", "T. Hwa", "M. Weigt"], "venue": "Proceedings of the National Academy of Sciences, vol. 108, no. 49, pp. E1293\u2013E1301, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Weak pairwise correlations imply strongly correlated network states in a neural population", "author": ["E. Schneidman", "M.J. Berry", "R. Segev", "W. Bialek"], "venue": "Nature, vol. 440, pp. 1007\u20131012, Apr 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Fields of experts: a framework for learning image priors", "author": ["S. Roth", "M.J. Black"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, vol. 2, pp. 860\u2013867 vol. 2, June 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Inferring friendship network structure by using mobile phone data", "author": ["N. Eagle", "A.S. Pentland", "D. Lazer"], "venue": "Proceedings of the National Academy of Sciences, vol. 106, no. 36, pp. 15274\u2013 15278, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "A dependency graph approach for fault detection and localization towards secure smart grid", "author": ["M. He", "J. Zhang"], "venue": "IEEE Transactions on Smart Grid, vol. 2, pp. 342\u2013351, June 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Structure learning and statistical estimation in distribution networks", "author": ["D. Deka", "S. Backhaus", "M. Chertkov"], "venue": "submitted to IEEE Control of Networks; arXiv:1501.04131; arXiv:1502.07820, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C. Chow", "C. Liu"], "venue": "IEEE Transactions on Information Theory, vol. 14, pp. 462\u2013467, May 1968.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1968}, {"title": "First-order methods for sparse covariance selection", "author": ["A. d\u2019Aspremont", "O. Banerjee", "L.E. Ghaoui"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 30, no. 1, pp. 56\u201366, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning planar ising models", "author": ["J. Johnson", "D. Oyen", "P. Netrapalli", "M. Chertkov"], "venue": "Journal of Machine Learning, in press; arXiv:1502.00916, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Mean-field theory of Boltzmann machine learning", "author": ["T. Tanaka"], "venue": "Phys. Rev. E, vol. 58, pp. 2302\u2013 2310, Aug 1998. 14", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient learning in Boltzmann machines using linear response theory", "author": ["H.J. Kappen", "F. d. B. Rodr\u00edguez"], "venue": "Neural Computation, vol. 10, no. 5, pp. 1137\u20131156, 1998.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Ising model for neural data: Model quality and approximate methods for extracting functional connectivity", "author": ["Y. Roudi", "J. Tyrcha", "J. Hertz"], "venue": "Phys. Rev. E, vol. 79, p. 051915, May 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1915}, {"title": "The Bethe approximation for solving the inverse Ising problem: a comparison with other inference methods", "author": ["F. Ricci-Tersenghi"], "venue": "Journal of Statistical Mechanics: Theory and Experiment, vol. 2012, no. 08, p. P08015, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Hardness of parameter estimation in graphical models", "author": ["G. Bresler", "D. Gamarnik", "D. Shah"], "venue": "Advances in Neural Information Processing Systems 27 (Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, eds.), pp. 1062\u20131070, Curran Associates, Inc., 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Computational implications of reducing data to sufficient statistics", "author": ["A. Montanari"], "venue": "Electron. J. Statist., vol. 9, no. 2, pp. 2370\u20132390, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "High-dimensional Ising model selection using `1-regularized logistic regression", "author": ["P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty"], "venue": "Ann. Statist., vol. 38, pp. 1287\u20131319, 06 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Which graphical models are difficult to learn", "author": ["A. Montanari", "J.A. Pereira"], "venue": "Advances in Neural Information Processing Systems 22 (Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, eds.), pp. 1303\u20131311, Curran Associates, Inc., 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Reconstruction of Markov random fields from samples: Some observations and algorithms", "author": ["G. Bresler", "E. Mossel", "A. Sly"], "venue": "SIAM Journal on Computing, vol. 42, no. 2, pp. 563\u2013578, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficiently learning Ising models on arbitrary graphs", "author": ["G. Bresler"], "venue": "Proceedings of the Forty- Seventh Annual ACM on Symposium on Theory of Computing, pp. 771\u2013782, ACM, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Information-theoretic limits of selecting binary graphical models in high dimensions", "author": ["N.P. Santhanam", "M.J. Wainwright"], "venue": "IEEE Transactions on Information Theory, vol. 58, pp. 4117\u2013 4134, July 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "A unified framework for highdimensional analysis of M -estimators with decomposable regularizers", "author": ["S.N. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Statist. Sci., vol. 27, pp. 538\u2013557, 11 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast global convergence of gradient methods for high-dimensional statistical recovery", "author": ["A. Agarwal", "S. Negahban", "M.J. Wainwright"], "venue": "Ann. Statist., vol. 40, pp. 2452\u20132482, 10 2012. 15", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 139, "endOffset": 142}, {"referenceID": 2, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 179, "endOffset": 182}, {"referenceID": 4, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 5, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 220, "endOffset": 226}, {"referenceID": 6, "context": "Unsurprisingly, GM reconstruction plays an important role in various fields such as the study of gene expression [1], protein interactions [2], neuroscience [3], image processing [4], sociology [5] and even grid science [6, 7].", "startOffset": 220, "endOffset": 226}, {"referenceID": 7, "context": "The origin of the GM reconstruction problem is traced back to the seminal 1968 paper by Chow and Liu [8], where the problem was posed and resolved for the special case of tree-structured GMs.", "startOffset": 101, "endOffset": 104}, {"referenceID": 8, "context": "However, it is also known that in the case of general graphs with cycles, maximum likelihood estimators are intractable as they require computation of the partition function of the underlying GM, with notable exceptions of the Gaussian GM, see for instance [9], and some other special cases, like planar Ising models without magnetic field [10].", "startOffset": 257, "endOffset": 260}, {"referenceID": 9, "context": "However, it is also known that in the case of general graphs with cycles, maximum likelihood estimators are intractable as they require computation of the partition function of the underlying GM, with notable exceptions of the Gaussian GM, see for instance [9], and some other special cases, like planar Ising models without magnetic field [10].", "startOffset": 340, "endOffset": 344}, {"referenceID": 10, "context": "utilizing empirical correlation matrices [11, 12, 13, 14].", "startOffset": 41, "endOffset": 57}, {"referenceID": 11, "context": "utilizing empirical correlation matrices [11, 12, 13, 14].", "startOffset": 41, "endOffset": 57}, {"referenceID": 12, "context": "utilizing empirical correlation matrices [11, 12, 13, 14].", "startOffset": 41, "endOffset": 57}, {"referenceID": 13, "context": "utilizing empirical correlation matrices [11, 12, 13, 14].", "startOffset": 41, "endOffset": 57}, {"referenceID": 14, "context": "This observation is not surprising in light of recent results stating that learning the structure of Ising models using only their correlation matrix is, in general, computationally intractable [15, 16].", "startOffset": 194, "endOffset": 202}, {"referenceID": 15, "context": "This observation is not surprising in light of recent results stating that learning the structure of Ising models using only their correlation matrix is, in general, computationally intractable [15, 16].", "startOffset": 194, "endOffset": 202}, {"referenceID": 16, "context": "Among methods that do not rely solely on correlation matrices but take advantage of higherorder correlations that can be estimated from samples, we mention the so-called regularized pseudolikelihood estimator [17].", "startOffset": 209, "endOffset": 213}, {"referenceID": 17, "context": "It was also proven that this estimator fails to reconstruct the structure of graphs with long-range correlations, even for simple test cases [18].", "startOffset": 141, "endOffset": 145}, {"referenceID": 18, "context": "Bresler, Mossel and Sly in [19] suggested an algorithm which reconstructs the graph without errors in polynomial time.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "Prior to the work reported in this manuscript the best known procedure for perfect reconstruction of an Ising model was through a greedy algorithm proposed by Bresler in [20].", "startOffset": 170, "endOffset": 174}, {"referenceID": 20, "context": "This scaling is rather far from the information-theoretic lower-bound reported in [21] predicting instead a single exponential dependency on the two aforementioned quantities.", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "The algorithm is near-optimal in the sense that the number of samples required to achieve perfect reconstruction, and the run time, scale exponentially with respect to the maximum node-degree and the maximum coupling intensity, thus matching parametrically the information-theoretic lower bound of [21].", "startOffset": 298, "endOffset": 302}, {"referenceID": 21, "context": "Our statistical estimator has the structure of a consistent M-estimator [22] and is implemented via convex optimization.", "startOffset": 72, "endOffset": 76}, {"referenceID": 20, "context": "According to [21], in order to reconstruct the structure of the Ising model with minimum coupling intensity \u03b1, maximum coupling intensity \u03b2, and maximum degree d, the required number of samples should be at least n \u2265 max \uf8eb\uf8ede\u03b2d ln ( pd 4 \u2212 1 )", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "Unfortunately, the existence proof presented in [21] is non-constructive and thus it does not guarantee actual existence of an algorithm with low computational complexity.", "startOffset": 48, "endOffset": 52}, {"referenceID": 22, "context": "We believe that this running time estimate can actually be reduced to a quasi-quadratic one by using first-order minimization techniques, in the spirit of [23].", "startOffset": 155, "endOffset": 159}, {"referenceID": 21, "context": "proposed in [22] a framework to analyze the square error of such estimators.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "As per [22], enforcing only two conditions on the loss function is sufficient to get a handle on the square error of an `1-regularized M-estimator.", "startOffset": 7, "endOffset": 11}], "year": 2017, "abstractText": "We consider the problem of learning the underlying graph of an unknown Ising model on p spins from a collection of i.i.d. samples generated from the model. We suggest a new estimator that is computationally efficient and requires a number of samples that is near-optimal with respect to previously established information-theoretic lower-bound. Our statistical estimator has a physical interpretation in terms of \u201cinteraction screening\u201d. The estimator is consistent and is efficiently implemented using convex optimization. We prove that with appropriate regularization, the estimator recovers the underlying graph using a number of samples that is logarithmic in the system size p and exponential in the maximum coupling-intensity and maximum node-degree.", "creator": "LaTeX with hyperref package"}}}