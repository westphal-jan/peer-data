{"id": "1605.07262", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Measuring Neural Net Robustness with Constraints", "abstract": "Despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore, we show how existing approaches to improving robustness \"overfit\" to adversarial examples generated using a specific algorithm. Finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics.", "histories": [["v1", "Tue, 24 May 2016 02:18:21 GMT  (719kb,D)", "http://arxiv.org/abs/1605.07262v1", null], ["v2", "Fri, 16 Jun 2017 11:58:51 GMT  (817kb,D)", "http://arxiv.org/abs/1605.07262v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["osbert bastani", "yani ioannou", "leonidas lampropoulos", "dimitrios vytiniotis", "aditya v nori", "antonio criminisi"], "accepted": true, "id": "1605.07262"}, "pdf": {"name": "1605.07262.pdf", "metadata": {"source": "CRF", "title": "Measuring Neural Net Robustness with Constraints", "authors": ["Osbert Bastani", "Yani Ioannou", "Leonidas Lampropoulos"], "emails": ["obastani@cs.stanford.edu", "yai20@cam.ac.uk", "llamp@seas.upenn.edu", "dimitris@microsoft.com", "adityan@microsoft.com", "antcrim@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The latest paper [20] shows that it is often possible to construct an input pattern that is abbreviated by a neural network by restricting a correctly described input in a carefully chosen direction. [17] A number of approaches have since been proposed to improve robustness [6, 5, 7, 19]. However, work in this direction has been hampered by the lack of objective measures of robustness. [17] A typical approach to improving robustness is proposed to find adversarial examples and train the training with these examples. [5] Then, robustness is evaluated by using the same algorithms."}, {"heading": "1.1 Related work", "text": "The vulnerability of neural networks to adversarial examples was perceived by [20] as almost obvious. [20] In the face of a test point x \"with predicted label,\" [20] is an addressed example of the applicability of [20], [20] and [30], [30] and [30], [40] and [50], [50] and [50], [50] and [50], [50] and [50], [50] and [50], [50] and [60], [50 and [60], [50 and [60], [50 and [60], [60] and [60], and [60], and [60], and [60], and [20]."}, {"heading": "2 Robustness Metrics", "text": "We assume that our two statistics depend on a parameter that captures the idea that we can only care about robustness below a certain threshold - we first ignore adverse examples x whose L-spacing to x is greater than. We use = 20 in our experiments on MNIST and CIFAR-10 (on the pixel scale 0-255). Pointwise robust examples x whose L-spacing to x is greater than. We use = 20 in our experiments on MNIST and CIFAR-10 (on the pixel scale 0-255). Pointwise robustness is intuitive, f is robust on x-X if a \"small\" perturbation to x does not affect the assigned designation. We are interested in perturbations that have no effect on human classification; an established condition is that it has x-x-x-x-x for some parameters."}, {"heading": "3 Computing Pointwise Robustness", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Overview", "text": "In order to classify these data, we train a two-layer neural network f (x) = \u043c, \"using the ReLU function g pointwise. Figure 1 (a) contains contours of the pro-point loss function of this neural network. We exhaustively search the entrance space to determine the distance \u03c1 (f, x) to the next adverse example of input x \u0445 (labeled) is intractable. Let us remember that neural networks with rectified linear units (ReLU) as activations are piecewise linear [15]. Since there are contradictory examples of this linearity in the neural network [5], we limit our search to the region Z (x) on which the neural network is linear."}, {"heading": "3.2 Formulation as Optimization", "text": "We calculate \u03c1 (f,) by expressing (1) as constraints C resulting from 1Our neural network has 8 hidden units, but for this reason 6 of the half-spaces fully contain the convex region. \u2022 Linear relations; in particular inequalities C \u2261 (wTx + b \u2265 0) and equalities C \u2261 (wTx + b = 0), where x-Rm (for some m) are variables and w-Rm, b-R are constants. \u2022 Conjunctions C \u2261 C1-C2, where C1 and C2 are constraints themselves. Both constraints must be met in order for the conjunction to be satisfied. \u2022 Disjunctions C \u0445 C1-C2, where C1 and C2 are constraints themselves. One of the constraints must be met in order for the disjunction to be satisfied."}, {"heading": "3.3 Encoding a Neural Network", "text": "We show how to encode the constraint f (x) = \"as constraints Cf (x) (x \u2212 ni) (if f is a neural mesh). We assume that f has the form f (x) = argmax '(f (k) (f (k) (f (k \u2212 1) (... (f (1) (x))),\" where the ith layer of the network is a function f (i): Rni \u2212 1 \u2192 Rni, with n0 = n and nk = | L |. We describe the encoding of fully connected and ReLU layers (f); convolutionary layers are encoded similarly to fully connected layers, and max-pooling layers are encoded similarly to ReLU layers. We introduce the variables x (0),.."}, {"heading": "4 Approximate Computation of Pointwise Robustness", "text": "The challenge in solving (3) is the non-convex of the realizable unit of Cf (x, \"). In order to restore tractability, we need (3) the realizable options (x,\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" x, \"\" \"x,\" \"x,\" \"x,\" \"\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"\" x, \"\" \"x,\" \"\" x, \"\" \"x,\" \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"\" x, \"x,\" x, \"\" x, \"x,\" \"x,\" x, \"\" x, \"\" x, \"x,\" x, \"\" x, \"x,\" x, \"\" x, \"x,\" x, \"x,\" x, \"x,\" \"x,\" x, \"x,\" x, \"\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \",\" x, \"x,\" \"x,\" x, \"x,\", \"x,\" x, \"x,\" x, \"x,\" \"x,\" x, \"\" \"x,\", \"\" \"x,"}, {"heading": "5 Improving Neural Net Robustness", "text": "We can use our algorithm to calculate examples of opponents (f, x), to calculate examples of opponents (f, x).Fine tuning we use to reduce the susceptibility of a neural network to examples of opponents. First, we use an algorithm A to calculate examples of opponents for each x-row of opponents (f, x).Fine tuning we use to add them to the training set. Then we train the network on an extended training set with reduced training rate. We can repeat this process several rounds (T); in each round we consider only x \u00b2 in the original training set of opponents (instead of the expanded training set of opponents).Rounding errors are presented as a whole, so we have to circumnavigate the problem to get a picture of opponents, which we find all results of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents of opponents."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Adversarial Images for CIFAR-10 and MNIST", "text": "We find contrary examples for the neural network LeNet [12] (modified to use ReLUs instead of sigmoids), which was trained for the classification of MNIST [11], and for the neural network Network Network-in-Network (NiN), which is trained for the classification of CIFAR-10 [9]. Both neural networks are trained with Caffe [8]. For MNIST, Figure 2 (b) shows a contrary example (labeled with 1), which we find for the image in Figure 2 (a) labeled with 3, and Figure 2 (c) shows the corresponding contrarian disturbance, which is scaled so that the difference is visible (it has the standard 17). For CIFAR-10, Figure 2 (e) shows a contrarian example labeled with the designation \"LKW\" for the image in Figure 2 (d) labeled with the designation \"automotive,\" and Figure 2 shows the corresponding scale (L)."}, {"heading": "6.2 Comparison to Other Algorithms on MNIST", "text": "We compare our algorithm for estimating the baseline L-BFGS-B algorithm with the results of [20]. We use the tool provided by [21] to calculate this baseline. We use the adversarial target label \"= f\" (x \") for both algorithms. We use LeNet in our comparisons because we find that it is much more robust than the neural networks considered in most previous work (including [20]). We also use the fine-tuning of LeNet using both our algorithm and the baseline with T = 1. To focus on the most serious adversarial examples, we use a stricter threshold for the robustness of = 20 pixels. We performed a similar comparison with the proposed signed gradient algorithm by [5] (with the signed gradient multiplied by = 20 pixels). For LeNet, this algorithm found only an adversarial example of the MIST test results (four) and the 10,000 NIST test results."}, {"heading": "6.3 Scaling to CIFAR-10", "text": "We have also implemented our approach to the CIFAR-10 Network-in-Network (NiN) Neural Network [13], which achieves an accuracy of 91.31% of the test set. Calculating a single input to NiN takes about 10-15 seconds on an 8-core CPU. Unlike LeNet, NiN suffers from adverse examples - we measure a 61.5% accuracy of the test set and a contrary strength of 2.82 pixels. Our Neural Network (NiN fine-tuning with our algorithm and T = 1) has an accuracy of 90.35%, which is similar to the accuracy of the original NiN. As can be seen in Figure 3 (c), our neural network slightly improves in terms of robustness, especially with smaller ones. As before, these improvements are reflected in our metrics - the opposite frequency of our neural network drops slightly to 59.6%, and the resilience of our network improves markedly when we assume that fine tuning is required in contrast to 88."}, {"heading": "7 Conclusion", "text": "We have shown how to formulate, efficiently estimate and improve the robustness of neural networks by encoding the robustness property as a constraint system. Future work will include developing better approaches to improve the robustness of large neural networks such as NiN and studying properties that go beyond robustness. 2Furthermore, the algorithm for the signed gradient cannot be used to estimate the severity of contradictions, since all contradictory examples it finds have the L \u221e standard."}], "references": [{"title": "Visual causal feature learning", "author": ["K. Chalupka", "P. Perona", "F. Eberhardt"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Analysis of classifers\u2019 robustness to adversarial perturbations", "author": ["A. Fawzi", "O. Fawzi", "P. Frossard"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Ensemble robustness of deep learning algorithms", "author": ["Jiashi Feng", "Tom Zahavy", "Bingyi Kang", "Huan Xu", "Shie Mannor"], "venue": "arXiv preprint arXiv:1602.02389,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["Amir Globerson", "Sam Roweis"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S. Gu", "L. Rigazio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Learning with a strong adversary", "author": ["Ruitong Huang", "Bing Xu", "Dale Schuurmans", "Csaba Szepesv\u00e1ri"], "venue": "CoRR, abs/1511.03034,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Learning multiple layers of features from tiny", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Intelligent Signal Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Ken Nakae", "Shin Ishii"], "venue": "stat, 1050:25,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F. Mont\u00fafar", "Razvan Pascanu", "KyungHyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Adversarial manipulation of deep representations", "author": ["Sara Sabour", "Yanshuai Cao", "Fartash Faghri", "David J Fleet"], "venue": "arXiv preprint arXiv:1511.05122,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Understanding adversarial training: Increasing local stability of neural nets through robust optimization", "author": ["Uri Shaham", "Yutaro Yamada", "Sahand Negahban"], "venue": "arXiv preprint arXiv:1511.05432,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Exploring the space of adversarial images", "author": ["Pedro Tabacof", "Eduardo Valle"], "venue": "CoRR, abs/1510.05328,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Robustness and generalization", "author": ["Huan Xu", "Shie Mannor"], "venue": "Machine learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "Recent work [20] shows that it is often possible to construct an input mislabeled by a neural net by perturbing a correctly labeled input by a tiny amount in a carefully chosen direction.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "Lack of robustness can be problematic in a variety of settings, such as changing camera lens or lighting conditions, successive frames in a video, or adversarial attacks in security-critical applications [17].", "startOffset": 204, "endOffset": 208}, {"referenceID": 5, "context": "A number of approaches have since been proposed to improve robustness [6, 5, 1, 7, 19].", "startOffset": 70, "endOffset": 86}, {"referenceID": 4, "context": "A number of approaches have since been proposed to improve robustness [6, 5, 1, 7, 19].", "startOffset": 70, "endOffset": 86}, {"referenceID": 0, "context": "A number of approaches have since been proposed to improve robustness [6, 5, 1, 7, 19].", "startOffset": 70, "endOffset": 86}, {"referenceID": 6, "context": "A number of approaches have since been proposed to improve robustness [6, 5, 1, 7, 19].", "startOffset": 70, "endOffset": 86}, {"referenceID": 17, "context": "A number of approaches have since been proposed to improve robustness [6, 5, 1, 7, 19].", "startOffset": 70, "endOffset": 86}, {"referenceID": 4, "context": "A typical approach to improving the robustness of a neural net f is to use an algorithm A to find adversarial examples, augment the training set with these examples, and train a new neural net f \u2032 [5].", "startOffset": 197, "endOffset": 200}, {"referenceID": 18, "context": ", the L\u221e distance from x\u2217 to the nearest adversarial example) [20].", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "Common neural nets (specifically, those using rectified linear units as activation functions) are in fact piecewise linear functions [15]; we choose Z(x\u2217) to be the region around x\u2217 on which f is linear.", "startOffset": 133, "endOffset": 137}, {"referenceID": 4, "context": "Since the linear nature of neural nets is often the cause of adversarial examples [5], our choice of Z(x\u2217) focuses the search where adversarial examples are most likely to exist.", "startOffset": 82, "endOffset": 85}, {"referenceID": 18, "context": "We estimate \u03c1(f,x\u2217) using both our algorithm ALP and (as a baseline) the algorithm AL-BFGS introduced by [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "\u2022 We formalize the notion of pointwise robustness studied in previous work [5, 20, 6] and propose two statistics for measuring robustness based on this notion (\u00a72).", "startOffset": 75, "endOffset": 85}, {"referenceID": 18, "context": "\u2022 We formalize the notion of pointwise robustness studied in previous work [5, 20, 6] and propose two statistics for measuring robustness based on this notion (\u00a72).", "startOffset": 75, "endOffset": 85}, {"referenceID": 5, "context": "\u2022 We formalize the notion of pointwise robustness studied in previous work [5, 20, 6] and propose two statistics for measuring robustness based on this notion (\u00a72).", "startOffset": 75, "endOffset": 85}, {"referenceID": 18, "context": "The susceptibility of neural nets to adversarial examples was discovered by [20].", "startOffset": 76, "endOffset": 80}, {"referenceID": 18, "context": "Then, [20] devises an approximate algorithm for finding the smallest possible adversarial perturbation r.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "Our formalization of the robustness \u03c1(f,x\u2217) of f at x\u2217 corresponds to the notion in [20] of finding the minimal \u2016r\u2016\u221e.", "startOffset": 84, "endOffset": 88}, {"referenceID": 18, "context": "The algorithm in [20] can also be used to approximate \u03c1(f,x\u2217); we show experimentally that our algorithm is substantially more accurate than [20].", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "The algorithm in [20] can also be used to approximate \u03c1(f,x\u2217); we show experimentally that our algorithm is substantially more accurate than [20].", "startOffset": 141, "endOffset": 145}, {"referenceID": 14, "context": "There has been a range of subsequent work studying robustness; [16] devises an algorithm for finding purely synthetic adversarial examples (i.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": ", no initial image x\u2217), [21] searches for adversarial examples using random perturbations, showing that adversarial examples in fact exist in large regions of the pixel space, [18] shows that even intermediate layers of neural nets are not robust to adversarial noise, and [3] seeks to explain why neural nets may generalize well despite poor robustness properties.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": ", no initial image x\u2217), [21] searches for adversarial examples using random perturbations, showing that adversarial examples in fact exist in large regions of the pixel space, [18] shows that even intermediate layers of neural nets are not robust to adversarial noise, and [3] seeks to explain why neural nets may generalize well despite poor robustness properties.", "startOffset": 176, "endOffset": 180}, {"referenceID": 2, "context": ", no initial image x\u2217), [21] searches for adversarial examples using random perturbations, showing that adversarial examples in fact exist in large regions of the pixel space, [18] shows that even intermediate layers of neural nets are not robust to adversarial noise, and [3] seeks to explain why neural nets may generalize well despite poor robustness properties.", "startOffset": 273, "endOffset": 276}, {"referenceID": 4, "context": "Starting with [5], a major focus has been on devising faster algorithms for finding adversarial examples.", "startOffset": 14, "endOffset": 17}, {"referenceID": 9, "context": "Their idea is that adversarial examples can then be computed on-the-fly and used as training examples, analogous to data augmentation approaches typically used to train neural nets [10].", "startOffset": 181, "endOffset": 185}, {"referenceID": 4, "context": "To find adversarial examples quickly, [5] chooses the adversarial perturbation r to be in the direction of the signed gradient of loss(f(x\u2217 + r), `) with fixed magnitude.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "In this direction, [7] extends this idea to norms beyond the L\u221e norm, [6] takes the approach of [20] but fixes c, and [19] formalizes [5] as robust optimization.", "startOffset": 19, "endOffset": 22}, {"referenceID": 5, "context": "In this direction, [7] extends this idea to norms beyond the L\u221e norm, [6] takes the approach of [20] but fixes c, and [19] formalizes [5] as robust optimization.", "startOffset": 70, "endOffset": 73}, {"referenceID": 18, "context": "In this direction, [7] extends this idea to norms beyond the L\u221e norm, [6] takes the approach of [20] but fixes c, and [19] formalizes [5] as robust optimization.", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "In this direction, [7] extends this idea to norms beyond the L\u221e norm, [6] takes the approach of [20] but fixes c, and [19] formalizes [5] as robust optimization.", "startOffset": 118, "endOffset": 122}, {"referenceID": 4, "context": "In this direction, [7] extends this idea to norms beyond the L\u221e norm, [6] takes the approach of [20] but fixes c, and [19] formalizes [5] as robust optimization.", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "For example, [5] shows improved accuracy", "startOffset": 13, "endOffset": 16}, {"referenceID": 18, "context": "to adversarial examples generated using their own signed gradient method, but do not consider whether robustness increases for adversarial examples generated using more precise approaches such as [20].", "startOffset": 196, "endOffset": 200}, {"referenceID": 6, "context": "Similarly, [7] compares accuracy to adversarial examples generated using both itself and [5] (but not [20]), and [19] only considers accuracy on adversarial examples generated using their own approach on the baseline network.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "Similarly, [7] compares accuracy to adversarial examples generated using both itself and [5] (but not [20]), and [19] only considers accuracy on adversarial examples generated using their own approach on the baseline network.", "startOffset": 89, "endOffset": 92}, {"referenceID": 18, "context": "Similarly, [7] compares accuracy to adversarial examples generated using both itself and [5] (but not [20]), and [19] only considers accuracy on adversarial examples generated using their own approach on the baseline network.", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "Similarly, [7] compares accuracy to adversarial examples generated using both itself and [5] (but not [20]), and [19] only considers accuracy on adversarial examples generated using their own approach on the baseline network.", "startOffset": 113, "endOffset": 117}, {"referenceID": 5, "context": "Additionally, there has been work on designing neural network architectures [6] and learning procedures [1] that improve robustness to adversarial perturbations, though they do not obtain state-of-theart accuracy on the unperturbed test sets.", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "Additionally, there has been work on designing neural network architectures [6] and learning procedures [1] that improve robustness to adversarial perturbations, though they do not obtain state-of-theart accuracy on the unperturbed test sets.", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "There has also been work using smoothness regularization related to [5] to train neural nets, focusing on improving accuracy rather than robustness [14].", "startOffset": 68, "endOffset": 71}, {"referenceID": 12, "context": "There has also been work using smoothness regularization related to [5] to train neural nets, focusing on improving accuracy rather than robustness [14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": "Robustness has also been studied in more general contexts; [22] studies the connection between robustness and generalization, [2] establishes theoretical lower bounds on the robustness of linear and quadratic classifiers, and [4] seeks to improve robustness by promoting resiliance to deleting features during training.", "startOffset": 59, "endOffset": 63}, {"referenceID": 1, "context": "Robustness has also been studied in more general contexts; [22] studies the connection between robustness and generalization, [2] establishes theoretical lower bounds on the robustness of linear and quadratic classifiers, and [4] seeks to improve robustness by promoting resiliance to deleting features during training.", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "Robustness has also been studied in more general contexts; [22] studies the connection between robustness and generalization, [2] establishes theoretical lower bounds on the robustness of linear and quadratic classifiers, and [4] seeks to improve robustness by promoting resiliance to deleting features during training.", "startOffset": 226, "endOffset": 229}, {"referenceID": 4, "context": "This definition formalizes the notion of robustness in [5, 6, 20].", "startOffset": 55, "endOffset": 65}, {"referenceID": 5, "context": "This definition formalizes the notion of robustness in [5, 6, 20].", "startOffset": 55, "endOffset": 65}, {"referenceID": 18, "context": "This definition formalizes the notion of robustness in [5, 6, 20].", "startOffset": 55, "endOffset": 65}, {"referenceID": 4, "context": "accuracy on adversarial examples used to measure robustness in [5, 19].", "startOffset": 63, "endOffset": 70}, {"referenceID": 17, "context": "accuracy on adversarial examples used to measure robustness in [5, 19].", "startOffset": 63, "endOffset": 70}, {"referenceID": 13, "context": "Recall that neural nets with rectified-linear (ReLU) units as activations are piecewise linear [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 4, "context": "Since adversarial examples exists because of this linearity in the neural net [5], we restrict our search to the region Z(x\u2217) around x\u2217 on which the neural net is linear.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "In Figure 1 note that the direction of the nearest adversarial example is not necessary aligned with the signed gradient of the loss function, as observed by others [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 18, "context": "Our method discovers more adversarial examples than the baseline [20] for each neural net, hence producing better estimates.", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "We find adversarial examples for the neural net LeNet [12] (modified to use ReLUs instead of sigmoids) trained to classify MNIST [11], and for the network-in-network (NiN) neural net [13] trained to classify CIFAR-10 [9].", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "We find adversarial examples for the neural net LeNet [12] (modified to use ReLUs instead of sigmoids) trained to classify MNIST [11], and for the network-in-network (NiN) neural net [13] trained to classify CIFAR-10 [9].", "startOffset": 129, "endOffset": 133}, {"referenceID": 8, "context": "We find adversarial examples for the neural net LeNet [12] (modified to use ReLUs instead of sigmoids) trained to classify MNIST [11], and for the network-in-network (NiN) neural net [13] trained to classify CIFAR-10 [9].", "startOffset": 217, "endOffset": 220}, {"referenceID": 7, "context": "Both neural nets are trained using Caffe [8].", "startOffset": 41, "endOffset": 44}, {"referenceID": 18, "context": "We compare our algorithm for estimating \u03c1 to the baseline L-BFGS-B algorithm proposed by [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "We use the tool provided by [21] to compute this baseline.", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": "We use LeNet in our comparisons, since we find that it is substantially more robust than the neural nets considered in most previous work (including [20]).", "startOffset": 149, "endOffset": 153}, {"referenceID": 4, "context": "We performed a similar comparison to the signed gradient algorithm proposed by [5] (with the signed gradient multiplied by = 20 pixels).", "startOffset": 79, "endOffset": 82}], "year": 2016, "abstractText": "Despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore, we show how existing approaches to improving robustness \u201coverfit\u201d to adversarial examples generated using a specific algorithm. Finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics.", "creator": "LaTeX with hyperref package"}}}