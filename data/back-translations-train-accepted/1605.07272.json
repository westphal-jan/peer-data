{"id": "1605.07272", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Matrix Completion has No Spurious Local Minimum", "abstract": "Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for matrix completion has no spurious local minima -- all local minima must also be global. Therefore, many popular optimization algorithms such as (stochastic) gradient descent can provably solve matrix completion with \\textit{arbitrary} initialization in polynomial time.", "histories": [["v1", "Tue, 24 May 2016 02:53:27 GMT  (440kb,D)", "http://arxiv.org/abs/1605.07272v1", null], ["v2", "Fri, 16 Sep 2016 19:58:48 GMT  (451kb,D)", "http://arxiv.org/abs/1605.07272v2", "to appear in NIPS 2016 (with oral). added a section that explains the intuition of the proof"], ["v3", "Sun, 29 Jan 2017 18:45:48 GMT  (457kb,D)", "http://arxiv.org/abs/1605.07272v3", "NIPS'16 best student paper. added references and fixed typos"]], "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["rong ge 0001", "jason d lee", "tengyu ma"], "accepted": true, "id": "1605.07272"}, "pdf": {"name": "1605.07272.pdf", "metadata": {"source": "CRF", "title": "Matrix Completion has No Spurious Local Minimum", "authors": ["Rong Ge", "Jason D. Lee", "Tengyu Ma"], "emails": ["rongge@cs.duke.edu.", "jasondlee88@gmail.com.", "tengyu@cs.princeton.edu."], "sections": [{"heading": "1 Introduction", "text": "\"It's as if he was able to put himself at the top,\" he says, \"and:\" He's been able to put himself at the top. \"And:\" He's been able to put himself at the top. \"(\" It's as if he's been able to put himself at the top. \"(\" It's as if he's been able to put himself at the top. \")"}, {"heading": "1.1 Main results", "text": "Suppose the target matrix M is symmetrical and each entry of M is with probability p = independent 2. We assume that M = ZZ > represents two known problems with matrix completion for any matrix Z (ZR).First, the choice of Z is not unique, since for each orthonormal matrix R \u00b7 r, we have M = (ZR) (ZR) >. Our goal is to find one of these equivalent solutions.Another problem is that matrix completion is impossible if M is \"aligned\" with a standard basis. If M is the identity matrix in its first r \u00b7 r row, we will most likely observe only 0 entries. We make the following default assumption: We have Zi of Z for each line, we have Zi of Zi, 6 \u00b5 / verzw, d. \""}, {"heading": "1.2 Related Work", "text": "In fact, it is the case that most of them will be able to move into a different world, in which they are able to live than in another world, in which they live, in which they live, in which they will live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notations", "text": "To denote the i-th series of a matrix X., let us be the operator that maps a matrix A to PB (A), where PB (A) has the same values as A to B and 0 outside of B. We will use the following matrix norms: B) F the Frobenius norm, B) the spectral norm, A) the elementary infinity norm, and A | p \u00b2 q = max x x x x x x x p = 1 x A. We will use the abbreviation B). The inner trace product of two matrices is < A, B > = tr (A > B), and \u03c3min (X), \u03c3max (X) are the smallest and largest singular values of X. We will also use Xi to denote the i-th series of a matrix X."}, {"heading": "2.2 Necessary conditions for Optimality", "text": "Faced with an objective function f (x): n \u2192 we use f (x) to denote the gradient of the function, and b) 2 f (x) to denote the Hessian of the function (b). It is generally known that the local minimization of the function f (x) must fulfill some necessary conditions: Definition 2.1. A point x fulfills the first necessary condition for optimality (later abbreviated as a first order optimality condition) when f (x) = 0. A point x fulfills the second necessary condition for optimality (later abbreviated as a second order optimality condition) when 2 f (x) 0.These conditions are necessary for a local minimum, otherwise it is easy to find a direction in which the function value decreases."}, {"heading": "3 Warm-up: Rank-1 matrix completion", "text": "In this section, we analyze the geometry of the objective function for rank r = > 1 case with a simple and clean proof. > This case illustrates our main ideas. The objective analysis follows from the same approach and is presented in the next section. \u2212 In this case, we assume that M = zz >, where we use the regulation R (x) R (x) = d = 1 x x x (x) = 1 x (x) = 2 x (x) = 2 x (x).The parameters are chosen later than in theorem 3.2. We will opt for regulation R (x) = d (x) = 1 x (x)."}, {"heading": "3.1 Handling incoherent x", "text": "To demonstrate the key idea, we limit our attention in this section to the subset of d > > > < p (2) p (2) p (2) p (2) p (2) p (2) p (3) p (3) p (3) p (3) p (3) p (3) p (3) p (3) p (3) p (4) p (4) p (4) p (4) p (4) p (4) p (4) p (4) p (4) p) p (4) p) p (4) p) p (4) p (4) p (4) p \"p (4) p) p (4) p (4) p) p (4) p (4) p (4) p (4) p (4) p (4) p) p (p) p) p (4) p) p (p) p) p (4) p) p (p) p) p (p) p) p (4) p) p (p) p) p (4) p) p (4) p) p (4) p) p (4) p) p (4) p) p) p (4) p) p (4) p) p (4) p) p (4) p) p (4) p) p) p (4) p) p) p) p (4 p) p) p) p (4 p) p) p) p (4 p) p) p) p) p) p (4 p) p) p) p) p (4 p) p) p) p) p) p) p (4 p) p) p) p) p) p (4 p) p) p) p) p) p (p) p) p) p) p) p) p (p) p) p (p) p) p) p) < p < p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p) p) p (2) p"}, {"heading": "3.2 Extension to general x", "text": "We have shown that if x is contiguous and meets the conditions of the first and second order, then we must continue to be close to z or \u2212 e.g. Now we must consider more general cases where x can have some very large coordinates. Here, the most important intuition is that the optimality condition of the first order with a proper regulator is sufficient to guarantee that x cannot have an entry that is too much larger than \u00b5 / \u221a d.Lemma 3.7. With a high probability of choosing the x that meets the optimality condition of the first order (3.2), we have x x \u00b2 4 max. (3.11) Here we remember that x \u00b2 \u00b2 was chosen to be large so that the second condition of the order optimality condition (3.2) is met. Lemorem 3.2.Proof the lemor 3.2.Proof the lemor 3.7."}, {"heading": "4 Rank-r case", "text": "In this section, we will show how the results in section 3 can be extended to recover matrices of precedence. < R > < R > In this case, we will start from the original matrix M = ZZT, also assuming that Xi (X) = 1 r (X) is very similar to rank 1 Casef (X) = 1 2 x 2 x (M \u2212 XX >) 2 x 2 x (X), (4.1) where Xi (X) = 1 r (X Xi). And we will remember that r (t) = 2 x 2 x (M \u2212 XX >) 4 x 2 x (S \u2212 2 x), that we are again parameters that we will determine later. Without losing generality, we will assume that Z \u00b2 2F = r in this section. This implies that the maximum (Z) > 1 > 2 x (Z) satisfaction (Z). Now, we will specify the first and second order of optimum conditions: 4.x1."}, {"heading": "5 Conclusions", "text": "Although the objective of matrix completion is not convex, we have shown that the lens function has very nice properties that ensure that the local minima are also global. This property provides guarantees for many basic optimization algorithms. An important open problem is the robustness of this property under other model assumptions: can we extend the result to cope with asymmetric matrix completion? Is it possible to add weights to different entries (similar to the settings examined in [LLR16])? Can we replace the lens function with a distance measure other than the Frobenius standard (which is related to work on 1-bit matrix sensors [DPvdBW14])? We hope that this framework for analyzing the geometry of objective function can be applied to other problems."}, {"heading": "A Omitted Proofs in Section 3", "text": "We first prove the equivalent form of the optimality conditions of the first and second order: Lemma A.1 (Proposition 3.1 adjusted) and the optimality condition of the second order requires that the optimality condition of the first order (vx > + xv >), 2P (M \u2212 xx >), 2P (x), 2P (M \u2212 xx >), (A.1) and the optimality condition of the second order (vx > + xv >), 2P (x), 2P (M \u2212 xx >), 2P (M \u2212 xx >), (M \u2212 xx >), v (A.2), (A.3), Proof. Let us be an infinite vector, we hav \u2212 xx \u2212 & ltxxx \u2212 < pump (M \u2212 xx >), (M \u2212 xx >), (A.3), Proof. (Proof) (We assume the second order expansion."}, {"heading": "By symmetry \u3008P\u2126(M\u2212xx>), x\u03b4>\u3009 = \u3008P\u2126(M\u2212xx>), \u03b4x>\u3009 = \u3008P\u2126(M\u2212xx>)x, \u03b4\u3009, so the first order optimality condition", "text": "We assume that we have an optimal state in the situation we find ourselves in. < \u2212 2P (M \u2212 xx >) x > J (x > J = > J = > J > = 0, which corresponds to this 2P. \u2212 2P (M \u2212 xx >) x (M \u2212 xx >) x \u00b2 (M \u2212 xx >) x \u00b2 (M \u2212 xx >) x \u00b2 (X \u2212 2) x \u00b2 (X \u2212 2) x \u00b2 (X) x \u00b2 (X) x \u00b2 (X) x \u00b2 \u00b2 (X) \u00b2 \u00b2 (X) \u00b2) \u00b2 (X) \u00b2 (X) \u00b2) x \u00b2 (2) x \u00b2 (X) \u00b2). \u2212 2P (2) x \u00b2)."}, {"heading": "B Handling Noise", "text": "Suppose that instead of observing the matrix ZZT, we actually observe a noisy version M = ZZT + N, where N is a Gaussian matrix with independent N (0, 2) entries. In this case, we should not hope to accurately restore ZZT (since two near Z's can produce the same observation). In this section, we will show that our arguments can hold even at relatively high noise. Let's allow our arguments to hold. < < < < < 2rp / \u03b12."}, {"heading": "C Finding the Exact Factorization", "text": "In section 4, we have shown that any point that meets the first and second necessary condition must meet the requirements. < ZZT-F 6 c for a small, sufficient constant c. In this section, we will show that Xi Xi must be exactly the same as Xi. Evidence technology here is largely based on the work of Sun and Luo [SL15]. However, we must modify their evidence because we use slightly different regulators, and we will work in symmetrical case. The main lecture Lemma in [SL15] can be rephrased as follows: Lemma C.1 (analogous to Lemma 3.1 in [SL15]). Summary p is at least C\u00b54r6p \u2212 1 log d \u2212 d for any X that is so large that we XXT \u2212 ZZT F 6 (Z) 2 / 100, X X 2. \""}, {"heading": "D Concentration inequality", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "E Auxiliary Lemmas", "text": "Proposition E.1. Let a1,.., ar > 0, C > 0. Then C4 (a21 + \u00b7 \u00b7 + a2r) > a61 + \u00b7 \u00b7 + a6r implies that a21 + \u00b7 \u00b7 + a2r 6 C2r and that max ai 6 Cr1 / 6.Proof. Due to Cauchy-Schwarz inequality we have, r + a2r 6 C2r. This implies with the condition that a61 + \u00b7 \u00b7 \u00b7 + a6r 6 C6r, which means that max ai 6 Cr1 / 6.Proposition E.1 / 6.Proposition E.V (0, 1) implies that all propositions are the P21 + \u00b7 \u00b7 a2r 6 C2r."}], "references": [{"title": "Uncovering shared structures in multiclass classification", "author": ["Yonatan Amit", "Michael Fink", "Nathan Srebro", "Shimon Ullman"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Amit et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amit et al\\.", "year": 2007}, {"title": "On the low-rank approach for semidefinite programs arising in synchronization and community detection", "author": ["Afonso S Bandeira", "Nicolas Boumal", "Vladislav Voroninski"], "venue": "arXiv preprint arXiv:1602.04426,", "citeRegEx": "Bandeira et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bandeira et al\\.", "year": 2016}, {"title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization", "author": ["Samuel Burer", "Renato DC Monteiro"], "venue": "Mathematical Programming,", "citeRegEx": "Burer and Monteiro.,? \\Q2003\\E", "shortCiteRegEx": "Burer and Monteiro.", "year": 2003}, {"title": "Do we need good initialization for low rank matrix recovery", "author": ["Srinadh Bhojanapalli", "Behnam Neyshabur", "Nathan Srebro"], "venue": "Personal Communication,", "citeRegEx": "Bhojanapalli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bhojanapalli et al\\.", "year": 2016}, {"title": "Robust principal component analysis", "author": ["Emmanuel J Cand\u00e8s", "Xiaodong Li", "Yi Ma", "John Wright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2010\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2010}, {"title": "Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees", "author": ["Yudong Chen", "Martin J Wainwright"], "venue": "arXiv preprint arXiv:1509.03025,", "citeRegEx": "Chen and Wainwright.,? \\Q2015\\E", "shortCiteRegEx": "Chen and Wainwright.", "year": 2015}, {"title": "1-bit matrix completion", "author": ["Mark A Davenport", "Yaniv Plan", "Ewout van den Berg", "Mary Wootters"], "venue": "Information and Inference,", "citeRegEx": "Davenport et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Davenport et al\\.", "year": 2014}, {"title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": null, "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Understanding alternating minimization for matrix completion", "author": ["Moritz Hardt"], "venue": "In FOCS 2014. IEEE,", "citeRegEx": "Hardt.,? \\Q2014\\E", "shortCiteRegEx": "Hardt.", "year": 2014}, {"title": "A tail inequality for quadratic forms of subgaussian random vectors", "author": ["Daniel Hsu", "Sham M Kakade", "Tong Zhang"], "venue": "Electron. Commun. Probab,", "citeRegEx": "Hsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "Fast matrix completion without the condition number", "author": ["Moritz Hardt", "Mary Wootters"], "venue": "COLT", "citeRegEx": "Hardt and Wootters.,? \\Q2014\\E", "shortCiteRegEx": "Hardt and Wootters.", "year": 2014}, {"title": "Sums of random Hermitian matrices and an inequality by Rudelson", "author": ["R. Imbuzeiro Oliveira"], "venue": "ArXiv e-prints,", "citeRegEx": "Oliveira.,? \\Q2010\\E", "shortCiteRegEx": "Oliveira.", "year": 2010}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "Matrix completion from a few entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "The bellkor solution to the netflix grand prize", "author": ["Yehuda Koren"], "venue": "Netflix prize documentation,", "citeRegEx": "Koren.,? \\Q2009\\E", "shortCiteRegEx": "Koren.", "year": 2009}, {"title": "Recovery guarantee of weighted low-rank approximation via alternating minimization", "author": ["Yuanzhi Li", "Yingyu Liang", "Andrej Risteski"], "venue": "arXiv preprint arXiv:1602.02262,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Gradient descent converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "University of California, Berkeley,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Cubic regularization of Newton method and its global performance", "author": ["Yurii Nesterov", "Boris T Polyak"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov and Polyak.,? \\Q2006\\E", "shortCiteRegEx": "Nesterov and Polyak.", "year": 2006}, {"title": "Nonconvergence to unstable points in urn models and stochastic approximations", "author": ["Robin Pemantle"], "venue": "The Annals of Probability,", "citeRegEx": "Pemantle.,? \\Q1990\\E", "shortCiteRegEx": "Pemantle.", "year": 1990}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Recht.,? \\Q2011\\E", "shortCiteRegEx": "Recht.", "year": 2011}, {"title": "Fast maximum margin matrix factorization for collaborative prediction", "author": ["Jasson DM Rennie", "Nathan Srebro"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Rennie and Srebro.,? \\Q2005\\E", "shortCiteRegEx": "Rennie and Srebro.", "year": 2005}, {"title": "Guaranteed matrix completion via nonconvex factorization", "author": ["Ruoyu Sun", "Zhi-Quan Luo"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Sun and Luo.,? \\Q2015\\E", "shortCiteRegEx": "Sun and Luo.", "year": 2015}, {"title": "When are nonconvex problems not scary", "author": ["Ju Sun", "Qing Qu", "John Wright"], "venue": "arXiv preprint arXiv:1510.06096,", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "A nonconvex optimization framework for low rank matrix estimation", "author": ["Tuo Zhao", "Zhaoran Wang", "Han Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for matrix completion has no spurious local minima \u2013 all local minima must also be global. Therefore many popular optimization algorithms such as (stochastic) gradient descent can provably solve matrix completion with arbitrary initialization in polynomial time.", "creator": "LaTeX with hyperref package"}}}