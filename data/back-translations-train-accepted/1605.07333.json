{"id": "1605.07333", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Combining Recurrent and Convolutional Neural Networks for Relation Classification", "abstract": "This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task.", "histories": [["v1", "Tue, 24 May 2016 08:20:12 GMT  (48kb,D)", "http://arxiv.org/abs/1605.07333v1", "NAACL 2016"]], "COMMENTS": "NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ngoc thang vu", "heike adel", "pankaj gupta", "hinrich sch\u00fctze"], "accepted": true, "id": "1605.07333"}, "pdf": {"name": "1605.07333.pdf", "metadata": {"source": "CRF", "title": "Combining Recurrent and Convolutional Neural Networks for Relation Classification", "authors": ["Ngoc Thang", "Hinrich Sch\u00fctze"], "emails": ["thang.vu@ims.uni-stuttgart.de", "heike.adel@cis.lmu.de", "gupta.pankaj.ext@siemens.com", "inquiries@cislmu.org"], "sections": [{"heading": "1 Introduction", "text": "For example, the phrase \"We poured the < e1 > milk < / e1 > into the < e2 > pumpkin mixture < / e2 >\" expresses the entity-destination relationship (e1, e2) While early research focused mainly on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements through the application of neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the SemEval 2010 benchmark data showed joint task 8 (Hendrickx et al., 2010).This study examines two different types of nurrent networks (CNR networks) suitable for NR classification (CNR networks)."}, {"heading": "2 Related Work", "text": "In 2010, as part of a SemEval collaborative task (Hendrickx et al., 2010), manually annotated data for the classification of relationships were published. Participants in a joint task used, among other things, vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010). Recently, their results on this data set were exceeded by using NNNs (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015).ar Xiv: 160 5.07 333v 1 [cs.C L] 24 May 2Zeng et al. (2014) built a CNN based only on the context between the relationship arguments and extended it by several lexical features. Kim (2014) and other used revolutionary filters of different sizes for CNNs. Nguyen and Grishman (2015) applied the best contrast layer between our classification and Wang to the Nguyen synchron (2015)."}, {"heading": "3 Convolutional Neural Networks (CNN)", "text": "CNNs perform a discrete folding on an input matrix with a series of different filters. In NLP tasks, the input matrix represents a sentence: Each column of the matrix stores the word embedding of the corresponding word. By applying a filter with a width of e.g. three columns, three adjacent words (trigram) are entangled. Subsequently, the results of the folding are merged. Following Collobert et al. (2011), we perform a max pooling in which the maximum value for each filter, and thus the most informative n-gram, is extracted for the following steps. Finally, the resulting values are concatenated and used to classify the relationship expressed in the sentence."}, {"heading": "3.1 Input: Extended Middle Context", "text": "One of our contributions is a new input representation that is specifically designed for classifying relationships. Contexts are divided into three disjunct regions based on the two relation arguments: the left context, the middle context and the right context. As the middle context in most cases contains the most relevant information for the relationship, we want to focus on this, but not completely ignore the other regions. Therefore, we propose to use two contexts: (1) a combination of the left context, the left context and the middle context; and (2) a combination of the middle context, the right context and the right context. Due to the repetition of the middle context, we force the network to pay special attention to this. The two contexts are processed by two independent conventional and max-pooling layers. After pooling, the results are summarized to form the sentence representation. Figure 1 shows this procedure."}, {"heading": "3.2 Convolutional Layer", "text": "After previous work (e.g. (Nguyen and Grishman, 2015), (Dos Santos et al., 2015)), we use 2D filters that encompass all embedding dimensions. After folding, a maximum pooling process is applied that stores only the highest activation of each filter. We use filters with different window sizes 2-5 (multi-window) as in (Nguyen and Grishman, 2015), i.e. with a different number of input words."}, {"heading": "4 Recurrent Neural Networks (RNN)", "text": "Traditional RNNs consist of an input vector, a history vector and an output vector. In contrast to most traditional RNN architectures, we use the RNN for sentence modeling, i.e. we predict an output vector only after processing the entire sentence and not after every word. The training is done by means of back propagation through time (Werbos, 1990), which unfolds the recurring calculations of the history vector for a certain number of time steps. To avoid exploding gradients, we use gradient clipping with a threshold of 10 (Pascanu et al., 2012)."}, {"heading": "4.1 Input of the RNNs", "text": "Initial experiments showed that the use of trigrams as input instead of individual words led to superior results. Therefore, in the timeframe t, we not only give the model the word wt, but also the trigram wt \u2212 1wtwt + 1 by concatenating the corresponding word embeddings."}, {"heading": "4.2 Connectionist Bi-directional RNNs", "text": "Therefore, in bidirectional RNNs not only a history vector of the word wt is considered, but also a future vector. This leads to the following conditional probability for the story ht at the given time step t [1, n]: hft = f (Uf \u00b7 wt + V \u00b7 hft \u2212 1) (1) hbt = f (Ub \u00b7 wn \u2212 t + 1 + B \u00b7 hbt + 1) (2) ht = f (hbt + hft + H \u00b7 ht \u2212 1) (3) Thus, the network can be divided into three parts: a forward processing the original sentence word by word (Eq.1); a reverse processing the reverse sentence word by word (Eq.2); and a combination of both (Eq.+ hft + H \u00b7 ht \u2212 1). All three parts are trained together."}, {"heading": "5 Model Training", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Word Representations", "text": "In this study, we used the word2vec toolkit (Mikolov et al., 2013) to train the embedding of words in an English Wikipedia from May 2014. We considered only words that occurred more than 100 times and added a special PADDING toolkit for folding, resulting in an embedding text of approximately 485,000 terms and 6.7 \u00b7 109 characters. During the model training, the embedding is updated. Position characteristics. We include randomly initialized position embedding that resemble Zeng et al. (2014), Nguyen and Grishman (2015) and Dos Santos et al. (2015). In our RNN experiments, we investigate various ways of integrating position information: position embedding, position embedding with presence flags (flags that indicate whether the current word is one of the relational arguments) and position indicators (Zhang, 2015 and Wang)."}, {"heading": "5.2 Objective Function: Ranking Loss", "text": "Ranking. We have used the ranking loss function proposed in Dos Santos et al. (2015) to train our models. It maximizes the distance between the true label y + and the best competitive label c \u2212 taking into account a data point x. The objective function is:"}, {"heading": "L = log(1 + exp(\u03b3(m+ \u2212 s\u03b8(x)y+)))", "text": "+ log (1 + exp (\u03b3 (m \u2212 + s\u03b8 (x) c \u2212)))) (4), where s\u03b8 (x) y + and s\u03b8 (x) c \u2212 are the values for classes y + and c \u2212. The parameter \u03b3 controls the punishment of prediction errors and m + and m \u2212 are margins for the correct and wrong classes. According to Dos Santos et al. (2015) we set \u03b3 = 2, m + = 2.5, m \u2212 = 0.5. We do not learn a pattern for class Other, but increase its difference to the best competition label by using only the second sum in equation 4 during the training."}, {"heading": "6 Experiments and Results", "text": "We used the relation classification dataset of SemEval 2010 task 8 (Hendrickx et al., 2010), which consists of sentences manually labeled with 19 relationships (9 directed relationships and an artificial class Others), 8,000 sentences were distributed as a training set, and 2717 sentences served as a test set. We used the official scoring script for the evaluation and gave the macro F1 score, which also served as the official result of the joint task. RNN and CNN models were implemented with Theano (Bergstra et al., 2010; Bastien et al., 2012) for all our models we use the L2 regularization with a weight of 0.0001. For CNN training we use mini-stacks with 25 training examples, while we perform stochastic gradient departures for the RNN. Initial learning rates are 0.2 for the CNN and 0.01 for the RNN. We train the models for 10 (N50) and NN (50) prematurely."}, {"heading": "6.1 Performance of CNNs", "text": "As a basic system, we have implemented a CNN similar to the one described by Zeng et al. (2014). It consists of a standard convolutional layer with filters with only one window size followed by a Softmax layer. As an input, it uses the middle context. Unlike Zeng et al. (2014), our CNN does not have an additional fully connected hidden layer. Therefore, we have increased the number of coil filters to 1200 to keep the number of parameters comparable, giving us a base result of 73.0. After including 5-dimensional position features, performance has been improved to 78.6 (comparable to 78.9 as by Zeng et al. (2014) without linguistic features). In the next step, we will examine how this result changes when we successively add additional features to our CNN: Multi-window folding (window sizes: 2,3,4,5 and 300 features), with each embedding card having an embedding feature."}, {"heading": "6.2 Performance of RNNs", "text": "As a baseline for the RNN models, we use a unidirectional RNN that predicts the relationship after processing the whole set. With this model, we achieve an F1 score of 61.2 on the SemEval testset.Then, we investigate the effects of different position characteristics on the performance of unidirectional RNNs (position embedding, position embedding associated with a flag indicating whether the current word is a unit or not, and position indicators (Zhang and Wang, 2015)).The results suggest that position indicators (i.e. artificial words indicating the presence of the entity) perform best on the SemEval data, achieving an F1 score of 73.4. However, the difference from using position embedding with entity flags is not statistically significant. Similar to our CNN experiments, we vary successively the RNN connections by adding them (N)."}, {"heading": "6.3 Combination of CNNs and RNNs", "text": "Finally, we combine our CNN and RNN models using a voting process. For each set of the test set, we apply multiple CNN and RNN models, shown in Tables 1 and 2, and predict the class with the most votes. In case of a tie, we randomly select one of the most common classes. The combination achieves an F1 value of 84.9, which is better than the performance of the two NN types alone, confirming our assumption that the networks provide complementary information: While the RNN calculates a weighted combination of all the words in the set, CNN extracts the most meaningful Ngrams for the relationship and only takes into account their re-activation."}, {"heading": "6.4 Comparison with State of the Art", "text": "Table 3 shows the results of our ERCNN (advanced ranking CNN) and R-RNN (ranking RNN) models in context with other state-of-the-art models. Our proposed models obtain state-of-the-art results from the SemEval 2010 Task 8 dataset without using any linguistic characteristics."}, {"heading": "7 Conclusion", "text": "In this paper, we examined various characteristics and architectural options for Convolutionary and Recurrent Neural Networks to classify relationships without using any linguistic characteristics. For Convolutionary Neural Networks, we presented a new context representation for classifying relationships. Furthermore, we introduced connectionist recurring neural networks for sentence classification tasks and conducted the first experiments with classification of recurrent neural networks. Finally, we demonstrated that even a simple combination of Constitutional and Recurrent Neural Networks improved outcomes. Using our neural models, we achieved new state-of-the-art results based on SemEval 2010 benchmark data."}, {"heading": "Acknowledgments", "text": "Heike Adel is a scholarship holder of the Google European Doctoral Fellowship in Natural Language Processing and is supported by this scholarship. This research was also supported by the German Research Foundation: Grant SCHU 2246 / 4-2."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of ACL. Association for Computational Linguistics", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of EMNLP. Association for Computational Linguistics", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of the Workshop at ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Relation extraction: Perspective from convolutional neural networks", "author": ["Nguyen", "Grishman2015] Thien Huu Nguyen", "Ralph Grishman"], "venue": "In Proceedings of the NAACL Workshop on Vector Space Modeling for NLP", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Understanding the exploding gradient problem", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Utd: Classifying semantic relations by combining lexical and semantic resources", "author": ["Rink", "Harabagiu2010a] Bryan Rink", "Sanda Harabagiu"], "venue": "In Proceedings of the Workshop on SemEval", "citeRegEx": "Rink et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rink et al\\.", "year": 2010}, {"title": "Utd: Classifying semantic relations by combining lexical and semantic resources", "author": ["Rink", "Harabagiu2010b] Bryan Rink", "Sanda Harabagiu"], "venue": "In Proceedings of the Workshop on SemEval,", "citeRegEx": "Rink et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rink et al\\.", "year": 2010}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of EMNLP / CoNLL", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Isi: automatic classification of relations between nominals using a maximum entropy classifier", "author": ["Tratz", "Hovy2010] Stephen Tratz", "Eduard Hovy"], "venue": "In Proceedings of the Workshop on SemEval", "citeRegEx": "Tratz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tratz et al\\.", "year": 2010}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Factor-based compositional embedding models", "author": ["Yu et al.2014] Mo Yu", "Matthew Gormley", "Mark Dredze"], "venue": "In Proceedings of the NIPS Workshop on Learning Semantics", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of COLING", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Relation classification via recurrent neural network. In ArXiv", "author": ["Zhang", "Wang2015] Dongxu Zhang", "Dong Wang"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al.", "startOffset": 227, "endOffset": 358}, {"referenceID": 15, "context": "While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al.", "startOffset": 227, "endOffset": 358}, {"referenceID": 14, "context": "While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al.", "startOffset": 227, "endOffset": 358}, {"referenceID": 4, "context": ", 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al., 2010) .", "startOffset": 84, "endOffset": 108}, {"referenceID": 4, "context": "In 2010, manually annotated data for relation classification was released in the context of a SemEval shared task (Hendrickx et al., 2010).", "startOffset": 114, "endOffset": 138}, {"referenceID": 11, "context": "Recently, their results on this data set were outperformed by applying NNs (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015).", "startOffset": 75, "endOffset": 184}, {"referenceID": 15, "context": "Recently, their results on this data set were outperformed by applying NNs (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015).", "startOffset": 75, "endOffset": 184}, {"referenceID": 14, "context": "Recently, their results on this data set were outperformed by applying NNs (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015).", "startOffset": 75, "endOffset": 184}, {"referenceID": 4, "context": "Kim (2014) and others used convolutional filters of different sizes for CNNs.", "startOffset": 0, "endOffset": 11}, {"referenceID": 4, "context": "Kim (2014) and others used convolutional filters of different sizes for CNNs. Nguyen and Grishman (2015) applied this to relation classification and obtained improvements over single filter sizes.", "startOffset": 0, "endOffset": 105}, {"referenceID": 3, "context": "Dos Santos et al. (2015) replaced the softmax layer of the CNN with a ranking layer.", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": "Following Collobert et al. (2011), we perform max-pooling which extracts the maximum value for each filter and, thus, the most informative n-gram for the following steps.", "startOffset": 10, "endOffset": 34}, {"referenceID": 13, "context": "Training is performed using backpropagation through time (Werbos, 1990) which unfolds the recurrent computations of the history vector for a certain number of time steps.", "startOffset": 57, "endOffset": 71}, {"referenceID": 8, "context": "avoid exploding gradients, we use gradient clipping with a threshold of 10 (Pascanu et al., 2012).", "startOffset": 75, "endOffset": 97}, {"referenceID": 6, "context": "In this study, we used the word2vec toolkit (Mikolov et al., 2013) to train embeddings on an English Wikipedia from May 2014.", "startOffset": 44, "endOffset": 66}, {"referenceID": 5, "context": "In this study, we used the word2vec toolkit (Mikolov et al., 2013) to train embeddings on an English Wikipedia from May 2014. We only considered words appearing more than 100 times and added a special PADDING token for convolution. This results in an embedding training text of about 485,000 terms and 6.7 \u00b7 109 tokens. During model training, the embeddings are updated. Position features. We incorporate randomly initialized position embeddings similar to Zeng et al. (2014), Nguyen and Grishman (2015) and Dos Santos et al.", "startOffset": 45, "endOffset": 476}, {"referenceID": 5, "context": "In this study, we used the word2vec toolkit (Mikolov et al., 2013) to train embeddings on an English Wikipedia from May 2014. We only considered words appearing more than 100 times and added a special PADDING token for convolution. This results in an embedding training text of about 485,000 terms and 6.7 \u00b7 109 tokens. During model training, the embeddings are updated. Position features. We incorporate randomly initialized position embeddings similar to Zeng et al. (2014), Nguyen and Grishman (2015) and Dos Santos et al.", "startOffset": 45, "endOffset": 504}, {"referenceID": 3, "context": "(2014), Nguyen and Grishman (2015) and Dos Santos et al. (2015). In our RNN experiments, we investigate different possibilities of integrating position information: position embeddings, position embeddings with entity presence flags (flags indicating whether the current word is one of the relation arguments), and position indicators (Zhang and Wang, 2015).", "startOffset": 43, "endOffset": 64}, {"referenceID": 3, "context": "We applied the ranking loss function proposed in Dos Santos et al. (2015) to train our models.", "startOffset": 53, "endOffset": 74}, {"referenceID": 3, "context": "Following Dos Santos et al. (2015), we set \u03b3 = 2,m+ = 2.", "startOffset": 14, "endOffset": 35}, {"referenceID": 4, "context": "We used the relation classification dataset of the SemEval 2010 task 8 (Hendrickx et al., 2010).", "startOffset": 71, "endOffset": 95}, {"referenceID": 1, "context": "RNN and CNN models were implemented with theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 48, "endOffset": 93}, {"referenceID": 0, "context": "RNN and CNN models were implemented with theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 48, "endOffset": 93}, {"referenceID": 15, "context": "As a baseline system, we implemented a CNN similar to the one described by Zeng et al. (2014). It consists of a standard convolutional layer with filters CNN F1 Baseline (emb dim: 50) 73.", "startOffset": 75, "endOffset": 94}, {"referenceID": 15, "context": "In contrast to Zeng et al. (2014), our CNN does not have an additional fully connected hidden layer.", "startOffset": 15, "endOffset": 34}, {"referenceID": 15, "context": "In contrast to Zeng et al. (2014), our CNN does not have an additional fully connected hidden layer. Therefore, we increased the number of convolutional filters to 1200 to keep the number of parameters comparable. With this, we obtain a baseline result of 73.0. After including 5 dimensional position features, the performance was improved to 78.6 (comparable to 78.9 as reported by Zeng et al. (2014) without linguistic features).", "startOffset": 15, "endOffset": 402}, {"referenceID": 11, "context": "2 RNN (Socher et al., 2012) 77.", "startOffset": 6, "endOffset": 27}, {"referenceID": 11, "context": "6 MVRNN (Socher et al., 2012) 82.", "startOffset": 8, "endOffset": 29}, {"referenceID": 15, "context": "4 CNN (Zeng et al., 2014) 82.", "startOffset": 6, "endOffset": 25}, {"referenceID": 14, "context": "7 FCM (Yu et al., 2014) 83.", "startOffset": 6, "endOffset": 23}], "year": 2016, "abstractText": "This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task.", "creator": "LaTeX with hyperref package"}}}