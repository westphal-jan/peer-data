{"id": "1605.07416", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Refined Lower Bounds for Adversarial Bandits", "abstract": "We provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms. The new results show that recent upper bounds that either (a) hold with high-probability or (b) depend on the total lossof the best arm or (c) depend on the quadratic variation of the losses, are close to tight. Besides this we prove two impossibility results. First, the existence of a single arm that is optimal in every round cannot improve the regret in the worst case. Second, the regret cannot scale with the effective range of the losses. In contrast, both results are possible in the full-information setting.", "histories": [["v1", "Tue, 24 May 2016 12:36:47 GMT  (22kb)", "http://arxiv.org/abs/1605.07416v1", null], ["v2", "Mon, 27 Feb 2017 13:48:10 GMT  (23kb)", "http://arxiv.org/abs/1605.07416v2", null]], "reviews": [], "SUBJECTS": "math.ST cs.LG stat.ML stat.TH", "authors": ["s\u00e9bastien gerchinovitz", "tor lattimore"], "accepted": true, "id": "1605.07416"}, "pdf": {"name": "1605.07416.pdf", "metadata": {"source": "CRF", "title": "Refined Lower Bounds for Adversarial Bandits", "authors": ["S\u00e9bastien Gerchinovitz"], "emails": ["sebastien.gerchinovitz@math.univ-toulouse.fr", "tor.lattimore@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.07 416v 1 [mat h.ST] 2 4M ay"}, {"heading": "1 Introduction", "text": "We look at the default K-arm bandit problem, which is a game played over T rounds between a learner and an opponent. (...) We look at the default K-arm addon problem, which is a game over T rounds between a learner and an opponent. (...) We look at the default K-arm addon problem, which is a game over T rounds between a learner and an opponent. (...) The opponent then selects a loss vector. (...) The learner would like to minimize her regret, which is the difference between cumulative loss and loss in hindsight.RT (...). (...) We know that it is an optimal action in hindsight.RT (...). (...) It is not that it is an optimal action in hindsight.RT (...). (...) It is not that it is an optimal action in hindsight.RT (...)."}, {"heading": "2 Zero-Order High Probability Lower Bounds", "text": "The first shows that no strategy can enjoy a lesser regret than \"KT log\" (1 / 3) with a probability of at least 1 \u2212 3 percent. Upper limits of this form have been shown for various algorithms, including Exp.3P [Auer et al., 2002] and Exp3-IX [New, 2015]. Although this result is not very surprising, we are not aware of any existing work on this issue and the proof is less straightforward than one might expect. An addedbenefit of our result is that the loss sequences that elicit great regret have two particular characteristics. Firstly, the optimal arm in each round is equal and secondly, the range of losses in each round is O (1 / 3) / T. These characteristics will be useful in subsequent analyses."}, {"heading": "3 First-Order Lower Bound", "text": "The first order of upper limits provides an improvement over minimax limits when the loss of the optimal action is low. (Let's remember Corolla 1 that the first order limits can be redefined in terms of the smallest losses. (It is standard for minimax results, T defined in (4). (Theorem 3 below represents a new lower limit of order.) Rather, it shows that we cannot hope for a better limit of order if we only know the value of L * T. (Theorem 3) Let's leave K > 2, T > K > 118, and T > distribution of lower losses (32 T). (T, 1 / 2), where c = 64 / 9. Then we are sup.1: T. (RT) > TK > TK / 27, where we have the highest expectation."}, {"heading": "4 Second-Order Lower Bounds", "text": "We start by giving a lower limit of regret in terms of square variation, which is close to the existing upper limits (except in terms of the number of arms), and then prove that bandit strategies cannot adapt to losses that are in a small range, or the existence of an action that is always optimal; this lower limit corresponds to the upper limit of the Korollar2 up to a multiplier factor of the K2 log (T). Closing this gap is left as an open question, but we suspect that the upper limit is loose (see also the COLT problem of Hazan and Kale [2011a]."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Aur\u00e9lien Garivier and \u00c9milie Kaufmann for their insightful discussions and the Agence Nationale de la Recherche (ANR) for their support within the framework of the funding programmes ANR-13-BS01-0005 (project SPADRO) and ANR-13-CORD-0020 (project ALICIA)."}, {"heading": "A Proof of Lemma 1", "text": "The proof is known (e.g., Auer et al. [2002]). We write it only for the convenience of the reader (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = 1. We remember that ht = (Is, i, s, s, p = 2). Next, we write QX = Yj the law of X under Qj. Following the chain rule for the Kullback-Leibler divergence, note that KL (Q, s, Is) 1 \u2212 Kp \u2212 p (H, Is) 2) 2) = t \u2212 1 [KL (Q Is, Q = 1, Q Is | hs 2) 2) + KL (Q, s, Is) 1, Q Is, s, s | 2) 2) + EQL (KL (Q, Q, Q It | ht 2). We note that Q1 (Is = i | hs, s = Q2 (Is, s = Q2) for all."}, {"heading": "C Proof of Theorem 4", "text": "The proof follows the same lines as those of Theorem Part 3 = > Answer = > Answer = > Answer = > Answer = Answer = Answer = Answer = Answer = Answer = Answer = Answer = Answer = Answer = Answer = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer (Answer) = Answer (Answer) = Answer (Answer (Answer) = Answer (Answer) > (Answer (Answer): Answer (Answer (Answer) > (Answer (Answer): Answer (Answer (Answer): Answer (Answer): Answer (Answer (Answer (Answer) > (Answer (Answer): Answer (Answer): Answer (Answer (Answer): Answer (Answer (Answer) = Answer (Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer) = Answer (Answer (Answer (Answer) ="}, {"heading": "D Proof of Lemma 3", "text": "First of all, we use the definition of losses to the limit RT (i 1: T) = T \u2211 t = 1 (\"It,\" \"It,\" \"T,\" \"T,\" \"T,\" \"11 {Zt,\" \"It 2: T\" and \"It 6 = i.\" Let us allow Wt = 1 {Zt, \"\" 2, \"\" 1 \u2212 2 \"), which forms an i.i.d. Bernoulli sequence with Qi (Wt = 0) 6 Exp (\u2212 (-1 / 2 \u2212 2) 22\u03c32) = p 6 1 / 8, with the inequality on the Gaussian integral following standard limits [Boucheron et al., 2013,\" Exercise 2.7]. Therefore, Hoeffding's limit Qi (T = 1Wt 6 3T4) = Qi (T = 1Wt = 1 > T = 1Wt = 1Wt and \"T\" (W1 \u2212 t) 6 Exp (Qt) Part 6 > / 2 = 4. \""}, {"heading": "E Proof of Corollary 3", "text": "Suppose, on the contrary, that such a strategy exists. ThenE [RT (1: T)] 6% 0P (RT (1: T) > x) dx6% 0exp \u2212 (xC \u221a (K \u2212 1) T) 1% dx 6% (K \u2212 1) T.Assuming in the logical sequence we have P (RT (1: T) > C (K \u2212 1) T logp (1 / \u043c))) = P (RT (1: T) > \u221a (K \u2212 1) T log (1 / (16\u043c)) 203C \u00b7 203C 2 logp (1 / \u043c) log (1 / (16\u043c)))), which leads to a contradiction by selecting \"sufficiently small\" and \"sufficiently large\" and applying Theorem 2 (K \u2212 1) to show that there is a \"1: T\" [0, 1] KT for which P (RT (1: T) > 4% (K \u2212 1)."}], "references": [{"title": "Hannan consistency in on-line learning in case of unbounded losses under partial monitoring", "author": ["C. Allenberg", "P. Auer", "L. Gy\u00f6rfi", "G. Ottucs\u00e1k"], "venue": "In Proceedings of ALT\u20192006,", "citeRegEx": "Allenberg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Allenberg et al\\.", "year": 2006}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["J. Audibert", "S. Bubeck"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Audibert and Bubeck.,? \\Q2009\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2009}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "The nonstochastic multi-armed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Concentration inequalities: a nonasymptotic theory of independence", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": null, "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Bounded regret in stochastic multi-armed bandits", "author": ["S. Bubeck", "V. Perchet", "P. Rigollet"], "venue": "In Proceedings of The 26th Conference on Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["N. Cesa-Bianchi", "Y. Mansour", "G. Stoltz"], "venue": "Mach. Learn.,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2007}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "Wiley-Interscience [John Wiley & Sons], second edition,", "citeRegEx": "Cover and Thomas.,? \\Q2006\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 2006}, {"title": "Follow the leader if you can, hedge if you must", "author": ["S. de Rooij", "T. van Erven", "P.D. Gr\u00fcnwald", "W.M. Koolen"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Rooij et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rooij et al\\.", "year": 2014}, {"title": "A second-order bound with excess losses", "author": ["P. Gaillard", "G. Stoltz", "T. van Erven"], "venue": "In Proceedings of the 27th Conference on Learning Theory (COLT\u201914),", "citeRegEx": "Gaillard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gaillard et al\\.", "year": 2014}, {"title": "A simple multi-armed bandit algorithm with optimal variation-bounded regret", "author": ["E. Hazan", "S. Kale"], "venue": "In Proceedings of the 24th Conference on Learning Theory,", "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Better algorithms for benign bandits", "author": ["E. Hazan", "S. Kale"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Adv. in Appl. Math.,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "First-order regret bounds for combinatorial semi-bandits", "author": ["G. Neu"], "venue": "In Proceedings of The 28th Conference on Learning Theory, pages 1360\u20131375,", "citeRegEx": "Neu.,? \\Q2015\\E", "shortCiteRegEx": "Neu.", "year": 2015}, {"title": "Online learning with predictable sequences", "author": ["A. Rakhlin", "K. Sridharan"], "venue": "In Proceedings of the 26th Conference on Learning Theory,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2013}, {"title": "Incomplete information and internal regret in prediction of individual sequences", "author": ["G. Stoltz"], "venue": "PhD thesis, Paris-Sud XI University,", "citeRegEx": "Stoltz.,? \\Q2005\\E", "shortCiteRegEx": "Stoltz.", "year": 2005}, {"title": "Introduction to nonparametric estimation", "author": ["A. Tsybakov"], "venue": "Springer Science & Business Media,", "citeRegEx": "Tsybakov.,? \\Q2008\\E", "shortCiteRegEx": "Tsybakov.", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "A famous strategy is called Exp3, which satisfies E[RT (l1:T )] = O( \u221a KT log(K))) where the expectation is taken over the randomness in the algorithm and the choices of the adversary [Auer et al., 2002].", "startOffset": 184, "endOffset": 203}, {"referenceID": 2, "context": "There is also a lower bound showing that for every learner there is an adversary for which the expected regret is E[RT (l1:T )] = \u03a9( \u221a KT ) [Auer et al., 1995].", "startOffset": 140, "endOffset": 159}, {"referenceID": 1, "context": "If the losses are chosen ahead of time, then the adversary is called oblivious, and in this case there exists a learner for which E[RT (l1:T )] = O( \u221a KT ) [Audibert and Bubeck, 2009].", "startOffset": 156, "endOffset": 183}, {"referenceID": 3, "context": "P [Auer et al., 2002] and Exp-IX [Neu, 2015] are tuned with a confidence parameter \u03b4 \u2208 (0, 1) and satisfy for some universal constant c > 0", "startOffset": 2, "endOffset": 21}, {"referenceID": 14, "context": ", 2002] and Exp-IX [Neu, 2015] are tuned with a confidence parameter \u03b4 \u2208 (0, 1) and satisfy for some universal constant c > 0", "startOffset": 19, "endOffset": 30}, {"referenceID": 5, "context": "P [Bubeck and Cesa-Bianchi, 2012] leads to a single algorithm for which", "startOffset": 2, "endOffset": 33}, {"referenceID": 13, "context": ", FPL-TRIX by Neu [2015] with earlier results by Stoltz [2005], Allenberg et al.", "startOffset": 14, "endOffset": 25}, {"referenceID": 13, "context": ", FPL-TRIX by Neu [2015] with earlier results by Stoltz [2005], Allenberg et al.", "startOffset": 14, "endOffset": 63}, {"referenceID": 0, "context": ", FPL-TRIX by Neu [2015] with earlier results by Stoltz [2005], Allenberg et al. [2006], Rakhlin and Sridharan [2013]) such that for all l1:T \u2208 [0, 1] E[RT (l1:T )] 6 O (\u221a LTK log(K) +K log(KT ) ) , with LT = min 16i6K T \u2211", "startOffset": 64, "endOffset": 88}, {"referenceID": 0, "context": ", FPL-TRIX by Neu [2015] with earlier results by Stoltz [2005], Allenberg et al. [2006], Rakhlin and Sridharan [2013]) such that for all l1:T \u2208 [0, 1] E[RT (l1:T )] 6 O (\u221a LTK log(K) +K log(KT ) ) , with LT = min 16i6K T \u2211", "startOffset": 64, "endOffset": 118}, {"referenceID": 14, "context": "The first-order regret bound (3) of Neu [2015] is equivalent to: \u2200\u03b1 \u2208 [0, 1], sup l1:T\u2208B\u03b1,T E[RT (l1:T )] 6 O (\u221a \u03b1TK log(K) +K log(KT ) ) .", "startOffset": 36, "endOffset": 47}, {"referenceID": 11, "context": "Second-order bounds Another type of improved regret bound was derived by Hazan and Kale [2011b] and involves a second-order quantity called the quadratic variation.", "startOffset": 73, "endOffset": 96}, {"referenceID": 11, "context": "Hazan and Kale [2011b] addressed the general online linear optimisation setting.", "startOffset": 0, "endOffset": 23}, {"referenceID": 11, "context": "The second-order regret bound (6) of Hazan and Kale [2011b] is equivalent to: \u2200\u03b1 \u2208 [0, 1/4], sup l1:T\u2208V\u03b1,T E[RT (l1:T )] 6 c ( K \u221a \u03b1TK logT +K log T +K log T ) .", "startOffset": 37, "endOffset": 60}, {"referenceID": 7, "context": "Two impossibility results in the bandit setting We also show in Section 4 that, in contrast to the full-information setting, regret bounds involving the cumulative variance of the algorithm as in [Cesa-Bianchi et al., 2007] cannot be obtained in the bandit setting.", "startOffset": 196, "endOffset": 223}, {"referenceID": 3, "context": "Results of roughly this form are well known and the proof follows immediately from the chain rule for the relative entropy and the independence of the loss vectors across time (see [Auer et al., 2002] or Appendix A).", "startOffset": 181, "endOffset": 200}, {"referenceID": 2, "context": "Results of roughly this form are well known and the proof follows immediately from the chain rule for the relative entropy and the independence of the loss vectors across time (see [Auer et al., 2002] or Appendix A). One difference is that the losses need not be independent across the arms, which we heavily exploit in our proofs by using correlated losses. The second key lemma is an alternative to Pinsker\u2019s inequality that proves useful when the Kullback-Leibler divergence is larger than 2. It has previously been used for bandit lower bounds (in the stochastic setting) by Bubeck et al. [2013]. Lemma 2 (Lemma 2.", "startOffset": 182, "endOffset": 600}, {"referenceID": 3, "context": "3P [Auer et al., 2002] and Exp3-IX [Neu, 2015].", "startOffset": 3, "endOffset": 22}, {"referenceID": 14, "context": ", 2002] and Exp3-IX [Neu, 2015].", "startOffset": 20, "endOffset": 31}, {"referenceID": 4, "context": "38) of Boucheron et al. [2013] with Xt = lj,t \u2212 \u03b1/2 6 1 = b, with v = T (\u03b1/2)(1 \u2212 \u03b1/2), and with c = b/3 = 1/3), we get that, for all \u03b4 \u2208 (0, 1), with Qj-probability at least 1\u2212 \u03b4, LT 6 T \u2211", "startOffset": 7, "endOffset": 31}, {"referenceID": 11, "context": "Closing this gap is left as an open question, but we conjecture that the upper bound is loose (see also the COLT open problem by Hazan and Kale [2011a]).", "startOffset": 129, "endOffset": 152}, {"referenceID": 7, "context": "If all the losses li,t are nonnegative, then by Corollary 3 of [Cesa-Bianchi et al., 2007] the second-order bound (17) implies the first-order bound", "startOffset": 63, "endOffset": 90}, {"referenceID": 13, "context": "If one is prepared to ignore logarithmic terms, then point 4 also has an analogue in the bandit setting due to the existence of logarithmic regret guarantees for stochastic bandits [Lai and Robbins, 1985].", "startOffset": 181, "endOffset": 204}, {"referenceID": 7, "context": "This is the case for the algorithm1 of Cesa-Bianchi et al. [2007, Section 4.1] tuned with E = 1 (if all losses lie in [0, 1]). Then by the translation invariance of the algorithm all losses li,t appearing in the regret bound can be replaced with the translated losses li,t \u2212 li\u2217,t > 0, so that a bound of the same form as (18) implies a regret bound of O(logK). 4. Assume that the loss vectors lt are i.i.d. with a unique optimal arm in expectation (i.e., there exists i such that E[li\u2217,1] < E[li,1] for all i 6= i). Then using the Hoeffding-Azuma inequality we can show that the algorithm of Cesa-Bianchi et al. [2007, Section 4.2] has with high probability a bounded cumulative variance VT , and therefore (by (17)) incurs a bounded regret, in the same spirit as in de Rooij et al. [2014], Gaillard et al.", "startOffset": 39, "endOffset": 791}, {"referenceID": 7, "context": "This is the case for the algorithm1 of Cesa-Bianchi et al. [2007, Section 4.1] tuned with E = 1 (if all losses lie in [0, 1]). Then by the translation invariance of the algorithm all losses li,t appearing in the regret bound can be replaced with the translated losses li,t \u2212 li\u2217,t > 0, so that a bound of the same form as (18) implies a regret bound of O(logK). 4. Assume that the loss vectors lt are i.i.d. with a unique optimal arm in expectation (i.e., there exists i such that E[li\u2217,1] < E[li,1] for all i 6= i). Then using the Hoeffding-Azuma inequality we can show that the algorithm of Cesa-Bianchi et al. [2007, Section 4.2] has with high probability a bounded cumulative variance VT , and therefore (by (17)) incurs a bounded regret, in the same spirit as in de Rooij et al. [2014], Gaillard et al. [2014]. We already know that point 2 has a counterpart in the bandit setting.", "startOffset": 39, "endOffset": 815}], "year": 2017, "abstractText": "We provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms. The new results show that recent upper bounds that either (a) hold with high-probability or (b) depend on the total loss of the best arm or (c) depend on the quadratic variation of the losses, are close to tight. Besides this we prove two impossibility results. First, the existence of a single arm that is optimal in every round cannot improve the regret in the worst case. Second, the regret cannot scale with the effective range of the losses. In contrast, both results are possible in the full-information setting.", "creator": "LaTeX with hyperref package"}}}