{"id": "1605.07515", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Neural Semantic Role Labeling with Dependency Path Embeddings", "abstract": "This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as sub-sequences of lexicalized dependency paths and learns suitable embedding representations. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers, and showcase qualitative improvements obtained by our method.", "histories": [["v1", "Tue, 24 May 2016 15:54:48 GMT  (38kb)", "https://arxiv.org/abs/1605.07515v1", "Accepted at ACL 2016"], ["v2", "Mon, 18 Jul 2016 09:08:51 GMT  (34kb)", "http://arxiv.org/abs/1605.07515v2", "Camera-ready ACL paper"]], "COMMENTS": "Accepted at ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["michael roth", "mirella lapata"], "accepted": true, "id": "1605.07515"}, "pdf": {"name": "1605.07515.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mroth@inf.ed.ac.uk", "mlap@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.07 515v 2 [cs.C L] 18 Jul 2 016"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to abide by the rules that they have imposed on themselves, and they are able to abide by the rules that they have imposed on themselves. (...) In fact, it is so that they are able to understand the rules that they have given themselves. (...) In fact, it is so that they are able to break the rules. \"(...)"}, {"heading": "2 Dependency Path Embeddings", "text": "In the context of neural networks, the term embedding refers to the output of a function f within the network that transforms any input into real vector output. For example, word embedding is typically calculated by passing a uniform word vector representation from the input layer of a neural network to its first hidden layer, usually using matrix multiplication and an optional nonlinear function whose parameters are learned during neural network training. Here, we try to calculate real vector representations for dependency paths between a word pair < wi, w >. We define a dependency path as a sequence of nodes (representing words) and edges (representing relationships between words) that are traversed on a dependency tree to move from node wi to node w >."}, {"heading": "2.1 Recurrent Neural Networks", "text": "The recurring model we use in this thesis is a variant of long-term short-term memory (LSTM). It takes a sequence of elements X = x1,..., xn as input, processes each element xt-X repeatedly at a time and finally returns an embed state for the complete input sequence. For each time step t, the LSTM model updates an internal memory state mt, which is from both the current input and the previous memory state mt \u2212 1. To capture long-term dependencies, a so-called gating mechanism controls the extent to which each component of a memory cell state is modified. In this work, we use input gates i, output gates o and (optional) oblivion f. We formalize the state of the network in each time step t as follows: es = Gate ([Wmimt \u2212 1] + Wxixt + bi) (1) + Wxfxt + bmt = 1 (b) Willx = 4 (1) (x) (1)."}, {"heading": "2.2 Embedding Dependency Paths", "text": "We define the embedding of a dependency path as the final memory output state of a recurring LSTM layer that takes a path as input, with each input step representing a binary indicator for a part-of-speech tag, word form, or dependency relationship. In the context of the semantic role designation, we define each path as a sequence of a predicate to its potential argumentation.1 Specifically, we define the first point x1, which corresponds to the part-of-speech tag of the predicate word wi, followed by its actual word form and the relationship to the next word wi + 1. The embedding of a dependency path corresponds to the state that the LSTM layer represents after entering the last item, xn, which corresponds to the word form of the predicate word wi, followed by its actual word form, and the relationship to the next word wi + 1. An example, in Figure 2.The main category of this word model, and the representation that we can expect to represent the word relations, is that all the relationships are plausible."}, {"heading": "2.3 Joint Embedding and Feature Learning", "text": "Our SRL model consists of four components shown in Figure 3: (1) an LSTM component uses lexicalized dependency paths as input, (2) an additional input layer binary characteristics as input, (3) a hidden layer combines dependency path embedding and binary characteristics using corrected linear units, and (4) a Softmax classification layer generates output based on the hidden layer state as input. Therefore, we are learning LSTM embedding along with feature detectors based on traditional binary indicator characteristics. Given a dependency path X with steps xk, x1,..., xn} and a set of binary characteristics B as input, we are using the LSTM formalization of equa-1We experimented with various sequential orders and found that this leads to the best validation sets."}, {"heading": "3 System Architecture", "text": "The overall architecture of our SFL system largely follows previous work (Toutanova et al., 2008; Bjo \ufffd rkelund et al., 2009) and is shown in Figure 4. We use a pipeline consisting of the following steps: predicate identification and disambiguation, argument identification, argument classification and re-ranking. Neural network components introduced in Section 2 are used in the last three steps. In the following subsections, all components are described in more detail."}, {"heading": "3.1 Predicate Identification and Disambiguation", "text": "Considering a syntactically analyzed sentence, the first two steps in an end-to-end SRL system are to identify and disambiguate the semantic predicates of the sentence. In this context, we focus on verbal and nominal predicates, but note that other syntactic categories in the NLP literature have also been interpreted as predicates (e.g. prepositions; Srikumar and Roth (2013)). For both identification and disambiguation steps, we apply the same logistic regression classifiers used in the SRL components of partner tools (Bjo \ufffd rkelund et al., 2010). The classifiers for both tasks use a set of lexico-syntactic indicator characteristics, including the predicative word form, its predicted part-of-speech tags, and dependency relationships with all syntactic children."}, {"heading": "3.2 Argument Identification and Classification", "text": "The next two steps of our SRL system are to identify all the arguments of each predicate and assign them appropriate role designations. In both steps, we train multiple LSTM-based neural network models, as described in Section 2. Specifically, we train separate networks for nominal and verbal predicates, as well as for identification and classification. Following the results of previous work (Xue and Palmer, 2004), we assume that different character groups are relevant for the respective tasks and therefore different embedding representations should be learned. As binary input characteristics, we use the following sentences from the SRL literature (Bjo Buerkelund et al., 2010). Lexicosyntactic characteristics Word formulas and word categories of the predicate and candidate argument; dependence relationships between predicate and argument to their respective syntactic heads; complete dependence sequence of predicate to arguments. Localculation contextual characteristics of the candidate's word and candidate's argument types and candidate's argument types; and candidate's argument syntactic candidates."}, {"heading": "3.3 Reranker", "text": "Since all reasoning (and classification) decisions are independent of each other, we use a global re-anchor as the last step of our pipeline. With a predicate p, the re-anchor inputs the n best sets of identified arguments and their n best markup assignments, and predicts the best general argument structure. We implement the re-anchor as a logistic regression classifier, with hidden and embedded level states of identified arguments as characteristics compensated by the argument label, and binary markup as output (1: best predicted structure, 0: any other structure). At test time, we select the structure with the highest cumulative score, which we calculate as a geometric mean of global regression and all argument-specific values."}, {"heading": "4 Experiments", "text": "This year we are dealing with a number of countries where people are able to unfold, and where people are able to unfold. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have managed to change the world. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) We have succeeded. (...) we have succeeded. (...) we have succeeded. (...). (...) we have succeeded. (... \"We have succeeded. (...\" We have succeeded."}, {"heading": "5 Path Embeddings in other Languages", "text": "In this section, we report on the results of further experiments with Chinese, German and Spanish data. The basic question is to what extent the improvements of our English SRL system are transferable to other languages. To answer this question, we train and test separate SRL models for each language, using the system architecture and hyperparameters described in Sections 3 and 4. We train our models on data from the CoNLL-2009 joint task, using the same features as one of the participating systems (Bjo \ufffd rkelund et al., 2009) and evaluating them with the official goal scorer. For direct comparison, we rely on the (automatic) syntactical pre-processing information provided with the CoNLL test data, and compare our results with the best two systems for each language that use the same pre-processing information. The results summarized in Table 7 show that PathLSTM performs better in all cases than the Bjo \ufffd rkelet system (2009)."}, {"heading": "6 Related Work", "text": "They developed a forward-looking network that uses a convolution function across the windows of words to assign SRL labels. However, aside from the dependency problems of constituencies, their system has not used any syntactic information. Foland and Martin (2015) expanded their model and showed significant improvements in the use of binary indicator traits for dependency pathways. Similar traits were found by FitzGerald et al. (2015), which include role-labeling predictions by neural networks as factors in a global model. These approaches use all binary traits derived from syntactic particles, either to show constituency boundaries or to represent full dependency paths. An extreme alternative was recently proposed in Zhou and Xu (2015), who drives SRL decisions with a multi-layer LSTM network that accepts word sequences as input but not syntactical particle information."}, {"heading": "7 Conclusions", "text": "Our experimental results indicate that our model significantly improves classification performance, leading to new, state-of-the-art results. In a qualifying analysis, we found that our model is capable of covering examples of various linguistic phenomena that are overlooked by other methods. Beyond SRL, we expect dependency path embedding to be useful for related tasks and downstream applications, for example, our representations could be of direct use for semantic and discursive analysis tasks. The jointly learned feature space also makes our model a good starting point for linguistic transfer methods based on the representation of traits to induce new models (Kozhevnikov and Titov, 2014). We thank the three anonymous ACL speakers whose feedback has helped to significantly improve the present work."}], "references": [{"title": "Shallow semantic trees for smt", "author": ["Wilker Aziz", "Miguel Rios", "Lucia Specia."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 316\u2013322, Edinburgh, Scotland.", "citeRegEx": "Aziz et al\\.,? 2011", "shortCiteRegEx": "Aziz et al\\.", "year": 2011}, {"title": "Multilingual semantic role labeling", "author": ["Anders Bj\u00f6rkelund", "Love Hafdell", "Pierre Nugues."], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, pages 43\u201348, Boulder, Colorado.", "citeRegEx": "Bj\u00f6rkelund et al\\.,? 2009", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2009}, {"title": "A high-performance syntactic and semantic dependency parser", "author": ["Anders Bj\u00f6rkelund", "Bernd Bohnet", "Love Hafdell", "Pierre Nugues."], "venue": "Coling 2010: Demonstration Volume, pages 33\u201336, Beijing, China.", "citeRegEx": "Bj\u00f6rkelund et al\\.,? 2010", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2010}, {"title": "Top accuracy and fast dependency parsing is not a contradiction", "author": ["Bernd Bohnet."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, pages 89\u201397, Beijing, China.", "citeRegEx": "Bohnet.,? 2010", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "Multilingual dependency-based syntactic and semantic parsing", "author": ["Wanxiang Che", "Zhenghua Li", "Yongqiang Li", "Yuhang Guo", "Bing Qin", "Ting Liu."], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared", "citeRegEx": "Che et al\\.,? 2009", "shortCiteRegEx": "Che et al\\.", "year": 2009}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semi-supervised semantic role labeling using the Latent Words Language Model", "author": ["Koen Deschacht", "Marie-Francine Moens."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21\u201329, Singapore.", "citeRegEx": "Deschacht and Moens.,? 2009", "shortCiteRegEx": "Deschacht and Moens.", "year": 2009}, {"title": "Semantic role labeling with neural network factors", "author": ["Nicholas FitzGerald", "Oscar T\u00e4ckstr\u00f6m", "Kuzman Ganchev", "Dipanjan Das."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 960\u2013970,", "citeRegEx": "FitzGerald et al\\.,? 2015", "shortCiteRegEx": "FitzGerald et al\\.", "year": 2015}, {"title": "Dependency-based semantic role labeling using convolutional neural networks", "author": ["William Foland", "James Martin."], "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics, pages 279\u2013288, Denver,", "citeRegEx": "Foland and Martin.,? 2015", "shortCiteRegEx": "Foland and Martin.", "year": 2015}, {"title": "Automatic labeling of semantic roles", "author": ["Daniel Gildea", "Daniel Jurafsky."], "venue": "Computational Linguistics, 28(3):245\u2013288.", "citeRegEx": "Gildea and Jurafsky.,? 2002", "shortCiteRegEx": "Gildea and Jurafsky.", "year": 2002}, {"title": "Multilingual joint parsing of syntactic and semantic dependencies with a latent variable model", "author": ["James Henderson", "Paola Merlo", "Ivan Titov", "Gabriele Musillo."], "venue": "Computational Linguistics, 39(4):949\u2013998.", "citeRegEx": "Henderson et al\\.,? 2013", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Open-domain semantic role labeling by modeling word spans", "author": ["Fei Huang", "Alexander Yates."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 968\u2013 978, Uppsala, Sweden.", "citeRegEx": "Huang and Yates.,? 2010", "shortCiteRegEx": "Huang and Yates.", "year": 2010}, {"title": "The effect of syntactic representation on semantic role labeling", "author": ["Richard Johansson", "Pierre Nugues."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics, pages 393\u2013400, Manchester, United Kingdom.", "citeRegEx": "Johansson and Nugues.,? 2008", "shortCiteRegEx": "Johansson and Nugues.", "year": 2008}, {"title": "A framework for multi-document abstractive summarization based on semantic role labelling", "author": ["Atif Khan", "Naomie Salim", "Yogan Jaya Kumar."], "venue": "Applied Soft Computing, 30:737\u2013747.", "citeRegEx": "Khan et al\\.,? 2015", "shortCiteRegEx": "Khan et al\\.", "year": 2015}, {"title": "The importance of syntactic parsing and inference in semantic role labeling", "author": ["Vasin Punyakanok", "Dan Roth", "Wen-tau Yih."], "venue": "Computational Linguistics, 34(2):257\u2013287.", "citeRegEx": "Punyakanok et al\\.,? 2008", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2008}, {"title": "Contextaware frame-semantic role labeling", "author": ["Michael Roth", "Mirella Lapata."], "venue": "Transactions of the Association for Computational Linguistics, 3:449\u2013460.", "citeRegEx": "Roth and Lapata.,? 2015", "shortCiteRegEx": "Roth and Lapata.", "year": 2015}, {"title": "Composition of word representations improves semantic role labelling", "author": ["Michael Roth", "Kristian Woodsend."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 407\u2013413, Doha, Qatar.", "citeRegEx": "Roth and Woodsend.,? 2014", "shortCiteRegEx": "Roth and Woodsend.", "year": 2014}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P. Adams."], "venue": "Advances in Neural Information Processing Systems, pages 2951\u20132959, Lake Tahoe, Nevada.", "citeRegEx": "Snoek et al\\.,? 2012", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Modeling semantic relations expressed by prepositions", "author": ["Vivek Srikumar", "Dan Roth."], "venue": "Transactions of the Association for Computational Linguistics, 1:231\u2013242.", "citeRegEx": "Srikumar and Roth.,? 2013", "shortCiteRegEx": "Srikumar and Roth.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A global joint model for semantic role labeling", "author": ["Kristina Toutanova", "Aria Haghighi", "Christopher Manning."], "venue": "Computational Linguistics, 34(2):161\u2013191.", "citeRegEx": "Toutanova et al\\.,? 2008", "shortCiteRegEx": "Toutanova et al\\.", "year": 2008}, {"title": "Visualizing data using t-SNE", "author": ["Laurens Van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9:2579\u20132605.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Modeling the translation of predicate-argument structure for smt", "author": ["Deyi Xiong", "Min Zhang", "Haizhou Li."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 902\u2013911, Jeju Island, Korea.", "citeRegEx": "Xiong et al\\.,? 2012", "shortCiteRegEx": "Xiong et al\\.", "year": 2012}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Calibrating features for semantic role labeling", "author": ["Nianwen Xue", "Martha Palmer."], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 88\u201394, Barcelona, Spain.", "citeRegEx": "Xue and Palmer.,? 2004", "shortCiteRegEx": "Xue and Palmer.", "year": 2004}, {"title": "Selectional preferences for semantic role classification", "author": ["Be\u00f1at Zapirain", "Eneko Agirre", "Llu\u0131\u0301s M\u00e0rquez", "Mihai Surdeanu"], "venue": "Computational Linguistics,", "citeRegEx": "Zapirain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zapirain et al\\.", "year": 2013}, {"title": "Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies", "author": ["Hai Zhao", "Wenliang Chen", "Jun\u2019ichi Kazama", "Kiyotaka Uchimoto", "Kentaro Torisawa"], "venue": null, "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}, {"title": "End-to-end learning of semantic role labeling using recurrent neural networks", "author": ["Jie Zhou", "Wei Xu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-", "citeRegEx": "Zhou and Xu.,? 2015", "shortCiteRegEx": "Zhou and Xu.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Previous work has shown that semantic roles are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al.", "startOffset": 177, "endOffset": 216}, {"referenceID": 23, "context": "Previous work has shown that semantic roles are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al.", "startOffset": 177, "endOffset": 216}, {"referenceID": 14, "context": ", 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015).", "startOffset": 76, "endOffset": 95}, {"referenceID": 0, "context": "Previous work has shown that semantic roles are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In System Analysis", "startOffset": 178, "endOffset": 434}, {"referenceID": 15, "context": "Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008).", "startOffset": 64, "endOffset": 111}, {"referenceID": 13, "context": ", 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008).", "startOffset": 121, "endOffset": 149}, {"referenceID": 1, "context": "For instance, consider the sentence He had trouble raising funds and the analyses provided by four publicly available tools in Table 1 (mate-tools, Bj\u00f6rkelund et al. (2010); mateplus, Roth and Woodsend (2014); TensorSRL, Lei et al.", "startOffset": 148, "endOffset": 173}, {"referenceID": 1, "context": "For instance, consider the sentence He had trouble raising funds and the analyses provided by four publicly available tools in Table 1 (mate-tools, Bj\u00f6rkelund et al. (2010); mateplus, Roth and Woodsend (2014); TensorSRL, Lei et al.", "startOffset": 148, "endOffset": 209}, {"referenceID": 1, "context": "For instance, consider the sentence He had trouble raising funds and the analyses provided by four publicly available tools in Table 1 (mate-tools, Bj\u00f6rkelund et al. (2010); mateplus, Roth and Woodsend (2014); TensorSRL, Lei et al. (2015); and easySRL, Lewis et al.", "startOffset": 148, "endOffset": 239}, {"referenceID": 1, "context": "For instance, consider the sentence He had trouble raising funds and the analyses provided by four publicly available tools in Table 1 (mate-tools, Bj\u00f6rkelund et al. (2010); mateplus, Roth and Woodsend (2014); TensorSRL, Lei et al. (2015); and easySRL, Lewis et al. (2015)).", "startOffset": 148, "endOffset": 273}, {"referenceID": 11, "context": "We then apply long-short term memory networks (Hochreiter and Schmidhuber, 1997) to find a recurrent composition function that can reconstruct an appropriate representation of the full path from its individual parts (Section 2).", "startOffset": 46, "endOffset": 80}, {"referenceID": 21, "context": "The overall architecture of our SRL system closely follows that of previous work (Toutanova et al., 2008; Bj\u00f6rkelund et al., 2009) and is depicted in Figure 4.", "startOffset": 81, "endOffset": 130}, {"referenceID": 1, "context": "The overall architecture of our SRL system closely follows that of previous work (Toutanova et al., 2008; Bj\u00f6rkelund et al., 2009) and is depicted in Figure 4.", "startOffset": 81, "endOffset": 130}, {"referenceID": 2, "context": "For both identification and disambiguation steps, we apply the same logistic regression classifiers used in the SRL components of mate-tools (Bj\u00f6rkelund et al., 2010).", "startOffset": 141, "endOffset": 166}, {"referenceID": 17, "context": ", prepositions; Srikumar and Roth (2013)).", "startOffset": 16, "endOffset": 41}, {"referenceID": 25, "context": "Following the findings of earlier work (Xue and Palmer, 2004), we assume that different feature sets are relevant for the respective tasks and hence different embedding representations should be learned.", "startOffset": 39, "endOffset": 61}, {"referenceID": 2, "context": "we use the following sets from the SRL literature (Bj\u00f6rkelund et al., 2010).", "startOffset": 50, "endOffset": 75}, {"referenceID": 3, "context": "For direct comparison with previous work, we use the same preprocessing models and predicate-specific SRL components as provided with mate-tools (Bohnet, 2010; Bj\u00f6rkelund et al., 2010).", "startOffset": 145, "endOffset": 184}, {"referenceID": 2, "context": "For direct comparison with previous work, we use the same preprocessing models and predicate-specific SRL components as provided with mate-tools (Bohnet, 2010; Bj\u00f6rkelund et al., 2010).", "startOffset": 145, "endOffset": 184}, {"referenceID": 18, "context": "The best parameters were chosen using the Spearmint hyperparameter optimization toolkit (Snoek et al., 2012), applied for approx.", "startOffset": 88, "endOffset": 108}, {"referenceID": 17, "context": "5% (ensemble) F1-score, outperforming the previous best system by Roth and Woodsend (2014) by 0.", "startOffset": 66, "endOffset": 91}, {"referenceID": 1, "context": "We train our models on data from the CoNLL-2009 shared task, relying on the same features as one of the participating systems (Bj\u00f6rkelund et al., 2009), and evaluate with the official scorer.", "startOffset": 126, "endOffset": 151}, {"referenceID": 1, "context": "The results, summarized in Table 7, indicate that PathLSTM performs better than the system by Bj\u00f6rkelund et al. (2009) in all cases.", "startOffset": 94, "endOffset": 119}, {"referenceID": 1, "context": "4 Bj\u00f6rkelund et al. (2009) 82.", "startOffset": 2, "endOffset": 27}, {"referenceID": 1, "context": "4 Bj\u00f6rkelund et al. (2009) 82.4 75.1 78.6 Zhao et al. (2009) 80.", "startOffset": 2, "endOffset": 61}, {"referenceID": 1, "context": "1 Bj\u00f6rkelund et al. (2009) 81.", "startOffset": 2, "endOffset": 27}, {"referenceID": 1, "context": "1 Bj\u00f6rkelund et al. (2009) 81.2 78.3 79.7 Che et al. (2009) 82.", "startOffset": 2, "endOffset": 60}, {"referenceID": 1, "context": "2 Bj\u00f6rkelund et al. (2009) 78.", "startOffset": 2, "endOffset": 27}, {"referenceID": 5, "context": "Neural Networks for SRL Collobert et al. (2011) pioneered neural networks for the task of semantic role labeling.", "startOffset": 24, "endOffset": 48}, {"referenceID": 5, "context": "Neural Networks for SRL Collobert et al. (2011) pioneered neural networks for the task of semantic role labeling. They developed a feed-forward network that uses a convolution function over windows of words to assign SRL labels. Apart from constituency boundaries, their system does not make use of any syntactic information. Foland and Martin (2015) extended their model and showcased significant improvements when including binary indicator features for dependency paths.", "startOffset": 24, "endOffset": 351}, {"referenceID": 5, "context": "Neural Networks for SRL Collobert et al. (2011) pioneered neural networks for the task of semantic role labeling. They developed a feed-forward network that uses a convolution function over windows of words to assign SRL labels. Apart from constituency boundaries, their system does not make use of any syntactic information. Foland and Martin (2015) extended their model and showcased significant improvements when including binary indicator features for dependency paths. Similar features were used by FitzGerald et al. (2015), who include role labeling predictions by neural networks as factors in a global model.", "startOffset": 24, "endOffset": 529}, {"referenceID": 28, "context": "An extreme alternative has been recently proposed in Zhou and Xu (2015), who model SRL decisions with a multi-layered LSTM network that takes word sequences as input but no syntactic parse information at all.", "startOffset": 53, "endOffset": 72}, {"referenceID": 10, "context": "(Henderson et al., 2013) as well as low-rank tensor models (Lei et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences.", "startOffset": 14, "endOffset": 41}, {"referenceID": 6, "context": "For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences.", "startOffset": 14, "endOffset": 68}, {"referenceID": 6, "context": "For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences. Zapirain et al. (2013) propose different selection preference models in order to deal with the sparseness of lexical features.", "startOffset": 14, "endOffset": 204}, {"referenceID": 6, "context": "For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences. Zapirain et al. (2013) propose different selection preference models in order to deal with the sparseness of lexical features. Roth and Woodsend (2014) address the same problem with word embeddings and compositions thereof.", "startOffset": 14, "endOffset": 333}, {"referenceID": 6, "context": "For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences. Zapirain et al. (2013) propose different selection preference models in order to deal with the sparseness of lexical features. Roth and Woodsend (2014) address the same problem with word embeddings and compositions thereof. Roth and Lapata (2015) recently introduced features that model the influence of discourse on role labeling decisions.", "startOffset": 14, "endOffset": 428}, {"referenceID": 23, "context": "Xu et al. (2015) and Liu et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 23, "context": "Xu et al. (2015) and Liu et al. (2015) use neural networks to embed dependency paths between entity pairs.", "startOffset": 0, "endOffset": 39}, {"referenceID": 20, "context": "Tai et al. (2015) and Ma et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 20, "context": "Tai et al. (2015) and Ma et al. (2015) learn embeddings of dependency structures representing full sentences, in a sentiment classification task.", "startOffset": 0, "endOffset": 39}], "year": 2016, "abstractText": "This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as subsequences of lexicalized dependency paths and learns suitable embedding representations. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers, and showcase qualitative improvements obtained by our method.", "creator": "LaTeX with hyperref package"}}}