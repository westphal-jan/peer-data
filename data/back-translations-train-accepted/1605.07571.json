{"id": "1605.07571", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Sequential Neural Models with Stochastic Layers", "abstract": "How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.", "histories": [["v1", "Tue, 24 May 2016 18:23:58 GMT  (341kb,D)", "http://arxiv.org/abs/1605.07571v1", null], ["v2", "Sun, 13 Nov 2016 18:04:41 GMT  (181kb,D)", "http://arxiv.org/abs/1605.07571v2", "NIPS 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["marco fraccaro", "s\u00f8ren kaae s\u00f8nderby", "ulrich paquet", "ole winther"], "accepted": true, "id": "1605.07571"}, "pdf": {"name": "1605.07571.pdf", "metadata": {"source": "CRF", "title": "Sequential Neural Models with Stochastic Layers", "authors": ["Marco Fraccaro", "S\u00f8ren Kaae S\u00f8nderby", "Ulrich Paquet", "Ole Winther"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2 Recurrent Neural Networks and State Space Models", "text": "Recursive neural networks and state space models are often used to model temporal sequences of vectors x1: T = (x1, x2,.., xT), which may depend on inputs u1: T = (u1, u2,.., uT) Both models are based on the assumption that the sequence x1: t of observations up to time t can be summarized by a latent state dt or zt, determined deterministically (dt in an RNN) or treated as a random variable to be removed (zt in an SSM). The difference in the treatment of the latent state has traditionally led to vastly different models: RNNs compose recursively dt = f (dt \u2212 1, ut) using a parameterized nonlinear function f, such as an LSTM cell or a GRU. The RNN observation probabilities p (xt | dt) are equally associated with nonlinear functions."}, {"heading": "3 Stochastic Recurrent Neural Networks", "text": "The common probability of a single sequence and its latent states, provided that knowledge of the output states z0 = 0 and d0 = 0, and inputs u1: T, factorize asp (x1: T, z1: T, z0, d0) = p\u03b8x (x1: T, d1: T) p\u03b8z (z1: T, d1: T, d1: T, d0: T) = T (x1: T, d1: T) p\u03b8x (x1: T, d1: T) p\u03b8z (z1: T, d1: T) p\u03b8d (d1: T, d0: D)."}, {"heading": "3.1 Variational inference for the SRNN", "text": "The stochastic variables z1: T of the nonlinear SSM cannot be integrated analytically to obtain L (\u03b8) in (2). Instead of maximizing L in relation to both factors, we maximize a sum of variable evidence that depends on other factors (ELBO) F (\u03b8, \u03c6) \u2264 L in relation to both factors and the more variable parameters. The ELBO is a sum of lower limits. (ELBO) \u2264 Li (\u03b8), one for each sequence i, Fi (\u03b8, \u03c6) = number of quantities. (z1: T, d1: T | x1: T) Log. (x1: T) Log. (x1: T) Q. (z1: T) Q. (z1: T) Q. (z1: T) We, d1: T) dz1: D, (4: T)."}, {"heading": "3.2 Exploiting the temporal structure", "text": "The true posterior distribution of the stochastic states z1: T, taking into account both the data and the deterministic states d1: 1. This can be verified by taking into account the conditional independence properties of the graphic model in Figure 2a using the d-separation [14]. This shows that the knowledge zt \u2212 1, the posterior distribution of zt does not depend on past outputs and deterrent states, but only on the present and future states; this was also stated in Figure 2a using the d-separation [14]. Instead of factoring qs as an average field approximation over time steps, we keep the structured form of the posterior factors, including the zt-dependence on zt \u2212 1, in the variable approximation qp."}, {"heading": "3.3 Parameterization of the inference network", "text": "The variational distribution q\u03c6 (zt) = Quoticized Quoticized (zt) = Quotititized (zt) Quotitized (zt) Quotized (z) Quotitized (z) Quotized (z) Quotitized (z) Quotititized (z) Quotitized (z) Quotitized (z) Quotized (z) Quotized (z) Quotized (z) Quotized (z) Quotized (z) Quotized (z) Quotitized (z) Quotitized (z) Quotitized (z) Quotized (z) Quotized (z) Quotized (z) Quotized (z) Quotized (z) Quotized (z) Quotized (z) Quotized (z) Quotized (z) Quotified (z) Quotified (z) Quotified (z) Quotified (z) Quotified (z) Quotified (z) Quotified (z) Quotified (z) Quotitized (z) Quotitized (z) Quotized (z) Quotized (z)"}, {"heading": "4 Results", "text": "In this section, the SRNN are evaluated on the basis of modeling speech and polyphonic music data, as they have proven difficult to model without a good representation of uncertainty in the latent states. [3, 8, 12, 13, 16] We test SRNN on the Blizzard page [19] and raw TIMIT data (Table 1) used in [8]. Pre-processing of the data sets and the metrics for test performance are identical to those reported in [8]. Blizzard is a dataset of 300 hours of English spoken by a single speaker. TIMIT is a dataset of 6300 English sentences read by 630 speakers. As in [8], for Blizzard we report the average log similarity of half-second sequences, and for TIMIT we report the average log probability per sequence."}, {"heading": "5 Related work", "text": "A number of studies have extended RNNNs to include stochastic units to model motion capture, language, and music data [3, 8, 12, 13, 16]. The performance of these models depends to a large extent on how the dependence between stochastic units is modelled over time, on the type of interaction between stochastic units and deterministic units, and on the method used to evaluate the typically intractable log probability. Figure 4 shows how SRNN differs from some of these works. In STORN [3] (Figure 4a) and DRAW [15] ddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd"}, {"heading": "6 Conclusion", "text": "This work has shown how the modelling capacities of recurrent neural networks can be expanded by combining them with nonlinear state-space models. Inspired by the independence properties of the insoluble real posterior distribution over the latent states, we developed an inference network in principle. Variable proximation for the stochastic layer has been improved by using information from the entire sequence and by using Resq parameterization to help the inference network track the non-stationary posterior. SRNN achieves state-of-the-art performance in Blizzard and TIMIT language datasets and is comparable to competing methods for polyphonic music modeling."}, {"heading": "Acknowledgements", "text": "We would like to thank Casper Kaae S\u00f8nderby and Lars Maal\u00f8e for many fruitful discussions and the NVIDIA Corporation for donating TITAN X and Tesla K40 GPUs. Marco Fraccaro is supported by Microsoft Research as part of his PhD program."}, {"heading": "A Experimental setup", "text": "In the first half of the 20th century, the number of women in employment who were able to stay in the city multiplied; in the second half of the 20th century, the number of women in employment in the city multiplied; in the third half of the 20th century, the number of men in employment in the city multiplied; in the third half of the 20th century, the number of women in employment in the city multiplied; in the third half of the 20th century, the number of women in employment in the city multiplied."}], "references": [{"title": "Black box variational inference for state space models", "author": ["E. Archer", "I.M. Park", "L. Buesing", "J. Cunningham", "L. Paninski"], "venue": "arXiv:1511.07367", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "arXiv:1211.5590", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning stochastic recurrent networks", "author": ["J. Bayer", "C. Osendorfer"], "venue": "arXiv:1411.7610", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "arXiv:1206.6392", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating sentences from a continuous space", "author": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio"], "venue": "arXiv:1511.06349", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "\u00c7. G\u00fcl\u00e7ehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP, pages 1724\u20131734", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv:1412.3555", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A recurrent latent variable model for sequential data", "author": ["J. Chung", "K. Kastner", "L. Dinh", "K. Goel", "A.C. Courville", "Y. Bengio"], "venue": "NIPS, pages 2962\u20132970", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B, 39(1)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1977}, {"title": "and A", "author": ["S. Dieleman", "J. Schl\u00fcter", "C. Raffel", "E. Olson", "S.K. S\u00f8nderby", "D. Nouri", "E. Battenberg"], "venue": "van den Oord. Lasagne: First release", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "An introduction to sequential Monte Carlo methods. In Sequential Monte Carlo Methods in Practice, Statistics for Engineering and Information", "author": ["A. Doucet", "N. de Freitas", "N. Gordon"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Variational recurrent auto-encoders", "author": ["O. Fabius", "J.R. van Amersfoort"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Deep temporal sigmoid belief networks for sequence modeling", "author": ["Z. Gan", "C. Li", "R. Henao", "D.E. Carlson", "L. Carin"], "venue": "NIPS, pages 2458\u20132466", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Identifying independence in Bayesian networks", "author": ["D. Geiger", "T. Verma", "J. Pearl"], "venue": "Networks, 20:507\u2013534", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "ICML", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural adaptive sequential Monte Carlo", "author": ["S. Gu", "Z. Ghahramani", "R.E. Turner"], "venue": "NIPS, pages 2611\u20132619", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine Learning, 37(2):183\u2013233", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "The Blizzard challenge 2013", "author": ["S. King", "V. Karaiskos"], "venue": "The Ninth Annual Blizzard Challenge", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv:1412.6980", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-encoding variational Bayes", "author": ["D. Kingma", "M. Welling"], "venue": "ICLR", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Kalman filters", "author": ["R.G. Krishnan", "U. Shalit", "D. Sontag"], "venue": "arXiv:1511.05121", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "arXiv:1402.0030", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Variational Bayesian inference with stochastic search", "author": ["J.W. Paisley", "D.M. Blei", "M.I. Jordan"], "venue": "ICML", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Building blocks for variational Bayesian learning of latent variable models", "author": ["T. Raiko", "H. Valpola", "M. Harva", "J. Karhunen"], "venue": "Journal of Machine Learning Research, 8:155\u2013201", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "ICML, pages 1278\u20131286", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "A unifying review of linear Gaussian models", "author": ["S. Roweis", "Z. Ghahramani"], "venue": "Neural Computation, 11(2):305\u201345", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 5, "context": "Recurrent neural networks (RNNs) are able to represent long-term dependencies in sequential data, by adapting and propagating a deterministic hidden (or latent) state [6, 17].", "startOffset": 167, "endOffset": 174}, {"referenceID": 16, "context": "Recurrent neural networks (RNNs) are able to represent long-term dependencies in sequential data, by adapting and propagating a deterministic hidden (or latent) state [6, 17].", "startOffset": 167, "endOffset": 174}, {"referenceID": 2, "context": "There is recent evidence that when complex sequences such as speech and music are modeled, the performances of RNNs can be dramatically improved when uncertainty is included in their hidden states [3, 4, 8, 12, 13, 16].", "startOffset": 197, "endOffset": 218}, {"referenceID": 3, "context": "There is recent evidence that when complex sequences such as speech and music are modeled, the performances of RNNs can be dramatically improved when uncertainty is included in their hidden states [3, 4, 8, 12, 13, 16].", "startOffset": 197, "endOffset": 218}, {"referenceID": 7, "context": "There is recent evidence that when complex sequences such as speech and music are modeled, the performances of RNNs can be dramatically improved when uncertainty is included in their hidden states [3, 4, 8, 12, 13, 16].", "startOffset": 197, "endOffset": 218}, {"referenceID": 11, "context": "There is recent evidence that when complex sequences such as speech and music are modeled, the performances of RNNs can be dramatically improved when uncertainty is included in their hidden states [3, 4, 8, 12, 13, 16].", "startOffset": 197, "endOffset": 218}, {"referenceID": 12, "context": "There is recent evidence that when complex sequences such as speech and music are modeled, the performances of RNNs can be dramatically improved when uncertainty is included in their hidden states [3, 4, 8, 12, 13, 16].", "startOffset": 197, "endOffset": 218}, {"referenceID": 15, "context": "There is recent evidence that when complex sequences such as speech and music are modeled, the performances of RNNs can be dramatically improved when uncertainty is included in their hidden states [3, 4, 8, 12, 13, 16].", "startOffset": 197, "endOffset": 218}, {"referenceID": 26, "context": "Although SSMs have an illustrious history [27], their stochasticity has limited their widespread use in the deep learning community, as inference can only be exact for two relatively simple classes of SSMs, namely hidden Markov models and linear Gaussian models, neither of which are well-suited to modeling long-term dependencies and complex probability distributions over high-dimensional sequences.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "On the other hand modern RNNs rely on gated nonlinearities such as long short-term memory (LSTM) [17] cells or gated recurrent units (GRUs) [7], that let the deterministic hidden state of the RNN act as an internal memory for the model.", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "On the other hand modern RNNs rely on gated nonlinearities such as long short-term memory (LSTM) [17] cells or gated recurrent units (GRUs) [7], that let the deterministic hidden state of the RNN act as an internal memory for the model.", "startOffset": 140, "endOffset": 143}, {"referenceID": 20, "context": "We use recent advances in variational inference to efficiently approximate the intractable posterior distribution over the latent states with an inference network [21, 26].", "startOffset": 163, "endOffset": 171}, {"referenceID": 25, "context": "We use recent advances in variational inference to efficiently approximate the intractable posterior distribution over the latent states with an inference network [21, 26].", "startOffset": 163, "endOffset": 171}, {"referenceID": 7, "context": "Unlike the VRNN [8], zt directly depends on zt\u22121, as it does in a SSM, via p\u03b8z(zt|zt\u22121,dt).", "startOffset": 16, "endOffset": 19}, {"referenceID": 17, "context": "bound (ELBO) F(\u03b8, \u03c6) = \u2211 i Fi(\u03b8, \u03c6) \u2264 L(\u03b8) with respect to both \u03b8 and the variational parameters \u03c6 [18].", "startOffset": 99, "endOffset": 103}, {"referenceID": 20, "context": "Each sequence\u2019s approximation q\u03c6 shares parameters \u03c6 with all others, to form the auto-encoding variational Bayes inference network or variational auto encoder (VAE) [21, 26] shown in Figure 2b.", "startOffset": 166, "endOffset": 174}, {"referenceID": 25, "context": "Each sequence\u2019s approximation q\u03c6 shares parameters \u03c6 with all others, to form the auto-encoding variational Bayes inference network or variational auto encoder (VAE) [21, 26] shown in Figure 2b.", "startOffset": 166, "endOffset": 174}, {"referenceID": 20, "context": "All the intractable expectations in (4) would typically be approximated by sampling, using the reparameterization trick [21, 26] or control variates [24] to obtain low-variance estimators of its gradients.", "startOffset": 120, "endOffset": 128}, {"referenceID": 25, "context": "All the intractable expectations in (4) would typically be approximated by sampling, using the reparameterization trick [21, 26] or control variates [24] to obtain low-variance estimators of its gradients.", "startOffset": 120, "endOffset": 128}, {"referenceID": 23, "context": "All the intractable expectations in (4) would typically be approximated by sampling, using the reparameterization trick [21, 26] or control variates [24] to obtain low-variance estimators of its gradients.", "startOffset": 149, "endOffset": 153}, {"referenceID": 8, "context": "Iteratively maximizing F over \u03b8 and \u03c6 separately would yield an expectation maximization-type algorithm, which has formed a backbone of statistical modeling for many decades [9].", "startOffset": 174, "endOffset": 177}, {"referenceID": 13, "context": "This can be verified by considering the conditional independence properties of the graphical model in Figure 2a using d-separation [14].", "startOffset": 131, "endOffset": 135}, {"referenceID": 21, "context": "This shows that, knowing zt\u22121, the posterior distribution of zt does not depend on the past outputs and deterministic states, but only on the present and future ones; this was also noted in [22].", "startOffset": 190, "endOffset": 194}, {"referenceID": 10, "context": "In the spirit of sequential Monte Carlo methods [11], we improve the parameterization of q\u03c6(zt|zt\u22121,at) by using q\u2217 \u03c6(zt\u22121) from (9).", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "In this section the SRNN is evaluated on the modeling of speech and polyphonic music data, as they have shown to be difficult to model without a good representation of the uncertainty in the latent states [3, 8, 12, 13, 16].", "startOffset": 205, "endOffset": 223}, {"referenceID": 7, "context": "In this section the SRNN is evaluated on the modeling of speech and polyphonic music data, as they have shown to be difficult to model without a good representation of the uncertainty in the latent states [3, 8, 12, 13, 16].", "startOffset": 205, "endOffset": 223}, {"referenceID": 11, "context": "In this section the SRNN is evaluated on the modeling of speech and polyphonic music data, as they have shown to be difficult to model without a good representation of the uncertainty in the latent states [3, 8, 12, 13, 16].", "startOffset": 205, "endOffset": 223}, {"referenceID": 12, "context": "In this section the SRNN is evaluated on the modeling of speech and polyphonic music data, as they have shown to be difficult to model without a good representation of the uncertainty in the latent states [3, 8, 12, 13, 16].", "startOffset": 205, "endOffset": 223}, {"referenceID": 15, "context": "In this section the SRNN is evaluated on the modeling of speech and polyphonic music data, as they have shown to be difficult to model without a good representation of the uncertainty in the latent states [3, 8, 12, 13, 16].", "startOffset": 205, "endOffset": 223}, {"referenceID": 18, "context": "We test SRNN on the Blizzard [19] and TIMIT raw audio data sets (Table 1) used in [8].", "startOffset": 29, "endOffset": 33}, {"referenceID": 7, "context": "We test SRNN on the Blizzard [19] and TIMIT raw audio data sets (Table 1) used in [8].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "The preprocessing of the data sets and the testing performance measures are identical to those reported in [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "As done in [8], for Blizzard we report the average log-likelihood for half-second sequences and for TIMIT we report the average log likelihood per sequence for the test set sequences.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "Additionally, we test SRNN for modeling sequences of polyphonic music (Table 2), using the four data sets of MIDI songs introduced in [4].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "All models where implemented using Theano [2], Lasagne [10] and Parmesan2.", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "All models where implemented using Theano [2], Lasagne [10] and Parmesan2.", "startOffset": 55, "endOffset": 59}, {"referenceID": 7, "context": "Table 1 compares the average log-likelihood per test sequence of SRNN to the results from [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "For RNNs and VRNNs the authors of [8] test two different output distributions, namely a Gaussian distribution (Gauss) and a Gaussian Mixture Model (GMM).", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "VRNN-I differs from the VRNN in that the prior over the latent variables is independent across time steps, and it is therefore similar to STORN [3].", "startOffset": 144, "endOffset": 147}, {"referenceID": 10, "context": "We prefer to only report the more conservative evidence lower bound for SRNN, as the approximation of the log-likelihood using standard importance sampling is known to be difficult to compute accurately in the sequential setting [11].", "startOffset": 229, "endOffset": 233}, {"referenceID": 7, "context": "The non-SRNN results are reported as in [8].", "startOffset": 40, "endOffset": 43}, {"referenceID": 12, "context": "The TSBN results are from [13], NASMC from [16], STORN from [3], RNN-NADE and RNN from [4].", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "The TSBN results are from [13], NASMC from [16], STORN from [3], RNN-NADE and RNN from [4].", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "The TSBN results are from [13], NASMC from [16], STORN from [3], RNN-NADE and RNN from [4].", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "The TSBN results are from [13], NASMC from [16], STORN from [3], RNN-NADE and RNN from [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "Table 2 compares the average log-likelihood on the test sets obtained with SRNN and the models introduced in [3, 4, 13, 16].", "startOffset": 109, "endOffset": 123}, {"referenceID": 3, "context": "Table 2 compares the average log-likelihood on the test sets obtained with SRNN and the models introduced in [3, 4, 13, 16].", "startOffset": 109, "endOffset": 123}, {"referenceID": 12, "context": "Table 2 compares the average log-likelihood on the test sets obtained with SRNN and the models introduced in [3, 4, 13, 16].", "startOffset": 109, "endOffset": 123}, {"referenceID": 15, "context": "Table 2 compares the average log-likelihood on the test sets obtained with SRNN and the models introduced in [3, 4, 13, 16].", "startOffset": 109, "endOffset": 123}, {"referenceID": 2, "context": "A number of works have extended RNNs with stochastic units to model motion capture, speech and music data [3, 8, 12, 13, 16].", "startOffset": 106, "endOffset": 124}, {"referenceID": 7, "context": "A number of works have extended RNNs with stochastic units to model motion capture, speech and music data [3, 8, 12, 13, 16].", "startOffset": 106, "endOffset": 124}, {"referenceID": 11, "context": "A number of works have extended RNNs with stochastic units to model motion capture, speech and music data [3, 8, 12, 13, 16].", "startOffset": 106, "endOffset": 124}, {"referenceID": 12, "context": "A number of works have extended RNNs with stochastic units to model motion capture, speech and music data [3, 8, 12, 13, 16].", "startOffset": 106, "endOffset": 124}, {"referenceID": 15, "context": "A number of works have extended RNNs with stochastic units to model motion capture, speech and music data [3, 8, 12, 13, 16].", "startOffset": 106, "endOffset": 124}, {"referenceID": 2, "context": "In STORN [3] (Figure 4a) and DRAW [15] the stochastic units at each time step have an isotropic Gaussian prior and are independent between time steps.", "startOffset": 9, "endOffset": 12}, {"referenceID": 14, "context": "In STORN [3] (Figure 4a) and DRAW [15] the stochastic units at each time step have an isotropic Gaussian prior and are independent between time steps.", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "As in our work, the reparametrization trick [21, 26] is used to optimize an ELBO.", "startOffset": 44, "endOffset": 52}, {"referenceID": 25, "context": "As in our work, the reparametrization trick [21, 26] is used to optimize an ELBO.", "startOffset": 44, "endOffset": 52}, {"referenceID": 7, "context": "The authors of the VRNN [8] (Figure 4b) note that it is beneficial to add information coming from the past states to the prior over latent variables zt.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "The works by [1, 22] (Figure 4c) show that it is possible to improve inference in SSMs by using ideas from VAEs, similar to what is done in the stochastic part (the top layer) of SRNN.", "startOffset": 13, "endOffset": 20}, {"referenceID": 21, "context": "The works by [1, 22] (Figure 4c) show that it is possible to improve inference in SSMs by using ideas from VAEs, similar to what is done in the stochastic part (the top layer) of SRNN.", "startOffset": 13, "endOffset": 20}, {"referenceID": 15, "context": "Towards the periphery of related works, [16] approximates the log likelihood of a SSM with sequential Monte Carlo, by learning flexible proposal distributions parameterized by deep networks, while [13] uses a recurrent model with discrete stochastic units that is optimized using the NVIL algorithm [23].", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "Towards the periphery of related works, [16] approximates the log likelihood of a SSM with sequential Monte Carlo, by learning flexible proposal distributions parameterized by deep networks, while [13] uses a recurrent model with discrete stochastic units that is optimized using the NVIL algorithm [23].", "startOffset": 197, "endOffset": 201}, {"referenceID": 22, "context": "Towards the periphery of related works, [16] approximates the log likelihood of a SSM with sequential Monte Carlo, by learning flexible proposal distributions parameterized by deep networks, while [13] uses a recurrent model with discrete stochastic units that is optimized using the NVIL algorithm [23].", "startOffset": 299, "endOffset": 303}], "year": 2016, "abstractText": "How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model\u2019s posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.", "creator": "LaTeX with hyperref package"}}}