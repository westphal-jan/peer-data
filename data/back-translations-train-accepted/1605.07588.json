{"id": "1605.07588", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "A Consistent Regularization Approach for Structured Prediction", "abstract": "We propose and analyze a regularization approach for structured prediction problems. We characterize a large class of loss functions that allows to naturally embed structured outputs in a linear space. We exploit this fact to design learning algorithms using a surrogate loss approach and regularization techniques. We prove universal consistency and finite sample bounds characterizing the generalization properties of the proposed methods. Experimental results are provided to demonstrate the practical usefulness of the proposed approach.", "histories": [["v1", "Tue, 24 May 2016 19:06:43 GMT  (528kb)", "https://arxiv.org/abs/1605.07588v1", "39 pages, 2 Tables, 1 Figure"], ["v2", "Thu, 26 May 2016 15:55:37 GMT  (528kb)", "http://arxiv.org/abs/1605.07588v2", "39 pages, 2 Tables, 1 Figure"], ["v3", "Fri, 28 Jul 2017 09:36:05 GMT  (528kb)", "http://arxiv.org/abs/1605.07588v3", "39 pages, 2 Tables, 1 Figure"]], "COMMENTS": "39 pages, 2 Tables, 1 Figure", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["carlo ciliberto", "lorenzo rosasco", "alessandro rudi"], "accepted": true, "id": "1605.07588"}, "pdf": {"name": "1605.07588.pdf", "metadata": {"source": "CRF", "title": "A Consistent Regularization Approach for Structured Prediction", "authors": ["Carlo Ciliberto", "Alessandro Rudi", "Lorenzo Rosasco"], "emails": ["cciliber@mit.edu", "ale_rudi@mit.edu", "lrosasco@mit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 5.07 588v 3 [cs.L G] 2"}, {"heading": "1 Introduction", "text": "Many machine learning applications require the use of data sets with complex structures, such as natural language processing, image segmentation, reconstruction or labeling, to represent estimation, protein folding prediction, to name a few. [1-3] Structured prediction problems pose a challenge to classic standard learning algorithms for regression or binary classification. Indeed, this has motivated the expansion of methods such as support vectors for structured problems [4]. Dealing with structured prediction problems is also a challenge for learning theory. While empirical risk minimization theory provides a very general statistical framework, in practice compatible considerations make things more involved. Efforts have been made in recent years to analyze specific structured problems, such as multiclass classification [5], multi-labeling [7] or quantile estimation of structure [8]."}, {"heading": "2 A Regularization Approach to Structured prediction", "text": "The goal of supervised learning is to learn functional relationships for: X \u2192 Y between two sets X, Y, based on a finite number of examples. In particular, in this work we are interested in structured predictions, namely in the case that Y is essentially a set of structured results (such as histograms, charts, time sequences, points on a manifold, etc.). In addition, the structure on Y can be implicitly induced by a suitable loss: Y \u00b7 Y \u2192 R (such as processing distance, ranking error, geodesic distance, indicator function of a subset, etc.) Then the problem of structured prediction becomesminimize f: X \u2192 YE (f) = X \u00d7 X \u00d7 Y (f) dr (x), y) dr (1) and the goal is to find a good estimator for minimizing the above equation, given a certain number of points (xi, yi)."}, {"heading": "3 Surrogate Framework and Derivation", "text": "The idea is to solve the problem (1) by using it (1), with which it (1), (2), (3), (3), (3), (3), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5),"}, {"heading": "4 Statistical Analysis", "text": "In this section, we examine the statistical properties of Alg. [1] In particular, we have the relationship between structural and replacement problems in relation to the comparability of inequality in Thm. [2] We begin our analysis by proving that Alg. [3] a continuous universal reproduction of Kernel2 is possible. [4] In any case, however, we can look at the distribution of Y to X-Y at least as it can be achieved by Y-Y. [5] Y-Z: X-Y-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z. (18) Thm. [4] shows that if the solution of Asm. 1, Alg."}, {"heading": "5 Connection with Previous Work", "text": "In this case, it is as if it were a reactionary party that would have been able to choose a party that is able to choose a party that is able to choose a party that is able to choose a party in which it is able to choose a party."}, {"heading": "6 Experiments", "text": "The goal was to predict the preferences of a particular user, i.e. an arrangement of the 1682 movies according to the subratings of the user. Note that, as observed in Example 2, the number of tested images in the ranking order of the tested results Y is the collection of all tested sequences of a predefined length. (1 \u2212 character is finite (albeit extremely large) and we can apply Alg. (1) to the ranking. (1) The ranking problem with the ranking loss [7] is determined by the ranking order of the tested methods Y (y, y \"s) = M (1 \u2212 character) ij (2 \u2212 yj)) / 2, (22) with M is the number of movies in the database, y Y is a vector of the M integers. (1, M} without repetition, where yi corresponds to the ranking order)."}, {"heading": "7 Conclusions and Future Work", "text": "In this paper, we looked at the problem of structured prediction from the perspective of statistical learning theory. We proposed a learning algorithm for structured prediction, which is divided into a learning and prediction step, similar to previous methods in the literature. We investigated the statistical properties of the proposed algorithm by applying a strategy inspired by substitute methods. In particular, we identified a large family of loss functions for which it is natural to identify a corresponding replacement problem. This perspective allows us to demonstrate a derivation of the algorithm proposed in this thesis. Furthermore, by exploiting a comparative inequality in relation to the original and surrounding problems, we were able to demonstrate universal consistency and generalization limits under mild assumption. In particular, the limits proven in this thesis recover those already known for classifying the least square surfaces of which our approach can be regarded as generalization. We supported our theoretical analysis with experiments showing that promising issues may be left open to a number of structured problems."}, {"heading": "Appendix", "text": "The appendix of this work is divided into the following three sections: A Proofs of Fisher consistency and comparison inequality (Thm. 2).B Universal Consistency and Generalization Bounds for Alg. 1. (Thm. 4 and 5).C The characterization of a large family of satisfactory Asm. 1 (Thm. 19)."}, {"heading": "Mathematical Setting", "text": "In the following we will always assume that X and Y are the main rooms, namely full metric rooms (>). < / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}, {"heading": "A Surrogate Problem, Fisher Consistency and Comparison In-", "text": "In this section, we focus on the surrogate framework presented in Sec. 3 and prove that it is consistent and that the comparative inequality is constant in each case. To do this, we will first of all provide the solution (s) of the expected risk minimization in Eq. (13) We remember that in our environment surroundings surrogate risk has been defined as functional risk (g) = x x x x x x x x x x x x x x x x x x x x x x x x x (x). Note: In most of our results, we assume that Y \u2192 HY is continuous (according to Asm. 1). Following, when bounded, we will denote with Q = supy (y) of the Y distribution. We start with a preliminary problem that is necessary to Lemma 1 and Thm. 2.Lemma 8. Let us assume a compact Y distribution. In these settings, we always have Q = maxy (y)."}, {"heading": "B Learning Bounds for Structured Prediction", "text": "In this section, we will focus on the analysis of the structured prediction algorithm proposed in this paper (Alg. 1). In particular, we will first prove that, given the minimizer g: X \u2192 HY of empirical risk at Equation (5), its decoding can be calculated in practice according to Alg. 1. Subsequently, we will report on the evidence for the universal consistency of such an approach (Thm. 4) and the limits of generalization (Thm. 5)."}, {"heading": "Notation", "text": "Let's call k \u00b7 X \u00b7 R a positive semidefinitive function on X, we call the Hilbert space defined by the CompletionHX = span {k (x, \u00b7) (65) according to the norm generated by the internal product < k (x, \u00b7), k (x, \u00b7) > HX (x, x). The spaces HX constructed in this way are known as Hilbert reproduction spaces and there is a one-to-one relationship between a kernel k and its associated RKHS. For more details on RKHS, we refer the reader to [5]. In the face of a kernel k, we will refer below to the function Hilbert spaces (x) = k (x, \u00b7) as an HX one-to-relationship between a kernel and its associated RKHS. We say that a kernel HX (x) is referred to as an HX-One HX-One (X, HX) as an X-One (X)."}, {"heading": "B.1 Reproducing Kernel Hilbert Spaces for Vector-valued Functions", "text": "We begin our analysis with the introduction of the concept of reproduction of Hilbert space (RKHS) for vector-weighted functions. Here, we offer a brief summary of the most important properties that will be useful below. We refer the reader to [6,7] for a more in-depth introduction to the topic. Similar to the case of scalar functions, an RKHS for vector-weighted functions g: X \u2192 H, where H is a divisible Hilbert space, is uniquely characterized by a so-called positive type, which is a nucleus evaluated by the operator."}, {"heading": "B.1.1 Separable Vector Valued Kernels", "text": "In this work we limit ourselves to the specific case of RKHS for vector-rated functions with associated kernel (X \u00b7 X \u00b7 HY). < (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D (D) (D) (D) (D (D) (D) (D (D) (D) (D (D) (D (D) (D) (D (D) (D (D) (D) (D (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D"}, {"heading": "B.2 The Structured Prediction Algorithm", "text": "In this section we prove that alg. 1 corresponds to the decoding of the spare part quantity (= *). (Eq. (Eq. (5)) via a map d: HY \u2192 Y-satisfactory Eq. (14). Let us remember that in term 15 we have proven that the vector-weighted RKHS G by a kernel (x, x) = k (x) \u2212 x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "B.3 Universal Consistency", "text": "HX (HY) - HX (HY) - HX (HY) - HY - (HY) - X (HY) - HX (HY) - HX (HY) - HX (HY) - HX (HY) - HX (HY) - HX (HY) - HX (HY) - HX (HY) - HX - (HY) - HX (HY) - HX (HY) - (HY) - (HY) - (HX) - (X) - (HX) - (HX) - (HX) - (HY) - HX (HY) - (HY) - (HY) - (HY) - (HY) - (HY) - (HX) (HX) - (HX) (HY) - (HY) - (HY) - (HX - (HY) - (HY) - (HY) - (HX - (HY) (HY) - (HY) (HX - (HY) - (HY) - (HY) (HX - (HY) - (HY) - (HY) - (HY) - (HX - (HY) - (HY) - (HY) - (HX - (HY) - (HY) - (HX - (HY) - (HY) - (HY) - (HX - (HY) - (HY) - (HX - (HY)) - (HX - (HY) - (HY) - (HX - (HX) - (HY) - (HX) - (HX - (HY)) - (HX) - (HX - (HX) - (HX) - (HX) - (HX) - (HX - (HY) - (HY) - (HX) - (HX) - (HX) - (HX - (HY) - (HX) - (HX - (HY) - (HX) (HY) - (HX) - (HX) (H"}, {"heading": "B.4 Generalization Bounds", "text": "G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-"}, {"heading": "C Examples of Loss Functions", "text": "In this section we prove that a wide range of functions, which are taken into account in Example 1, fulfils the loss trick (Asm. 1). The following are the following: \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2."}], "references": [{"title": "Real analysis and probability, volume 74", "author": ["Richard M Dudley"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Support Vector Machines. Information Science and Statistics", "author": ["Ingo Steinwart", "Andreas Christmann"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Infinite dimensional analysis: a hitchhiker\u2019s", "author": ["Charalambos D Aliprantis", "Kim Border"], "venue": "guide. Springer Science & Business Media,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Convex analysis and measurable multifunctions, volume 580", "author": ["Charles Castaing", "Michel Valadier"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Reproducing kernel Hilbert spaces in probability and statistics", "author": ["Alain Berlinet", "Christine Thomas-Agnan"], "venue": "Springer Science & Business Media,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Kernels for multi\u2013task learning", "author": ["Charles A Micchelli", "Massimiliano Pontil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Vector valued reproducing kernel hilbert spaces of integrable functions and mercer theorem", "author": ["Claudio Carmeli", "Ernesto De Vito", "Alessandro Toigo"], "venue": "Analysis and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Umanit\u00e1. Vector valued reproducing kernel hilbert spaces and universality", "author": ["Claudio Carmeli", "Ernesto De Vito", "Alessandro Toigo", "Veronica"], "venue": "Analysis and Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Regularization of inverse problems, volume 375", "author": ["Heinz Werner Engl", "Martin Hanke", "Andreas Neubauer"], "venue": "Springer Science & Business Media,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Optimal rates for the regularized leastsquares algorithm", "author": ["Andrea Caponnetto", "Ernesto De Vito"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Learning theory estimates via integral operators and their approximations", "author": ["Steve Smale", "Ding-Xuan Zhou"], "venue": "Constructive approximation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Fourier series and wavelets", "author": ["J-P Kahane"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "On the absolute convergence of multiple fourier series", "author": ["Ferenc M\u00f3ricz", "Antal Veres"], "venue": "Acta Mathematica Hungarica,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "[1\u20133].", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "[1\u20133].", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[1\u20133].", "startOffset": 0, "endOffset": 5}, {"referenceID": 3, "context": "Indeed, this has motivated the extension of methods such as support vector machines to structured problems [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 4, "context": "Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 5, "context": "Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 6, "context": "Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8].", "startOffset": 163, "endOffset": 166}, {"referenceID": 7, "context": "Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8].", "startOffset": 190, "endOffset": 193}, {"referenceID": 8, "context": "This fact allows to define a (least squares) surrogate loss and cast the problem in a multi-output regularized learning framework [9\u201311].", "startOffset": 130, "endOffset": 136}, {"referenceID": 9, "context": "This fact allows to define a (least squares) surrogate loss and cast the problem in a multi-output regularized learning framework [9\u201311].", "startOffset": 130, "endOffset": 136}, {"referenceID": 10, "context": "This fact allows to define a (least squares) surrogate loss and cast the problem in a multi-output regularized learning framework [9\u201311].", "startOffset": 130, "endOffset": 136}, {"referenceID": 3, "context": "This strategy was extended in [4] for the popular SVMstruct and adopted also in a variety of approaches for structured prediction [1,12,14].", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "This strategy was extended in [4] for the popular SVMstruct and adopted also in a variety of approaches for structured prediction [1,12,14].", "startOffset": 130, "endOffset": 139}, {"referenceID": 11, "context": "This strategy was extended in [4] for the popular SVMstruct and adopted also in a variety of approaches for structured prediction [1,12,14].", "startOffset": 130, "endOffset": 139}, {"referenceID": 12, "context": "(3) This choice of\u25b3 was originally considered in Kernel Dependency Estimation (KDE) [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "1 essentially reduces to [12, 14] and recalling their derivation is insightful.", "startOffset": 25, "endOffset": 33}, {"referenceID": 8, "context": "G is the reproducing kernel Hilbert space for vector-valued functions [9] with inner product \u3008k(xi, \u00b7)ci, k(xj, \u00b7)cj\u3009G = k(xi, xj)\u3008ci, cj\u3009HY", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "1 we consider ideas from surrogate approaches [7,17,18] and in particular [5].", "startOffset": 46, "endOffset": 55}, {"referenceID": 4, "context": "1 we consider ideas from surrogate approaches [7,17,18] and in particular [5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 11, "context": "Therefore, the approach in [12, 14] to recover f\u0302(x) = argminy L(g(x), y) can be interpreted as the result f\u0302(x) = d\u25e6 \u011d(x) of a suitable decoding of \u011d(x).", "startOffset": 27, "endOffset": 35}, {"referenceID": 8, "context": "(13) corresponds to a vector-valued regression problem [9\u201311].", "startOffset": 55, "endOffset": 61}, {"referenceID": 9, "context": "(13) corresponds to a vector-valued regression problem [9\u201311].", "startOffset": 55, "endOffset": 61}, {"referenceID": 10, "context": "(13) corresponds to a vector-valued regression problem [9\u201311].", "startOffset": 55, "endOffset": 61}, {"referenceID": 9, "context": "5, our analysis on \u011d borrows ideas from [10] and extends their result to our setting for the case of HY infinite dimensional (i.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Indeed, note that in this case [10] cannot be applied to the estimator \u011d considered in this work (see Appendix Sec.", "startOffset": 31, "endOffset": 35}, {"referenceID": 4, "context": "Promising results in this direction can be found in [5], where the Tsybakov condition was generalized to the multi-class setting and led to a tight comparison inequality analogous to the one for the binary setting.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "Indeed, it is not clear how the approach in [5] could be further generalized to the case where Y has infinite cardinality.", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "Rank Loss Linear [7] 0.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "012 SVM Struct [4] 0.", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "Loss KDE [15] (Gaussian) Alg.", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "Table 2: Digit reconstruction on USPS with KDE method [15] (with Gaussian loss) and Alg.", "startOffset": 54, "endOffset": 58}, {"referenceID": 12, "context": "2 we discussed the relation between KDE [12, 15] and Alg.", "startOffset": 40, "endOffset": 48}, {"referenceID": 11, "context": "4 and 5 to prove universal consistency and generalization bounds for methods such as [12,14].", "startOffset": 85, "endOffset": 92}, {"referenceID": 11, "context": "We are not aware of previous results proving consistency (and generalization bounds) for the KDE methods in [12, 14].", "startOffset": 108, "endOffset": 116}, {"referenceID": 3, "context": "A popular approach to structured prediction is the Support Vector Machine for Structured Outputs (SVMstruct) [4] that extends ideas from the well-known SVM algorithm to the structured setting.", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "Moreover, we note that generalization studies for SVMstruct are available [3] (Ch.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "1 to the ranking problem using the rank loss [7]", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "In the definition of \u25b3rank, \u03b3(y)ij denotes the costs (or reward) of having movie j ranked higher than movie i and, similarly to [7], we set \u03b3(y)ij equal to the difference of ratings provided by user associated to y (from 1 to 5).", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "1, a linear kernel on features similar to those proposed in [7], which were computed based on users\u2019 profession, age, similarity of previous ratings, etc.", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "1 for \u25b3rank is NPhard (see [7]) we adopted the Feedback Arc Set approximation (FAS) proposed in [27] to approximate the f\u0302(x) of Alg.", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "1 (Ours) with surrogate ranking methods using a Linear [7], Hinge [24] or Logistic [25] loss and Struct SVM [4]3.", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "1 (Ours) with surrogate ranking methods using a Linear [7], Hinge [24] or Logistic [25] loss and Struct SVM [4]3.", "startOffset": 108, "endOffset": 111}, {"referenceID": 12, "context": "We considered the USPS4 digits reconstruction experiment originally proposed in [15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "The standard approach is to use a Gaussian kernel kG on images in input and adopt KDE methods such as [12,14,15] with loss \u25b3G(y, y \u2032) = 1\u2212 kG(y, y \u2032).", "startOffset": 102, "endOffset": 112}, {"referenceID": 12, "context": "The standard approach is to use a Gaussian kernel kG on images in input and adopt KDE methods such as [12,14,15] with loss \u25b3G(y, y \u2032) = 1\u2212 kG(y, y \u2032).", "startOffset": 102, "endOffset": 112}, {"referenceID": 0, "context": "[1] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Andrej Karpathy and Li Fei-Fei.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Thomas Hofmann Bernhard Sch\u00f6lkopf Alexander J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, and Jean-Jacques Slotine.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Wei Gao and Zhi-Hua Zhou.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] John C Duchi, Lester W Mackey, and Michael I Jordan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Ingo Steinwart, Andreas Christmann, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Charles A Micchelli and Massimiliano Pontil.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Andrea Caponnetto and Ernesto De Vito.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] Jason Weston, Olivier Chapelle, Vladimir Vapnik, Andr\u00e9 Elisseeff, and Bernhard Sch\u00f6lkopf.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "We recall [1] that \u03c1(y|x) is a regular conditional distribution and its domain, which we will denoteD\u03c1|X in the following, is a measurable set contained in the support of \u03c1X and corresponds to the support of \u03c1X up to a set of measure zero.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "In the next Lemma, following [2] we show that the problem in Eq.", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "153) of [3]), namely continuous in y for each x \u2208 X and measurable in x for each y \u2208 Y.", "startOffset": 8, "endOffset": 11}, {"referenceID": 2, "context": "605) of [3] (or Aumann\u2019s measurable selection principle [2, 4]), we have that m is measurable and that there exists a measurable f\u2217 : X \u2192 Y such that r(x, f\u2217(x)) = m(x) for all x \u2208 X .", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "605) of [3] (or Aumann\u2019s measurable selection principle [2, 4]), we have that m is measurable and that there exists a measurable f\u2217 : X \u2192 Y such that r(x, f\u2217(x)) = m(x) for all x \u2208 X .", "startOffset": 56, "endOffset": 62}, {"referenceID": 3, "context": "605) of [3] (or Aumann\u2019s measurable selection principle [2, 4]), we have that m is measurable and that there exists a measurable f\u2217 : X \u2192 Y such that r(x, f\u2217(x)) = m(x) for all x \u2208 X .", "startOffset": 56, "endOffset": 62}, {"referenceID": 1, "context": "Therefore, since \u03c1(y|x) is a regular conditional probability, we have that g\u2217 is measurable on X (see for instance [2]).", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "416 in [3] the integral of \u25b3(d \u25e6 g(x), y) exists and therefore |E(d \u25e6 g)| \u2264 \u222b", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "605) of [3] (or Aumann\u2019s measurable selection principle [2,4]).", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "605) of [3] (or Aumann\u2019s measurable selection principle [2,4]).", "startOffset": 56, "endOffset": 61}, {"referenceID": 3, "context": "605) of [3] (or Aumann\u2019s measurable selection principle [2,4]).", "startOffset": 56, "endOffset": 61}, {"referenceID": 4, "context": "For more details on RKHS we refer the reader to [5].", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "The continuity of k with the fact that X is Polish implies HX to be separable [5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "We refer the reader to [6,7] for a more in-depth introduction on the topic.", "startOffset": 23, "endOffset": 28}, {"referenceID": 6, "context": "We refer the reader to [6,7] for a more in-depth introduction on the topic.", "startOffset": 23, "endOffset": 28}, {"referenceID": 7, "context": "Indeed, it was proven in [8] (see Example 14) that if k is a universal scalar kernel, then \u0393(\u00b7, \u00b7) = k(\u00b7, \u00b7)IHY is universal.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "We can conclude that R attains a minimum on G if and only if the range of S\u2217Z is contained in the range of C, namely Ran(S\u2217Z) \u2286 Ran(C) \u2282 HX (see [9] Chap.", "startOffset": 145, "endOffset": 148}, {"referenceID": 9, "context": "In this setting, the problem of finding a probabilistic bound has been studied (see [10] and references therein).", "startOffset": 84, "endOffset": 88}, {"referenceID": 9, "context": "of a decomposition of the surrogate excess risk that is similar to the one in [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "However, note that we cannot direct apply [10] to our setting, since in [10] the operator-valued kernel \u0393 associated to G is required to be such that \u0393(x, x \u2032) is trace class \u2200x, x \u2032 \u2208 X , which does not hold for the kernel used in this work, namely \u0393(x, x \u2032) = k(x, x )IHY when HY is infinite dimensional.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "However, note that we cannot direct apply [10] to our setting, since in [10] the operator-valued kernel \u0393 associated to G is required to be such that \u0393(x, x \u2032) is trace class \u2200x, x \u2032 \u2208 X , which does not hold for the kernel used in this work, namely \u0393(x, x \u2032) = k(x, x )IHY when HY is infinite dimensional.", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "According to [10], we define the following quantity A(\u03bb) = \u03bb\u2016Z\u2217(L+ \u03bb)\u2016HS.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "X Tr ( \u011c ( \u03c6(x)\u2297\u03c6(x) ) \u011c\u2217 ) \u2212 2Tr ( \u011c ( \u03c6(x)\u2297 g\u2217(x) )) + Tr(g\u2217(x)\u2297 g(x))d\u03c1X (x) (98) = Tr(\u011cS\u2217S\u011c) \u2212 2Tr(\u011cS\u2217Z) + Tr(Z\u2217Z) = \u2016\u011cS\u2217 \u2212 Z\u2016HS (99) To bound \u2016\u011cS\u2217 \u2212 Z\u2217\u2016HS, we proceed with a decomposition similar to the one in [10].", "startOffset": 215, "endOffset": 219}, {"referenceID": 10, "context": "Thus, by applying Lemma 2 of [11], we have", "startOffset": 29, "endOffset": 33}, {"referenceID": 10, "context": "Thus we can again apply Lemma 2 of [11], obtaining", "startOffset": 35, "endOffset": 39}, {"referenceID": 1, "context": "(126) This is a standard assumption for universal consistency (see [2]).", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "pag 263 of [1]) on the sequence (En)n\u2208N and conclude that the statement lim n\u2192\u221e E(f\u0302n) \u2212 E(f\u2217) > 0, (134) holds with probability 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "Y = [0, 1] with d \u2208 N, and the mixed partial derivative L(y, y \u2032) = \u2202 \u25b3(y1,.", "startOffset": 4, "endOffset": 10}, {"referenceID": 0, "context": "A discrete probability distribution (or a normalized histogram) over M entries can be represented as a y = (yi)i=1 \u2208 Y \u2282 [0, 1] the M-simplex, namely \u2211Mi=1 yi = 1 and yi \u2265 0 \u2200i = 1, .", "startOffset": 121, "endOffset": 127}, {"referenceID": 0, "context": "\u25b3H is obtained by M summations of \u25b30 on [0, 1], by Thm.", "startOffset": 40, "endOffset": 46}, {"referenceID": 0, "context": "Indeed the function (r \u2212 r \u2032)2/(r + r \u2032) satisfies point 2 on Y = [0, 1], then point 6 and 4 are applied.", "startOffset": 66, "endOffset": 72}, {"referenceID": 11, "context": "For the continuity, see [14] (pag.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "291 of [15]).", "startOffset": 7, "endOffset": 11}], "year": 2017, "abstractText": "We propose and analyze a regularization approach for structured prediction problems. We characterize a large class of loss functions that allows to naturally embed structured outputs in a linear space. We exploit this fact to design learning algorithms using a surrogate loss approach and regularization techniques. We prove universal consistency and finite sample bounds characterizing the generalization properties of the proposed methods. Experimental results are provided to demonstrate the practical usefulness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}