{"id": "1605.07659", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Adaptive Newton Method for Empirical Risk Minimization to Statistical Accuracy", "abstract": "We consider empirical risk minimization for large-scale datasets. We introduce Ada Newton as an adaptive algorithm that uses Newton's method with adaptive sample sizes. The main idea of Ada Newton is to increase the size of the training set by a factor larger than one in a way that the minimization variable for the current training set is in the local neighborhood of the optimal argument of the next training set. This allows to exploit the quadratic convergence property of Newton's method and reach the statistical accuracy of each training set with only one iteration of Newton's method. We show theoretically and empirically that Ada Newton can double the size of the training set in each iteration to achieve the statistical accuracy of the full training set with about two passes over the dataset.", "histories": [["v1", "Tue, 24 May 2016 21:02:50 GMT  (50kb,D)", "http://arxiv.org/abs/1605.07659v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["aryan mokhtari", "hadi daneshmand", "aur\u00e9lien lucchi", "thomas hofmann", "alejandro ribeiro"], "accepted": true, "id": "1605.07659"}, "pdf": {"name": "1605.07659.pdf", "metadata": {"source": "CRF", "title": "Adaptive Newton Method for Empirical Risk Minimization to Statistical Accuracy", "authors": ["Aryan Mokhtari"], "emails": ["aryanm@seas.upenn.edu", "aribeiro@seas.upenn.edu"], "sections": [{"heading": "1 Introduction", "text": "A hallmark of empirical risk mitigation (ERM) on large datasets is that the evaluation of parentage guidelines requires a complete overrun of the dataset. As this is not desirable due to the large number of training samples, we have seen fundamental progress in the development of alternatives with faster convergence. A partial list of this momentous literature includes Nesterov acceleration [18, 2], stochastic averaging of gradients [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25] and hybrid algorithms [28, 13]. When second-order stochastic methods occur, the first challenge is while the evaluation of Hessians is as costly as the evaluation of gradients, which turns out to be more challenging."}, {"heading": "2 Empirical risk minimization", "text": "We want to solve ERM problems = 1 f (w, zk), w (n). We want to formally consider this problem as argument w (w). (n) We want an argument w (w): = EZ [f (w, Z), w (w), w (w), w (w), w (n). (1) The loss in (1) cannot be evaluated because the distribution of Z is unknown. However, we have access to a training set T = {z1,.., zN) that contains the independent samples z1. (zN) that we can use to estimate L (w). Therefore, we consider a subset of T and settle for minimizing the empirical risk Ln (w): = (n)."}, {"heading": "3 Ada Newton", "text": "The solution (4) assumes that the problem has been solved within its statistical accuracy (7). (wm) \"We want to take a variable risk that we do not want to take.\" (wm) + cVnwm (5). (wm) + cVnwm (5). (3). (5). (4). (5). (4). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5. (5). (5). (5). (5). (5. (5). (5). (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5.).). (5.).). (5. (5.). (5.).). (5.). (5.).). (5.). (5. (5.). (5.).). (5.).). (5.). (5.). (5.). (5.). (5.). (5. (5.). (5.). (5.). (5.). (5.). (5.). (5.).). (5.).). (5.). (5. (5.). (5.).). (5. (5.).)"}, {"heading": "4 Convergence Analysis", "text": "In this section we investigate the conditions in Assumption 1 (14). In this area we initially assumed the following conditions: \"We are satisfied.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\". \"\" We. \"\" \"\" We. \".\" \"\" We. \"\" \".\" \"\". \"\" \"We.\" \"\" \"\" \".\" \"\".. \"\" \"\".. \"\" \"..\" \"\" \"..\" \"\" \"..\" \"\" \"\".. \"\". \"\" \"\".. \"\" \"..\" \"\" \"..\" \"\" \"..\" \"\" \"\".. \"\" \"\" \"..\" \"\" \"\" \"..\" \""}, {"heading": "5 Experiments", "text": "In this section, we will examine the performance of the proposed Ada Newton method and compare it with the state of the art in solving a large-scale classification problem. We will consider three algorithms to compare with the proposed Ada Newton method, one of which is the classic Newton method with trace line search. The second algorithm is Stochastic Descent (SGD) and the last is the SAGA algorithm introduced in [7]. In our experiments, we use logistical losses and set the regulation parameters to 200 and Vn = 1 / n. The step size of the SGD in our experiments is 2 \u00d7 10 \u2212 2. Note: Selecting larger step sizes leads to faster but less precise convergence and the selection of smaller step sizes improves accuracy with the price of lower convergence."}, {"heading": "6 Discussions", "text": "It follows from this that the Newton iteration makes the subopti-mality gapRn (wn) -Rn (w \u0445 n) square the suboptimality gapRn (wm) -Rn. This results in condition (9) and is the fact that makes Newton steps valuable when it comes to increasing the sample size. If we replace Newton iterations with a method with a linear convergence rate, the orders of both sides are the same under condition (9), which would make an aggressive increase in the sample size.In Section 1, we point out four reasons that call into question the development of stochastic Newton methods. It would not be entirely correct to call Ada Newton a stochastic method because it does not rely on stochastic lineage directions. However, it is a method for ERM to apply Newton methods."}, {"heading": "Acknowledgement", "text": "We would like to thank Hadi Daneshmand, Aurelien Lucci and Thomas Hofmann for useful discussions on the use of adaptive sample sizes to solve large ERM problems and on the importance of adaptive regulatory coefficients."}, {"heading": "7 Appendix", "text": "In this section we will define the difference between the empirical losses in the areas Sn (w) and Sm (w), where they are chosen so that we define n and m as the number of risks in training Sn and Sm and m as the number of risks in training Sn and Sm, respectively, the absolute amount of the difference between the empirical losses is limited from top to bottom. \u2212 Lm (w) \u2212 m"}, {"heading": "7.1 Proof of Proposition 3", "text": "s method we know that the variable wm (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n (wm) n ("}, {"heading": "7.2 Proof of Proposition 5", "text": "Note that the difference Rn (wm) \u2212 Rn (w) \u2212 Rm (w) \u2212 Rm (w) \u2212 Rm (w) + Rm (w) \u2212 n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (m) n (n) n (n) n (n) n (n) n (n) n (n) n (m) n (n) n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n (n) n) n (n) n (n n) n (n (n) n (n) n (n) n (n) n (n (n) n) n (n (n) n n (n) n (n) n (n (n) n (n) n n (n (n) n) n (n (n) n) n (n) n (n (n) n) n (m \u2212 n (n) n (n) n) n n (n (n n) n (n n n n n n (n) n (n) n (n"}], "references": [{"title": "Convexity, classification, and risk bounds", "author": ["Peter L Bartlett", "Michael I Jordan", "Jon D McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM journal on imaging sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Sgd-qn: Careful quasi-newton stochastic gradient descent", "author": ["Antoine Bordes", "L\u00e9on Bottou", "Patrick Gallinari"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "The tradeoffs of large scale learning", "author": ["Olivier Bousquet", "L\u00e9on Bottou"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Starting small\u2013learning with adaptive sample sizes", "author": ["Hadi Daneshmand", "Aurelien Lucchi", "Thomas Hofmann"], "venue": "arXiv preprint arXiv:1603.02839,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Convergence rates of sub-sampled newton methods", "author": ["Murat A Erdogdu", "Andrea Montanari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Competing with the empirical risk minimizer in a single pass", "author": ["Roy Frostig", "Rong Ge", "Sham M Kakade", "Aaron Sidford"], "venue": "arXiv preprint arXiv:1412.6606,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Stochastic block bfgs: Squeezing more curvature out of data", "author": ["Robert M Gower", "Donald Goldfarb", "Peter Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1603.09649,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "A globally convergent incremental newton method", "author": ["Mert G\u00fcrb\u00fczbalaban", "Asuman Ozdaglar", "Pablo Parrilo"], "venue": "Mathematical Programming,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Semi-stochastic gradient descent methods", "author": ["Jakub Kone\u010dn\u1ef3", "Peter Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1312.1666,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Res: Regularized stochastic bfgs algorithm", "author": ["Aryan Mokhtari", "Alejandro Ribeiro"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Global convergence of online limited memory bfgs", "author": ["Aryan Mokhtari", "Alejandro Ribeiro"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "A linearly-convergent stochastic l-bfgs algorithm", "author": ["Philipp Moritz", "Robert Nishihara", "Michael I Jordan"], "venue": "arXiv preprint arXiv:1508.02087,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Introductory lectures on convex programming volume", "author": ["Yu Nesterov"], "venue": "i: Basic course", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Yurii Nesterov"], "venue": "Technical report,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["Boris T Polyak", "Anatoli B Juditsky"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1951}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Nicolas L Roux", "Mark Schmidt", "Francis R Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["Nicol N Schraudolph", "Jin Yu", "Simon G\u00fcnter"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Learnability, stability and uniform convergence", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Mathematical Programming,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "The nature of statistical learning theory", "author": ["Vladimir Vapnik"], "venue": "Springer Science & Business Media,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Linear convergence with condition number independent access of full gradients", "author": ["Lijun Zhang", "Mehrdad Mahdavi", "Rong Jin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "First order stochastic optimization has a long history [20, 19] but the last decade has seen fundamental progress in developing alternatives with faster convergence.", "startOffset": 55, "endOffset": 63}, {"referenceID": 18, "context": "First order stochastic optimization has a long history [20, 19] but the last decade has seen fundamental progress in developing alternatives with faster convergence.", "startOffset": 55, "endOffset": 63}, {"referenceID": 17, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 79, "endOffset": 86}, {"referenceID": 1, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 79, "endOffset": 86}, {"referenceID": 20, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 118, "endOffset": 125}, {"referenceID": 6, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 118, "endOffset": 125}, {"referenceID": 11, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 146, "endOffset": 154}, {"referenceID": 26, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 146, "endOffset": 154}, {"referenceID": 23, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 180, "endOffset": 188}, {"referenceID": 24, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 180, "endOffset": 188}, {"referenceID": 27, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 212, "endOffset": 220}, {"referenceID": 12, "context": "A partial list of this consequential literature includes Nesterov acceleration [18, 2], stochastic averaging gradient [21, 7], variance reduction [12, 27], dual coordinate methods [24, 25], and hybrid algorithms [28, 13].", "startOffset": 212, "endOffset": 220}, {"referenceID": 10, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 84, "endOffset": 87}, {"referenceID": 21, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 150, "endOffset": 173}, {"referenceID": 2, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 150, "endOffset": 173}, {"referenceID": 13, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 150, "endOffset": 173}, {"referenceID": 14, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 150, "endOffset": 173}, {"referenceID": 15, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 150, "endOffset": 173}, {"referenceID": 9, "context": "This difficulty is addressed by incremental computations in [11] and subsampling in [8] or circumvented altogether in stochastic quasi-Newton methods [22, 3, 14, 15, 16, 10].", "startOffset": 150, "endOffset": 173}, {"referenceID": 5, "context": "In this paper we attempt to overcome (i)-(iv) with the Ada Newton algorithm that combines the use of Newton iterations with adaptive sample sizes [6].", "startOffset": 146, "endOffset": 149}, {"referenceID": 0, "context": "Bounds of order Vn = O(1/n) have been derived more recently under stronger regularity conditions that are not uncommon in practice, [1, 9, 4]", "startOffset": 132, "endOffset": 141}, {"referenceID": 8, "context": "Bounds of order Vn = O(1/n) have been derived more recently under stronger regularity conditions that are not uncommon in practice, [1, 9, 4]", "startOffset": 132, "endOffset": 141}, {"referenceID": 3, "context": "Bounds of order Vn = O(1/n) have been derived more recently under stronger regularity conditions that are not uncommon in practice, [1, 9, 4]", "startOffset": 132, "endOffset": 141}, {"referenceID": 22, "context": "Since the regularization in (4) is of order Vn and (3) holds, the difference between Rn(w n) and L(w\u2217) is also of order Vn \u2013 this may be not as immediate as it seems; see [23].", "startOffset": 171, "endOffset": 175}, {"referenceID": 4, "context": ", Chapter 9 of [5].", "startOffset": 15, "endOffset": 18}, {"referenceID": 16, "context": "11 of [17] which shows that", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": ", Chapter 9 of [5].", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "Thus, according to the quadratic convergence rate of Newton\u2019s method for self-concordant functions [5], we know that the Newton decrement has a quadratic convergence and we can write", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "The second algorithm is Stochastic Gradient Descent (SGD) and the last one is the SAGA algorithm introduced in [7].", "startOffset": 111, "endOffset": 114}], "year": 2016, "abstractText": "We consider empirical risk minimization for large-scale datasets. We introduce Ada Newton as an adaptive algorithm that uses Newton\u2019s method with adaptive sample sizes. The main idea of Ada Newton is to increase the size of the training set by a factor larger than one in a way that the minimization variable for the current training set is in the local neighborhood of the optimal argument of the next training set. This allows to exploit the quadratic convergence property of Newton\u2019s method and reach the statistical accuracy of each training set with only one iteration of Newton\u2019s method. We show theoretically and empirically that Ada Newton can double the size of the training set in each iteration to achieve the statistical accuracy of the full training set with about two passes over the dataset.", "creator": "LaTeX with hyperref package"}}}