{"id": "1605.07669", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems", "abstract": "The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user's intent is known in advance or data is available to pre-train a task success predictor off-line. In practice neither of these apply for most real world applications. Here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning.", "histories": [["v1", "Tue, 24 May 2016 21:56:08 GMT  (703kb,D)", "https://arxiv.org/abs/1605.07669v1", "Accepted as a long paper in ACL 2016"], ["v2", "Thu, 2 Jun 2016 14:01:07 GMT  (857kb,D)", "http://arxiv.org/abs/1605.07669v2", "Accepted as a long paper in ACL 2016"]], "COMMENTS": "Accepted as a long paper in ACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["pei-hao su", "milica gasic", "nikola mrksic", "lina maria rojas-barahona", "stefan ultes", "david vandyke", "tsung-hsien wen", "steve j young"], "accepted": true, "id": "1605.07669"}, "pdf": {"name": "1605.07669.pdf", "metadata": {"source": "CRF", "title": "On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems", "authors": ["Pei-Hao Su", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung-Hsien Wen"], "emails": ["sjy}@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "They can be roughly divided into two categories: choreographed systems aimed at engaging with users and providing appropriate contextually relevant answers (Vinyals and Le, 2015; Serban et al., 2015), and task-oriented systems designed to help users achieve specific goals (e.g. hotels, movies, or bus schedules) (Daubigney et al., 2014; Young et al., 2013), which are typically designed using a structured ontology (or database scheme) that defines the domain the system can talk about. Teaching a system how to respond appropriately in task-based dialogues is not trivial. This dialog task is often formulated as a manually defined flow of dialogue that directly determines the quality of the interaction. More recently, dialogue management has been formulated as an enhancement of learning (RL) that can be automatically optimized (Levin and Pieraccini, 1997; Roy Williams, 2000; and Young 2000)."}, {"heading": "2 Related Work", "text": "It has been an active field of research since the late 1990s. Walker et al. (1997) proposed the PARADISE framework, which used a linear function of task fulfillment and various dialog functions, such as dialogue duration, to infer user satisfaction, which was later used as a reward function for learning dialogue policy (Rieser and Lemon, 2011). However, task fulfillment is rarely available when the system interacts with real users and concerns have also been raised about the theoretical validity of the model (Larsen, 2003). Several approaches have been adopted for learning a dialogue reward model that includes a corpus of annotated dialogues. Yang et al. (2012) used collaborative filtering to derive user preferences (El Asri et al., 2014; Su et al., 2015b) to enrich the reward function of dialogue in order to accelerate political learning, and also showed a strong link (2015)."}, {"heading": "3 Proposed Framework", "text": "The proposed system framework is shown in Figure 2. It is divided into three main parts: a dialog policy, a dialog embedding function, and an active user feedback reward model. At the end of each dialog, a series of turn-level features ft is extracted and fed into an embedding function \u03c3 to obtain a dialog representation of fixed dimensions d that serves as the input space of the reward model R. This reward is modeled as a Gaussian process that provides an estimate of the task success for each input point along with a measurement uncertainty of the estimate. On the basis of this uncertainty, R decides whether the user should be asked for feedback or not. It then returns an amplification signal to update the dialog policy \u03c0, which is trained using the GP-SARSA algorithm (Gas-ic'and Young, 2014). GP-SARSA also uses a Gaussian process assessment to provide a minimum value-learning algorithm that is available in the sample learning algorithm."}, {"heading": "3.1 Unsupervised Dialogue Embeddings", "text": "In order to model user feedback about dialogs of varying lengths, an embedding function is used to map each dialogue into a fixed-dimensional continuous space. Embedding functions have recently attracted attention, particularly for word representations, and have increased performance in several natural language processing tasks (Mikolov et al., 2013; Turian et al., 2010; Levy and Goldberg, 2014). Embedding has also been successfully applied to machine translation (MT), where it is possible to map phrases of varying lengths onto fixed-length vectors using an RNN encoder decoder (Cho et al., 2014). Similar to MT, embedding dialogues allows variable length sequences of utterances to be mapped into an appropriate fixed-length vector. Although embedding is used here to create a fixed-dimensional space for the success of the GPC-based task, it should be pointed out to others that it potentially facilitates a variety of tasks."}, {"heading": "3.2 Active Reward Learning", "text": "It is particularly appealing because it can learn from a small number of observations by taking advantage of the correlations defined by a core function and providing a metric for the uncertainty of its estimates. In the context of spoken dialogue systems, it has been successfully used to optimize RL policies (Gaussian model), which includes estimating the probability p (y) and Young, 2014; Casanueva et al., 2015) and the IRL reward regression (Kim et al., 2014).Here, we propose to model the dialogue as a Gaussian process (GP), which includes estimating the probability p (y) and Young, D) that the task was successful in light of the current dialogue representation d and pool D, which contains previously classified dialogues. We present this as a classification problem where evaluation is a binary observation."}, {"heading": "4 Experimental results", "text": "The target application is a live telephone-based spoken dialogue system that provides restaurant information for the Cambridge (UK) area. The domain consists of about 150 locations, each with 6 slots (attributes), 3 of which can be used by the system to narrow the search (food type, area and price range) and the remaining 3 are informable properties (phone number, address and zip code) that are available as soon as a required database unit is found. Common core components of the SDS common to all experiments include an HMM-based detector, a semantic input code (Henderson et al., 2012), the BUDS belief state tracker (Thomson and Young, 2010), which takes into account the state of dialogue using a dynamic Bayesian network, and a template-based natural language generator to transform semantic actions of the system into natural speech responses for the user. All guidelines were set using the GP SARM algorithm and the acumen number for the success of the RSA 1 signal, each of which is determined by the acoustic signal."}, {"heading": "4.1 Dialogue representations", "text": "The LSTM encoder decoder model in \u00a7 3.1 was used to generate an embed d for each dialog. For each dialog that contains a statement by the user and a response from the system, a feature vector f of size 74 was extracted (Vandyke et al., 2015). This vector consists of the concatenation of the most likely user intention, which is used as input and target for each concept defined in ontology, an onehot encryption of the system, which is normalized by the maximum number of turns (here 30). This feature vector was used as input and the target for the LSTM encoder decoder model was defined, whereby the encryption of the system takes place."}, {"heading": "4.2 Dialogue Policy Learning", "text": "In view of the well-established dialogues between the various countries, in which the question is whether it is a country, in which it is a country, in which it is not a country, in which it is not a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country in which it is not a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which is a country, in which is a country, in which is a country, in which is a country"}, {"heading": "4.3 Dialogue Policy Evaluation", "text": "In order to compare the performance, the first section of Table 1 shows the average results achieved between 400-500 training dialogs together with a standard error. In the 400-500 interval, the Subj, offline RNN and Online GP systems achieved comparable results without statistical differences. Also, the results of further training on the Subj and Online GP systems from 500 to 850 training dialogs are shown. As you can see, the Online GP system was probably significantly better because it is more robust against faulty user feedback than the Subj system."}, {"heading": "4.4 Reward Model Evaluation", "text": "The above results confirm the effectiveness of the proposed reward model for political learning. Here, we further examine the accuracy of the model in predicting the subjective success rate. An evaluation of the online GP reward model between 1 and 850 training dialogues is given in Table 2. Since three reward models with 850 dialogues each were learned, there were a total of 2550 training dialogues. Of these, the models asked the user for feedback 454 times, leaving 2096 dialogues in which learning was based on predicting the reward model. Thus, the results presented in the table are the average over 2096 dialogues. As can be seen, there was a significant imbalance between success and failure markers, as the policy with the training dialogues improved, reducing the memory of failed dialogues predictions, as the model was distorted to data with positive labels. Nevertheless, its precision values were good. On the other hand, the successful dialogues were accurately predicted by the model."}, {"heading": "4.5 Example Dialogues", "text": "The main advantages of the Online GP Reward Model over other models are its robustness to noise and the efficient use of user monitoring. Since the four systems mentioned above differ only in the design of the reward model (learning objective), their online behaviors are broadly similar. Two sample dialogs between users and the Online GP System are listed in Table 3 to illustrate how the system works under different noise conditions."}, {"heading": "5 Conclusion", "text": "In this paper, we have proposed an active reward learning model that uses Gaussian process classification and uncontrolled neural network-based dialogue embedding to enable real online learning in spoken dialogue systems. It enables stable policy optimization through robust modeling of inherent noise in real user feedback and uses active learning to minimize the number of feedback requests to the user. We found that the proposed model achieves efficient policy learning and better performance compared to other state-of-the-art methods in the restaurant sector of Cambridge. A key advantage of this Bayesian model is that its uncertainty estimation allows active learning and dealing with noise in a natural way. Uncontrolled dialogue embedding does not require labeled data to train while providing a compact and useful input for the reward prodictor. Overall, the techniques developed in this paper allow for a first-time manual user approach to large-scale online learning that does not require a large-scale correlation system."}, {"heading": "Acknowledgments", "text": "Pei-Hao Su is supported by the Cambridge Trust and the Taiwanese Ministry of Education. This research was partly funded by the EPSRC grant EP / M018946 / 1 Open Domain Statistical Spoken Dialogue Systems. Data used in the experiments can be found at www.repository.cam.ac.uk / handle / 1810 / 256020."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Learning the reward model of dialogue pomdps from data", "author": ["Hamid R Chinaei", "Brahim Chaib-draa"], "venue": "In NIPS Workshop on Machine Learning for Assistive Techniques", "citeRegEx": "Boularias et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boularias et al\\.", "year": 2010}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["Brochu et al.2010] Eric Brochu", "Vlad M Cora", "Nando De Freitas"], "venue": null, "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Knowledge transfer between speakers for personalised dialogue management", "author": ["Thomas Hain", "Heidi Christensen", "Ricard Marxer", "Phil Green"], "venue": "In Proc of SigDial", "citeRegEx": "Casanueva et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Casanueva et al\\.", "year": 2015}, {"title": "Hyper-parameter optimisation of gaussian process reinforcement learning for statistical dialogue management", "author": ["Chen et al.2015] Lu Chen", "Pei-Hao Su", "Milica"], "venue": "Gas\u030cic", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Preference-based policy iteration: Leveraging preference learning for reinforcement learning. In Machine learning and knowledge", "author": ["Cheng et al.2011] Weiwei Cheng", "Johannes F\u00fcrnkranz", "Eyke H\u00fcllermeier", "Sang-Hyeun Park"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2011}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine", "author": ["Cho et al.2014] Kyunghyun Cho", "Dzmitry Bahdanau", "Fethi Bougares Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Pomdp-based control of workflows for crowdsourcing", "author": ["Dai et al.2013] Peng Dai", "Christopher H Lin", "Daniel S Weld"], "venue": "Artificial Intelligence,", "citeRegEx": "Dai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2013}, {"title": "Active reward learning", "author": ["Malte Viering", "Jan Metz", "Oliver Kroemer", "Jan Peters"], "venue": "In Proc of RSS", "citeRegEx": "Daniel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2014}, {"title": "A comprehensive reinforcement learning framework for dialogue management", "author": ["Matthieu Geist", "Senthilkumar Chandramohan", "Olivier Pietquin"], "venue": null, "citeRegEx": "Daubigney et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daubigney et al\\.", "year": 2014}, {"title": "Task completion transfer learning for reward inference", "author": ["Romain Laroche", "Olivier Pietquin"], "venue": "In Proc of MLIS", "citeRegEx": "Asri et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Asri et al\\.", "year": 2014}, {"title": "Gaussian processes for pomdp-based dialogue manager optimization", "author": ["Ga\u0161i\u0107", "Young2014] Milica Ga\u0161i\u0107", "Steve Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2014}, {"title": "Online policy optimisation of spoken dialogue systems via live interaction with human subjects", "author": ["Ga\u0161i\u0107 et al.2011] Milica Ga\u0161i\u0107", "Filip Jurcicek", "Blaise. Thomson", "Kai Yu", "Steve Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2011}, {"title": "On-line policy optimisation of bayesian spoken dialogue systems via human", "author": ["Ga\u0161i\u0107 et al.2013] Milica Ga\u0161i\u0107", "Catherine Breslin", "Matthew Henderson", "Dongho Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Steve J. Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2013}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Graves et al.2013] Alax Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "In IEEE ASRU", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Discriminative spoken language understanding using word confusion networks", "author": ["Milica Ga\u0161i\u0107", "Blaise Thomson", "Pirros Tsiakoulis", "Kai Yu", "Steve Young"], "venue": null, "citeRegEx": "Henderson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2012}, {"title": "GPy: A gaussian process framework in python. http: //github.com/SheffieldML/GPy", "author": ["Nicolo Fusi", "Ricardo Andrade", "Nicolas Durrande", "Alan Saul", "Max Zwiessele", "Neil D. Lawrence"], "venue": null, "citeRegEx": "Hensman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hensman et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Active learning with gaussian processes for object categorization", "author": ["Kapoor et al.2007] Ashish Kapoor", "Kristen Grauman", "Raquel Urtasun", "Trevor Darrell"], "venue": "In Proc of ICCV", "citeRegEx": "Kapoor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kapoor et al\\.", "year": 2007}, {"title": "Inverse reinforcement learning for micro-turn management", "author": ["Kim et al.2014] Dongho Kim", "Catherine Breslin", "Pirros Tsiakoulis", "Matthew Henderson", "Steve J Young"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Issues in the evaluation of spoken dialogue systems using objective and subjective measures", "author": ["L.B. Larsen"], "venue": "In IEEE ASRU", "citeRegEx": "Larsen.,? \\Q2003\\E", "shortCiteRegEx": "Larsen.", "year": 2003}, {"title": "A stochastic model of computerhuman interaction for learning dialogue strategies. Eurospeech", "author": ["Levin", "Pieraccini1997] Esther Levin", "Roberto Pieraccini"], "venue": null, "citeRegEx": "Levin et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Levin et al\\.", "year": 1997}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "To re (label), or not to re (label)", "author": ["Daniel S Weld"], "venue": "In Second AAAI Conference on Human Computation and Crowdsourcing", "citeRegEx": "Lin and Weld,? \\Q2014\\E", "shortCiteRegEx": "Lin and Weld", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Approximations for binary gaussian process classification", "author": ["Nickisch", "Rasmussen2008] Hannes Nickisch", "Carl Edward Rasmussen"], "venue": null, "citeRegEx": "Nickisch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nickisch et al\\.", "year": 2008}, {"title": "Automating spoken dialogue management design using machine learning: An industry perspective", "author": ["Paek", "Pieraccini2008] Tim Paek", "Roberto Pieraccini"], "venue": "Speech communication,", "citeRegEx": "Paek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Paek et al\\.", "year": 2008}, {"title": "Gaussian processes for machine learning", "author": ["Rasmussen", "Chris Williams"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2006}, {"title": "Learning and evaluation of dialogue strategies for new applications: Empirical methods for optimization from small data sets", "author": ["Rieser", "Lemon2011] Verena Rieser", "Oliver Lemon"], "venue": null, "citeRegEx": "Rieser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rieser et al\\.", "year": 2011}, {"title": "Bayesian Inverse Reinforcement Learning for Modeling Conversational Agents in a Virtual Environment", "author": ["Rojas Barahona", "Christophe Cerisara"], "venue": "In Conference on Intelligent Text", "citeRegEx": "Barahona et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barahona et al\\.", "year": 2014}, {"title": "Spoken dialogue management using probabilistic reasoning", "author": ["Roy et al.2000] Nicholas Roy", "Joelle Pineau", "Sebastian Thrun"], "venue": "In Proc of SigDial", "citeRegEx": "Roy et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2000}, {"title": "Learning agents for uncertain environments", "author": ["Stuart Russell"], "venue": "In Proc of COLT", "citeRegEx": "Russell.,? \\Q1998\\E", "shortCiteRegEx": "Russell.", "year": 1998}, {"title": "A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies", "author": ["Karl Weilhammer", "Matt Stuttle", "Steve Young"], "venue": "The knowledge engineering review,", "citeRegEx": "Schatzmann et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2006}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1507.04808", "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Active learning literature survey", "author": ["Burr Settles"], "venue": "Computer Sciences Technical Report", "citeRegEx": "Settles.,? \\Q2010\\E", "shortCiteRegEx": "Settles.", "year": 2010}, {"title": "Learning from real users: Rating dialogue success with neural networks for reinforcement learning in spoken dialogue", "author": ["Su et al.2015a] Pei-Hao Su", "David Vandyke", "Milica Ga\u0161i\u0107", "Dongho Kim", "Nikola Mrk\u0161i\u0107", "Tsung-Hsien Wen", "Steve Young"], "venue": null, "citeRegEx": "Su et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Su et al\\.", "year": 2015}, {"title": "Reward shaping with recurrent neural networks for speeding up on-line policy learning in spoken dialogue systems", "author": ["Su et al.2015b] Pei-Hao Su", "David Vandyke", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Tsung-Hsien Wen", "Steve Young"], "venue": "Proc of SigDial", "citeRegEx": "Su et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Su et al\\.", "year": 2015}, {"title": "Preferencelearning based inverse reinforcement learning for dialog control", "author": ["Toyomi Meguro", "Yasuhiro Minami"], "venue": "In Proc of Interspeech", "citeRegEx": "Sugiyama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2012}, {"title": "Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems", "author": ["Thomson", "Young2010] Blaise Thomson", "Steve Young"], "venue": "Computer Speech and Language,", "citeRegEx": "Thomson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Thomson et al\\.", "year": 2010}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proc of ACL", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Quality-adaptive spoken dialogue initiative selection and implications on reward modelling", "author": ["Ultes", "Minker2015] Stefan Ultes", "Wolfgang Minker"], "venue": "In Proc of SigDial", "citeRegEx": "Ultes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ultes et al\\.", "year": 2015}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Multi-domain dialogue success classifiers for policy training", "author": ["Pei-Hao Su", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Tsung-Hsien Wen", "Steve Young"], "venue": null, "citeRegEx": "Vandyke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vandyke et al\\.", "year": 2015}, {"title": "A neural conversational model. arXiv preprint arXiv:1506.05869", "author": ["Vinyals", "Le2015] Oriol Vinyals", "Quoc Le"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "PARADISE: A framework for evaluating spoken dialogue agents", "author": ["Diane J. Litman", "Candace A. Kamm", "Alicia Abella"], "venue": "Proc of EACL", "citeRegEx": "Walker et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Walker et al\\.", "year": 1997}, {"title": "Partially observable Markov decision processes for spoken dialog systems", "author": ["Williams", "Young2007] Jason D. Williams", "Steve Young"], "venue": "Computer Speech and Language,", "citeRegEx": "Williams et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2007}, {"title": "Predicting user satisfaction in spoken dialog system evaluation with collaborative filtering", "author": ["Yang et al.2012] Zhaojun Yang", "G Levow", "Helen Meng"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "Pomdp-based statistical spoken dialogue systems: a review", "author": ["Young et al.2013] Steve Young", "Milica Ga\u0161ic", "Blaise Thomson", "Jason Williams"], "venue": "In Proc of IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Active learning from weak and strong labelers", "author": ["Zhang", "Chaudhuri2015] Chicheng Zhang", "Kamalika Chaudhuri"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Incremental relabeling for active learning with noisy crowdsourced annotations", "author": ["Zhao et al.2011] Liyue Zhao", "Gita Sukthankar", "Rahul Sukthankar"], "venue": "In Proc of PASSAT and Proc of SocialCom", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 34, "context": "They can be broadly divided into two categories: chatoriented systems which aim to converse with users and provide reasonable contextually relevant responses (Vinyals and Le, 2015; Serban et al., 2015), and task-oriented systems designed to assist users to achieve specific goals (e.", "startOffset": 158, "endOffset": 201}, {"referenceID": 10, "context": "find hotels, movies or bus schedules) (Daubigney et al., 2014; Young et al., 2013).", "startOffset": 38, "endOffset": 82}, {"referenceID": 48, "context": "find hotels, movies or bus schedules) (Daubigney et al., 2014; Young et al., 2013).", "startOffset": 38, "endOffset": 82}, {"referenceID": 31, "context": "More recently, dialogue management has been formulated as a reinforcement learning (RL) problem which can be automatically optimised (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Young et al., 2013).", "startOffset": 133, "endOffset": 225}, {"referenceID": 48, "context": "More recently, dialogue management has been formulated as a reinforcement learning (RL) problem which can be automatically optimised (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Young et al., 2013).", "startOffset": 133, "endOffset": 225}, {"referenceID": 45, "context": "not the pre-specified task was completed (Walker et al., 1997; Ga\u0161i\u0107 et al., 2013).", "startOffset": 41, "endOffset": 82}, {"referenceID": 14, "context": "not the pre-specified task was completed (Walker et al., 1997; Ga\u0161i\u0107 et al., 2013).", "startOffset": 41, "endOffset": 82}, {"referenceID": 50, "context": "der to give feedback, resulting in unstable learning (Zhao et al., 2011; Ga\u0161i\u0107 et al., 2011).", "startOffset": 53, "endOffset": 92}, {"referenceID": 13, "context": "der to give feedback, resulting in unstable learning (Zhao et al., 2011; Ga\u0161i\u0107 et al., 2011).", "startOffset": 53, "endOffset": 92}, {"referenceID": 12, "context": ", 2011; Ga\u0161i\u0107 et al., 2011). In order to filter out incorrect user feedback, Ga\u0161i\u0107 et al. (2013) used only dialogues for which Obj = Subj.", "startOffset": 8, "endOffset": 97}, {"referenceID": 36, "context": "In light of the above, Su et al. (2015a) proposed learning a neural network-based Obj estimator from off-line simulated dialogue data.", "startOffset": 23, "endOffset": 41}, {"referenceID": 33, "context": "However, a user simulator will only provide a rough approximation of real user statistics and developing a user simulator is a costly process (Schatzmann et al., 2006).", "startOffset": 142, "endOffset": 167}, {"referenceID": 45, "context": "Walker et al. (1997) proposed the PARADISE framework, where a linear function of task completion and various dialogue features such as dialogue duration were used to in-", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "have been raised regarding the theoretical validity of the model (Larsen, 2003).", "startOffset": 65, "endOffset": 79}, {"referenceID": 44, "context": "Yang et al. (2012) used collaborative filtering to infer user preferences.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "The use of reward shaping has also been investigated in (El Asri et al., 2014; Su et al., 2015b) to enrich the reward function in order to speed up dialogue policy learning. Also, Ultes and Minker (2015) demonstrated that there is a strong correlation between expert\u2019s user satisfaction ratings and dialogue success.", "startOffset": 60, "endOffset": 204}, {"referenceID": 8, "context": "One effective way to mitigate the effects of annotator error is to obtain multiple ratings for the same data and several methods have been developed to guide the annotation process with uncertainty models (Dai et al., 2013; Lin et al., 2014).", "startOffset": 205, "endOffset": 241}, {"referenceID": 35, "context": "Active learning is particularly useful for determining when an annotation is needed (Settles, 2010; Zhang and Chaudhuri, 2015).", "startOffset": 84, "endOffset": 126}, {"referenceID": 3, "context": "It is often utilised using Bayesian optimisation approaches (Brochu et al., 2010).", "startOffset": 60, "endOffset": 81}, {"referenceID": 3, "context": "It is often utilised using Bayesian optimisation approaches (Brochu et al., 2010). Based on this, Daniel et al. (2014)", "startOffset": 61, "endOffset": 119}, {"referenceID": 32, "context": "Rather than explicitly defining a reward function, inverse RL (IRL) aims to recover the underlying reward from demonstrations of good behaviour and then learn a policy which maximises the recovered reward (Russell, 1998).", "startOffset": 205, "endOffset": 220}, {"referenceID": 6, "context": "ments than absolute scores, another related line of research has focused on preference-based approaches to RL (Cheng et al., 2011).", "startOffset": 110, "endOffset": 130}, {"referenceID": 38, "context": "In (Sugiyama et al., 2012), users were asked to provide rankings between pairs of dialogues.", "startOffset": 3, "endOffset": 26}, {"referenceID": 25, "context": "tions has recently gained attention especially for word representations, and has boosted performance on several natural language processing tasks (Mikolov et al., 2013; Turian et al., 2010; Levy and Goldberg, 2014).", "startOffset": 146, "endOffset": 214}, {"referenceID": 40, "context": "tions has recently gained attention especially for word representations, and has boosted performance on several natural language processing tasks (Mikolov et al., 2013; Turian et al., 2010; Levy and Goldberg, 2014).", "startOffset": 146, "endOffset": 214}, {"referenceID": 7, "context": "been successfully applied to machine translation (MT) where it enables varying-length phrases to be mapped to fixed-length vectors using an RNN Encoder-Decoder (Cho et al., 2014).", "startOffset": 160, "endOffset": 178}, {"referenceID": 15, "context": "In our proposed model, the encoder is a Bi-directional Long Short-Term Memory network (BLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013).", "startOffset": 94, "endOffset": 149}, {"referenceID": 4, "context": "In the context of spoken dialogue systems it has been successfully used for RL policy optimisation (Ga\u0161i\u0107 and Young, 2014; Casanueva et al., 2015) and IRL reward function regression (Kim et al.", "startOffset": 99, "endOffset": 146}, {"referenceID": 20, "context": ", 2015) and IRL reward function regression (Kim et al., 2014).", "startOffset": 43, "endOffset": 61}, {"referenceID": 5, "context": "The hyper-parameters p, l, \u03c3n can be adequately optimised by maximising the marginal likelihood using a gradient-based method (Chen et al., 2015).", "startOffset": 126, "endOffset": 145}, {"referenceID": 19, "context": "model (Kapoor et al., 2007).", "startOffset": 6, "endOffset": 27}, {"referenceID": 16, "context": "The shared core components of the SDS common to all experiments comprise a HMM-based recogniser, a confusion network (CNet) semantic input decoder (Henderson et al., 2012), the BUDS belief state tracker (Thomson and Young, 2010) that factorises the dialogue state using a dynamic Bayesian network, and a template based natural language generator to map system semantic actions into natural language responses to the user.", "startOffset": 147, "endOffset": 171}, {"referenceID": 43, "context": "tains a user\u2019s utterance and a system\u2019s response, a feature vector f of size 74 was extracted (Vandyke et al., 2015).", "startOffset": 94, "endOffset": 116}, {"referenceID": 1, "context": "The model was implemented using the Theano library (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 51, "endOffset": 96}, {"referenceID": 0, "context": "The model was implemented using the Theano library (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 51, "endOffset": 96}, {"referenceID": 17, "context": "The system was implemented using the GPy library (Hensman et al., 2012).", "startOffset": 49, "endOffset": 71}, {"referenceID": 14, "context": "\u2022 the Obj=Subj system which uses prior knowledge of the task to only use training dialogues for which the user\u2019s subjective assessment of success is consistent with the objective assessment of success as in (Ga\u0161i\u0107 et al., 2013).", "startOffset": 207, "endOffset": 227}, {"referenceID": 13, "context": "Similar to the conclusions drawn in (Ga\u0161i\u0107 et al., 2011), the Subj system suffers from unreliable user feedback.", "startOffset": 36, "endOffset": 56}], "year": 2016, "abstractText": "The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user\u2019s intent is known in advance or data is available to pre-train a task success predictor off-line. In practice neither of these apply for most real world applications. Here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning.", "creator": "LaTeX with hyperref package"}}}