{"id": "1605.07717", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Deep Structured Energy Based Models for Anomaly Detection", "abstract": "In this paper, we attack the anomaly detection problem by directly modeling the data distribution with deep architectures. We propose deep structured energy based models (DSEBMs), where the energy function is the output of a deterministic deep neural network with structure. We develop novel model architectures to integrate EBMs with different types of data such as static data, sequential data, and spatial data, and apply appropriate model architectures to adapt to the data structure. Our training algorithm is built upon the recent development of score matching \\cite{sm}, which connects an EBM with a regularized autoencoder, eliminating the need for complicated sampling method. Statistically sound decision criterion can be derived for anomaly detection purpose from the perspective of the energy landscape of the data distribution. We investigate two decision criteria for performing anomaly detection: the energy score and the reconstruction error. Extensive empirical studies on benchmark tasks demonstrate that our proposed model consistently matches or outperforms all the competing methods.", "histories": [["v1", "Wed, 25 May 2016 03:40:18 GMT  (8782kb,D)", "http://arxiv.org/abs/1605.07717v1", "To appear in ICML 2016"], ["v2", "Thu, 16 Jun 2016 02:36:10 GMT  (8782kb,D)", "http://arxiv.org/abs/1605.07717v2", "To appear in ICML 2016"]], "COMMENTS": "To appear in ICML 2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["shuangfei zhai", "yu cheng", "weining lu", "zhongfei zhang"], "accepted": true, "id": "1605.07717"}, "pdf": {"name": "1605.07717.pdf", "metadata": {"source": "META", "title": "Deep Structured Energy Based Models for Anomaly Detection", "authors": ["Shuangfei Zhai", "Yu Cheng", "Weining Lu", "Zhongfei (Mark) Zhang"], "emails": ["SZHAI2@BINGHAMTON.EDU", "CHENGYU@US.IBM.COM", "LUWN14@MAILS.TSINGHUA.EDU.CN", "ZHONGFEI@CS.BINGHAMTON.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Energy Based Models (EBMs)", "text": "EBMs are a family of probability models that can be used to build probability density functions. An EBM parameterizes a density function for input x; Rd in the form: p (x; \u03b8) = e \u2212 E (x; \u03b8) Z (\u03b8), (1) where E (x; \u03b8) is the energy (negative log probability) associated with instance x; Z (\u03b8) = x e \u2212 E (x; \u03b8) dx, the partition function to ensure that the density function matches probability 1; \u03b8 are the model parameters to be learned. The nice property of EBM is that you can parameterize the energy in a meaningful way, which gives it a lot of flexibility and expressiveness. Learning is done by assigning less energy (hence a higher probability) to observed instances and vice versa. However, the direct application of MLE is impossible due to the intractability of partition fusion Z (striker), and therefore one usually has to resort to summarize MCMC samples and the MCMC methods."}, {"heading": "2.2. Restricted Boltzmann Machine (RBM)", "text": "RBM (Hinton, 2010) is one of the best known examples of EBM. For continuous input data, the energy function of an RBM takes the form of: E (x; \u03b8) = 12 x-b \u00b2 22 \u2212 K = 1 g (WTj x + bj), (2) where W-Rd \u00b7 K (where Wj is the jth column), b-RK (with bj as the jth element) and b-Rd are the parameters to be learned; g (x) is the soft plus function protocol (1 + ex). Multiple RBMs can be trained and stacked on top of each other to formulate a deep RBM, which makes it useful to initiate multi-layered neural networks. Although efficient training algorithms, such as contrasting divergences, are proposed to make RBM scalable, it is still considerably more difficult than a deterministic neural network (Hinton, 2010)."}, {"heading": "2.3. Denoising Autoencoders and Score Matching", "text": "A typical form of autoencoder minimizes the following objective function: N \u2211 i = 1 \u0445 xi \u2212 f (xi; \u03b8) \u0442 22 (3), where f (\u00b7; \u03b8) is the reconstruction function that maps Rd \u2192 Rd, which normally consists of an encoder, followed by a decoder with symmetrical architecture and common parameters. A particularly interesting variant of autoencoders is DAEs (Vincent et al., 2010), which learn to construct the inputs based on their randomly corrupted versions: N \u2211 i = 1 E \u2212 xi \u2212 f (xi +;), where a particularly interesting variant of autoencoders is an isotropic Gaussian structure. DAEs are easy to train with standard stochastic gradients x x."}, {"heading": "3. Deep Structured EBMs", "text": "Deep architectures allow complex patterns to be efficiently modeled, making them particularly suitable for high-dimensional data. On the other hand, it is often necessary to adapt the architecture of a deep model to the structure of data. For example, recursive neural networks (RNNs) have been shown to be very good at modeling sequential data; Convolutionary neural networks (CNNs) are very effective at modeling data with spatial structure. In this work, we extend EBMs further to deeply structured EBMs (DSEBMs), where we allow the underlying deep neural network that encodes energy function to take into account architectures that vary from fully networked, recursive, and convolutionary structures. This generalizes EBMs (LeCun et al., 2006) as well as deep EBMs (Ngiam et al., 2011), because it makes our model applicable to a much broader range of applications, including static data, sequential data, and spatial data, and we can easily train the following DSED with the following spatial data, in addition to show that we can easily correct the following DSED for the following cases."}, {"heading": "3.1. Fully Connected EBM", "text": "This case is conceptually the same as the deep EBMs proposed in (Ngiam et al., 2011). Without loss of universality, we express the energy function of a fully connected L-layer EBM as follows: E (x; \u03b8) = 12 x -b \u00b2 22 \u2212 KL \u00b2 j = 1 hL, js.t. hl = g (W T l hl \u2212 1 + b1), l \u00b2 [1, L] (8), where Wl \u00b2 RKl \u2212 1 \u00b7 Kl, bl \u00b2 RKl are the parameters for the Lth layer; Kl is the dimensionality of the Lth layer. The 0th layer is defined as the input itself; and so we have K0 = d, h0 = x. We have explicitly included the term \"x \u2212 b \u00b2 22,\" which functions as a precursor and penalizes the probability of the inputs that are far away from b \u00b2 l. If we follow the chain rule of the grave discharge computation, the probability can be derived from the reconstruction function as 22 \u00b7 b \u00b2, where f \u00b2 is punished."}, {"heading": "3.2. Recurrent EBMs", "text": "Our formulation of the recurring EBM is comparable to (Boulanger-Lewandowski et al., 2012), where an EBM is built up in each time step, with parameters determined by an underlying RNN. Formally, a sequence of the length T x = [x1,..., xT], xt [x1,..., t \u2212 1) is modelled as EBM with energy E (xt | T = 1 p,..., t \u2212 1) with the chain rule of probability. For each time step t, p (xt | x1,..., t \u2212 1) is modelled as EBM with energy E. In contrast to the conventional formulation of the EBM, the parameters of the EMB rule of probability are a function of the inputs from all previous time steps x1,..., t \u2212 1. A natural choice of EquT is to be the output of an RNN."}, {"heading": "3.3. Convolutional EBMs", "text": "Previously, the combination of CNN and RBM is in (Lee et al., 2009), where several layers of RBMs are alternately connected to an image, which is then stacked on top of each other. In this paper, we take a significantly different approach by directly building deep EBMs with convolution operators (with optional pooling layers or fully connected layers) by simply replacing hL in Equation 8 with the output of a CNN. Using a deterministic deep convolutional EBM allows to train the model directly end-to-end with score matching, which greatly simplifies the training procedure compared to (Lee et al., 2009). Formally, input of the (l \u2212 1) th Layer1While one can also choose to use the full gradient, we find this simplification good in practice, resulting in a goal in a much more successful form."}, {"heading": "4. Deep Structured EBMs for Anomaly Detection", "text": "With a trained DSEBM, we can then select samples whose probability is lower than some pre-selected threshold pth as an outlier. Although the calculation of the exact probability according to Eq.1 is intractable, the following logic can be immediately recognized: p (x; \u03b8) < pth \u21d2 E (x; log pth \u21d2 E (x; log pth + logZ (\u03b8) > Eth. (15) Here we have used the fact that Z (\u03b8) is a constant that does not depend on x. < log pth \u21d2 E (x; log pth) > log pth + logZ (\u03b8) \u21d2 E (x; \u03b8) > reos the ability to use Z (\u03b8) as a constant that does not depend on probability. Therefore, the selection of samples with probability lower than pth is equivalent to the selection of those that are higher than a corresponding energy threshold Eth. (2Furthermore, motivated by the combination of EBMs and DAEs, we examine another decision criteria based on reconstruction."}, {"heading": "5. Experimental Evaluation", "text": "In this section, we evaluate the proposed framework for detecting anomalies, in which our two proposed anomaly detection criteriesFor a relapsing DSEBM, this decision rule is abbreviated as \u2211 T t = 1E (xt; \u03b8t) > Eth 3For a relapsing DSEBM, this decision rule is abbreviated as \u2211 T \u2212 1 \u0442 xtE (x t; \u03b8t) \u0445 22 > Errorthterien using energy and reconstruction errors as DSEBM-e or DSEBM-r. Our experiments consist of three data types: static data, sequential data (e.g. audio) and spatial data (e.g. image), in which we apply fully connected EBM, relapsed EBM or Convolutionary EBM. The specifications of the benchmark data sets used are summarized in Table 1. To demonstrate the effectiveness of DSEBM, we compare our approach with several well-established EBM results, which we refer to publicly available EBM results (the EBM)."}, {"heading": "5.1. Static Data", "text": "Benchmark data sets are used in this study: KDD99 10 percent, Thyroid and Usenet from the UCI repository (Lichman, 2013), training and test sets are split 1: 1 and only normal samples are used for training the model. We compare DSEBMs (with 2-layer, fully connected energy function) with a variety of competing methods, including two reconstruction-based outlier detection methods, PCA and Kernel PCA, two density-based methods Kernel Density Estimator (KDE) (Parzen, 1962) and Robust Kernel Density Estimator (RKDE) (Kim & Scott, 2012), along with the traditional one-class learning method, One-Class SVM (OC-SVM) (Schoolkopf et al., 2001). We also incorporate the method proposed in (Williams et al., 2002), which is the best data dimension of DF1 and the best data dimension of DM."}, {"heading": "5.2. Sequential Data", "text": "For this task, we use three sequential datasets: (1) CUAVE, which contains audiovisual data of ten spoken digits (zero to nine); (2) NATOPS, which contains 24 classes of body and hand gestures used by the US Navy in handling aircraft on board aircraft carriers; (3) FITNESS, which contains the daily fitness behavior of users collected from health devices, including nutritional, sleep and training information. According to the BMI change, users are divided into two groups, \"losing weight\" and \"gaining weight.\" For a single category, the outlier samples, containing 0.1 \u2264 0.4 from other categories, are simulated; the datasets are divided into training and tests according to 2: 1, using 2 / 3 of the normal samples for training distribution. We compare DSEBM-r and DSEBM-e with three static baselines, the SEF1 indicators, SVRC-KOC and SVRM-e."}, {"heading": "5.3. Spatial Data", "text": "We use three public image sets: Caltech-101, MNIST, and CIFAR-10 for this subset. On Caltech-101, we select 11 object categories as outliers, each containing at least 100 images, and sample outlier images with a ratio of 0.1 \u2264 \u03c1 \u2264 0.4 from the other categories. On MNIST and CIFAR-10, we use images from a single category as outliers and sample images from the other categories with a ratio of 0.1 \u2264 \u2264 0.4. Each data set is in a training and test set with a ratio of 2: 1. We compare DSEBMs (with a revolutionary layer + a pooling layer + a fully bonded layer) with several basic methods, including: High-dimensional Robust PCA (HR-PCA), Kernel PCA (KPCA), Robust Kernel Density Estimator (RKDE), One-Class SVM (OSVM), and Unvised Class A (PCA)."}, {"heading": "5.4. Energy VS. Reconstruction Error", "text": "With regard to the two DSEBM decision criteria, we note that DSEBM-e consistently performs better than DSEBM-r on all benchmarks except for the thyroid data set. This confirms our assumption that the energy value is a more accurate decision criterion than a reconstruction error. To gain further insight into the behavior of the two criteria, we show seven outliers selected from the Caltech 101 benchmark in Figure 4. For each image, the energy values in the second row are shown in red, followed by the reconstruction error in green and the correct Inlier class. Interestingly, all seven outliers in the Inlier class are visually similar and exhibit small reconstruction errors (compared to the threshold). Nevertheless, we are able to successfully identify all of them with energy (which is above the energy threshold)."}, {"heading": "6. Related Work", "text": "There has been a large number of papers focusing on the detection of anomalies (Chandola et al., 2009), noticeable: (1) the methods based on reconstruction such as PCA and Kernel PCA, Robust PCA and Robust Kernel PCA; (2) the methods based on probability density, including parametric estimators (Zimek et al., 2012) and non-parametric estimators such as the Kernel Density Estimator (KDE) and the newer robust Kernel Density Estimator (RKDE); (3) methods for learning a compact data model such as that as many as possible normal samples are included inside, such as an SVM and SVDD sequencer. Graham et al. proposed a method based on the autoencode (Williams et al., 2002). However, all of the above methods are static in nature, which do not originate from the structure of the data."}, {"heading": "7. Conclusion", "text": "We proposed to train deeply structured energy-based models for the problem of anomaly detection and extended EBMs to deep architectures with three types of structures: fully networked, recurrent and convolutional. To significantly simplify the training process, a score matching is proposed instead of MLE as the training algorithm. Furthermore, we have investigated the proper use of DSEBMs for the purpose of detecting anomalies, focusing in particular on two decision criteria: energy core and reconstruction errors. Systematic experiments are conducted on three types of data sets: static, sequential and spatial, showing that DSEBMs consistently match or exceed the most advanced anomaly detection algorithms. To maximize our knowledge, this is the first work to comprehensively evaluate deeply structured models with the problem of detecting anomalies."}], "references": [{"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["Boulanger-Lewandowski", "Nicolas", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": "arXiv preprint arXiv:1206.6392,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Anomaly detection: A survey", "author": ["Chandola", "Varun", "Banerjee", "Arindam", "Kumar", "Vipin"], "venue": "ACM computing surveys (CSUR),", "citeRegEx": "Chandola et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chandola et al\\.", "year": 2009}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["Fei-Fei", "Li", "Fergus", "Rob", "Perona", "Pietro"], "venue": "Comput. Vis. Image Underst.,", "citeRegEx": "Fei.Fei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2007}, {"title": "A practical guide to training restricted boltzmann machines. Momentum", "author": ["Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Hinton and Geoffrey.,? \\Q2010\\E", "shortCiteRegEx": "Hinton and Geoffrey.", "year": 2010}, {"title": "Estimation of non-normalized statistical models by score matching", "author": ["Hyv\u00e4rinen", "Aapo"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hyv\u00e4rinen and Aapo.,? \\Q2005\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Aapo.", "year": 2005}, {"title": "Robust kernel density estimation", "author": ["Kim", "JooSeuk", "Scott", "Clayton D"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kim et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2012}, {"title": "Regularized estimation of image statistics by score matching", "author": ["Kingma", "Diederik P", "Cun", "Yann L"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Kingma et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2010}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Master\u2019s thesis,", "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Lecun", "Yann", "Bottou", "Lon", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Unsupervised one-class learning for automatic outlier removal", "author": ["Liu", "Wei", "Hua", "Gang", "Smith", "John R"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Direct density ratio estimation with convolutional neural networks with application in outlier detection", "author": ["Nam", "Hyun Ha", "Sugiyama", "Masashi"], "venue": "IEICE Transactions,", "citeRegEx": "Nam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nam et al\\.", "year": 2015}, {"title": "Learning deep energy models", "author": ["Ngiam", "Jiquan", "Chen", "Zhenghao", "Koh", "Pang W", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "On estimation of a probability density function and mode", "author": ["Parzen", "Emanuel"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Parzen and Emanuel.,? \\Q1962\\E", "shortCiteRegEx": "Parzen and Emanuel.", "year": 1962}, {"title": "Cuave: A new audio-visual database for multimodal human-computer interface research", "author": ["E.K. Patterson", "S. Gurbuz", "Z. Tufekci", "J.N. Gowdy"], "venue": null, "citeRegEx": "Patterson et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Patterson et al\\.", "year": 2017}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Glorot", "Bengio", "Yoshua"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["Sch\u00f6lkopf", "Bernhard", "Platt", "John C", "Shawe-Taylor", "Smola", "Alex J", "Williamson", "Robert C"], "venue": "Neural Comput.,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "One-class conditional random fields for sequential anomaly detection", "author": ["Song", "Yale", "Wen", "Zhen", "Lin", "Ching-Yung", "Davis", "Randall"], "venue": "In Proceedings of the TwentyThird International Joint Conference on Artificial Intelligence,", "citeRegEx": "Song et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Song et al\\.", "year": 2013}, {"title": "Mining for Outliers in Sequential Databases", "author": ["Sun", "Pei", "Chawla", "Sanjay", "Arunasalam", "Bavani"], "venue": "SDM. SIAM,", "citeRegEx": "Sun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2006}, {"title": "On autoencoders and score matching for energy based models", "author": ["Swersky", "Kevin", "Buchman", "David", "Freitas", "Nando D", "Marlin", "Benjamin M"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Swersky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2011}, {"title": "A connection between score matching and denoising autoencoders", "author": ["Vincent", "Pascal"], "venue": "Neural computation,", "citeRegEx": "Vincent and Pascal.,? \\Q2011\\E", "shortCiteRegEx": "Vincent and Pascal.", "year": 2011}, {"title": "A comparative study of rnn for outlier detection in data mining", "author": ["Williams", "Graham J", "Baxter", "Rohan A", "He", "Hongxing", "Hawkins", "Simon", "Gu", "Lifang"], "venue": "In ICDM,", "citeRegEx": "Williams et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2002}, {"title": "A survey on unsupervised outlier detection in highdimensional numerical data", "author": ["Zimek", "Arthur", "Schubert", "Erich", "Kriegel", "Hans-Peter"], "venue": "Statistical Analysis and Data Mining,", "citeRegEx": "Zimek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zimek et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "Anomaly detection (also called novelty or outlier detection) is to identify patterns that do not conform to the expected normal patterns (Chandola et al., 2009).", "startOffset": 137, "endOffset": 160}, {"referenceID": 22, "context": "A variety of methods can be found in the survey (Zimek et al., 2012).", "startOffset": 48, "endOffset": 68}, {"referenceID": 15, "context": "For example, it is shown that properly regularized autoencoders (Vincent et al., 2010; Rifai et al., 2011) are able to effectively characterize the data distribution and learn useful representations, which are not achieved by shallow methods such as PCA or K-Means.", "startOffset": 64, "endOffset": 106}, {"referenceID": 12, "context": "In this work, we focus on deep energy based models (Ngiam et al., 2011), where the energy function is composed of a deep neural network.", "startOffset": 51, "endOffset": 71}, {"referenceID": 8, "context": "Likewise, convolutional neural networks (CNNs) are significantly more efficient at modeling spatial structures (Krizhevsky et al., 2012), such as on images.", "startOffset": 111, "endOffset": 136}, {"referenceID": 12, "context": "As will be demonstrated below, this makes training a deep EBM with various underlying structures in an end-to-end fashion, without the need of resorting to sophisticated sampling procedures (Ngiam et al., 2011) or layer wise pretraining (Hinton, 2010).", "startOffset": 190, "endOffset": 210}, {"referenceID": 12, "context": ", 2006) as well as deep EBMs (Ngiam et al., 2011) as it makes our model applicable to a much wider spectrum of applications, including static data, sequential data and spatial data.", "startOffset": 29, "endOffset": 49}, {"referenceID": 12, "context": "This case is conceptually the same as the deep EBMs proposed in (Ngiam et al., 2011).", "startOffset": 64, "endOffset": 84}, {"referenceID": 0, "context": "Our formulation of recurrent EBMs is similar to (Boulanger-Lewandowski et al., 2012), where an EBM is built at each time step, with parameters determined by an underlying RNN.", "startOffset": 48, "endOffset": 84}, {"referenceID": 21, "context": "In the early work of (Williams et al., 2002), autoencoders (which they call replicator neural networks) have been applied to anomaly detection.", "startOffset": 21, "endOffset": 44}, {"referenceID": 16, "context": "We compare DSEBMs (with 2-layer fully connected energy function) with a variety of competing methods, including two reconstruction-based outlier detection methods, PCA and Kernel PCA, two density-based methods Kernel Density Estimator (KDE) (Parzen, 1962) and Robust Kernel Density Estimator (RKDE) (Kim & Scott, 2012), along with the traditional one-class learning method One-Class SVM (OC-SVM) (Sch\u00f6lkopf et al., 2001).", "startOffset": 396, "endOffset": 420}, {"referenceID": 21, "context": "We also include the method proposed in (Williams et al., 2002), named AutoEncoder Outlier Detection (AEOD) as one baseline.", "startOffset": 39, "endOffset": 62}, {"referenceID": 2, "context": "4 Caltech-101 (Fei-Fei et al., 2007) 101 300 \u00d7 200 9,146 NA 0.", "startOffset": 14, "endOffset": 36}, {"referenceID": 9, "context": "4 MNIST (Lecun et al., 1998) 10 28 \u00d7 28 70,000 NA 0.", "startOffset": 8, "endOffset": 28}, {"referenceID": 17, "context": "Also, we include two sequential methods: 1) HMMs, where the model is trained with the normal training sequences, and the posterior probability p(y|x) of each test sequence is computed as the normalized negative log-likelihood; 2) OCCRF (Song et al., 2013), where the model learns from a one-class dataset and captures the temporal dependence structure using conditional random fields (CRFs).", "startOffset": 236, "endOffset": 255}, {"referenceID": 10, "context": "We compare DSEBMs (with one convolutional layer + one pooling layer + one fully connected layer) with several baseline methods including: High-dimensional Robust PCA (HR-PCA), Kernel PCA (KPCA), Robust Kernel Density Estimator (RKDE), One-Class SVM (OC-SVM) and Unsupervised One-Class Learning (UOCL) (Liu et al., 2014).", "startOffset": 301, "endOffset": 319}, {"referenceID": 1, "context": "There has been a large body of work concentrating on anomaly detection (Chandola et al., 2009), noticeably: (1) the reconstruction based methods such as PCA and Kernel PCA, Robust PCA and Robust Kernel PCA; (2) the probability density based methods, including parametric estimators (Zimek et al.", "startOffset": 71, "endOffset": 94}, {"referenceID": 22, "context": ", 2009), noticeably: (1) the reconstruction based methods such as PCA and Kernel PCA, Robust PCA and Robust Kernel PCA; (2) the probability density based methods, including parametric estimators (Zimek et al., 2012) and nonparametric estimators such as the kernel density estimator (KDE) and the more recent robust kernel density estimator (RKDE); (3) methods of learning a compact data model such that as many as possible normal samples are enclosed inside, for example, oneclass SVM and SVDD.", "startOffset": 195, "endOffset": 215}, {"referenceID": 21, "context": "proposed a method based on autoencoder (Williams et al., 2002).", "startOffset": 39, "endOffset": 62}, {"referenceID": 18, "context": "proposes a technique that uses Probabilistic Suffix Trees (PST) to find the nearest neighbors for a given sequence to detect sequential anomalies in protein sequences (Sun et al., 2006).", "startOffset": 167, "endOffset": 185}, {"referenceID": 17, "context": "presents a one class conditional random fields method for general sequential anomaly detection tasks (Song et al., 2013).", "startOffset": 101, "endOffset": 120}, {"referenceID": 19, "context": "Methodology-wise, there is also a recent surge of training EBMs with score matching (Vincent, 2011; Swersky et al., 2011; Kingma & Cun, 2010).", "startOffset": 84, "endOffset": 141}], "year": 2017, "abstractText": "In this paper, we attack the anomaly detection problem by directly modeling the data distribution with deep architectures. We propose deep structured energy based models (DSEBMs), where the energy function is the output of a deterministic deep neural network with structure. We develop novel model architectures to integrate EBMs with different types of data such as static data, sequential data, and spatial data, and apply appropriate model architectures to adapt to the data structure. Our training algorithm is built upon the recent development of score matching (Hyv\u00e4rinen, 2005), which connects an EBM with a regularized autoencoder, eliminating the need for complicated sampling method. Statistically sound decision criterion can be derived for anomaly detection purpose from the perspective of the energy landscape of the data distribution. We investigate two decision criteria for performing anomaly detection: the energy score and the reconstruction error. Extensive empirical studies on benchmark tasks demonstrate that our proposed model consistently matches or outperforms all the competing methods.", "creator": "LaTeX with hyperref package"}}}