{"id": "1605.07723", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Data Programming: Creating Large Training Sets, Quickly", "abstract": "Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users provide a set of labeling functions, which are programs that heuristically label large subsets of data points, albeit noisily. By viewing these labeling functions as implicitly describing a generative model for this noise, we show that we can recover the parameters of this model to \"denoise\" the training set. Then, we show how to modify a discriminative loss function to make it noise-aware. We demonstrate our method over a range of discriminative models including logistic regression and LSTMs. We establish theoretically that we can recover the parameters of these generative models in a handful of settings. Experimentally, on the 2014 TAC-KBP relation extraction challenge, we show that data programming would have obtained a winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a supervised LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way to create machine learning models for non-experts.", "histories": [["v1", "Wed, 25 May 2016 04:14:59 GMT  (69kb,D)", "http://arxiv.org/abs/1605.07723v1", null], ["v2", "Sat, 3 Dec 2016 20:03:26 GMT  (72kb,D)", "http://arxiv.org/abs/1605.07723v2", null], ["v3", "Sun, 8 Jan 2017 19:48:53 GMT  (72kb,D)", "http://arxiv.org/abs/1605.07723v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["alexander j ratner", "christopher de sa", "sen wu 0002", "daniel selsam", "christopher r\u00e9"], "accepted": true, "id": "1605.07723"}, "pdf": {"name": "1605.07723.pdf", "metadata": {"source": "CRF", "title": "Data Programming: Creating Large Training Sets, Quickly", "authors": ["Alexander Ratner", "Christopher De Sa", "Sen Wu", "Daniel Selsam", "Christopher R\u00e9"], "emails": ["ajratner@stanford.edu", "cdesa@stanford.edu", "senwu@stanford.edu", "dselsam@stanford.edu", "chrismre@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "2 Related Work", "text": "In fact, the fact is that most of them will be able to assert themselves, that they will be able to assert themselves, that they will be able to assert themselves."}, {"heading": "3 The Data Programming Paradigm", "text": "In many cases, we want to use machine learning, but we face the following challenges: (i) hand-labeled training data is unavailable and prohibitively expensive to obtain in sufficient quantity because it requires expensive domain experts; (ii) related external knowledge bases are either unavailable or insufficiently specific, precluding a traditional remote supervision or co-training model; (iii) application specifications are in flux, changing the model we ultimately want to learn. In such an environment, we would like to have a simple, scalable and adaptable approach to monitoring a model that is applicable to our problem. More specifically, we would ideally like our approach to achieve an expected loss with high probability, as O (1) inputs of some type of domain expert user instead of the traditional O-class (\u2212 2) hand-labeled training examples required by most monitored methods (where O-logarization factors)."}, {"heading": "4 Handling Dependencies", "text": "In our experience with data programming, we have found that users often write labeling functions that have clear dependencies with each other. As more labeling functions are added as the system develops, an implicit dependency structure naturally emerges among the labeling functions: modeling these dependencies can improve accuracy in some cases. We describe a method by which the user can specify this dependency knowledge as a dependency diagram and show how the system can use it to generate better parameter estimates. To support the injection of dependency information into the model, we extend the data programming specification with a dependency diagram labeling function, G \u00b2 D \u00b7 D \u00b7 {1, m}."}, {"heading": "5 Experiments", "text": "This year it has come to the point that it will be able to use the aforementioned lcihsrcsrteeSi rf\u00fc eid eerwtlrsrteeeaeKi rf\u00fc eid eerwtlrteeoiKn rf\u00fc eid eerwtlrsrteeeoiKn rf\u00fc eid eerwtlrteeeeeeoiKn rf\u00fc eid eerwlrsrteeoiueaeGtn zu.nlrf\u00fc"}, {"heading": "6 Conclusion and Future Work", "text": "We have introduced data programming, a new approach to generating large, labeled training sets. We demonstrate that our approach can be used with automatic methods of feature generation to achieve high-quality results, and for some relationship extraction tasks we have provided anecdotal evidence on which our methods can build for domain experts. We hope to test the limits of our approach to more demanding machine learning tasks, especially imaging techniques and structured predictions. Thank you to Theodoros Rekatsinas, Manas Joglekar, Henry Ehrenberg, Jason Fries, Percy Liang, the many helpful and intrepid users of DeepDive and DLite, and many others for their helpful conversations and feedback. We are grateful to the Defense Advanced Research Projects Agency (DARPA) SIMPLEX program, N66001-15-C-4043, the National Science Foundation (NSF) SIAREER Award, N66001MPLEX, N661MPLEX, N661MPLEX, N661MPLE1MPLEX, N661MPLEX, N661MPLEX, N661MPLE1MPLEX, and N661MPLE1MPLEX, N661MPLE1MPLEX, N661MPLE1MPLEX, N661MPLE1MPLE1MPLEX."}, {"heading": "A General Theoretical Results", "text": "In this section, we will specify the complete form of the theoretical results to which we have alluded in the body of work. (First, we restate, in long form, our setup and assumptions.) We assume that we program for some functions h: {\u2212 1, 0, 1, 1, 1, 7, {\u2212 1, 0, 1, 1} M of sufficient statistics, using learning distributions, using the set functions x: {\u2212 1, 0, 1, 1} m \u00b7 {\u2212 1, the shape language x, Y = 1, 1, 2, 2, 2, 2, (2, Y) M of learning distributions, using the set functions x: 1, 0, 1, 1} m \u00b7 {\u2212 1, 1}, the shape distributions, Y = 1, 2, 2, exp (2, Y) exp (\u03b8T h (, Y), Y)) where the selected properties x: (10), where the set RM is a parameter, we assume that this partition function, we can derive some of the parameters, i.e., the partition function, we derive some of which we specify."}, {"heading": "B Theoretical Results for Independent Model", "text": "For the independent model, we obtain a more specific version of Theorem A.1. In the independent model, the variables are defined as follows: (1, 0, 1), (2), (1), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (, (2), (, (2), (, (, (2), (, (2), (, (, (, (, (), (, (, (), (, (), (, (), (, (), (, (), (, (, (), (, (, (), (, (, (), (, (, (), (, (, (), (,"}, {"heading": "C Proof of Theorem A.1", "text": "First, we give some examples that will be useful for the detection of two different functions.Lemma D.1. Given a family of maximum entropy distributions (x) = 1 million dollars exp (x) = 1 million dollars exp (x) = 1 million dollars exp (x) = 1 million dollars exp (x) = 1 million dollars exp (x) = 1 million dollars exp (x) = 1 million dollars exp (x) = 1 million dollars exp (x) = 1 million dollars exp (x), then its gradient [x) = 1 million dollars exp (x) = 1 million dollars exp (x) = 1 million dollars exp (x), then its gradient [x) = 1 million dollars exp (x) # 2 million dollars exp (x) # 2 million dollars exp (x)."}, {"heading": "D Proofs of Lemmas", "text": "Lemma x h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h"}, {"heading": "E Proofs of Results for the Independent Model", "text": "It is therefore possible to combine the simple assumption that P (Y) = 12 to complete a complete distribution. Using these assumptions we can demonstrate the following simple result about the moments of sufficient statistics."}, {"heading": "F Proofs of Independent Model Lemmas", "text": "Lemma E.1. The expected values and covariances of the sufficient statistics are for all i, j, E [Number] = Number (Number) = Number (Number) = Number (Number) = Number (Number) = Number (Number) = Number (Number) = Number (Number) = Number (Number) = Number (Number) (Number) = Number (Number) (Number) (Number) = Number (Number) (Number) + Number) (Number) (Number) + Number) (Number) (Number) (Number) + Number) (Number) + Number) (Number) (Number) + Number) (Number) (Number) + Number) (Number) (Number) (Number) (Number)."}, {"heading": "G Additional Experimental Details", "text": "G.1 Synthetic Experiments In Fig. 3 (a-b), we performed synthetic experiments with labeling functions that had a constant coverage \u03b2 = 0.1 and an accuracy resulting from \u03b1 \u0445 uniform (\u00b5\u03b1 \u2212 0.25, \u00b5\u03b1 + 0.25) using \u00b5\u03b1 = 0.75 in the diagrams above. In both cases, we used 1000 normally drawn features that had an average correlation with the true labeling class of 0.5. In this case, we compared data programming (DP-Pipelined) with two baselines. First, we compared with an if-then-return setup where the sequence is optimal (ITR-Oracle). Second, we compared with simple majority matching (MV). In Fig. 3 (c), we showed an experiment in which we added dependent labeling functions to a group of Mind = 50 independent labeling functions and either did not have the same label structure (in this case) as the independent labeling functions (in this case)."}], "references": [{"title": "Pattern learning for relation extraction with a hierarchical topic model", "author": ["E. Alfonseca", "K. Filippova", "J.-Y. Delort", "G. Garrido"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 54\u201359. Association for Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Scalable semi-supervised aggregation of classifiers", "author": ["A. Balsubramani", "Y. Freund"], "venue": "Advances in Neural Information Processing Systems, pages 1351\u20131359,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Consistency of weighted majority votes", "author": ["D. Berend", "A. Kontorovich"], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 3446\u20133454. Curran Associates, Inc.,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "Proceedings of the eleventh annual conference on Computational learning theory, pages 92\u2013100. ACM,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Label-noise robust logistic regression and its applications", "author": ["J. Bootkrajang", "A. Kab\u00e1n"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 143\u2013158. Springer,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to extract relations from the web using minimal supervision", "author": ["R. Bunescu", "R. Mooney"], "venue": "Annual meeting-association for Computational Linguistics, volume 45, page 576,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["M. Craven", "J. Kumlien"], "venue": "In ISMB,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Aggregating crowdsourced binary ratings", "author": ["N. Dalvi", "A. Dasgupta", "R. Kumar", "V. Rastogi"], "venue": "Proceedings of the 22Nd International Conference on World Wide Web, WWW \u201913, pages 285\u2013294,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Applied statistics, pages 20\u201328,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1979}, {"title": "An improved corpus of disease mentions in pubmed citations", "author": ["R.I. Do\u011fan", "Z. Lu"], "venue": "Proceedings of the 2012 workshop on biomedical natural language processing, pages 91\u201399. Association for Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Harnessing the crowdsourcing power of social media for disaster relief", "author": ["H. Gao", "G. Barbier", "R. Goolsby", "D. Zeng"], "venue": "Technical report, DTIC Document,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["R. Hoffmann", "C. Zhang", "X. Ling", "L. Zettlemoyer", "D.S. Weld"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 541\u2013550. Association for Computational Linguistics,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Comprehensive and reliable crowd assessment algorithms", "author": ["M. Joglekar", "H. Garcia-Molina", "A. Parameswaran"], "venue": "Data Engineering (ICDE), 2015 IEEE 31st International Conference on, pages 195\u2013206. IEEE,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Iterative learning for reliable crowdsourcing systems", "author": ["D.R. Karger", "S. Oh", "D. Shah"], "venue": "Advances in neural information processing systems, pages 1953\u20131961,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Multi-relational learning, text mining, and semi-supervised learning for functional genomics", "author": ["M.-A. Krogel", "T. Scheffer"], "venue": "Machine Learning, 57(1-2):61\u201381,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning with an unreliable teacher", "author": ["G. Lugosi"], "venue": "Pattern Recognition, 25(1):79 \u2013 87,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1992}, {"title": "Large-scale extraction of gene interactions from full-text literature using deepdive", "author": ["E.K. Mallory", "C. Zhang", "C. R\u00e9", "R.B. Altman"], "venue": "Bioinformatics,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["M. Mintz", "S. Bills", "R. Snow", "D. Jurafsky"], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, pages 1003\u20131011,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning with noisy labels", "author": ["N. Natarajan", "I.S. Dhillon", "P.K. Ravikumar", "A. Tewari"], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 1196\u20131204. Curran Associates, Inc.,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Ranking and combining multiple predictors without labeled data", "author": ["F. Parisi", "F. Strino", "B. Nadler", "Y. Kluger"], "venue": "Proceedings of the National Academy of Sciences, 111(4):1253\u20131258,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling relations and their mentions without labeled text", "author": ["S. Riedel", "L. Yao", "A. McCallum"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 148\u2013163. Springer,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Combining generative and discriminative model scores for distant supervision", "author": ["B. Roth", "D. Klakow"], "venue": "EMNLP, pages 24\u201329,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature-based models for improving the quality of noisy training data for relation extraction", "author": ["B. Roth", "D. Klakow"], "venue": "Proceedings of the 22nd ACM international conference on Conference on information & knowledge management, pages 1181\u20131184. ACM,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Boosting: Foundations and algorithms", "author": ["R.E. Schapire", "Y. Freund"], "venue": "MIT press,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Incremental knowledge base construction using deepdive", "author": ["J. Shin", "S. Wu", "F. Wang", "C. De Sa", "C. Zhang", "C. R\u00e9"], "venue": "Proceedings of the VLDB Endowment, 8(11):1310\u20131321,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Overview of the english slot filling track at the tac2014 knowledge base population evaluation", "author": ["M. Surdeanu", "H. Ji"], "venue": "Proc. Text Analysis Conference (TAC2014),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Reducing wrong labels in distant supervision for relation extraction", "author": ["S. Takamatsu", "I. Sato", "H. Nakagawa"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 721\u2013729. Association for Computational Linguistics,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Multilingual relation extraction using compositional universal schema", "author": ["P. Verga", "D. Belanger", "E. Strubell", "B. Roth", "A. McCallum"], "venue": "arXiv preprint arXiv:1511.06396,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Spectral methods meet em: A provably optimal algorithm for crowdsourcing", "author": ["Y. Zhang", "X. Chen", "D. Zhou", "M.I. Jordan"], "venue": "Advances in Neural Information Processing Systems 27, pages 1260\u20131268.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "This trend has also been fueled by the recent empirical success of automated feature generation approaches, notably deep learning methods such as long short term memory (LSTM) networks [12], which ameliorate the burden of feature engineering given large enough labeled training sets.", "startOffset": 185, "endOffset": 189}, {"referenceID": 19, "context": "Data programming extends the idea of distant supervision, in which an external knowledge base is mapped onto an input dataset to generate training examples [20].", "startOffset": 156, "endOffset": 160}, {"referenceID": 14, "context": "In Section 4, we extend our results to more sophisticated data programming models, generalizing related results in crowdsourcing [15].", "startOffset": 129, "endOffset": 133}, {"referenceID": 29, "context": "98 point F1 score gain over a state-of-the-art LSTM baseline trained on hand-labeled data [30].", "startOffset": 90, "endOffset": 94}, {"referenceID": 6, "context": "The canonical example is relation extraction from text, wherein a knowledge base of known relations is heuristically mapped to label a set of mentions in an input corpus as ground truth examples [7, 20].", "startOffset": 195, "endOffset": 202}, {"referenceID": 19, "context": "The canonical example is relation extraction from text, wherein a knowledge base of known relations is heuristically mapped to label a set of mentions in an input corpus as ground truth examples [7, 20].", "startOffset": 195, "endOffset": 202}, {"referenceID": 12, "context": "Basic extensions group these mapped examples by the particular textual pattern w that they occur with, and cast the problem as a multiple instance learning one [13, 23].", "startOffset": 160, "endOffset": 168}, {"referenceID": 22, "context": "Basic extensions group these mapped examples by the particular textual pattern w that they occur with, and cast the problem as a multiple instance learning one [13, 23].", "startOffset": 160, "endOffset": 168}, {"referenceID": 24, "context": "Other extensions actually model the accuracy of this pattern w using a discriminative feature-based model [25], or generative models such as hierarchical topic models [1, 24, 29].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "Other extensions actually model the accuracy of this pattern w using a discriminative feature-based model [25], or generative models such as hierarchical topic models [1, 24, 29].", "startOffset": 167, "endOffset": 178}, {"referenceID": 23, "context": "Other extensions actually model the accuracy of this pattern w using a discriminative feature-based model [25], or generative models such as hierarchical topic models [1, 24, 29].", "startOffset": 167, "endOffset": 178}, {"referenceID": 28, "context": "Other extensions actually model the accuracy of this pattern w using a discriminative feature-based model [25], or generative models such as hierarchical topic models [1, 24, 29].", "startOffset": 167, "endOffset": 178}, {"referenceID": 5, "context": "There is also a wealth of examples where additional heuristic patterns used to label training data are collected from unlabeled data [6] or directly from users [19, 27], in a similar manner to our approach, but without a framework to deal with the fact that said labels are explicitly noisy.", "startOffset": 133, "endOffset": 136}, {"referenceID": 18, "context": "There is also a wealth of examples where additional heuristic patterns used to label training data are collected from unlabeled data [6] or directly from users [19, 27], in a similar manner to our approach, but without a framework to deal with the fact that said labels are explicitly noisy.", "startOffset": 160, "endOffset": 168}, {"referenceID": 26, "context": "There is also a wealth of examples where additional heuristic patterns used to label training data are collected from unlabeled data [6] or directly from users [19, 27], in a similar manner to our approach, but without a framework to deal with the fact that said labels are explicitly noisy.", "startOffset": 160, "endOffset": 168}, {"referenceID": 10, "context": "Crowdsourcing is widely used for various machine learning tasks [11, 16].", "startOffset": 64, "endOffset": 72}, {"referenceID": 15, "context": "Crowdsourcing is widely used for various machine learning tasks [11, 16].", "startOffset": 64, "endOffset": 72}, {"referenceID": 8, "context": "Of particular relevance to our problem setting is the theoretical question of how to model the accuracy of various experts without ground truth available, classically raised in the context of crowdsourcing [9].", "startOffset": 206, "endOffset": 209}, {"referenceID": 2, "context": "More recent results provide formal guarantees even in the absence of labeled data using various approaches [3, 8, 14, 15, 22, 31].", "startOffset": 107, "endOffset": 129}, {"referenceID": 7, "context": "More recent results provide formal guarantees even in the absence of labeled data using various approaches [3, 8, 14, 15, 22, 31].", "startOffset": 107, "endOffset": 129}, {"referenceID": 13, "context": "More recent results provide formal guarantees even in the absence of labeled data using various approaches [3, 8, 14, 15, 22, 31].", "startOffset": 107, "endOffset": 129}, {"referenceID": 14, "context": "More recent results provide formal guarantees even in the absence of labeled data using various approaches [3, 8, 14, 15, 22, 31].", "startOffset": 107, "endOffset": 129}, {"referenceID": 21, "context": "More recent results provide formal guarantees even in the absence of labeled data using various approaches [3, 8, 14, 15, 22, 31].", "startOffset": 107, "endOffset": 129}, {"referenceID": 30, "context": "More recent results provide formal guarantees even in the absence of labeled data using various approaches [3, 8, 14, 15, 22, 31].", "startOffset": 107, "endOffset": 129}, {"referenceID": 3, "context": "Co-training is a classic procedure for effectively utilizing both a small amount of labeled data and a large amount of unlabeled data by selecting two conditionally independent views of the data [4].", "startOffset": 195, "endOffset": 198}, {"referenceID": 16, "context": "In addition to not needing a set of labeled data, and allowing for more than two views (labeling functions in our case), our approach allows explicit modeling of dependencies between views, for example allowing observed issues with dependencies between views to be explicitly modeled [17].", "startOffset": 284, "endOffset": 288}, {"referenceID": 25, "context": "Boosting is a well known procedure for combining the output of many \u201cweak\u201d classifiers to create a strong classifier in a supervised setting [26].", "startOffset": 141, "endOffset": 145}, {"referenceID": 1, "context": "Recently, boosting-like methods have been proposed which leverage unlabeled data in addition to labeled data, which is also used to set constraints on the accuracies of the individual classifiers being ensembled [2].", "startOffset": 212, "endOffset": 215}, {"referenceID": 17, "context": "The general case of learning with noisy labels is treated both in classical [18] and more recent contexts [21].", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "The general case of learning with noisy labels is treated both in classical [18] and more recent contexts [21].", "startOffset": 106, "endOffset": 110}, {"referenceID": 4, "context": "It has also been studied specifically in the context of label-noise robust logistic regression [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 14, "context": "In contrast, in the crowdsourcing setting [15], the number of workers m tends to infinity while here it is constant while the dataset grows.", "startOffset": 42, "endOffset": 46}, {"referenceID": 27, "context": "34 points in F1 score, including what would have been a winning score on the 2014 TAC-KBP challenge [28].", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "Automatically-generated Features We additionally compare both hand-tuned and automatically-generated features, where the latter are learned via an LSTM recurrent neural network (RNN) [12].", "startOffset": 183, "endOffset": 187}, {"referenceID": 29, "context": "98 point F1 score improvement over a state-of-the-art LSTM approach applied to the TAC-KBP task which was trained on hand-labeled data [30].", "startOffset": 135, "endOffset": 139}, {"referenceID": 9, "context": "Their goal was to build a disease tagging system, a common and important challenge in the bioinformatics domain [10].", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "References [1] E.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] Y.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users provide a set of labeling functions, which are programs that heuristically label large subsets of data points, albeit noisily. By viewing these labeling functions as implicitly describing a generative model for this noise, we show that we can recover the parameters of this model to \u201cdenoise\u201d the training set. Then, we show how to modify a discriminative loss function to make it noise-aware. We demonstrate our method over a range of discriminative models including logistic regression and LSTMs. We establish theoretically that we can recover the parameters of these generative models in a handful of settings. Experimentally, on the 2014 TAC-KBP relation extraction challenge, we show that data programming would have obtained a winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a supervised LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way to create machine learning models for non-experts.", "creator": "LaTeX with hyperref package"}}}