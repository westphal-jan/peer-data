{"id": "1605.07736", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Learning Multiagent Communication with Backpropagation", "abstract": "Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNN, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.", "histories": [["v1", "Wed, 25 May 2016 05:33:21 GMT  (1955kb,D)", "http://arxiv.org/abs/1605.07736v1", null], ["v2", "Mon, 31 Oct 2016 17:29:58 GMT  (2132kb,D)", "http://arxiv.org/abs/1605.07736v2", "Accepted to NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["sainbayar sukhbaatar", "arthur szlam", "rob fergus"], "accepted": true, "id": "1605.07736"}, "pdf": {"name": "1605.07736.pdf", "metadata": {"source": "CRF", "title": "Learning Multiagent Communication with Backpropagation", "authors": ["Sainbayar Sukhbaatar", "Arthur Szlam"], "emails": ["sainbar@cs.nyu.edu", "aszlam@fb.com", "robfergus@fb.com"], "sections": [{"heading": null, "text": "It is essential to perform complex tasks in real-world environments where each actor has limited capabilities and / or visibility of the world. Practical examples include elevator control [4] and sensor networks [6]; communication is also important for success in robotic football [26]. In any partially observed environment, communication between actors is crucial to coordinate the behavior of each individual. While each actor's model of control is typically learned through reinforcement learning [2, 29], the specification and format of communication is usually predetermined. In robotic football, for example, bots are designed to communicate their position and proximity to the ball at all times. In this work, we propose a model in which cooperating actors learn to communicate with each other before starting exclusively from reward in the environment. Each actor is controlled by a deep network that has additional access to a communication channel that provides a continuous vector step to each."}], "references": [{"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "ICML", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["L. Busoniu", "R. Babuska", "B. De Schutter"], "venue": "Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, 38(2):156\u2013172", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "An overview of recent progress in the study of distributed multi-agent coordination", "author": ["Y. Cao", "W. Yu", "W. Ren", "G. Chen"], "venue": "IEEE Transactions on Industrial Informatics, 1(9):427\u2013438", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Elevator group control using multiple reinforcement learning agents", "author": ["R.H. Crites", "A.G. Barto"], "venue": "Machine Learning, 33(2):235\u2013262", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "N", "author": ["J.N. Foerster", "Y.M. Assael"], "venue": "de Freitas, and S. Whiteson. Learning to communicate to solve riddles with deep distributed recurrent q-networks. arXiv, abs/1602.02672", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic approach to collaborative multi-robot localization", "author": ["D. Fox", "W. Burgard", "H. Kruppa", "S. Thrun"], "venue": "Autonomous Robots, 8(3):325\u2013\u2013344", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning communication for multi-agent systems", "author": ["C.L. Giles", "K.C. Jim"], "venue": "Innovative Concepts for Agent Based Systems, pages 377\u2014-390. Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Multiagent planning with factored mdps", "author": ["C. Guestrin", "D. Koller", "R. Parr"], "venue": "NIPS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Deep learning for real-time atari game play using offline monte-carlo tree search planning", "author": ["X. Guo", "S. Singh", "H. Lee", "R.L. Lewis", "X. Wang"], "venue": "NIPS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning of communication codes in multi-agent reinforcement learning problem", "author": ["T. Kasai", "H. Tenmoto", "A. Kamiya"], "venue": "IEEE Conference on Soft Computing in Industrial Applications, pages 1\u20136", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "The International Conference on Learning Representations", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "An algorithm for distributed reinforcement learning in cooperative multi-agent systems", "author": ["M. Lauer", "M.A. Riedmiller"], "venue": "Proceedings of the Seventeenth International Conference on Machine Learning, ICML \u201900", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv:1504.00702", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "and R", "author": ["Y. Li", "D. Tarlow", "M. Brockschmidt"], "venue": "Zemel. Gated graph sequence neural networks", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Value-function reinforcement learning in markov games", "author": ["M.L. Littman"], "venue": "Cognitive Systems Research, 2(1):55\u201366", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Move evaluation in go using deep convolutional neural networks", "author": ["C.J. Maddison", "A. Huang", "I. Sutskever", "D. Silver"], "venue": "ICLR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Coordination of communication in robot teams by reinforcement learning", "author": ["D. Maravall", "J. De Lope", "R. Domnguez"], "venue": "Robotics and Autonomous Systems, 61(7):661\u2013666", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement learning in the multi-robot domain", "author": ["M. Matari"], "venue": "Autonomous Robots, 4(1):73\u201383", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Querypomdp: Pomdp-based communication in multiagent systems", "author": ["F.S. Melo", "M. Spaan", "S.J. Witwicki"], "venue": "Multi-Agent Systems, pages 189\u2013204", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Consensus and cooperation in networked multi-agent systems", "author": ["R. Olfati-Saber", "J. Fax", "R. Murray"], "venue": "Proceedings of the IEEE, 95(1):215\u2013233", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Reverend bayes on inference engines: A distributed hierarchical approach", "author": ["J. Pearl"], "venue": "AAAI, pages 133\u2013136", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1982}, {"title": "Towards Neural Network-based Reasoning", "author": ["B. Peng", "Z. Lu", "H. Li", "K. Wong"], "venue": "ArXiv preprint: 1508.05508", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "The graph neural network model", "author": ["F. Scarselli", "M. Gori", "A.C. Tsoi", "M. Hagenbuchner", "G. Monfardini"], "venue": "IEEE Trans. Neural Networks, 20(1):61\u201380", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards collaborative and adversarial learning: A case study in robotic soccer", "author": ["P. Stone", "M. Veloso"], "venue": "International Journal of Human Computer Studies, (48)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "Mazebase: A sandbox for learning from games", "author": ["S. Sukhbaatar", "A. Szlam", "G. Synnaeve", "S. Chintala", "R. Fergus"], "venue": "CoRR, abs/1511.07401", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "NIPS", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Introduction to Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "Multiagent cooperation and competition with deep reinforcement learning", "author": ["A. Tampuu", "T. Matiisen", "D. Kodelja", "I. Kuzovkin", "K. Korjus", "J. Aru", "R. Vicente"], "venue": "arXiv:1511.08779", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-agent reinforcement learning: Independent vs", "author": ["M. Tan"], "venue": "cooperative agents. In ICML", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1993}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Distributed Autonomous Robotic Systems 8", "author": ["P. Varshavskaya", "L.P. Kaelbling", "D. Rus"], "venue": "chapter Efficient Distributed Reinforcement Learning through Agreement, pages 367\u2013378. Springer Berlin Heidelberg", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Reinforcement learning to play an optimal nash equilibrium in team markov games", "author": ["X. Wang", "T. Sandholm"], "venue": "Advances in neural information processing systems, pages 1571\u20131578", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint: 1502.05698", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning, pages 229\u2013256", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1992}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "CoRR, abs/1603.01417", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Coordinating multi-agent reinforcement learning with limited communication", "author": ["C. Zhang", "V. Lesser"], "venue": "Proceedings of the 2013 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS \u201913, pages 1101\u20131108", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "Practical examples include elevator control [4] and sensor networks [6]; communication is also important for success in robot soccer [26].", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "Practical examples include elevator control [4] and sensor networks [6]; communication is also important for success in robot soccer [26].", "startOffset": 68, "endOffset": 71}, {"referenceID": 25, "context": "Practical examples include elevator control [4] and sensor networks [6]; communication is also important for success in robot soccer [26].", "startOffset": 133, "endOffset": 137}, {"referenceID": 1, "context": "While the model controlling each agent is typically learned via reinforcement learning [2, 29], the specification and format of the communication is usually pre-determined.", "startOffset": 87, "endOffset": 94}, {"referenceID": 28, "context": "While the model controlling each agent is typically learned via reinforcement learning [2, 29], the specification and format of the communication is usually pre-determined.", "startOffset": 87, "endOffset": 94}, {"referenceID": 35, "context": "We use policy gradient [36] with a state specific baseline for delivering a gradient to the model.", "startOffset": 23, "endOffset": 27}, {"referenceID": 32, "context": "47 We use policy gradient [33] with a state specific baseline for delivering a gradient to the model.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "47 We use policy gr dient [33] with a state specific baseline for delivering a gradient to the model.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "47 We use policy gradient [33] with a state specific baseline for delivering a gradient to the model.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "47 We use policy gr dient [33] with a state specific baseline for delivering a gradient to the model.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "47 We use policy gradient [33] with a state specific baseline for delivering a gradient to the model.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "47 We use policy gr dient [33] with a state specific baseline for delivering a gradient to the model.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "The edges within the graph represent the communication channel tween agents, with (4) being equivalent to belief propagation [22].", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "But if these are viewed as internal time steps of the agent, then the communication output can be treated as an action of the agent at a given (internal) time step and we can direct employ the policy gradient [36].", "startOffset": 209, "endOffset": 213}, {"referenceID": 8, "context": "4 Related Work Our model combines a deep network with reinforcement learning [9, 20, 13].", "startOffset": 77, "endOffset": 88}, {"referenceID": 19, "context": "4 Related Work Our model combines a deep network with reinforcement learning [9, 20, 13].", "startOffset": 77, "endOffset": 88}, {"referenceID": 12, "context": "4 Related Work Our model combines a deep network with reinforcement learning [9, 20, 13].", "startOffset": 77, "endOffset": 88}, {"referenceID": 15, "context": "Several recent works have applied these methods to multi-agent domains, such as Go [16, 25] and Atari games [30], but they assume full visibility of the environment and lack communication.", "startOffset": 83, "endOffset": 91}, {"referenceID": 24, "context": "Several recent works have applied these methods to multi-agent domains, such as Go [16, 25] and Atari games [30], but they assume full visibility of the environment and lack communication.", "startOffset": 83, "endOffset": 91}, {"referenceID": 29, "context": "Several recent works have applied these methods to multi-agent domains, such as Go [16, 25] and Atari games [30], but they assume full visibility of the environment and lack communication.", "startOffset": 108, "endOffset": 112}, {"referenceID": 1, "context": "There is a rich literature on multi-agent reinforcement learning (MARL) [2], particularly in the robotics domain [18, 26, 6, 21, 3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 17, "context": "There is a rich literature on multi-agent reinforcement learning (MARL) [2], particularly in the robotics domain [18, 26, 6, 21, 3].", "startOffset": 113, "endOffset": 131}, {"referenceID": 25, "context": "There is a rich literature on multi-agent reinforcement learning (MARL) [2], particularly in the robotics domain [18, 26, 6, 21, 3].", "startOffset": 113, "endOffset": 131}, {"referenceID": 5, "context": "There is a rich literature on multi-agent reinforcement learning (MARL) [2], particularly in the robotics domain [18, 26, 6, 21, 3].", "startOffset": 113, "endOffset": 131}, {"referenceID": 20, "context": "There is a rich literature on multi-agent reinforcement learning (MARL) [2], particularly in the robotics domain [18, 26, 6, 21, 3].", "startOffset": 113, "endOffset": 131}, {"referenceID": 2, "context": "There is a rich literature on multi-agent reinforcement learning (MARL) [2], particularly in the robotics domain [18, 26, 6, 21, 3].", "startOffset": 113, "endOffset": 131}, {"referenceID": 11, "context": "Amongst fully cooperative algorithms, many approaches [12, 15, 34] avoid the need for communication by making strong assumptions about visibility of other agents and the environment.", "startOffset": 54, "endOffset": 66}, {"referenceID": 14, "context": "Amongst fully cooperative algorithms, many approaches [12, 15, 34] avoid the need for communication by making strong assumptions about visibility of other agents and the environment.", "startOffset": 54, "endOffset": 66}, {"referenceID": 33, "context": "Amongst fully cooperative algorithms, many approaches [12, 15, 34] avoid the need for communication by making strong assumptions about visibility of other agents and the environment.", "startOffset": 54, "endOffset": 66}, {"referenceID": 30, "context": "Others use communication, but with a pre-determined protocol [31, 19, 38, 17].", "startOffset": 61, "endOffset": 77}, {"referenceID": 18, "context": "Others use communication, but with a pre-determined protocol [31, 19, 38, 17].", "startOffset": 61, "endOffset": 77}, {"referenceID": 37, "context": "Others use communication, but with a pre-determined protocol [31, 19, 38, 17].", "startOffset": 61, "endOffset": 77}, {"referenceID": 16, "context": "Others use communication, but with a pre-determined protocol [31, 19, 38, 17].", "startOffset": 61, "endOffset": 77}, {"referenceID": 9, "context": "[10] and Varshavskaya et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33], both use distributed tabular-RL approaches for simulated tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Giles & Jim [7] use an evolutionary algorithm, rather than reinforcement learning.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "[8] use a single large MDP to control a collection of agents, via a factored message passing framework where the messages are learned.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "[24], as expanded on by Li et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "In particular, in [24], the output of the model is the fixed point of iterating equations (4) and (2) to convergence, using recurrent models.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "In [14], these recurrence equations are unrolled a fixed number of steps and the model trained via backprop through time.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "2 Cooperative Games In this section, we consider two multi-agent tasks in the MazeBase environment [27] that use reward as their training signal.", "startOffset": 99, "endOffset": 103}, {"referenceID": 31, "context": "Both tasks are trained for 300 epochs, each epoch being 100 weight updates with RMSProp [32] on mini-batch of 288 game episodes (distributed over multiple CPU cores).", "startOffset": 88, "endOffset": 92}, {"referenceID": 34, "context": "3 bAbI tasks We apply our model to the bAbI [35] toy Q & A dataset, which consists of 20 tasks each requiring different kind of reasoning.", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "We use the same encoder representation as [28] to convert them to vectors.", "startOffset": 42, "endOffset": 46}, {"referenceID": 10, "context": "003 and mini-batch size 32 with Adam optimizer [11] (\u03b21 = 0.", "startOffset": 47, "endOffset": 51}, {"referenceID": 27, "context": "Our model is doing better than LSTM baseline, but worse than the MemN2N model [28], which is specifically designed to solve reasoning over long stories.", "startOffset": 78, "endOffset": 82}, {"referenceID": 27, "context": "> 5%) LSTM [28] 36.", "startOffset": 11, "endOffset": 15}, {"referenceID": 27, "context": "4 16 MemN2N [28] 4.", "startOffset": 12, "endOffset": 16}, {"referenceID": 36, "context": "2 3 DMN+ [37] 2.", "startOffset": 9, "endOffset": 13}], "year": 2016, "abstractText": "Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNN, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.", "creator": "LaTeX with hyperref package"}}}