{"id": "1605.07747", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and Stochastic Optimization", "abstract": "We study a stochastic and distributed algorithm for nonconvex problems whose objective consists of a sum of $N$ nonconvex $L_i/N$-smooth functions, plus a nonsmooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT) algorithm splits the problem into $N$ subproblems, and utilizes an augmented Lagrangian based primal-dual scheme to solve it in a distributed and stochastic manner. With a special non-uniform sampling, a version of NESTT achieves $\\epsilon$-stationary solution using $\\mathcal{O}((\\sum_{i=1}^N\\sqrt{L_i/N})^2/\\epsilon)$ gradient evaluations, which can be up to $\\mathcal{O}(N)$ times better than the (proximal) gradient descent methods. It also achieves Q-linear convergence rate for nonconvex $\\ell_1$ penalized quadratic problems with polyhedral constraints. Further, we reveal a fundamental connection between {\\it primal-dual} based methods and a few {\\it primal only} methods such as IAG/SAG/SAGA.", "histories": [["v1", "Wed, 25 May 2016 06:42:51 GMT  (66kb,D)", "https://arxiv.org/abs/1605.07747v1", "35 pages, 2 figures"], ["v2", "Mon, 7 Nov 2016 23:06:57 GMT  (70kb,D)", "http://arxiv.org/abs/1605.07747v2", "35 pages, 2 figures"]], "COMMENTS": "35 pages, 2 figures", "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["davood hajinezhad", "mingyi hong", "tuo zhao", "zhaoran wang"], "accepted": true, "id": "1605.07747"}, "pdf": {"name": "1605.07747.pdf", "metadata": {"source": "CRF", "title": "NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and Stochastic Optimization", "authors": ["Davood Hajinezhad", "Mingyi Hong", "Tuo Zhao", "Zhaoran Wang"], "emails": ["dhaji@iastate.edu", "mingyi@iastate.edu", "tzhao5@jhu.edu", "zhaoran@princeton.edu"], "sections": [{"heading": null, "text": "It also establishes a basic link between primary-dual methods and a few primary methods such as IAG / SAG / SAGA.1), in which the following non-convexic and non-conventional optimization problems are considered. Furthermore, we show a basic link between primary-dual methods and a few primary methods such as IAG / SAGA.1), in which the following non-convexic and non-conventional optimizations are problematic. (z): 1N (z) + g0 (z) + g0 (z).Rd; for each i {0, \u00b7 N}, gi: Rd \u2192 R is a smooth, possibly non-convexic function having Li-Lipschitz continuous gradients; p (z): Rd \u2192 R is a lower semi-continuous, but possibly smooth function."}], "references": [{"title": "Variance reduction for faster non-convex optimization. 2016", "author": ["Z.A.-Zhu", "E. Hazan"], "venue": "Preprint, available on arXiv,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Penalized likelihood regression for generalized linear models with non-quadratic penalties", "author": ["A. Antoniadis", "I. Gijbels", "M. Nikolova"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Incremental gradient, subgradient, and proximal methods f or convex optimization: A survey", "author": ["D. Bertsekas"], "venue": "LIDS Report", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Parallel and Distributed Computation: Numerical Methods, 2nd ed", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Optimal resource allocation in coordinated multi-cell systems", "author": ["E. Bjornson", "E. Jorswieck"], "venue": "Foundations and Trends in Communications and Information Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "A convergent incremental gradient method with a constant step size", "author": ["D. Blatt", "A.O. Hero", "H. Gauchman"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Convex optimization for big data: Scalable, randomized, and parallel algorithms for big data analytics", "author": ["V. Cevher", "S. Becker", "M. Schmidt"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Multi-agent distributed optimization via inexact consensus admm", "author": ["T.-H. Chang", "M. Hong", "X. Wang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["A. Defazio", "F. Bach", "S. Lacoste-Julien"], "venue": "In The Proceeding of NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Stochastic first- and zeroth-order methods for nonconvx stochastic programming", "author": ["S. Ghadimi", "G. Lan"], "venue": "SIAM Journal on Optimizatnoi,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Nonnegative matrix factorization using admm: Algorithm analysis and convergence guarantees", "author": ["D. Hajinezhad", "T.-H. Chang", "X. Wang", "Q. Shi", "M. Hong"], "venue": "In the Proceedings of ICASSP", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Nonconvex alternating direction method of multipliers for distributed sparse principal component analysis", "author": ["D. Hajinezhad", "M. Hong"], "venue": "In the Proceedings of GlobalSIPT,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems", "author": ["M. Hong", "Z.-Q. Luo", "M. Razaviyayn"], "venue": "SIAM Journal On Optimization,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In the Proceedings of the Neural Information Processing (NIPS)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Linear convergence of proximal-gradient methods under the polyaklojasiewicz condition", "author": ["H. Karimi", "M. Schmidt"], "venue": "In NIPS workshop on optimization,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "An optimal randomized incremental gradient method", "author": ["G. Lan"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity", "author": ["P.-L. Loh", "M. Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Next: In-network nonconvex optimization. 2016", "author": ["P.D. Lorenzo", "G. Scutari"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Error bounds and the convergence analysis of matrix splitting algorithms for the affine variational inequality problem", "author": ["Z.-Q. Luo", "P. Tseng"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1992}, {"title": "On the linear convergence of descent methods for convex essentially smooth minimization", "author": ["Z.-Q. Luo", "P. Tseng"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "A unified framework for highdimensional analysis of m-estimators with decomposable regularizers", "author": ["S.N. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Statist. Sci., 27(4):538\u2013557,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Introductory lectures on convex optimization: A basic course", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Parallel successive convex approximation for nonsmooth nonconvex optimization", "author": ["M. Razaviyayn", "M. Hong", "Z.-Q. Luo", "J.S. Pang"], "venue": "In the Proceedings of NIPS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Fast incremental method for nonconvex optimization. 2016", "author": ["S.J. Reddi", "S. Sra", "B. Poczos", "A. Smola"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["M. Schmidt", "N.L. Roux", "F. Bach"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Proximal stochastic dual coordinate ascent methods for regularzied loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Journal of Machine Learning Rsearch,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Scalable nonconvex inexact proximal splitting", "author": ["S. Sra"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "A coordinate gradient descent method for nonsmooth separable minimization", "author": ["P. Tseng", "S. Yun"], "venue": "Mathematical Programming,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Optimal computational and statistical rates of convergence for sparse nonconvex learning problems", "author": ["Z. Wang", "H. Liu", "T. Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "On the Liu - Floudas convexification of smooth programs", "author": ["S. Zlobec"], "venue": "Journal of Global Optimization,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}], "referenceMentions": [{"referenceID": 7, "context": "It arises frequently in applications such as machine learning and signal processing; see a recent survey [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "In particular, each smooth functions {gi}i=1 can represent: 1) a minibatch of loss functions modeling data fidelity, such as the `2 loss, the logistic loss, etc; 2) nonconvex activation functions for neural networks, such as the logit or the tanh functions; 3) nonconvex utility functions used in signal processing, machine learning, and resource allocation, see [5], and [12].", "startOffset": 363, "endOffset": 366}, {"referenceID": 11, "context": "In particular, each smooth functions {gi}i=1 can represent: 1) a minibatch of loss functions modeling data fidelity, such as the `2 loss, the logistic loss, etc; 2) nonconvex activation functions for neural networks, such as the logit or the tanh functions; 3) nonconvex utility functions used in signal processing, machine learning, and resource allocation, see [5], and [12].", "startOffset": 372, "endOffset": 376}, {"referenceID": 1, "context": "The smooth function g0 can represent smooth nonconvex regularizers such as the non-quadratic penalties [2], or the smooth part of the SCAD or MCP regularizers (which is a concave function) [30].", "startOffset": 103, "endOffset": 106}, {"referenceID": 29, "context": "The smooth function g0 can represent smooth nonconvex regularizers such as the non-quadratic penalties [2], or the smooth part of the SCAD or MCP regularizers (which is a concave function) [30].", "startOffset": 189, "endOffset": 193}, {"referenceID": 6, "context": "Such distributed computation model has been popular in large-scale machine learning and signal processing [7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 16, "context": "Such model is also closely related to the (centralized) stochastic finite-sum optimization problem [17, 10, 15, 25, 1, 26], in which each time the iterate is updated based on the gradient information of a random component function.", "startOffset": 99, "endOffset": 122}, {"referenceID": 9, "context": "Such model is also closely related to the (centralized) stochastic finite-sum optimization problem [17, 10, 15, 25, 1, 26], in which each time the iterate is updated based on the gradient information of a random component function.", "startOffset": 99, "endOffset": 122}, {"referenceID": 14, "context": "Such model is also closely related to the (centralized) stochastic finite-sum optimization problem [17, 10, 15, 25, 1, 26], in which each time the iterate is updated based on the gradient information of a random component function.", "startOffset": 99, "endOffset": 122}, {"referenceID": 24, "context": "Such model is also closely related to the (centralized) stochastic finite-sum optimization problem [17, 10, 15, 25, 1, 26], in which each time the iterate is updated based on the gradient information of a random component function.", "startOffset": 99, "endOffset": 122}, {"referenceID": 0, "context": "Such model is also closely related to the (centralized) stochastic finite-sum optimization problem [17, 10, 15, 25, 1, 26], in which each time the iterate is updated based on the gradient information of a random component function.", "startOffset": 99, "endOffset": 122}, {"referenceID": 25, "context": "Such model is also closely related to the (centralized) stochastic finite-sum optimization problem [17, 10, 15, 25, 1, 26], in which each time the iterate is updated based on the gradient information of a random component function.", "startOffset": 99, "endOffset": 122}, {"referenceID": 6, "context": "Note that such splitting scheme has been popular in the convex setting [7], but not so when the problem becomes nonconvex.", "startOffset": 71, "endOffset": 74}, {"referenceID": 22, "context": "Compared with the classical gradient descent, which in the worst case requires O( \u2211N i=1 Li/ ) gradient evaluation to achieve -stationarity [23], our obtained rate can be up to O(N) times better in the case where the Li\u2019s are not equal.", "startOffset": 140, "endOffset": 144}, {"referenceID": 9, "context": "Our work also reveals a fundamental connection between primal-dual based algorithms and the primal only average-gradient based algorithm such as SAGA/SAG/IAG [10, 25, 26, 6].", "startOffset": 158, "endOffset": 173}, {"referenceID": 24, "context": "Our work also reveals a fundamental connection between primal-dual based algorithms and the primal only average-gradient based algorithm such as SAGA/SAG/IAG [10, 25, 26, 6].", "startOffset": 158, "endOffset": 173}, {"referenceID": 25, "context": "Our work also reveals a fundamental connection between primal-dual based algorithms and the primal only average-gradient based algorithm such as SAGA/SAG/IAG [10, 25, 26, 6].", "startOffset": 158, "endOffset": 173}, {"referenceID": 5, "context": "Our work also reveals a fundamental connection between primal-dual based algorithms and the primal only average-gradient based algorithm such as SAGA/SAG/IAG [10, 25, 26, 6].", "startOffset": 158, "endOffset": 173}, {"referenceID": 9, "context": "Popular algorithms include the SAG/SAGA [10, 26], the SDCA [27], the SVRG [15], the RPDG [17] and so on.", "startOffset": 40, "endOffset": 48}, {"referenceID": 25, "context": "Popular algorithms include the SAG/SAGA [10, 26], the SDCA [27], the SVRG [15], the RPDG [17] and so on.", "startOffset": 40, "endOffset": 48}, {"referenceID": 26, "context": "Popular algorithms include the SAG/SAGA [10, 26], the SDCA [27], the SVRG [15], the RPDG [17] and so on.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "Popular algorithms include the SAG/SAGA [10, 26], the SDCA [27], the SVRG [15], the RPDG [17] and so on.", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "Popular algorithms include the SAG/SAGA [10, 26], the SDCA [27], the SVRG [15], the RPDG [17] and so on.", "startOffset": 89, "endOffset": 93}, {"referenceID": 27, "context": "When the problem becomes nonconvex, the well-known incremental based algorithm can be used [28, 3], but these methods generally lack convergence rate guarantees.", "startOffset": 91, "endOffset": 98}, {"referenceID": 2, "context": "When the problem becomes nonconvex, the well-known incremental based algorithm can be used [28, 3], but these methods generally lack convergence rate guarantees.", "startOffset": 91, "endOffset": 98}, {"referenceID": 10, "context": "The SGD based method has been studied in [11], with O(1/ 2) convergence rate.", "startOffset": 41, "endOffset": 45}, {"referenceID": 0, "context": "Recent works [1] and [25] develop algorithms based on SVRG and SAGA for a special case of (1) where the entire problem is smooth and unconstrained.", "startOffset": 13, "endOffset": 16}, {"referenceID": 24, "context": "Recent works [1] and [25] develop algorithms based on SVRG and SAGA for a special case of (1) where the entire problem is smooth and unconstrained.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "On the other hand, distributed stochastic algorithms for solving problem (1) in the nonconvex setting has been proposed in [14], [13], in which each time a randomly picked subset of agents update their local variables.", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "On the other hand, distributed stochastic algorithms for solving problem (1) in the nonconvex setting has been proposed in [14], [13], in which each time a randomly picked subset of agents update their local variables.", "startOffset": 129, "endOffset": 133}, {"referenceID": 18, "context": "There has been some recent distributed algorithms designed for (1) [19], but again without global convergence rate guarantee.", "startOffset": 67, "endOffset": 71}, {"referenceID": 23, "context": "It can be checked that the pGRAD vanishes at the set of stationary solutions of (1) [24].", "startOffset": 84, "endOffset": 88}, {"referenceID": 6, "context": "We remark that NESTT-G is related to the popular ADMM method for convex optimization [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 17, "context": "In this section we show that the NESTT-G is capable of linear convergence for a family of nonconvex quadratic problems, which has important applications, for example in high-dimensional statistical learning [18].", "startOffset": 207, "endOffset": 211}, {"referenceID": 20, "context": "Our linear convergence result is based upon certain error bound condition around the stationary solutions set, which has been shown in [21] for smooth quadratic problems and has been extended to including `1 penalty in [29, Theorem 4].", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "There has been some recent works showing linear convergence for nonconvex problems satisfying certain quadratic growth condition [16, 25, 1].", "startOffset": 129, "endOffset": 140}, {"referenceID": 24, "context": "There has been some recent works showing linear convergence for nonconvex problems satisfying certain quadratic growth condition [16, 25, 1].", "startOffset": 129, "endOffset": 140}, {"referenceID": 0, "context": "There has been some recent works showing linear convergence for nonconvex problems satisfying certain quadratic growth condition [16, 25, 1].", "startOffset": 129, "endOffset": 140}, {"referenceID": 15, "context": "However the problems considered in [16, 25, 1] are smooth unconstrained problems, and every stationary point is a global minimum, therefore they do no cover our nonconvex quadratic problems, whose stationary solutions are not global minimizers.", "startOffset": 35, "endOffset": 46}, {"referenceID": 24, "context": "However the problems considered in [16, 25, 1] are smooth unconstrained problems, and every stationary point is a global minimum, therefore they do no cover our nonconvex quadratic problems, whose stationary solutions are not global minimizers.", "startOffset": 35, "endOffset": 46}, {"referenceID": 0, "context": "However the problems considered in [16, 25, 1] are smooth unconstrained problems, and every stationary point is a global minimum, therefore they do no cover our nonconvex quadratic problems, whose stationary solutions are not global minimizers.", "startOffset": 35, "endOffset": 46}, {"referenceID": 28, "context": "To proceed, A sequence {x} is said to converge Q-linearly to some x\u0304 if lim supr \u2016x \u2212 x\u0304\u2016/\u2016x \u2212 x\u0304\u2016 \u2264 \u03c1, where \u03c1 \u2208 (0, 1) is some constant; cf [29] and references therein.", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "(21) We define the optimality gap by adding to \u2016\u2207\u0303L(w)\u20162 the size of the constraint violation [14]: H(w) := \u2016\u2207\u0303L(w)\u2016 + N \u2211", "startOffset": 94, "endOffset": 98}, {"referenceID": 25, "context": "Then (23) takes the same form as the SAG presented in [26].", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "Further, when the component functions gi\u2019s are picked cyclically in a Gauss-Seidel manner, the iteration (23) takes the same form as the IAG algorithm [6].", "startOffset": 151, "endOffset": 154}, {"referenceID": 9, "context": "Then (23) is the same as the SAGA algorithm [10], which is design for optimizing convex nonsmooth finite sum problems.", "startOffset": 44, "endOffset": 48}, {"referenceID": 24, "context": "Thirdly, we note that a recent paper [25] has shown that SAGA works for smooth and unconstrained nonconvex problem.", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "Compared with GD, which achieves -stationarity using O( \u2211N i=1 Li/ ) gradient evaluations in the worse case (in the sense that \u2211N i=1 Li/N = L), the rate in [25] is O(N1/3) times better.", "startOffset": 157, "endOffset": 161}, {"referenceID": 24, "context": "However, the algorithm in [25] is different from NESTT-G in two aspects: 1) it does not generalize to the nonsmooth constrained problem (1); 2) it samples two component functions at each iteration, while NESTT-G only samples once.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "For example in LASSO problem the data matrix is often normalized by feature (or \u201ccolumn-normalized\u201d [22]), therefore the `2 norm of each row of the data matrix (which corresponds to the Lipschitz constant for each component function) can be dramatically different.", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "Consider the high dimensional regression problem with noisy observation [18], where M observations are generated by y = X\u03bd + .", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "To test the performance of the proposed algorithm, we generate the problem following similar setups as [18].", "startOffset": 103, "endOffset": 107}, {"referenceID": 24, "context": "We implement NESTT-G/E, the SGD, and the nonconvex SAGA proposed in [25] with stepsize \u03b2 = 1 3LmaxN (with Lmax := maxi Li).", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "Note that the SAGA proposed in [25] only works for the unconstrained problems with uniform Li, therefore when applied to (24) it is not guaranteed to converge.", "startOffset": 31, "endOffset": 35}], "year": 2016, "abstractText": "We study a stochastic and distributed algorithm for nonconvex problems whose objective consists of a sum of N nonconvex Li/N -smooth functions, plus a nonsmooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT) algorithm splits the problem into N subproblems, and utilizes an augmented Lagrangian based primal-dual scheme to solve it in a distributed and stochastic manner. With a special non-uniform sampling, a version of NESTT achieves -stationary solution using O(( \u2211N i=1 \u221a Li/N) / ) gradient evaluations, which can be up to O(N) times better than the (proximal) gradient descent methods. It also achieves Q-linear convergence rate for nonconvex `1 penalized quadratic problems with polyhedral constraints. Further, we reveal a fundamental connection between primal-dual based methods and a few primal only methods such as IAG/SAG/SAGA.", "creator": "LaTeX with hyperref package"}}}