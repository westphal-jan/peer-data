{"id": "1605.07779", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Neural Universal Discrete Denoiser", "abstract": "We present a new framework of applying deep neural networks (DNN) to devise a universal discrete denoiser. Unlike other approaches that utilize supervised learning for denoising, we do not require any additional training data. In such setting, while the ground-truth label, i.e., the clean data, is not available, we devise \"pseudo-labels\" and a novel objective function such that DNN can be trained in a same way as supervised learning to become a discrete denoiser. We experimentally show that our resulting algorithm, dubbed as Neural DUDE, significantly outperforms the previous state-of-the-art in several applications with a systematic rule of choosing the hyperparameter, which is an attractive feature in practice.", "histories": [["v1", "Wed, 25 May 2016 08:50:21 GMT  (590kb,D)", "https://arxiv.org/abs/1605.07779v1", "Submitted to NIPS 2016"], ["v2", "Wed, 24 Aug 2016 01:50:04 GMT  (590kb,D)", "http://arxiv.org/abs/1605.07779v2", "Accepted to NIPS 2016"]], "COMMENTS": "Submitted to NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["taesup moon", "seonwoo min", "byunghan lee", "sungroh yoon"], "accepted": true, "id": "1605.07779"}, "pdf": {"name": "1605.07779.pdf", "metadata": {"source": "CRF", "title": "Neural Universal Discrete Denoiser", "authors": ["Taesup Moon", "Seonwoo Min", "Byunghan Lee", "Sungroh Yoon"], "emails": ["tsmoon@dgist.ac.kr", "mswzeus@gmail.com", "bhannara@gmail.com", "sryoon@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it is a purely reactionary project, which is a purely reactionary project, in which it is a reactionary, reactionary, reactionary, reactionary and reactionary project, in which it is a reactionary, reactionary and reactionary project."}, {"heading": "2 Notations and related work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Problem setting of discrete denoising", "text": "In this paper, we will generally refer to a sequence (n-tuple) as, for example, an = (a1,.., an), and aji refers to the sub-sequence (ai,..., aj). In the discrete denunciation problem, we refer to the clean, underlying source data as xn and assume that each component xi has a value in a finite set X. The source sequence is corrupted by a DMC and results in a loud version of source zn, from which each component zi takes a value in some finite set Z. The DMC is completely characterized by the channel transition matrix of which the (x, z) th element stands for Pr (Zi = z | Xi = x)."}, {"heading": "2.2 Discrete Universal DEnoiser (DUDE)", "text": "DUDE in [22] is a two-pass algorithm, which has a linear complexity in the data size n. During the first pass, the algorithm collects the statistical vector [zn, lk, rk] (a) = [i: k + 1 \u2264 i \u2264 n \u2212 k, zi + ki \u2212 k = lkark}, (1) for all one \"Z,\" which is the number of occurrences of the symbol along the loud sequence \"Z,\" which has the double-sided context (lk, rk). Once the m vector is captured, DUDE applies the string \"i,\" \"DUDE\" (z n) = arg \"min x\" X m \"[zn, ci] > 1 [x] > the same window for the second pass."}, {"heading": "2.3 Deep neural networks (DNN) and related work", "text": "Deep Neural Networks (DNN), often referred to as deep learning algorithms, have recently had a significant impact in several practical applications, such as speech recognition, image recognition and machine translation, etc. For a thorough review of DNN's recent advances, we refer readers to [7] and references to it. In terms of denociation, we have successfully applied DNN to grayscale image denociation by applying supervised learning at the level of small image fields. Namely, they have generated clean and noisy image fields and trained neural networks to learn how to map loud to clean image fields. While such an approach has achieved state-of-the-art performance, as mentioned in Introduction, it has several limitations. That is, it typically requires huge amounts of training data, and multiple copies of the data must be generated for different noise types and levels to achieve robust results."}, {"heading": "3 An alternative interpretation of DUDE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Unbiased estimated loss", "text": "In order to make an alternative interpretation of DUDE, which can also be found in [13], we need the tool developed in [23]. To be self-contained, let us recapitulate the idea here. Let us consider a single-letter case, namely a clean symbol x, which is corrupted by the clean symbol x and has led to the loud observation Z. Let us then assume that a denociant with a single symbol is applied and receives the denoted symbol X = s (Z). In this case, the true loss caused by s for the clean symbol x and the loud observation Z can be derived from the letter X (x, s (Z). It is clear that s cannot evaluate its loss because it does not know what x is, but the following shows an unbiased estimate of the expected true loss based only on Z and s."}, {"heading": "3.2 DUDE: Minimizing the sum of estimated losses", "text": "As mentioned in Section 2.2, DUDE with context size k is the k-th sequence of sliding window denoisers. (Generally, we can call this k-th sequence of sliding window denoisers sk: Z2k + 1 \u2192 X, which represents the reconstruction at i-th location asX + i (z n) = sk (z i + k i \u2212 k) = sk (ci, zi). (4) To remember, ci = (zi \u2212 1i \u2212 k, z i + k i + 1). Now, from the formulation (4), we can interpret that sk defines a single symbol denoiser at location i, i.e., sk (ci, \u00b7), depending on this view of sk, as derived in [13], we can show that the DUDE is defined in (2) is equivalent to searching for a single symbol denoiser at location i, i.e."}, {"heading": "4 Neural DUDE: A DNN based discrete denoiser", "text": "As seen in the previous section, DUDE uses the estimated loss matrix L, which does not depend on the clean sequence xn. However, the main disadvantage of DUDE is that, as seen in (5), it treats each context c independently of others. Namely, if the context size k grows, the number of different contexts will grow exponentially with k, so the sample size for each context will decrease exponentially for a given sequence length n. To solve this problem, we develop the neural DUDE, which assumes a single neural network so that information from similar contexts can be shared via network parameters."}, {"heading": "4.1 A lemma", "text": "s define a cost function C (g, p), \u2212 \u2211 | S | i = 1 gi log pi, i.e. a generalized cross entropy function with the first argument that is not normalized to a probability vector. Note C (g, p) is linear in g and convex in p. Now the following Lemma shows a different path, DUDE.Lemma 1 Define Lnew, \u2212 L + Lmax11 > in the Lmax, maxz, s L (z, s), the maximum element of L. Using the cost function C (\u00b7 \u00b7) defines above, for each c \u00b2 Ck, let us define c \u00b2 C (c), arg \u00b2 c \u00b2 p: c \u00b2 arzip: c \u00b2 c c \u00b2 C (L > new1zi, p).Then we have zi, the definition of c \u00b2 Ck that we define."}, {"heading": "4.2 Neural DUDE", "text": "The main idea of Neural DUDE is to use a single neural network to learn the k-order slider window, which denotes the rule for all c's. Namely, we define p (w, \u00b7): Z2k \u2192 \u2206 | S | as a forward-facing neural network that takes the c-Ck context vector as input and outputs a probability vector to \u2206 | S |. We leave w standing for all parameters in the network. The network architecture of p (w, \u00b7) has the soft maximum output layer, and it corresponds to that used for multi-class classification. So, if the parameters are learned correctly, we expect p (w, ci) to predict which one-symbol denoiser at position i will apply to the context ci."}, {"heading": "4.2.1 Learning", "text": "Inspired by Lemma 1, however, we define the objective function for minimizing learning w asL (w, zn), 1n \u2211 i = 1 C (L > newIzi, p (w, ci)), (10), which is similar to the widespread cross-entropy objective function in the supervised multi-class classification. In contrast to the classification in which the soil-truth label is given as a single vector, we treat L > newIzi ni = 1, which depends exclusively on the loud sequence zn, analogously as input-label pairs in supervised learning. But in contrast to the classification in which the soil-truth label is given as a single vector, we treat L > newIzi \u0432R | S | + as a target \"pseudo-label\" in the sense of \"loss.\""}, {"heading": "4.2.2 Denoising", "text": "It is not as if the objective functioning of the DUDE (11) and the definition (11) s (11) s (11) and the reconstruction at location i of X-i, DUDE (zn) = sk, DUDE (zn) = sk, DUDE (zn) = sk, DUDE (zn) = sk, DUDE (ci, zi).From the objective functioning (10) and the definition (11) it is obvious that Neural DUDE shares information about different contexts, since w-i is learned from all data and shared contexts."}, {"heading": "5 Experimental results", "text": "In this section we show the results of Neural DUDE for synthetic binary data, real binary images and real DNA sequence data from nanopores. All our experiments were performed with Python 2.7 and Keras package (http: / / keras.io) with Theano [1] backend."}, {"heading": "5.1 Synthetic binary data", "text": "We first experimented with a simple synthetic binary sequence to emphasize the core strength of the Neural DUDE. That is, we assume that X = Z = Z = Z = 0, 1} and E = E is a binary symmetric channel (B = 0.1).We advocate the pure binary sequence xn of length n = 106 from a binary symmetric Markov chain (BSMC) with a transition probability \u03b1 = 0.1. The noise-corrupted sequence zn is generated by hamming the sequence xn of length n = 106 from the average loss of a denoiser X [n] s, 1n [S] n \"s, X [S] s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s. \""}, {"heading": "5.2 Real binary image denoising", "text": "In this section we experiment with real binary image data. We tested with 5 representative binary images with different textual characteristics: Einstein, Lena, Barbara, cameraman and scanned Shannon paper. Einstein and Shannon images had a resolution of 256 x 256 and the rest had 512 x 512. For neural DUDE we tested with 4-layer models with 40 hidden nodes with ReLU activations in each layer. Figure 3 (b) shows the result of the denozation of the Einstein image in Figure 3 (a) with a resolution of 0.1. We see that the BER of the neural DUDE (4L) decreases further when we increase k, whereas DUDE is quickly unable to denoze for higher DUD densities. Furthermore, we observe that the estimated BER of the neural DUDE (4L) strongly correlates with the real DUDE."}, {"heading": "5.3 Nanopore DNA sequence denoising", "text": "We now go beyond binary data and apply Neural DUDE to DNA sequences that are directly treated by DUDE. As investigated in [11], the denocialization of DNA sequences is becoming increasingly important as the sequencing devices become cheaper but inject more noise than before. In our experiment, we used simulated MinION nanopore readings that were generated as follows; we obtained 16S rDNA reference sequences for 20 species [3] and randomly generated Noise stem plate readings. The number of read and read lengths for each species was determined to be identical to those of real MinION nanopore readings [3]. Then, based on the results of the nanopore sequencer (Figure 4 (a)), which were obtained in [8] (with 20.375% average error rate), we induced substitution errors in the readings and received corresponding read noises."}, {"heading": "6 Concluding remark and future work", "text": "There are several future research directions. First, we plan to conduct thorough experiments to denoise and quantify the effects of Neural DUDE in downstream analysis. Second, we plan to conduct theoretical analyses of concentration (12) and justify the derived k-selection rule. Third, it would be fruitful to extend the framework to the handling of continuously evaluated signals and to find a link with the SURE principle [20]. Finally, the use of relapsing neural networks (RNN) instead of DNNs could be another promising direction."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "JMLR, 3:1137\u20131155", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Species level resolution of 16S rRNA gene amplicons sequenced through MinIONTM portable nanopore sequencer", "author": ["A. Benitez-Paez", "K. Portune", "Y. Sanz"], "venue": "bioRxiv:021758", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "and S", "author": ["H. Burger", "C. Schuler"], "venue": "Harmeling. Image denoising: Can plain neural networks compete with BM3D? In CVPR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, 12:2121\u20132159", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Oxford nanopore sequencing", "author": ["S. Goodwin", "J. Gurtowski", "S Ethe-Sayers", "P. Deshpande", "M. Schatz", "W.R. McCombie"], "venue": "hybrid error correction, and de novo assembly of a eukaryotic genome. Genome Res.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning", "author": ["G. Hinton", "Y. LeCun", "Y. Bengio"], "venue": "Nature, 521:436\u2013444", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved data analysis for the minion nanopore sequencer", "author": ["M. Jain", "I. Fiddes", "K. Miga", "H. Olsen", "B. Paten", "M. Akeson"], "venue": "Nature Methods, 12:351\u2013356", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural image denoising with convolutional networks", "author": ["V. Jain", "H.S. Seung"], "venue": "NIPS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Denoising DNA deep sequencing data\u2013 high-throughput sequencing errors and their corrections", "author": ["D. Laehnemann", "A. Borkhardt", "A.C. McHardy"], "venue": "Brief Bioinform, 17(1):154\u2013179", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "DUDE-Seq: Fast", "author": ["B. Lee", "T. Moon", "S. Yoon", "T. Weissman"], "venue": "flexible, and robust denoising of nucleotide sequences. arXiv:1511.04836", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Discrete denoising with shifts", "author": ["T. Moon", "T. Weissman"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "The iDUDE framework for grayscale image denoising", "author": ["Giovanni Motta", "Erik Ordentlich", "Ignacio Ramirez", "Gadiel Seroussi", "Marcelo J. Weinberger"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A method of solving a convex programming problem with convergence rate o(1/sqr(k))", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady, 27:372\u2013376", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1983}, {"title": "Universal algorithms for channel decoding of uncompressed sources", "author": ["E. Ordentlich", "G. Seroussi", "S. Verd\u00fa", "K. Viswanathan"], "venue": "IEEE Trans. Information Theory, 54(5):2243\u20132262", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "A universal discrete image denoiser and its application to binary images", "author": ["E. Ordentlich", "G. Seroussi", "S. Verd\u00fa", "M.J. Weinberger", "T. Weissman"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B.T Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics, 4(5):1\u201317", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1964}, {"title": "Correcting errors in short reads by multiple alignments", "author": ["L. Salmela", "J. Schroder"], "venue": "BioInformatics, 27(11):1455\u20131461", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimation of the mean of a multivariate normal distribution", "author": ["C. Stein"], "venue": "The Annals of Statistics, 9(6):1135\u20131151", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1981}, {"title": "RMSProp: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "G. Hinton"], "venue": "Technical report, University of Toronto,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Universal discrete denoising: Known channel", "author": ["T. Weissman", "E. Ordentlich", "G. Seroussi", "S. Verd\u00fa", "M.J. Weinberger"], "venue": "IEEE Transactions on Information Theory, 51(1):5\u201328", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Universal filtering via prediction", "author": ["T. Weissman", "E. Ordentlich", "M. Weinberger", "A. Somekh-Baruch", "N. Merhav"], "venue": "IEEE Trans. Inform. Theory, 53(4):1253\u20131264", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["J. Xie", "L. Xu", "E. Chen"], "venue": "Image Denoising and Inpainting with Deep Neural Networks, NIPS", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Adadelta: An adaptive learning rate method", "author": ["M. Zeiler"], "venue": "arXiv:1212.5701", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "Such setting covers several applications in different domains, such as image denoising [17, 14], DNA sequence denoising[12], and channel decoding[16].", "startOffset": 87, "endOffset": 95}, {"referenceID": 13, "context": "Such setting covers several applications in different domains, such as image denoising [17, 14], DNA sequence denoising[12], and channel decoding[16].", "startOffset": 87, "endOffset": 95}, {"referenceID": 11, "context": "Such setting covers several applications in different domains, such as image denoising [17, 14], DNA sequence denoising[12], and channel decoding[16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "Such setting covers several applications in different domains, such as image denoising [17, 14], DNA sequence denoising[12], and channel decoding[16].", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "In order to alleviate the above mentioned limitations, [22] proposed a universal approach for discrete denoising.", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": ", [17, 12, 16, 14].", "startOffset": 2, "endOffset": 18}, {"referenceID": 11, "context": ", [17, 12, 16, 14].", "startOffset": 2, "endOffset": 18}, {"referenceID": 15, "context": ", [17, 12, 16, 14].", "startOffset": 2, "endOffset": 18}, {"referenceID": 13, "context": ", [17, 12, 16, 14].", "startOffset": 2, "endOffset": 18}, {"referenceID": 6, "context": "In this paper, we present a novel framework of addressing above limitations of DUDE by adopting the machineries of deep neural networks (DNN) [7], which recently have seen great empirical success in many practical applications.", "startOffset": 142, "endOffset": 145}, {"referenceID": 3, "context": "While there have been some previous attempts of applying neural networks to grayscale image denoising [4, 24], they all remained in supervised learning setting, i.", "startOffset": 102, "endOffset": 109}, {"referenceID": 23, "context": "While there have been some previous attempts of applying neural networks to grayscale image denoising [4, 24], they all remained in supervised learning setting, i.", "startOffset": 102, "endOffset": 109}, {"referenceID": 10, "context": ", DNA sequence denoising [11].", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "DUDE in [22] is a two-pass algorithm that has a linear complexity in the data size n.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "For more rigorous analyses, we refer to the original paper [22].", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "For a thorough review on recent progresses of DNN, we refer the readers to [7] and refereces therein.", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "Regarding denoising, [4, 24, 9] have successfully applied the DNN to grayscale image denoising by utilizing supervised learning at the small image patch level.", "startOffset": 21, "endOffset": 31}, {"referenceID": 23, "context": "Regarding denoising, [4, 24, 9] have successfully applied the DNN to grayscale image denoising by utilizing supervised learning at the small image patch level.", "startOffset": 21, "endOffset": 31}, {"referenceID": 8, "context": "Regarding denoising, [4, 24, 9] have successfully applied the DNN to grayscale image denoising by utilizing supervised learning at the small image patch level.", "startOffset": 21, "endOffset": 31}, {"referenceID": 12, "context": "In order to make an alternative interpretation of DUDE, which can be also found in [13], we need the tool developed in [23].", "startOffset": 83, "endOffset": 87}, {"referenceID": 22, "context": "In order to make an alternative interpretation of DUDE, which can be also found in [13], we need the tool developed in [23].", "startOffset": 119, "endOffset": 123}, {"referenceID": 22, "context": "With this definition, we can show that L(Z, s) is an unbiased estimate of Ex\u039b(x, s(Z)) as follows (as shown in [23]): ExL(Z, s) = \u2211", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "With this view on sk, as derived in [13], we can show that the DUDE defined in (2) is equivalent to finding a single-symbol denoiser sk,DUDE(c, \u00b7) = arg min s\u2208S \u2211", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "The interpretation (5) gives some intuition on why DUDE enjoys strong theoretical guarantees in [22]; since L(Zi, s) is an unbiased estimate of Exi\u039b(xi, s(Zi)), \u2211 i\u2208{i:ci=c} L(Zi, s) will concentrate on \u2211 i\u2208{i:ci=c} \u039b(xi, s(Zi)) as long as |{i : ci = c}| is sufficiently large.", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "We note that our usage of DNN resembles that of the neural language model (NLM) [2], which improved upon the conventional N -gram models.", "startOffset": 80, "endOffset": 83}, {"referenceID": 14, "context": "In fact, most of the well-known improvements to the SGD method, such as the momentum [15, 18], mini-batch SGD, and several others [21, 10, 5, 25], can be all used for learning w.", "startOffset": 85, "endOffset": 93}, {"referenceID": 17, "context": "In fact, most of the well-known improvements to the SGD method, such as the momentum [15, 18], mini-batch SGD, and several others [21, 10, 5, 25], can be all used for learning w.", "startOffset": 85, "endOffset": 93}, {"referenceID": 20, "context": "In fact, most of the well-known improvements to the SGD method, such as the momentum [15, 18], mini-batch SGD, and several others [21, 10, 5, 25], can be all used for learning w.", "startOffset": 130, "endOffset": 145}, {"referenceID": 9, "context": "In fact, most of the well-known improvements to the SGD method, such as the momentum [15, 18], mini-batch SGD, and several others [21, 10, 5, 25], can be all used for learning w.", "startOffset": 130, "endOffset": 145}, {"referenceID": 4, "context": "In fact, most of the well-known improvements to the SGD method, such as the momentum [15, 18], mini-batch SGD, and several others [21, 10, 5, 25], can be all used for learning w.", "startOffset": 130, "endOffset": 145}, {"referenceID": 24, "context": "In fact, most of the well-known improvements to the SGD method, such as the momentum [15, 18], mini-batch SGD, and several others [21, 10, 5, 25], can be all used for learning w.", "startOffset": 130, "endOffset": 145}, {"referenceID": 0, "context": "io) with Theano [1] backend.", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": "We used Adam [10] with default setting in Keras as an optimizer to minimize (10).", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "We believe this is a significant result since DUDE is shown to outperform many state-of-the-art sliding window denoisers in practice such as median filters [22, 17].", "startOffset": 156, "endOffset": 164}, {"referenceID": 16, "context": "We believe this is a significant result since DUDE is shown to outperform many state-of-the-art sliding window denoisers in practice such as median filters [22, 17].", "startOffset": 156, "endOffset": 164}, {"referenceID": 13, "context": "Furthermore, folloiwng DUDE\u2019s extension to grayscale image denoising [14], the result gives strong motivation for extending Neural DUDE to grayscale image denoising.", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "As surveyed in [11], denoising DNA sequences are becoming increasingly important as the sequencing devices are getting cheaper, but injecting more noise than before.", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "For our experiment, we used simulated MinION nanopore reads, which were generated as follows; we obtained 16S rDNA reference sequences for 20 species [3] and randomly generated noiseless", "startOffset": 150, "endOffset": 153}, {"referenceID": 2, "context": "The number of reads and read length for each species were set as identical to those of real MinION nanopore reads [3].", "startOffset": 114, "endOffset": 117}, {"referenceID": 7, "context": "Then, based on \u03a0 of nanopore sequencer (Figure 4(a)) obtained in [8] (with 20.", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": ", nanocorr [6], did not produce read-by-read correction sequence, but returns downstream analyses results after denoising.", "startOffset": 11, "endOffset": 14}, {"referenceID": 18, "context": "Coral [19], which gives read-by-read denoising result for Illumina data, completely failed for nanopore data.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "Given that DUDE ourperforms state-of-the-art schemes, including Coral, for Illumina sequenced data as shown in [12], we expect the improvement of Neural DUDE over DUDE could translate into fruitful downstream analyses gain for nanopore data.", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "Third, extending the framework to deal with continuous-valued signal and finding connection with SURE principle [20] would be fruitful.", "startOffset": 112, "endOffset": 116}], "year": 2016, "abstractText": "We present a new framework of applying deep neural networks (DNN) to devise a universal discrete denoiser. Unlike other approaches that utilize supervised learning for denoising, we do not require any additional training data. In such setting, while the ground-truth label, i.e., the clean data, is not available, we devise \u201cpseudolabels\u201d and a novel objective function such that DNN can be trained in a same way as supervised learning to become a discrete denoiser. We experimentally show that our resulting algorithm, dubbed as Neural DUDE, significantly outperforms the previous state-of-the-art in several applications with a systematic rule of choosing the hyperparameter, which is an attractive feature in practice.", "creator": "LaTeX with hyperref package"}}}