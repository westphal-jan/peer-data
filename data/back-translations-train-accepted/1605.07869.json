{"id": "1605.07869", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Variational Neural Machine Translation", "abstract": "Models of neural machine translation are often from a discriminative family of encoder-decoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoder-decoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform an efficient posterior inference, we build a neural posterior approximator that is conditioned only on the source side. Additionally, we employ a reparameterization technique to estimate the variational lower bound so as to enable standard stochastic gradient optimization and large-scale training for the variational model. Experiments on NIST Chinese-English translation tasks show that the proposed variational neural machine translation achieves significant improvements over both state-of-the-art statistical and neural machine translation baselines.", "histories": [["v1", "Wed, 25 May 2016 13:18:57 GMT  (239kb,D)", "http://arxiv.org/abs/1605.07869v1", "10 pages"], ["v2", "Sun, 25 Sep 2016 23:37:14 GMT  (206kb,D)", "http://arxiv.org/abs/1605.07869v2", "10 pages, accepted at emnlp 2016"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["biao zhang", "deyi xiong", "jinsong su", "hong duan", "min zhang"], "accepted": true, "id": "1605.07869"}, "pdf": {"name": "1605.07869.pdf", "metadata": {"source": "CRF", "title": "Variational Neural Machine Translation", "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su"], "emails": ["zb@stu.xmu.edu.cn,", "jssu@xmu.edu.cn", "dyxiong@suda.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "It is a question of whether and to what extent it is a question of a way in which the people of the individual countries follow the rules. (...) It is a question of whether the people of the individual countries are bound by the rules of the Basic Law. (...) It is a question of the way in which they obey the rules. (...) It is a question of the way in which they obey the rules. (...) It is a question of the way in which they obey the rules of the Basic Law. (...) It is a question of the way in which they obey obediently. (...) It is a question of the way in which they obey the rules of the Basic Law. (...) It is a question of the way in which they obey obediently obey. (...) It is a question of the way in which they obey. (...) It is a question of the way in which they obey themselves obey. (...) It is a question of the way in which they obey. (...) It is a question of the way in which they obey themselves obey the rules. (...) It is a question of the way in which they obey themselves obey the rules of the Basic Law. (...) It is a question of the way in which they obey the way in which they obey themselves obey the Basic Law. (...) It is a question of the way in which they obey the way in which they obey themselves obey the Basic Law. (...) It is a question of the way in which they obey the way in which they obey themselves obey the Basic Law. (...) It is a question of the way in which they obey the way in which they obey themselves obey the rules of the Basic Law. (...) It is a question of the way in which they obey the way in which they obey the way in which they obey the Basic Law. (...) It is a question of the way in which they obey the way in which they obey the way in which they obey the Basic Law. (... It is obey the way in which they obey the way in which they obey the way in the way in which they obey the Basic Law. (...) The way in which they obe"}, {"heading": "2 Background: Variational Autoencoder", "text": "In this section we briefly consider the variable autoencoder (VAE) (Kingma and Welling, 2014; Rezende et al., 2014), one of the most classic variable neural models. Another reason to introduce the variable autoencoder in this paper is to ensure the integrity of the proposed variational NMT, since it also belongs to the family of encoder decoders. Faced with an observed variable x, VAE introduces a continuous latent variable z and assumes that x is generated from z, i.e., p\u03b8 (x, z) = p\u03b8 (x) p\u03b8 (z) p\u03b8 x (2), where the parameter of the model is designated, p\u03b8 (z) is the previous property (x | z) the conditional distribution that models the generational procedure. Typically, p\u03b8 (z) is treated as a simple Gaussian distribution, and deep non-linear neural networks are used to perform the generation."}, {"heading": "3 Variational Neural Machine Translation", "text": "In contrast to previous work, we introduce a latent variable z to model the underlying semantic space. Formally, the lower limit of VNMT can be formulated as follows: LVNMT (\u03b8, \u03c6; x, y) = \u2212 KL (q\u03c6 (z | x) | | p\u03b8 (z)) + Eq\u03c6 (z | x) [log p\u03b8 (y | z, x)] (5), where q\u03c6 (z | x) is our posterior approximator, and p\u03b8 (y | z, x) the decoder with the instruction of e.g. Based on this formulation, VNMT can be divided into three components, each of which is modelled by a neural network: a variable neural approximator that forms the models q\u03c6 (z | x) (see part (b) in Figure 2), a variational neural decoder that pairs (part | z) or a source (see Figure)."}, {"heading": "3.1 Variational Neural Encoder", "text": "As shown in Figure 2 (a), the variational neural encoder aims to encode an input set (x1, x2,.., xTf) into a continuous vector. In this thesis, we adopt the encoder architecture proposed by Bahdanau et al. (2014), which is a bidirectional RNN consisting of a forward RNN and a backward RNN. The forward RNN reads the source set from left to right, while the backward RNN is in the opposite direction (see the parallel arrows in Figure 2 (a): \u2212 \u2192 h i = RNN (\u2212 \u2192 h i \u2212 1, Exi) (6). \u2212 h i = RNN (\u2190 h \u2212 h i + 1, Exi) (7), where Exi-Rdw is the dw-dimensional embedding for the source word xi, and \u2212 h \u2192 h \u2212 dimensional states are generated in two directions."}, {"heading": "3.2 Variational Neural Approximator", "text": "Since the posterior inference model p (z | x) is in most cases insoluble, we adopt an approximation method to simplify the posterior inference. Conventional models usually use the interpretation approaches. However, a major limitation of this approach is its inability to grasp the true posterior element of z due to its over-simplification. Following the spirit of UAE, we use neural networks for better approximation in this paper. Similar to previous work (Kingma and Welling, 2014; Rezende et al., 2014), we have q\u03c6 (z | x) perform a multivariate Gaussian distribution with a diagonal covariance structure."}, {"heading": "3.3 Variational Neural Decoder", "text": "Considering the source set x and the latent variables z, our decoder defines the probability of the translation y as the common probability of the ordered conditions: p (y | z, x) = Te, j = 1 p (yj | y < j, z, x) (14), where p (yj | y < j, z, x) = g \"(yj \u2212 1, sj, cj) is the feed model g\" (\u00b7) (see the yellow arrows in Figure 2) and the context vector cj = \"i\" (see Figure 2) are the same as (Bahdanau et al., 2014). The difference between our decoder and Bahdanau et al. \"s decoder (2014) is that in addition to the context vector, our decoder the decoder represents the latent variables, i.e. he, in the compilation of sj."}, {"heading": "3.4 Model Training", "text": "As shown in Eq. (5), in order to optimize our model, we must calculate an expectation about the approximate posterior, that is, Eq\u03c6 (z | x) [\u00b7], which is in turn insoluble. Following UAE, we omit the term bias for clarity. 3Note that we do not include the latent representation in the calculation of the original hidden state, because we find that the model may suffer from the noise it causes in our preliminary experiments.Algorithm 1 VNMT training algorithm. Inputs: A, the maximum number of iterations; M, the number of instances in a stack; L, the number of samples used; Medicare, the initial parameters repeat themselves D; getRandomMiniBatch (M) training algorithm GaussianNoise (): The maximum number of iterations; M, the number of instances in a stack; L, the number of samples used; the starting parameters; Medicare."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "In order to evaluate the effectiveness of the proposed VNMT system, we conducted experiments with the NIST-Chinese-English translation tasks. Our trainingdata4 consists of 2.9M pairs of sentences, with 80.9M Chinese words and 86.4M English words. We used the 2005 NIST data sets as our development set and the 2002, 2003, 2006, 2008 NIST data sets as our test sets. We used the case-insensitive BLEU-4 metrics (Papineni et al., 2002) to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for signature tests. We compared our model with two state-of-the-art SMT and NMT systems: \u2022 Moses (Koehn et al., 2007): the conventional sentence-based SMT sampling system. \u2022 GroundHootstrap sampling sampling 2004 sampling for sampling."}, {"heading": "4.2 Translation Results", "text": "Table 1 summarizes the BLEU values of different systems for Sino-English translation tasks. No matter what dimensionality we set for dz, VNMT improves the translation quality with respect to BLEU for all test sentences. In particular, VNMTx achieves the best average results for dz = 4000, reaching 0.71 and 1.20 BLEU points, respectively, over Moses and GroundHog. This indicates that explicit modeling of the underlying semantics by a latent variable is actually advantageous for the translation of neural machines. In terms of the dimensionality dz of the latent variables, we do not observe consistent improvements for each test sentence as dz increases. This could be because our test sentences show different data6https: / / github.com / lisa-GroundHog / GroundHog distributions. However, the consistent improvements in the average results may indicate that a relatively greater value for these two groups of quellz we share the redundz between the two."}, {"heading": "4.3 Translation Analysis", "text": "Table 2 shows a translation example that contributes to the advantage of VNMT over NMT.7 Since the source sentence in this example is long (more than 40 words), the translation generated by Moses is relatively chaotic and incomprehensible. In contrast, translations generated by neural models (both GroundHog and VNMT) are much more fluid and comprehensible. However, there are significant differences between GroundHog and our VNMT. In particular, GroundHog does not translate the sentence at the beginning of the source sentence. The translation of the sentence... \""}, {"heading": "5 Related Work", "text": "There are approximately two areas of research related to our work: Neural Machine Translation and Variational Neural Model. We describe them one after the other."}, {"heading": "5.1 Neural Machine Translation", "text": "Kalchbrenner and Blunsom (2013) use an evolutionary neural network to encode source sentences, and then use a recursive neural network (RNN) to generate target translations. Cho et al. (2014) also propose the framework of encoder decoders with two RNNs. However, the work mentioned above mainly focuses on calculating the score / probability of bilingual phrases, rather than entire translations. With respect to NMT, Sutskever et al. (2014) employs two multi-layered Long Term Memory (LSTM) models that first encode a source sentence into a single vector and then decode the translation word by word until a specific end term is generated."}, {"heading": "5.2 Variational Neural Model", "text": "In order to perform efficient conclusions and learning processes in directed probability models on large-scale datasets, Kingma and Welling (2014) and Rezende et al. (2014) introduce variational neural networks. Typically, these models use a neural inference model to approximate the intractable posterior and optimize model parameters along with a repaired lower limit of variation.This approach is of growing interest due to its success in various tasks. In this regard, Kingma et al. (2014) are reviewing the approach of semi-supervised learning with generative models and developing new models that allow effective generalization from a small, labeled dataset to a large, unlabeled dataset. Chung et al. (2015) incorporate latent variables into the hidden state of a recursive neural network, while the translation mechanisms of the human eyes related to a variable framework are variable."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we presented a variation model for neural machine translation, which contains a continuous latent variable for modelling the underlying semantics of sentence pairs. We approximate posterior distribution to neural networks and repair the variational lower boundary. This allows our model to be an end-to-end neural network that can be optimized by conventional stochastic gradient algorithms. Compared to conventional attention-based NMT, our model is better at translating long sentences. It also benefits greatly from a special regulatory concept associated with this latent variable. Experiments with Sino-English translation tasks confirmed the effectiveness of our model. Since the latents in our model are at the sentence level, we intend to investigate finer-grained latent latent variables for neural machine translation in the future, such as the Recurrent Latent Variable (Chung al, 2015)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proc. of ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Generating Sentences from a Continuous Space", "author": ["Bowman et al.2015] S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Agreementbased Joint Training for Bidirectional Attentionbased Neural Machine Translation", "author": ["Cheng et al.2015] Y. Cheng", "S. Shen", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A recurrent latent variable model for sequential data", "author": ["Chung et al.2015] Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C. Courville", "Yoshua Bengio"], "venue": "In Proc. of NIPS", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Character-based Neural Machine Translation", "author": ["Costa-Juss\u00e0", "Fonollosa2016] M.R. Costa-Juss\u00e0", "J.A.R. Fonollosa"], "venue": null, "citeRegEx": "Costa.Juss\u00e0 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Costa.Juss\u00e0 et al\\.", "year": 2016}, {"title": "Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder", "author": ["S. Feng", "S. Liu", "M. Li", "M. Zhou"], "venue": "NMT Model. ArXiv e-prints, January", "citeRegEx": "Feng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "DRAW: A recurrent neural network for image generation. CoRR, abs/1502.04623", "author": ["Gregor et al.2015] Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": null, "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "On Using Monolingual Corpora in Neural Machine Translation", "author": ["Gulcehre et al.2015] C. Gulcehre", "O. Firat", "K. Xu", "K. Cho", "L. Barrault", "H.-C. Lin", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "Gulcehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean et al.2015] S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proc. of ACL-IJCNLP,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Auto-Encoding Variational Bayes", "author": ["Kingma", "Welling2014] Diederik P Kingma", "Max Welling"], "venue": "In Proc. of ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In Proc. of NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Factored translation models", "author": ["Koehn", "Hoang2007] Philipp Koehn", "Hieu Hoang"], "venue": "In Proc. of EMNLP-CoNLL,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn"], "venue": "In Proc. of EMNLP", "citeRegEx": "Koehn.,? \\Q2004\\E", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Character-based Neural Machine Translation", "author": ["Ling et al.2015] W. Ling", "I. Trancoso", "C. Dyer", "A. W Black"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015a] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong et al.2015b] Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proc. of ACL-IJCNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A Deep Memory-based Architecture for Sequence-to-Sequence Learning", "author": ["Meng et al.2015] F. Meng", "Z. Lu", "Z. Tu", "H. Li", "Q. Liu"], "venue": null, "citeRegEx": "Meng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "Neural Variational Inference for Text Processing", "author": ["Miao et al.2015] Y. Miao", "L. Yu", "P. Blunsom"], "venue": null, "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proc. of ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Shakir Mohamed", "Daan Wierstra"], "venue": "In Proc. of ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Improving Neural Machine Translation Models with Monolingual Data", "author": ["B. Haddow", "A. Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["B. Haddow", "A. Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Minimum Risk Training for Neural Machine Translation", "author": ["Shen et al.2015] S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. CoRR, abs/1409.3215", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Coverage-based neural machine translation. CoRR, abs/1601.04811", "author": ["Tu et al.2016] Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": null, "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).", "startOffset": 155, "endOffset": 368}, {"referenceID": 25, "context": "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).", "startOffset": 155, "endOffset": 368}, {"referenceID": 0, "context": "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).", "startOffset": 155, "endOffset": 368}, {"referenceID": 9, "context": "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).", "startOffset": 155, "endOffset": 368}, {"referenceID": 24, "context": "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).", "startOffset": 155, "endOffset": 368}, {"referenceID": 18, "context": "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).", "startOffset": 155, "endOffset": 368}, {"referenceID": 26, "context": "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).", "startOffset": 155, "endOffset": 368}, {"referenceID": 3, "context": "Most NMT models take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into a distributed representation, and a neural decoder generates the corresponding target sentence y according to the distributed representation1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 264, "endOffset": 329}, {"referenceID": 25, "context": "Most NMT models take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into a distributed representation, and a neural decoder generates the corresponding target sentence y according to the distributed representation1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 264, "endOffset": 329}, {"referenceID": 0, "context": "Most NMT models take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into a distributed representation, and a neural decoder generates the corresponding target sentence y according to the distributed representation1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 264, "endOffset": 329}, {"referenceID": 21, "context": "In order to address these issues, we propose a variational encoder-decoder model to neural machine translation (VNMT), motivated by the recent success of variational neural models (Rezende et al., 2014; Kingma and Welling, 2014).", "startOffset": 180, "endOffset": 228}, {"referenceID": 21, "context": "With respect to efficient learning, we apply a reparameterization technique (Rezende et al., 2014; Kingma and Welling, 2014) on the variational lower bound.", "startOffset": 76, "endOffset": 124}, {"referenceID": 0, "context": "\u2022 A variational neural encoder transforms source sentence into a distributed representation, which is the same as the encoder of NMT (Bahdanau et al., 2014) (see section 3.", "startOffset": 133, "endOffset": 156}, {"referenceID": 21, "context": "In this section, we briefly reviews the variational autoencoder (VAE) (Kingma and Welling, 2014; Rezende et al., 2014), one of the most classical variational neural models.", "startOffset": 70, "endOffset": 118}, {"referenceID": 0, "context": "In this paper, we adopt the encoder architecture proposed by Bahdanau et al. (2014), which is a bidirectional RNN that consists of a forward RNN and backward RNN.", "startOffset": 61, "endOffset": 84}, {"referenceID": 0, "context": "Following Bahdanau et al. (2014), we employ the Gated Recurrent Unit (GRU) as our RNN unit due to its capacity in capturing longdistance dependencies.", "startOffset": 10, "endOffset": 33}, {"referenceID": 21, "context": "Similar to previous work (Kingma and Welling, 2014; Rezende et al., 2014), we let q\u03c6(z|x) be a multivariate Gaussian distribution with a diagonal covariance structure:", "startOffset": 25, "endOffset": 73}, {"referenceID": 0, "context": "The feed forward model g\u2032(\u00b7) (see the yellow arrows in Figure 2) and context vector cj = \u2211 i \u03b1jihi (see the \u201c\u2295\u201d in Figure 2) are the same as (Bahdanau et al., 2014).", "startOffset": 141, "endOffset": 164}, {"referenceID": 0, "context": "The feed forward model g\u2032(\u00b7) (see the yellow arrows in Figure 2) and context vector cj = \u2211 i \u03b1jihi (see the \u201c\u2295\u201d in Figure 2) are the same as (Bahdanau et al., 2014). The difference between our decoder and Bahdanau et al.\u2019s decoder (2014) lies in that in addition to the context vector, our decoder integrates the representation of the latent variable, i.", "startOffset": 142, "endOffset": 238}, {"referenceID": 0, "context": "The initial hidden state s0 is initialized in the same way by Bahdanau et al. (2014) (see the arrow to s0 in Figure 2).", "startOffset": 62, "endOffset": 85}, {"referenceID": 20, "context": "We employed the case-insensitive BLEU-4 metric (Papineni et al., 2002) to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test.", "startOffset": 47, "endOffset": 70}, {"referenceID": 14, "context": ", 2002) to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test.", "startOffset": 71, "endOffset": 84}, {"referenceID": 0, "context": "\u2022 GroundHog (Bahdanau et al., 2014): the attentional NMT system.", "startOffset": 12, "endOffset": 35}, {"referenceID": 0, "context": "Following Bahdanau et al. (2014), we set dw = 620, df = 1000, de = 1000, and M = 80.", "startOffset": 10, "endOffset": 33}, {"referenceID": 3, "context": "Cho et al. (2014) further propose the Encoder-Decoder framework with two RNNs.", "startOffset": 0, "endOffset": 18}, {"referenceID": 24, "context": "With regard to NMT, Sutskever et al. (2014) employ two multilayered Long Short-Term Memory (LSTM) models that first encode a source sentence into a single vector and then decode the translation word by word until a special end token is generated.", "startOffset": 20, "endOffset": 44}, {"referenceID": 0, "context": "In order to deal with issues caused by encoding all source-side information into a fixed-length vector, Bahdanau et al. (2014) introduce attentionbased NMT that aims at automatically concentrating on relevant source parts for predicting target words during decoding.", "startOffset": 104, "endOffset": 127}, {"referenceID": 9, "context": "Following the success of attentional NMT, a number of approaches and models have been proposed for NMT recently, which can be grouped into different categories according to their motivations: dealing with rare words or large vocabulary (Jean et al., 2015; Luong et al., 2015b; Sennrich et al., 2015b), learning better attentional structures (Luong et al.", "startOffset": 236, "endOffset": 300}, {"referenceID": 2, "context": ", 2015a), integrating SMT techniques (Cheng et al., 2015; Shen et al., 2015; Feng et al., 2016; Tu et al., 2016), character-level NMT (Ling et al.", "startOffset": 37, "endOffset": 112}, {"referenceID": 24, "context": ", 2015a), integrating SMT techniques (Cheng et al., 2015; Shen et al., 2015; Feng et al., 2016; Tu et al., 2016), character-level NMT (Ling et al.", "startOffset": 37, "endOffset": 112}, {"referenceID": 6, "context": ", 2015a), integrating SMT techniques (Cheng et al., 2015; Shen et al., 2015; Feng et al., 2016; Tu et al., 2016), character-level NMT (Ling et al.", "startOffset": 37, "endOffset": 112}, {"referenceID": 26, "context": ", 2015a), integrating SMT techniques (Cheng et al., 2015; Shen et al., 2015; Feng et al., 2016; Tu et al., 2016), character-level NMT (Ling et al.", "startOffset": 37, "endOffset": 112}, {"referenceID": 15, "context": ", 2016), character-level NMT (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016), the exploitation of monolingual corpora (Gulcehre et al.", "startOffset": 29, "endOffset": 81}, {"referenceID": 8, "context": ", 2015; Costa-Juss\u00e0 and Fonollosa, 2016), the exploitation of monolingual corpora (Gulcehre et al., 2015; Sennrich et al., 2015a) and memory network (Meng et al.", "startOffset": 82, "endOffset": 129}, {"referenceID": 18, "context": ", 2015a) and memory network (Meng et al., 2015).", "startOffset": 28, "endOffset": 47}, {"referenceID": 15, "context": "In order to perform efficient inference and learning in directed probabilistic models on large-scale dataset, Kingma and Welling (2014) as well as Rezende et al. (2014) introduce variational neural networks.", "startOffset": 147, "endOffset": 169}, {"referenceID": 8, "context": "In this respect, Kingma et al. (2014) revisit the approach to semi-supervised learning with generative models and further develop new models that allow effective generalization from a small labeled dataset to a large unlabeled dataset.", "startOffset": 17, "endOffset": 38}, {"referenceID": 3, "context": "Chung et al. (2015) incorporate latent variables into the hidden state of a recurrent neural network, while Gregor et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Chung et al. (2015) incorporate latent variables into the hidden state of a recurrent neural network, while Gregor et al. (2015) combine a novel spatial attention mechanism that mimics the foveation of human eyes, with a sequential variational auto-encoding framework that allows the iterative construction of complex images.", "startOffset": 0, "endOffset": 129}, {"referenceID": 3, "context": "Chung et al. (2015) incorporate latent variables into the hidden state of a recurrent neural network, while Gregor et al. (2015) combine a novel spatial attention mechanism that mimics the foveation of human eyes, with a sequential variational auto-encoding framework that allows the iterative construction of complex images. Very recently, Miao et al. (2015) propose a generic variational inference framework for generative and conditional models of text.", "startOffset": 0, "endOffset": 360}, {"referenceID": 1, "context": "The most related work to ours is that of Bowman et al. (2015), where they develop a variational autoencoder for unsupervised generative language modeling.", "startOffset": 41, "endOffset": 62}, {"referenceID": 4, "context": "In the future, since the latent variable in our model is at the sentence level, we want to explore more fine-grained latent variables for neural machine translation, such as the Recurrent Latent Variable Model (Chung et al., 2015).", "startOffset": 210, "endOffset": 230}], "year": 2017, "abstractText": "Models of neural machine translation are often from a discriminative family of encoder-decoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoder-decoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform an efficient posterior inference, we build a neural posterior approximator that is conditioned only on the source side. Additionally, we employ a reparameterization technique to estimate the variational lower bound so as to enable standard stochastic gradient optimization and large-scale training for the variational model. Experiments on NIST Chinese-English translation tasks show that the proposed variational neural machine translation achieves significant improvements over both stateof-the-art statistical and neural machine translation baselines.", "creator": "LaTeX with hyperref package"}}}