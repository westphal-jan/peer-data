{"id": "1605.07874", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "BattRAE: Bidimensional Attention-Based Recursive Autoencoders for Learning Bilingual Phrase Embeddings", "abstract": "In this paper, we propose a bidimensional attention based recursive autoencoder (BattRAE) to integrate cues and source-target interactions at multiple levels of granularity into bilingual phrase representations. We employ recursive autoencoders to generate tree structures of phrase with embeddings at different levels of granularity (e.g., words, sub-phrases, phrases). Over these embeddings on the source and target side, we introduce a bidimensional attention network to learn their interactions encoded in a bidimensional attention matrix, from which we extract two soft attention weight distributions simultaneously. The weight distributions enable BattRAE to generate compositive phrase representations via convolution. Based on the learned phrase representations, we further use a bilinear neural model, trained via a max-margin method, to measure bilingual semantic similarity. In order to evaluate the effectiveness of BattRAE, we incorporate this semantic similarity as an additional feature into a state-of-the-art SMT system. Extensive experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.82 BLEU points over the baseline.", "histories": [["v1", "Wed, 25 May 2016 13:29:07 GMT  (206kb,D)", "https://arxiv.org/abs/1605.07874v1", "9 pages"], ["v2", "Fri, 25 Nov 2016 03:26:45 GMT  (180kb,D)", "http://arxiv.org/abs/1605.07874v2", "7 pages, accepted by AAAI 2017"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["biao zhang", "deyi xiong", "jinsong su"], "accepted": true, "id": "1605.07874"}, "pdf": {"name": "1605.07874.pdf", "metadata": {"source": "CRF", "title": "BattRAE: Bidimensional Attention-Based Recursive Autoencoders for Learning Bilingual Phrase Embeddings", "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su"], "emails": ["zb@stu.xmu.edu.cn,", "dyxiong@suda.edu.cn,", "jssu@xmu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 Learning Embeddings at Different Levels of Granularity", "text": "We use recursive autoencoders (RAE) to learn initial embeddings at different levels of granularity for our model. By combining two child vectors recursively from bottom to top, RAE is able to generate low-dimensional vector representations for variable sequences. The recursion procedure usually consists of two neonal operations: composition and reconstruction. Input to RAE is a list of ordered words in a phrase (x1, x2, x3), each of which is embedded in a d-dimensional continuous vector system. In each reconstruction, RAE selects two adjacent children (e.g. c1 = x2 = x2), and then composes them into a parent role embedded in a parent role."}, {"heading": "3 Bidimensional Attention-Based Recursive Autoencoders", "text": "In this section, we present the proposed BattRAE model. First, we develop the bidimensional attention network, then the semantic similarity model based on phrase embedding we learned with the attention network. Finally, we present the objective function and the training procedure."}, {"heading": "3.1 Bidimensional Attention Network", "text": "In fact, we are in a position to move in a direction in which we are in a position in which we are in."}, {"heading": "3.2 Semantic Similarity", "text": "To measure the semantic similarity of a bilingual phrase, we first transform the learned bilingual phrases ps and pt into a common semantic space using a nonlinear projection as follows: ss = f (W (5) ps + b s) (9) st = f (W (6) pt + b s) (10), with W (5), D (6), D (Rdsem) and B (Rdsem) being the parameters. Similar to the transformation into equivalents (4) and (5), we share the same biased notion of ss and st.We then use a bilingual model to calculate the semantic similarity value as follows: s (f, e) = sTs Sst (11), where f and e are the source and target phrases respectively, and s (\u00b7, \u00b7) represents the semantic similarity function."}, {"heading": "3.3 Objective and Training", "text": "There are two types of errors involved in our objective function: reconstruction errors (see Eq. (3)) and semantic errors. The latter error measures how well a source phrase semantically matches its counterpart target sentence. We use a maxmargin method to estimate this semantic error. In the face of a training instance (f, e) with negative samples (f \u2212, e \u2212), we define the following rank-based errors: Esem (f, e) = max. (f, e) \u2212 s (f \u2212, e) \u2212 s (f \u2212, e) \u2212 s (f \u2212, e) \u2212 s (f) \u2212 s (12) Intuitively, minimizing this error will maximize the semantic similarity of the correct translation pair (f, e) and minimize (to a margin) the similarity of the negative translation paths (f \u2212, e \u2212) and (f, e \u2212)."}, {"heading": "4 Experiment", "text": "To test the effectiveness of BattRAE in learning the bilingual embedding of phrases, we conducted large-scale experiments on NIST translation tasks in Chinese-English.5"}, {"heading": "4.1 Setup", "text": "Our parallel corpus consists of 1.25M sentence pairs extracted from LDC corpora6, with 27.9M Chinese words and2The s and t scripts are used to identify the source and target language.3https: / / code.google.com / p / word2vec / 4http: / / www.chokkan.org / software / liblbfgs / 5Source code is available at https: / / github.com / DeepLearnXMU / BattRAE. 6These include LDC2002E18, LDC2003E07, LDC2003E14, Hansard's stake in LDC2004T07, LDC2004T08 and LDC2005T06.34.5M English words. We trained a 5 gram language model on Xinhua part of the GIGAWORD corpus (247.6M English words) with SRILM Toolkit7 with modified smooey thing."}, {"heading": "4.2 Translation Performance", "text": "We compared BattRAE with the following three methods: \u2022 Baseline: Our baseline decoder is a state-of-the-art bracket system that combines transduction grammar with a maximum entropy-based reordering model (Wu 1997; Xiong, Liu, and Lin 2006). Features used in this baseline include: rule translation probabilities in two directions, lexical weights in two directions, target-side word count, phrase count, language model evaluation, and the assessment of the maximum entropy-based reordering model. \u2022 BRAE: The neural model proposed by Zhang et al. (2014) incorporates the semantic distances calculated according to BRAE as new features in SMT's log-linear model for selecting translations. \u2022 BCorrRAE: The neural model proposed by Su et al. (2015) expands BRAE with word alignment information."}, {"heading": "4.3 Attention Analysis", "text": "Considering the significant improvements and advantages of BattRAE over BRAE and BCorrRAE, we would like to take a deeper look at how the bidimensional attention mechanism works in the BattRAE model. Specifically, we ask ourselves which words are heavily weighted by the attention mechanism. Table 3 shows some examples in our translation model. We provide phrase structures learned from RAE, and visualize attention weights for these examples. We note that the BattRAE model is able to learn what is important for calculating semantic similarity; the model can detect the correspondence between \"yige\" and \"equal,\" \"yanzhong\" and \"serious concern,\" \"\" weizhi \"and\" so far. \"These word pairs tend to give these translation instances high semantic similarity values. In contrast, the translation model, based on false translation pairs,\" yu \"and\" very critical. \""}, {"heading": "5 Related Work", "text": "Our work relates to bilingual embedding and attention-based neural networks. Previous work on these two lines will be presented in this section."}, {"heading": "5.1 Bilingual Embeddings", "text": "The studies on bilingual embedding are based on bilingual word embedding learning. Zou et al. (2013) use word alignments to combine source and target word embedding. To alleviate the dependence of bilingual embedding learning on parallel corpus, Vulic \u0301 and Moens (2015) examine documented rather than sentence-oriented data, while Gouws et al. (2015) examine monolingual raw texts. Unlike the above-mentioned corpus-centric methods, Koc \u0445 isky et al. (2014) develop a probabilistic model to capture deep semantic information, while Chandar et al. (2014) attest to the use of autoencoder-based methods. More recently, Luong et al. (2015b) share model context information and meaningful signals to learn high-quality bilingual embedding."}, {"heading": "5.2 Attention-Based Neural Networks", "text": "In recent months, we have seen the enormous success of attention-based neural networks in a variety of tasks where learning alignments between different modalities is a key interest. For example, Mnih et al. (2014) learns visual objects and actions in a dynamic control problem. Xu et al. (2015) continue to use an attention mechanism in the task of captioning images. With regard to neural machine translation, we propose a two-dimensional attention network that is able to translate and align words. Luong et al. (2015) evaluate other attention architectures related to translation. In relation to this work, we propose a two-dimensional attention network that is suitable in a bilingual context."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we have presented a two-dimensional, attention-based recursive autoencoder for learning bilingual phrases, which contains clues and interactions between source and target phrases at multiple levels of granularity. Through the bidimensional attention network, our model is able to integrate them into bilingual sentence embeddings. Experimental results show that our approach significantly improves translation quality. In the future, we would like to use various functions to calculate semantically matching values (Eq. (6) and other neural models for generating phrase structures. Furthermore, the bidimensional attention mechanism can be used in conventional neural networks and recurring neural networks. Furthermore, we are interested in adapting our model to semantic tasks such as paraphrase identification and natural language inference."}, {"heading": "Acknowledgements", "text": "The authors were supported by the National Natural Science Foundation of China (grant no. 61303082, 61403269, 61622209 and 61672440), the Natural Science Foundation of Fujian Province (grant no. 2016J05161) and the Natural Science Foundation of Jiangsu Province (grant no. BK20140355). We also thank the anonymous critics for their insightful comments."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proc. of ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "and Bengio", "author": ["J. Bergstra"], "venue": "Y.", "citeRegEx": "Bergstra and Bengio 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "V", "author": ["S. Chandar A P", "S. Lauly", "H. Larochelle", "M. Khapra", "B. Ravindran", "Raykar"], "venue": "C.; and Saha, A.", "citeRegEx": "Chandar A P et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho"], "venue": null, "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Gao"], "venue": "In Proc. of ACL,", "citeRegEx": "Gao,? \\Q2014\\E", "shortCiteRegEx": "Gao", "year": 2014}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Bengio Gouws", "S. Corrado 2015] Gouws", "Y. Bengio", "G. Corrado"], "venue": "In ICML,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "F", "author": ["Koehn, P.", "Och"], "venue": "J.; and Marcu, D.", "citeRegEx": "Koehn. Och. and Marcu 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "K", "author": ["Ko\u010disk\u00fd, T.", "Hermann"], "venue": "M.; and Blunsom, P.", "citeRegEx": "Ko\u010disk\u00fd. Hermann. and Blunsom 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Pham Luong", "M. Manning 2015a] Luong", "H. Pham", "C.D. Manning"], "venue": "Proc. of EMNLP 1412\u20131421", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bilingual word representations with monolingual quality in mind", "author": ["Pham Luong", "T. Manning 2015b] Luong", "H. Pham", "C.D. Manning"], "venue": "In Proc. of VSM-NLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Mnih"], "venue": "In Proc. of NIPS. Curran Associates,", "citeRegEx": "Mnih,? \\Q2014\\E", "shortCiteRegEx": "Mnih", "year": 2014}, {"title": "and Ney", "author": ["F.J. Och"], "venue": "H.", "citeRegEx": "Och and Ney 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni"], "venue": "In Proc. of ACL,", "citeRegEx": "Papineni,? \\Q2002\\E", "shortCiteRegEx": "Papineni", "year": 2002}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Socher"], "venue": "In Proc. of NIPS,", "citeRegEx": "Socher,? \\Q2011\\E", "shortCiteRegEx": "Socher", "year": 2011}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["Socher"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Socher,? \\Q2011\\E", "shortCiteRegEx": "Socher", "year": 2011}, {"title": "C", "author": ["R. Socher", "D. Chen", "Manning"], "venue": "D.; and Ng, A.", "citeRegEx": "Socher et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Bilingual correspondence recursive autoencoder for statistical machine translation", "author": ["Su"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Su,? \\Q2015\\E", "shortCiteRegEx": "Su", "year": 2015}, {"title": "and Moens", "author": ["I. Vuli\u0107"], "venue": "M.-F.", "citeRegEx": "Vuli\u0107 and Moens 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "D", "author": ["Wu"], "venue": "1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, Volume 23, Number 3, September", "citeRegEx": "Wu 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Training phrase translation models with leaving-one-out", "author": ["Mauser Wuebker", "J. Ney 2010] Wuebker", "A. Mauser", "H. Ney"], "venue": "In Proc. of ACL,", "citeRegEx": "Wuebker et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wuebker et al\\.", "year": 2010}, {"title": "Maximum entropy based phrase reordering model for statistical machine translation", "author": ["Liu Xiong", "D. Lin 2006] Xiong", "Q. Liu", "S. Lin"], "venue": "In Proc. of ACL,", "citeRegEx": "Xiong et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2006}, {"title": "R", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "Zemel"], "venue": "S.; and Bengio, Y.", "citeRegEx": "Xu et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Sch\u00fctze", "author": ["W. Yin"], "venue": "H.", "citeRegEx": "Yin and Sch\u00fctze 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Bilingually-constrained phrase embeddings for machine translation", "author": ["Zhang"], "venue": "In Proc. of ACL,", "citeRegEx": "Zhang,? \\Q2014\\E", "shortCiteRegEx": "Zhang", "year": 2014}, {"title": "C", "author": ["W.Y. Zou", "R. Socher", "D. Cer", "Manning"], "venue": "D.", "citeRegEx": "Zou et al. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we propose a bidimensional attention based recursive autoencoder (BattRAE) to integrate clues and sourcetarget interactions at multiple levels of granularity into bilingual phrase representations. We employ recursive autoencoders to generate tree structures of phrases with embeddings at different levels of granularity (e.g., words, sub-phrases and phrases). Over these embeddings on the source and target side, we introduce a bidimensional attention network to learn their interactions encoded in a bidimensional attention matrix, from which we extract two soft attention weight distributions simultaneously. These weight distributions enable BattRAE to generate compositive phrase representations via convolution. Based on the learned phrase representations, we further use a bilinear neural model, trained via a max-margin method, to measure bilingual semantic similarity. To evaluate the effectiveness of BattRAE, we incorporate this semantic similarity as an additional feature into a state-of-the-art SMT system. Extensive experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.63 BLEU points on average over the baseline.", "creator": "LaTeX with hyperref package"}}}