{"id": "1605.07891", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Query Expansion with Locally-Trained Word Embeddings", "abstract": "Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability to model term similarity and other relationships. We study the use of term relatedness in the context of query expansion for information retrieval. We demonstrate that word embeddings such as word2vec and GloVe, when trained globally, underperform corpus and query specific embeddings for retrieval tasks. These results suggest that other tasks benefiting from global embeddings may also benefit from local embeddings.", "histories": [["v1", "Wed, 25 May 2016 14:09:00 GMT  (424kb,D)", "http://arxiv.org/abs/1605.07891v1", "ACL 2016, to appear"], ["v2", "Thu, 23 Jun 2016 00:46:06 GMT  (441kb,D)", "http://arxiv.org/abs/1605.07891v2", null]], "COMMENTS": "ACL 2016, to appear", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["fernando diaz 0001", "bhaskar mitra", "nick craswell"], "accepted": true, "id": "1605.07891"}, "pdf": {"name": "1605.07891.pdf", "metadata": {"source": "CRF", "title": "Query Expansion with Locally-Trained Word Embeddings", "authors": ["Fernando Diaz", "Bhaskar Mitra", "Nick Craswell"], "emails": ["fdiaz@microsoft.com", "bmitra@microsoft.com", "nickr@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Continuity space embedding such as word2vec (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014a) project terms into a vocabulary at a dense, lower dimensional space. Recent results in the natural language processing community show the effectiveness of these methods for analogy and word similarity tasks. In general, these approaches provide global representations of words. Each word has a fixed representation, regardless of any discourse topic. While global representation offers some advantages, language usage can be very topic-specific. For example, ambiguous terms can be easily disamused given local information in the immediate vicinity of words (Harris, 1954; Yarowsky, 1993). In fact, the window-based formation of wort2vec style algorithms exploits this distributional property. A global word embedding, even when trained using local windows, only risks rough representations or those prevailing in the corpus."}, {"heading": "2 Motivation", "text": "For the purpose of motivating our approach, we will limit ourselves to word2vec, although other methods behave similarly (Levy and Goldberg, 2014). The training procedure for word2vec involves discriminatory training of a neural network to predict a word that has a high number of context words (w), while instance losses are often defined as \"(w, c) = logbook,\" (w, c) having a very high number of terms (w) where the instance collection of a term is embedded in a dimensional space: Vm \u2192 < k Projects that embed a set of m terms in a high dimensional space. Parameters control the sampling of random negative terms. These matrices are formed through a set of contexts sampled by a large corpus and minimize the expected loss."}, {"heading": "3 Local Word Embeddings", "text": "The previous section describes several reasons why global embedding may lead to supergeneric word embedding, although these models are similar to 2001. In order to perform theme-specific training, we need a set of theme-specific documents. In query scenarios, users rarely provide the system with examples of theme-specific documents, instead of providing a small set of keywords.Fortunately, we can use query techniques to generate a query-specific set of theme-specific documents. Specifically, we use a language modeling approach (Croft and Lafferty, 2003) to do so. In this query model, each document is presented as a language model with maximum probability, which is estimated from the frequency of documents. Query-language models are similarly valued using the term frequency in the query (Croft and Lafferty, 2003).In this query model, each document is presented as a language model with maximum probability, which is derived from the frequency of documents. Query models are used. Query-language models are v, using the term frequency models, we use the term frequency in the query (Croft and Lafferty, 2003).A document evaluation is then the cullback leibler divergence between the query model, and each document, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v v, v, v, v v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v,"}, {"heading": "4 Query Expansion with Word Embeddings", "text": "When using language models for retrieval, the query extension includes the estimation of an alternative to pq. If each expansion term is associated with a weight, we normalize these weights to derive the expansion language model pq +. This language model will then be used with the original query model p1q (w) = \u03bbpq (w) + (1 \u2212 \u03bb) pq + (w) (6) This interpolated language model can then be used with Equation 4 for embedding documents (Abdul-Jaleel et al., 2004). We will call this the extended query result of a document. Now, let's turn to the use of word embedding for query extension. Let's be a | V | \u00d7 k term that embeds the matrix. If q is a | V | \u00d7 1 column terminator for a query, then the expansion term weights UTq are coached on the first, then we will normalize the top weights for the following term, and then we will take the top weights for the next."}, {"heading": "5 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data", "text": "To evaluate the various retrieval strategies described in Section 3, we use the following data sets: Two newswire records, trec12 and robust, consist of newswire documents and related queries from TREC ad hoc retrieval reports. The trec12 corpus consists of tipster records 1 and 2; and the robust corpus consists of tipster records 4 and 5. Our third data set, Web, consists of the Clue Web 2009 Category B web corpus. For the web corpus, we retain only documents with a waterloo spam rank above 70.1 We present corpus statistics in Table 1. We look at several publicly available global embeddings. We use four GloVe embeddings of varying dimensions that have been trained on the Union of Wikipedia and Gigaword documents."}, {"heading": "5.2 Evaluation", "text": "We look at several standard on-demand metrics, including NDCG @ 10 and interpolated precision at standard on-demand points (Ja \ufffd rvelin and Keka \ufffd la \ufffd inen, 2002; van Rijsbergen, 1979).NDCG @ 10 provides insight into the performance of specifically higher ranks. An interpolated precision on-demand curve describes system performance across the league table."}, {"heading": "5.3 Training", "text": "All on-demand experiments were performed by performing 10-fold cross-validation across queries. Specifically, we validate the number of expansion terms, k {5, 10, 25, 50, 100, 250, 500} and the interpolation weight, sm [0, 1]. For local word2vec training, we cross the learning rate \u03b1 {10 \u2212 1, 10 \u2212 2, 10 \u2212 3}. All word2vec training used the publicly available word2vec Cbow implementation.7 In training the local models, we trampled 1000 documents from p (d) with replacement. To compensate for the much smaller body size, we performed word2vec training for 80 iterations. Local word2vec models use a fixed embedding dimension of 400, although other options did not significantly affect our results."}, {"heading": "6 Results", "text": "eDi eeisrteeGsrsrteeee\u00fccnlhsrlhsrtee\u00fccnlhsrtee\u00fccnlhsrteeeirsrteeSrteeu uzm rf\u00fc ide eeisrteeSrlrteee\u00fcgcnlhsrrtee\u00fccnlhsrrrrrteee\u00fccnlrrrrrrrrrrrsrrrteeoiuiuiuiuiueeglllrsrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteerrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "7 Discussion", "text": "The success of local embedding in this task should alarm researchers who use global embedding as a representative tool. Furthermore, our results suggest that the approach to learning from vast amounts of data is only effective when the data is appropriate for the task at hand, and when it is made available, much smaller, high-quality data can perform much better. Furthermore, our results suggest that the approach to estimating global representations, while mathematically practical, can overlook insights that are possible at the time of the query or at the evaluation time in general. A similar local embedding approach can be used for any natural language processing task where topical locations can be expected and estimated. Although we have used a query to rebalance the corpus in our experiments, we could just as easily use alternative contextual information in other assignments.globalDespite these strong results, we believe that there are still some open issues in this work, firstly, although local embedding gains can be relatively global in effectiveness."}, {"heading": "8 Related Work", "text": "The problem is that one has to deal with a global model in which there are several terms per word. (...) It is the first time in the history of a country that such a model has been used all over the world. (...) It is the second time that such a model has been used in another country. (...) It is the second time that such a model has been used in another country. (...) It is the third time that such a model has been used in another country. (...) It is the first time that such a model has been used in another country. (...) It is the second time that such a model has been used in another country. (...) It is the third time that such a model has been used in another country. (...) It is the third time that such a model has been used in another country. (...) It is the third time that such a model has been used in another country. (...) It is the third time that another has been used in another country. (...) It is the third time that it has been used in another country."}, {"heading": "9 Conclusion", "text": "We have demonstrated a simple and effective method of performing query enhancements with Word embedding. Importantly, our results underscore the value of locally trained Word embedding in a query-specific way.The strength of these results suggests that other research using global embedding vectors should consider local embedding as potentially better representation.Although embedding techniques such as word2vec are referred to as the \"Sriracha sauce of deep learning,\" we argue that a dish might taste better with its own unique sauce."}], "references": [{"title": "Latent semantic indexing (lsi) fails for trec collections", "author": ["Atreya", "Elkan2011] Avinash Atreya", "Charles Elkan"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Atreya et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Atreya et al\\.", "year": 2011}, {"title": "Local feedback in full-text retrieval systems", "author": ["Attar", "Fraenkel1977] R. Attar", "A.S. Fraenkel"], "venue": "J. ACM,", "citeRegEx": "Attar et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Attar et al\\.", "year": 1977}, {"title": "Statistical language model adaptation: review and perspectives", "author": ["Jerome R Bellegarda"], "venue": "Speech communication,", "citeRegEx": "Bellegarda.,? \\Q2004\\E", "shortCiteRegEx": "Bellegarda.", "year": 2004}, {"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Language Modeling for Information Retrieval", "author": ["Croft", "Lafferty2003] W. Bruce Croft", "John Lafferty"], "venue": null, "citeRegEx": "Croft et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Croft et al\\.", "year": 2003}, {"title": "Predicting query performance", "author": ["Yun Zhou", "W. Bruce Croft"], "venue": "In SIGIR \u201902: Proceedings of the 25th annual international ACM SIGIR conference on Research", "citeRegEx": "CronenTownsend et al\\.,? \\Q2002\\E", "shortCiteRegEx": "CronenTownsend et al\\.", "year": 2002}, {"title": "Indexing by latent semantic analysis", "author": ["Susan T. Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman"], "venue": "Journal of the American Society of Information", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Condensed list relevance models", "author": ["Fernando Diaz"], "venue": "In Proceedings of the 2015 International Conference on The Theory of Information Retrieval,", "citeRegEx": "Diaz.,? \\Q2015\\E", "shortCiteRegEx": "Diaz.", "year": 2015}, {"title": "Bayesian estimation methods for n-gram language model adaptation", "author": ["Marcello Federico"], "venue": "In Spoken Language,", "citeRegEx": "Federico.,? \\Q1996\\E", "shortCiteRegEx": "Federico.", "year": 1996}, {"title": "One sense per discourse", "author": ["Gale et al.1992] William A. Gale", "Kenneth W. Church", "David Yarowsky"], "venue": "In Proceedings of the Workshop on Speech and Natural Language,", "citeRegEx": "Gale et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Gale et al\\.", "year": 1992}, {"title": "Word embedding based generalized language model for information retrieval", "author": ["Dwaipayan Roy", "Mandar Mitra", "Gareth J.F. Jones"], "venue": "In Proceedings of the 38th International ACM SIGIR", "citeRegEx": "Ganguly et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ganguly et al\\.", "year": 2015}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the As-", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Modeling long distance dependence in language: topic mixtures versus dynamic cache models", "author": ["Iyer", "Ostendorf1999] R.M. Iyer", "M. Ostendorf"], "venue": "Speech and Audio Processing, IEEE Transactions on,", "citeRegEx": "Iyer et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 1999}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["J\u00e4rvelin", "Kek\u00e4l\u00e4inen2002] Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen"], "venue": null, "citeRegEx": "J\u00e4rvelin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin et al\\.", "year": 2002}, {"title": "Viewing morphology as an inference process", "author": ["Robert Krovetz"], "venue": "Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Krovetz.,? \\Q1993\\E", "shortCiteRegEx": "Krovetz.", "year": 1993}, {"title": "A cache-based natural language model for speech recognition", "author": ["Kuhn", "De Mori1990] Roland Kuhn", "Renato De Mori"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions", "citeRegEx": "Kuhn et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Kuhn et al\\.", "year": 1990}, {"title": "Relevance based language models", "author": ["Lavrenko", "Croft2001] Victor Lavrenko", "W. Bruce Croft"], "venue": "In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information", "citeRegEx": "Lavrenko et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lavrenko et al\\.", "year": 2001}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Improving document ranking with dual word embeddings", "author": ["Bhaskar Mitra", "Nick Craswell", "Rich Caruana"], "venue": "In Proc. WWW. International World Wide Web Conferences Steering Commit-", "citeRegEx": "Nalisnick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nalisnick et al\\.", "year": 2016}, {"title": "Language model and speaking rate adaptation for spontaneous presentation speech recognition", "author": ["Nanjo", "Kawahara2004] Hiroaki Nanjo", "Tatsuya Kawahara"], "venue": "Speech and Audio Processing, IEEE Transactions on,", "citeRegEx": "Nanjo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nanjo et al\\.", "year": 2004}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space. arXiv preprint arXiv:1504.06654", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proc. EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A mixture model with sharing for lexical semantics", "author": ["Reisinger", "Mooney2010a] Joseph Reisinger", "Raymond Mooney"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Multiprototype vector-space models of word meaning", "author": ["Reisinger", "Mooney2010b] Joseph Reisinger", "Raymond J Mooney"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["Hidetoshi Shimodaira"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Shimodaira.,? \\Q2000\\E", "shortCiteRegEx": "Shimodaira.", "year": 2000}, {"title": "Query-sensitive similarity measures for the calculation of interdocument relationships", "author": ["Tombros", "C.J. van Rijsbergen"], "venue": "In CIKM", "citeRegEx": "Tombros et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Tombros et al\\.", "year": 2001}, {"title": "The effectiveness of query-specific hierarchic clustering in information retrieval", "author": ["Robert Villa", "C.J. Van Rijsbergen"], "venue": "Inf. Process. Manage.,", "citeRegEx": "Tombros et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tombros et al\\.", "year": 2002}, {"title": "sense2vec-a fast and accurate method for word sense disambiguation in neural word embeddings", "author": ["Trask et al.2015] Andrew Trask", "Phil Michalak", "John Liu"], "venue": "arXiv preprint arXiv:1511.06388", "citeRegEx": "Trask et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Trask et al\\.", "year": 2015}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["van der Maaten", "Geoffrey E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "LDA-based document models for ad-hoc retrieval", "author": ["Wei", "Croft2006] Xing Wei", "W. Bruce Croft"], "venue": "Proceedings of the 29th annual international ACM SIGIR conference on Research and development", "citeRegEx": "Wei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2006}, {"title": "Query-specific automatic document classification", "author": ["Peter Willett"], "venue": "In International Forum on Information and Documentation,", "citeRegEx": "Willett.,? \\Q1985\\E", "shortCiteRegEx": "Willett.", "year": 1985}, {"title": "Query expansion using local and global document analysis", "author": ["Xu", "Croft1996] Jinxi Xu", "W. Bruce Croft"], "venue": "In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information", "citeRegEx": "Xu et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Xu et al\\.", "year": 1996}, {"title": "One sense per collocation", "author": ["David Yarowsky"], "venue": "In Proceedings of the Workshop on Human Language Technology,", "citeRegEx": "Yarowsky.,? \\Q1993\\E", "shortCiteRegEx": "Yarowsky.", "year": 1993}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["David Yarowsky"], "venue": "In Proceedings of the 33rd Annual Meeting on Association", "citeRegEx": "Yarowsky.,? \\Q1995\\E", "shortCiteRegEx": "Yarowsky.", "year": 1995}, {"title": "Language model adaptation for statistical machine translation with structured query models", "author": ["Zhao et al.2004] Bing Zhao", "Matthias Eck", "Stephan Vogel"], "venue": "In Proceedings of the 20th International Conference on Compu-", "citeRegEx": "Zhao et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 36, "context": "immediately surrounding words (Harris, 1954; Yarowsky, 1993).", "startOffset": 30, "endOffset": 60}, {"referenceID": 9, "context": "refer to this as the \u2018one sense per discourse\u2019 property (Gale et al., 1992).", "startOffset": 56, "endOffset": 75}, {"referenceID": 37, "context": "Previous work by Yarowsky demonstrates that this property can be successfully combined with information from nearby terms (Yarowsky, 1995).", "startOffset": 122, "endOffset": 138}, {"referenceID": 30, "context": "sion (Xu and Croft, 1996), cluster-based retrieval (Tombros and van Rijsbergen, 2001; Tombros et al., 2002; Willett, 1985), and term clustering (Attar and Fraenkel, 1977).", "startOffset": 51, "endOffset": 122}, {"referenceID": 34, "context": "sion (Xu and Croft, 1996), cluster-based retrieval (Tombros and van Rijsbergen, 2001; Tombros et al., 2002; Willett, 1985), and term clustering (Attar and Fraenkel, 1977).", "startOffset": 51, "endOffset": 122}, {"referenceID": 28, "context": "The expected loss under this distribution is (Shimodaira, 2000),", "startOffset": 45, "endOffset": 63}, {"referenceID": 9, "context": "In fact, we suspect that pt(w|c) 6= pc(w|c) because of the \u2018one sense per discourse\u2019 claim (Gale et al., 1992).", "startOffset": 91, "endOffset": 110}, {"referenceID": 15, "context": "All corpora in Table 1 were stopped using the SMART stopword list5 and stemmed using the Krovetz algorithm (Krovetz, 1993).", "startOffset": 107, "endOffset": 122}, {"referenceID": 7, "context": "in performance nearly identical with an expanded retrieval at a much lower cost (Diaz, 2015).", "startOffset": 80, "endOffset": 92}, {"referenceID": 5, "context": "To test this, we computed the KL divergence between the local unigram distribution, \u2211 d p(w|d)p(d), and the corpus unigram language model (CronenTownsend et al., 2002).", "startOffset": 138, "endOffset": 167}, {"referenceID": 12, "context": "The problem can be addressed by training a global model with multiple vector embeddings per word (Reisinger and Mooney, 2010a; Huang et al., 2012).", "startOffset": 97, "endOffset": 146}, {"referenceID": 23, "context": "number of senses for each word may be fixed (Neelakantan et al., 2015), or determined using class labels (Trask et al.", "startOffset": 44, "endOffset": 70}, {"referenceID": 31, "context": ", 2015), or determined using class labels (Trask et al., 2015).", "startOffset": 42, "endOffset": 62}, {"referenceID": 2, "context": "Several methods exist in the language modeling community for topic-dependent adaptation of language models (Bellegarda, 2004).", "startOffset": 107, "endOffset": 125}, {"referenceID": 38, "context": "These can lead to performance improvements in tasks such as machine translation (Zhao et al., 2004) and speech recognition (Nanjo and Kawahara, 2004).", "startOffset": 80, "endOffset": 99}, {"referenceID": 8, "context": "or other more sophisticated approaches (Federico, 1996; Kuhn and De Mori, 1990).", "startOffset": 39, "endOffset": 79}, {"referenceID": 6, "context": "Using the term-document matrix for embedding leads to several well-studied approaches such as LSA (Deerwester et al., 1990), PLSA (Hofmann, 1999), and LDA (Blei et al.", "startOffset": 98, "endOffset": 123}, {"referenceID": 11, "context": ", 1990), PLSA (Hofmann, 1999), and LDA (Blei et al.", "startOffset": 14, "endOffset": 29}, {"referenceID": 10, "context": "(Ganguly et al., 2015) used the word similarity in the word2vec embedding space as a way to estimate term transformation probabilities in a language modelling setting for retrieval.", "startOffset": 0, "endOffset": 22}, {"referenceID": 21, "context": "(Nalisnick et al., 2016) proposed to model document aboutness by computing the similarity between all pairs of query and document terms using dual embedding spaces.", "startOffset": 0, "endOffset": 24}], "year": 2016, "abstractText": "Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability to model term similarity and other relationships. We study the use of term relatedness in the context of query expansion for information retrieval. We demonstrate that word embeddings such as word2vec and GloVe, when trained globally, underperform corpus and query specific embeddings for retrieval tasks. These results suggest that other tasks benefiting from global embeddings may also benefit from local embeddings.", "creator": "LaTeX with hyperref package"}}}