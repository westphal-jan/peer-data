{"id": "1605.07912", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Review Networks for Caption Generation", "abstract": "We propose a novel module, the reviewer module, to improve the encoder-decoder learning framework. The reviewer module is generic, and can be plugged into an existing encoder-decoder model. The reviewer module performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a fact vector after each review step; the fact vectors are used as the input of the attention mechanism in the decoder. We show that the conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework can improve over state-of-the-art encoder-decoder systems on the tasks of image captioning and source code captioning.", "histories": [["v1", "Wed, 25 May 2016 14:49:58 GMT  (1874kb,D)", "http://arxiv.org/abs/1605.07912v1", null], ["v2", "Thu, 26 May 2016 00:47:21 GMT  (1874kb,D)", "http://arxiv.org/abs/1605.07912v2", null], ["v3", "Tue, 7 Jun 2016 01:39:35 GMT  (1875kb,D)", "http://arxiv.org/abs/1605.07912v3", null], ["v4", "Thu, 27 Oct 2016 17:50:27 GMT  (1877kb,D)", "http://arxiv.org/abs/1605.07912v4", "NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.CV", "authors": ["zhilin yang", "ye yuan", "yuexin wu", "william w cohen", "ruslan salakhutdinov"], "accepted": true, "id": "1605.07912"}, "pdf": {"name": "1605.07912.pdf", "metadata": {"source": "CRF", "title": "Encode, Review, and Decode: Reviewer Module for Caption Generation", "authors": ["Zhilin Yang", "Ye Yuan", "Yuexin Wu", "Ruslan Salakhutdinov", "William W. Cohen"], "emails": ["zhiliny@cs.cmu.edu", "yey1@cs.cmu.edu", "yuexinw@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "wcohen@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "The idea behind it is not new, but it is about the question of whether it is about a way, in which it is about a way, in which it is about the conceptuality, about a way, in which the conceptuality and the way, how the conceptuality and the way, how the conceptuality and the way, how the conceptuality and the way, how the conceptuality and the way, how the conceptuality and the way, how the conceptuality and the way, how the conceptuality and the way, how the conceptuality and the way, how the conceptuality and the way, how the conceptuality and the way, how the conceptuality and the way, how the conceptuality and the way, and the way, how the conceptuality and the way, and the way, and the way, and the way, and the way, and the way, and the way, and the way, and the way, and the way, and the way,"}, {"heading": "2 Related Work", "text": "The encoder decoder framework in the context of sequence-to-sequence learning was recently introduced to learn the transformation between text sequences [3, 15], using RNNs for both encoding and decoding. Encoder decoders can generally refer to models that learn representational transformation with two network components, an encoder and a decoder. In addition to RNNNs, Convolutionary encoders are also being developed to address multimodal tasks such as image labeling [17]. The attention mechanism was later introduced into the encoder decoder decoder framework for machine translation, with an explanation of explicit token-level sequences [1]. Unlike vanilla encoder decoder decoder decoder decoder decoder decoder systems, the encoder framing is linked to the encoder decoder framework or the encoder framework."}, {"heading": "3 Model", "text": "Considering the input representation x and the output representation y, the goal is to learn a function mapping from x to y. For example, caption aims to learn a mapping from an image x to an image caption y. For convenience, we use x and y to denote both a tensor and a sequence of tensors. For example, x can be a 3D tensor that represents an image with RGB channels in the caption, or a sequence of 1d tensors (i.e. vectors) x = (x1, \u00b7, xTx) in machine translation, where xt is the embedding of the t-th word in the input sequence of the length Tx.Unlike conventional (attentive) encoder decoder models, our model consists of three components, encoder, reviewer and decoder. Comparison of architectures is shown in our Figure 1 as opposed to the conventional or three-component model."}, {"heading": "3.1 Encoder", "text": "The encoder encodes the input x into a context vector c and a number of hidden states H = {ht} t. We discuss two types of encoders, RNN encoders and CNN encoders. RNN encoders: Let Tx = | H | be the length of the input sequence. An RNN encoder processes the input sequence x = (x1, \u00b7, xTx) sequentially. At step t the RNN encoder updates the hidden state byht = f (xt, ht \u2212 1).In this work we implement f with an LSTM unit. The context vector is defined as the final hidden state c = hTx. The cell state and the hidden state h0 of the first LSTM unit are initialized as zeros. CNN encoder: We take a widely used CNN architecture - VGGNet [13] - as an example to describe how we use a NSTi as the first encoder state."}, {"heading": "3.2 Reviewer", "text": "The intuition behind the review module is to review all the information encoded by the encoder and learn fact vectors that are more compact, abstract and global than the hidden states of the original encoder. The reviewer performs Tr review steps on the hidden states of the encoder H and outputs a fact vector ft after each step. Specifically ft = gt (H, ft \u2212 1), with a modified LSTM unit with attention mechanism at review step gt. We examine two variants of gt, an attentive input reviewer and an attentive output reviewer."}, {"heading": "3.2.1 Attentive Input Reviewer", "text": "For each review step t, the attentive input rapporteur first applies an attention mechanism to H and uses the attention result as input to an LSTM unit (cf. Figure 2a). Leave the attentive input rapporteur the attention result at step t. The attentive input rapporteur is formulated as follows: asf-t = att (H, ft-1) = | H-t = 1 \u03b1 (hi, ft-1) \u2211 | H-i \u2032 = 1 \u03b1 (hi-i, ft-1) hi, gt (H, ft-1) = f-t (f-t, ft-1), (1) where \u03b1 (hi, ft-1) is a function that determines the weight for the i-th hidden state. \u03b1 (x1, x2) can be implemented as a point product between x1 and x2 or as a multi-layer perceptron (MLP), which is the concatenation of x1 and x1-M."}, {"heading": "3.2.2 Attentive Output Reviewer", "text": "In contrast to the attentive input reviewer, the attentive output reviewer uses a zero vector as input to the LSTM unit, and the fact vector is calculated as a weighted sum of attention outcomes and the output of the LSTM unit (see Figure 2b). More specifically, the attentive output reviewer is formulated as f-t = att (H, ft-1), gt (H, ft-1) = f-t (0, ft-1) + Wf-t, with the attentiveness mechanism att following the definition in the equation. (1), 0 stands for a zero vector, W for a model parameter matrix, and f-t for an LSTM unit in step. We note that performing attention on an RNN unit is usually used in sequence-to-learn [1, 9, 12]. We apply a linear transformation with a matrix that can be da \u00b7 t and f \u00b7 t (dimensions)."}, {"heading": "3.2.3 Weight Tying", "text": "s wt the parameters for the unit f \u2032 t. The first variant follows the common setting in RNNs, where the weights are divided among all units; i.e. w1 = \u00b7 \u00b7 \u00b7 = wTr. We also observe that the reviewer unit has no sequential input, so we experiment with the second variant, where the weights are loosened, i.e. wi 6 = wj, Hi 6 = j. The cell state and the hidden state of the first unit f \u2032 1 are initialized as context vector c. In both cases of the weight bonding, the cell states and hidden states are passed through all the reviewer units."}, {"heading": "3.3 Decoder", "text": "The decoder is formulated as an LSTM network with attention to the fact vectors F. (cf. figure 2c) Let the hidden state of the t-th LSTM unit be in the decoder. The decoder is formulated as follows: s-t = att (F, st-1), st = f- \u2032 ([s-t; yt-1], st-1), yt = argmax y Softmaxy (st), (2) where [\u00b7; \u00b7] denotes the concatenation of two vectors, f-s denotes the decoder LSTM, Softmaxy denotes the probability of the word y given by a Softmax layer, yt is the t-th decoded token, and yt is the word embedding yt. The attention mechanism follows the definition in Eq. (1) The initial state of the cell and the state of the STr-sc code model are both concealed."}, {"heading": "3.4 Discriminative Supervision", "text": "In traditional encoder decoders, monitoring is provided in a generative way; i.e., the model aims to maximize the conditional probability of generating sequential output p (y | x). However, discriminatory monitoring has proven useful in [5], where the model is used to predict discriminatory targets, such as word occurrences in output y. We argue that the reviewer module provides a natural way to include discriminatory monitoring in the model. Here, for example, we take the prediction of word occurrences to describe how discriminatory monitoring should be included. As shown in the blue components in Figure 1b, we first apply a linear layer to the fact vector to calculate a score for each word at each verification step. We then apply a max-pooling layer over all verification units to extract the most prominent signal for each word and add multi-label margin discriminatory monitoring."}, {"heading": "3.5 Training", "text": "The training loss for a single training session (x, y) is defined as the weighted sum of the negative probability of the conditional log and discriminatory loss. Let Ty be the length of the output sequence y. The loss can be written as L (x, y) = 1 Ty Ty \u2211 t = 1 \u2212 log Softmaxyt (st) + \u03bbLd, with the definition of Softmaxy and st following equation. (2) and the formulation of Ld following equation. (3). \u03bb is a constant weighting factor. Since Ld is optional, we use Ld = 0 when it is not used. We use adaptive stochastic gradient derivation (AdaGrad) [4] to train the model in an end-to-end manner."}, {"heading": "3.6 Connection to Encoder-Decoders", "text": "We now show that in one specific case, our model can be reduced to the conventional (attentive) encoder decoder, while in our encoder-reviewer decoder model, the input of the decoder is instead the context vector c and the amount of hidden states of the encoder H = {ht} t. To show that our model can be reduced to attentive encoder decoder, we just need to construct a case in which H = F and c = r.Since r = W \u2032 [fTr; c] can be reduced to r = {ft} t, we can further set Tr = Tx and define each reviewer unit as an identity mapping gt (H, ft \u2212 1) = ht that can fit into both the definition of the attentive input encoder and the attentive output of the viewer."}, {"heading": "4 Experiments", "text": "We are experimenting with two sets of data for different tasks, captions and source code captions. Since these two tasks are essentially different, we can use them to test the robustness and generalisability of our model."}, {"heading": "4.1 Image Captioning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Data and Settings", "text": "We evaluate our model using the MSCOCO benchmark dataset [2] for captions. The dataset contains 123,000 images with at least 5 captions for each image. We use the same data split as in [7, 19, 20], where we reserve 5,000 images for development and testing and use the rest for training. Models are evaluated using the official MSCOCO rating scripts. We report three widely used automatic rating metrics, BLEU-4, METEOR and CIDE. We remove all non-alphabetic characters in captions, transform all captions into lowercase letters and tokenize captions using white space. We replace all words that occur less than 5 times with an unknown token < UNK > and get a vocabulary of 9,520 words in captions. We shorten all captions to lowercase letters and place captions using 100 captions with an unknown token < UNK >, and get a vocabulary of 9,520 words in captions. We add captions to the captions using 100 captions, adding the number of captions to the number of captions, the number of TICK = 1, the number of captions, the TICK = 1."}, {"heading": "4.1.2 Results", "text": "We compare our model with encoder decoders to investigate the effectiveness of the reviewer module. We also compare different variants of our model to assess the impact of different weight-binding strategies and discriminatory monitoring, the results of which are presented in Table 1. All results in Table 1 are obtained using VGGNet [13] as an encoder as described in Section 3.1. Table 1 indicates that the reviewer module can consistently improve performance across all three metrics compared to traditional attentive encoder decoders. We also observe that adding discriminatory monitoring can increase model performance, suggesting that the reviewer module provides an effective way to incorporate discriminatory monitoring in an end-to-end-to-end manner. Decoupling the weights between reviewer units can further improve performance. Our guess is that models with unbound weights are more balanced than the common weight models, as each unit can have its own parametric function."}, {"heading": "4.1.3 Case Study and Visualization", "text": "To better understand the reviewer module, we visualize the attention weights \u03b1 in the reviewer module in Figure 3. The visualization is based on the encoder-reviewer decoder model with unbound weights and discriminatory monitoring. We also list the top 5 words with the highest scores (calculated on the basis of fact vectors) in each reviewer unit. We find that the top words with the highest scores can reveal the mindset underlying the reviewer module. For example, in the first image (a giraffe in a zoo), the first reviewer focuses on the movement of the giraffe and the tree around it, the second reviewer analyzes the relative position between the giraffe and the tree, and the third reviewer looks at the overall picture and finds that the scene is in a zoo based on the recognition of fences and fences. \"All of the above information is stored in the decoders and the natural language."}, {"heading": "4.2 Source Code Captioning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Data and Settings", "text": "We are experimenting with a benchmark dataset for subtitling the source code, HabeasCorpus [11]. HabeasCorpus collects nine popular open source Java code repositories, such as Apache Ant and Lucene. The dataset contains 6, 734 Java source code files with 7, 903, 872 source code tokens and 251, 565 comment word tokens. We sample 10% of the files as a test set and use the rest for training. We sample an additional validation division for tuning hyperparameters. Our evaluation follows previous work on modeling the source code language [10] and subtitling [11]. We report on the probability of generating the actual code capacities based on the metadress learned. We also determine the approaches from the source code comment perspective, where the actual percentage of characters saved can be determined (11)."}, {"heading": "4.2.2 Results", "text": "The basic model \"Language Model\" is an LSTM decoder whose output is insensitive to the input code sequence. Our preliminary experiment shows that the LSTM decoder significantly exceeds the Ngram models used in [11] (+ 3% in CS-2), so we use the LSTM decoder as the starting point for comparison. We also compare with various variants of encoder decoders, including the inclusion of bidirectional RNN encoders and attention mechanisms. Table 3 shows that both bidirectional encoders and attention mechanisms can be improved compared to vanilla encoder decoder models. Encoder-reviewer decoder encoders can uniformly improve attentive encoder decoders in all metrics, indicating that the reviewer module is effective at learning meaningful representation."}, {"heading": "5 Conclusion", "text": "We present a novel module, the reviewer module, to improve the encoder decoder learning framework. The reviewer module performs several verification steps with attention to the hidden states of the encoder and calculates a set of fact vectors that summarize the global information in the input. We demonstrate empirically consistent improvements over traditional encoder decoders in terms of captioning and source code subtitling tasks. In the future, it will be interesting to apply our model to other tasks that can be modeled under the encoder decoder framework."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Microsoft coco captions: Data collection and evaluation", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "server. arXiv preprint arXiv:1504.00325,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In ACL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "JMLR, 12:2121\u20132159,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh K Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C Platt"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "In ACL,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Structured generative models of natural source code", "author": ["Chris J Maddison", "Daniel Tarlow"], "venue": "In ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Natural language models for predicting programming comments", "author": ["Dana Movshovitz-Attias", "William W Cohen"], "venue": "In ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "In EMNLP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Image captioning with semantic attention", "author": ["Quanzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo"], "venue": "In CVPR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "The encoder-decoder framework was recently introduced for sequence-to-sequence learning based on recurrent neural networks (RNNs) with applications to machine translation [3, 15], where the input is a text sequence in one language and the output is a text sequence in the other language.", "startOffset": 171, "endOffset": 178}, {"referenceID": 14, "context": "The encoder-decoder framework was recently introduced for sequence-to-sequence learning based on recurrent neural networks (RNNs) with applications to machine translation [3, 15], where the input is a text sequence in one language and the output is a text sequence in the other language.", "startOffset": 171, "endOffset": 178}, {"referenceID": 16, "context": ", encoders based on convolutional neural networks (CNNs) are used for image captioning [17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "Improvements by the attention mechanism are shown on various tasks, including machine translation [1], image captioning [19], and text summarization [12].", "startOffset": 98, "endOffset": 101}, {"referenceID": 17, "context": "Improvements by the attention mechanism are shown on various tasks, including machine translation [1], image captioning [19], and text summarization [12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "Improvements by the attention mechanism are shown on various tasks, including machine translation [1], image captioning [19], and text summarization [12].", "startOffset": 149, "endOffset": 153}, {"referenceID": 4, "context": ", predicting word occurrences in the caption) is beneficial for generative models [5], but it is not clear how to integrate discriminative supervision into the encoder-decoder framework in an end-to-end manner.", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "The encoder-decoder framework in the context of sequence-to-sequence learning was recently introduced for learning transformation between text sequences [3, 15], where RNNs are used for both encoding and decoding.", "startOffset": 153, "endOffset": 160}, {"referenceID": 14, "context": "The encoder-decoder framework in the context of sequence-to-sequence learning was recently introduced for learning transformation between text sequences [3, 15], where RNNs are used for both encoding and decoding.", "startOffset": 153, "endOffset": 160}, {"referenceID": 16, "context": "Besides RNNs, convolutional encoders are developed to address multi-modal tasks such as image captioning [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 0, "context": "Attention mechanism was later introduced to the encoder-decoder framework for machine translation, with an explanation of explicit token-level alignment between input and output sequences [1].", "startOffset": 188, "endOffset": 191}, {"referenceID": 17, "context": "Attention mechanism demonstrates considerable success in other applcations as well, including image captioning [19] and text summarization [12].", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "Attention mechanism demonstrates considerable success in other applcations as well, including image captioning [19] and text summarization [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "Our work is also related to memory networks [18, 14].", "startOffset": 44, "endOffset": 52}, {"referenceID": 7, "context": "Dynamic memory networks extend memory networks to model sequential memories [8].", "startOffset": 76, "endOffset": 79}, {"referenceID": 12, "context": "CNN Encoder: We take a widely-used CNN architecture\u2014VGGNet [13]\u2014as an example to describe how we use CNNs as encoders.", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "\u03b1(x1,x2) can be implemented as a dot product between x1 and x2 or a multi-layer perceptron (MLP) that takes the concatenation of x1 and x2 as input [9].", "startOffset": 148, "endOffset": 151}, {"referenceID": 0, "context": "We note that performing attention on top of an RNN unit is commonly used in sequence-to-sequence learning [1, 9, 12].", "startOffset": 106, "endOffset": 116}, {"referenceID": 8, "context": "We note that performing attention on top of an RNN unit is commonly used in sequence-to-sequence learning [1, 9, 12].", "startOffset": 106, "endOffset": 116}, {"referenceID": 11, "context": "We note that performing attention on top of an RNN unit is commonly used in sequence-to-sequence learning [1, 9, 12].", "startOffset": 106, "endOffset": 116}, {"referenceID": 4, "context": "However, discriminative supervision has been shown to be useful in [5], where the model is guided to predict discriminative objectives, such as the word occurrences in the output y.", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "We adopt adaptive stochastic gradient descent (AdaGrad) [4] to train the model in an end-to-end manner.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "We evaluate our model on the MSCOCO benchmark dataset [2] for image captioning.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "We use the same data split as in [7, 19, 20], where we reserve 5,000 images for development and test respectively and use the rest for training.", "startOffset": 33, "endOffset": 44}, {"referenceID": 17, "context": "We use the same data split as in [7, 19, 20], where we reserve 5,000 images for development and test respectively and use the rest for training.", "startOffset": 33, "endOffset": 44}, {"referenceID": 18, "context": "We use the same data split as in [7, 19, 20], where we reserve 5,000 images for development and test respectively and use the rest for training.", "startOffset": 33, "endOffset": 44}, {"referenceID": 6, "context": "Model BLEU-4 METEOR CIDEr BRNN [7] 23.", "startOffset": 31, "endOffset": 34}, {"referenceID": 17, "context": "0 Soft Attention [19] 24.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "9 \u2014 Hard Attention [19] 25.", "startOffset": 19, "endOffset": 23}, {"referenceID": 4, "context": "0 \u2014 MS Research [5]v\u2020\u2217 25.", "startOffset": 16, "endOffset": 19}, {"referenceID": 16, "context": "6 \u2014 Google NIC [17]g\u2021\u2217 27.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "5 Semantic Attention [20]g\u2020\u2021 30.", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "We use an MLP with one hidden layer of size 512 to define the function \u03b1(\u00b7, \u00b7) in the attention mechanism, and use an attentive input reviewer in our experiments to be consistent with visual attention models [19].", "startOffset": 208, "endOffset": 212}, {"referenceID": 12, "context": "All the results in Table 1 are obtained using VGGNet [13] as encoders as described in Section 3.", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "Since VGGNet and GoogLeNet [16] are both used as the image encoders in previous work, we additionally report the performance of our model using GoogLeNet as the encoder for fair comparison.", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "Table 2 shows that our models outperform all of the other approaches except for Semantic Attention [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "performance dramatically (by a few BLEU-4 points [17]), our results are competitive and possibly better.", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "Different from attentive encoder-decoders [19] that attend to a single object at a time during generation, it can be clearly seen from Figure 3 that the reviewer module captures more global signals, usually combining multiple objects into one fact, including objects not finally shown in the caption (e.", "startOffset": 42, "endOffset": 46}, {"referenceID": 10, "context": "We experiment with a benchmark dataset for source code captioning, HabeasCorpus [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "Our evaluation follows previous works on source code language modeling [10] and captioning [11].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "Our evaluation follows previous works on source code language modeling [10] and captioning [11].", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "More specifically, we use a metric of top-k character savings [11] (CS-k).", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "We follow the tokenization used in [11], where we transform camel case identifiers into multiple separate words (e.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "Our preliminary experiment shows that the LSTM decoder significantly outperforms the Ngram models used in [11] (+3% in CS-2), so we use the LSTM decoder as a baseline for comparison.", "startOffset": 106, "endOffset": 110}], "year": 2017, "abstractText": "We propose a novel module, the reviewer module, to improve the encoder-decoder learning framework. The reviewer module is generic, and can be plugged into an existing encoder-decoder model. The reviewer module performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a fact vector after each review step; the fact vectors are used as the input of the attention mechanism in the decoder. We show that the conventional encoderdecoders are a special case of our framework. Empirically, we show that our framework can improve over state-of-the-art encoder-decoder systems on the tasks of image captioning and source code captioning.", "creator": "LaTeX with hyperref package"}}}