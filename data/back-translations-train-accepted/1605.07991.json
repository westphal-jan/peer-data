{"id": "1605.07991", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Efficient Distributed Learning with Sparsity", "abstract": "We propose a novel, efficient approach for distributed sparse learning in high-dimensions, where observations are randomly partitioned across machines. Computationally, at each round our method only requires the master machine to solve a shifted ell_1 regularized M-estimation problem, and other workers to compute the gradient. In respect of communication, the proposed approach provably matches the estimation error bound of centralized methods within constant rounds of communications (ignoring logarithmic factors). We conduct extensive experiments on both simulated and real world datasets, and demonstrate encouraging performances on high-dimensional regression and classification tasks.", "histories": [["v1", "Wed, 25 May 2016 18:15:43 GMT  (493kb,D)", "http://arxiv.org/abs/1605.07991v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["jialei wang", "mladen kolar", "nathan srebro", "tong zhang 0001"], "accepted": true, "id": "1605.07991"}, "pdf": {"name": "1605.07991.pdf", "metadata": {"source": "CRF", "title": "Efficient Distributed Learning with Sparsity", "authors": ["Jialei Wang", "Mladen Kolar", "Nathan Srebro", "Tong Zhang"], "emails": [], "sections": [{"heading": null, "text": "From a mathematical point of view, our method only requires the master calculator to solve a postponed '1 regulated M estimation problem and other staff to calculate the gradient using local data for each round. In terms of communication, the proposed approach is demonstrably consistent with the estimation errors of centralized methods within continuous rounds of communication (ignoring logarithmic factors).We conduct extensive experiments with both simulated and real data sets and demonstrate encouraging performance in high-dimensional regression and classification tasks."}, {"heading": "1 Introduction", "text": "Many machine learning problems can be seen as minimizing the expected loss where the problem is greater or greater."}, {"heading": "1.1 Overview of main results", "text": "Without loss of generality, let the master machine be the first machine to have access to local data sets \u03b2 \u03b2, y1iuni 1. We consider the following two baseline estimators of the minimizer \u03b2 of (1,1). The local estimator ignores data available on other machines and calculates \u03b21nn that local estimation errors min \u03b21nn, i 1'py1i, xx1i, \u03b2yq, \u03b2 | | 1 (1,2) using locally available data. The local method is efficient in both communication and calculation, but the resulting estimation error is large compared to an estimator that uses all available data. The other idealized basis is the centralized estimator that we wish we could use the arg, min \u03b21mnm, j, 1n, i, pyji, xxxji, \u03b2yq, \u03b2yq, \u00b2."}, {"heading": "1.2 Related Work", "text": "Due to their importance, there is a large amount of literature on the distributed optimization of modern massive datasets. See for example (Dekel et al., 2012; Duchi et al., 2012; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2015; Jaggi et al., 2014; Shamir et al., 2015; Shamir et al., 2014; Shamir et al., 2014; Zhang et al., 2015; Zhang and Xiao et al., 2015; Arjevani and Shamir, 2015; Jaggi et al., 2014; Shamir et al., 2014; Shamir and Srebro."}, {"heading": "2 Methodology", "text": "(1) The algorithms 1, 2, 3, 4, 5, 5, 6, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8"}, {"heading": "3 Theoretical Results", "text": "11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,"}, {"heading": "3.1 Sketch of Proof", "text": "First of all, we analyze how the number of miscalculations caused by an estimation error decreases after a communication round, i.e. how the number of miscalculations after a communication round decreases, i.e. how the number of miscalculations after a communication round decreases, i.e. how the number of miscalculations after a communication round decreases, i.e. how the number of miscalculations after a communication round decreases, i.e. how the number of miscalculations after a communication round decreases, i.e. how the number of miscalculations after a communication round decreases, i.e. how the number of miscalculations after a communication round decreases, i.e. how the number of miscalculations after a communication round decreases."}, {"heading": "4 Illustrative Examples", "text": "In this section we will discuss some representative examples of high-dimensional statistical learning problems that have been extensively studied in recent years. To obtain guarantees for the proposed algorithm for these problems, we will first apply the definition of the subgaussian standard (Vershynin, 2012).Definition 4.1 (subgaussian standard).The subgaussian standard | X | | 2 of a subgaussian p-dimensional random vector X is defined as | X | | 2 sup xPSp 1 sup q 1 {2pE | xX, xy | qq1 {q, where Sp 1 is the p-dimensional sphere of unity."}, {"heading": "4.1 Sparse Linear Regression", "text": "It is easy to see that square loss is most common in high-dimensional statistics (Bu \ufffd hlmann and van de Geer, 2011)."}, {"heading": "4.2 Sparse Logistic Regression", "text": "Logistic regression is a popular classification model in which the binary label yji P t 1, \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 q is drawn according to a Bernoulli distribution. (4.4) For logistic models, the maximum probability leads to logistic loss function pyji, x\u03b2, xjiq 1 exppxxji, \u03b2 yq 1. (4.4) For high-dimensional problems in which we obtain the 1 regulated logistic regression model (Zhu and Hastie, 2004; Wu et al., 2009): \u03b2 centralize arg min \u03b21mn, jPrms."}, {"heading": "4.2.1 High-dimensional Generalized Linear Models", "text": "The results are easily extendable to other high-dimensional, generalized linear models (McCullagh and Nelder, 1989; van de Geer, 2008), in which the response variable yji P Y is taken from the distribution Ppyji | xjiq9 exp yjixxji, \u03b2 y \u03a6pxxji, \u03b2 yqAp\u03c3q, where \u03a6p q is a linkage function and Ap\u03c3q a scale parameter. As long as the loss function has a Lipschitz gradient, the algorithm and the corresponding estimation error are bound and applied."}, {"heading": "4.3 High-dimensional Graphical Models", "text": "The results can also be used for the distributed, unattended learning environment, where the task is to learn a sparse graphical structure that represents conditional independence between variables. Graphic models are Gaussian graphical models (Meinshausen and Bu \ufffd hlmann, 2006; Yuan and Lin, 2007) for continuous data and Ising graphical models (Ravikumar et al., 2010) for binary observations. As shown in (Meinshausen and Bu \ufffd hlmann, 2006; Ravikumar et al., 2010), these model selection problems can be reduced to solving parallel '1 regulated linear regression or logistic regression problems."}, {"heading": "5 Experiments", "text": "In this section, we present extensive comparisons between different approaches on both simulated and real data sets. We execute the algorithms for both distributed regression and classification problems and output the solution. This approach is obviously communication-free. \u2022 Centralize: The master collects all the data from different machines and solves a centralized '1 regulated loss minimization problem with the optimal \u03bb and issues the solution.This approach is communication-intensive, as all data must be communicated, but it usually gives us the best estimation and prediction capability. \u2022 Prox GD: Distributed' 1 regulated loss minimization is executed on the '1 regulated target, where we initialized the starting point with the solution of the first machine. \u2022 Avg-Debias: the method proposed in Lee is prohibitive."}, {"heading": "5.1 Simulations", "text": "We first explore the algorithms of the simulated data. We generate txjiujPrms, iPrns from the multivariate normal distribution with the mean of zero vector and the covariance matrix \u03a3, which controls the condition of the problem. We vary how it affects the performance of the various methods. We advocate for the well-conditioned framework conditions, namely the problems of regression, which proceed from the standard normal distribution."}, {"heading": "5.2 Real-world Data Evaluation", "text": "In this section, we compare the scant distributed learning algorithms across several real data sets publicly available from the LIBSVM website4 and UCI Machine Learning Repository5. Statistics of these data sets are summarized in Table 2, where some of the multi-class classification data sets are adopted under the regression setting with square losses. For all data, we use 60% of them for training and 20% as held validation for tuning parameters, and the remaining 20% for testing. We test 10 random partitions of training, validation and test sets, and report the averaged performance of the test data sets. For evaluation tasks, the standardized Mean Squared Error (standardized MSE) is for the classification tasks that we point to the data error."}, {"heading": "6 Conclusion and Discussion", "text": "Our theoretical analysis showed that the proposed method works under weaker conditions than the averaging of the neutral estimator. Furthermore, the estimation error can be improved over a few rounds of calculations as the additional error concept decreases exponentially until it matches the centralized method. Extensive experiments with simulated and real data show that the proposed method improves performance via a one-shot averaging approach within a few rounds of communication. There could be several ways to further improve this work. As we see in real data experiments, the proposed approach can still work slightly worse than the centralized approach for certain data sets. It is interesting to study how EDSL can be proven to work under even weaker assumptions. For example, EDSL requires Ops2 log pq samples per machine to adapt the centralized method in Oplogmq rounds of communication."}, {"heading": "A Appendix", "text": "& & # 8222; & # 8220; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & & # 10; & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & & & # 10; & # 10; & & # 10; & # 10; & & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & & & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & & # 10; & & & & # 10; & & & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & & & & & & & # 10; & & &"}], "references": [{"title": "Communication complexity of distributed convex learning and optimization", "author": ["Y. Arjevani", "O. Shamir"], "venue": "ArXiv e-prints,", "citeRegEx": "Arjevani and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Arjevani and Shamir.", "year": 2015}, {"title": "Distributed learning, communication complexity and privacy", "author": ["M.-F. Balcan", "A. Blum", "S. Fine", "Y. Mansour"], "venue": "JMLR W&CP 23: COLT 2012,", "citeRegEx": "Balcan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2012}, {"title": "Distributed estimation and inference with statistical guarantees", "author": ["H. Battey", "J. Fan", "H. Liu", "J. Lu", "Z. Zhu"], "venue": "ArXiv e-prints,", "citeRegEx": "Battey et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Battey et al\\.", "year": 2015}, {"title": "Simultaneous analysis of lasso and Dantzig selector", "author": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"], "venue": "Ann. Stat.,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S.P. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Communication lower bounds for statistical estimation problems via a distributed data processing inequality", "author": ["M. Braverman", "A. Garg", "T. Ma", "H.L. Nguyen", "D.P. Woodruff"], "venue": "ArXiv e-prints,", "citeRegEx": "Braverman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Braverman et al\\.", "year": 2015}, {"title": "Statistics for high-dimensional data", "author": ["P. B\u00fchlmann", "S.A. van de Geer"], "venue": null, "citeRegEx": "B\u00fchlmann and Geer.,? \\Q2011\\E", "shortCiteRegEx": "B\u00fchlmann and Geer.", "year": 2011}, {"title": "Computational limits of divide-and-conquer method", "author": ["G. Cheng", "Z. Shang"], "venue": "ArXiv e-prints,", "citeRegEx": "Cheng and Shang.,? \\Q2015\\E", "shortCiteRegEx": "Cheng and Shang.", "year": 2015}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "Dual averaging for distributed optimization: convergence analysis and network scaling", "author": ["J.C. Duchi", "A. Agarwal", "M.J. Wainwright"], "venue": "IEEE Trans. Automat. Control,", "citeRegEx": "Duchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2012}, {"title": "Optimality guarantees for distributed statistical estimation", "author": ["J.C. Duchi", "M.I. Jordan", "M.J. Wainwright", "Y. Zhang"], "venue": "ArXiv e-prints,", "citeRegEx": "Duchi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2014}, {"title": "Statistical learning with sparsity: the lasso and generalizations", "author": ["T.J. Hastie", "R.J. Tibshirani", "M.J. Wainwright"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2015}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "J. Am. Stat. Assoc.,", "citeRegEx": "Hoeffding.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding.", "year": 1963}, {"title": "A distributed one-step estimator", "author": ["C. Huang", "X. Huo"], "venue": "ArXiv e-prints,", "citeRegEx": "Huang and Huo.,? \\Q2015\\E", "shortCiteRegEx": "Huang and Huo.", "year": 2015}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["M. Jaggi", "V. Smith", "M. Tak\u00e1c", "J. Terhorst", "S. Krishnan", "T. Hofmann", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jaggi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaggi et al\\.", "year": 2014}, {"title": "Confidence intervals and hypothesis testing for high-dimensional regression", "author": ["A. Javanmard", "A. Montanari"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Javanmard and Montanari.,? \\Q2014\\E", "shortCiteRegEx": "Javanmard and Montanari.", "year": 2014}, {"title": "Communication-efficient distributed statistical learning", "author": ["M.I. Jordan", "J.D. Lee", "Y. Yang"], "venue": "Working Paper,", "citeRegEx": "Jordan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 2016}, {"title": "Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity", "author": ["J.D. Lee", "Q. Lin", "T. Ma", "T. Yang"], "venue": "ArXiv e-prints,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Communication-efficient sparse regression: a one-shot approach", "author": ["J.D. Lee", "Y. Sun", "Q. Liu", "J.E. Taylor"], "venue": "ArXiv e-prints,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Nonparametric heterogeneity testing for massive data", "author": ["J. Lu", "G. Cheng", "H. Liu"], "venue": "ArXiv e-prints,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Adding vs. averaging in distributed primal-dual optimization", "author": ["C. Ma", "V. Smith", "M. Jaggi", "M.I. Jordan", "P. Richtrik", "M. Tak"], "venue": "ArXiv e-prints,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Generalized linear models. Monographs on Statistics and Applied Probability", "author": ["P. McCullagh", "J.A. Nelder"], "venue": null, "citeRegEx": "McCullagh and Nelder.,? \\Q1989\\E", "shortCiteRegEx": "McCullagh and Nelder.", "year": 1989}, {"title": "Efficient large-scale distributed training of conditional maximum entropy models", "author": ["R. Mcdonald", "M. Mohri", "N. Silberman", "D. Walker", "G.S. Mann"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mcdonald et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mcdonald et al\\.", "year": 2009}, {"title": "High dimensional graphs and variable selection with the lasso", "author": ["N. Meinshausen", "P. B\u00fchlmann"], "venue": "Ann. Stat.,", "citeRegEx": "Meinshausen and B\u00fchlmann.,? \\Q2006\\E", "shortCiteRegEx": "Meinshausen and B\u00fchlmann.", "year": 2006}, {"title": "Lasso-type recovery of sparse representations for high-dimensional data", "author": ["N. Meinshausen", "B. Yu"], "venue": "Ann. Stat.,", "citeRegEx": "Meinshausen and Yu.,? \\Q2009\\E", "shortCiteRegEx": "Meinshausen and Yu.", "year": 2009}, {"title": "A unified framework for highdimensional analysis of m-estimators with decomposable regularizers", "author": ["S.N. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Stat. Sci.,", "citeRegEx": "Negahban et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Negahban et al\\.", "year": 2012}, {"title": "A method of solving a convex programming problem with convergence rate op1{k2q", "author": ["Y. Nesterov"], "venue": "In Soviet Mathematics Doklady,", "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Restricted eigenvalue properties for correlated gaussian designs", "author": ["G. Raskutti", "M.J. Wainwright", "B. Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Raskutti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raskutti et al\\.", "year": 2010}, {"title": "High-dimensional ising model selection using `1-regularized logistic regression", "author": ["P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty"], "venue": "Ann. Stat.,", "citeRegEx": "Ravikumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2010}, {"title": "On the optimality of averaging in distributed statistical learning", "author": ["J. Rosenblatt", "B. Nadler"], "venue": "ArXiv e-prints,", "citeRegEx": "Rosenblatt and Nadler.,? \\Q2014\\E", "shortCiteRegEx": "Rosenblatt and Nadler.", "year": 2014}, {"title": "Reconstruction from anisotropic random measurements", "author": ["M. Rudelson", "S. Zhou"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Rudelson and Zhou.,? \\Q2013\\E", "shortCiteRegEx": "Rudelson and Zhou.", "year": 2013}, {"title": "Distributed stochastic optimization and learning", "author": ["O. Shamir", "N. Srebro"], "venue": "In 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton),", "citeRegEx": "Shamir and Srebro.,? \\Q2014\\E", "shortCiteRegEx": "Shamir and Srebro.", "year": 2014}, {"title": "Communication efficient distributed optimization using an approximate newton-type method", "author": ["O. Shamir", "N. Srebro", "T. Zhang"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Shamir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2014}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R.J. Tibshirani"], "venue": "J. R. Stat. Soc. B,", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "High-dimensional generalized linear models and the lasso", "author": ["S.A. van de Geer"], "venue": "Ann. Stat.,", "citeRegEx": "Geer.,? \\Q2008\\E", "shortCiteRegEx": "Geer.", "year": 2008}, {"title": "On the conditions used to prove oracle results for the lasso", "author": ["S.A. van de Geer", "P. B\u00fchlmann"], "venue": "Electron. J. Stat.,", "citeRegEx": "Geer and B\u00fchlmann.,? \\Q2009\\E", "shortCiteRegEx": "Geer and B\u00fchlmann.", "year": 2009}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": null, "citeRegEx": "Vershynin.,? \\Q2012\\E", "shortCiteRegEx": "Vershynin.", "year": 2012}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1constrained quadratic programming (lasso)", "author": ["M.J. Wainwright"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "Wainwright.,? \\Q2009\\E", "shortCiteRegEx": "Wainwright.", "year": 2009}, {"title": "Distributed multitask learning", "author": ["J. Wang", "M. Kolar", "N. Srebro"], "venue": "ArXiv e-prints,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Genome-wide association analysis by lasso penalized logistic regression", "author": ["T.T. Wu", "Y.F. Chen", "T.J. Hastie", "E. Sobel", "K.L. Lange"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["T. Yang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Yang.,? \\Q2013\\E", "shortCiteRegEx": "Yang.", "year": 2013}, {"title": "Model selection and estimation in the gaussian graphical model", "author": ["M. Yuan", "Y. Lin"], "venue": null, "citeRegEx": "Yuan and Lin.,? \\Q2007\\E", "shortCiteRegEx": "Yuan and Lin.", "year": 2007}, {"title": "Confidence intervals for low dimensional parameters in high dimensional linear models", "author": ["C.-H. Zhang", "S.S. Zhang"], "venue": "J. R. Stat. Soc. B,", "citeRegEx": "Zhang and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Zhang and Zhang.", "year": 2013}, {"title": "Communication-efficient distributed optimization of self-concordant empirical loss", "author": ["Y. Zhang", "L. Xiao"], "venue": "ArXiv e-prints,", "citeRegEx": "Zhang and Xiao.,? \\Q2015\\E", "shortCiteRegEx": "Zhang and Xiao.", "year": 2015}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Y. Zhang", "M.J. Wainwright", "J.C. Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Information-theoretic lower bounds for distributed statistical estimation with communication constraints", "author": ["Y. Zhang", "J.C. Duchi", "M.I. Jordan", "M.J. Wainwright"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Y. Zhang", "J.C. Duchi", "M.J. Wainwright"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Divide and conquer kernel ridge regression: A distributed algorithm with minimax optimal rates", "author": ["Y. Zhang", "J.C. Duchi", "M.J. Wainwright"], "venue": "arXiv preprint arXiv:1305.5029,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "A partially linear framework for massive heterogeneous data", "author": ["T. Zhao", "G. Cheng", "H. Liu"], "venue": "ArXiv e-prints,", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "A general framework for robust testing and confidence regions in high-dimensional quantile regression", "author": ["T. Zhao", "M. Kolar", "H. Liu"], "venue": "ArXiv e-prints,", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "Classification of gene microarrays by penalized logistic regression", "author": ["J. Zhu", "T.J. Hastie"], "venue": null, "citeRegEx": "Zhu and Hastie.,? \\Q2004\\E", "shortCiteRegEx": "Zhu and Hastie.", "year": 2004}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "A.J. Smola", "L. Li"], "venue": "In Advances in Neural Information Processing,", "citeRegEx": "Zinkevich et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 11, "context": "Learning a sparse \u03b2 in a high-dimensional setting is a well studied statistical problem (B\u00fchlmann and van de Geer, 2011; Hastie et al., 2015), however, it creates unique computational challenges in the distributed setting that we address here.", "startOffset": 88, "endOffset": 141}, {"referenceID": 31, "context": "In the paper, we assume the communication occurs in rounds, where in each round, machines exchange messages with the master machine and, between two rounds, the machines only compute based on their local information, which includes local data points and messages received before (Zhang et al., 2013b; Shamir and Srebro, 2014; Arjevani and Shamir, 2015).", "startOffset": 279, "endOffset": 352}, {"referenceID": 0, "context": "In the paper, we assume the communication occurs in rounds, where in each round, machines exchange messages with the master machine and, between two rounds, the machines only compute based on their local information, which includes local data points and messages received before (Zhang et al., 2013b; Shamir and Srebro, 2014; Arjevani and Shamir, 2015).", "startOffset": 279, "endOffset": 352}, {"referenceID": 42, "context": "(2015b) studied a one-shot approach to learning \u03b2 based on averaging the debiased lasso estimators (Avg-Debias) (Zhang and Zhang, 2013).", "startOffset": 112, "endOffset": 135}, {"referenceID": 42, "context": "In particular, the debiasing step requires solving Oppq generalized lasso problems, which is computationally prohibitive for high-dimensional problems (Zhang and Zhang, 2013; Javanmard and Montanari, 2014).", "startOffset": 151, "endOffset": 205}, {"referenceID": 15, "context": "In particular, the debiasing step requires solving Oppq generalized lasso problems, which is computationally prohibitive for high-dimensional problems (Zhang and Zhang, 2013; Javanmard and Montanari, 2014).", "startOffset": 151, "endOffset": 205}, {"referenceID": 16, "context": "In a related setting, Lee et al. (2015b) studied a one-shot approach to learning \u03b2 based on averaging the debiased lasso estimators (Avg-Debias) (Zhang and Zhang, 2013).", "startOffset": 22, "endOffset": 41}, {"referenceID": 8, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 51, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 4, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 1, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 40, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 14, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 20, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 31, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 43, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 0, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 22, "context": "formed locally by different machines (Mcdonald et al., 2009; Zinkevich et al., 2010; Zhang et al., 2012; Huang and Huo, 2015).", "startOffset": 37, "endOffset": 125}, {"referenceID": 51, "context": "formed locally by different machines (Mcdonald et al., 2009; Zinkevich et al., 2010; Zhang et al., 2012; Huang and Huo, 2015).", "startOffset": 37, "endOffset": 125}, {"referenceID": 44, "context": "formed locally by different machines (Mcdonald et al., 2009; Zinkevich et al., 2010; Zhang et al., 2012; Huang and Huo, 2015).", "startOffset": 37, "endOffset": 125}, {"referenceID": 13, "context": "formed locally by different machines (Mcdonald et al., 2009; Zinkevich et al., 2010; Zhang et al., 2012; Huang and Huo, 2015).", "startOffset": 37, "endOffset": 125}, {"referenceID": 7, "context": "Divide-and-conquer procedures also found applications in statistical inference (Zhao et al., 2014a; Cheng and Shang, 2015; Lu et al., 2016).", "startOffset": 79, "endOffset": 139}, {"referenceID": 19, "context": "Divide-and-conquer procedures also found applications in statistical inference (Zhao et al., 2014a; Cheng and Shang, 2015; Lu et al., 2016).", "startOffset": 79, "endOffset": 139}, {"referenceID": 26, "context": "However, their communication complexity bounds have a bad dependence on the condition number, resulting in a procedure which is not better than first-order approaches in terms of communication, such as (proximal) accelerated gradient descent (Nesterov, 1983).", "startOffset": 242, "endOffset": 258}, {"referenceID": 4, "context": "(2014) and Zhang and Xiao (2015) proposed truly communication-efficient distributed optimization algorithms which leveraged the local second-order information, resulting in milder dependence on the condition number, compared to the first-order approaches (Boyd et al., 2011; Shamir and Srebro, 2014; Ma et al., 2015).", "startOffset": 255, "endOffset": 316}, {"referenceID": 31, "context": "(2014) and Zhang and Xiao (2015) proposed truly communication-efficient distributed optimization algorithms which leveraged the local second-order information, resulting in milder dependence on the condition number, compared to the first-order approaches (Boyd et al., 2011; Shamir and Srebro, 2014; Ma et al., 2015).", "startOffset": 255, "endOffset": 316}, {"referenceID": 20, "context": "(2014) and Zhang and Xiao (2015) proposed truly communication-efficient distributed optimization algorithms which leveraged the local second-order information, resulting in milder dependence on the condition number, compared to the first-order approaches (Boyd et al., 2011; Shamir and Srebro, 2014; Ma et al., 2015).", "startOffset": 255, "endOffset": 316}, {"referenceID": 5, "context": "Lower bounds were studied in (Zhang et al., 2013a; Braverman et al., 2015; Arjevani and Shamir, 2015).", "startOffset": 29, "endOffset": 101}, {"referenceID": 0, "context": "Lower bounds were studied in (Zhang et al., 2013a; Braverman et al., 2015; Arjevani and Shamir, 2015).", "startOffset": 29, "endOffset": 101}, {"referenceID": 38, "context": "(2015b) for distributed estimation, multi-task learning (Wang et al., 2015), and statistical inference (Battey et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 2, "context": ", 2015), and statistical inference (Battey et al., 2015).", "startOffset": 35, "endOffset": 56}, {"referenceID": 16, "context": "Concurrent work of (Jordan et al., 2016) (personal communication) also improved computational efficiency of Lee et al.", "startOffset": 19, "endOffset": 40}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions.", "startOffset": 9, "endOffset": 75}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions.", "startOffset": 9, "endOffset": 108}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions. Yang (2013), Jaggi et al.", "startOffset": 9, "endOffset": 237}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions. Yang (2013), Jaggi et al. (2014), and Ma et al.", "startOffset": 9, "endOffset": 258}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions. Yang (2013), Jaggi et al. (2014), and Ma et al. (2015) studied distributed optimization using stochastic dual coordinate descent.", "startOffset": 9, "endOffset": 280}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions. Yang (2013), Jaggi et al. (2014), and Ma et al. (2015) studied distributed optimization using stochastic dual coordinate descent. These approaches try to find a good balance between computation and communication. However, their communication complexity bounds have a bad dependence on the condition number, resulting in a procedure which is not better than first-order approaches in terms of communication, such as (proximal) accelerated gradient descent (Nesterov, 1983). Shamir et al. (2014) and Zhang and Xiao (2015) proposed truly communication-efficient distributed optimization algorithms which leveraged the local second-order information, resulting in milder dependence on the condition number, compared to the first-order approaches (Boyd et al.", "startOffset": 9, "endOffset": 719}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions. Yang (2013), Jaggi et al. (2014), and Ma et al. (2015) studied distributed optimization using stochastic dual coordinate descent. These approaches try to find a good balance between computation and communication. However, their communication complexity bounds have a bad dependence on the condition number, resulting in a procedure which is not better than first-order approaches in terms of communication, such as (proximal) accelerated gradient descent (Nesterov, 1983). Shamir et al. (2014) and Zhang and Xiao (2015) proposed truly communication-efficient distributed optimization algorithms which leveraged the local second-order information, resulting in milder dependence on the condition number, compared to the first-order approaches (Boyd et al.", "startOffset": 9, "endOffset": 745}, {"referenceID": 0, "context": ", 2015; Arjevani and Shamir, 2015). However, it is not clear how to extend these approaches with non-smooth objectives, for example, the `1 regularized problems. Most of the above mentioned work is focused on estimators that are (asymptotically) linear. Averaging at the end reduces the variance of these estimators, resulting in an estimator that matches the performance of centralized procedure. With `2 regularization, Zhang et al. (2013c) studied the averaging the estimations with weaker regularization to avoid the large bias problem, under kernel ridge regression setting.", "startOffset": 8, "endOffset": 443}, {"referenceID": 0, "context": ", 2015; Arjevani and Shamir, 2015). However, it is not clear how to extend these approaches with non-smooth objectives, for example, the `1 regularized problems. Most of the above mentioned work is focused on estimators that are (asymptotically) linear. Averaging at the end reduces the variance of these estimators, resulting in an estimator that matches the performance of centralized procedure. With `2 regularization, Zhang et al. (2013c) studied the averaging the estimations with weaker regularization to avoid the large bias problem, under kernel ridge regression setting. The situation in a high-dimensional setting is not so straightforward, due to the biased induced by the sparsity inducing penalty. Zhao et al. (2014b) illustrated how averaging debiased composite quantile regression estimators can be used for efficient inference in a highdimensional setting.", "startOffset": 8, "endOffset": 731}, {"referenceID": 0, "context": ", 2015; Arjevani and Shamir, 2015). However, it is not clear how to extend these approaches with non-smooth objectives, for example, the `1 regularized problems. Most of the above mentioned work is focused on estimators that are (asymptotically) linear. Averaging at the end reduces the variance of these estimators, resulting in an estimator that matches the performance of centralized procedure. With `2 regularization, Zhang et al. (2013c) studied the averaging the estimations with weaker regularization to avoid the large bias problem, under kernel ridge regression setting. The situation in a high-dimensional setting is not so straightforward, due to the biased induced by the sparsity inducing penalty. Zhao et al. (2014b) illustrated how averaging debiased composite quantile regression estimators can be used for efficient inference in a highdimensional setting. Averaging debiased high-dimensional estimators was subsequently used in Lee et al. (2015b) for distributed estimation, multi-task learning (Wang et al.", "startOffset": 8, "endOffset": 964}, {"referenceID": 0, "context": ", 2015; Arjevani and Shamir, 2015). However, it is not clear how to extend these approaches with non-smooth objectives, for example, the `1 regularized problems. Most of the above mentioned work is focused on estimators that are (asymptotically) linear. Averaging at the end reduces the variance of these estimators, resulting in an estimator that matches the performance of centralized procedure. With `2 regularization, Zhang et al. (2013c) studied the averaging the estimations with weaker regularization to avoid the large bias problem, under kernel ridge regression setting. The situation in a high-dimensional setting is not so straightforward, due to the biased induced by the sparsity inducing penalty. Zhao et al. (2014b) illustrated how averaging debiased composite quantile regression estimators can be used for efficient inference in a highdimensional setting. Averaging debiased high-dimensional estimators was subsequently used in Lee et al. (2015b) for distributed estimation, multi-task learning (Wang et al., 2015), and statistical inference (Battey et al., 2015). Concurrent work of (Jordan et al., 2016) (personal communication) also improved computational efficiency of Lee et al. (2015b) using ideas of Shamir et al.", "startOffset": 8, "endOffset": 1209}, {"referenceID": 0, "context": ", 2015; Arjevani and Shamir, 2015). However, it is not clear how to extend these approaches with non-smooth objectives, for example, the `1 regularized problems. Most of the above mentioned work is focused on estimators that are (asymptotically) linear. Averaging at the end reduces the variance of these estimators, resulting in an estimator that matches the performance of centralized procedure. With `2 regularization, Zhang et al. (2013c) studied the averaging the estimations with weaker regularization to avoid the large bias problem, under kernel ridge regression setting. The situation in a high-dimensional setting is not so straightforward, due to the biased induced by the sparsity inducing penalty. Zhao et al. (2014b) illustrated how averaging debiased composite quantile regression estimators can be used for efficient inference in a highdimensional setting. Averaging debiased high-dimensional estimators was subsequently used in Lee et al. (2015b) for distributed estimation, multi-task learning (Wang et al., 2015), and statistical inference (Battey et al., 2015). Concurrent work of (Jordan et al., 2016) (personal communication) also improved computational efficiency of Lee et al. (2015b) using ideas of Shamir et al. (2014).", "startOffset": 8, "endOffset": 1245}, {"referenceID": 32, "context": "2) is inspired by the proposal in Shamir et al. (2014), where the authors studied distributed optimization for smooth and strongly convex empirical objectives.", "startOffset": 34, "endOffset": 55}, {"referenceID": 25, "context": "Our analysis also require the notion of restricted strong convexity (Negahban et al., 2012).", "startOffset": 68, "endOffset": 91}, {"referenceID": 25, "context": "The restricted strong convexity is an assumption used for showing consistent estimation in highdimensions (van de Geer and B\u00fchlmann, 2009; Negahban et al., 2012).", "startOffset": 106, "endOffset": 161}, {"referenceID": 25, "context": "3 following the proof strategy as in Negahban et al. (2012).", "startOffset": 37, "endOffset": 60}, {"referenceID": 36, "context": "For completeness, we first provide the definition of subgaussian norm (Vershynin, 2012).", "startOffset": 70, "endOffset": 87}, {"referenceID": 33, "context": "For the regression problem, the typical choice of loss function is the squared loss `pyji, x\u03b2,xjiyq 1 2pyji x\u03b2,xjiyq, and adding a `1 penalty to the empirical loss leads to the lasso estimator (Tibshirani, 1996)", "startOffset": 193, "endOffset": 211}, {"referenceID": 30, "context": "2) with high probability as long as n \u00c1 s log p (Rudelson and Zhou, 2013).", "startOffset": 48, "endOffset": 73}, {"referenceID": 37, "context": "The following `1 error bound is standard for lasso with random design, which was established, for example, in (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009) Lemma 4.", "startOffset": 110, "endOffset": 175}, {"referenceID": 24, "context": "The following `1 error bound is standard for lasso with random design, which was established, for example, in (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009) Lemma 4.", "startOffset": 110, "endOffset": 175}, {"referenceID": 3, "context": "The following `1 error bound is standard for lasso with random design, which was established, for example, in (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009) Lemma 4.", "startOffset": 110, "endOffset": 175}, {"referenceID": 37, "context": "We compare these bounds to the performance of local and centralized lasso (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009).", "startOffset": 74, "endOffset": 139}, {"referenceID": 24, "context": "We compare these bounds to the performance of local and centralized lasso (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009).", "startOffset": 74, "endOffset": 139}, {"referenceID": 3, "context": "We compare these bounds to the performance of local and centralized lasso (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009).", "startOffset": 74, "endOffset": 139}, {"referenceID": 17, "context": "These bounds match the results in Lee et al. (2015b). Furthermore, when m \u00c0 n s2 log p , match the performance for centralized lasso.", "startOffset": 34, "endOffset": 53}, {"referenceID": 50, "context": "a `1 regularization, we obtain the `1 regularized logistic regression model (Zhu and Hastie, 2004; Wu et al., 2009):", "startOffset": 76, "endOffset": 115}, {"referenceID": 39, "context": "a `1 regularization, we obtain the `1 regularized logistic regression model (Zhu and Hastie, 2004; Wu et al., 2009):", "startOffset": 76, "endOffset": 115}, {"referenceID": 25, "context": "The logistic loss is 14 -smooth, let Ljp\u03b2q 1 n \u00b0 iPrns logp1 expp yjix\u03b2,xjiyqq, (Negahban et al., 2012) showed that if xji are drawn from mean zero distribution with sub-Gaussian tails, then L1p\u03b2q satisfies the restricted strong condition (3.", "startOffset": 80, "endOffset": 103}, {"referenceID": 25, "context": "The following `1 error bound states the estimation error for logistic regression with `1 regularization, which was established, for example, in (van de Geer, 2008; Negahban et al., 2012).", "startOffset": 144, "endOffset": 186}, {"referenceID": 21, "context": "1 High-dimensional Generalized Linear Models The results are readily extendable to other high-dimensional generalized linear models (McCullagh and Nelder, 1989; van de Geer, 2008), where the response variable yji P Y is drawn from the distribution Ppyji|xjiq9 exp yjixxji,\u03b2 y \u03a6pxxji,\u03b2 yq Ap\u03c3q , where \u03a6p q is a link function and Ap\u03c3q is a scale parameter.", "startOffset": 132, "endOffset": 179}, {"referenceID": 23, "context": "Widely studied graphical models are Gaussian graphical models (Meinshausen and B\u00fchlmann, 2006; Yuan and Lin, 2007) for continuous data and Ising graphical models (Ravikumar et al.", "startOffset": 62, "endOffset": 114}, {"referenceID": 41, "context": "Widely studied graphical models are Gaussian graphical models (Meinshausen and B\u00fchlmann, 2006; Yuan and Lin, 2007) for continuous data and Ising graphical models (Ravikumar et al.", "startOffset": 62, "endOffset": 114}, {"referenceID": 28, "context": "Widely studied graphical models are Gaussian graphical models (Meinshausen and B\u00fchlmann, 2006; Yuan and Lin, 2007) for continuous data and Ising graphical models (Ravikumar et al., 2010) for binary observations.", "startOffset": 162, "endOffset": 186}, {"referenceID": 23, "context": "As shown in (Meinshausen and B\u00fchlmann, 2006; Ravikumar et al., 2010), these model selection problems can be reduced to solving parallel `1 regularized linear regression and logistic regression problems, respectively.", "startOffset": 12, "endOffset": 68}, {"referenceID": 28, "context": "As shown in (Meinshausen and B\u00fchlmann, 2006; Ravikumar et al., 2010), these model selection problems can be reduced to solving parallel `1 regularized linear regression and logistic regression problems, respectively.", "startOffset": 12, "endOffset": 68}, {"referenceID": 17, "context": "\u2022 Avg-Debias: the method proposed in Lee et al. (2015b), with fine tuned regularization and hard thresholding parameters.", "startOffset": 37, "endOffset": 56}, {"referenceID": 38, "context": "Last but not least, it is interesting to explore the ideas presented to improve the computational cost of communicationefficient distributed multi-task learning with shared support (Wang et al., 2015).", "startOffset": 181, "endOffset": 200}, {"referenceID": 25, "context": "The proof uses ideas presented in (Negahban et al., 2012).", "startOffset": 34, "endOffset": 57}, {"referenceID": 36, "context": "10 in (Vershynin, 2012)) and an union bound over rps leads to the desired bound.", "startOffset": 6, "endOffset": 23}, {"referenceID": 36, "context": "10 in (Vershynin, 2012)) and an union bound over j P rms, i P rns and rps leads to the desired bound.", "startOffset": 6, "endOffset": 23}, {"referenceID": 12, "context": "then apply Azuma-Hoeffding inequality (Hoeffding, 1963) and an union bound over rps leads to the desired bound.", "startOffset": 38, "endOffset": 55}], "year": 2016, "abstractText": "We propose a novel, efficient approach for distributed sparse learning in high-dimensions, where observations are randomly partitioned across machines. Computationally, at each round our method only requires the master machine to solve a shifted `1 regularized M-estimation problem, and other workers to compute the gradient on local data. In respect of communication, the proposed approach provably matches the estimation error bound of centralized methods within constant rounds of communications (ignoring logarithmic factors). We conduct extensive experiments on both simulated and real world datasets, and demonstrate encouraging performances on high-dimensional regression and classification tasks.", "creator": "LaTeX with hyperref package"}}}