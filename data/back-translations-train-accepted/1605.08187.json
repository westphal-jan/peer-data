{"id": "1605.08187", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "The Symbolic Interior Point Method", "abstract": "A recent trend in probabilistic inference emphasizes the codification of models in a formal syntax, with suitable high-level features such as individuals, relations, and connectives, enabling descriptive clarity, succinctness and circumventing the need for the modeler to engineer a custom solver. Unfortunately, bringing these linguistic and pragmatic benefits to numerical optimization has proven surprisingly challenging. In this paper, we turn to these challenges: we introduce a rich modeling language, for which an interior-point method computes approximate solutions in a generic way. While logical features easily complicates the underlying model, often yielding intricate dependencies, we exploit and cache local structure using algebraic decision diagrams (ADDs). Indeed, standard matrix-vector algebra is efficiently realizable in ADDs, but we argue and show that well-known optimization methods are not ideal for ADDs. Our engine, therefore, invokes a sophisticated matrix-free approach. We demonstrate the flexibility of the resulting symbolic-numeric optimizer on decision making and compressed sensing tasks with millions of non-zero entries.", "histories": [["v1", "Thu, 26 May 2016 08:26:34 GMT  (613kb,D)", "https://arxiv.org/abs/1605.08187v1", null], ["v2", "Sat, 28 May 2016 17:11:30 GMT  (745kb,D)", "http://arxiv.org/abs/1605.08187v2", null], ["v3", "Tue, 14 Jun 2016 18:29:14 GMT  (746kb,D)", "http://arxiv.org/abs/1605.08187v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LO cs.SC", "authors": ["martin mladenov", "vaishak belle", "kristian kersting"], "accepted": true, "id": "1605.08187"}, "pdf": {"name": "1605.08187.pdf", "metadata": {"source": "CRF", "title": "The Symbolic Interior Point Method", "authors": ["Martin Mladenov", "Vaishak Belle"], "emails": ["martin.mladenov@cs.tu-dortmund.de", "vaishak@cs.kuleuven.be", "kristian.kersting@cs.tu-dortmund.de"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Lineage", "text": "Expressivity in modeling languages for numerical optimization is a focal point of many proposals, e.g. [14, 15]. However, they blur the boundary between declarative and imperative paradigms, using object sets to index LP variables, and do not embody logical thinking. Disciplined programming [7] allows an object-oriented approach to constructing optimization problems, but falls short of our previously stated desires. Starting from statistical relational learning processes [16, 17] we argue for algebraic modeling, which is fully integrated into the classical logical machinery (and not just logical programming [18]), allowing the specification of correlations, groups, and properties in a natural way, as observed elsewhere [19, 20]. The efficiency of ADDs for matrix-vector algebra has been established in [12]. Specifically, the use of ADDs for compact specification (and resolution of Markov's work) as the symbolic representation of transitions and transition processes [22] is the detail of our decision-making process."}, {"heading": "3 Primer on Logic and Decision Diagrams", "text": "We cover some of the logical premises in this section. To prepare ourselves for the syntax of our high-level mathematical programming language, we recapitulate basic concepts from mathematical logic [24]. A propositional language L consists of a finite number of propositions P = {p,.., q}, from which formulas are built with connectors {{p, q}. An L model M is a {0, 1} mapping to the symbols in P, which is essentially extended to complex formulas. For example, if P = {p, q} and M = < p = 1, q = 0 >, we have M | = p \u00b2 mapping to the symbols in P, which are essentially extended to complex formulas. The logical language of the finite freedom of function of first-level logic consists of an endless number of predicate symbols {P (x),."}, {"heading": "4 First-Order Logical Quadratic Programming", "text": "A convex program (QP) is an optimization problem over space Rn, that is, we want to find a real weighted vector x-Rn from the solution set of a system of linear inequalities. (This means that a convex square objective function f (x) = xTQx + cTx from the solution of a system of linear inequalities (x) provides a complementary (dual) QP-D solution of QP-D provides an upper limit to the minimum of QP, for the maximizer of QP-D this limit is narrow. In this paper we assume that QP and QP-D can be reduced to the following standard QP: minimizes cTx + 1 / 2xTQxTQxar to Ax = b, x-DUAL-QP: maxize bT y y \u2212 Qx = 0.2 x."}, {"heading": "5 Solution Strategies for First-Order Logical QPs", "text": "Considering the language of representation, let us now turn to the solution of logical QPs. To prepare ourselves for discussion, let us establish an elementary notion of algorithmic correctness for ADD implementations. We assume that in the face of a first-order logical mathematical program, we have the ADDs for A, b, and c in our hands. Then, we can show: Theorem 2: Suppose A, b, and c are as above, and e is any arithmetic expression about them that includes standard matrix binary operators. Then, there is a sequence of operations on their ADDs that yield a function h, so that h =. The types of expressions we have in mind are e = Ax \u2212 b (which corresponds to the rest in the corresponding system of linear equations). The proof is as follows: For each f, g: {0} n \u2192 R, and (default) n \u2192 R, and (standard) binary matrix operators (multiplication, subtraction, subtraction)."}, {"heading": "5.1 A Naive Ground-and-Solve Method", "text": "The simplest approach to solving a first-order logical QP (FOQP) is to reduce the problem to its normal form and use a standard solution for QPs, such as an internal scoring method, an advanced lagrange method, or a form of active quantity method, such as a generalized simplex. The correctness of this solution strategy is guaranteed by the semantics of the first-order constraints: in general, any FOQP degree can be returned to its normal form. Although this method would work, it has a significant disadvantage, since the optimizer cannot use knowledge of the symbolic structure of the problem. That is, even if the problem is compiled into a very small ADD, the runtime of the optimizer (at best linear) depends on the number of non-zeros in the soil matrix. It is clear that large dense problems (1) will be completely intractable."}, {"heading": "5.2 The Symbolic Interior Point Method", "text": "In this section, we will construct a solver that automatically exploits the symbolic structure of FOQP by invoking the strengths of ADD representation. Readers should note that much of this discussion is based on problems that have a considerable logical structure, as is often the case in real word problems that involve relationships and properties. Recall from the previous discussion, which in the presence of cache, (compact) ADDs leads to very fast matrix vector multiplications. Furthermore, vector operations are efficient, implying that an ADD for x and the compilation of residuals y = Ax \u2212 b is more efficient than its matrix counterpart [11]."}, {"heading": "6 Empirical Illustration", "text": "There are three main questions we would like to examine, namely: (Q1) in the presence of the symbolic structure, is our ADD-based solver substantially better than its matrix-based counterpart? (Q2) To evaluate the performance of the approach, we have implemented the entire pipeline described here, that is, a symbolic environment to specify QPs, a compiler to ADDs based on the popular CUDD package, and the symbolic interior-point method used in the previous section. To address Q1 and Q2, we applied the symbolic IPM function to the problem of compiling the decision processes used in a Markov family. These MDPs concern an agent whose task is to connect and connect two objects."}, {"heading": "7 Conclusions", "text": "A long-standing goal of machine learning and artificial intelligence, which is also reflected in the philosophy of democratizing data, is to make the specification and solution of real-world problems simple and natural, possibly even for non-experts. To this end, we have considered first-order logical mathematical programs that support individuals, relationships, and relationships, and developed a new direction of research to symbolically solve these programs in a generic manner. In our case, a matrix-free method of inner points was advocated. Our empirical results demonstrate the flexibility of this line of research. The most interesting way for future work is to explore richer modeling languages paired with more powerful circuit representations. Acknowledgements The authors thank the anonymous reviews for their feedback. The work was partially supported by the DFG Collaborative Research Centre CRC 876, Project A6."}], "references": [{"title": "The interplay of optimization and machine learning research", "author": ["K.P. Bennett", "E. Parrado-Hern\u00e1ndez"], "venue": "JMLR, vol. 7, pp. 1265\u20131281, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming, 1st ed", "author": ["L. M"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Machine learning: a probabilistic perspective", "author": ["K. Murphy"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["B. Milch", "B. Marthi", "S.J. Russell", "D. Sontag", "D.L. Ong", "A. Kolobov"], "venue": "Proc. IJCAI, 2005, pp. 1352\u20131359.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine learning, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Random-world semantics and syntactic independence for expressive languages", "author": ["D. McAllester", "B. Milch", "N.D. Goodman"], "venue": "MIT, Tech. Rep. MIT-CSAIL-TR-2008-025, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Graph implementations for nonsmooth convex programs", "author": ["M. Grant", "S. Boyd"], "venue": "Recent Advances in Learning and Control, ser. Lecture Notes in Control and Information Sciences, 2008, pp. 95\u2013110.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "A few useful things to know about machine learning", "author": ["P.M. Domingos"], "venue": "Commun. ACM, vol. 55, no. 10, pp. 78\u201387, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Recursive decomposition for nonconvex optimization", "author": ["A. Friesen", "P. Domingos"], "venue": "IJCAI, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "On probabilistic inference by weighted model counting", "author": ["M. Chavira", "A. Darwiche"], "venue": "Artif. Intell., vol. 172, no. 6-7, pp. 772\u2013799, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-terminal binary decision diagrams: An efficient data structure for matrix representation", "author": ["M. Fujita", "P. McGeer", "J.-Y. Yang"], "venue": "Formal Methods in System Design, vol. 10, no. 2, pp. 149\u2013169, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Matrix-free interior point method", "author": ["J. Gondzio"], "venue": "Comp. Opt. and Appl., vol. 51, no. 2, pp. 457\u2013480, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "AMPL: A Mathematical Programming Language", "author": ["R. Fourer", "D.M. Gay", "B.W. Kernighan"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Unifying logical and statistical AI", "author": ["P. Domingos", "S. Kok", "H. Poon", "M. Richardson", "P. Singla"], "venue": "Proc. AAAI, 2006, pp. 2\u20137.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Global inference for entity and relation identification via a linear programming formulation", "author": ["W. Yih", "D. Roth"], "venue": "An Introduction to Statistical Relational Learning. MIT Press, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Algebraic modeling in a deductive database language", "author": ["D. Klabjan", "R. Fourer", "J. Ma"], "venue": "11th INFORMS Computing Society Conference, 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Relational linear programming", "author": ["K. Kersting", "M. Mladenov", "P. Tokmakov"], "venue": "Artificial Intelligence Journal (AIJ), vol. OnlineFirst, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "First-order mixed integer linear programming", "author": ["G. Gordon", "S. Hong", "M. Dud\u00edk"], "venue": "UAI, 2009, pp. 213\u2013222.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Spudd: Stochastic planning using decision diagrams", "author": ["J. Hoey", "R. St-Aubin", "A. Hu", "C. Boutilier"], "venue": "UAI, 1999, pp. 279\u2013288.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Robust optimization for hybrid mdps with state-dependent noise", "author": ["Z. Zamani", "S. Sanner", "K.V. Delgado", "L.N. de Barros"], "venue": "IJCAI, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Online symbolic gradient-based optimization for factored action mdps", "author": ["H. Cui", "R. Khardon"], "venue": "IJCAI, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "A mathematical introduction to logic", "author": ["H. Enderton"], "venue": "Academic press New York,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1972}, {"title": "Graph-based algorithms for boolean function manipulation", "author": ["R.E. Bryant"], "venue": "Computers, IEEE Transactions on, vol. 100, no. 8, pp. 677\u2013691, 1986.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1986}, {"title": "AMPL : a modeling language for mathematical programming", "author": ["R. Fourer", "D.M. Gay", "B.W. Kernighan"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1993}, {"title": "Interior-point methods", "author": ["F. Potra", "S.J. Wright"], "venue": "Journal of Computational and Applied Mathematics, vol. 124, pp. 281\u2013302, 2000.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Matrix Computations (3rd Ed.)", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1996}, {"title": "Matrix-free interior point method for compressed sensing problems", "author": ["K. Fountoulakis", "J. Gondzio", "P. Zhlobich"], "venue": "Mathematical Programming Computation, vol. 6, no. 1, pp. 1\u201331, 2014. 10", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It can seen as an assembly language for hard combinatorial problems ranging from classification and regression in learning, to computing optimal policies and equilibria in decision theory, to entropy minimization in information sciences [1, 2, 3].", "startOffset": 237, "endOffset": 246}, {"referenceID": 1, "context": "It can seen as an assembly language for hard combinatorial problems ranging from classification and regression in learning, to computing optimal policies and equilibria in decision theory, to entropy minimization in information sciences [1, 2, 3].", "startOffset": 237, "endOffset": 246}, {"referenceID": 2, "context": "It can seen as an assembly language for hard combinatorial problems ranging from classification and regression in learning, to computing optimal policies and equilibria in decision theory, to entropy minimization in information sciences [1, 2, 3].", "startOffset": 237, "endOffset": 246}, {"referenceID": 3, "context": "An emerging discipline in probabilistic inference emphasizes the codification of models in a suitable formal syntax [4, 5, 6], enabling the rapid prototyping and solving of complex probabilistic structures.", "startOffset": 116, "endOffset": 125}, {"referenceID": 4, "context": "An emerging discipline in probabilistic inference emphasizes the codification of models in a suitable formal syntax [4, 5, 6], enabling the rapid prototyping and solving of complex probabilistic structures.", "startOffset": 116, "endOffset": 125}, {"referenceID": 5, "context": "An emerging discipline in probabilistic inference emphasizes the codification of models in a suitable formal syntax [4, 5, 6], enabling the rapid prototyping and solving of complex probabilistic structures.", "startOffset": 116, "endOffset": 125}, {"referenceID": 6, "context": "The approach taken in recent influential proposals, such as disciplined programming [7], is to carefully constrain the specification language so as to provide a structured interface between the model and the solver, by means of which geometric properties such as the curvature of the objective can be inferred.", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "Doing so has the potential to greatly simplify the specification and prototyping of AI and ML models [8, 1].", "startOffset": 101, "endOffset": 107}, {"referenceID": 0, "context": "Doing so has the potential to greatly simplify the specification and prototyping of AI and ML models [8, 1].", "startOffset": 101, "endOffset": 107}, {"referenceID": 8, "context": "Local structure, on the other hand, can be exploited [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 9, "context": "In probabilistic inference, local structure has enabled tractability in high-treewidth models, culminating in the promising direction of using circuits and decision-diagrams for the underlying graphical model [10].", "startOffset": 209, "endOffset": 213}, {"referenceID": 10, "context": "Here, we would like to consider the idea of using such a data structure for optimization, for which we turn to early pioneering research on algebraic decision diagrams (ADDs) that supports efficient matrix manipulations (compositionality) and caching of submatrices (repeated local structure) [11, 12].", "startOffset": 293, "endOffset": 301}, {"referenceID": 11, "context": "We employ ideas from the matrix-free interior point method [13], which appeals to an iterative linear equation solver together with the log-barrier method, to achieve a regime where the constraint matrix is only accessed through matrix-vector multiplications.", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "[14, 15].", "startOffset": 0, "endOffset": 8}, {"referenceID": 6, "context": "Disciplined programming [7] enables an object-oriented approach to constructing optimization problems, but falls short of our desiderata as argued before.", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "Taking our cue from statistical relational learning [16, 17], we argue for algebraic modeling that is fully integrated with classical logical machinery (and not just logic programming [18]).", "startOffset": 52, "endOffset": 60}, {"referenceID": 14, "context": "Taking our cue from statistical relational learning [16, 17], we argue for algebraic modeling that is fully integrated with classical logical machinery (and not just logic programming [18]).", "startOffset": 52, "endOffset": 60}, {"referenceID": 15, "context": "Taking our cue from statistical relational learning [16, 17], we argue for algebraic modeling that is fully integrated with classical logical machinery (and not just logic programming [18]).", "startOffset": 184, "endOffset": 188}, {"referenceID": 16, "context": "This allows the specification of correlations, groups and properties in a natural manner, as also observed elsewhere [19, 20].", "startOffset": 117, "endOffset": 125}, {"referenceID": 17, "context": "This allows the specification of correlations, groups and properties in a natural manner, as also observed elsewhere [19, 20].", "startOffset": 117, "endOffset": 125}, {"referenceID": 18, "context": "representing transitions and rewards as Boolean functions) was popularized in [21]; see [22, 23] for recent offerings.", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "representing transitions and rewards as Boolean functions) was popularized in [21]; see [22, 23] for recent offerings.", "startOffset": 88, "endOffset": 96}, {"referenceID": 20, "context": "representing transitions and rewards as Boolean functions) was popularized in [21]; see [22, 23] for recent offerings.", "startOffset": 88, "endOffset": 96}, {"referenceID": 21, "context": "To prepare for the syntax of our highlevel mathematical programming language, we recap basic notions from mathematical logic [24].", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "statistical relational learning [16].", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "A BDD [25] is a compact and efficiently manipulable data structure for a Boolean function f : {0, 1} \u2192 {0, 1} .", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "ADDs generalize BDDs in representing functions of the form {0, 1} \u2192 R, and so inherit the same structural properties as BDDs except, of course, that terminal nodes are labeled with real numbers [11, 12].", "startOffset": 194, "endOffset": 202}, {"referenceID": 23, "context": "While (some) high-level features are prominent in many optimization packages such as AMPL [26], they are reduced (naively) to canonical forms transparently to the user, and do not embody logical reasoning.", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "Logic programming is supported in other proposals [18], but their restricted use of negation makes it difficult to understand the implications when modeling a domain.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "We take our cue from statistical relational learning [16], as considered in [19], to support any (finite) fragment of first-order logic with classical interpretations for operators.", "startOffset": 53, "endOffset": 57}, {"referenceID": 16, "context": "We take our cue from statistical relational learning [16], as considered in [19], to support any (finite) fragment of first-order logic with classical interpretations for operators.", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "(Quantifiers are eliminated as usual: existentials as disjunctions and universals as conjunctions [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "To guide the construction of the engine, we will briefly go over the operations previously established as efficient with ADDs [11, 12], and some implications thereof for a solver strategy.", "startOffset": 126, "endOffset": 134}, {"referenceID": 10, "context": "Theorem 3: [11, 12] Suppose A,A\u2032 are real-valued matrices.", "startOffset": 11, "endOffset": 19}, {"referenceID": 10, "context": "We refer interested readers to [11, 12] for the complexity-theoretic properties of these operations.", "startOffset": 31, "endOffset": 39}, {"referenceID": 10, "context": "Moreover, from Corollary 4, vector operations are efficiently implementable, which implies that given an ADD for A, x and b, the computation of the residual y = Ax\u2212b is more efficient than its matrix counterpart [11].", "startOffset": 212, "endOffset": 216}, {"referenceID": 24, "context": "[27], sketched in Alg.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "The underlying idea is as follows [27, 13]: by applying a perturbed Newton method to the equalities in the above system, the algorithm progresses the current solution along a direction obtained by solving the following linear system: \uf8ee\uf8f0 A 0 0 \u2212Q A I", "startOffset": 34, "endOffset": 42}, {"referenceID": 11, "context": "The underlying idea is as follows [27, 13]: by applying a perturbed Newton method to the equalities in the above system, the algorithm progresses the current solution along a direction obtained by solving the following linear system: \uf8ee\uf8f0 A 0 0 \u2212Q A I", "startOffset": 34, "endOffset": 42}, {"referenceID": 24, "context": "where \u0398 is the diagonal matrix \u0398ii = xi si [27, 13].", "startOffset": 43, "endOffset": 51}, {"referenceID": 11, "context": "where \u0398 is the diagonal matrix \u0398ii = xi si [27, 13].", "startOffset": 43, "endOffset": 51}, {"referenceID": 11, "context": "(The general case of going beyond these assumptions is omitted here for space reasons [13].", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "To solve (2), we employ the conjugate gradient method [28], sketched in Fig.", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "To remedy this situation, Gondzio [13] proposes the following approach: first, the system can be regularized to achieve a condition number bounded by the largest singular value of A.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "More details on this can be found in [13].", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "As demonstrated in [13] this approach does lead to a practical algorithm.", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "To address Q1 and Q2, we applied the symbolic IPM on the problem of computing the value function of a family of Markov decision processes used in [21].", "startOffset": 146, "endOffset": 150}, {"referenceID": 26, "context": "We apply the Symbolic IPM to the BPDN reformulation of [29], with the Walsh matrix specified symbolically, recovering random sparse vectors.", "startOffset": 55, "endOffset": 59}, {"referenceID": 26, "context": "Problem Statistics Symbolic IPM FWHT IPM [29] n m time[s] time[s] 2 2 8.", "startOffset": 41, "endOffset": 45}], "year": 2016, "abstractText": "A recent trend in probabilistic inference emphasizes the codification of models in a formal syntax, with suitable high-level features such as individuals, relations, and connectives, enabling descriptive clarity, succinctness and circumventing the need for the modeler to engineer a custom solver. Unfortunately, bringing these linguistic and pragmatic benefits to numerical optimization has proven surprisingly challenging. In this paper, we turn to these challenges: we introduce a rich modeling language, for which an interior-point method computes approximate solutions in a generic way. While logical features easily complicates the underlying model, often yielding intricate dependencies, we exploit and cache local structure using algebraic decision diagrams (ADDs). Indeed, standard matrix-vector algebra is efficiently realizable in ADDs, but we argue and show that well-known second-order methods are not ideal for ADDs. Our engine, therefore, invokes a sophisticated matrix-free approach. We demonstrate the flexibility of the resulting symbolic-numeric optimizer on decision making and compressed sensing tasks with millions of non-zero entries.", "creator": "LaTeX with hyperref package"}}}