{"id": "1605.08257", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Low-rank tensor completion: a Riemannian manifold preconditioning approach", "abstract": "We propose a novel Riemannian manifold preconditioning approach for the tensor completion problem with rank constraint. A novel Riemannian metric or inner product is proposed that exploits the least-squares structure of the cost function and takes into account the structured symmetry that exists in Tucker decomposition. The specific metric allows to use the versatile framework of Riemannian optimization on quotient manifolds to develop preconditioned nonlinear conjugate gradient and stochastic gradient descent algorithms for batch and online setups, respectively. Concrete matrix representations of various optimization-related ingredients are listed. Numerical comparisons suggest that our proposed algorithms robustly outperform state-of-the-art algorithms across different synthetic and real-world datasets.", "histories": [["v1", "Thu, 26 May 2016 12:55:02 GMT  (4088kb,D)", "http://arxiv.org/abs/1605.08257v1", "The 33rd International Conference on Machine Learning (ICML 2016). arXiv admin note: substantial text overlap witharXiv:1506.02159"]], "COMMENTS": "The 33rd International Conference on Machine Learning (ICML 2016). arXiv admin note: substantial text overlap witharXiv:1506.02159", "reviews": [], "SUBJECTS": "cs.LG cs.NA math.OC stat.ML", "authors": ["hiroyuki kasai", "bamdev mishra"], "accepted": true, "id": "1605.08257"}, "pdf": {"name": "1605.08257.pdf", "metadata": {"source": "CRF", "title": "Low-rank tensor completion: a Riemannian manifold preconditioning approach", "authors": ["Hiroyuki Kasai", "Bamdev Mishra"], "emails": ["kasai@is.uec.ac.jp", "bamdevm@amazon.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Exploiting the problem structure", "text": "To this end, we focus on two basic structures in (1): symmetry in the constraints and the least-square structure of the cost function. (2) The symmetry structure in Tucker decomposition (1), where Ud St (rd, nd) for d, 2, 3} belongs to the boot manifolds of size nd-Rd with orthogonal columns and G, Rr1 \u00d7 r2 \u00d7 r2 \u00d7 r3 [12]."}, {"heading": "3 Notions of manifold optimization", "text": "The question that arises is whether this is a specific compatibility between the belt manic structure of M and the belt manic structure, i.e. a limited optimization problem that, for example, (1) is conceptually transformed into an unrestricted optimization via the belt manic quotient multiplicity (5). Below, we will briefly show the development of various geometric objects that are necessary to optimize a smooth cost function via the quotient multiplicity (5)."}, {"heading": "4 Riemannian algorithms for (1)", "text": "This year it has come to the point where it will be able to get to the top of the leaderboard, \"he told the German Press Agency.\" We have to get to the top, \"he said.\" We have to get to the top, \"he said."}, {"heading": "5 Numerical comparisons", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "6 Conclusion", "text": "We have proposed pre-conditioned batch algorithms (conjugate gradients) and online algorithms (stochastic gradient descendence) for the problem of tensor completion, based on the Rieman preconditioning approach, which utilizes the basic structures of symmetry (due to the non-uniqueness of Tucker decomposition) and the least square cost function. A novel Rieman metric (inner product) is proposed, which allows the use of the versatile Rieman optimization framework. Numerical comparisons suggest that our proposed algorithms perform superior on different benchmarks."}, {"heading": "Acknowledgments", "text": "We thank Rodolphe Sepulchre, Paul Van Dooren and Nicolas Boumal for useful discussions about the paper. This paper presents research results of the Belgian network DYSCO (Dynamical Systems, Control, and Optimization), funded by the Belgian state-initiated Interuniversity Attraction Poles Program, Science Policy Office. Scientific responsibility lies with its authors. Hiroyuki Kasai is (partially) supported by the Japanese Ministry of Internal Affairs and Communications as SCOPE project (150201002), which was initiated when Bamdev Mishra was at the Department of Electrical Engineering and Computer Science of the University of Lie ge, 4000 Lie ge, Belgium and visited the Department of Engineering (Control Group) of the University of Cambridge, Cambridge, UK. He was supported as a research fellow (aspirant) of the Belgian National Fund for Scientific Research (FNRS). 3http: / / perception.i2r.a-star.du.sg / bk _ bindexmodel / indexindexx.html"}, {"heading": "A Proof and derivation of manifold-related ingredients", "text": "Specific calculations of optimization-related ingredients in the work are discussed below: < < < < (r1, n1) < (r2, n2, n3, n3) < (r2, n3, n3) < (r2, U3O3, G). (U1O1, U3O3, G). (U1O1, U3O3, G). (UOT1, UOT2, UOT3, G). (UOT2, UOT3). (UOT2, UOT3). (rd). (UOT2, UOT3). (rd). (UOT2). (UOT2). (UOT2). (UOT2). (UOT2). (UOT2). (UOT2). (UOT2). (UOT2). (UOT2). (UOT2). (UOT2. (.). (UOT2). (. (.)."}, {"heading": "B Additional numerical comparisons", "text": "In fact, it is the case that we are able to find a solution that is capable of finding a solution that is capable of finding a solution and that is able to find a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution."}], "references": [{"title": "Optimization Algorithms on Matrix Manifolds", "author": ["Absil", "P.-A", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Stochastic gradient descent on Riemannian manifolds", "author": ["S. Bonnabel"], "venue": "IEEE Trans. Autom. Control,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Stochastic gradient descent tricks", "author": ["L. Bottou"], "venue": "Neural Networks: Tricks of the Trade (2nd ed.),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Low-rank matrix completion via preconditioned optimization on the Grassmann manifold", "author": ["N. Boumal", "Absil", "P.-A"], "venue": "Linear Algebra Appl.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Manopt: a Matlab toolbox for optimization on manifolds", "author": ["N. Boumal", "B. Mishra", "Absil", "P.-A", "R. Sepulchre"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Found. Comput. Math.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "The geometry of algorithms with orthogonality constraints", "author": ["A. Edelman", "T.A. Arias", "S.T. Smith"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Tucker factorization with missing data with application to low-n-rank tensor completion", "author": ["M. Filipovi\u0107", "A. Juki\u0107"], "venue": "Multidim. Syst. Sign. P.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Information limits on neural identification of colored surfaces in natural scenes", "author": ["D.H. Foster", "S.M.C. Nascimento", "K. Amano"], "venue": "Visual Neurosci.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Online low-rank tensor subspace tracking from incomplete data by CP decomposition using recursive least squares", "author": ["H. Kasai"], "venue": "In IEEE ICASSP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Riemannian preconditioning for tensor completion", "author": ["Kasai", "Hiroyuki", "Mishra", "Bamdev"], "venue": "Technical report, arXiv preprint arXiv:1506.02159,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM Rev.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Low-rank tensor completion by Riemannian optimization", "author": ["D. Kressner", "M. Steinlechner", "B. Vandereycken"], "venue": "BIT Numer. Math.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Introduction to smooth manifolds, volume 218 of Graduate Texts in Mathematics", "author": ["J.M. Lee"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Subspace learning and imputation for streaming big data matrices and tensors", "author": ["M. Mardani", "G. Mateos", "G.B. Giannakis"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "R3MC: A Riemannian three-factor algorithm for low-rank matrix completion", "author": ["B. Mishra", "R. Sepulchre"], "venue": "In IEEE CDC, pp", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Scaled gradients on Grassmann manifolds for matrix completion", "author": ["T. Ngo", "Y. Saad"], "venue": "In NIPS, pp", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Numerical Optimization, volume Second Edition", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Optimization methods on Riemannian manifolds and their application to shape space", "author": ["W. Ring", "B. Wirth"], "venue": "SIAM J. Optim.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "A new, globally convergent Riemannian conjugate gradient method", "author": ["H. Sato", "T. Iwai"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Tensor versus matrix completion: A comparison with application to spectral data", "author": ["M. Signoretto", "Plas", "R.V. d", "B.D. Moor", "J.A.K. Suykens"], "venue": "IEEE Signal Process. Lett.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Learning with tensors: a framework based on convex optimization and spectral regularization", "author": ["M. Signoretto", "Q.T. Dinh", "L.D. Lathauwer", "J.A.K. Suykens"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Estimation of low-rank tensors via convex optimization", "author": ["R. Tomioka", "K. Hayashi", "H. Kashima"], "venue": "Technical report, arXiv preprint arXiv:1010.0789,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Low-rank matrix completion by Riemannian optimization", "author": ["B. Vandereycken"], "venue": "SIAM J. Optim.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Problem (1) has many variants, and one of those is extending the nuclear norm regularization approach from the matrix case [6] to the tensor case.", "startOffset": 123, "endOffset": 126}, {"referenceID": 14, "context": "While this generalization leads to good results [15, 25, 24], its applicability to large-scale instances is not trivial, especially due to the necessity of high-dimensional singular value decomposition computations.", "startOffset": 48, "endOffset": 60}, {"referenceID": 23, "context": "While this generalization leads to good results [15, 25, 24], its applicability to large-scale instances is not trivial, especially due to the necessity of high-dimensional singular value decomposition computations.", "startOffset": 48, "endOffset": 60}, {"referenceID": 22, "context": "While this generalization leads to good results [15, 25, 24], its applicability to large-scale instances is not trivial, especially due to the necessity of high-dimensional singular value decomposition computations.", "startOffset": 48, "endOffset": 60}, {"referenceID": 7, "context": ", in [8, 13].", "startOffset": 5, "endOffset": 12}, {"referenceID": 12, "context": ", in [8, 13].", "startOffset": 5, "endOffset": 12}, {"referenceID": 10, "context": "This paper extends the earlier work [11] to include a stochastic gradient descent algorithm for low-rank tensor completion.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "The multilinear rank constraint forms a smooth manifold [13].", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "While preconditioning in unconstrained optimization is well studied [20, Chapter 5], preconditioning on constraints with symmetries, owing to non-uniqueness of Tucker decomposition [12], is not straightforward.", "startOffset": 181, "endOffset": 185}, {"referenceID": 0, "context": "We build upon the recent work [18] that suggests to use preconditioning with a tailored metric (inner product) in the Riemannian optimization framework on quotient manifolds [1, 7, 18].", "startOffset": 174, "endOffset": 184}, {"referenceID": 6, "context": "We build upon the recent work [18] that suggests to use preconditioning with a tailored metric (inner product) in the Riemannian optimization framework on quotient manifolds [1, 7, 18].", "startOffset": 174, "endOffset": 184}, {"referenceID": 12, "context": "[13], which also exploits the manifold structure, are twofold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] exploit the search space as an embedded submanifold of the Euclidean space, whereas we view it as a product of simpler search spaces with symmetries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] work with the standard Euclidean metric, whereas we use a metric that is tuned to the least-squares cost function, thereby inducing a preconditioning effect.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "They also connect to state-of-the-art algorithms proposed in [19, 27, 17, 4].", "startOffset": 61, "endOffset": 76}, {"referenceID": 16, "context": "They also connect to state-of-the-art algorithms proposed in [19, 27, 17, 4].", "startOffset": 61, "endOffset": 76}, {"referenceID": 3, "context": "They also connect to state-of-the-art algorithms proposed in [19, 27, 17, 4].", "startOffset": 61, "endOffset": 76}, {"referenceID": 4, "context": "Our proposed algorithms are implemented in the Matlab toolbox Manopt [5].", "startOffset": 69, "endOffset": 72}, {"referenceID": 11, "context": "The Tucker decomposition of a tensor X \u2208 Rn1\u00d7n2\u00d7n3 of rank r (=(r1, r2, r3)) is X = G\u00d71U1\u00d72U2\u00d73U3, (2) where Ud \u2208 St(rd, nd) for d \u2208 {1, 2, 3} belongs to the Stiefel manifold of matrices of size nd \u00d7 rd with orthogonal columns and G \u2208 Rr1\u00d7r2\u00d7r3 [12].", "startOffset": 245, "endOffset": 249}, {"referenceID": 13, "context": "(4) The set of equivalence classes is the quotient manifold [14] M/\u223c := M/(O(r1)\u00d7O(r2)\u00d7O(r3)), (5)", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "Consequently, the problem (1) is an optimization problem on a quotient manifold for which systematic procedures are proposed in [1, 7].", "startOffset": 128, "endOffset": 134}, {"referenceID": 6, "context": "Consequently, the problem (1) is an optimization problem on a quotient manifold for which systematic procedures are proposed in [1, 7].", "startOffset": 128, "endOffset": 134}, {"referenceID": 18, "context": "In unconstrained optimization, the Newton method is interpreted as a scaled steepest descent method, where the search space is endowed with a metric (inner product) induced by the Hessian of the cost function [20].", "startOffset": 209, "endOffset": 213}, {"referenceID": 6, "context": ", TxM has the matrix characterization [7] TxM = {(ZU1 ,ZU2 ,ZU3 ,ZG) \u2208 Rn1\u00d7r1 \u00d7 Rn2\u00d7r2 \u00d7 Rn3\u00d7r3 \u00d7 Rr1\u00d7r2\u00d7r3 : Ud ZUd + Z T Ud Ud = 0, for d \u2208 {1, 2, 3}}.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "From [1], endowed with the Riemannian metric (9), the quotient manifoldM/\u223c is a Riemannian submersion ofM.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": ", the gradient of a smooth cost function [1].", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "A retraction is a mapping that maps vectors in the horizontal space to points on the search spaceM and satisfies the local rigidity condition [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 4, "context": "In the batch setting, we use the off-the-shelf conjugate gradient implementation of Manopt for any smooth cost function [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "In the online setting, we use the stochastic gradient descent implementation [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 20, "context": "For fixed rank, theoretical convergence of the Riemannian algorithms are to a stationary point, and the convergence analysis follows from [22, 21, 2].", "startOffset": 138, "endOffset": 149}, {"referenceID": 19, "context": "For fixed rank, theoretical convergence of the Riemannian algorithms are to a stationary point, and the convergence analysis follows from [22, 21, 2].", "startOffset": 138, "endOffset": 149}, {"referenceID": 1, "context": "For fixed rank, theoretical convergence of the Riemannian algorithms are to a stationary point, and the convergence analysis follows from [22, 21, 2].", "startOffset": 138, "endOffset": 149}, {"referenceID": 16, "context": "Following [17, 26, 13], the least-squares structure of the cost function in (1) is exploited to compute a linearized step-size guess efficiently along a search direction by considering a polynomial approximation of degree 2 over the manifold.", "startOffset": 10, "endOffset": 22}, {"referenceID": 24, "context": "Following [17, 26, 13], the least-squares structure of the cost function in (1) is exploited to compute a linearized step-size guess efficiently along a search direction by considering a polynomial approximation of degree 2 over the manifold.", "startOffset": 10, "endOffset": 22}, {"referenceID": 12, "context": "Following [17, 26, 13], the least-squares structure of the cost function in (1) is exploited to compute a linearized step-size guess efficiently along a search direction by considering a polynomial approximation of degree 2 over the manifold.", "startOffset": 10, "endOffset": 22}, {"referenceID": 2, "context": "Following [3], we select \u03b30 in the pre-training phase using a small sample size of a training set.", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "It should be stressed that the computational cost of our conjugate gradient implementation is equal to that of [13].", "startOffset": 111, "endOffset": 115}, {"referenceID": 7, "context": "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.", "startOffset": 184, "endOffset": 187}, {"referenceID": 12, "context": "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.", "startOffset": 199, "endOffset": 203}, {"referenceID": 14, "context": "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.", "startOffset": 276, "endOffset": 280}, {"referenceID": 23, "context": "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.", "startOffset": 289, "endOffset": 293}, {"referenceID": 22, "context": "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.", "startOffset": 304, "endOffset": 308}, {"referenceID": 15, "context": "In the online setting, we compare our proposed stochastic gradient descent algorithm with CANDECOMP/PARAFAC based TeCPSGD [16] and OLSTEC [10].", "startOffset": 122, "endOffset": 126}, {"referenceID": 9, "context": "In the online setting, we compare our proposed stochastic gradient descent algorithm with CANDECOMP/PARAFAC based TeCPSGD [16] and OLSTEC [10].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "Algorithms are initialized randomly, as suggested in [13], and are stopped when either the mean square error (MSE) on the train set \u03a9 is below 10\u221212 or the number of iterations exceeds 250.", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "We evaluate the convergence properties of algorithms under the presence of noise by adding scaled Gaussian noise P\u03a9(E) to P\u03a9(X ?) as in [13].", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "Figure 2(g) shows that the test error for each is almost identical to the \u2016P\u03a9(X )\u2016F [13], but our proposed algorithm converges faster than geomCG.", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "We consider the hyperspectral image \u201cRibeira\u201d [9] discussed in [23, 13].", "startOffset": 46, "endOffset": 49}, {"referenceID": 21, "context": "We consider the hyperspectral image \u201cRibeira\u201d [9] discussed in [23, 13].", "startOffset": 63, "endOffset": 71}, {"referenceID": 12, "context": "We consider the hyperspectral image \u201cRibeira\u201d [9] discussed in [23, 13].", "startOffset": 63, "endOffset": 71}, {"referenceID": 21, "context": "As suggested in [23, 13], we resize it to 203 \u00d7 268 \u00d7 33.", "startOffset": 16, "endOffset": 24}, {"referenceID": 12, "context": "As suggested in [23, 13], we resize it to 203 \u00d7 268 \u00d7 33.", "startOffset": 16, "endOffset": 24}, {"referenceID": 12, "context": "We perform five random samplings of the pixels based on the OS values 11 and 22, corresponding to the rank r=(15, 15, 6) adopted in [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "While OS = 22 corresponds to the observation ratio of 10% studied in [13], OS = 11 considers a challenging scenario with the observation ratio of 5%.", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "We compare the proposed stochastic gradient descent algorithm with its batch counterpart gradient descent algorithm and with TeCPSGD [16] and OLSTEC [10].", "startOffset": 133, "endOffset": 137}, {"referenceID": 9, "context": "We compare the proposed stochastic gradient descent algorithm with its batch counterpart gradient descent algorithm and with TeCPSGD [16] and OLSTEC [10].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "References [1] Absil, P.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Bonnabel, S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Bottou, L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Boumal, N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Boumal, N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Cand\u00e8s, E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Edelman, A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Filipovi\u0107, M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Foster, D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Kasai, H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Kasai, Hiroyuki and Mishra, Bamdev.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Kolda, T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Kressner, D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Lee, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Liu, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Mardani, M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Mishra, B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] Ngo, T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] Nocedal, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] Ring, W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Sato, H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] Signoretto, M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] Signoretto, M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] Tomioka, R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] Vandereycken, B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "From [1], the tangent space has the matrix characterization TxM = {(ZU1 ,ZU2 ,ZU3 ,ZG) \u2208 Rn1\u00d7r1 \u00d7 Rn2\u00d7r2 \u00d7 Rn3\u00d7r3 \u00d7 Rr1\u00d7r2\u00d7r3 : Ud ZUd + Z T Ud Ud = 0, for d \u2208 {1, 2, 3}}.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "Additionally from [1], \u03b7Ud has the characterization \u03b7Ud = Ud\u03a9 + Ud\u22a5K, (A.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "From the characterization of linearization of an orthogonal matrix [1], we have the characterization for the vertical space as Vx = {(U1\u03a91,U2\u03a92,U3\u03a93,\u2212(G\u00d71\u03a91 + G\u00d72\u03a92 + G\u00d73\u03a93)) : \u03a9d \u2208 Rrd\u00d7rd ,\u03a9d = \u2212\u03a9d for d \u2208 {1, 2, 3}}.", "startOffset": 67, "endOffset": 70}], "year": 2016, "abstractText": "We propose a novel Riemannian manifold preconditioning approach for the tensor completion problem with rank constraint. A novel Riemannian metric or inner product is proposed that exploits the least-squares structure of the cost function and takes into account the structured symmetry that exists in Tucker decomposition. The specific metric allows to use the versatile framework of Riemannian optimization on quotient manifolds to develop preconditioned nonlinear conjugate gradient and stochastic gradient descent algorithms for batch and online setups, respectively. Concrete matrix representations of various optimization-related ingredients are listed. Numerical comparisons suggest that our proposed algorithms robustly outperform state-of-the-art algorithms across different synthetic and real-world datasets1.", "creator": "LaTeX with hyperref package"}}}