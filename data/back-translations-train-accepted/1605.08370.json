{"id": "1605.08370", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent", "abstract": "Matrix completion, where we wish to recover a low rank matrix by observing a few entries from it, is a widely studied problem in both theory and practice with wide applications. Most of the provable algorithms so far on this problem have been restricted to the offline setting where they provide an estimate of the unknown matrix using all observations simultaneously. However, in many applications, the online version, where we observe one entry at a time and dynamically update our estimate, is more appealing. While existing algorithms are efficient for the offline setting, they could be highly inefficient for the online setting.", "histories": [["v1", "Thu, 26 May 2016 17:26:18 GMT  (35kb)", "http://arxiv.org/abs/1605.08370v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["chi jin", "sham m kakade", "praneeth netrapalli"], "accepted": true, "id": "1605.08370"}, "pdf": {"name": "1605.08370.pdf", "metadata": {"source": "CRF", "title": "Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent", "authors": ["Chi Jin", "Sham M. Kakade", "Praneeth Netrapalli"], "emails": ["chijin@cs.berkeley.edu", "sham@cs.washington.edu", "praneeth@microsoft.com"], "sections": [{"heading": null, "text": "It's not just the way in which it's about the question of whether it's about a way in which it's about a way in which it's about a way in which it's about a way in which it's about a way in which it's about a way in which it's about a way in which it's about a way in which it's about a way in which it's about a way in which it's about a way and a way in which it's about a way and a way in which it's about a way and a way in which it's about a way and a way in which it's about a way and a way in which it's about a way and a way in which it's about a way and a way in which it's about a way and a way in which it's about a way and a way in which it's about a way and a way in which it's about a way in which it's about and a way in which it's about a way in which it's about a way and a way in which it's about a way in which it's about a way in which it's about a way in which it's about a way and a way in which it's about a way in which it's about a way in which it's about a way and a way in which it's about a way in which it's about a way in which it's about a way and a way in which it's about a way in which it's about a way and a way it's about a way in which it's about a way and a way it's about a way in which it's about a way in which it's about a way and a way it's about a way and a way it's about a way in which it's about a way and a way and a way it's about a way and a way it's about a way and a way it's about a way and a way it's about a way and a way in which it's about a way and a way it's about a way and a way in which it's about a way and a way and a way it's about a way in which it's about a way in which it's about a way and a way in which it's about a way and a way and a way it's about a way and a way and a way and a way and a way it's about a way and a way it's about a way and a way it's about a way"}], "references": [{"title": "Simple, efficient, and neural algorithms for sparse coding", "author": ["Sanjeev Arora", "Rong Ge", "Tengyu Ma", "Ankur Moitra"], "venue": "arXiv preprint arXiv:1503.00778,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Fast online svd revisions for lightweight recommender systems", "author": ["Matthew Brand"], "venue": "In SDM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Phase retrieval via matrix completion", "author": ["Emmanuel J Candes", "Yonina C Eldar", "Thomas Strohmer", "Vladislav Voroninski"], "venue": "SIAM Review,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J. Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "The youtube video recommendation system", "author": ["James Davidson", "Benjamin Liebald", "Junning Liu", "Palash Nandy", "Taylor Van Vleet", "Ullas Gargi", "Sujoy Gupta", "Yu He", "Mike Lambert", "Blake Livingston"], "venue": "In Proceedings of the fourth ACM conference on Recommender systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Global convergence of stochastic gradient descent for some non-convex matrix problems", "author": ["Christopher De Sa", "Kunle Olukotun", "Christopher R\u00e9"], "venue": "arXiv preprint arXiv:1411.1134,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "arXiv preprint arXiv:1503.02101,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Matrix completion has no spurious local minimum", "author": ["Rong Ge", "Jason D. Lee", "Tengyu Ma"], "venue": "arXiv preprint arXiv:1605.07272,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Understanding alternating minimization for matrix completion", "author": ["Marcus Hardt"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Computational limits for matrix completion", "author": ["Moritz Hardt", "Raghu Meka", "Prasad Raghavendra", "Benjamin Weitz"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Computing matrix squareroot via non convex local search", "author": ["Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli"], "venue": "arXiv preprint arXiv:1507.05854,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja\u2019s algorithm", "author": ["Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli", "Aaron Sidford"], "venue": "arXiv preprint arXiv:1602.06929,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Fast exact matrix completion with finite samples", "author": ["Prateek Jain", "Praneeth Netrapalli"], "venue": "arXiv preprint arXiv:1411.1087,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Robust video denoising using low rank matrix completion", "author": ["Hui Ji", "Chaoqiang Liu", "Zuowei Shen", "Yuhong Xu"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Efficient algorithms for collaborative filtering", "author": ["Raghunandan Hulikal Keshavan"], "venue": "PhD thesis, STANFORD UNIVERSITY,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "The BellKor solution to the Netflix grand prize", "author": ["Yehuda Koren"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Low-rank matrix and tensor completion via adaptive sampling", "author": ["Akshay Krishnamurthy", "Aarti Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Gradient descent converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "University of California, Berkeley,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Amazon.com recommendations: item-to-item collaborative filtering", "author": ["G. Linden", "B. Smith", "J. York"], "venue": "IEEE Internet Computing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Incremental collaborative filtering recommender based on regularized matrix factorization", "author": ["Xin Luo", "Yunni Xia", "Qingsheng Zhu"], "venue": "Knowledge-Based Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Gradient descent converges to minimizers: The case of non-isolated critical points", "author": ["Ioannis Panageas", "Georgios Piliouras"], "venue": "arXiv preprint arXiv:1605.00405,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "A simple approach to matrix completion", "author": ["Benjamin Recht"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Parallel stochastic gradient algorithms for large-scale matrix completion", "author": ["Benjamin Recht", "Christopher R\u00e9"], "venue": "Mathematical Programming Computation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Guaranteed matrix completion via nonconvex factorization", "author": ["Ruoyu Sun", "Zhi-Quan Luo"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["Joel A Tropp"], "venue": "Foundations of computational mathematics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "This problem arises in several applications such as video denoising [14], phase retrieval [3] and most famously in movie recommendation engines [16].", "startOffset": 68, "endOffset": 72}, {"referenceID": 2, "context": "This problem arises in several applications such as video denoising [14], phase retrieval [3] and most famously in movie recommendation engines [16].", "startOffset": 90, "endOffset": 93}, {"referenceID": 15, "context": "This problem arises in several applications such as video denoising [14], phase retrieval [3] and most famously in movie recommendation engines [16].", "startOffset": 144, "endOffset": 148}, {"referenceID": 3, "context": "The seminal works of Cand\u00e8s and Recht [4] first identified regularity conditions under which low rank matrix completion can be solved in polynomial time using convex relaxation \u2013 low rank matrix UC Berkeley.", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "completion could be ill-posed and NP-hard in general without such regularity assumptions [10].", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "However in several applications [5, 19], we encounter the online setting where observations are only revealed sequentially and at each step the recovery algorithm is required to maintain an estimate of the low rank matrix based on the observations so far.", "startOffset": 32, "endOffset": 39}, {"referenceID": 18, "context": "However in several applications [5, 19], we encounter the online setting where observations are only revealed sequentially and at each step the recovery algorithm is required to maintain an estimate of the low rank matrix based on the observations so far.", "startOffset": 32, "endOffset": 39}, {"referenceID": 1, "context": "On the other hand, in order to deal with the online matrix completion scenario in practical applications, several heuristics (with no convergence guarantees) have been proposed in literature [2, 20].", "startOffset": 191, "endOffset": 198}, {"referenceID": 19, "context": "On the other hand, in order to deal with the online matrix completion scenario in practical applications, several heuristics (with no convergence guarantees) have been proposed in literature [2, 20].", "startOffset": 191, "endOffset": 198}, {"referenceID": 23, "context": "Moreover, SGD, in the context of matrix completion, is also useful for parallelization and distributed implementation [24].", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "While [25] shows that SGD updates stay away from saddle surfaces, the stepsizes they can handle are quite small (scaling as 1/poly(d1, d2)), leading to suboptimal computational complexity.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "The nuclear norm relaxation algorithm [23] has near-optimal sample complexity for this problem but is computationally expensive.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc.", "startOffset": 83, "endOffset": 98}, {"referenceID": 8, "context": "Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc.", "startOffset": 83, "endOffset": 98}, {"referenceID": 12, "context": "Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc.", "startOffset": 83, "endOffset": 98}, {"referenceID": 24, "context": "Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc.", "startOffset": 83, "endOffset": 98}, {"referenceID": 14, "context": "Our sample complexity is better than that of [15] and is incomparable to those of [9, 13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 8, "context": "Our sample complexity is better than that of [15] and is incomparable to those of [9, 13].", "startOffset": 82, "endOffset": 89}, {"referenceID": 12, "context": "Our sample complexity is better than that of [15] and is incomparable to those of [9, 13].", "startOffset": 82, "endOffset": 89}, {"referenceID": 24, "context": "To the best of our knowledge, the only provable online algorithm for this problem is that of Sun and Luo [25].", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": ", [17, 27].", "startOffset": 2, "endOffset": 10}, {"referenceID": 22, "context": "Nuclear Norm [23] \u00d5(\u03bcdk) \u00d5(d3/ \u221a \u01eb) No Alternating minimization [15] \u00d5(\u03bcdk\u03ba8 log 1 \u01eb ) \u00d5(\u03bcdk2\u03ba8 log 1 \u01eb ) No", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "Nuclear Norm [23] \u00d5(\u03bcdk) \u00d5(d3/ \u221a \u01eb) No Alternating minimization [15] \u00d5(\u03bcdk\u03ba8 log 1 \u01eb ) \u00d5(\u03bcdk2\u03ba8 log 1 \u01eb ) No", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "Alternating minimization [9] \u00d5 ( \u03bcdk2\u03ba2 ( k + log 1 \u01eb )) \u00d5 ( \u03bcdk3\u03ba2 ( k + log 1 \u01eb )) No", "startOffset": 25, "endOffset": 28}, {"referenceID": 12, "context": "Projected gradient descent[13] \u00d5(\u03bcdk) \u00d5(\u03bcdk log 1 \u01eb ) No", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "SGD [25] \u00d5(\u03bcdk\u03ba) poly(\u03bc, d, k, \u03ba) log 1 \u01eb Yes SGD [8]1 d \u00b7 poly(\u03bc, k, \u03ba) poly(\u03bc, d, k, \u03ba, 1 \u01eb ) Yes Our result \u00d5 ( \u03bcdk\u03ba4 ( k + log 1 \u01eb )) \u00d5 ( \u03bcdk4\u03ba4 log 1 \u01eb ) Yes", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "SGD [25] \u00d5(\u03bcdk\u03ba) poly(\u03bc, d, k, \u03ba) log 1 \u01eb Yes SGD [8]1 d \u00b7 poly(\u03bc, k, \u03ba) poly(\u03bc, d, k, \u03ba, 1 \u01eb ) Yes Our result \u00d5 ( \u03bcdk\u03ba4 ( k + log 1 \u01eb )) \u00d5 ( \u03bcdk4\u03ba4 log 1 \u01eb ) Yes", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "eigenvector computation [6, 12], sparse coding [21, 1] etc.", "startOffset": 24, "endOffset": 31}, {"referenceID": 11, "context": "eigenvector computation [6, 12], sparse coding [21, 1] etc.", "startOffset": 24, "endOffset": 31}, {"referenceID": 20, "context": "eigenvector computation [6, 12], sparse coding [21, 1] etc.", "startOffset": 47, "endOffset": 54}, {"referenceID": 0, "context": "eigenvector computation [6, 12], sparse coding [21, 1] etc.", "startOffset": 47, "endOffset": 54}, {"referenceID": 6, "context": "For general non-convex optimization, an interesting line of recent work is that of [7], which proves gradient descent with noise can also escape saddle point, but they only provide polynomial rate without explicit dependence.", "startOffset": 83, "endOffset": 86}, {"referenceID": 17, "context": "Later [18, 22] show that without noise, the space of points from where gradient descent converges to a saddle point is a measure zero set.", "startOffset": 6, "endOffset": 14}, {"referenceID": 21, "context": "Later [18, 22] show that without noise, the space of points from where gradient descent converges to a saddle point is a measure zero set.", "startOffset": 6, "endOffset": 14}, {"referenceID": 10, "context": "Another related piece of work to ours is [11], proves global convergence along with rates of convergence, for the special case of computing matrix squareroot.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "During the preparation of this draft, the recent work [8] was announced which proves the global convergence of SGD for matrix completion and can also be applied to the online setting.", "startOffset": 54, "endOffset": 57}, {"referenceID": 9, "context": "This task is ill-posed and NP-hard in general [10].", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "2 (\u03bc-incoherence[4, 23]).", "startOffset": 16, "endOffset": 23}, {"referenceID": 22, "context": "2 (\u03bc-incoherence[4, 23]).", "startOffset": 16, "endOffset": 23}, {"referenceID": 0, "context": "[1] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Matthew Brand.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Emmanuel J Candes, Yonina C Eldar, Thomas Strohmer, and Vladislav Voroninski.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Emmanuel J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Christopher De Sa, Kunle Olukotun, and Christopher R\u00e9.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Rong Ge, Jason D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Marcus Hardt.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Moritz Hardt, Raghu Meka, Prasad Raghavendra, and Benjamin Weitz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Prateek Jain, Chi Jin, Sham M Kakade, and Praneeth Netrapalli.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Prateek Jain, Chi Jin, Sham M Kakade, Praneeth Netrapalli, and Aaron Sidford.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Prateek Jain and Praneeth Netrapalli.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Hui Ji, Chaoqiang Liu, Zuowei Shen, and Yuhong Xu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Raghunandan Hulikal Keshavan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Yehuda Koren.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Akshay Krishnamurthy and Aarti Singh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Xin Luo, Yunni Xia, and Qingsheng Zhu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Ioannis Panageas and Georgios Piliouras.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Benjamin Recht.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Benjamin Recht and Christopher R\u00e9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Ruoyu Sun and Zhi-Quan Luo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Joel A Tropp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Most of the argument of this section follows from [15].", "startOffset": 50, "endOffset": 54}, {"referenceID": 25, "context": "2 (Matrix Bernstein [26]).", "startOffset": 20, "endOffset": 24}], "year": 2016, "abstractText": "Matrix completion, where we wish to recover a low rank matrix by observing a few entries from it, is a widely studied problem in both theory and practice with wide applications. Most of the provable algorithms so far on this problem have been restricted to the offline setting where they provide an estimate of the unknown matrix using all observations simultaneously. However, in many applications, the online version, where we observe one entry at a time and dynamically update our estimate, is more appealing. While existing algorithms are efficient for the offline setting, they could be highly inefficient for the online setting. In this paper, we propose the first provable, efficient online algorithm for matrix completion. Our algorithm starts from an initial estimate of the matrix and then performs non-convex stochastic gradient descent (SGD). After every observation, it performs a fast update involving only one row of two tall matrices, giving near linear total runtime. Our algorithm can be naturally used in the offline setting as well, where it gives competitive sample complexity and runtime to state of the art algorithms. Our proofs introduce a general framework to show that SGD updates tend to stay away from saddle surfaces and could be of broader interests for other non-convex problems to prove tight rates.", "creator": "LaTeX with hyperref package"}}}