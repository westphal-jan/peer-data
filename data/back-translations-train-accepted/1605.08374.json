{"id": "1605.08374", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Kronecker Determinantal Point Processes", "abstract": "Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set of $N$ items. They have recently gained prominence in several applications that rely on \"diverse\" subsets. However, their applicability to large problems is still limited due to the $\\mathcal O(N^3)$ complexity of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling. But contrary to what one may expect, leveraging the Kronecker product structure for speeding up DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a KronDPP.", "histories": [["v1", "Thu, 26 May 2016 17:33:31 GMT  (220kb,D)", "http://arxiv.org/abs/1605.08374v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["zelda e mariet", "suvrit sra"], "accepted": true, "id": "1605.08374"}, "pdf": {"name": "1605.08374.pdf", "metadata": {"source": "CRF", "title": "Kronecker Determinantal Point Processes", "authors": ["Zelda Mariet"], "emails": ["zelda@csail.mit.edu", "suvrit@mit.edu"], "sections": [{"heading": null, "text": "They have recently grown in importance in several applications that rely on \"diverse\" subsets, but their applicability to major problems is limited due to the complexity of core tasks such as sampling and learning through O (N3). We enable efficient sampling and learning for DPPs by introducing KRONDPP, a DPP model whose kernel matrix decomposes as a tensor product from several smaller kernel matrices. This decomposition immediately enables fast, accurate sampling, but, contrary to what might be expected, using the Kronecker product structure to accelerate DPP learning is proving more difficult. We overcome this challenge and derive batch and stochastic optimization algorithms to efficiently learn the parameters of a KRONDPP."}, {"heading": "1 Introduction", "text": "In fact, most people are able to survive themselves if they go in search of another way. (...) Most of them are able to go in search of another way. (...) Most of them are able to go in search of another way. (...) Most of them are able to go in search of another way. (...) Most of them are able to go in search of another way. (...) Most of them are able to go in search of another way. (...) Most of them are able to start a new life for themselves. (...) Most of them are able to start a new life for themselves. (...) Most of them are able to start a new life for themselves. \""}, {"heading": "2 Preliminaries", "text": "The Kronecker (tensor) product of A-Rp \u00b7 q with B-Rr \u00b7 s two matrices is in fact called pr \u00b7 qs block matrix A B = [aijB] p, qi, j = 1. We denote the aijB block in A B by (A B) (ij) for each valid pair (i, j) and extend the notation to non-Kronecker product matrix to specify the submatrix of size r \u00b7 s at position (i, j). Proposition 2.1. Let A, B, C, D be matrices of sizes so that AC and BD are well defined. Then (i) Trector (Tri) If A, B 0, then A B 0, then A B 0."}, {"heading": "3 Learning the kernel matrix for KRONDPP", "text": "In this section, we will consider the key task for KRONDPPs: learning a Kronecker product kernel matrix from n observed subsets Y1,..., Yn. Using the definition (2) of P (Yi), the most likely learning of a DPP with kernel L leads to the optimization problem: arg max L 0\u03c6 (L), \u03c6 (L) = 1n \u0445 i = 1 (log det (LYi) \u2212 log det (L + I)))). (3) This problem is non-convex and is assumed to be NP-hard [15, guess 4.1]. Furthermore, the constraint L 0 is not trivial to handle. Ui as an indicator matrix for Yi of size N \u00d7 | Yi | so that LYi = U > i LUi guarantees that the gradient of the solution of the solution (L > I LUi) is not easy to recognize."}, {"heading": "3.1 Optimization algorithm", "text": "Our goal is to obtain an efficient algorithm for (local) optimization (3). Beyond its non-convexity, the Kronecker structure L = L1 imposes another constraint on L2. As in [25], we first rewrite \u03c6 as a function of S = L \u2212 1 and rearrange the terms to render them as (S) = log (S) f (S) f (S) + 1n n n i = 1 log det (U > i S \u2212 1Ui) \u2212 log det (I + S) g (S) g f (S) f, while a short argument shows that g \u2212 n \u2212 f \u2212 the convex concave method [29] \u2212 log det is that updating S by solving f (S + 1) and Sfi g (S (k)) is Salive."}, {"heading": "3.1.1 Positive definite iterates and ascent", "text": "For S1 0, S2 0, (S2), (S2), (S2), (S2), (S2), (S2), (S2), (S1), (S1), (S1), (S1), S1, S1, (S1), (S1), (S1), (S1), (S1), (S1), (S1), S1, S1, S1, S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), S1 (1), 1 (1), 1 (1), 1), S1 (1 (1), 1 (1), 1 (1), S1 (1), 1 (1), 1 (1), 1 (1), 1 (1), 1 (1), 1 (1), 1 (1), 1 (1), 1), 1 (1), 1 (1 (1), 1 (1), 1), 1 (1 (1), 1), 1), 1"}, {"heading": "3.1.2 Algorithm and complexity analysis", "text": "From Theorem 3.2 we get algorithm 1 (which differs from the Picard iteration in [25] because it works alternately on each subkernel. It is important to note that further acceleration of algorithm 1 can be achieved by stochastic updates, i.e. instead of calculating the full course of the log probability, we perform our updates with only one (or a small minibatch) subset of Yi in each step, instead of iterating over the entire training set, using the stochastic gradient that takes into account the full course of the log probability."}, {"heading": "3.2 Joint updates", "text": "We have also analyzed the possibility of updating L1 and L2 together: We will update L \u2190 L + L \u0445 L and then restore the Kronecker structure of the kernel by defining the updates L \u2032 1 and L \u2032 2 in such a way that: {(L \"1, L\" 2) minimizes the number of such solutions and calculates from the first singular value and the vectors of the matrix R = [vec ((((L \u2212 1 +) (ij)) >] N1i, j = 1. Note, however, that in this case there is no guaranteed increase in the probability of logging. The pseudocode for the associated algorithm (JOINT-PICARD) is included in Appendix C.1. An analysis similar to the evidence for Thm. 3.3 shows that the updates can be achieved at most (N2 + N2)."}, {"heading": "3.3 Memory-time trade-off", "text": "Although KRONDPPS has tractable learning algorithms, memory requirements for non-stochastic updates remain high, as the matrix of 1 Yi U > i must be subdivided such that {Y1,.., Yn} = 1 Sk s.t., | 1 Yi > i must be stored so that O (N2) memory is required. However, if the training set can be subdivided such that {Y1,.., Yn} = 1Sk s.t., | 1 Yi > i SkY | < z, (9) \u0432 can be disassembled as 1n \u0445 m k = 1 \u0441k, we can then store each item in O (nK1 + mz2 + N) time and O (mz2 + N) distance with minimal storage and updating of L1 and L2."}, {"heading": "4 Sampling", "text": "Sampling exactly (see Alg. 2 and [17]) from a complete DPP kernel costs O (N3 + Nk3), where k is the size of the sampled subset. A major part of the calculation lies in the initial self-decomposition of L; k-orthonormalization costs O (Nk3). Although the self-decomposition has to be done only once for many iterations of the sampling, the exact sampling is still not feasible for large N. Algorithm 2 Sampling from a DPP kernel costs O (Nk3). Input: Matrix L. Self-decomposed L as {(I, vi)} 1 \u2264 i N. J-Prompling for i = 1 to N do J \u2192 J-Probability."}, {"heading": "5 Experimental results", "text": "To validate our learning algorithm, we compared KRK-PICARD with JOINT-PICARD and picard iteration (PICARD) on multiple real and synthetic datasets.1"}, {"heading": "5.1 Synthetic tests", "text": "All three algorithms were used to learn from synthetic data drawn from a \"real\" kernel. However, the partial cores were initialized with Li = X > X, whereby the X coefficients were uniformly extracted from [0, \u221a 2]; for PICARD, L was initialized with L1 L2. For Figures 1a and 1b, the training data from 1All experiments were repeated and averaged five times, whereby MATLAB on a Linux Mint system with 16GB RAM and an i7-4710HQ CPU @ 2.50GHz.sampling was used 100 subsets of the true kernel with equally distributed sizes between 10 and 190. To evaluate KRK-PICARD on matrices that are too large to fit into the memory, while we used samples from a 50 \u00b7 103 \u00d7 50 \u00b7 103 kernel of rank 1, 000 (on average | Yi | \u2248 1, 000) ICPICDARD.- and ICDARD.- (RKAR.PAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KAR.KARD.K.K.KAR.KAR.KK.K.K.K.K.K.K.KI | \u2248 1, 000)"}, {"heading": "5.2 Small-scale real data: baby registries", "text": "We compared KRK-PICARD with PICARD and EM [10] in the Baby Registry dataset (described in detail in [10]), which was also used to evaluate other DPP learning algorithms [9, 10, 25]. The dataset contains 17 categories of baby-related products sourced from Amazon. We learned cores for the 6 largest categories (N = 100); in this case, PICARD is sufficiently efficient to prefer KRK-PICARD; this comparison serves only to evaluate the quality of the final core estimates. The initial boundary core K for EM was selected from a wishart distribution with N degrees of freedom and an identity covariance matrix, which is then scaled by 1 / N; for PICARD, L was set to K (I \u2212 K) \u2212 1; for KRK-PICARD, L1 and L2 were selected (as in JOINT \u2212 PICARD) by determining L \u2212 1 slightly below 1, increasing the probability by 1."}, {"heading": "5.3 Large-scale real dataset: GENES", "text": "Finally, to evaluate KRK-PICARD on large matrices of real data, we train it on data from the GENES [3] dataset (which was also used to evaluate DPPs in [3, 19]), which consists of 10,000 genes, each of which is represented by 331 characteristics corresponding to the distance of a gene to hubs in the BioGRID gene interaction network. We construct a Gaussian-DPP kernel on the GENES dataset and use it to obtain 100 training samples with uniformly distributed sizes between 50 and 200 items. Similar to these synthetic experiments, we initialized the KRK-PICARD kernel by specifying Li = X > i Xi, where Xi displays a random matrix x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x The LICARD 2.2 kernel algorithm is based on the performance of both LICARD algorithms."}, {"heading": "6 Conclusion and future work", "text": "We have introduced KRONDPPS, a variant of DPPs with cores structured as Kronecker products of m smaller matrices, and demonstrated that typical operations via DPPs, such as sampling and learning the core from data, can be efficiently done for KRONDPPS on previously untractable ground set sizes. By carefully using the properties of the Kronecker product, we have derived a low complexity algorithm for m = 2 to learn the kernel from data that guarantees positive iterates and a monotonous increase in the log probability, and run in O (n\u03ba3 + N2) time. This algorithm provides even more significant speed gains and memory gains in stochastic cases, with only O (N3 / 2 + N2) time and O (N + 2) probability executed."}, {"heading": "Appendix: Kronecker Determinantal Point Processes", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Proof of Prop. 3.1", "text": "We use \"vec\" to denote the operator that lines up the columns of a matrix to form a vector; conversely, \"mat\" takes a vector with k2 coefficients and returns a k \u2212 k \u2212 j = \u2212 L1 \u2212 L2, S1 = L \u2212 11, S2 = L \u2212 1 and S = S1 S2 \u2212 S2 = L \u2212 1. We observe the matrix with all zeros except a 1 at position (i, j), where its size can be seen from the context. We would like to solve the question (X) f2 (X) = Eij g2 (S1) and f1 (X) = \u2212 f1 f1 (X)."}, {"heading": "B Efficient updates for KRK-PICARD", "text": "(1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) () ()) () () () () ()) () () () () () () () () () ()) () () () ()) () () () ()) () () () ()) () () () ()) () () () () () ()) () () () () () () () () () ()) () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() ((() () () () (() () () () (() () () (() () (((() () () ((() () ((() () ((() () () (() () () () ((() (() ((() () () (() () () ((() () () (() () () () (() () () (("}, {"heading": "C Proof of validity for joint updates", "text": "To minimize the number of subsequent matrix multiplications, we minimize the equation (based on the properties of the Frobenius standard): \"L - 1 + - X - Y - 2F (11) and set {L - 1 - L1XL1 L\" 2 - L2Y L2. \"Theorem C.1. Leave L 0.\" Define R: = [vec (L (11)) >;. vec (L (N1N1) >] N1 i, j = 1 - RN1N1 - N2N2 - N2. Assuming that R has an eigengap between its largest singular value and the next, and leave u, v - the first singular vectors and value of R. Let us leave U = mat (u) - and V = mat (v) - the first positive verification of L2 and V - the pair (U - / V) - Lalgorithm - L2 - either positive or negative."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set<lb>of N items. They have recently gained prominence in several applications that rely on \u201cdiverse\u201d<lb>subsets. However, their applicability to large problems is still limited due to theO(N) complexity<lb>of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs<lb>by introducing KRONDPP, a DPP model whose kernel matrix decomposes as a tensor product of<lb>multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling.<lb>But contrary to what one may expect, leveraging the Kronecker product structure for speeding up<lb>DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and<lb>stochastic optimization algorithms for efficiently learning the parameters of a KRONDPP.", "creator": "LaTeX with hyperref package"}}}