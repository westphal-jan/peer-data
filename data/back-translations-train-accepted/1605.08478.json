{"id": "1605.08478", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Model-Free Imitation Learning with Policy Optimization", "abstract": "In imitation learning, an agent learns how to behave in an environment with an unknown cost function by mimicking expert demonstrations. Existing imitation learning algorithms typically involve solving a sequence of planning or reinforcement learning problems. Such algorithms are therefore not directly applicable to large, high-dimensional environments, and their performance can significantly degrade if the planning problems are not solved to optimality. Under the apprenticeship learning formalism, we develop alternative model-free algorithms for finding a parameterized stochastic policy that performs at least as well as an expert policy on an unknown cost function, based on sample trajectories from the expert. Our approach, based on policy gradients, scales to large continuous environments with guaranteed convergence to local minima.", "histories": [["v1", "Thu, 26 May 2016 23:43:32 GMT  (459kb,D)", "http://arxiv.org/abs/1605.08478v1", "In Proceedings of the 33rd International Conference on Machine Learning, 2016"]], "COMMENTS": "In Proceedings of the 33rd International Conference on Machine Learning, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jonathan ho", "jayesh k gupta", "stefano ermon"], "accepted": true, "id": "1605.08478"}, "pdf": {"name": "1605.08478.pdf", "metadata": {"source": "META", "title": "Model-Free Imitation Learning with Policy Optimization", "authors": ["Jonathan Ho", "Jayesh K. Gupta", "Stefano Ermon"], "emails": ["HOJ@CS.STANFORD.EDU", "JKG@CS.STANFORD.EDU", "ERMON@CS.STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "In practice, however, it can be difficult to trigger a cost function that achieves desirable behavior (Bagnell, 1991; Russell, 1998).An alternative and often more practical approach, called the imitation learning method, is to encode preferences and distinguish between desirable and undesirable outcomes by using demonstrations provided by an expert (Pomerleau, 1991; Russell, 1998).The simplest approach to imitation learning is behavioral cloning, where the goal is to understand the relationship between states and optimal policies as an overarching learning process of the 33rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W & CP Volume 48. Creator (s).Problem (Pomerleau, 1991)."}, {"heading": "2. Preliminaries", "text": "We start with the definition of basic concepts in the field of reinforcement learning. We are given an environment consisting of a state space S, a space A, a dynamic model p (s) and an initial state distribution p0 (s0). Agents act according to stationary stochastic guidelines \u03c0 (a), which determine the probabilities for the choice of action for each state. We will work with finite S and A, but our methods will extend to continuous spaces.With respect to a cost function c: S \u00b7 A \u2192 R, a discount factor \u03b3 [0, 1) and a policy \u03c0, the state value function Qc\u03c0 (s, a), the state value function V c\u03c0 (s) and the advantage function A c (s, a), the Qc\u03c0 (st, at) = Ep0, p, p [zi \u00b2 s] defines the state probability."}, {"heading": "3. Apprenticeship learning", "text": "To address the problem of counterfeit learning, we adopt the apprentice learning formalism, in which the learner must find a policy that works at least as well as the expert \u03c0E on an unknown true cost function ctrue: S \u00b7 A \u2192 R (Abbeel & Ng, 2004; Syed et al., 2008). Formally, the student's goal is to find a policy that goes so far as to assign the true cost function to a class of cost functions. Accordingly, they seek a policy that works as well as a strategy for all c-C. That is, they seek to satisfy the constraints. Learning algorithms carry the assumption that the true cost function belongs to a class of cost functions."}, {"heading": "3.1. Examples from prior work", "text": "Before considering the methodology, we will first review two prototypical examples of learning algorithms where costs are in the range (2004); we will show how they are presented in detail in this section (namely, how they select the Cost Function Class C); and we will briefly describe their solution techniques for the solution (3), which differ significantly from our new policy optimization methods.Feature Expectation, based on Abbeel & Ng (2004), defines the cost of these basic functions by first defining a set of basic functions c1, where cj: S \u00b7 R, and then defining the cost class as a specific set of linear combinations of these basic functions: Clinear = {cw, sp i = 1 wici, 4) The structure of Clinear allows the expected costs in relation to cw to be written as an internal product of Clinear."}, {"heading": "4. Policy optimization for apprenticeship learning", "text": "After reviewing existing algorithms to solve various learning scenarios in apprenticeship training, we are now delving into policy optimization strategies that directly address (3) as a stochastic optimization problem. To enable us to apply the optimization problem to large, continuous environments, we first define a class of smoothly parameterized stochastic strategies. (3) We minimize these strategies through policy parameters: first, we minimize the increase in costs in relation to (4), where there are a number of valid parameter vectors. (4) With this class of strategies, our goal is to solve the optimization problem (3) through policy parameters: minimize. (4) Let us first examine the increase in costs in relation to (4). (5) Let us determine the cost function in relation to learning processes in relation to (5). (5) We propose to first address a local minimum of this problem by means of policy-based optimization (5)."}, {"heading": "4.1. Policy gradient", "text": "The simplest method of optimization (3) is the stochastic gradient descent with an estimate of the gradient (7): This is the classic political gradient formula for the cost function (Sutton et al., 1999). To estimate this using random samples, we propose the following algorithm, called IM-REINFORCE, which describes the development of REINFORCE (Williams, 1992) for the learning of fastening methods parallel to the development of REINFORCE (Williams et al., 1999). IM-REINFORCE expert trajectories (i.e. rollouts of expert policy) are given as input. For each iteration, the current parameter vector vector vector (0) is calculated using samptories (2) and samplemp (2)."}, {"heading": "4.2. Monotonic policy improvements", "text": "This question of variance is not limited to our learning processes in education, and it is a hallmark of the difficulty of political gradient algorithms in this area (Peters & Schaal, 2008). Instead, we will draw direct inspiration from a recently developed algorithm development called trust regions. (TRPO), a model-free search for algorithms capable of quickly training large neural tasks. (Schulman et al., 2015).TRPO for enhanced learning In this section, we will develop a fixed learning method, and in the next, we will develop a fixed analogy for complex tasks. (Schulman et al., 2015)."}, {"heading": "4.3. Finding cost functions", "text": "So far, we have postponed the discussion of cost functions that achieve the higher learning goal (2). We are addressing this issue here for the function expectation matching setting described in Section 3.1 (2). \u2212 As mentioned in Section 4.1, the higher learning algorithm only has access to sample trajectories from \u03c0 and \u03c0E, so we will look at the costs that achieve the higher learning goal. \u2212 Using c to determine the optimal cost of this empirical goal, we have access to sample learning trajectories from \u03c0 and \u03c0E, so we will look at the costs that reach the higher learning goal. \u2212 Using c to determine the optimal cost of this empirical goal, we will have access to sample learning trajectories from \u03c0 and \u03c0E, so we will look at the costs that reach the higher goal of the empirical learning objective region. \u2212 Using c to determine the optimal cost of this empirical goal, we will have an overarching learning goal."}, {"heading": "5. Experiments", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "6. Discussion and future work", "text": "Our method is capable of finding a locally optimal solution, even in situations where optimal planning is out of reach, which is a significant advantage over competing algorithms that repeatedly require solving planning problems in an inner loop. Indeed, competing algorithms do not even provide local guarantees of optimality (Ermon et al., 2015).Our approach does not use expert interaction or reinforcement signals that fit into a family of such approaches, which include apprenticeship training and inverse reinforcement learning. If one of these additional resources is made available, alternative approaches are possible (Kim et al., 2013; Daume \u0301 III et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011)."}, {"heading": "Acknowledgements", "text": "We thank John Schulman for valuable conversations about TRPO. This work was supported by a scholarship from the SAILToyota Center for AI Research and a Graduate Research Fellowship from the National Science Foundation (scholarship number DGE-114747)."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "An invitation to imitation", "author": ["Bagnell", "J Andrew"], "venue": "Technical report,", "citeRegEx": "Bagnell and Andrew.,? \\Q2015\\E", "shortCiteRegEx": "Bagnell and Andrew.", "year": 2015}, {"title": "Search-based structured prediction", "author": ["Daum\u00e9 III", "Hal", "Langford", "John", "Marcu", "Daniel"], "venue": "Machine learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In Proceedings of the 19th International Conference on Machine Learning,", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "URL http://cs.stanford.edu/people/ karpathy/reinforcejs/waterworld.html", "author": ["Karpathy", "Andrej"], "venue": "Reinforcejs: Waterworld demo,", "citeRegEx": "Karpathy and Andrej.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Andrej.", "year": 2015}, {"title": "Learning from limited demonstrations", "author": ["Kim", "Beomjoon", "Farahmand", "Amir-massoud", "Pineau", "Joelle", "Precup", "Doina"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2013}, {"title": "Continuous inverse optimal control with locally optimal examples", "author": ["Levine", "Sergey", "Koltun", "Vladlen"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Levine et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2012}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["Levine", "Sergey", "Popovic", "Zoran", "Koltun", "Vladlen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levine et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2011}, {"title": "Training parsers by inverse reinforcement learning", "author": ["Neu", "Gergely", "Szepesv\u00e1ri", "Csaba"], "venue": "Mach. Learn.,", "citeRegEx": "Neu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2009}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Ng", "Andrew Y", "Russell", "Stuart J"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "Ng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2000}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["Peters", "Jan", "Schaal", "Stefan"], "venue": "Neural networks,", "citeRegEx": "Peters et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2008}, {"title": "Efficient training of artificial neural networks for autonomous navigation", "author": ["Pomerleau", "Dean A"], "venue": "Neural Computation,", "citeRegEx": "Pomerleau and A.,? \\Q1991\\E", "shortCiteRegEx": "Pomerleau and A.", "year": 1991}, {"title": "Maximum margin planning", "author": ["Ratliff", "Nathan D", "Bagnell", "J Andrew", "Zinkevich", "Martin A"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Ratliff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2006}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["Ratliff", "Nathan D", "Silver", "David", "Bagnell", "J Andrew"], "venue": "Autonomous Robots,", "citeRegEx": "Ratliff et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2009}, {"title": "Efficient reductions for imitation learning", "author": ["Ross", "St\u00e9phane", "Bagnell", "Drew"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoffrey J", "Bagnell", "Drew"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Learning agents for uncertain environments", "author": ["Russell", "Stuart"], "venue": "In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pp", "citeRegEx": "Russell and Stuart.,? \\Q1998\\E", "shortCiteRegEx": "Russell and Stuart.", "year": 1998}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael", "Moritz", "Philipp"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "A game-theoretic approach to apprenticeship learning", "author": ["Syed", "Umar", "Schapire", "Robert E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Syed et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2007}, {"title": "A reduction from apprenticeship learning to classification", "author": ["Syed", "Umar", "Schapire", "Robert E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Syed et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2010}, {"title": "Apprenticeship learning using linear programming", "author": ["Syed", "Umar", "Bowling", "Michael", "Schapire", "Robert E"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Syed et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2008}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Ziebart", "Brian D", "Maas", "Andrew L", "Bagnell", "J Andrew", "Dey", "Anind K"], "venue": "In AAAI,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 13, "context": "Inverse reinforcement learning (IRL) methods (Russell, 1998; Ng & Russell, 2000; Ratliff et al., 2006; Ziebart et al., 2008), which are some of the most successful approaches to imitation learning, assume that the behavior the learner desires to imitate is generated by an expert behaving optimally with respect to an unknown cost function.", "startOffset": 45, "endOffset": 124}, {"referenceID": 24, "context": "Inverse reinforcement learning (IRL) methods (Russell, 1998; Ng & Russell, 2000; Ratliff et al., 2006; Ziebart et al., 2008), which are some of the most successful approaches to imitation learning, assume that the behavior the learner desires to imitate is generated by an expert behaving optimally with respect to an unknown cost function.", "startOffset": 45, "endOffset": 124}, {"referenceID": 22, "context": "We first develop a simple, unified view of a certain class of imitation learning algorithms called apprenticeship learning algorithms (Abbeel & Ng, 2004; Syed et al., 2008).", "startOffset": 134, "endOffset": 172}, {"referenceID": 22, "context": "To address the imitation learning problem, we adopt the apprenticeship learning formalism, in which the learner must find a policy that performs at least as well as the expert \u03c0E on an unknown true cost function ctrue : S \u00d7A \u2192 R (Abbeel & Ng, 2004; Syed et al., 2008).", "startOffset": 229, "endOffset": 267}, {"referenceID": 20, "context": "Game-theoretic approaches Syed & Schapire (2007); Syed et al. (2008) proposed two apprenticeship learning algorithms, Multiplicative Weights Apprenticeship Learning (MWAL) and Linear Programming Apprenticeship Learning (LPAL), that also use basis cost functions, but with the weights restricted to give a convex combination:", "startOffset": 50, "endOffset": 69}, {"referenceID": 20, "context": "Syed et al. (2008) address this computational complexity with their LPAL method.", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "This is the classic policy gradient formula for the cost function c\u2217 (Sutton et al., 1999).", "startOffset": 69, "endOffset": 90}, {"referenceID": 18, "context": "Instead, we will directly draw inspiration from a recently developed algorithm called trust region policy optimization (TRPO), a model-free policy search algorithm capable of quickly training large neural network stochastic policies for complex tasks (Schulman et al., 2015).", "startOffset": 251, "endOffset": 274}, {"referenceID": 18, "context": "Schulman et al. (2015) address this by showing that minimizing a certain surrogate loss function can guarantee policy improvement with a large step size.", "startOffset": 0, "endOffset": 23}, {"referenceID": 18, "context": "Further discussion on sampling methodologies and effective optimization algorithms for solving this constrained problem can be found in Schulman et al. (2015).", "startOffset": 136, "endOffset": 159}, {"referenceID": 18, "context": "In all of the continuous environments, we used policies constructed according to Schulman et al. (2015): the policies have Gaussian action distributions, with mean given by a multi-layer perceptron taking observations as input, and standard deviations given by an extra set of parameters.", "startOffset": 81, "endOffset": 104}, {"referenceID": 22, "context": "1, LPAL (Syed et al., 2008) finds a global optimum for the apprenticeship problem (3) with Cconvex in finite state and action spaces.", "startOffset": 8, "endOffset": 27}, {"referenceID": 6, "context": "When either of these additional resources is provided, alternative approaches (Kim et al., 2013; Daum\u00e9 III et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011) may be more sample efficient, and investigating ways to combine these resources with our framework is an interesting research direction.", "startOffset": 78, "endOffset": 161}, {"referenceID": 16, "context": "When either of these additional resources is provided, alternative approaches (Kim et al., 2013; Daum\u00e9 III et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011) may be more sample efficient, and investigating ways to combine these resources with our framework is an interesting research direction.", "startOffset": 78, "endOffset": 161}, {"referenceID": 14, "context": "Nonlinear cost function classes have been successful in IRL (Ratliff et al., 2009; Levine et al., 2011) as well as in other machine learning problems reminiscent of ours, in particular that of training generative image models.", "startOffset": 60, "endOffset": 103}, {"referenceID": 8, "context": "Nonlinear cost function classes have been successful in IRL (Ratliff et al., 2009; Levine et al., 2011) as well as in other machine learning problems reminiscent of ours, in particular that of training generative image models.", "startOffset": 60, "endOffset": 103}, {"referenceID": 3, "context": "In the language of generative adversarial networks (Goodfellow et al., 2014), the policy parameterizes a generative model of state-action pairs, and the cost function serves as an adversary.", "startOffset": 51, "endOffset": 76}], "year": 2016, "abstractText": "In imitation learning, an agent learns how to behave in an environment with an unknown cost function by mimicking expert demonstrations. Existing imitation learning algorithms typically involve solving a sequence of planning or reinforcement learning problems. Such algorithms are therefore not directly applicable to large, high-dimensional environments, and their performance can significantly degrade if the planning problems are not solved to optimality. Under the apprenticeship learning formalism, we develop alternative model-free algorithms for finding a parameterized stochastic policy that performs at least as well as an expert policy on an unknown cost function, based on sample trajectories from the expert. Our approach, based on policy gradients, scales to large continuous environments with guaranteed convergence to local minima.", "creator": "LaTeX with hyperref package"}}}