{"id": "1605.08491", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2016", "title": "Provable Algorithms for Inference in Topic Models", "abstract": "Recently, there has been considerable progress on designing algorithms with provable guarantees -- typically using linear algebraic methods -- for parameter learning in latent variable models. But designing provable algorithms for inference has proven to be more challenging. Here we take a first step towards provable inference in topic models. We leverage a property of topic models that enables us to construct simple linear estimators for the unknown topic proportions that have small variance, and consequently can work with short documents. Our estimators also correspond to finding an estimate around which the posterior is well-concentrated. We show lower bounds that for shorter documents it can be information theoretically impossible to find the hidden topics. Finally, we give empirical results that demonstrate that our algorithm works on realistic topic models. It yields good solutions on synthetic data and runs in time comparable to a {\\em single} iteration of Gibbs sampling.", "histories": [["v1", "Fri, 27 May 2016 02:18:43 GMT  (426kb,D)", "http://arxiv.org/abs/1605.08491v1", "to appear at ICML'2016"]], "COMMENTS": "to appear at ICML'2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["sanjeev arora", "rong ge 0001", "frederic koehler", "tengyu ma", "ankur moitra"], "accepted": true, "id": "1605.08491"}, "pdf": {"name": "1605.08491.pdf", "metadata": {"source": "META", "title": "Provable Algorithms for Inference in Topic Models", "authors": ["Sanjeev Arora", "Rong Ge", "Frederic Koehler", "Tengyu Ma", "Ankur Moitra"], "emails": ["ARORA@CS.PRINCETON.EDU", "RONGGE@CS.DUKE.EDU", "FKOEHLER@PRINCETON.EDU", "TENGYU@CS.PRINCETON.EDU", "MOITRA@MIT.EDU"], "sections": [{"heading": "1. Introduction", "text": "Generative data models are ubiquitous in unmonitored learning, leading to two types of computational problems: In parameter learning, the goal is to find the parameters of the Proceedings of the 33rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W & CP Volume 48. Copyright 2016 by the author (s). The model that best fits a given data set. In conclusion, the goal is to learn the values of latent variables for a specific datapoint. A wide range of approaches is empirically effective for both tasks, including sampling and variable conclusions. However, we largely lack strong verifiable guarantees - on time or quality of solution - for these approaches. Recently, there has been considerable progress in developing new algorithms for parameter learning with such verifiable guarantees."}, {"heading": "1.1. Setup and Overview", "text": "In fact, it is as if most people living in the US, in the US and in other European countries, have come up with the idea that they are able to integrate into the EU, and that they do not feel able to integrate into the EU in the EU. (...) It is not as if they are able to establish themselves in the EU in the EU. (...) It is as if they have been accepted into the EU in the EU. (...) It is as if they have been accepted into the EU in the EU. (...) It is as if they have been accepted into the EU. (...) It is as if they have been accepted into the EU. (...) It is as if they have been accepted into the EU. (...) It is as if they have been accepted into the EU. (...) It is as if they have been accepted into the EU."}, {"heading": "2. Notations and Setup", "text": "In addition to the description of the theme model in Section 1.1, we introduce the following notations: We use Sk = {z-Q-Q-0: | z-1 = 1} to denote the k-dimensional probability simplex. We assume that the true subject proportional vector x-x-Sk is sparse throughout the work r. Sometimes we also use notations and use y as a D-dimensional vector instead of a set, in which case yi is the number of times word i appears in the document. We will use a > i to denote the i-th series of A. We will use cat (p) to denote the categorical distribution defined by the probability vector p. Euclidean norm, \"1,\" the norm of a vector is denoted in the form of a vector."}, {"heading": "3. \u03b4-Biased Minimum Variance Estimators", "text": "We try to estimate the true vector x by multiplying it by any matrix. Intuitively, each word will select a column of B, and its variance to any entry is limited by the maximum entry in B. Therefore, we would like to optimize two things: First, we want BA to be close to identity; second, we want the matrixB to be small so that the matrixB has a small column of B. This inspires the following linear program: Definition 3.1."}, {"heading": "5. Rate of MLE estimator", "text": "In this section, we show that, given the correct support of the x-th group, we can optimize the log probability function using the variables inR and obtain a finer solution with a smaller \"1 error.\" We assume that the x-th coordinates of the x-th group are delimited by the size of the x-th group and that the x-th groups of the x-th group are delimited by the size of the x-th group. < M-th group is delimited by the size of the x-th group i and x-th group is delimited by the i-th group. < M-th group is delimited by the i-th group 5.2 (limited \"1\" conditional number). We assume that the x-th groups of the x-th group i-limited group A are satisfied that the x-th groups of R = supp (x) are satisfied by the size and x-th group i-th group by the i-th group of the i-limited group (\"1\" conditional group x \") are satisfied."}, {"heading": "As a corollary,", "text": "Q 1-2 \u00b7 Idr. (12)"}, {"heading": "6. Sample Complexity Lower Bounds", "text": "In this section, we construct a natural (distribution of) word matrix A with low (A) values containing approximately half of the selected words. < p > p > p > p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p"}, {"heading": "7. Experiments", "text": "The corpora we use consists in part of New York Times articles (295,000 documents, average document length 298), Enron emails (39,861 documents, average document length 136), and NIPS papers (1500 documents, average length 1042). We calculate the word-topic matrices using the algorithm in (Arora et al., 2013a), using 100 topics for NIPS, 100 topics for Enron, and 400 topics for NYTimes.Condition Numbers of Matrices First we empirically verify the assumption that the word-topic matrix is small. \""}, {"heading": "8. Conclusion", "text": "This work takes a step toward designing algorithms with verifiable guarantees of inferences in topic modeling 71, building on previous work by Kleinberg and Sandler (Kleinberg & Sandler, 2008) in collaborative filtering. We use an idea of the approximate inversion of a topic matrix (as opposed to its exact inversion) and characterize the mathematical con-4We list only as recall values, because in this setting precision = 3 / 5 \u00b7 remember. Condition - namely the \"\u221e \u2192\" 1 condition number of the topic - which determines how well it performs as an estimator. Furthermore, we showed that this algorithm roughly solves inferences in a document with as few as O (r2 log k) words where k is the number of topics and r the conciseness of the topic vector that generates the document. We have shown that such guarantees are optimal in the sense that there are word matrices that make the topic less meaningful for the matrices that exist for the conclusions there is too."}, {"heading": "A. Missing proofs in Section 5", "text": "We start with the assertion that we only have a reasonable proximity of x-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-"}, {"heading": "B. Proof of Proposition 3.2", "text": "The proof of the proposition 3.2. Let J be the matrix for all 1. We rewrite the program (3) as LP by introducing auxiliary variable. < (A) = min \u2212 j \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s"}, {"heading": "C. Missing proofs in Section 6", "text": "s be a random subset of size r, and let R \u2212 R be a random subset that is removed from R (so | R \u2212 1). Let xi = 1 / r if i-R, and xi = 0 otherwise. Let x \u2212 i = 1 / (r \u2212 1) if i-R \u2212 and 0 otherwise.Each word in the document is from a multinomic distribution whose probabilities are specified by either Ax or Ax \u2212. We will show the two distributions that have small CLL divergence, so that no algorithm can distinguish between the two. First, we observe that most of the lines between r / 4 and 3r / 4 are entries with a value of approximately 2 / D in the subset R \u2212 of coordinates. We say that a series is biased if it is less than r / 4 or more than zero in R \u2212."}, {"heading": "D. Lower bound on `1 \u2192 `1 condition number", "text": "Here we describe how we limit the condition number \"1\" in Table 1 from bottom to top. Fix \u03b4 > 0 and letB\u03b4-Rk \u00b7 D are a minimizer of LP (3) with a given \u03b4, i.e. | B\u03b4 | \u221e = \u03bb\u03b4 and | BA \u2212 I | \u043c \u2264 \u03b4. By compactness there is a v so that | v | \u221e / | Av | 1 = \u03bb (A), then there is a v so that there is a v that | v | 3 (A), then there is a v so that there is a v (v | 3)."}], "references": [{"title": "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation", "author": ["A. Anandkumar", "D. Foster", "D. Hsu", "S. Kakade", "Y. Liu"], "venue": "In NIPS,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A tensor approach to learning mixed membership community models", "author": ["Anandkumar", "Animashree", "Ge", "Rong", "Hsu", "Daniel", "Kakade", "Sham M"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["Zhu", "Michael"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Zhu and Michael.,? \\Q2013\\E", "shortCiteRegEx": "Zhu and Michael.", "year": 2013}, {"title": "New algorithms for learning incoherent and overcomplete dictionaries", "author": ["Arora", "Sanjeev", "Ge", "Rong", "Moitra", "Ankur"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "Provable bounds for learning some deep representations", "author": ["Arora", "Sanjeev", "Bhaskara", "Aditya", "Ge", "Rong", "Ma", "Tengyu"], "venue": "In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing,", "citeRegEx": "Arora et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2014}, {"title": "Simultaneous analysis of lasso and dantzig selector", "author": ["Bickel", "Peter J", "Ritov", "Yaacov", "Tsybakov", "Alexandre B"], "venue": "Ann. Statist., 37(4):1705\u20131732,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Latent dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research, pp. 993\u20131022,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Decoding by linear programming", "author": ["E.J. Candes", "T. Tao"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Candes and Tao,? \\Q2005\\E", "shortCiteRegEx": "Candes and Tao", "year": 2005}, {"title": "Learning mixtures of gaussians in high dimensions", "author": ["Ge", "Rong", "Huang", "Qingqing", "Kakade", "Sham M"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Finding scientific topics", "author": ["Griffiths", "Thomas L", "Steyvers", "Mark"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Hsu", "Daniel", "Kakade", "Sham M"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,", "citeRegEx": "Hsu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2013}, {"title": "Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": null, "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Some highlights include algorithms for topic modeling (Arora et al., 2013b; Anandkumar et al., 2012), learning mixture models (Moitra & Valiant, 2010; Hsu & Kakade, 2013; Ge et al.", "startOffset": 54, "endOffset": 100}, {"referenceID": 8, "context": ", 2012), learning mixture models (Moitra & Valiant, 2010; Hsu & Kakade, 2013; Ge et al., 2015), community detection (Anandkumar et al.", "startOffset": 33, "endOffset": 94}, {"referenceID": 1, "context": ", 2015), community detection (Anandkumar et al., 2014) and (special cases of) deep learning (Arora et al.", "startOffset": 29, "endOffset": 54}, {"referenceID": 4, "context": ", 2014) and (special cases of) deep learning (Arora et al., 2014; Janzamin et al., 2015).", "startOffset": 45, "endOffset": 88}, {"referenceID": 11, "context": ", 2014) and (special cases of) deep learning (Arora et al., 2014; Janzamin et al., 2015).", "startOffset": 45, "endOffset": 88}, {"referenceID": 6, "context": "The model assumes a distribution on x that favors sparse or approximately sparse vectors; a popular choice is the Dirichlet distribution (Blei et al., 2003).", "startOffset": 137, "endOffset": 156}, {"referenceID": 0, "context": "Recent (provable) algorithms for this problem (Anandkumar et al., 2012; Arora et al., 2013b) use the method of moments, leveraging the fact that some form of averaging over the corpus yields a linear algebraic problem for recovering A.", "startOffset": 46, "endOffset": 92}, {"referenceID": 0, "context": "Alternatively, one can use a co-occurrence tensor and recover A under weaker assumptions (Anandkumar et al., 2012).", "startOffset": 89, "endOffset": 114}, {"referenceID": 5, "context": "Moreover, the restricted `1 \u2192 `1 condition number can be viewed as `1 analog of the restricted isometry property (Candes & Tao, 2005) or restricted eigenvalue conditions (Bickel et al., 2009; Meinshausen & Yu, 2009) associated to `2 norm.", "startOffset": 170, "endOffset": 215}], "year": 2016, "abstractText": "Recently, there has been considerable progress on designing algorithms with provable guarantees \u2014 typically using linear algebraic methods \u2014 for parameter learning in latent variable models. But designing provable algorithms for inference has proven to be more challenging. Here we take a first step towards provable inference in topic models. We leverage a property of topic models that enables us to construct simple linear estimators for the unknown topic proportions that have small variance, and consequently can work with short documents. Our estimators also correspond to finding an estimate around which the posterior is well-concentrated. We show lower bounds that for shorter documents it can be information theoretically impossible to find the hidden topics. Finally, we give empirical results that demonstrate that our algorithm works on realistic topic models. It yields good solutions on synthetic data and runs in time comparable to a single iteration of Gibbs sampling.", "creator": "LaTeX with hyperref package"}}}