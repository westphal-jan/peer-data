{"id": "1605.08527", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2016", "title": "Stochastic Optimization for Large-scale Optimal Transport", "abstract": "Optimal transport (OT) defines a powerful framework to compare probability distributions in a geometrically faithful way. However, the practical impact of OT is still limited because of its computational burden. We propose a new class of stochastic optimization algorithms to cope with large-scale problems routinely encountered in machine learning applications. These methods are able to manipulate arbitrary distributions (either discrete or continuous) by simply requiring to be able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error. These algorithms rely on two main ideas: (a) the dual OT problem can be re-cast as the maximization of an expectation ; (b) entropic regularization of the primal OT problem results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence. We instantiate these ideas in three different setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat Sinkhorn's algorithm, the current state-of-the-art finite dimensional OT solver; (ii) when comparing a discrete distribution to a continuous density, a semi-discrete reformulation of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization ; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space (RKHS). This is currently the only known method to solve this problem, apart from computing OT on finite samples. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems.", "histories": [["v1", "Fri, 27 May 2016 07:47:30 GMT  (611kb,D)", "http://arxiv.org/abs/1605.08527v1", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG math.NA", "authors": ["aude genevay", "marco cuturi", "gabriel peyr\u00e9", "francis r bach"], "accepted": true, "id": "1605.08527"}, "pdf": {"name": "1605.08527.pdf", "metadata": {"source": "CRF", "title": "Stochastic Optimization for Large-scale Optimal Transport", "authors": ["Aude Genevay"], "emails": ["genevay@ceremade.dauphine.fr", "mcuturi@i.kyoto-u.ac.jp", "gabriel.peyre@ceremade.dauphine.fr", "francis.bach@inria.fr"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Optimal Transport: Primal, Dual and Semi-dual Formulations", "text": "We consider the optimal traffic problem between two metrics. We simply assume that both can be described in a single convex optimization problem as follows: \"Primal, Dual, and Semi-Dual Formulas.\" (D) The Kantorovich formula [10] of OT and its entropic regularization [6] can be conveniently written in a single convex optimization problem as follows: \"Primal, Dual, and Semi-Dual Formulas.\" (D) The Kantorovich formula [10] of OT and its entropic regularization [6] can be conveniently written in a single convex optimization problem as follows: \"Primal, Dual, and Semi-Dual Formulas.\" (D). The Kantorovich formula [10] of OT and its entropic regularization [6] can be conveniently written in a single convex optimization problem as follows: \"promiblem.\" (D +) (D)"}, {"heading": "3 Discrete Optimal Transport", "text": "We assume in this section that both problems are discrete measures, i.e. finite sums of Diracs = finite sums of Diracs = finite sums of Diracs = finite sums of Diracs = finite sums of Diracs = finite sums of Diracs = finite sums of Diracs, of the form p = 1 infinite and infinite. These discrete measures can come from the evaluation of continuous densities on a grid where features are counted in a structured object or empirical measures are used based on random samples. This setting is relevant for several applications, including all known applications of Earth mover distance. We show in this section that our stochastic formulation can be extremely efficient in comparing measures with a large number of points."}, {"heading": "4 Semi-Discrete Optimal Transport", "text": "In this section we assume that it is an arbitrary problem (in particular, it does not have to be discreet) and that it is a finite dimensionality problem, which has been written in anticipation of the way, how it is the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way, how the way and the way, how the way, how the way, how the way and the way, how the way, how the way and the way, how the way, how the way and the way, how the way and the way, how the way, the way and the way, how the way, the way and the way, the way and the way, the way, the way and the way, the way, the way, the way and the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the way, the"}, {"heading": "5 Continuous optimal transport using RKHS", "text": "In the case where neither \u00b5 nor \u03bd are discrete, problem (S\u03b5) is infinitely dimensional, therefore it cannot be solved directly with stochastic SGD. In this section we propose to solve the initial dual problem (D\u03b5) by using extensions of the dual variables in two reproducing kernels Hilbertspaces (RKHS). Let us remember that, conversely to the methods from previous sections, we can only solve the regularized problem here (i.e. \u03b5 > 0), since (D\u03b5) cannot be cast as an expectation problem when it comes to continued optimization. We consider two reproducing kernels Hilbert spaces (RKHS) H and G defined on X and Y, with kernels and, \"associated with norms."}, {"heading": "Acknowledgement", "text": "The work of G. Peyr\u00e9 was supported by the European Research Council (ERC project SIGMA-Vision) and the work of A. Genevay was supported by the Ile-de-France region. M. Cuturi thanks JSPS Young Research A grant 26700002."}, {"heading": "A Convergence of (S\u03b5) as \u03b5\u2192 0", "text": "The convergence of the solutions of (P\u03b5) to a solution of (P0) such as (P0) such as (P0) such as (P0) such as (P0) such as (P0) such as (P0) such as (P0) such as (P0). The convergence of the solutions of (D\u03b5) to solutions of (D0) such as (P0) such as (P0) is demonstrated for the special case of discrete metrics in [5]. To the best of our knowledge and belief, the behaviour of (S\u03b5) is not examined in the literature, and we propose a convergence result if it is discrete, which is the setting in which this formulation is most advantageous. Proposition A.1. We assume that (S\u03b5 y) how much Y, c (\u00b7, y) such as L1 (\u00b5) such as (\u00b5), that the convergence of Yj) is as high, and we fix x0."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Optimal transport (OT) defines a powerful framework to compare probability distri-<lb>butions in a geometrically faithful way. However, the practical impact of OT is still<lb>limited because of its computational burden. We propose a new class of stochastic opti-<lb>mization algorithms to cope with large-scale problems routinely encountered in machine<lb>learning applications. These methods are able to manipulate arbitrary distributions<lb>(either discrete or continuous) by simply requiring to be able to draw samples from them,<lb>which is the typical setup in high-dimensional learning problems. This alleviates the<lb>need to discretize these densities, while giving access to provably convergent methods<lb>that output the correct distance without discretization error. These algorithms rely<lb>on two main ideas: (a) the dual OT problem can be re-cast as the maximization of an<lb>expectation ; (b) entropic regularization of the primal OT problem results in a smooth<lb>dual optimization optimization which can be addressed with algorithms that have a<lb>provably faster convergence. We instantiate these ideas in three different setups: (i)<lb>when comparing a discrete distribution to another, we show that incremental stochastic<lb>optimization schemes can beat Sinkhorn\u2019s algorithm, the current state-of-the-art finite<lb>dimensional OT solver; (ii) when comparing a discrete distribution to a continuous<lb>density, a semi-discrete reformulation of the dual program is amenable to averaged<lb>stochastic gradient descent, leading to better performance than approximately solving<lb>the problem by discretization ; (iii) when dealing with two continuous densities, we<lb>propose a stochastic gradient descent over a reproducing kernel Hilbert space (RKHS).<lb>This is currently the only known method to solve this problem, apart from computing<lb>OT on finite samples. We backup these claims on a set of discrete, semi-discrete and<lb>continuous benchmark problems.", "creator": "LaTeX with hyperref package"}}}