{"id": "1605.08636", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2016", "title": "PAC-Bayesian Theory Meets Bayesian Inference", "abstract": "We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by a i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-Gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.", "histories": [["v1", "Fri, 27 May 2016 13:41:33 GMT  (525kb,D)", "http://arxiv.org/abs/1605.08636v1", "under review"], ["v2", "Tue, 1 Nov 2016 14:49:05 GMT  (530kb,D)", "http://arxiv.org/abs/1605.08636v2", "To appear at NIPS 2016"], ["v3", "Sat, 3 Dec 2016 22:48:15 GMT  (529kb,D)", "http://arxiv.org/abs/1605.08636v3", "To appear at NIPS 2016 (revised version)"], ["v4", "Mon, 13 Feb 2017 17:14:52 GMT  (530kb,D)", "http://arxiv.org/abs/1605.08636v4", "Published at NIPS 2015 (this http URL)"]], "COMMENTS": "under review", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["pascal germain", "francis r bach", "alexandre lacoste", "simon lacoste-julien"], "accepted": true, "id": "1605.08636"}, "pdf": {"name": "1605.08636.pdf", "metadata": {"source": "CRF", "title": "PAC-Bayesian Theory Meets Bayesian Inference", "authors": ["Pascal Germain", "Francis Bach", "Alexandre Lacoste", "Simon Lacoste-Julien"], "emails": ["firstname.lastname@inria.fr", "allac@google.com"], "sections": [{"heading": "1 Introduction", "text": "Since its early beginnings [Shawe-Taylor and Williamson, 1997], the PAC-Bayesian theory has claimed to provide \"PAC guarantees for Bayesian algorithms\" [McAllester, 1999]. However, despite the amount of work devoted to this statistical learning theory - many authors improved the initial results 1 and / or generalized them for various machine learning setupse2 - it is mostly used as a frequentistic method. That is, assuming that the learning samples i.i.d. are generated by a data distribution, this theory probably expresses roughly correct (PAC) boundaries in terms of generalization risk. In other words, the probability that the generalization risk is furthest from the educational risk. The Bayesian side of PAC-Bayes comes largely from the fact that these boundaries are expressed on the average / aggregation of Bayest."}, {"heading": "2 PAC-Bayesian Theory", "text": "We refer to the learning sample (X, Y) = (xi, yi) ni = (X, Y) n, which contains n input-output pairs. The main assumption of frequency learning theories - including PAC bayes - is that (X, Y) in this unemployment function is randomly selected from a data generation distribution, which we call a (discrete or continuous) distribution set. (F, Y) n, in which we use the predictors f: X, Y) and the generalization errors in the distribution D asL, Y (f) n, i = 1 (f, xi, yi)."}, {"heading": "3 Bridging Bayes and PAC-Bayes", "text": "In this section, we show that by selecting the negative log loss function, we minimize the Y function (PAC-Bayes bound is equivalent to maximizing the Bayean marginal probability. To get this result, we first consider the Bayean approach, which begins by defining a preceding p and Y value via the set of possible model parameters, i.e., p (y), p (y), p (y), p (y), p (y), p (y), p (y), p (y), p (y), p (y), p (y), p), p (y), p).4 With the Bayes rule, we get the posterior p (i), p (p), p (Y) = p (Y), p (y), p (p), p, y, p (p)."}, {"heading": "4 PAC-Bayesian Bounds for Regression", "text": "This section aims to extend the results of Section 3 to real-value unlimited losses. These results are used in the following sections to examine \"nll,\" but they are valid for broader classes of loss functions. Importantly, our new results are focused on regression problems, as opposed to the usual PAC-Bayesian classification framework. The new limits are supported by a newer theory by Alquier et al. [2015], which is given below (we provide evidence in Appendix A.1 for completeness). Theorem 3 (Alquier et al. [2015]). Given a distribution D over X \u00b7 Y, a hypothesis based on F \u00b7 X \u00b7 Y, a loss function. \""}, {"heading": "5 Analysis of Model Selection", "text": "The PACBayesian theorems, of course, suggest selecting the model best suited to the given task by selecting the boundary for each model, as we have shown in Section 3 that minimizing the PAC-Bayes boundary amounts to maximizing the marginal probability [McAllester, 2003, Ambroladze et al., 2006, Zhang, 2006]. Indeed, given a collection of L optimal Gibbs posteriors - one for each mode - given by Equation (8)., p (Comprehension | X, Y). The PAC-Bayes boundary amounts to maximizing the marginal probability. In fact, given a collection of L optimal Gibbs posteriors - one for each mode."}, {"heading": "6 Linear Regression", "text": "In this section, we perform a linear regression using the parameterization of Bishop (2006).The output space is Y: = R and, for an arbitrary input space, we use a mapping functionality: X \u2192 R. The model is (x, y), X \u00b7 Y and model parameters. Therefore, the negative log loss is \"nll\" (< w, p), p (y), p (y), p (y), < w, p >) = N (y), p), p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "7 Conclusion", "text": "The first contribution of this paper is to bridge the concepts underlying the Bayesian and Bayesian approaches. This study, based on the newly evaluated negative log-probelihood loss function, motivates the second contribution of this paper, which aims to demonstrate the PAC-Bayesian generalization limit for regression with unlimited sub-gamma loss functions, which provides generalization guarantees for the quadratic loss of regression tasks. In this paper, we examined model selection techniques. In a broader perspective, we would like to propose both Bayesian and PAC-Bayesian frameworks that may have more to learn from each other than what has been done recently. As future work, we plan to study other Bayesian techniques in the light of PAC-Bayesian tools, such as varying Bayesian and empirical Bayes methods."}, {"heading": "A Supplementary material", "text": "From Donsker-Varadhan's change of measurement (f): p (f) \u2212 p (f) = p (f), p (f) = p (f), p (f) = p (f), p (f). \u2212 p (F): p (f), p (f). \u2212 p (F): p (f). \u2212 p (F): p (f). \u2212 p (f). \u2212 p (f). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F. (F). (F). (F). (F). (F. (F). (F). (F). (F. (F). (F. (F). (F.). (F. (F). (F. (F). (F. (F). (F.). (F. (F.). (F. (F.). (F. (F.). (F.). (F. (F.). (F. (F. (F.). (F.). (F.). (F. (F. (F.). (F.). (F. (F.). (F. (F.). (F. (F.). (F. (F.). (F.). (F.).). (F. (F"}], "references": [{"title": "On the properties of variational approximations of Gibbs posteriors", "author": ["Pierre Alquier", "James Ridgway", "Nicolas Chopin"], "venue": "ArXiv e-prints,", "citeRegEx": "Alquier et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alquier et al\\.", "year": 2015}, {"title": "Tighter PAC-Bayes bounds", "author": ["A. Ambroladze", "E. Parrado-Hern\u00e1ndez", "J. Shawe-Taylor"], "venue": "In NIPS,", "citeRegEx": "Ambroladze et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ambroladze et al\\.", "year": 2006}, {"title": "On Bayesian bounds", "author": ["Arindam Banerjee"], "venue": "In ICML, pages", "citeRegEx": "Banerjee.,? \\Q2006\\E", "shortCiteRegEx": "Banerjee.", "year": 2006}, {"title": "PAC-Bayesian theory for transductive learning", "author": ["Luc B\u00e9gin", "Pascal Germain", "Fran\u00e7ois Laviolette", "Jean-Francis Roy"], "venue": "In AISTATS,", "citeRegEx": "B\u00e9gin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "B\u00e9gin et al\\.", "year": 2014}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "A general framework for updating belief distributions", "author": ["P.G. Bissiri", "C.C. Holmes", "S.G. Walker"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Bissiri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bissiri et al\\.", "year": 2016}, {"title": "Concentration inequalities : a nonasymptotic theory of independence", "author": ["St\u00e9phane Boucheron", "G\u00e1bor Lugosi", "Pascal Massart"], "venue": "Oxford university press,", "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "PAC-Bayesian supervised classification: the thermodynamics of statistical learning, volume 56", "author": ["Olivier Catoni"], "venue": "Inst. of Mathematical Statistic,", "citeRegEx": "Catoni.,? \\Q2007\\E", "shortCiteRegEx": "Catoni.", "year": 2007}, {"title": "Aggregation by exponential weighting, sharp PAC-Bayesian bounds and sparsity", "author": ["Arnak S. Dalalyan", "Alexandre B. Tsybakov"], "venue": "Machine Learning,", "citeRegEx": "Dalalyan and Tsybakov.,? \\Q2008\\E", "shortCiteRegEx": "Dalalyan and Tsybakov.", "year": 2008}, {"title": "PAC-Bayesian learning of linear classifiers", "author": ["Pascal Germain", "Alexandre Lacasse", "Fran\u00e7ois Laviolette", "Mario Marchand"], "venue": "In ICML,", "citeRegEx": "Germain et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2009}, {"title": "Risk bounds for the majority vote: From a PAC-Bayesian analysis to a learning", "author": ["Pascal Germain", "Alexandre Lacasse", "Francois Laviolette", "Mario Marchand", "Jean-Francis Roy"], "venue": null, "citeRegEx": "Germain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2015}, {"title": "Probabilistic machine learning and artificial intelligence", "author": ["Zoubin Ghahramani"], "venue": "Nature, 521:452\u2013459,", "citeRegEx": "Ghahramani.,? \\Q2015\\E", "shortCiteRegEx": "Ghahramani.", "year": 2015}, {"title": "Suboptimal behavior of Bayes and MDL in classification under misspecification", "author": ["Peter Gr\u00fcnwald", "John Langford"], "venue": "Machine Learning,", "citeRegEx": "Gr\u00fcnwald and Langford.,? \\Q2007\\E", "shortCiteRegEx": "Gr\u00fcnwald and Langford.", "year": 2007}, {"title": "Learning efficient random maximum a-posteriori predictors with non-decomposable loss functions", "author": ["Tamir Hazan", "Subhransu Maji", "Joseph Keshet", "Tommi S. Jaakkola"], "venue": "In NIPS,", "citeRegEx": "Hazan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2013}, {"title": "Ockham\u2019s razor and Bayesian analysis", "author": ["William H. Jeffreys", "James O. Berger"], "venue": "American Scientist,", "citeRegEx": "Jeffreys and Berger.,? \\Q1992\\E", "shortCiteRegEx": "Jeffreys and Berger.", "year": 1992}, {"title": "Agnostic Bayes", "author": ["Alexandre Lacoste"], "venue": "PhD thesis, Universite\u0301 Laval,", "citeRegEx": "Lacoste.,? \\Q2015\\E", "shortCiteRegEx": "Lacoste.", "year": 2015}, {"title": "Approximate inference for the loss-calibrated Bayesian", "author": ["Simon Lacoste-Julien", "Ferenc Huszar", "Zoubin Ghahramani"], "venue": "In AISTATS,", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2011}, {"title": "PAC-Bayes & margins", "author": ["John Langford", "John Shawe-Taylor"], "venue": "In NIPS, pages 423\u2013430,", "citeRegEx": "Langford and Shawe.Taylor.,? \\Q2002\\E", "shortCiteRegEx": "Langford and Shawe.Taylor.", "year": 2002}, {"title": "Tighter PAC-Bayes bounds through distributiondependent priors", "author": ["Guy Lever", "Fran\u00e7ois Laviolette", "John Shawe-Taylor"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Lever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lever et al\\.", "year": 2013}, {"title": "A note on the PAC-Bayesian theorem", "author": ["Andreas Maurer"], "venue": "CoRR, cs.LG/0411099,", "citeRegEx": "Maurer.,? \\Q2004\\E", "shortCiteRegEx": "Maurer.", "year": 2004}, {"title": "Some PAC-Bayesian theorems", "author": ["David McAllester"], "venue": "Machine Learning,", "citeRegEx": "McAllester.,? \\Q1999\\E", "shortCiteRegEx": "McAllester.", "year": 1999}, {"title": "PAC-Bayesian stochastic model selection", "author": ["David McAllester"], "venue": "Machine Learning,", "citeRegEx": "McAllester.,? \\Q2003\\E", "shortCiteRegEx": "McAllester.", "year": 2003}, {"title": "Generalization bounds and consistency for latent structural probit and ramp loss", "author": ["David A. McAllester", "Joseph Keshet"], "venue": "In NIPS,", "citeRegEx": "McAllester and Keshet.,? \\Q2011\\E", "shortCiteRegEx": "McAllester and Keshet.", "year": 2011}, {"title": "Generalization error bounds for Bayesian mixture algorithms", "author": ["Ron Meir", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Meir and Zhang.,? \\Q2003\\E", "shortCiteRegEx": "Meir and Zhang.", "year": 2003}, {"title": "On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes", "author": ["Andrew Y. Ng", "Michael I. Jordan"], "venue": "In NIPS,", "citeRegEx": "Ng and Jordan.,? \\Q2001\\E", "shortCiteRegEx": "Ng and Jordan.", "year": 2001}, {"title": "Robust forward algorithms via PAC-Bayes and Laplace distributions", "author": ["Asf Noy", "Koby Crammer"], "venue": "In AISTATS,", "citeRegEx": "Noy and Crammer.,? \\Q2014\\E", "shortCiteRegEx": "Noy and Crammer.", "year": 2014}, {"title": "A PAC-Bayesian bound for lifelong learning", "author": ["Anastasia Pentina", "Christoph H. Lampert"], "venue": "In ICML,", "citeRegEx": "Pentina and Lampert.,? \\Q2014\\E", "shortCiteRegEx": "Pentina and Lampert.", "year": 2014}, {"title": "PAC-Bayesian generalization bounds for gaussian processes", "author": ["Matthias Seeger"], "venue": "JMLR, 3:233\u2013269,", "citeRegEx": "Seeger.,? \\Q2002\\E", "shortCiteRegEx": "Seeger.", "year": 2002}, {"title": "Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error Bounds and Sparse Approximations", "author": ["Matthias Seeger"], "venue": "PhD thesis, University of Edinburgh,", "citeRegEx": "Seeger.,? \\Q2003\\E", "shortCiteRegEx": "Seeger.", "year": 2003}, {"title": "PAC-Bayesian analysis of co-clustering and beyond", "author": ["Yevgeny Seldin", "Naftali Tishby"], "venue": null, "citeRegEx": "Seldin and Tishby.,? \\Q2010\\E", "shortCiteRegEx": "Seldin and Tishby.", "year": 2010}, {"title": "PAC-Bayesian analysis of contextual bandits", "author": ["Yevgeny Seldin", "Peter Auer", "Fran\u00e7ois Laviolette", "John Shawe-Taylor", "Ronald Ortner"], "venue": "In NIPS,", "citeRegEx": "Seldin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Seldin et al\\.", "year": 2011}, {"title": "PAC-Bayesian inequalities for martingales", "author": ["Yevgeny Seldin", "Fran\u00e7ois Laviolette", "Nicol\u00f2 Cesa-Bianchi", "John Shawe-Taylor", "Peter Auer"], "venue": "In UAI,", "citeRegEx": "Seldin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Seldin et al\\.", "year": 2012}, {"title": "A PAC analysis of a Bayesian estimator", "author": ["John Shawe-Taylor", "Robert C. Williamson"], "venue": "In COLT,", "citeRegEx": "Shawe.Taylor and Williamson.,? \\Q1997\\E", "shortCiteRegEx": "Shawe.Taylor and Williamson.", "year": 1997}, {"title": "Information-theoretic upper and lower bounds for statistical estimation", "author": ["Tong Zhang"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Zhang.,? \\Q2006\\E", "shortCiteRegEx": "Zhang.", "year": 2006}], "referenceMentions": [{"referenceID": 32, "context": "Since its early beginning [Shawe-Taylor and Williamson, 1997], the PAC-Bayesian theory claims to provide \u201cPAC guarantees to Bayesian algorithms\u201d [McAllester, 1999].", "startOffset": 26, "endOffset": 61}, {"referenceID": 20, "context": "Since its early beginning [Shawe-Taylor and Williamson, 1997], the PAC-Bayesian theory claims to provide \u201cPAC guarantees to Bayesian algorithms\u201d [McAllester, 1999].", "startOffset": 145, "endOffset": 163}, {"referenceID": 11, "context": "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 4, "context": "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al.", "startOffset": 34, "endOffset": 48}, {"referenceID": 4, "context": "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc.", "startOffset": 34, "endOffset": 69}, {"referenceID": 4, "context": "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc.", "startOffset": 34, "endOffset": 99}, {"referenceID": 4, "context": "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc. Langford and Shawe-Taylor [2002], Seldin and Tishby [2010], Seldin et al.", "startOffset": 34, "endOffset": 138}, {"referenceID": 4, "context": "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc. Langford and Shawe-Taylor [2002], Seldin and Tishby [2010], Seldin et al.", "startOffset": 34, "endOffset": 164}, {"referenceID": 2, "context": "[2011, 2012], B\u00e9gin et al. [2014], Pentina and Lampert [2014], etc.", "startOffset": 14, "endOffset": 34}, {"referenceID": 2, "context": "[2011, 2012], B\u00e9gin et al. [2014], Pentina and Lampert [2014], etc.", "startOffset": 14, "endOffset": 62}, {"referenceID": 2, "context": "[2011, 2012], B\u00e9gin et al. [2014], Pentina and Lampert [2014], etc. Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al.", "startOffset": 14, "endOffset": 124}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al.", "startOffset": 57, "endOffset": 73}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al.", "startOffset": 57, "endOffset": 87}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al.", "startOffset": 57, "endOffset": 103}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Gr\u00fcnwald and Langford [2007], Lacoste-Julien et al.", "startOffset": 57, "endOffset": 126}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Gr\u00fcnwald and Langford [2007], Lacoste-Julien et al.", "startOffset": 57, "endOffset": 157}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Gr\u00fcnwald and Langford [2007], Lacoste-Julien et al.", "startOffset": 57, "endOffset": 180}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Gr\u00fcnwald and Langford [2007], Lacoste-Julien et al.", "startOffset": 57, "endOffset": 210}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Gr\u00fcnwald and Langford [2007], Lacoste-Julien et al. [2011] for other studies drawing links between frequentist statistics and Bayesian inference.", "startOffset": 57, "endOffset": 240}, {"referenceID": 7, "context": "Theorem 1 (Catoni, 2007).", "startOffset": 10, "endOffset": 24}, {"referenceID": 7, "context": "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al.", "startOffset": 18, "endOffset": 32}, {"referenceID": 7, "context": "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al.", "startOffset": 18, "endOffset": 111}, {"referenceID": 7, "context": "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al.", "startOffset": 18, "endOffset": 141}, {"referenceID": 7, "context": "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al. [2013], Noy and Crammer [2014].", "startOffset": 18, "endOffset": 162}, {"referenceID": 7, "context": "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al. [2013], Noy and Crammer [2014]. Theorem 1 (Catoni, 2007).", "startOffset": 18, "endOffset": 186}, {"referenceID": 28, "context": "As mentioned by Zhang [2006], Catoni [2007], Germain et al.", "startOffset": 16, "endOffset": 29}, {"referenceID": 6, "context": "As mentioned by Zhang [2006], Catoni [2007], Germain et al.", "startOffset": 30, "endOffset": 44}, {"referenceID": 6, "context": "As mentioned by Zhang [2006], Catoni [2007], Germain et al. [2009], Lever et al.", "startOffset": 30, "endOffset": 67}, {"referenceID": 6, "context": "As mentioned by Zhang [2006], Catoni [2007], Germain et al. [2009], Lever et al. [2013], Alquier et al.", "startOffset": 30, "endOffset": 88}, {"referenceID": 0, "context": "[2013], Alquier et al. [2015], the optimal Gibbs posterior \u03c1\u0302\u2217 is given by \u03c1\u0302\u2217(f) = 1 ZX,Y \u03c0(f) e \u2212n L\u0302 ` X,Y (f) , (4)", "startOffset": 8, "endOffset": 30}, {"referenceID": 0, "context": "The new bounds are obtained through a recent theorem of Alquier et al. [2015], stated below (we provide a proof in Appendix A.", "startOffset": 56, "endOffset": 78}, {"referenceID": 0, "context": "The new bounds are obtained through a recent theorem of Alquier et al. [2015], stated below (we provide a proof in Appendix A.1 for completeness). Theorem 3 (Alquier et al. [2015]).", "startOffset": 56, "endOffset": 180}, {"referenceID": 26, "context": "A similar result to Equation (15) leads to long-life learning algorithms in Pentina and Lampert [2014].", "startOffset": 76, "endOffset": 103}, {"referenceID": 0, "context": "The above sub-Gaussian assumption corresponds to the Hoeffding assumption of Alquier et al. [2015], and allows to obtain the following result.", "startOffset": 77, "endOffset": 99}, {"referenceID": 19, "context": "Maurer [2004] shows that we can generalize PAC-Bayesian bounds on the generalization risk of the Gibbs classifier to any loss function with output between zero and one.", "startOffset": 0, "endOffset": 14}, {"referenceID": 8, "context": "In other situations, we may have RD(G\u03c1\u0302) = 0 even if RD(G\u03c1\u0302) = 12\u2212 (see Germain et al. [2015] for an extensive study).", "startOffset": 72, "endOffset": 94}, {"referenceID": 4, "context": "In this section, we perform Bayesian linear regression using the parameterization of Bishop [2006]. The output space is Y := R and, for an arbitrary input spaceX , we use a mapping function\u03c6 :X\u2192R.", "startOffset": 85, "endOffset": 99}, {"referenceID": 0, "context": "Finally, the result of Theorem 3 [Alquier et al., 2015], combined with \u03bb = 1/ \u221a n (Eq 15), converges to the expected loss, but it provides good guarantees only for large training sample (n & 10).", "startOffset": 33, "endOffset": 55}], "year": 2017, "abstractText": "We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam\u2019s razor criteria, under the assumption that the data is generated by a i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-Gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.", "creator": "LaTeX with hyperref package"}}}