{"id": "1605.08671", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2016", "title": "An optimal algorithm for the Thresholding Bandit Problem", "abstract": "We study a specific \\textit{combinatorial pure exploration stochastic bandit problem} where the learner aims at finding the set of arms whose means are above a given threshold, up to a given precision, and \\textit{for a fixed time horizon}. We propose a parameter-free algorithm based on an original heuristic, and prove that it is optimal for this problem by deriving matching upper and lower bounds. To the best of our knowledge, this is the first non-trivial pure exploration setting with \\textit{fixed budget} for which optimal strategies are constructed.", "histories": [["v1", "Fri, 27 May 2016 14:35:29 GMT  (80kb,D)", "http://arxiv.org/abs/1605.08671v1", "ICML 2016"]], "COMMENTS": "ICML 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["andrea locatelli", "maurilio gutzeit", "alexandra carpentier"], "accepted": true, "id": "1605.08671"}, "pdf": {"name": "1605.08671.pdf", "metadata": {"source": "META", "title": "An optimal algorithm for the Thresholding Bandit Problem", "authors": ["Andrea Locatelli", "Maurilio Gutzeit", "Alexandra Carpentier"], "emails": ["ANDREA.LOCATELLI@UNI-POTSDAM.DE", "MGUTZEIT@UNI-POTSDAM.DE", "CARPENTIER@UNI-POTSDAM.DE"], "sections": [{"heading": "1. Introduction", "text": "In this paper, we study a specific, combinatorial, pure problem, stochastic bandit setting. Specifically, we consider a stochastic bandit setting in which each arm has a specific value. (In this paper, we refer to this problem as the Thresholding Bandit Problem (TBP), which is a specific problem of combinatorial pure exploration bandit setting in (Chen et al., 2014). A simpler version of this problem is known as the Thresholding Bandit Problem (TBP), which is a specific instance of combinatorial pure exploration bandit setting in (Chen et al., 2014). A simpler version of this problem is known as the SIGN problem, see (Chen & Li, 2015).This problem refers to the popular combinatorial pure exploration threshold bandit setting, known as the TopM problem, where the learner's goal is to return the group of M weapons with the highest mean beck (al)."}, {"heading": "2. The Thresholding Bandit Problem", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Problem formulation", "text": "Each of these arms is characterized by a distribution that we assume to be R-subGaussian. (R-sub-Gaussian distribution) Let R-sub-Gaussian, if for all of these errors we assume to be R-subGaussian distributions of variance R2 for R-R. Such distributions have a finite mean, let us discard the microbes as an EX threshold. We consider the following dynamic game setting, which is common in the bandit literature. (R-1) The learner chooses an arm from A = {1, K}. It receives a noisy reward from the distribution. We consider the following dynamic game settings, which are common in the bandit literature. (R-2) The learner chooses an arm from A = {1, K}."}, {"heading": "2.2. A lower bound", "text": "Specifically, for each sequence of gaps (dk) k, we define a finite series of problems where the distributions of the arms of these problems correspond to these gauss of variance 1. We reduce the probability of error among these problems, for the best possible algorithm. Theorem 1. Let K, T \u2265 0. Let us allow for all i \u2264 K, di \u2265 0. Let us allow for 0 \u2264 i \u2264 K Bi to be written for the problem where the distribution of the arm j \u0432 {1,...,..., K} N (= di +, 1) if i 6 = j and N (ergo \u2212 di \u2212, 1) otherwise. For all of these problems, H: = H: = HA, = DL i (di + 2) \u2212 2 is by definition the same."}, {"heading": "It holds that for any bandit algorithm", "text": "The lower limit implies that even if the learner is given the distance of the mean value of each arm to the threshold and the shape of the distribution of each arm (here gauge of variance 1), each algorithm still makes an error of at least exp (\u2212 3T / H \u2212 4 log (12 (log (T) + 1) K) one of the problems. This is a lower limit in a very strong sense, because we limit the amount of options to an area where we know all the gaps and prove that this lower limit applies. Also, it is not maximimaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaximaxima"}, {"heading": "2.3. Algorithm APT and associated upper bound", "text": "In this section we introduce APT (Anytime Parameter-free Thresholding algorithm =), a learning algorithm that is parameter-free at all times. Its heuristic analysis is based on a simple observation, namely that an almost optimal static strategy that maps TC samples to an arm so that Tk-2k is constant over k (and with T) - see Theorem 1, and in particular the second half of step 3 of its evidentiary force in Appendix A - is therefore a natural idea to simply pull the arm that minimizes an estimator of this quantity. Note: In this paper, for simplicity reasons, we consider that each arm is tested against the same threshold, but this can be relaxed at no additional costs.Algorithm The algorithms receive as input the definition of the problem."}, {"heading": "2.4. Discussion", "text": "An important point we want to emphasize for our APT strategy is that no parameters are required, such as complexity H, horizon T or sub-Gauss constant R. This is in contrast to any supreme confidence-based approach such as in e.g. (Audibert & Bubeck, 2010; Gabillon et al., 2012) (e.g. the UCB-E algorithm in (Audibert & Bubeck, 2010)), which requires an upper limit for R and precise knowledge of H as parameters, while the limit of Kulorem 2 will apply to all R and all H, and our algorithm adapts to these quantities. We would also like to emphasize that for the associated problem of best array identification, existing fixed strategies must know the T budget in advance (Audibert & Bubeck, 2010; Karnin et al, 2013; Chen et al, 2014) - while our algorithm can be stopped at any time."}, {"heading": "3. Extensions of our results to related settings", "text": "In this section, we describe some implications of the results of the previous section on some specific issues."}, {"heading": "3.1. Active level set detection : Active classification and active anomaly detection", "text": "Here we explain how a simple modification of our setting transforms them into the setting of the active level of classification, and why it can be applied to the active classification and active anomaly. We define the problem of discrete, active level of detection as the problem of deciding as efficiently as possible, in our bandit setting, whether the samples of weapon detection above or below a certain level L are higher or lower than a threshold detection up to a precision, i.e. it is the problem for everyone to decide whether \u00b5 k (L): = PX, whether the probability that the weapons detection samples are above or below a certain level L is higher or lower than a threshold detection method, i.e. it is the problem for everyone to decide whether \u00b5 k (L): = PX that the probability (X > L), or not up to a precision. This problem can be solved immediately by our approach by making a simple variable change."}, {"heading": "3.2. Best arm identification and cumulative reward maximization with known highest mean value", "text": "The goal of the learner is to identify the arm with the highest mean value (Bubeck et al., 2009). In the second phase, the goal is to maximize the sum of the samples collected by the algorithm up to the time T (Auer et al., 1995). Intuitively, both problems should require different strategies - in the best arm identification problem, one would like to explore all arms hard, while stamping as much as possible the arm with the highest mean. Such intuition is supported by Theorem 1 of (Bubeck et al., 2009), which states that in the absence of additional information and with a fixed budget, the lower regret is suffered in the cumulative setting, expressed in rewards that suffered higher remorse in identification, expressed in terms of the likelihood of a mistake."}, {"heading": "4. Experiments", "text": "We illustrate the performance of this algorithm APT in a series of experiments. For comparison, we use the following methods, which include the state of the art CSAR algorithm of (Chen et al., 2014) and two minor adjustments known methods that are also suitable for our problem. Uniform Allocation (UA): For each t: 1, 2,., T), we select It is known that it is optimal if all arms are equally difficult to classify, which is in our setting when the quantities (UA), i, i, are very close. UCB-type algorithm UCBE: The algorithms given and analyzed in UCBE (Audibert & Bubeck, 2010) is designed for finding the best arm - and its heuristic is to pull the arm that maximizes a UCB tied - see also (Gabillon et al., 2012) for an adaptation of this algorithm to the general TopM problem."}, {"heading": "A. Proofs", "text": "In this proof, we will prove that in at least one example of the problem, each algorithm commits an error of sequence (\u2212 cT / H).Step 0: Set and notation. Let's look at the real numbers of K = 0, and let's specify the real numbers of K = 0, = 0. Note that this construction can easily be generalized to cases where the distribution of M = 0 or 6 = 0 is defined by translation or careful selection of product distributions Bi, where i {0, K} is the distribution of M \u2212 i and variance of M = 1. Note that this construction can easily be generalized to cases where the distribution of M = 0 or 6 = careful choice of product distributions Bi is defined, where i {0, K} is the distribution of M = 1 and variance."}, {"heading": "B. Further Experimental Results", "text": "We now also provide simulation results for our three settings in the case of the Gaussian arms with the means \u00b5i and variances \u03c32i = 0.25. Here, too, only the correctly tuned UCBE algorithm exceeds APT."}], "references": [{"title": "The space complexity of approximating the frequency moments", "author": ["Alon", "Noga", "Matias", "Yossi", "Szegedy", "Mario"], "venue": "In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing,", "citeRegEx": "Alon et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Alon et al\\.", "year": 1996}, {"title": "Best arm identification in multi-armed bandits", "author": ["Audibert", "Jean-Yves", "Bubeck", "S\u00e9bastien"], "venue": "In Proceedings of the 23rd Conference on Learning Theory,", "citeRegEx": "Audibert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2010}, {"title": "Gambling in a Rigged Casino: The Adversarial Multi-Armed Bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicol\u00f2", "Freund", "Yoav", "Schapire", "Robert"], "venue": "In Proceedings of the 36th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "Bandits with heavy tail", "author": ["Bubeck", "Sebastian", "Cesa-Bianchi", "Nicolo", "Lugosi", "G\u00e1bor"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["Bubeck", "S\u00e9bastien", "Munos", "R\u00e9mi", "Stoltz", "Gilles"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "Multiple identifications in multi-armed bandits", "author": ["Bubeck", "S\u00e9ebastian", "Wang", "Tengyao", "Viswanathan", "Nitin"], "venue": "In Proceedings of The 30th International Conference on Machine Learning", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "On topk selection in multi-armed bandits and hidden bipartite graphs", "author": ["Cao", "Wei", "Li", "Jian", "Tao", "Yufei", "Zhize"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Tight (lower) bounds for the fixed budget best arm identification bandit problem", "author": ["Carpentier", "Alexandra", "Locatelli", "Andrea"], "venue": "In Proceedings of the 29th Conference on Learning", "citeRegEx": "Carpentier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carpentier et al\\.", "year": 2016}, {"title": "Challenging the empirical mean and empirical variance: a deviation study", "author": ["Catoni", "Olivier"], "venue": "In Annales de l\u2019Institut Henri Poincare\u0301, Probabilite\u0301s et Statistiques,", "citeRegEx": "Catoni and Olivier,? \\Q2012\\E", "shortCiteRegEx": "Catoni and Olivier", "year": 2012}, {"title": "On the optimal sample complexity for best arm identification", "author": ["Chen", "Lijie", "Li", "Jian"], "venue": "arXiv preprint arXiv:1511.03774,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Combinatorial pure exploration of multiarmed bandits", "author": ["Chen", "Shouyuan", "Lin", "Tian", "King", "Irwin", "Lyu", "Michael R", "Wei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Pac bounds for multi-armed bandit and markov decision processes", "author": ["Even-Dar", "Eyal", "Mannor", "Shie", "Mansour", "Yishay"], "venue": "In Computational Learning Theory,", "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["Gabillon", "Victor", "Ghavamzadeh", "Mohammad", "Lazaric", "Alessandro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2012}, {"title": "lil\u2019ucb: An optimal exploration algorithm for multi-armed bandits", "author": ["Jamieson", "Kevin", "Malloy", "Matthew", "Nowak", "Robert", "Bubeck", "S\u00e9bastien"], "venue": "In Proceedings of the 27th Conference on Learning Theory,", "citeRegEx": "Jamieson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2014}, {"title": "Pac subset selection in stochastic multiarmed bandits", "author": ["Kalyanakrishnan", "Shivaram", "Tewari", "Ambuj", "Auer", "Peter", "Stone"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Kalyanakrishnan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalyanakrishnan et al\\.", "year": 2012}, {"title": "Almost optimal exploration in multi-armed bandits", "author": ["Karnin", "Zohar", "Koren", "Tomer", "Somekh", "Oren"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Karnin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karnin et al\\.", "year": 2013}, {"title": "On the complexity of best arm identification in multiarmed bandit models", "author": ["Kaufmann", "Emilie", "Capp\u00e9", "Olivier", "Garivier", "Aur\u00e9lien"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kaufmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2015}, {"title": "The Sample Complexity of Exploration in the Multi-Armed Bandit Problem", "author": ["S Mannor", "Tsitsiklis", "J N"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mannor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2004}, {"title": "Deviations of stochastic bandit regret", "author": ["Salomon", "Antoine", "Audibert", "Jean-Yves"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Salomon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Salomon et al\\.", "year": 2011}, {"title": "A classification framework for anomaly detection", "author": ["Steinwart", "Ingo", "Hush", "Don R", "Scovel", "Clint"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Steinwart et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Steinwart et al\\.", "year": 2005}, {"title": "Selecting among heuristics by solving thresholded k-armed bandit problems", "author": ["Streeter", "Matthew J", "Smith", "Stephen F"], "venue": "ICAPS", "citeRegEx": "Streeter et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Streeter et al\\.", "year": 2006}, {"title": "Optimal pac multiple arm identification with applications to crowdsourcing", "author": ["Zhou", "Yuan", "Chen", "Xi", "Li", "Jian"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "In this paper, we refer to this setting as the Thresholding Bandit Problem (TBP), which is a specific instance of the combinatorial pure exploration bandit setting introduced in (Chen et al., 2014).", "startOffset": 178, "endOffset": 197}, {"referenceID": 12, "context": "This problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al.", "startOffset": 189, "endOffset": 294}, {"referenceID": 16, "context": "This problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al.", "startOffset": 189, "endOffset": 294}, {"referenceID": 21, "context": "This problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al.", "startOffset": 189, "endOffset": 294}, {"referenceID": 6, "context": "This problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al.", "startOffset": 189, "endOffset": 294}, {"referenceID": 10, "context": "As mentioned previously, the TBP problem is a specific instance of the combinatorial pure exploration bandit framework introduced in (Chen et al., 2014).", "startOffset": 133, "endOffset": 152}, {"referenceID": 10, "context": "Without going into the details of the combinatorial pure exploration setting for which the paper (Chen et al., 2014) derives interesting general results, we will summarize what these results imply for the particular TBP and TopM problems, which are specific cases of the combinatorial pure exploration setting.", "startOffset": 97, "endOffset": 116}, {"referenceID": 10, "context": "As it is often the case for pure exploration problems, the paper (Chen et al., 2014) distinguishes between two settings:", "startOffset": 65, "endOffset": 84}, {"referenceID": 12, "context": "The similarities and dissemblance of these two settings have been discussed in the literature in the case of the TopM problem (in particular in the case M = 1), see (Gabillon et al., 2012; Karnin et al., 2013; Chen et al., 2014).", "startOffset": 165, "endOffset": 228}, {"referenceID": 15, "context": "The similarities and dissemblance of these two settings have been discussed in the literature in the case of the TopM problem (in particular in the case M = 1), see (Gabillon et al., 2012; Karnin et al., 2013; Chen et al., 2014).", "startOffset": 165, "endOffset": 228}, {"referenceID": 10, "context": "The similarities and dissemblance of these two settings have been discussed in the literature in the case of the TopM problem (in particular in the case M = 1), see (Gabillon et al., 2012; Karnin et al., 2013; Chen et al., 2014).", "startOffset": 165, "endOffset": 228}, {"referenceID": 12, "context": "While as explained in (Audibert & Bubeck, 2010; Gabillon et al., 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al.", "startOffset": 22, "endOffset": 70}, {"referenceID": 4, "context": ", 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al., 2009; Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015).", "startOffset": 284, "endOffset": 374}, {"referenceID": 15, "context": ", 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al., 2009; Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015).", "startOffset": 284, "endOffset": 374}, {"referenceID": 16, "context": ", 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al., 2009; Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015).", "startOffset": 284, "endOffset": 374}, {"referenceID": 14, "context": "In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting.", "startOffset": 177, "endOffset": 291}, {"referenceID": 13, "context": "In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting.", "startOffset": 177, "endOffset": 291}, {"referenceID": 15, "context": "In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting.", "startOffset": 177, "endOffset": 291}, {"referenceID": 16, "context": "In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting.", "startOffset": 177, "endOffset": 291}, {"referenceID": 12, "context": "the complexity H defined in Table 1, there is a gap between the known upper and lower bounds, see (Audibert & Bubeck, 2010; Gabillon et al., 2012; Karnin et al., 2013; Kaufmann et al., 2015).", "startOffset": 98, "endOffset": 190}, {"referenceID": 15, "context": "the complexity H defined in Table 1, there is a gap between the known upper and lower bounds, see (Audibert & Bubeck, 2010; Gabillon et al., 2012; Karnin et al., 2013; Kaufmann et al., 2015).", "startOffset": 98, "endOffset": 190}, {"referenceID": 16, "context": "the complexity H defined in Table 1, there is a gap between the known upper and lower bounds, see (Audibert & Bubeck, 2010; Gabillon et al., 2012; Karnin et al., 2013; Kaufmann et al., 2015).", "startOffset": 98, "endOffset": 190}, {"referenceID": 10, "context": "This knowledge gap is more acute for the general combinatorial exploration bandit problem defined in the paper (Chen et al., 2014) (see their Theorem 3) - and therefore for the TBP problem (where in fact no fixed budget lower bound exists to the best of our knowledge).", "startOffset": 111, "endOffset": 130}, {"referenceID": 10, "context": "The quantitiesH,H2 depend on the means \u03bck of the arm distributions and are defined in (Chen et al., 2014) (and are not the same for TopM and TBP).", "startOffset": 86, "endOffset": 105}, {"referenceID": 10, "context": "Our problem is a pure exploration bandit problem, and is in fact, shifting the means by \u2212\u03c4 , a specific case of the pure exploration bandit problem considered in (Chen et al., 2014) - namely the specific case where the set of sets of arms that they callM and which is their decision class is the set of all possible set of arms.", "startOffset": 162, "endOffset": 181}, {"referenceID": 10, "context": "A similar quantity was introduce for general combinatorial bandit problems (Chen et al., 2014) and is similar in essence to the complexity introduced for the best arm identification problem, see (Audibert & Bubeck, 2010).", "startOffset": 75, "endOffset": 94}, {"referenceID": 12, "context": "(Audibert & Bubeck, 2010; Gabillon et al., 2012) (e.", "startOffset": 0, "endOffset": 48}, {"referenceID": 15, "context": "Also we would like to highlight that for the related problem of best arm identification, existing fixed budget strategies need to know the budget T in advance (Audibert & Bubeck, 2010; Karnin et al., 2013; Chen et al., 2014) - while our algorithm can be stopped at any time and the bound of Theorem 2 will hold.", "startOffset": 159, "endOffset": 224}, {"referenceID": 10, "context": "Also we would like to highlight that for the related problem of best arm identification, existing fixed budget strategies need to know the budget T in advance (Audibert & Bubeck, 2010; Karnin et al., 2013; Chen et al., 2014) - while our algorithm can be stopped at any time and the bound of Theorem 2 will hold.", "startOffset": 159, "endOffset": 224}, {"referenceID": 0, "context": ", 2012) or in (Alon et al., 1996).", "startOffset": 14, "endOffset": 33}, {"referenceID": 16, "context": "exponential, models, it is possible to obtain a refined lower bound in terms of Kullback- Leibler divergences rather than gaps following (Kaufmann et al., 2015).", "startOffset": 137, "endOffset": 160}, {"referenceID": 10, "context": "Settings related to ours have been analyzed in the literature and the state of the art result on our problem can be found (to the best of our knowledge) in the paper (Chen et al., 2014).", "startOffset": 166, "endOffset": 185}, {"referenceID": 10, "context": "We believe that this lack of optimality for CSAR is not an artefact of the proof of the paper (Chen et al., 2014), and that CSAR is sub-optimal, as it is a successive reject algorithm with fixed and nonadaptive reject phase length.", "startOffset": 94, "endOffset": 113}, {"referenceID": 15, "context": "A similar gap between upper and lower bounds for successive reject based algorithms in the fixed budget setting was also observed for the best arm identification problem when no additional information such as the complexity are known to the learner, see (Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015; Chen et al., 2014).", "startOffset": 254, "endOffset": 342}, {"referenceID": 16, "context": "A similar gap between upper and lower bounds for successive reject based algorithms in the fixed budget setting was also observed for the best arm identification problem when no additional information such as the complexity are known to the learner, see (Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015; Chen et al., 2014).", "startOffset": 254, "endOffset": 342}, {"referenceID": 10, "context": "A similar gap between upper and lower bounds for successive reject based algorithms in the fixed budget setting was also observed for the best arm identification problem when no additional information such as the complexity are known to the learner, see (Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015; Chen et al., 2014).", "startOffset": 254, "endOffset": 342}, {"referenceID": 10, "context": "The paper (Chen et al., 2014) also provides results in the fixed confidence setting, where the objective is to provide an optimal set using the smallest possible sample size.", "startOffset": 10, "endOffset": 29}, {"referenceID": 14, "context": "In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting.", "startOffset": 123, "endOffset": 237}, {"referenceID": 13, "context": "In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting.", "startOffset": 123, "endOffset": 237}, {"referenceID": 15, "context": "In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting.", "startOffset": 123, "endOffset": 237}, {"referenceID": 16, "context": "In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting.", "startOffset": 123, "endOffset": 237}, {"referenceID": 10, "context": "To the best of our knowledge, all strategies except ours have such an optimality gap for fixed budget pure exploration combinatorial bandit problems, while there exists fixed confidence strategies for general pure exploration combinatorial bandits that are very close to optimal, see (Chen et al., 2014).", "startOffset": 284, "endOffset": 303}, {"referenceID": 12, "context": "the complexity H , it has been proved in the TopM problem that a UCB-type strategy has probability of error upper bounded as exp(\u2212T/H), see (Audibert & Bubeck, 2010; Gabillon et al., 2012).", "startOffset": 140, "endOffset": 188}, {"referenceID": 19, "context": "Active anomaly detection In the case anomaly detection, a common way to characterize anomalies is to describe them as naturally not concentrated (Steinwart et al., 2005).", "startOffset": 145, "endOffset": 169}, {"referenceID": 19, "context": "This illustrates the fact that as described in (Steinwart et al., 2005), the problem of anomaly detection is indeed a problem of level set detection - and so the problem of active anomaly detection is a problem of active level set detection on which we can use our approach as explained above.", "startOffset": 47, "endOffset": 71}, {"referenceID": 4, "context": "In the former, the goal of the learner is to identify the arm with the highest mean (Bubeck et al., 2009).", "startOffset": 84, "endOffset": 105}, {"referenceID": 2, "context": "In the latter, the goal is to maximize the sum of the samples collected by the algorithm up to time T (Auer et al., 1995).", "startOffset": 102, "endOffset": 121}, {"referenceID": 4, "context": "Such intuition is backed up by Theorem 1 of (Bubeck et al., 2009), which states that in the absence of additional information and with a fixed budget, the lower the regret suffered in the cumulative setting, expressed in terms of rewards, the higher the regret suffered in the identification problem, expressed in terms of probability of error.", "startOffset": 44, "endOffset": 65}, {"referenceID": 4, "context": "This intuition is formalized in (Bubeck et al., 2009) where the authors prove that no algorithm can achieve this without additional information.", "startOffset": 32, "endOffset": 53}, {"referenceID": 4, "context": "Our results therefore imply that the knowledge of \u03bc\u2217 by the learner is a sufficient information so that Theorem 1 of (Bubeck et al., 2009) does not hold anymore and there exists algorithms that solve both problems at the same time, as APT does.", "startOffset": 117, "endOffset": 138}, {"referenceID": 12, "context": "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).", "startOffset": 196, "endOffset": 320}, {"referenceID": 16, "context": "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).", "startOffset": 196, "endOffset": 320}, {"referenceID": 21, "context": "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).", "startOffset": 196, "endOffset": 320}, {"referenceID": 10, "context": "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).", "startOffset": 196, "endOffset": 320}, {"referenceID": 6, "context": "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).", "startOffset": 196, "endOffset": 320}, {"referenceID": 16, "context": "The upper bound and proof for this problem is a direct consequence of Theorem 2, and granted one has such extra-information, outperforms existing results for the fixed budget setting, see (Bubeck et al., 2013b; Kaufmann et al., 2015; Chen et al., 2014; Cao et al., 2015).", "startOffset": 188, "endOffset": 270}, {"referenceID": 10, "context": "The upper bound and proof for this problem is a direct consequence of Theorem 2, and granted one has such extra-information, outperforms existing results for the fixed budget setting, see (Bubeck et al., 2013b; Kaufmann et al., 2015; Chen et al., 2014; Cao et al., 2015).", "startOffset": 188, "endOffset": 270}, {"referenceID": 6, "context": "The upper bound and proof for this problem is a direct consequence of Theorem 2, and granted one has such extra-information, outperforms existing results for the fixed budget setting, see (Bubeck et al., 2013b; Kaufmann et al., 2015; Chen et al., 2014; Cao et al., 2015).", "startOffset": 188, "endOffset": 270}, {"referenceID": 12, "context": "If the complexity H were also known to the learner, the strategy in (Gabillon et al., 2012) would attain a similar performance.", "startOffset": 68, "endOffset": 91}, {"referenceID": 10, "context": "For comparison, we use the following methods which include the state of the art CSAR algorithm of (Chen et al., 2014) and two minor adaptations of known methods that are also suitable for our problem.", "startOffset": 98, "endOffset": 117}, {"referenceID": 12, "context": "UCB-type algorithm: The algorithm UCBE given and analyzed in (Audibert & Bubeck, 2010) is designed for finding the best arm - and its heuristic is to pull the arm that maximizes a UCB bound - see also (Gabillon et al., 2012) for an adaptation of this algorithm to the general TopM problem.", "startOffset": 201, "endOffset": 224}, {"referenceID": 12, "context": "From the theoretical analysis in the paper (Audibert & Bubeck, 2010; Gabillon et al., 2012), it is not hard to see that setting a \u2248 (T \u2212K)/H minimizes their upper bound, and that this algorithm attains the same expected loss as ours - but it requires the knowledge of H .", "startOffset": 43, "endOffset": 91}, {"referenceID": 10, "context": "CSAR: As mentioned before, this method is given in (Chen et al., 2014).", "startOffset": 51, "endOffset": 70}], "year": 2016, "abstractText": "We study a specific combinatorial pure exploration stochastic bandit problem where the learner aims at finding the set of arms whose means are above a given threshold, up to a given precision, and for a fixed time horizon. We propose a parameter-free algorithm based on an original heuristic, and prove that it is optimal for this problem by deriving matching upper and lower bounds. To the best of our knowledge, this is the first non-trivial pure exploration setting with fixed budget for which optimal strategies are constructed.", "creator": "LaTeX with hyperref package"}}}