{"id": "1605.08754", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Faster Eigenvector Computation via Shift-and-Invert Preconditioning", "abstract": "We give faster algorithms and improved sample complexities for estimating the top eigenvector of a matrix $\\Sigma$ -- i.e. computing a unit vector $x$ such that $x^T \\Sigma x \\ge (1-\\epsilon)\\lambda_1(\\Sigma)$:", "histories": [["v1", "Thu, 26 May 2016 03:53:00 GMT  (44kb,D)", "http://arxiv.org/abs/1605.08754v1", "Appearing in ICML 2016. Combination of work inarXiv:1509.05647andarXiv:1510.08896"]], "COMMENTS": "Appearing in ICML 2016. Combination of work inarXiv:1509.05647andarXiv:1510.08896", "reviews": [], "SUBJECTS": "cs.DS cs.LG math.NA math.OC", "authors": ["dan garber", "elad hazan", "chi jin", "sham m kakade", "cameron musco", "praneeth netrapalli", "aaron sidford"], "accepted": true, "id": "1605.08754"}, "pdf": {"name": "1605.08754.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Dan Garber", "Elad Hazan"], "emails": ["dgarber@ttic.edu", "ehazan@cs.princeton.edu", "chijin@eecs.berkeley.edu", "sham@cs.washington.edu", "cnmusco@mit.edu", "praneeth@microsoft.com", "asid@microsoft.com"], "sections": [{"heading": null, "text": "\u2022 Offline eigenvector estimation: Given an explicit A-Rn \u00b7 d with \u03a3 = A > A, we show how to get an approximate top eigenvector in the time O-Rp ([nnz (A) + d sr (A) gap2] \u00b7 log 1 /) and O-Rp ([nnz (A) 3 / 4 (d sr (A))) 1 / 4 \u221a gap] \u00b7 log 1 /). Here is nnz (A) the number of zeros in A, sr (A) def = A-2F-A-22is the stable rank, gap is the relative eigengap, and O-Rp hides log factors in d and gap. By separating the gap dependence from the nnz (A) term, our first runtime improves according to the classical power and Lanczos methods. It also improves the previous work with fast subspace embedding [AC09, CW13] and chastostic optimization [Lancoc] methods, which we apply significantly in relation to the classical power and shafts."}, {"heading": "1 Introduction", "text": "Calculation of the uppermost eigenvector of A > A is a fundamental problem in numerical linear algebra, applicable to principal component analysis [Jol02], spectral clustering and learning [NJW02, VW04], pagerank calculation and many other graph calculations [PBMW99, Kor03, Spi07]. In this thesis, we provide improved algorithms for calculating the uppermost eigenvector, both in the offline case, when A is explicit, and in the online or statistical case, when we access samples from a distribution D over Rd, and want to estimate the uppermost eigenvector of the Ea technique."}, {"heading": "1.1 Our Approach", "text": "In fact, it is as if this is a way in which most people are able to survive themselves. (...) It is not as if they are able to put themselves in a position to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "1.2 Our Results", "text": "This year it is so far that it will be able to do the mentioned for the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref"}, {"heading": "1.3 Previous Work", "text": "In fact, it is so that it is about a way in which it is about a way in which it is about the terms that spread in the individual countries of the world. (...) In fact, it is about a way in which it is about a way in which it is about the terminology. (...) In fact, it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about the terminology. (...) In fact, it is as if it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about and in which it is about a way in which it is about and in which it is about a way in which it is about and in which it is about a way in which it is about and in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about which it is about a way in which it is about and in which it is about a way in which it is about and in which it is about a way in which it is about which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about it is about and in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about"}, {"heading": "1.4 Paper Organization", "text": "Section 2 Review problem definitions and parameters for our runtime and sample limits. Section 3 Describe the method of shifted and inverted performance and show how it can be implemented with approximate system solutions. Section 4 Show how SVRG can be used to solve systems in our shifted matrix, and specify our main results for offline eigenvector calculation. Section 5 Show how you can use an online variant of SVRG to perform the method of shifted and inverted performance, and specify our main sampling complexity and runtime results in the statistical setting. Section 6 Show how you can efficiently estimate the shifting parameters required by our algorithms. Section 7 Specify a lower limit in the statistical setting, showing that our results are asymptotically optimal for a wide range of parameters. Section 8 Specify gap-free runtime limits that apply > if there is a gap."}, {"heading": "2 Preliminaries", "text": "We use [n] def = {1,..., n}. For a symmetric positive semidefinitive (PSD) matrix M we leave the condition that x > Mx \u2264 x > Mx and \u03bb1 (M),..., \u03bbd (M) denote their eigenvalues in descending order, with M N we denote the condition that x > Mx \u2264 x > Nx for all x."}, {"heading": "2.1 The Offline Problem", "text": "We get a matrix A \u0435Rn \u00b7 d with lines a (1),..., a (n) and want an approximation to the uppermost eigenvector of \u03a3 def = A > A. Especially for error parameters we want a unit vector x, so that x > \u044bx \u2265 (1 \u2212) \u03bb1 (\u03a3) is created."}, {"heading": "2.2 The Statistical Problem", "text": "We have access to an oracle that returns independent samples from a distribution D to Rd and want to calculate the uppermost eigenvector of \u03a3 def = Ea \u0445 D [aa >]."}, {"heading": "2.3 Problem Parameters", "text": "We parameterise the runtimes and sample complexities of our algorithms with respect to several natural properties of A, D and \u03a3. We define the eigenvalue gap by gapdef = \u03bb1 \u2212 \u03bb2\u03bb1. We use the following additional parameters for offline or statistical problems: \u2022 Offline problem: Let sr (A) def = [1] def = [2] F = [3] A [4] 22 have stable rank A. Note that we always have sr (A) \u2264 rank (A). Let nz (A) specify the number of non-zero entries in A. \u2022 Online problem: Let v (D) def = [(aa >) D [(aa >) 2] specify the number of entries in A [4]."}, {"heading": "3 Algorithmic Framework", "text": "This is where we develop our robust shift-and-invert framework. In Section 3.1 we give a basic overview of the framework, and in Section 3.2 we present the potential function with which we measure the progress of our algorithms. In Section 3.3 we show how to analyze the framework if you have access to an exact linear system solver, and in Section 3.4 we strengthen this analysis to work with an imprecise linear system solver. Finally, in Section 3.5 we discuss the initialization of the framework."}, {"heading": "3.1 Shifted-and-Inverted Power Method Basics", "text": "To have a large eigenvalue gap, we should, as discussed, set a constant c \u2265 0 to (1 + c \u00b7 gap) \u03bb1. Throughout this section, we assume that we have a rough estimate of \u03bb1 and gap and gap and fix \u03bb to be a value of (1 + gap150) \u03bb1 \u2264 (1 + gap100) \u03bb1. (See Section 6 on how to calculate such a gap.) For the rest of this section, we work with such a fixed value of \u03bb and therefore, for convenience, refer to B\u03bb as B. Note that \u03bbi (B \u2212 1) = 1\u03bbi (B) = 1 \u03bb \u2212 \u03bbi (B \u2212 \u03bbi) = 1 \u03bbi (B \u2212 1) \u03bb2 (B \u2212 1) > gap / 100. This large gap will ensure that we apply this method very quickly to convergence 1, which we will apply to the convergence of performance."}, {"heading": "3.2 Potential Function", "text": "Our analysis of the potential method focuses on the goal of maximizing the Rayleigh quotient = > > Gap closing, x > eigenvector error according to Rayleigh quotient x. Note that, as the following gap closing shows, this has a direct correlation to the error in maximizing the gap. \u2212 \u2212 If \u2264 \u2212 \u2212 gap, then there is a gap. \u2212 Lemma 1 \u00b7 gap. \u2212 Proof. Among all unit vectors x of this type, which = 1 \u2212 x > x x x x, a minimizer of the gap v > 1 x x, then has the form x = gap. \u2212 v1 + Lemma 1 \u00b7 gap. Proof. Among all unit vectors x x of this type, x x x x. \u2212 x, a minimizer of the gap v > 1 x. \u2212 x."}, {"heading": "3.3 Power Iteration", "text": "Here we show that the displacement \u2212 and reverse potentiality iteration actually makes progress in relation to our objective function, since there is an exact linear system solver for B. Formally, we show that applying B \u2212 1 to a vector x reduces the potential function G (x) geometrically. Theorem 4. Let us have x have a unit vector with < x, v1 > 6 = 0 and let us have x \u2212 1 applied to a vector x, i.e. the potential method update from B \u2212 1 to x. Then, under our assumption, we have x: G (x) \u2264 2 (B \u2212 1) \u2264 1 (B \u2212 1) G (x) \u2264 1 100 G (x). Note, however, that x \u2212 1 must no longer be a unit vector. G (x, v1) = G (cx, v1) for each scaling parameter c (B \u2212 1), so that the theorem must also have x \u2212 G (we have a corresponding letter to \u2212 \u00b2 and vice versa form x)."}, {"heading": "3.4 Approximate Power Iteration", "text": "We are now ready to prove our main findings, showing that each iteration of the shifted and inverted system (1) is a constant factor of expected progress on our potential function if (1) we have a sufficiently good x-number and an approximate number of x-numbers. (3) Can we estimate that the solver occasionally returns a solution that is completely orthogonal to v1, causing us to make unlimited progress on our potential function. (3) This third assumption is necessary because the second assumption is quite weak. An expected progress tied to the linear system solver, for example, allows us to occasionally return a fully orthogonal to v1, causing us to make unlimited progress on our potential function. (3) The third assumption allows us to reject potentially harmful updates and still ensure that we make progress in anticipation."}, {"heading": "3.5 Initialization", "text": "Theorem 5 and Corollary 6 show that we can quickly refine this approach by applying the method of the shifted and inverted start vector x0.Lemma 7 (random initialization quality). \u2212 This section is about how we get a good enough approximation to apply these results. \u2212 In this section we first set a simple limit on the quality of a randomly selected start vector x0.Lemma 7 (random initialization quality). \u2212 In this section we will x0 (0, I), and we will initialize x0 as x x 2, then with a probability greater than 1 \u2212 O (1 d10), we have: G (x0) \u2264 (B \u2212 1) d10.5 \u2264 15 1 \u221a Gap \u00b7 d10.5wo we (B \u2212 1) = 1 (B \u2212 1) / 2 (B \u2212 1) / 2) / 2 percent of probability (B \u2212 1)."}, {"heading": "4 Offline Eigenvector Computation", "text": "In this section we will show how to instantiate the framework of Section 3 to calculate an approximate top eigenvector in the offline setting. As discussed, in the offline setting we can trivially calculate the Rayleigh quotient of a vector in nnz (A) time, how to explicitly access A > A. Consequently, most of our work in this section is focused on showing how to efficiently solve linear systems in B in anticipation, so that we can solve corollary 6 of theorem 5.In Section 4.1 we will first show how Stochastic Variance Reduced Gradient (SVRG) [JZ13] can be applied to solve linear systems in the form Bx = b."}, {"heading": "4.1 SVRG Based Solver", "text": "We offer an algorithm for solving the more general problem, in which we are given a strongly convex function, which is a sum of possibly non-convex functions that follow smooth properties. We provide a general result for limiting the progress of an algorithm that can limit such a problem by non-uniform sampling in Theorem 9, and then, in the rest of this section, we show how we can limit the quantities required for solving linear systems in B.Theorem 9 (SVRG for Sums of Non-Convex Functions). Let us consider a set of functions that have a number of functions, a number of functions, a number of others, a number of mapping Rd \u00b2 n \u00b2. Let us leave f (x) = x) and let us leave xopt def = arg minx (x)."}, {"heading": "4.2 Accelerated Solver", "text": "However, we first show that the runtime in Theorem 12 can be accelerated in some cases. Specifically, Lemma 13 (Theorem 1.1 of [FGKS15b]) shows that a solver for a regularized version of a convex function f (x), we can make a quick solver for f (x) ourselves. Specifically, Lemma 13 (Theorem 1.1 of [FGKS15b]). Let f (x) be a-strong convex function and let xopt def = arg minx (x). For any result > 0 and any x0."}, {"heading": "4.3 Shifted-and-Inverted Power Method", "text": "Finally, we are able to combine the solvers from sections 4.1 and 4.2 with the framework of section 3 to get faster algorithms for the uppermost eigenvector calculation. Theorem 16 (Shifted-and-Inverted Power Method With SVRG). Let's leave B = \u03bbI \u2212 A > A for (1 + gap150) \u03bb1 (1 + gap100) \u03bb1 and let x0 \u0445 N (0, I) apply a random initial vector. If we run the inverted power method on B initialized with x0, let's use the SVRG solver from theorem 12 to apply approximately B \u2212 1 in each step, x returns so that with probability 1 \u2212 O (1 d10), x > a gap (1 \u2212) in total timeO (A) we (A) + d sr (A) gap2)."}, {"heading": "5 Online Eigenvector Computation", "text": "This setting is more difficult than the offline case. As there is no canonical matrix A and we only have access to the distribution D via random samples, we must show how we estimate both the Rayleigh quotient (Section 5.1) and solve the required linear systems as expected (Section 5.2). After doing this basic work, our main result is in Section 5.3. Ultimately, the results in this section allow us to use more efficient algorithms to calculate the uppermost eigenvector in the statistical setting as well as to improve the best known sample complexity for peak eigenvector calculation. As we show in Section 7, the limits we offer in this section are actually narrow for general distributions."}, {"heading": "5.1 Estimating the Rayleigh Quotient", "text": "Here we show how to calculate the Rayleigh quotient of a vector with respect to the value D > > i > j = > i > i = i > i = i > i = i = i = i = i = i = i = i =. Our analysis is standard - we first repeat the Rayleigh quotient with respect to its empirical value on a series of k samples and prove, based on Chebyshev's inequality, that the error in this sample is with constant probability low. We then repeat this procedure O (log (1 / p) times and output the median, which results in a good estimate with probability 1 \u2212 p. The formal statement of this result and its evidence covers the rest of this subsection. Theorem 18 (Online Rayleigh Quotient Estimation) \u2212 p. Therefore (0, 1], p [0, 1] and unit of measurement x-Set k = d4 v (D) \u2212 2e and m = O (1 log / leip) therein lies Rayo.Estimation (Rayo.18)."}, {"heading": "5.2 Solving the Linear system", "text": "We follow the general strategy of offline algorithms in Section 4, which replaces the traditional SVRG algorithms with the streaming SVRG algorithm of [FGKS15a]. \u2212 Just as in the offline case, we minimize f (x) = 12x > Bx \u2212 b > x and define a streaming SVRG (D) \u2212 a (x) def = 1 x > (2) x \u2212 b > x (13) the f (x) = Ea (x) x (x). The performance of the streaming SVRG [FGKS15a] is subject to three regularity parameters. \u2212 As in the offline case, we use the fact that f (\u00b7) the f (Ea) is highly variable and we need a balance parameter that we need."}, {"heading": "5.3 Online Shifted-and-Inverted Power Method", "text": "We apply the results in Section 5.1 and Section 5.2 to the Gap with Shift and Inverse Potential Method of Section 3 to give our main result in the online setting, an algorithm that quickly refines a rough approximation of v1 to a finer approximation. Theorem 25 (Online Shifted-and-Inverted Power Method - Warm Start) Leave B = \u03bbI \u2212 A for (1 + gap150) \u03bb1 \u2264 (1 + gap100) \u03bb1 and leave x0 to be any vector with G (x0). Leave the shifted and inverted potential method initialized to B \u2212 \u2212 \u2212 A for (1 + gap150) \u03bb1 \u2264 Gap (1 + gap100) and leave x0 to be any vector with G (x0)."}, {"heading": "6 Parameter Estimation for Offline Eigenvector Computation", "text": "In section 4, referring to theorems 5 and 8, we assume that we have knowledge of some things (1 + c1 \u00b7 Gap) (1 + c2 \u00b7 Gap) (1 + c2 \u00b7 Gap) (1 + c2 \u2212 Gap) (1 + c2). \u2212 In this section, we first assume that we have access to an oracle to calculate any constant. \u2212 \u2212 In this section, we will then show how we can achieve the same results when estimating the eigenvalue and eigengap. \u2212 In this section, we assume that we have access to an oracle to calculate any constant. \u2212 In this section, we will then show how we can achieve the same results when evaluating the eigenvalue and eigengap. \u2212 In this section, we assume that we have access to an oracle. \u2212 In this section, we have access to the constant. \u2212 In this section, we have access. \u2212 In this section, we assume that we have access to the eigenvalue and eigengap."}, {"heading": "7 Lower Bounds", "text": "Here we show that our online eigenvector estimation algorithm (theorem 25) is asymptotically optimal - as sample size increases, it achieves optimal accuracy as a function of sample size. We rely on the following lower limit for eigenvector estimates in the Gaussian peak model: Lemma 31 (lower limit for the Gaussian mirror model [BJNP13]). Suppose that data is generated asai = \u2212 V? + Zi (16) with peaks N (0, 1) and Zi N (0, Id). Suppose there is any estimator of the uppermost eigenvector v?."}, {"heading": "8 Gap-Free Bounds", "text": "In this section, we show that our techniques can be easily extended to obtain gapless runtime limits for the regime in which there is a gap. In many ways, in fact, these limits are much easier to achieve than the gap-dependent limits because they require less careful error analysis. Let's assume our error parameter and the number of eigenvalues of solutions (1 \u2212 / 2) that are larger than the number of eigenvalues of solutions (1 \u2212 / 2). Let's have columns that are equal to all bottom eigenvectors with eigenvalues. < (1 \u2212 / 2) Let's have columns that are equal to all bottom eigenvectors with eigenvalues. < (1 \u2212 / 2) Let's have Vthave columns that are equal to the remaining top eigenvectors. Let's define a simple modified potential: G (x)."}, {"heading": "9 Acknowledgements", "text": "Sham Kakade recognizes funding from the Washington Research Foundation for innovation in data-intensive discovery."}, {"heading": "A Appendix", "text": "Lemma 37 (Eigenvector Estimation via Spectral Norm Matrix (A > > BB)."}], "references": [{"title": "The fast Johnson-Lindenstrauss transform and approximate nearest neighbors", "author": ["Nir Ailon", "Bernard Chazelle"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Ailon and Chazelle.,? \\Q2009\\E", "shortCiteRegEx": "Ailon and Chazelle.", "year": 2009}, {"title": "Minimax bounds for sparse PCA with noisy high-dimensional data", "author": ["Aharon Birnbaum", "Iain M Johnstone", "Boaz Nadler", "Debashis Paul"], "venue": "Annals of Statistics,", "citeRegEx": "Birnbaum et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Birnbaum et al\\.", "year": 2013}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L\u00e9on Bottou"], "venue": "In Proceedings of COMPSTAT,", "citeRegEx": "Bottou.,? \\Q2010\\E", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "Uniform sampling for matrix approximation", "author": ["Michael B Cohen", "Yin Tat Lee", "Cameron Musco", "Christopher Musco", "Richard Peng", "Aaron Sidford"], "venue": "In Proceedings of the 6th Conference on Innovations in Theoretical Computer Science (ITCS),", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "Primal method for ERM with flexible minibatching schemes and non-convex losses", "author": ["Dominik Csiba", "Peter Richt\u00e1rik"], "venue": null, "citeRegEx": "Csiba and Richt\u00e1rik.,? \\Q2015\\E", "shortCiteRegEx": "Csiba and Richt\u00e1rik.", "year": 2015}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L Clarkson", "David P Woodruff"], "venue": "In Proceedings of the 45th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Competing with the empirical risk minimizer in a single pass", "author": ["Roy Frostig", "Rong Ge", "Sham M Kakade", "Aaron Sidford"], "venue": "In Proceedings of the 28th Annual Conference on Computational Learning Theory (COLT),", "citeRegEx": "Frostig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Frostig et al\\.", "year": 2015}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham M Kakade", "Aaron Sidford"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Frostig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Frostig et al\\.", "year": 2015}, {"title": "Fast and simple PCA via convex optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": null, "citeRegEx": "Garber and Hazan.,? \\Q2015\\E", "shortCiteRegEx": "Garber and Hazan.", "year": 2015}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["Chi Jin", "Sham M Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford"], "venue": null, "citeRegEx": "Jin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2015}, {"title": "Principal component analysis", "author": ["Ian Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "Jolliffe.,? \\Q2002\\E", "shortCiteRegEx": "Jolliffe.", "year": 2002}, {"title": "On spectral graph drawing", "author": ["Yehuda Koren"], "venue": "In Computing and Combinatorics,", "citeRegEx": "Koren.,? \\Q2003\\E", "shortCiteRegEx": "Koren.", "year": 2003}, {"title": "Powers of tensors and fast matrix multiplication", "author": ["Fran\u00e7ois Le Gall"], "venue": "In Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation,", "citeRegEx": "Gall.,? \\Q2014\\E", "shortCiteRegEx": "Gall.", "year": 2014}, {"title": "A universal catalyst for first-order optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition", "author": ["Cameron Musco", "Christopher Musco"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Musco and Musco.,? \\Q2015\\E", "shortCiteRegEx": "Musco and Musco.", "year": 2015}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y Ng", "Michael I Jordan", "Yair Weiss"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2002}, {"title": "The PageRank citation ranking: bringing order to the Web", "author": ["Lawrence Page", "Sergey Brin", "Rajeev Motwani", "Terry Winograd"], "venue": null, "citeRegEx": "Page et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Page et al\\.", "year": 1999}, {"title": "Numerical methods for large eigenvalue problems", "author": ["Yousef Saad"], "venue": null, "citeRegEx": "Saad.,? \\Q1992\\E", "shortCiteRegEx": "Saad.", "year": 1992}, {"title": "Convergence of stochastic gradient descent for PCA", "author": ["Ohad Shamir"], "venue": null, "citeRegEx": "Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Shamir.", "year": 2015}, {"title": "Fast stochastic algorithms for SVD and PCA: Convergence properties and convexity", "author": ["Ohad Shamir"], "venue": null, "citeRegEx": "Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Shamir.", "year": 2015}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["Ohad Shamir"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Shamir.", "year": 2015}, {"title": "Spectral graph theory and its applications", "author": ["Daniel A Spielman"], "venue": "In null,", "citeRegEx": "Spielman.,? \\Q2007\\E", "shortCiteRegEx": "Spielman.", "year": 2007}, {"title": "Global convergence of stochastic gradient descent for some non-convex matrix problems", "author": ["Christopher D Sa", "Christopher Re", "Kunle Olukotun"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Sa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sa et al\\.", "year": 2015}, {"title": "An introduction to matrix concentration inequalities", "author": ["Joel A Tropp"], "venue": null, "citeRegEx": "Tropp.,? \\Q2015\\E", "shortCiteRegEx": "Tropp.", "year": 2015}, {"title": "A spectral algorithm for learning mixture models", "author": ["Santosh Vempala", "Grant Wang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Vempala and Wang.,? \\Q2004\\E", "shortCiteRegEx": "Vempala and Wang.", "year": 2004}, {"title": "Multiplying matrices faster than CoppersmithWinograd", "author": ["Virginia Vassilevska Williams"], "venue": "In Proceedings of the 44th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Williams.,? \\Q2012\\E", "shortCiteRegEx": "Williams.", "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "We give faster algorithms and improved sample complexities for estimating the top eigenvector of a matrix \u03a3 \u2013 i.e. computing a unit vector x such that x>\u03a3x \u2265 (1\u2212 )\u03bb1(\u03a3): \u2022 Offline Eigenvector Estimation: Given an explicit A \u2208 Rn\u00d7d with \u03a3 = A>A, we show how to compute an approximate top eigenvector in time \u00d5 ([ nnz(A) + d sr(A) gap2 ] \u00b7 log 1/ ) and \u00d5 ([ nnz(A)(d sr(A)) \u221a gap ] \u00b7 log 1/ ) . Here nnz(A) is the number of nonzeros in A, sr(A) def = \u2016A\u20162F \u2016A\u201622 is the stable rank, gap is the relative eigengap, and \u00d5(\u00b7) hides log factors in d and gap. By separating the gap dependence from the nnz(A) term, our first runtime improves upon the classical power and Lanczos methods. It also improves prior work using fast subspace embeddings [AC09, CW13] and stochastic optimization [Sha15c], giving significantly better dependencies on sr(A) and . Our second running time improves these further when nnz(A) \u2264 d sr(A) gap2 . \u2022 Online Eigenvector Estimation: Given a distribution D with covariance matrix \u03a3 and a vector x0 which is an O(gap) approximate top eigenvector for \u03a3, we show how to refine to an approximation using O ( v(D) gap\u00b7 ) samples from D. Here v(D) is a natural notion of variance. Combining our algorithm with previous work to initialize x0, we obtain improved sample complexity and runtime results under a variety of assumptions on D. We achieve our results using a general framework that we believe is of independent interest. We give a robust analysis of the classic method of shift-and-invert preconditioning to reduce eigenvector computation to approximately solving a sequence of linear systems. We then apply fast stochastic variance reduced gradient (SVRG) based system solvers to achieve our claims. We believe our results suggest the general effectiveness of shift-and-invert based approaches and imply that further computational gains may be reaped in practice. \u2217This paper combines work first appearing in [GH15] and [JKM15] ar X iv :1 60 5. 08 75 4v 1 [ cs .D S] 2 6 M ay 2 01 6", "creator": "LaTeX with hyperref package"}}}