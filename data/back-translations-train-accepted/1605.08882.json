{"id": "1605.08882", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2016", "title": "Optimal Learning for Multi-pass Stochastic Gradient Methods", "abstract": "We analyze the learning properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. In particular, we consider the square loss and show that for a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on a unifying approach, encompassing both batch and stochastic gradient methods as special cases.", "histories": [["v1", "Sat, 28 May 2016 12:11:22 GMT  (76kb,D)", "http://arxiv.org/abs/1605.08882v1", "31 pages, 6 figures"], ["v2", "Sat, 21 Oct 2017 22:55:48 GMT  (91kb,D)", "http://arxiv.org/abs/1605.08882v2", "Extended versions of the previous one. Fixed some typos, JMLR, 2017"]], "COMMENTS": "31 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["junhong lin", "lorenzo rosasco"], "accepted": true, "id": "1605.08882"}, "pdf": {"name": "1605.08882.pdf", "metadata": {"source": "CRF", "title": "Optimal Learning for Multi-pass Stochastic Gradient Methods", "authors": ["Junhong Lin", "Lorenzo Rosasco"], "emails": ["jhlin5@hotmail.com", "lrosasco@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it is all about one single person who is able to move, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to see the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to see the world, to see the world, to see the world, to see the world, to see the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the"}, {"heading": "2 Learning with SGM", "text": "We begin with the introduction of the learning environment we are considering and then describe the SGM learning algorithm. Subsequently [19] we consider the formulation to be close to the setting of functional regression and treat the setting of the reproductive Hilbert space (RKHS) as special cases. In particular, it is reduced to a linear regression for finite dimensions."}, {"heading": "2.1 Learning Problems", "text": "Let H be a separable Hilbert space, where the inner product and the induced standard are denoted by < \u00b7, \u00b7 > H and vice versa. Let the input space X H and the output space Y R. Let \u03c1 be an unknown probability measure to Z = X \u00b7 Y, \u03c1X (\u00b7) the induced limit measure to X and (\u00b7 x) the conditional probability measure to Y with respect to x-X and \u03c1.In view of the square loss function, the problem being investigated is the minimization of the therisk, inf-H E (\u03c9), E (\u03c9) = x-Y (< \u03c9, x-H \u2212 y) 2dB (x, y), (1), if the measurement is only by a sample z = {zi = (xi, yi)} mi = 1 of the size m \u00b2 N, independently and identically distributed (i.i.d.). In the following, let us measure the quality of an approximate solution z = (xi), yi = independently of the size m \u00b2 and i.i."}, {"heading": "2.2 Stochastic Gradient Method", "text": "Let us examine the following SGM (with mini-stacks). Algorithm 1. Let us leave b = [m]. For any sample z, the b-minibatch stochastic gradient method is defined by \u03c91 = 0 and\u03c9t + 1 = \u03c9t \u2212 \u03b7t 1b bt \u2211 i = b (t \u2212 1) + 1 (< \u03c9t, xji > H \u2212 yji) xji, t = 1,.., T, (4), where {\u03b7t > 0} can be a step size sequence. Here, j1, j2, \u00b7, jbT are independent and identically distributed (i.i.d.) random variables from the even distribution on [m] 1.Different decisions for the (mini) stack size b can lead to different algorithms. In particular, for b = 1, the above algorithm corresponds to a simple SGM, while for b = m it is a stochastic version of the stack gradient. The objective of this paper is the transcription of this {we assume the risk for T = 1."}, {"heading": "3 Main Results with Discussions", "text": "In this section we first make some basic assumptions. Then we present and discuss our main results. 1Note that the random variables j1, \u00b7 \u00b7 \u00b7, jbT are conditionally independent of the sample z."}, {"heading": "3.1 Assumptions", "text": "We first make the following assumption. Assumption 1: There are constants M: 0 = > self-identification (and v: 1) 1, [so that] 2, [2], [3], [4], [5], [5], [5], [5], [6], [6], [6], [7], [7], [7], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [9], [9], [7], [7], [7], [7], [7], [7], [7], [8], [8], [8], [8], [8], [8], [8], [9], [9], [7], [7], [7], [7], [7], [7], [7], [8], [8], [8, [8], [8, [8], [9], [9], [9], [9], [9], [9], [9], [9], [9], [9], [9], [9], [11, [11], [11, 11, 11, 11, 11, [11, 11, 11, 11, [11], [11, 11, 11, 11, [11], 11, [11], 11, [11], 11, 11, [11], [11], 11, [11], [11], [11], [11], 11, [11], [11], [11], [11], [11], 12, 12, 12], 12, 12, 12, [12], 12, [12], 12, 12, [12, 12], [12], [12, 12], 12, 12, 12, [12], 12, 12, [12],], [11], [11, 11], 11, 11, 11, 11, 11, [, 11, 11, 11, [, 11], 11, 11,"}, {"heading": "3.2 Main Results", "text": "We start with the following corollary sequence, which is a simplified version of our main results that goes beyond the specified data."}, {"heading": "A smaller p may lead to a larger bias, while a larger p may lead to a larger sample error. From", "text": "The second sequence provides error limits for SGM with a fixed mini-batch size and a fixed step size (which depends on the number of samples); the second sequence provides error limits for SGM with a fixed mini-batch size (which depends on the number of samples); the second sequence shows error rates for SGM with a fixed mini-batch size and a fixed step size (which depends on the number of samples); the second sequence is that SGM has fewer errors with a maximum probability of at least 1 \u2212 1 / m; and the third sequence is that SGM has fewer errors with at least 1 \u2212 1 / m."}, {"heading": "3.3 Discussions", "text": "We compare our results with previous work. For non-parametric regression with square loss, a pass SGM in, for example, [4, 22, 5, 6] has been investigated. In particular, [4] has proven that capacity independent rate of order O (m \u2212 2\u0432 2\u0432 + 1 logm) with a fixed step variable \u03b7'm \u2212 2\u0432 + 1, and [6] derived capacity dependent error limits of order O (m \u2212 2min (1) 2min (1) + \u03b3) (if 2\u0445 + \u03b3 > 1) for the average. Also, note that a regularized version of SGM is independent in [5] where the derived convergence rate there is of order O (m \u2212 2\u0445 m + 1), provided that this convergence rate [12, 1] is better than this convergence rate, that our rates from (10) are comparable, either taking into account the capacity state, or allowing a wider regularity parameter."}, {"heading": "3.4 Error Decomposition", "text": "The key to our proof is a novel error decomposition, which can also be used in the analysis of other learning algorithms. (J) First, we present two sequences. (J) Population siteration is defined by (X), (T), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H), (H)."}, {"heading": "4 Numerical Simulations", "text": "To illustrate our theoretical results and the error decomposition, we first performed some simulations on a simple problem. We constructed m = 100 i.i.d. training examples for the form y = f\u03c1 (xi) + \u03c9i. Here, the regression function f\u03c1 (x) = | x \u2212 1 / 2 | \u2212 2, the input point xi is uniformly distributed in [0, 1], and \u03c9i is a Gaussian noise with zero mean and standard deviation 1, for each i error. We perform three experiments with the same H, an RKHS in connection with a Gaussian Kernel K (x, x \u00b2) = exp (\u2212 x \u00b2) 2 / (2\u03c32), in which the first experiment in which we run mini-batch SGM, in which the mini-batch batch size b = 270 and the step size of the batch batch batch size b = 1 / (8 \u00d7 m)."}, {"heading": "Acknowledgments", "text": "This material is based on work supported by the Center for Brains, Minds and Machines (CBMM) and awarded the CCF-1231216 by the NSF STC. L. R. acknowledges the financial support of the Italian Ministry of Education, University and Research FIRB project RBFR12M3AC."}, {"heading": "A Preliminary", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Notation", "text": "We first introduce some notations. < < < < / p > p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p (H) p) p (H) p (H) p) p (H) p (H) p (H) p (H) p (H) p) p (H) p (H) p (H) p (H) p) p (H) p) p (H) p) p (H) p) p (H) p) p (H) p) p. \"p.\" p (H) p. \"p.\" p. \"p.\" p (H) p. \"p.\" p. \"p.\" p (H) p. \"p.\" p. \"p.\" p. \"p (H) p.\" p. \"p.\" p) p. \"p.\" p. (H) p. \"p.\" p. \"p) p. (H) p.\" p. \"p.\" p) p. \"p. (H) p.\" p. \"p.\" p. \"p) p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p) p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p. (H) p. (H) p) p. (H) p) p) p.\" p. (H) p. (H) p. \"p. (H) p. (H) p. (H) p. (H) p. (H) p) p. (H) p. (H) p. (H) p. (H) p. (H) p.) p. (H) p. (H) p. (H) p. (H) p. (H) p.) p."}, {"heading": "A.2 Concentration Inequality", "text": "We need the following concentration result for the Hilbert space, which is used as a random variable in Caponnetto and De Vito [13] and is based on the results in Pinelis and Sachanenko [25]. Lemma A.1. Let us allow w1, \u00b7, wm be i.i.d random variables in a Hilbert space with the norms B and \u03c32. Let us assume that there are two positive constants B and \u03c32, such as E [\u0109w1 \u2212 E [w1] l] \u2264 12 l! Bl \u2212 2\u04412, \u0441l \u04322. (22) Then the following applies to all 0 < \u043c < < 1, with a probability of at least 1 \u2212 \u043c, i.e. 1 wm \u00b2 k = 1 wm \u2212 E [w1]."}, {"heading": "In particular, (22) holds if", "text": "These are grades 1, 2, B and E as well as grades 2 and 3. (23)"}, {"heading": "A.3 Basic Estimates", "text": "Lemma A.2 t \u00b2 t t \u00b2 t t \u00b2 t t \u00b2 t \u00b2 k \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t"}, {"heading": "B Bias", "text": "In this section, we will develop upper limits for bias, i.e., we can form a compact, self-complementary operator on a separable Hilbert space. Suppose we introduce the following problem, the proof of which is the idea of [4, 5].Lemma B.1. Let L be a compact, self-complementary operator on a separable Hilbert space. (24) Suppose the proof is the order of the eigenvalues of L. We have the order of eigenvalues of L. We have the order of eigenvalues of L. tk + 1 (L) lump + 1 (L) lump + 1 (1 \u2212 l) l = k + 1 (1 \u2212 l) lump."}, {"heading": "In particular, if \u03b7t = \u03b7t", "text": "For the sake of completeness, we provide a proof. As \u00b5t + 1 is given by (19), the introduction is given with (21), \u00b5t + 1 = \u00b5t \u2212 \u03b7t (T \u00b5t \u2212 S \u03c1fH). (29) Therefore, we provide the proof with (29). \u2212 tSuttle (T \u00b5t \u2212 S \u0445fH) = S\u03c1\u00b5t \u2212 tL (S\u0445\u00b5t \u2212 fH). (30) Subtract both sides with fH, S\u03c1\u00b5t + 1 \u2212 fH = (I \u2212 fH)."}, {"heading": "C Sample Variance", "text": "In this section, the aim is to estimate the variance of the sample, i.e. there is a provisional perspective for the estimation of E [Surface + Surface Freight] (Surface + Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) (Surface Freight) () (() () (() () (() () (() () (() () (() (()) ((()) ((()) ((()) ((()) ((()) ((()) ((()) ((()) ((()) ((()) ((()) ((()) ((()) ((()) ((()) (((()) ((()) ((()) (((()) (((()) ((()) ((((())) ((((()) (((()) (((()) (((()) (((()) ((((())) ((((((())) (((((())) (((()) ((((())) (((((())) (((((((()))) (((((((()))) ((((((()))) ((((((()))) (((((())) (((()) (((((()))) ((((((())))) (((((((()))) (((((())"}, {"heading": "D Computational Variance", "text": "In this section, we estimate the arithmetical variance, E [\u0441 S\u03c1\u03c9t \u2212 S\u03c1\u03bdt \u0442 2\u03c1]. For this purpose, a series of lemmas will necessarily be introduced."}, {"heading": "D.1 Bounding the Empirical Risk", "text": "This subsection is devoted to the upper limit EJ [Ez (\u03c9l)]. The process is based on some tools from the convex analysis and a decomposition with respect to the weighted averages and the last iterations from [22, 30]. We begin with the introduction of the following problem, a fact based on the square loss of \"special properties.\" Lemma D.1. If each sample z, and l \u00b2 N, is used, the number of iterations from Jl \u2212 \u2212 l (Ez (\u03c9l) \u2212 l (Ez) \u2212 l (Ez) \u2212 l (Ez) \u2212 l (Ez) \u2212 l (2H \u2212 2H) cannot be taken into account. (45) Proof. \u2212 l Since the beginning of the process, (4) \u2212 l (4), both sides of the expectation (4) have been subtracted by the inclusion of the square H standard, and the extension of the square h (1H \u2212 2H) has not been taken into account."}, {"heading": "D.3 Deriving Error Bounds", "text": "With Lemmas D.4 and D.6 we are ready to estimate the arithmetical variance = < b = > b = > J + gt = 2\u03c1, as follows. Proposition D.7: Assumption (40) applies to any case more than 1 / 2, (51) and (53). Then we have for all t + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ, EJ + 1, EJ + 1, EJ + 1, EJ, EJ + 1, EJ + 1, EJ, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1, EJ + 1,"}, {"heading": "Then, for all t \u2208 [T ],", "text": "This applies to Proposition D.7, in order to verify the result (51). Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note. Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note. Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Comment: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Comment: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note: Note:"}, {"heading": "E Deriving Total Error Bounds", "text": "The purpose of this section is to derive the overall error limits."}, {"heading": "E.1 Attainable Case", "text": "Under assumptions 1, 2 and 3, we have the following general theorem to prove our main results given in Section 3.Theorem E.1. Under assumptions 1, 2 and 3, we leave assumptions 1, 2, 3, 4, 5, 5, 5, 8 (log t + 1), 5 (log t + 1), 5 (log t + 1), 6 (log t + 1), 8 (log t + 1), 8 (log t + 1), 8 (log t + 1), 8 (log t + 1), 8 (log t + 1), 8 (log t + 1), 8 (log t + 1), 8 (log t + 1), 8 (log t + 1). (59) If for some assumptions there is 0, 1), m (18p \u00b2 (18p \u00b2 T), 8 (27p \u00b2 T), 1 /, (60) then it is probable that at least 1 \u2212 3 applies to all t [T], EJ [E (+ 1), EJ (+ 1), E\u00b2 (+ 2p \u2212 2\u043c), + 2p \u2212 \u043c (1)."}, {"heading": "Here, q1 = 2R", "text": "We may prove that the Condition (60) ensures that we comply with the Condition (60) that we comply with the Condition (60) that we comply with the Condition (60) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (60) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we may comply with the Condition (60) that we comply with the Condition (60) that we comply with the Condition (60) that we comply with the Condition (60) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we may comply with the Condition (2) that we comply with the Condition (2) that we comply with the Condition (2) that we may comply with the Condition (2 that we comply with the Condition (2) that we may comply with the Condition (2) that we."}, {"heading": "E.2 Non Attainable Case", "text": "(), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (, (), (), (), (), (, (), (, (), (, (), (, (), (, (), (, (), (, (), (, (), (, (), (), (, (), (, (), (, (), (, (), (, (), (), (, (), (), (, (), (, (), (), (, (, (), (), (, (, (,), (,), (, (,), (, (,), (, (,), (,), (, (,), (, (,), (,), (,), (, (,), (, (,), (, (,), (, (,), (,), (, (,), (, (,), (,), (,), (,), (, (,), (, (,), (,), (, (,), (,), (, (, (,), (,), (, (,), (,), (, (, (,),), (, (,), (, (,), (, (, (,), (, (,),), (,), (, (,), (,), (, (,), (, ("}], "references": [{"title": "The tradeoffs of large scale learning", "author": ["Olivier Bousquet", "L\u00e9on Bottou"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Nicolo Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["Arkadi Nemirovski", "Anatoli Juditsky", "Guanghui Lan", "Alexander Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Online gradient descent learning algorithms", "author": ["Yiming Ying", "Massimiliano Pontil"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence", "author": ["Pierre Tarres", "Yuan Yao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Non-parametric stochastic approximation with large step sizes", "author": ["Aymeric Dieuleveut", "Francis Bach"], "venue": "arXiv preprint arXiv:1408.0361,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Simultaneous model selection and optimization through parameter-free stochastic learning", "author": ["Francesco Orabona"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Introduction to Optimization", "author": ["Boris T Poljak"], "venue": "Optimization Software,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1987}, {"title": "Stochastic subgradient methods", "author": ["Stephen Boyd", "Almir Mutapcic"], "venue": "Notes for EE364b,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Generalization properties and implicit regularization of multiple passes SGM", "author": ["Junhong Lin", "Raffaello Camoriano", "Lorenzo Rosasco"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Learning bounds for kernel regression using effective data dimensionality", "author": ["Tong Zhang"], "venue": "Neural Computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Optimal rates for the regularized least-squares algorithm", "author": ["Andrea Caponnetto", "Ernesto De Vito"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Optimization for Machine Learning", "author": ["Suvrit Sra", "Sebastian Nowozin", "Stephen J Wright"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Machine learning. Coursera", "author": ["Andrew Ng"], "venue": "Standford University,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Learning theory estimates via integral operators and their approximations", "author": ["Steve Smale", "Ding-Xuan Zhou"], "venue": "Constructive Approximation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Learning with incremental iterative regularization", "author": ["Lorenzo Rosasco", "Silvia Villa"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Learning Theory: an Approximation", "author": ["Felipe Cucker", "Ding-Xuan Zhou"], "venue": "Theory Viewpoint,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Support Vector Machines", "author": ["Ingo Steinwart", "Andreas Christmann"], "venue": "Springer Science Business Media,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes", "author": ["Ohad Shamir", "Tong Zhang"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Remarks on inequalities for large deviation probabilities", "author": ["IF Pinelis", "AI Sakhanenko"], "venue": "Theory of Probability & Its Applications,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1986}, {"title": "On early stopping in gradient descent learning", "author": ["Yuan Yao", "Lorenzo Rosasco", "Andrea Caponnetto"], "venue": "Constructive Approximation,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Less is more: Nystr\u00f6m computational regularization", "author": ["Alessandro Rudi", "Raffaello Camoriano", "Lorenzo Rosasco"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "User-friendly tools for random matrices: An introduction", "author": ["Joel A Tropp"], "venue": "Technical report, DTIC Document,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "On some extensions of bernstein\u2019s inequality for self-adjoint operators", "author": ["Stanislav Minsker"], "venue": "arXiv preprint arXiv:1112.5448,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Modern machine learning applications require computational approaches that are at the same time statistically accurate and numerically efficient [1].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 141, "endOffset": 159}, {"referenceID": 2, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 141, "endOffset": 159}, {"referenceID": 3, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 141, "endOffset": 159}, {"referenceID": 4, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 141, "endOffset": 159}, {"referenceID": 5, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 141, "endOffset": 159}, {"referenceID": 6, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 141, "endOffset": 159}, {"referenceID": 7, "context": "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).", "startOffset": 192, "endOffset": 195}, {"referenceID": 3, "context": "In particular, recent works show how the step-size can be seen to play the role of a regularization parameter whose choice controls the bias and variance properties of the obtained solution [4, 5, 6].", "startOffset": 190, "endOffset": 199}, {"referenceID": 4, "context": "In particular, recent works show how the step-size can be seen to play the role of a regularization parameter whose choice controls the bias and variance properties of the obtained solution [4, 5, 6].", "startOffset": 190, "endOffset": 199}, {"referenceID": 5, "context": "In particular, recent works show how the step-size can be seen to play the role of a regularization parameter whose choice controls the bias and variance properties of the obtained solution [4, 5, 6].", "startOffset": 190, "endOffset": 199}, {"referenceID": 8, "context": "While the role of multiple passes is well understood if the goal is empirical risk minimization [9], its effect with respect to generalization is less clear and a few recent works have recently started to tackle this question.", "startOffset": 96, "endOffset": 99}, {"referenceID": 9, "context": "In particular, results in this direction have been derived in [10] and [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "In particular, results in this direction have been derived in [10] and [11].", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "The main shortcoming of these latter results is that they are in the worst case, in the sense that they do not consider the possible effect of capacity assumptions [12, 13] shown to lead to faster rates for other learning approaches such as Tikhonov regularization.", "startOffset": 164, "endOffset": 172}, {"referenceID": 12, "context": "The main shortcoming of these latter results is that they are in the worst case, in the sense that they do not consider the possible effect of capacity assumptions [12, 13] shown to lead to faster rates for other learning approaches such as Tikhonov regularization.", "startOffset": 164, "endOffset": 172}, {"referenceID": 13, "context": "Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17].", "startOffset": 125, "endOffset": 141}, {"referenceID": 14, "context": "Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17].", "startOffset": 125, "endOffset": 141}, {"referenceID": 15, "context": "Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17].", "startOffset": 125, "endOffset": 141}, {"referenceID": 16, "context": "Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17].", "startOffset": 125, "endOffset": 141}, {"referenceID": 17, "context": "In particular, we show for the first time that multipass SGM with early stopping and a universal step-size choice can achieve optimal learning bounds, matching those of ridge regression [18, 13].", "startOffset": 186, "endOffset": 194}, {"referenceID": 12, "context": "In particular, we show for the first time that multipass SGM with early stopping and a universal step-size choice can achieve optimal learning bounds, matching those of ridge regression [18, 13].", "startOffset": 186, "endOffset": 194}, {"referenceID": 18, "context": "Finally we note that a recent work [19] is tightly related to the analysis in the paper.", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "The generalization properties of a multi-pass incremental gradient are analyzed in [19], for a cyclic, rather than a stochastic, choice of the gradients and with no mini-batches.", "startOffset": 83, "endOffset": 87}, {"referenceID": 18, "context": "The analysis in this latter case appears to be harder and results in [19] give good learning bounds only in restricted setting and considering iterates rather than the excess risk.", "startOffset": 69, "endOffset": 73}, {"referenceID": 18, "context": "Compared to [19] our results show how stochasticity can be exploited to get faster capacity dependent rates and analyze the role of mini-batches.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "Following [19], the formulation we consider is close to the setting of functional regression, and covers the reproducing kernel Hilbert space (RKHS) setting as special cases.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "Under Assumption (3), L can be proved to be positive trace class operators, and hence L with \u03b6 \u2208 R can be defined by using the spectrum theory [20].", "startOffset": 143, "endOffset": 147}, {"referenceID": 19, "context": "The above assumption is fairly standard [20, 19] in non-parametric regression.", "startOffset": 40, "endOffset": 48}, {"referenceID": 18, "context": "The above assumption is fairly standard [20, 19] in non-parametric regression.", "startOffset": 40, "endOffset": 48}, {"referenceID": 20, "context": "In particular, for \u03b6 = 0, we are assuming \u2016fH\u2016\u03c1 < \u221e, while for \u03b6 = 1/2, we are requiring fH \u2208 H\u03c1, since [21, 19] H\u03c1 = L(L(H, \u03c1X)).", "startOffset": 104, "endOffset": 112}, {"referenceID": 18, "context": "In particular, for \u03b6 = 0, we are assuming \u2016fH\u2016\u03c1 < \u221e, while for \u03b6 = 1/2, we are requiring fH \u2208 H\u03c1, since [21, 19] H\u03c1 = L(L(H, \u03c1X)).", "startOffset": 104, "endOffset": 112}, {"referenceID": 11, "context": "(7) The left-hand side of (7) is the so-called effective dimension, or the degrees of freedom [12, 13].", "startOffset": 94, "endOffset": 102}, {"referenceID": 12, "context": "(7) The left-hand side of (7) is the so-called effective dimension, or the degrees of freedom [12, 13].", "startOffset": 94, "endOffset": 102}, {"referenceID": 20, "context": "It can be related to covering/entropy number conditions, see [21] for further details.", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "The above result asserts that, at p\u2217 passes over the data, the simple SGM with fixed step-size achieves optimal learning error bounds, matching those of ridge regression [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 20, "context": "Using an argument similar to that in Chapter 6 from [21], it is possible to show that this procedure can achieve the same convergence rate.", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "First, the upper bound in (10) is optimal up to a logarithmic factor, in the sense that it matches the minimax lower rate in [13].", "startOffset": 125, "endOffset": 129}, {"referenceID": 3, "context": "6 recovers the result in [4] for one pass SGM.", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": ", [4, 22, 5, 6].", "startOffset": 2, "endOffset": 15}, {"referenceID": 21, "context": ", [4, 22, 5, 6].", "startOffset": 2, "endOffset": 15}, {"referenceID": 4, "context": ", [4, 22, 5, 6].", "startOffset": 2, "endOffset": 15}, {"referenceID": 5, "context": ", [4, 22, 5, 6].", "startOffset": 2, "endOffset": 15}, {"referenceID": 3, "context": "In particular, [4] proved capacity independent rate of order O(m\u2212 2\u03b6 2\u03b6+1 logm) with a fixed step-size \u03b7 ' m\u2212 2\u03b6 2\u03b6+1 , and [6] derived capacity dependent error bounds of order O(m 2min(\u03b6,1) 2min(\u03b6,1)+\u03b3 ) (when 2\u03b6 + \u03b3 > 1) for the average.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "In particular, [4] proved capacity independent rate of order O(m\u2212 2\u03b6 2\u03b6+1 logm) with a fixed step-size \u03b7 ' m\u2212 2\u03b6 2\u03b6+1 , and [6] derived capacity dependent error bounds of order O(m 2min(\u03b6,1) 2min(\u03b6,1)+\u03b3 ) (when 2\u03b6 + \u03b3 > 1) for the average.", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m\u2212 2\u03b6 2\u03b6+1 ) assuming that \u03b6 \u2208 [ 1 2 , 1].", "startOffset": 64, "endOffset": 67}, {"referenceID": 0, "context": "Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m\u2212 2\u03b6 2\u03b6+1 ) assuming that \u03b6 \u2208 [ 1 2 , 1].", "startOffset": 155, "endOffset": 165}, {"referenceID": 1, "context": "Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m\u2212 2\u03b6 2\u03b6+1 ) assuming that \u03b6 \u2208 [ 1 2 , 1].", "startOffset": 155, "endOffset": 165}, {"referenceID": 0, "context": "Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m\u2212 2\u03b6 2\u03b6+1 ) assuming that \u03b6 \u2208 [ 1 2 , 1].", "startOffset": 155, "endOffset": 165}, {"referenceID": 18, "context": "More recently, [19] studied multiple passes SGM with a fixed ordering at each pass, also called incremental gradient method.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "Note also that [19] proved sharp rate in H-norm for \u03b6 \u2265 1/2 in the capacity independent case.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": ", in [14, 15, 16, 17].", "startOffset": 5, "endOffset": 21}, {"referenceID": 14, "context": ", in [14, 15, 16, 17].", "startOffset": 5, "endOffset": 21}, {"referenceID": 15, "context": ", in [14, 15, 16, 17].", "startOffset": 5, "endOffset": 21}, {"referenceID": 16, "context": ", in [14, 15, 16, 17].", "startOffset": 5, "endOffset": 21}, {"referenceID": 22, "context": "Besides, it has been shown in [23, 15] that for one pass mini-batch SGM with a fixed step-size \u03b7 ' b/ \u221a m and a smooth loss function, assuming the existence of at least one solution in the hypothesis space for the expected risk minimization, the convergence", "startOffset": 30, "endOffset": 38}, {"referenceID": 14, "context": "Besides, it has been shown in [23, 15] that for one pass mini-batch SGM with a fixed step-size \u03b7 ' b/ \u221a m and a smooth loss function, assuming the existence of at least one solution in the hypothesis space for the expected risk minimization, the convergence", "startOffset": 30, "endOffset": 38}, {"referenceID": 20, "context": "For any \u03c9 \u2208 H, we have [21, 19] E(\u03c9)\u2212 inf f\u2208H E(f) = \u2016S\u03c1\u03c9 \u2212 fH\u2016\u03c1.", "startOffset": 23, "endOffset": 31}, {"referenceID": 18, "context": "For any \u03c9 \u2208 H, we have [21, 19] E(\u03c9)\u2212 inf f\u2208H E(f) = \u2016S\u03c1\u03c9 \u2212 fH\u2016\u03c1.", "startOffset": 23, "endOffset": 31}, {"referenceID": 0, "context": "Here, the regression function is f\u03c1(x) = |x\u22121/2|\u22121/2, the input point xi is uniformly distributed in [0, 1], and \u03c9i is a Gaussian noise with zero mean and standard deviation 1, for each i \u2208 [m].", "startOffset": 101, "endOffset": 107}, {"referenceID": 0, "context": "Here, we replace the unknown marginal distribution \u03c1X by an empirical measure \u03c1\u0302 = 1 2000 \u22112000 i=1 \u03b4x\u0302i , where each x\u0302i is uniformly distributed in [0, 1].", "startOffset": 150, "endOffset": 156}, {"referenceID": 0, "context": "[1] Olivier Bousquet and L\u00e9on Bottou.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Yiming Ying and Massimiliano Pontil.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Pierre Tarres and Yuan Yao.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Aymeric Dieuleveut and Francis Bach.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Francesco Orabona.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Boris T Poljak.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Stephen Boyd and Almir Mutapcic.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Moritz Hardt, Benjamin Recht, and Yoram Singer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Junhong Lin, Raffaello Camoriano, and Lorenzo Rosasco.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Andrea Caponnetto and Ernesto De Vito.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Suvrit Sra, Sebastian Nowozin, and Stephen J Wright.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Andrew Ng.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Steve Smale and Ding-Xuan Zhou.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Lorenzo Rosasco and Silvia Villa.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Felipe Cucker and Ding-Xuan Zhou.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Ingo Steinwart and Andreas Christmann.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Ohad Shamir and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] IF Pinelis and AI Sakhanenko.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] Joel A Tropp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] Stanislav Minsker.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "For any \u03c9 \u2208 H, it is easy to prove the following isometry property [21] \u2016S\u03c1\u03c9\u2016\u03c1 = \u2016 \u221a T \u03c9\u2016H .", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "2 Concentration Inequality We need the following concentration result for Hilbert space valued random variable used in Caponnetto and De Vito [13] and based on the results in Pinelis and Sakhanenko [25].", "startOffset": 142, "endOffset": 146}, {"referenceID": 23, "context": "2 Concentration Inequality We need the following concentration result for Hilbert space valued random variable used in Caponnetto and De Vito [13] and based on the results in Pinelis and Sakhanenko [25].", "startOffset": 198, "endOffset": 202}, {"referenceID": 3, "context": "Towards this end, we introduce the following lemma, whose proof borrows idea from [4, 5].", "startOffset": 82, "endOffset": 88}, {"referenceID": 4, "context": "Towards this end, we introduce the following lemma, whose proof borrows idea from [4, 5].", "startOffset": 82, "endOffset": 88}, {"referenceID": 24, "context": "The result is essentially proved in [26], see also [19].", "startOffset": 36, "endOffset": 40}, {"referenceID": 18, "context": "The result is essentially proved in [26], see also [19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "The proof for the fixed step-size can be found in [19].", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "The case for \u03b6 \u2265 1/2 is similar to that in [19].", "startOffset": 43, "endOffset": 47}, {"referenceID": 25, "context": "The next lemma is borrowed from [27], derived by applying a recent Bernstein inequality from [28, 29] for a sum of random operators.", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": "The next lemma is borrowed from [27], derived by applying a recent Bernstein inequality from [28, 29] for a sum of random operators.", "startOffset": 93, "endOffset": 101}, {"referenceID": 27, "context": "The next lemma is borrowed from [27], derived by applying a recent Bernstein inequality from [28, 29] for a sum of random operators.", "startOffset": 93, "endOffset": 101}, {"referenceID": 21, "context": "The process relies on some tools from convex analysis and a decomposition related to the weighted averages and the last iterates from [22, 30].", "startOffset": 134, "endOffset": 142}, {"referenceID": 21, "context": "Using the above lemma and a decomposition related to the weighted averages and the last iterates from [22, 30], we can prove the following relationship.", "startOffset": 102, "endOffset": 110}], "year": 2016, "abstractText": "We analyze the learning properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. In particular, we consider the square loss and show that for a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on a unifying approach, encompassing both batch and stochastic gradient methods as special cases.", "creator": "LaTeX with hyperref package"}}}