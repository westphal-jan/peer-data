{"id": "1605.08889", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2016", "title": "Building an Evaluation Scale using Item Response Theory", "abstract": "We introduce Item Response Theory (IRT) from psychometrics as an alternative to majority voting to create an IRT gold standard ($GS_{IRT}$). IRT describes characteristics of individual items in $GS_{IRT}$ - their difficulty and discriminating power - and is able to account for these characteristics in its estimation of human intelligence or ability for an NLP task. In this paper, we evaluated IRT's model-fitting of a majority vote gold standard designed for Recognizing Textual Entailment (RTE), denoted as $GS_{RTE}$. By collecting human responses and fitting our IRT model, we found that up to 31% of $GS_{RTE}$ were not useful in building $GS_{IRT}$ for RTE. In addition, we found low inter-annotator agreement for some items in $GS_{RTE}$ suggesting that more work is needed for creating intelligent gold-standards.", "histories": [["v1", "Sat, 28 May 2016 13:19:15 GMT  (70kb,D)", "http://arxiv.org/abs/1605.08889v1", null], ["v2", "Fri, 23 Sep 2016 16:35:16 GMT  (86kb,D)", "http://arxiv.org/abs/1605.08889v2", "To appear in the proceedings of EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["john p lalor", "hao wu", "hong yu"], "accepted": true, "id": "1605.08889"}, "pdf": {"name": "1605.08889.pdf", "metadata": {"source": "CRF", "title": "Beyond Majority Voting: Generating Evaluation Scales using Item Response Theory", "authors": ["John Lalor", "Hao Wu", "Hong Yu"], "emails": ["lalor@cs.umass.edu", "hao.wu.5@bc.edu", "hong.yu@umassmed.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of us will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Background and Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Item Response Theory", "text": "IRT is one of the most widely used methods in scale construction and evaluation. It is typically used to analyze human responses (classified as right or wrong) to a number of questions (referred to as \"items\"). IRT uses a statistical model that links these responses to the ability of the person and the characteristics of the items (Baker and Kim, 2004). This statistical model typically assumes: (a) humans differ from others on an unobserved feature dimension of interest (referred to as \"ability\" or \"factor\"). The probability that an item is answered correctly is a function of the person's ability. This function is referred to as an item characteristic curve (ICC) and includes item characteristic characteristics as parameters. (c) Responses to different items are independent of each other for a particular skill level of the person (\"local assumption of independence\"); (d) responses by different individuals are independent of each other."}, {"heading": "2.1.1 IRT Terminology", "text": "This section outlines the common IRT terminology in the sense of RTE. An item refers to two sentences that people or NLP systems label (discord, contradiction, or neutrality).A set of responses to all items (each of which is classified as right or wrong) is a response pattern. A rating scale is a set of items that can be administered to an NLP system as a test set and assign a performance score (or theta score) to the system."}, {"heading": "2.2 Gold Standard Generation", "text": "There are a variety of methods for generating gold standard datasets, including web crawling (Guo et al., 2013), automatic and semi-automatic generation (An et al., 2003) and expert datasets (Roller and Stevenson, 2015) and annotations for laymen (Bowman et al., 2015; Wiebe et al., 1999). In any case, some validation is required to ensure that the data collected is suitable and usable for the task at hand. Automatically generated data can be refined by visual inspection or post-processing. Human-annotated data typically includes more than one annotator, so that comparative metrics such as Cohens or Fleiss'\u043c can be used to determine whether they match."}, {"heading": "2.3 Recognizing Textual Entailment", "text": "RTE was introduced to standardize the challenge of taking semantic variations into account when building models for a range of NLP applications (Dagan et al., 2006). RTE defines a directional relationship between a sentence pair, the text (T), and the hypothesis (H). T includes H if a person who has read T would conclude that H is true. If a person concludes that H is wrong, then H contradicts T. If the two sentences have nothing to do with each other, then the pair is considered neutral. Table 1 shows examples of T-H pairs and their respective classifications. Recent state-of-the-art systems for RTE require a high level of feature engineering and specialization to achieve high performance (Beltagy et al., 2015; Lai and Hockenmaier, 2014; Jimenez et al., 2014)."}, {"heading": "2.4 Stanford SNLI Dataset", "text": "Many gold standard datasets are publicly available, including a number of datasets to test RTE models (Marelli et al., 2014; Young et al., 2014; Levy et al., 2014). A recently introduced dataset for RTE is the dataset Stanford Natural Language Inference (SNLI) (Bowman et al., 2015). SNLI examples were created only using human-generated datasets to mitigate the problem of bad data that were used to build models for RTE. Users of Amazon Mechanical Turk (AMT) were shown a picture caption that was taken from the Patchr30k corpus (Young et al., 2014) and notified that the picture caption was associated with a photo. The user was not shown the appropriate photo. Subsequently, they were asked to write three alternate picture captions that could describe the photo: (i) one that is definitely true, (ii) one that is true, the picture caption was associated with a photograph, (ii) one that was possibly true, and the photo was not shown. The user was shown the appropriate photograph for RTE. Subsequently, they were asked to write three alternate picture captions that could describe the photograph: (i) one that is definitely true, (ii) one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one phrase, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one sentence, one, one sentence, one, one sentence, one sentence, one, one sentence, one sentence, one, one sentence, one sentence, one, one, one sentence, one, one, one sentence, one, one sentence, one, one, one sentence, one, one"}, {"heading": "3 Methods", "text": "We collected and evaluated a random selection from the SNLI dataset (GSRTE) to build IRT models and determine the effectiveness of the quality control method by majority vote. First, we randomly selected a subset of GSRTE and then used the sample in an AMT Human Intelligence Task (HIT) to collect more labels for each text-hypotheses pair. Then, we applied IRT to evaluate the quality of the examples and used the final IRT models to create assessment sets (GSIRT) to measure the ability for RTE."}, {"heading": "3.1 Item Selection", "text": "We identified a subset of GSRTE that should be used as a test set according to the following steps: (1) Identify all the \"quality control elements\" of GSRTE, obtaining four additional comments to confirm that the label provided by the first AMT user was appropriate, (2) Divide this section of data according to the number of users who agreed to the final gold standard label, (3) randomly select 30 sentence pairs, 30 neutral sentence pairs and 30 contradiction sentence pairs from each of the 4-annotator gold standards (4GS) and 5-annotator gold standards (5GS) to obtain two sets of 90 sentence pairs. 90 sentence pairs for 4GS and 5GS were sampled so that the annotations (which provide 90 labels) could be executed in a relatively short period of time during which the users remained engaged and did not lose focus. We selected elements from the set of RT4GS and 5GS were considered to be high-quality based on both sets of 5GS and 5GS results."}, {"heading": "3.2 AMT Annotation", "text": "In order to maintain consistency in the creation of labels, we designed our AMT HIT to be consistent with the process of validating SNLI quality control points (Bowman et al., 2015), which is based on the process of creating labels for the SICK dataset, another gold-standard RTE dataset (Marelli et al., 2014). Each AMT HIT user was required to select for each of the 90 premise hypotheses set pairs either the 5GS set or the 4GS set the appropriate label, in order to avoid the user simply replying with the same label for each element. Users were shown a set pair of sentences and asked to select one label for each pair. For each 90-sentence pair set (5GS and 4GS), the best set-sentence-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-label-set-set-set-set-set-set-set-set-set-set-set-set-set-label-set-set-set-set-set-set-set-label-set-set-set-set-set-label-set-set-set-set-label-set-set-set-label-set-set-set-set-set-set-set-label-4GS, and-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-set-label-4GS, and-set-set-set-set-set-set-set-set-"}, {"heading": "3.3 Statistical Analysis", "text": "We used a standard R package (Chalmers et al., 2015) for our analyses, and the data collected was analyzed separately for 4GS and 5GS items. For both items, the number of latent features was identified using a graph of eigenvalues of items of 90 by 90 tetrachoric correlation matrix and a further comparison between IRT models of different factors. Subsequently, a target rotation (Browne, 2001) was used to identify a meaningful load pattern associated with the factors and items, and each factor could then be interpreted as a user's ability to detect the correct relationship between the pairs of sentences associated with that factor. We created a one-dimensional IRT model for each group of items associated with a single factor. We matched and compared one- and two-dimensional 3PL models to confirm the underlying structure of these items, assuming that the answers were well available in the people."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Response Statistics", "text": "Table 2 lists the most important statistics we have obtained as a result of our AMT HITs. Most of the sentence pairs collected were identified by majority decision. Due to the large number of individuals who submit their labels during HIT, we also wanted to see if such a label can be provided with a two-thirds majority to see if that label matches the original label."}, {"heading": "4.2 IRT Evaluation", "text": "We have used the methods described in Section 3.3 in both cases to scale performance according to the RTE pattern."}, {"heading": "5 Discussion", "text": "It is easy to implement and evaluate, and allows disagreements between commentators as long as an election reaches a certain threshold, typically 50% approval. However, many factors can contribute to majority decision making. However, as NLP systems have become more complex, we need more complex methods to compare their performance. Expanding the number of commentators, while it can be expensive and time-consuming, can indicate which positions are of higher quality and which positions may need to be reconsidered for inclusion. As NLP systems have become more sophisticated, we need more complex methods to compare their performance. Expanding the number of commentators, while it can be expensive and time-consuming, can indicate which positions are of higher quality and which positions may need to be reconsidered for inclusion. Elements that receive a majority designation with a small number of commentators can change this label if the number of commentators is increased. This inequality could have an impact on NLP systems that are formed with the data."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we analyze a current gold standard dataset for the RTE task using the psychometric model of Item Response Theory (IRT). We show that by increasing the number of people used to evaluate the quality of the dataset items, the general agreement for the gold standard dataset decreases. There are a number of set pairs in the dataset that have been identified as high quality in the sense that they may be suitable for IRT models that represent the individual's ability to correctly identify a particular RTE label. Future work will adapt this analysis to create an evaluation mechanism for the RTE task and to evaluate a number of gold standard RTE methods that use it. The expectation is that methods that successfully perform a standard accuracy measurement on the basis of what types of items they work well can be layered so that instead of them can be gradually tested on the basis of a new measurement of performance."}], "references": [{"title": "Automatic Acquisition of Named Entity Tagged Corpus from World Wide Web", "author": ["An et al.2003] Joohui An", "Seungwoo Lee", "Gary Geunbae Lee"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume", "citeRegEx": "An et al\\.,? \\Q2003\\E", "shortCiteRegEx": "An et al\\.", "year": 2003}, {"title": "Item Response Theory: Parameter Estimation Techniques, Second Edition", "author": ["Baker", "Kim2004] Frank B. Baker", "Seock-Ho Kim"], "venue": null, "citeRegEx": "Baker et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Baker et al\\.", "year": 2004}, {"title": "Representing Meaning with a Combination of Logical Form and Vectors", "author": ["Stephen Roller", "Pengxiang Cheng", "Katrin Erk", "Raymond J. Mooney"], "venue": null, "citeRegEx": "Beltagy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Beltagy et al\\.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference. arXiv:1508.05326 [cs", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "An overview of analytic rotation in exploratory factor analysis", "author": ["Michael W Browne"], "venue": "Multivariate Behavioral Research,", "citeRegEx": "Browne.,? \\Q2001\\E", "shortCiteRegEx": "Browne.", "year": 2001}, {"title": "mirt: Multidimensional Item Response Theory, November", "author": ["Joshua Pritikin", "Alexander Robitzsch", "Mateusz Zoltak"], "venue": null, "citeRegEx": "Chalmers et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chalmers et al\\.", "year": 2015}, {"title": "Local Dependence Indexes for Item Pairs Using Item Response Theory", "author": ["Chen", "Thissen1997] Wen-Hung Chen", "David Thissen"], "venue": "Journal of Educational and Behavioral Statistics,", "citeRegEx": "Chen et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1997}, {"title": "The PASCAL Recognising Textual Entailment Challenge", "author": ["Dagan et al.2006] Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "venue": "In Machine Learning Challenges", "citeRegEx": "Dagan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Building watson: An overview of the deepqa project", "author": ["Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A Kalyanpur", "Adam Lally", "J William Murdock", "Eric Nyberg", "John Prager"], "venue": null, "citeRegEx": "Ferrucci et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2010}, {"title": "Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media", "author": ["Guo et al.2013] Weiwei Guo", "Hao Li", "Heng Ji", "Mona T. Diab"], "venue": null, "citeRegEx": "Guo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2013}, {"title": "Illinois-LH: A Denotational and Distributional Approach to Semantics", "author": ["Lai", "Hockenmaier2014] Alice Lai", "Julia Hockenmaier"], "venue": "SemEval", "citeRegEx": "Lai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2014}, {"title": "Focused entailment graphs for open IE propositions", "author": ["Levy et al.2014] Omar Levy", "Ido Dagan", "Jacob Goldberger"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Power analysis and determination of sample size for covariance structure modeling", "author": ["Michael W Browne", "Hazuki M Sugawara"], "venue": "Psychological methods,", "citeRegEx": "MacCallum et al\\.,? \\Q1996\\E", "shortCiteRegEx": "MacCallum et al\\.", "year": 1996}, {"title": "A SICK cure for the evaluation of compositional distributional semantic models", "author": ["Marelli et al.2014] M. Marelli", "S. Menini", "M. Baroni", "L. Bentivogli", "R. Bernardi", "R. Zamparelli", "Fondazione Bruno Kessler"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Likelihood-Based Item-Fit Indices for Dichotomous Item Response Theory Models", "author": ["Orlando", "Thissen2000] Maria Orlando", "David Thissen"], "venue": "Applied Psychological Measurement,", "citeRegEx": "Orlando et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Orlando et al\\.", "year": 2000}, {"title": "Held-out versus Gold Standard: Comparison of Evaluation Strategies for Distantly Supervised Relation Extraction from Medline abstracts", "author": ["Roller", "Stevenson2015] Roland Roller", "Mark Stevenson"], "venue": "In Sixth International Workshop on Health", "citeRegEx": "Roller et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Roller et al\\.", "year": 2015}, {"title": "Mastering the game of go", "author": ["Silver et al.2016] David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Development and use of a gold-standard data set for subjectivity classifications", "author": ["Rebecca F. Bruce", "Thomas P. O\u2019Hara"], "venue": "In Proceedings of the 37th annual meeting of the Association", "citeRegEx": "Wiebe et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 1999}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Young et al.2014] Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "Advances in artificial intelligence have made it possible to compare computer performance directly with human intelligence (Campbell et al., 2002; Ferrucci et al., 2010; Silver et al., 2016).", "startOffset": 123, "endOffset": 190}, {"referenceID": 16, "context": "Advances in artificial intelligence have made it possible to compare computer performance directly with human intelligence (Campbell et al., 2002; Ferrucci et al., 2010; Silver et al., 2016).", "startOffset": 123, "endOffset": 190}, {"referenceID": 9, "context": "There are a variety of methods to generate goldstandard datasets, among them web crawling (Guo et al., 2013), automatic and semi-automatic generation (An et al.", "startOffset": 90, "endOffset": 108}, {"referenceID": 0, "context": ", 2013), automatic and semi-automatic generation (An et al., 2003), and expert (Roller and Stevenson, 2015) and non-expert human annotation (Bowman et al.", "startOffset": 49, "endOffset": 66}, {"referenceID": 3, "context": ", 2003), and expert (Roller and Stevenson, 2015) and non-expert human annotation (Bowman et al., 2015; Wiebe et al., 1999).", "startOffset": 81, "endOffset": 122}, {"referenceID": 17, "context": ", 2003), and expert (Roller and Stevenson, 2015) and non-expert human annotation (Bowman et al., 2015; Wiebe et al., 1999).", "startOffset": 81, "endOffset": 122}, {"referenceID": 7, "context": "RTE was introduced to standardize the challenge of accounting for semantic variation when building models for a number of NLP applications (Dagan et al., 2006).", "startOffset": 139, "endOffset": 159}, {"referenceID": 13, "context": "Many gold-standard datasets are publicly available, including a number of datasets to test RTE models (Marelli et al., 2014; Young et al., 2014; Levy et al., 2014).", "startOffset": 102, "endOffset": 163}, {"referenceID": 18, "context": "Many gold-standard datasets are publicly available, including a number of datasets to test RTE models (Marelli et al., 2014; Young et al., 2014; Levy et al., 2014).", "startOffset": 102, "endOffset": 163}, {"referenceID": 11, "context": "Many gold-standard datasets are publicly available, including a number of datasets to test RTE models (Marelli et al., 2014; Young et al., 2014; Levy et al., 2014).", "startOffset": 102, "endOffset": 163}, {"referenceID": 3, "context": "A recently introduced dataset for RTE is the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).", "startOffset": 96, "endOffset": 117}, {"referenceID": 18, "context": "Amazon Mechanical Turk (AMT) users were shown a caption that was taken from the Flickr30k corpus (Young et al., 2014) and told that the caption was associated with a photo.", "startOffset": 97, "endOffset": 117}, {"referenceID": 3, "context": "The authors found that most of the items received a goldstandard label, with only 2% of pairs not receiving a gold-standard label (Bowman et al., 2015).", "startOffset": 130, "endOffset": 151}, {"referenceID": 3, "context": "original dataset (Bowman et al., 2015) and found that accuracy scores were similar compared to performance on the SNLI test set.", "startOffset": 17, "endOffset": 38}, {"referenceID": 3, "context": "to validate the SNLI quality control items (Bowman et al., 2015), which was based on the process used to generate labels for the SICK dataset, another gold standard RTE dataset (Marelli et al.", "startOffset": 43, "endOffset": 64}, {"referenceID": 13, "context": ", 2015), which was based on the process used to generate labels for the SICK dataset, another gold standard RTE dataset (Marelli et al., 2014).", "startOffset": 120, "endOffset": 142}, {"referenceID": 12, "context": "While there is no set standard for sample sizes in 2PL and 3PL IRT models, this sample size satisfies the standards based on a noncentral \u03c72 distribution (MacCallum et al., 1996), the distribution of the test statistic when comparing two multidimensional IRT models.", "startOffset": 154, "endOffset": 178}, {"referenceID": 5, "context": "We used a standard R package (Chalmers et al., 2015) for our analyses.", "startOffset": 29, "endOffset": 52}, {"referenceID": 4, "context": "A target rotation (Browne, 2001) was then", "startOffset": 18, "endOffset": 32}, {"referenceID": 3, "context": "As a demonstration, we used the LSTM model presented in (Bowman et al., 2015) to provide responses to items on our IRT evaluation scales.", "startOffset": 56, "endOffset": 77}], "year": 2016, "abstractText": "We introduce Item Response Theory (IRT) from psychometrics as an alternative to majority voting to create an IRT gold standard (GSIRT ). IRT describes characteristics of individual items in GSIRT their difficulty and discriminating power and is able to account for these characteristics in its estimation of human intelligence or ability for an NLP task. In this paper, we evaluated IRT\u2019s model-fitting of a majority vote gold standard designed for Recognizing Textual Entailment (RTE), denoted as GSRTE . By collecting human responses and fitting our IRT model, we found that up to 31% of GSRTE were not useful in building GSIRT for RTE. In addition, we found low inter-annotator agreement for some items in GSRTE suggesting that more work is needed for creating intelligent gold-standards.", "creator": "LaTeX with hyperref package"}}}