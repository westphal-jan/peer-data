{"id": "1605.08900", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2016", "title": "Aspect Level Sentiment Classification with Deep Memory Network", "abstract": "We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.", "histories": [["v1", "Sat, 28 May 2016 14:47:49 GMT  (601kb,D)", "http://arxiv.org/abs/1605.08900v1", null], ["v2", "Sat, 24 Sep 2016 06:04:15 GMT  (311kb,D)", "http://arxiv.org/abs/1605.08900v2", "published in EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["duyu tang", "bing qin", "ting liu"], "accepted": true, "id": "1605.08900"}, "pdf": {"name": "1605.08900.pdf", "metadata": {"source": "CRF", "title": "Aspect Level Sentiment Classification with Deep Memory Network", "authors": ["Duyu Tang", "Bing Qin", "Ting Liu"], "emails": ["tliu}@ir.hit.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point where it will be able to retaliate, \"he says.\" It's the way it is, \"he says.\" But it's the way it is, \"he says.\" It's the way it is, \"he says."}, {"heading": "2 Background: Memory Network", "text": "Our approach is inspired by the recent success of the storage network in answering the question (Weston et al., 2014; Sukhbaatar et al., 2015). We describe the background of the storage network in this part. Storage network is a general machine learning system introduced by Weston et al. (2014). Its central idea is to conclude with a long-term storage component that could be read, written and learned together, with the aim of using it for prediction. Formally, a storage network consists of a memory m and four components I, G, O and R, where m is an array of objects such as an array of vectors. Among these four components, I convert inputs into internal feature representation, G updates old memories with new inputs, O generates an output representation that gives a new input, and the current memory state, R gives an answer based on the output representation of a word."}, {"heading": "3 Deep Memory Network for Aspect Level Sentiment Classification", "text": "In this section, we describe the deep memory network approach for the aspect level sentiment classification. First, we give the task definition, then we describe an overview of the approach, before introducing the content and location-based attention models in each calculation layer. Finally, we describe the use of this approach for aspect level sentiment classification."}, {"heading": "3.1 Task Definition and Notation", "text": "In view of a sentence s = {w1, w2,..., wi,... wn} that consists of n words and an aspect wi 1 that occurs in sentence s, the sentiment classification aims at determining the sentiment polarity of 1. In practice, one aspect could be a multi-word expression like \"battery life.\" For simplicity's sake, we still consider aspect in this definition to be a single word. Sentence s versus aspect wi. For example, the sentiment polarity of the sentence \"great food, but the service was terrible!\" versus aspect \"food\" is positive, while the polarity versus aspect \"service\" is negative. When treating a text corpus, we place each word in a low-dimensional, continuous and real valuable vector, also known as word embedding (Mikolov et al., 2013; Pennington et al., 2014). All word vectors are stacked in one word that is the matrix, the word L | V, where the word is embedded."}, {"heading": "3.2 An Overview of the Approach", "text": "Considering a sentence s = {w1, w2,..., wi,... wn} and the aspect word wi, we map each word into its embedding vector. These word vectors are divided into two parts, aspect representation and context representation. If aspect is a single word such as \"food\" or \"service,\" aspect representation is the embedding of the aspect word. In the case where aspect is a multiword expression such as \"battery life,\" aspect representation is an average of its constituent word vectors (Sun et al., 2015). To simplify the interpretation, we consider the aspect as a single word wi. Context word vectors {e1, e2... ei + 1... en} are stacked and considered as external memory m: m that Rd \u00d7 (n \u2212 1), where n is the sentence length."}, {"heading": "3.3 Content Attention", "text": "In this part, we describe our attention model. The basic idea of the attention mechanism is that it assigns a weight / meaning to each lower position when calculating a representation at the upper level (Bahdanau et al., 2015). In this work, we use an attention model to calculate the representation of a sentence in relation to an aspect. Intuition is that context words do not contribute equally to the semantic meaning of a sentence. Furthermore, the meaning of a word should be different when we focus on another aspect. Let's take again the example of \"great food, but the service was terrible!\" The context word \"great\" is more important than \"dreamy\" for aspect \"food.\" On the contrary, \"terrible\" is more important than \"for aspect\" service. \"Take an external memory m\" Rd \"k and an aspect vaspectoral aspect, so that Rd \u00b7 1 is assigned as an input, the attention model memory vector function 1."}, {"heading": "3.4 Location Attention", "text": "We have described our neural attention frame and a content-based model in the previous subsection. However, the above-mentioned model ignores the position information between context word and aspect. Such position information is helpful for an attention model, because intuitively, a context word closer to the aspect should be more important than a more distant one. \u2022 In this thesis, we define the position of a context word as its absolute distance from the aspect in the original sentence sequence3. On this basis, we study four strategies for encoding the position information in the attention model. \u2022 Model 1. Following Sukhbaatar et al. (2015), we calculate the memory vector mi with mi = ei vi, where element-wise multiplication and vi-Rd \u00d7 1 is a position vector for word wi. Each element in vi is calculated as follows: vki = (1 \u2212 li / n) \u2212 (k / d) \u2212 vametervameters (1 \u2212 n) could also be calculated from its location along a sat."}, {"heading": "3.5 The Need for Multiple Hops", "text": "It is widely accepted that computational models consisting of multiple layers of processing have the ability to learn representations of data with multiple levels of abstraction (LeCun et al., 2015). In this work, the attention layer in one layer is essentially a weighted average compositional function that is not strong enough to cope with complex computing power such as denial, intensification and the opposite in language. Multiple layers of computation allow the deep memory network to learn representations of text with multiple levels of abstraction. Each layer / hop retrieves important context words and transforms the representation at the previous level into a representation at a higher, slightly more abstract level. By composing enough of such transformations, very complex functions of sentence representation can be learned in one aspect."}, {"heading": "3.6 Aspect Level Sentiment Classification", "text": "We consider the output vector in the last hop to be the feature and feed it to a Softmax layer for aspect-level sentiment classification. The model is trained in a controlled way by minimizing the cross-entropy error of the sentiment classification, the loss function of which is given below, where T means all training instances, C is the collection of sentiment categories, (s, a) means a sentence-aspect pairing. Loss = (s, a) is a category produced by our system. P gc (s, a) is 1 or 0, indicating whether the correct answer is c. We use backpropagation to calculate the gradients of all parameters, and update it with stochastic gradient decrease. We clamp the word embedding with 300-dimensional glove vectors (Pennington 4. et 2014, which consists of a uniform set of data of 0.001 and 0.001)."}, {"heading": "4 Experiment", "text": "In this section we describe experimental settings and report on empirical results."}, {"heading": "4.1 Experimental Setting", "text": "We are conducting experiments with two data sets from SemEval 2014 (Pontiki et al., 2014), one from the laptop area and another from the restaurant area. Statistics of the data sets are in Table 1. The yardstick for evaluation is classification accuracy."}, {"heading": "4.2 Comparison to Other Methods", "text": "We compare with the following basic methods on both datasets. (1) Majority is a basic basic method that assigns the majority feeling label in the training of each instance in the test set. (2) Feature-based SVM performs state-of-the-art on aspect level sentiment classification. We compare with a top system that uses ngram features, parse features and lexicon features (Kiritchenko et al., 2014).4Available at: http: / / nlp.stanford.edu / projects / glove /. (3) We compare with three LSTM models (Tang et al., 2015a). In LSTM, an LSTM-based recurrent model is applied from the beginning to the end of a set, and the last hidden vector is used as record representation. TDLSTM extends LSTM by considering the aspect, and uses two LSTM networks, one forward and one backward to the model, to the DLM aspect."}, {"heading": "4.3 Runtime Analysis", "text": "We are investigating the runtime of recursive neural models and the proposed deep memory network approach with different hops. We are implementing all of these approaches based on the same neural network infrastructure, using the same 300-dimensional glove word vectors, and executing them on the same CPU server.The training time of each iteration on the restau-rant dataset is shown in Table 3. We can find that LSTM-based recursive models are actually computationally expensive, caused by the complex operations in each LSTM unit along the word sequence. Instead, the storage network approach is simpler and obviously faster as it does not require recurring computers with sequence length. Our approach of nine hops is nearly 15 times faster than the basic LSTM model."}, {"heading": "4.4 Effects of Location Attention", "text": "As described in Section 3.4, we examine four strategies for integrating location information into the attention model. We integrate each of these strategies separately into the basic content-based attention model. It is helpful to emphasize again that the difference between four location-based attention models lies in the use of location vectors for context words. In Model 1 and Model 2, the values of location vectors are determined and calculated heuristically. In Model 3 and Model 4, location vectors are also considered as parameters and learned along with other parameters in the deep memory network.1 2 3 4 5 6 7 8 9 0,720,730.740,750,760,780,790,80.81Number of hopsA ccur acyContent + Location 1 + Location 2 + Location 3 + Location 4Figure 2: Classification accuracy of different attention models on the restaurant dataset.Figure 2 has the classification of each on the model attention restaurant dataset."}, {"heading": "4.5 Visualize Attention Models", "text": "The results of the contextual model and the location-based model (model 2) are presented in Table 4 and Table 5 respectively. Table 4 (a) shows that in the first jump the contextual words \"big,\" \"but\" and \"terrible\" contribute equally to the aspect of \"service.\" Whereas after the second jump the weight of \"terrible\" increases and finally the model predicts the polarity of \"service\" as negatively correct, this case shows the effects of several hops. However, in Table 4 (b) the content-based model also puts more emphasis on \"terrible\" if the goal we focus on is \"food.\" Consequently, the model predicts the polarity of \"food\" as negatively wrong. This phenomenon could be caused by the neglect of location information. Table 5 (b) shows that the weight of \"big\" is increased if the location of the text is classified as \"negative.\""}, {"heading": "4.6 Error Analysis", "text": "We perform an error analysis of our site-enhancing model (Model 2) on the restaurant data set and find that most errors could be summarized as follows: The first factor is the non-compositional sentiment expression. This model considers a single contextual word as the basic unit of calculation and cannot deal with this situation. An example is \"Dessert was also for dying!,\" underlining the aspect. The emotion expression is \"die,\" the meaning of which could not be composed of its constituent parts \"die\" and \"for this.\" The second factor is the complex aspect expression consisting of many words, such as \"ask for the round table next to the large window.\" This model represents an aspect expression, weighing its constituent word vectors, which could not cope well with this situation. The third factor is the sentimental relationship between contextual words such as negation, comparison and condition. An example is \"but the dinner here is never disappointing, even if the average price is caused by some weakness in each of us.\""}, {"heading": "5 Related Work", "text": "This work is linked to three areas of research on the processing of natural language, each of which we describe briefly."}, {"heading": "5.1 Aspect Level Sentiment Classification", "text": "Most existing work uses machine learning and uses external resources such as parsers and sentimental lexicons (Kiritchenko et al., 2014; Wagner et al., 2014). In recent years, neural networking approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) have gained increasing attention for their ability to learn strong text representation from data. However, in recent years, these neural models (e.g. LSTM) have been computationally expensive and have not been able to explicitly demonstrate the importance of context evidence in terms of an aspect that we are developing an aspect and instead developing an aspect (Choity et) in terms of an aspect."}, {"heading": "5.2 Compositionality in Vector Space", "text": "In the NLP community, composition means that the meaning of a composite expression (e.g. a phrase / sentence / document) derives from the meaning of its components (Frege, 1892). Mitchell and Lapata (2010) use a variety of addition and multiplication to calculate phrase vectors. Yessenalina and Cardie (2011) use matrix multiplication as a compositional function to calculate vectors for longer phrases. To calculate sentence representation, researchers develop sequence-based recurring neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhang et al., 2015; Li et al."}, {"heading": "5.3 Attention and Memory Networks", "text": "Recently, there has been a resurgence of computer models with attention mechanisms and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as continuous representation, and operations in memory (e.g. reading and writing) are typically implemented using neural networks. Attention mechanisms can be considered a compositional function, where lower-level representation is considered memory, and the function is to decide \"where to look\" by assigning a weight / meaning to each lower position in the computation of an upper-level representation. Such attention-based approaches have shown promise in a variety of NLP tasks (Luong et al., 2015; Kumar et al., 2015; Rush et al., 2015)."}, {"heading": "6 Conclusion", "text": "We are developing deep memory networks that capture the importance of context words for aspect-level classification. Compared to recurrent neural models like LSTM, this approach is simpler and faster. Empirical results from two sets of data show that the proposed approach is comparable to modern function-based SVM systems and significantly better than LSTM architectures. We are implementing different attention strategies and show that the use of content and location information could learn better context weighting and text representation. We also show that the use of multiple layers of computing in the storage network could lead to improved performance."}, {"heading": "Acknowledgments", "text": "In particular, we would like to thank Xiaodan Zhu for running their system on our setup. We thank Yaming Sun very much for the tremendously helpful discussions."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Better document-level sentiment analysis from rst discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bhatia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhatia et al\\.", "year": 2015}, {"title": "Hierarchical sequential learning for extracting opinions and their attributes", "author": ["Choi", "Cardie2010] Yejin Choi", "Claire Cardie"], "venue": "In Proceedings of the ACL 2010 Conference Short Papers,", "citeRegEx": "Choi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2010}, {"title": "Adaptive recursive neural network for target-dependent twitter sentiment classification", "author": ["Dong et al.2014] Li Dong", "Furu Wei", "Chuanqi Tan", "Duyu Tang", "Ming Zhou", "Ke Xu"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Opinion mining with deep recurrent neural networks", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "Claire Cardie"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Nrccanada-2014: Detecting aspects and sentiment in customer reviews", "author": ["Xiaodan Zhu", "Colin Cherry", "Saif Mohammad"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Kiritchenko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiritchenko et al\\.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar et al.2015] Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Aspect specific sentiment analysis using hierarchical deep learning", "author": ["Richard Socher", "Chris Manning"], "venue": "In NIPS Workshop on Deep Learning and Representation Learning", "citeRegEx": "Lakkaraju et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lakkaraju et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc V. Le", "Tomas Mikolov"], "venue": "In Proceedings of The 31nd International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "2015a. A hierarchical neural autoencoder for paragraphs and documents", "author": ["Li et al.2015a] Jiwei Li", "Thang Luong", "Dan Jurafsky"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "2015b. When are tree structures necessary for deep learning of representations", "author": ["Li et al.2015b] Jiwei Li", "Thang Luong", "Dan Jurafsky", "Eduard Hovy"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Fine-grained opinion mining with recurrent neural networks and word embeddings", "author": ["Liu et al.2015] Pengfei Liu", "Shafiq Joty", "Helen Meng"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Sentiment analysis and opinion mining", "author": ["Bing Liu"], "venue": "Synthesis Lectures on Human Language Technologies,", "citeRegEx": "Liu.,? \\Q2012\\E", "shortCiteRegEx": "Liu.", "year": 2012}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Lapata2010] Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis", "author": ["Nguyen", "Shirai2015] Thien Hai Nguyen", "Kiyoaki Shirai"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Opinion mining and sentiment analysis", "author": ["Pang", "Lee2008] Bo Pang", "Lillian Lee"], "venue": "Foundations and trends in information retrieval,", "citeRegEx": "Pang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2008}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Semeval-2014 task 4: Aspect based sentiment analysis", "author": ["Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evalua-", "citeRegEx": "Pontiki et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pontiki et al\\.", "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Endto-end memory networks", "author": ["Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Modeling mention, context and entity with neural networks for entity disambiguation", "author": ["Sun et al.2015] Yaming Sun", "Lei Lin", "Duyu Tang", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang"], "venue": "Proceedings of the TwentyFourth International Joint Conference on Artificial In-", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "2015a. Target-Dependent Sentiment Classification with Long Short Term Memory. ArXiv preprint arXiv:1512.01100", "author": ["Tang et al.2015a] Duyu Tang", "Bing Qin", "Xiaocheng Feng", "Ting Liu"], "venue": null, "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Tang et al.2015b] Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Target-dependent twitter sentiment classification with rich automatic features", "author": ["Vo", "Zhang2015] Duy-Tin Vo", "Yue Zhang"], "venue": "In Proceedings of the TwentyFourth International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Vo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vo et al\\.", "year": 2015}, {"title": "Dcu: Aspectbased polarity classification for semeval task", "author": ["Piyush Arora", "Santiago Cortes", "Utsab Barman", "Dasha Bogdanova", "Jennifer Foster", "Lamia Tounsi"], "venue": "Proceedings of the 8th International Workshop on Se-", "citeRegEx": "Wagner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wagner et al\\.", "year": 2014}, {"title": "Hierarchical attention networks for document classification", "author": ["Yang et al.2016] Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Compositional matrix-space models for sentiment analysis", "author": ["Yessenalina", "Cardie2011] Ainur Yessenalina", "Claire Cardie"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Yessenalina et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yessenalina et al\\.", "year": 2011}, {"title": "Multichannel variable-size convolution for sentence classification", "author": ["Yin", "Sch\u00fctze2015] Wenpeng Yin", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the Nineteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Long short-term memory over tree structures", "author": ["Zhu et al.2015] Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Aspect level sentiment classification is a fundamental task in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Pontiki et al., 2014).", "startOffset": 95, "endOffset": 148}, {"referenceID": 22, "context": "Aspect level sentiment classification is a fundamental task in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Pontiki et al., 2014).", "startOffset": 95, "endOffset": 148}, {"referenceID": 3, "context": ", 2014) and neural network models (Dong et al., 2014; Lakkaraju et al., 2014; Vo and Zhang, 2015; Nguyen and Shirai, 2015; Tang et al., 2015a).", "startOffset": 34, "endOffset": 142}, {"referenceID": 10, "context": ", 2014) and neural network models (Dong et al., 2014; Lakkaraju et al., 2014; Vo and Zhang, 2015; Nguyen and Shirai, 2015; Tang et al., 2015a).", "startOffset": 34, "endOffset": 142}, {"referenceID": 0, "context": "which is inspired by the recent success of computational models with attention mechanism and explicit memory (Graves et al., 2014; Bahdanau et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 109, "endOffset": 178}, {"referenceID": 25, "context": "which is inspired by the recent success of computational models with attention mechanism and explicit memory (Graves et al., 2014; Bahdanau et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 109, "endOffset": 178}, {"referenceID": 22, "context": "restaurant datasets from SemEval 2014 (Pontiki et al., 2014).", "startOffset": 38, "endOffset": 60}, {"referenceID": 8, "context": "Experimental results show that our approach performs comparable to a top system using feature-based SVM (Kiritchenko et al., 2014).", "startOffset": 104, "endOffset": 130}, {"referenceID": 25, "context": "memory network in question answering (Weston et al., 2014; Sukhbaatar et al., 2015).", "startOffset": 37, "endOffset": 83}, {"referenceID": 25, "context": "Sukhbaatar et al. (2015) demonstrate that multiple hops could uncover more abstractive evidences than single hop, and could yield improved results on question answering and language modeling.", "startOffset": 0, "endOffset": 25}, {"referenceID": 17, "context": "When dealing with a text corpus, we map each word into a low dimensional, continuous and real-valued vector, also known as word embedding (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 138, "endOffset": 185}, {"referenceID": 21, "context": "When dealing with a text corpus, we map each word into a low dimensional, continuous and real-valued vector, also known as word embedding (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 138, "endOffset": 185}, {"referenceID": 26, "context": "(Sun et al., 2015).", "startOffset": 0, "endOffset": 18}, {"referenceID": 25, "context": "An illustration of our approach is given in Figure 1, which is inspired by the use of memory network in question answering (Sukhbaatar et al., 2015).", "startOffset": 123, "endOffset": 148}, {"referenceID": 0, "context": "a weight/importance to each lower position when computing an upper level representation (Bahdanau et al., 2015).", "startOffset": 88, "endOffset": 111}, {"referenceID": 25, "context": "Following Sukhbaatar et al. (2015), we calculate the memory vector mi with mi = ei vi, where means element-wise multiplication and vi \u2208 Rd\u00d71 is a location vector for word wi.", "startOffset": 10, "endOffset": 35}, {"referenceID": 21, "context": "We clamp the word embeddings with 300-dimensional Glove vectors (Pennington et al., 2014), which is trained from web data and the vocabulary size is 1.", "startOffset": 64, "endOffset": 89}, {"referenceID": 22, "context": "We conduct experiments on two datasets from SemEval 2014 (Pontiki et al., 2014), one from laptop domain and another from restaurant domain.", "startOffset": 57, "endOffset": 79}, {"referenceID": 8, "context": "We compare with a top system using ngram features, parse features and lexicon features (Kiritchenko et al., 2014).", "startOffset": 87, "endOffset": 113}, {"referenceID": 0, "context": "(Bahdanau et al., 2015) over the hidden vectors.", "startOffset": 0, "endOffset": 23}, {"referenceID": 22, "context": "Aspect level sentiment classification is a finegrained classification task in sentiment analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect (Pontiki et al., 2014).", "startOffset": 189, "endOffset": 211}, {"referenceID": 8, "context": "resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014).", "startOffset": 45, "endOffset": 92}, {"referenceID": 32, "context": "resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014).", "startOffset": 45, "endOffset": 92}, {"referenceID": 3, "context": "In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data.", "startOffset": 43, "endOffset": 131}, {"referenceID": 10, "context": "In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data.", "startOffset": 43, "endOffset": 131}, {"referenceID": 14, "context": "B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015).", "startOffset": 58, "endOffset": 123}, {"referenceID": 4, "context": "coder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al.", "startOffset": 6, "endOffset": 27}, {"referenceID": 6, "context": ", 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch\u00fctze, 2015), sequence based recurrent neural models (Sutskever et al.", "startOffset": 38, "endOffset": 99}, {"referenceID": 7, "context": ", 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch\u00fctze, 2015), sequence based recurrent neural models (Sutskever et al.", "startOffset": 38, "endOffset": 99}, {"referenceID": 27, "context": ", 2014; Kim, 2014; Yin and Sch\u00fctze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks", "startOffset": 82, "endOffset": 144}, {"referenceID": 24, "context": "(Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015).", "startOffset": 0, "endOffset": 57}, {"referenceID": 28, "context": "(Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015).", "startOffset": 0, "endOffset": 57}, {"referenceID": 36, "context": "(Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015).", "startOffset": 0, "endOffset": 57}, {"referenceID": 1, "context": "Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016).", "startOffset": 94, "endOffset": 194}, {"referenceID": 33, "context": "Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016).", "startOffset": 94, "endOffset": 194}, {"referenceID": 16, "context": "Such attention based approaches have achieved promising performances on a variety of NLP tasks (Luong et al., 2015; Kumar et al., 2015; Rush et al., 2015).", "startOffset": 95, "endOffset": 154}, {"referenceID": 9, "context": "Such attention based approaches have achieved promising performances on a variety of NLP tasks (Luong et al., 2015; Kumar et al., 2015; Rush et al., 2015).", "startOffset": 95, "endOffset": 154}, {"referenceID": 23, "context": "Such attention based approaches have achieved promising performances on a variety of NLP tasks (Luong et al., 2015; Kumar et al., 2015; Rush et al., 2015).", "startOffset": 95, "endOffset": 154}], "year": 2016, "abstractText": "We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.", "creator": "LaTeX with hyperref package"}}}