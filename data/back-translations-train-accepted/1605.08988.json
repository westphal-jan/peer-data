{"id": "1605.08988", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2016", "title": "On Explore-Then-Commit strategies", "abstract": "We study the problem of minimising regret in two-armed bandit problems with Gaussian rewards. Our objective is to use this simple setting to illustrate that strategies based on an exploration phase (up to a stopping time) followed by exploitation are necessarily suboptimal. The results hold regardless of whether or not the difference in means between the two arms is known. Besides the main message, we also refine existing deviation inequalities, which allow us to design fully sequential strategies with finite-time regret guarantees that are (a) asymptotically optimal as the horizon grows and (b) order-optimal in the minimax sense. Furthermore we provide empirical evidence that the theory also holds in practice and discuss extensions to non-gaussian and multiple-armed case.", "histories": [["v1", "Sun, 29 May 2016 10:35:33 GMT  (641kb,D)", "http://arxiv.org/abs/1605.08988v1", null], ["v2", "Mon, 14 Nov 2016 12:40:20 GMT  (322kb,D)", "http://arxiv.org/abs/1605.08988v2", null]], "reviews": [], "SUBJECTS": "math.ST cs.LG stat.TH", "authors": ["aur\u00e9lien garivier", "tor lattimore", "emilie kaufmann"], "accepted": true, "id": "1605.08988"}, "pdf": {"name": "1605.08988.pdf", "metadata": {"source": "CRF", "title": "On Explore-Then-Commit Strategies", "authors": ["Aur\u00e9lien Garivier", "Emilie Kaufmann"], "emails": ["aurelien.garivier@math.univ-toulouse.fr", "emilie.kaufmann@inria.fr", "tor.lattimore@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Imagine a company that wants to optimize its daily profit by choosing from one of two possible website layouts. A natural approach is to start with a period of A / B testing (exploration), during which the two versions are presented uniformly to users. Once the test is completed, the company shows the version that is believed to generate the most profit for the rest of the day (exploitation). The time spent on exploration can be adaptively chosen based on past observations, but could also be fixed in advance. Our contribution is to show that strategies of this form are much worse than allowing the company to dynamically select which website to display without restrictions for the whole day. Our analysis focuses on a simple sequential decision problem played over T time steps."}, {"heading": "2 Notation and Summary of Results", "text": "We assume that the horizon T is known to the agent. The optimal action is an \"arg max\" (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\" (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\" (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\") (\"arg (\" arg) (\"arg) (\" (\"arg) (\" arg) (\"(\" arg max \") (\" (\"arg max\") (\"arg max\") (\"arg max\") (\"arg max\" (\"arg max\") (\"arg max\") (\"arg max\" (\"arg max\") (\"arg max\") (\"arg max\" (\"arg max\") (\"arg max\") (\"arg max\" (\"arg max\") (\"arg max\" (\") (\" arg max \") (\" arg max \"(\") (\"arg max\") (\"arg max\" (\") (\" arg max \"(\" arg max \")\" (\"arg max\" (\") (\" arg max \")\" (\") (\" (\"arg max\") (\"arg max\") (arg max \") (\" (\"(\" arg max \") (\" arg max \") (\" arg max \"(\" arg max \") (\" arg max \") (\" arg max \") (\" arg max \") (\") (\"arg max\" (\") (\" (\") (\" arg max \") (\" arg max \"(\") (\") (\") (\"arg max (\") (\"arg max\") ("}, {"heading": "4 Fully Sequential Strategies for Known and Unknown Gaps", "text": "In the previous section, we have seen that a random stop time leads to a factor of 4 improvements in terms of asymptotic regret in terms of the naive fixed design strategy. We are now turning our attention fully to sequential strategies when this is known and unknown, the latter case being the classic 2-armed bandit problem and is now fairly well understood. Our modest contribution in this case is the first algorithm that is both asymptotically optimal and optimal in sequence at the same time. In both cases, we see that the complete sequential strategies on the best ETC strategies improve by a factor of 2.Known Gaps. We start by indicating the lower limit (proven in Section 5), which is a direct generalization of Lai and Robbins."}, {"heading": "5 Proof of the Lower Bounds (Theorems 3, 4, 6 and Lai&Robbins)", "text": "It is the same approach used by Lai and Robbins [1985], but rewritten and generalised in a more general way (especially with regard to the ETC strategies).The improvements result from the fact that the ETC strategies for the ETC strategies are very different."}, {"heading": "6 Beyond Uniform Exploration, Two Arms and Gaussian distributions", "text": "It is worth stressing the impossibility of a non-trivial lower limit based on the regret of the ETC strategies defined within the active limit. It is useful to stress the impossibility of a non-trivial lower limit based on the regret of the ETC strategies, with the exception of a possible (non-uniform) sampling rule. In fact, the use of UCB as a sampling rule, together with an infinite stop rule, is an artificial but formally valid ETC strategy that achieves the best possible rate for general strategies. This strategy is not a faithful counter-example to our assertion that ETC strategies are suboptimal because UCB is not a satisfactory exploration rule. If exploration is the goal, then a uniform sampling strategy is known to be optimal in the two-armed Gaussian cases [Kaufmann et al., 2014] which does not justify the use of TC strategies (e.g. for miniature rigging and rigging)."}, {"heading": "A Proof of Theorem 1", "text": "The number of pulls of suboptimal arm 2 is N2 = n + (T \u2212 2n) 1 {Sn \u2264 0}, where Sn = (X1 \u2212 Y1) + \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (Xn \u2212 Yn) \u0445 N (n \u0445, 2n).The expected regret of the strategy using 2n exploration steps is Rn\u00b5 (T) = \u0445 E\u00b5 (n + (T \u2212 2n) P\u00b5 (Sn \u2264 0))).( 5) ButP\u00b5 (Sn \u2264 0) = P\u00b5 (Sn \u2264 0) = P\u00b5 (Sn \u2264 2).2 = \u2212 n = \u2212 implicit n2) the pdf (or cdf) of the standard Gaussian distribution, and the reminder that the lambert function is defined for all y > 0 of W (y), exp (W (y).2 exp (y)."}, {"heading": "B Proof of Theorem 2", "text": "Remember that a protocol (2) = a protocol (T) = a protocol (T) = a protocol (T) = a protocol (T) = a protocol (T) = a protocol (T) = a protocol (T) = a protocol (T) = a protocol (T) (2) + a protocol (T) (2) with a probability of less than 1 / 2 and the protocol is limited by (T) + 2. Otherwise, note that a protocol (T) = 0, Sn = (X1 \u2212 Y1) + \u00b7 a protocol (Xn \u2212 Yn) + a protocol (Xn \u2212 Yn) for each n (T). For each n > 0, let nu = (T) a protocol (T) = (T) = a protocol (T)."}, {"heading": "C Proof of Theorem 5", "text": "Remember that when introducing the random variables F = (a) in Appendix F (c), one assumed from (2) RBAI-ETC\u00b5 (T) \u2264 T-P\u00b5 (F) + 0, s \u2212 4 s log (T2s)) = P\u00b5 (s) in Appendix F (2) that T-4e2, an obtainsP\u00b5 (F) \u2264 T-4s: 2s \u2264 T, \u00b5 1, s \u2212 4 s log (T2s)) = P\u00b5 (s \u2264 T / 2: 2 s log (T-4e2, an obtainsP\u00b5 (F) \u2264 P\u00b5 (T-2s) \u2264 T, p-4s log (T-4s) and T-4s log (T-4s) = P\u00b5 (T / 2 s) \u2264 2 s log (T / 2 s) \u2212 0."}, {"heading": "D Proof of Theorem 7", "text": "Define a random time duration (\u03b52) = max. \u2212 4.1 \u2212 \u2212 \u2212 \u2212 4.2), in which \u03c4i = min {t \u2264 T: sups \u2265 t | \u00b5 i, s \u2212 \u2212 4.0. By concentrating the term 1. (a) in Appendix F we have E\u00b5 [\u03c4i] \u2264 1 + 9 / \u03b52T and so E\u00b5 [4.1 + 4.2] \u2264 E\u00b5 [4.2 + 18 / \u03b52T. For t > 2.0 we have | 4.0 At, max (t \u2212 1) \u2212 \u00b5At, max | < \u03b5T. Therefore, the expected number of pulls of the suboptimal arm of E\u00b5 can be limited [N2 (T) \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 4.0 T = 1 {It = 2}] \u2264 E\u00b5 / 4.0 \u00b2 T (2.0% / 2.0%)."}, {"heading": "E Proof of Theorem 8", "text": "For each \u03b5 (0, \u2206), we have E\u00b5 [N2 (T)] = E\u00b5 [T \u2211 t = 1 {At = 2}] \u2264 E\u00b5 [T \u2211 t = 1 {At = 2 and \u00b5 \u00b2 (t \u2212 1) + \u221a 2 N2 (t \u2212 1) log (T N2 (t \u2212 1) log (T N2 (t \u2212 1))) \u2265 \u00b51 \u2212 \u03b5] + TP\u00b5 (s: p = 1, s + 270 s log (Ts) \u2264 \u00b51 \u2212 \u03b5).For the concentration Lemma 1. (b) in Appendix F, whenever T (T \u2212 \u03b5) implicitly we have implicitly 2 \u2265 2, E\u00b5 [T = 1 {At = 2 and p \u2212 2 (t \u2212 1) implicitly (T) + 270 (t \u2212 1) log (T N2 (t \u2212 1) implicitly. For each T (t \u2212 1) implicitly log (T) implicitly (p \u2212 2) implicitly (p \u2212 \u03b5) implicitly (p \u2212 2) implicitly, implicitly (T \u2212 2) implicitly (T) implicitly, implicitly (T) implicitly (T), implicitly (T (T), implicitly (T), implicitly (T (T)."}, {"heading": "F Deviation Inequalities", "text": "As was already the case in the proof of theorem 1, we rely heavily on the following known inequality at the end of a Gaussian distribution: if X \u0445 N (0, 1), then for all x > 0P (X). Let us put both together (1, 1 and 2). Lemma 1 gathers some more specific results that are useful in our regret analyses and which we believe to be of some interest to ourselves. Lemma 1. Let us apply both > 0 and W1, W2, W2,. Its standard i.e. Gaussian 2 random variables and \u00b5 2 (T)."}, {"heading": "G Numerical Experiments", "text": "We present here the regret of the five strategies presented in this article on a bandit problem with \u2206 = 1 / 5 for different horizon values, estimated by 4,105 Monte Carlo replications. In the legend, the estimated steep slopes of \u2206 R\u03c0 (T) (in logarithmic scale) are given by political name.50 100 200 500 1000 2000 5000 10000 20000 500000 2040 6080 100FB \u2212 ETC: 3.65 BAI \u2212 ETC: 2.98 UCB: 1.59 SPRT \u2212 ETC: 1.03 D \u2212 UCB: 0.77The experimental behavior of the algorithms reflects the theoretical results presented above: the regret grows asymptotically with the logarithm of the horizon, the experimental coefficients roughly correspond to the theory, and the relative order of the strategies is respected. However, it should be noted that for short horizons the hierarchy is not quite the same and the growth rate is not logically posed in 2016; this question is not logically.]"}], "references": [{"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Audibert and Bubeck.,? \\Q2009\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2009}, {"title": "UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem", "author": ["Peter Auer", "Ronald Ortner"], "venue": "Periodica Mathematica Hungarica,", "citeRegEx": "Auer and Ortner.,? \\Q2010\\E", "shortCiteRegEx": "Auer and Ortner.", "year": 2010}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f3 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Prior-free and prior-dependent regret bounds for thompson sampling", "author": ["S\u00e9bastien Bubeck", "Che-Yu Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bubeck and Liu.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck and Liu.", "year": 2013}, {"title": "Bounded regret in stochastic multi-armed bandits", "author": ["S\u00e9bastien Bubeck", "Vianney Perchet", "Philippe Rigollet"], "venue": "In Proceedings of the 26th Conference On Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "Kullback\u2013Leibler upper confidence bounds for optimal sequential allocation", "author": ["Olivier Capp\u00e9", "Aur\u00e9lien Garivier", "Odalric-Ambrym Maillard", "R\u00e9mi Munos", "Gilles Stoltz"], "venue": "The Annals of Statistics,", "citeRegEx": "Capp\u00e9 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Capp\u00e9 et al\\.", "year": 2013}, {"title": "Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems", "author": ["Eyal Even-Dar", "Shie Mannor", "Yishay Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Even.Dar et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2006}, {"title": "Optimal best arm identification with fixed confidence", "author": ["Aur\u00e9lien Garivier", "Emilie Kaufmann"], "venue": "In Proceedings of the 29th Conference On Learning Theory (to appear),", "citeRegEx": "Garivier and Kaufmann.,? \\Q2016\\E", "shortCiteRegEx": "Garivier and Kaufmann.", "year": 2016}, {"title": "Explore first, exploit next: The true shape of regret in bandit problems", "author": ["Aur\u00e9lien Garivier", "Pierre M\u00e9nard", "Gilles Stoltz"], "venue": "arXiv preprint arXiv:1602.07182,", "citeRegEx": "Garivier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garivier et al\\.", "year": 2016}, {"title": "Inequalities on the lambert w function and hyperpower function", "author": ["Abdolhossein Hoorfar", "Mehdi Hassani"], "venue": "J. Inequal. Pure and Appl. Math,", "citeRegEx": "Hoorfar and Hassani.,? \\Q2008\\E", "shortCiteRegEx": "Hoorfar and Hassani.", "year": 2008}, {"title": "Sequential choice from several populations", "author": ["Michael N Katehakis", "Herbert Robbins"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Katehakis and Robbins.,? \\Q1995\\E", "shortCiteRegEx": "Katehakis and Robbins.", "year": 1995}, {"title": "On the Complexity of A/B Testing", "author": ["Emilie Kaufmann", "Olivier Capp\u00e9", "Aur\u00e9lien Garivier"], "venue": "In Proceedings of the 27th Conference On Learning Theory,", "citeRegEx": "Kaufmann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2014}, {"title": "Adaptive treatment allocation and the multi-armed bandit problem", "author": ["Tze Leung Lai"], "venue": "The Annals of Statistics,", "citeRegEx": "Lai.,? \\Q1987\\E", "shortCiteRegEx": "Lai.", "year": 1987}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Sequential design of comparative clinical trials", "author": ["Tze Leung Lai", "Herbert Robbins", "David Siegmund"], "venue": "Recent advances in statistics: papers in honor of Herman Chernoff on his sixtieth birthday,", "citeRegEx": "Lai et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1983}, {"title": "Optimally confident UCB: Improved regret for finite-armed bandits", "author": ["Tor Lattimore"], "venue": "Technical report,", "citeRegEx": "Lattimore.,? \\Q2015\\E", "shortCiteRegEx": "Lattimore.", "year": 2015}, {"title": "Brownian motion, volume 30", "author": ["Peter M\u00f6rters", "Yuval Peres"], "venue": null, "citeRegEx": "M\u00f6rters and Peres.,? \\Q2010\\E", "shortCiteRegEx": "M\u00f6rters and Peres.", "year": 2010}, {"title": "The multi-armed bandit with covariates", "author": ["Vianney Perchet", "Philippe Rigollet"], "venue": "The Annals of Statistics,", "citeRegEx": "Perchet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Perchet and Rigollet.", "year": 2013}, {"title": "Batched bandit problems", "author": ["Vianney Perchet", "Philippe Rigollet", "Sylvain Chassang", "Eric Snowberg"], "venue": "In Proceedings of the 28th Conference On Learning Theory,", "citeRegEx": "Perchet et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Perchet et al\\.", "year": 2015}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}, {"title": "Sequential Tests of Statistical Hypotheses", "author": ["Abraham Wald"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Wald.,? \\Q1945\\E", "shortCiteRegEx": "Wald.", "year": 1945}], "referenceMentions": [{"referenceID": 19, "context": "This framework is known as the multi-armed bandit problem, which has many applications and has been studied for almost a century [Thompson, 1933].", "startOffset": 129, "endOffset": 145}, {"referenceID": 10, "context": "Surprisingly it is possible to do even better by using a fully sequential strategy inspired by the UCB algorithm for multi-armed bandits [Katehakis and Robbins, 1995].", "startOffset": 137, "endOffset": 166}, {"referenceID": 10, "context": "As before, strategies based on ETC are suboptimal by a factor of 2 relative to the optimal rates achieved by fully sequential strategies such as UCB, which satisfies R \u03bc(T ) \u223c 2 log(T )/\u2206 [Katehakis and Robbins, 1995].", "startOffset": 188, "endOffset": 217}, {"referenceID": 1, "context": "Moreover, strategies of this kind have been proposed in the literature for more complicated settings [Auer and Ortner, 2010, Perchet and Rigollet, 2013, Perchet et al., 2015]. Recent progress on optimal exploration policies (e.g., by Garivier and Kaufmann [2016]) could have suggested that well-tuned variants of two-phase strategies might be near-optimal.", "startOffset": 102, "endOffset": 263}, {"referenceID": 1, "context": "Moreover, strategies of this kind have been proposed in the literature for more complicated settings [Auer and Ortner, 2010, Perchet and Rigollet, 2013, Perchet et al., 2015]. Recent progress on optimal exploration policies (e.g., by Garivier and Kaufmann [2016]) could have suggested that well-tuned variants of two-phase strategies might be near-optimal. We show, on the contrary, that optimal strategies for multi-armed bandit problems must be fully-sequential, and in particular should mix exploration and exploitation. We study two settings, one when the gap \u2206 = |\u03bc1 \u2212 \u03bc2| is known and the other when it is not. The most straight-forward strategy in the former case is to explore each action a fixed number of times n and subsequently exploit by choosing the action that appeared best while exploring. It is easy to calculate the optimal n and consequently show that this strategy suffers a regret of R \u03bc(T ) \u223c 4 log(T )/\u2206. A more general approach is to use a so-called Explore-Then-Commit (ETC) strategy, following a nomenclature introduced by Perchet et al. [2015]. An ETC strategy explores each action alternately until some data-dependent stopping time and subsequently commits to a single action for the remaining time-steps.", "startOffset": 102, "endOffset": 1072}, {"referenceID": 1, "context": "Moreover, strategies of this kind have been proposed in the literature for more complicated settings [Auer and Ortner, 2010, Perchet and Rigollet, 2013, Perchet et al., 2015]. Recent progress on optimal exploration policies (e.g., by Garivier and Kaufmann [2016]) could have suggested that well-tuned variants of two-phase strategies might be near-optimal. We show, on the contrary, that optimal strategies for multi-armed bandit problems must be fully-sequential, and in particular should mix exploration and exploitation. We study two settings, one when the gap \u2206 = |\u03bc1 \u2212 \u03bc2| is known and the other when it is not. The most straight-forward strategy in the former case is to explore each action a fixed number of times n and subsequently exploit by choosing the action that appeared best while exploring. It is easy to calculate the optimal n and consequently show that this strategy suffers a regret of R \u03bc(T ) \u223c 4 log(T )/\u2206. A more general approach is to use a so-called Explore-Then-Commit (ETC) strategy, following a nomenclature introduced by Perchet et al. [2015]. An ETC strategy explores each action alternately until some data-dependent stopping time and subsequently commits to a single action for the remaining time-steps. We show in Theorem 2 that by using a sequential probability ratio test (SPRT) it is possible to design an ETC strategy for which R \u03bc(T ) \u223c log(T )/\u2206, which improves on the above result by a factor of 4. We also prove a lower bound showing that no ETC strategy can improve on this result. Surprisingly it is possible to do even better by using a fully sequential strategy inspired by the UCB algorithm for multi-armed bandits [Katehakis and Robbins, 1995]. We design a new strategy for which R \u03bc(T ) \u223c log(T )/(2\u2206), which improves on the fixed-design strategy by a factor of 8 and on SPRT by a factor of 2. Again we prove a lower bound showing that no strategy can improve on this result. For the case where \u2206 is unknown, fixed-design strategies are hopeless because there is no reasonable tuning for the exploration budget n. However, it is possible to design an ETC strategy for unknown gaps. Our approach uses a modified fixed-budget best arm identification (BAI) algorithm in its exploration phase (see e.g., Even-Dar et al. [2006], Garivier and Kaufmann [2016]) and chooses the recommended arm for the remaining time-steps.", "startOffset": 102, "endOffset": 2271}, {"referenceID": 1, "context": "Moreover, strategies of this kind have been proposed in the literature for more complicated settings [Auer and Ortner, 2010, Perchet and Rigollet, 2013, Perchet et al., 2015]. Recent progress on optimal exploration policies (e.g., by Garivier and Kaufmann [2016]) could have suggested that well-tuned variants of two-phase strategies might be near-optimal. We show, on the contrary, that optimal strategies for multi-armed bandit problems must be fully-sequential, and in particular should mix exploration and exploitation. We study two settings, one when the gap \u2206 = |\u03bc1 \u2212 \u03bc2| is known and the other when it is not. The most straight-forward strategy in the former case is to explore each action a fixed number of times n and subsequently exploit by choosing the action that appeared best while exploring. It is easy to calculate the optimal n and consequently show that this strategy suffers a regret of R \u03bc(T ) \u223c 4 log(T )/\u2206. A more general approach is to use a so-called Explore-Then-Commit (ETC) strategy, following a nomenclature introduced by Perchet et al. [2015]. An ETC strategy explores each action alternately until some data-dependent stopping time and subsequently commits to a single action for the remaining time-steps. We show in Theorem 2 that by using a sequential probability ratio test (SPRT) it is possible to design an ETC strategy for which R \u03bc(T ) \u223c log(T )/\u2206, which improves on the above result by a factor of 4. We also prove a lower bound showing that no ETC strategy can improve on this result. Surprisingly it is possible to do even better by using a fully sequential strategy inspired by the UCB algorithm for multi-armed bandits [Katehakis and Robbins, 1995]. We design a new strategy for which R \u03bc(T ) \u223c log(T )/(2\u2206), which improves on the fixed-design strategy by a factor of 8 and on SPRT by a factor of 2. Again we prove a lower bound showing that no strategy can improve on this result. For the case where \u2206 is unknown, fixed-design strategies are hopeless because there is no reasonable tuning for the exploration budget n. However, it is possible to design an ETC strategy for unknown gaps. Our approach uses a modified fixed-budget best arm identification (BAI) algorithm in its exploration phase (see e.g., Even-Dar et al. [2006], Garivier and Kaufmann [2016]) and chooses the recommended arm for the remaining time-steps.", "startOffset": 102, "endOffset": 2301}, {"referenceID": 11, "context": "As we consider two-armed, Gaussian bandits with equal variances, we focus here on uniform sampling rules, which have been shown in Kaufmann et al. [2014] to be optimal in that setting.", "startOffset": 131, "endOffset": 154}, {"referenceID": 12, "context": "For unknown \u2206 we briefly recall the well-known results, but also propose a new regret analysis of the UCB* algorithm, a variant of UCB that can be traced back to Lai [1987], for which we also obtain order-optimal minimax regret.", "startOffset": 162, "endOffset": 173}, {"referenceID": 8, "context": "Garivier et al. [2016]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 20, "context": "The work of Wald [1945] shows that a significant gain in terms of expected number of samples can be obtained by using a sequential rather than a batch test.", "startOffset": 12, "endOffset": 24}, {"referenceID": 11, "context": "In a bandit model with two Gaussian arms, Kaufmann et al. [2014] propose a \u03b4-PAC algorithm using a uniform sampling rule and a stopping rule \u03c4\u03b4 that asymptotically attains the minimal sample complexity E\u03bc[\u03c4\u03b4] \u223c (8/\u2206) log(1/\u03b4).", "startOffset": 42, "endOffset": 65}, {"referenceID": 12, "context": "Note that a similar strategy was proposed and analysed by Lai et al. [1983], but in the continuous time framework and with asymptotic analysis only.", "startOffset": 58, "endOffset": 76}, {"referenceID": 3, "context": "the former case, we are not aware of any previous research where the gap is known except the line of work by Bubeck et al. [2013], Bubeck and Liu [2013], where different questions are treated.", "startOffset": 109, "endOffset": 130}, {"referenceID": 3, "context": "[2013], Bubeck and Liu [2013], where different questions are treated.", "startOffset": 8, "endOffset": 30}, {"referenceID": 7, "context": "In the classical bandit setting where \u2206 is unknown, UCB by Katehakis and Robbins [1995] is known to be asymptotically optimal: R \u03bc (T ) \u223c 2 log(T )/\u2206, which matches the lower bound of Lai and Robbins [1985].", "startOffset": 59, "endOffset": 88}, {"referenceID": 7, "context": "In the classical bandit setting where \u2206 is unknown, UCB by Katehakis and Robbins [1995] is known to be asymptotically optimal: R \u03bc (T ) \u223c 2 log(T )/\u2206, which matches the lower bound of Lai and Robbins [1985]. Non-asymptotic regret bounds are given for example by Auer et al.", "startOffset": 59, "endOffset": 207}, {"referenceID": 1, "context": "Non-asymptotic regret bounds are given for example by Auer et al. [2002], Capp\u00e9 et al.", "startOffset": 54, "endOffset": 73}, {"referenceID": 1, "context": "Non-asymptotic regret bounds are given for example by Auer et al. [2002], Capp\u00e9 et al. [2013]. Unfortunately, UCB is not optimal in the minimax sense, which is so far only achieved by algorithms that are not asymptotically optimal [Audibert and Bubeck, 2009, Lattimore, 2015].", "startOffset": 54, "endOffset": 94}, {"referenceID": 0, "context": "Unfortunately, UCB is not optimal in the minimax sense, which is so far only achieved by algorithms that are not asymptotically optimal [Audibert and Bubeck, 2009, Lattimore, 2015]. Here, with only two arms, we are able to show that Algorithm 5 below is simultaneously minimax order-optimal and asymptotically optimal. The strategy is essentially the same as suggested by Lai [1987], but with a fractionally smaller confidence bound.", "startOffset": 137, "endOffset": 383}, {"referenceID": 8, "context": "The improvements come thanks to Inequality 4 in [Garivier et al., 2016], which states that for every (\u03bc1, \u03bc \u2032 2) \u2208 H and for every stopping time \u03c3 such that N2(T ) is F\u03c3-measurable, E\u03bc [ N1(\u03c3) ] (\u03bc1 \u2212 \u03bc1) 2 + E\u03bc [ N2(\u03c3) ] (\u03bc2 \u2212 \u03bc2) 2 \u2265 kl ( E\u03bc [ N2(T ) T ] , E\u03bc\u2032 [ N2(T ) T ]) ,", "startOffset": 48, "endOffset": 71}, {"referenceID": 11, "context": "This is the same approach used by Lai and Robbins [1985], but rewritten and generalised in a more powerful way (in particular regarding the ETC strategies).", "startOffset": 34, "endOffset": 57}, {"referenceID": 8, "context": "Moreover, note that this proof may also lead to (not-so-simple) non-asymptotic lower-bounds, as shown in Garivier et al. [2016] for example.", "startOffset": 105, "endOffset": 128}, {"referenceID": 11, "context": "If exploration is the objective, then uniform sampling is known to be optimal in the two-armed Gaussian case [Kaufmann et al., 2014], which justifies the uniform sampling assumption.", "startOffset": 109, "endOffset": 132}, {"referenceID": 8, "context": "If exploration is the objective, then uniform sampling is known to be optimal in the two-armed Gaussian case [Kaufmann et al., 2014], which justifies the uniform sampling assumption. The use of ETC strategies for regret minimisation (e.g., as presented by Perchet and Rigollet [2013]) is certainly not limited to bandit models with 2 arms.", "startOffset": 110, "endOffset": 284}, {"referenceID": 1, "context": "Arms are eliminated from the active set once their optimality becomes implausible and the exploration phase terminates when the active set contains only a single arm (an example is by Auer and Ortner [2010]).", "startOffset": 184, "endOffset": 207}, {"referenceID": 1, "context": "Arms are eliminated from the active set once their optimality becomes implausible and the exploration phase terminates when the active set contains only a single arm (an example is by Auer and Ortner [2010]). The Successive Elimination algorithm has been introduced by Even-Dar et al. [2006] for best-arm identification in the fixed-confidence setting.", "startOffset": 184, "endOffset": 292}, {"referenceID": 1, "context": "Arms are eliminated from the active set once their optimality becomes implausible and the exploration phase terminates when the active set contains only a single arm (an example is by Auer and Ortner [2010]). The Successive Elimination algorithm has been introduced by Even-Dar et al. [2006] for best-arm identification in the fixed-confidence setting. But as shown in Garivier and Kaufmann [2016], Successive Elimination is suboptimal for the best arm identification task in almost all settings except two-armed Gaussian bandits.", "startOffset": 184, "endOffset": 398}, {"referenceID": 1, "context": "Arms are eliminated from the active set once their optimality becomes implausible and the exploration phase terminates when the active set contains only a single arm (an example is by Auer and Ortner [2010]). The Successive Elimination algorithm has been introduced by Even-Dar et al. [2006] for best-arm identification in the fixed-confidence setting. But as shown in Garivier and Kaufmann [2016], Successive Elimination is suboptimal for the best arm identification task in almost all settings except two-armed Gaussian bandits. It is therefore interesting to investigate the performance in terms of regret of an ETC algorithm using an optimal BAI algorithm. This is actually possible not only for Gaussian distributions, but more generally for one-parameter exponential families, for which Garivier and Kaufmann [2016] propose the asymptotically optimal Track-and-Stop strategy.", "startOffset": 184, "endOffset": 822}, {"referenceID": 9, "context": "If T\u2206 \u2265 4 \u221a 2\u03c0e, the inequality W (y) \u2264 log ( (1 + e\u22121)y/ log(y) ) valid for all y \u2265 e (see Hoorfar and Hassani [2008]) entails", "startOffset": 92, "endOffset": 119}, {"referenceID": 16, "context": ", M\u00f6rters and Peres [2010]) it holds that", "startOffset": 2, "endOffset": 27}, {"referenceID": 8, "context": "However, it should be noted that for short horizons the hierarchy is not quite the same, and the growth rate is not logarithmic; this question is raised in Garivier et al. [2016].", "startOffset": 156, "endOffset": 179}], "year": 2017, "abstractText": "We study the problem of minimising regret in two-armed bandit problems with Gaussian rewards. Our objective is to use this simple setting to illustrate that strategies based on an exploration phase (up to a stopping time) followed by exploitation are necessarily suboptimal. The results hold regardless of whether or not the difference in means between the two arms is known. Besides the main message, we also refine existing deviation inequalities, which allow us to design fully sequential strategies with finite-time regret guarantees that are (a) asymptotically optimal as the horizon grows and (b) order-optimal in the minimax sense. Furthermore we provide empirical evidence that the theory also holds in practice and discuss extensions to non-gaussian and multiple-armed case.", "creator": "LaTeX with hyperref package"}}}