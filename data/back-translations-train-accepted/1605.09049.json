{"id": "1605.09049", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2016", "title": "Recycling Randomness with Structure for Sublinear time Kernel Expansions", "abstract": "We propose a scheme for recycling Gaussian random vectors into structured matrices to approximate various kernel functions in sublinear time via random embeddings. Our framework includes the Fastfood construction as a special case, but also extends to Circulant, Toeplitz and Hankel matrices, and the broader family of structured matrices that are characterized by the concept of low-displacement rank. We introduce notions of coherence and graph-theoretic structural constants that control the approximation quality, and prove unbiasedness and low-variance properties of random feature maps that arise within our framework. For the case of low-displacement matrices, we show how the degree of structure and randomness can be controlled to reduce statistical variance at the cost of increased computation and storage requirements. Empirical results strongly support our theory and justify the use of a broader family of structured matrices for scaling up kernel methods using random features.", "histories": [["v1", "Sun, 29 May 2016 19:21:22 GMT  (275kb,D)", "http://arxiv.org/abs/1605.09049v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA stat.ML", "authors": ["krzysztof choromanski", "vikas sindhwani"], "accepted": true, "id": "1605.09049"}, "pdf": {"name": "1605.09049.pdf", "metadata": {"source": "META", "title": "Recycling Randomness with Structure for Sublinear time Kernel Expansions", "authors": ["Krzysztof Choromanski", "Vikas Sindhwani"], "emails": ["KCHORO@GOOGLE.COM", "SINDHWANI@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that it is a way in which people move in the world in a way that they have not done in the past. (...) It is as if they had been able to change the world. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world in the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world, the world of the world of the world, the world of the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world of the world, the world of the world, the world of the world, the world of the world, the world, the world, the world of the world, the world of the world, the world of the world of the world of the world, the world of the world, the world of the world of the world of the world of the world, the world of the world of the world of the world, the world of the world of the world of the world, the world of the world of the world of the world of the world, the world of the world of the world of the world, the world of the world of the world of the world, the world of the world of the world of the world, the world of the world, the world of the world of the world of the world, the world, the world of the world, the world of the world of the world of the world, the world of the world, the world, the world, the world of the world, the world of the world, the world, the world of the world, the world of the world, the world of the world, the world, the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world, the world of the world, the world of the world of the world of the world, the world, the world of the world, the world, the world of the world of the world"}, {"heading": "2. Background and Preliminaries", "text": "We start with a brisk background via random feature cards and structured matrices."}, {"heading": "2.1. Random Embeddings, Nonlinearities and Kernels", "text": "The original construction of Rahimi & Recht (2007) was motivated by a classical result that defined the class of shift-invariant positive definitive functions.Theorem 2.1 (Bochner's Theorem (Bochner, 1933): A continuous shift-invariant scaled kernel function K (x, z): A continuous shift-invariant scaled kernel function K (x, z): a positive if and only if it is the Fourier transformation of a unique finite probability measurement p to Rn. This is for each x-invariant scaled kernel function K (x, z): an x-z threshold (w) Twp (w) dw = Ew-p (e \u2212 i).Bochner's theorem establishes a correspondence between shift-invariant kernel functions and probability densities to Rn."}, {"heading": "2.2. Structured Matrices", "text": "The question of the causes and the causes of the excesses of the global imbalances that are spreading in the individual countries is not new. - The question of the causes of the imbalances in the individual countries is not new. - The question of the causes of the imbalances in the individual countries is not new. - The question of the causes of the imbalances in the individual countries is not new. - The question of the causes of the imbalances in the individual countries is not new. - The question of the causes of the imbalances in the individual countries is not new. - The question of the causes of the imbalances in the individual countries is not new. - The question of the distribution of the imbalances in the individual countries is not new. - The question of the distribution of the imbalances in the individual countries is not new. - The question of the distribution of the imbalances is not new. - The question of the distribution of the imbalances in the individual countries is not new."}, {"heading": "2.3. FastFood", "text": "In connection with fast-kernel approximations (Le et al., 2013), we introduce the fast-food technique, in which the matrix M in Equation 1 is parameterized by a product of diagonal and simple matrices as follows: F = 1 \u221a n SHGPHB. (6) Here, S, G, B are diagonal random matrices, P is a permutation matrix and H is the Walsh-Hadamard matrix. The k \u00b7 n matrix M is achieved by vertically stacking k / n independent copies of the n \u00b7 n matrix F. Multiplying against such a matrix can be done in timeframes O (k log n). The authors prove that (1) the fast-food approximation is unbiased, (2) its variance is at most the variance of standard-Gaussian random features with an additional O (1k) term, and (3) for a given error of a fast-food exponent, which is limited by the exponent T."}, {"heading": "3. Structured Matrices from Gaussian Vectors", "text": "In this section, we present a general structured matrix model that makes it possible to recycle a small Gaussian vector to mimic the properties of a Gaussian random matrix. First, we present some basic concepts in our construction. Note that we emphasize intuitions in our exposure - formal evidence is provided in our complementary matrix. This vector represents the \"budget of randomness\" used in our structured matrix construction. Our goal is to recycle the Gaussian vector g to construct random matrices with desirable properties."}, {"heading": "3.2.1. CIRCULANT MATRICES", "text": "Circular matrices can be constructed using the P model with a random budget of t = n and matrices {Pi} mi = 1 of the entries in {0, 1}. See Fig. 1 for a descriptive construction. The coherence of the related P model is trivially satisfactory: \u00b5 [P] = O (1) and \u00b5 [P] = 0. The coherence diagrams are apex disc cycles. Since each cycle can be colored with a maximum of 3 colors, the chromatic number of the P model is satisfactory: \u0432 [P] \u2264 3."}, {"heading": "3.2.2. TOEPLITZ AND HANKEL MATRICES", "text": "The corresponding P models are derived in a similar way to circular matrices, in particular, each column of each pi is a binary vector. The corresponding coherence graphs have vertices of no more than 2 degrees, and therefore the chromatic number \u0430 [P] is no more than 3. As in the previous case, the coherence \u00b5 [P] is in the order O (1) and \u00b5 [P] = 0."}, {"heading": "3.2.3. FASTFOOD MATRICES", "text": "The fast food approach (Le et al., 2013) is a very specific case of the P model. Note that the core term in the fast food transformation, Eq.6, is the structured matrix HG, where H = {hi, j} Hadamard and G is a random diagonal Gaussian matrix (the most correct terms HB in Eq.6 implement a data pre-processing to dense all data points, and the normalization is done by the most left scale matrix S. The matrix HG can be constructed using the P model with the fixed budget of randomness g = (g1,..., gn) and using the sequence of the matrices P = (P1,..., Pn), with each Pi being a random diagonal matrix with entries on the diagonal of the form: hi, 1,..., hi, n. The quality of the fast food approach can now be explained in the general model-P-1 whereby each Pi model is closely related to the P1."}, {"heading": "3.2.4. TOEPLITZ-LIKE SEMI-GAUSSIAN MATRICES", "text": "It can be assumed that g1,..., gr, Rn, which defines the circular components in Eqn 5, are independent Gaussian vectors. They will serve as a \"budget of randomness\" in the related P model that we are about to describe. Vectors h1,... hrdefinition of the skewCirculant components in Eqn 5 can be defined in different ways. Below, we present two general schemes: Randomly disretired vectors hi: Each dimension of each Hi is independent of the binary set {\u2212 1, 1).Sparse The settings are sparse (but not zero), i."}, {"heading": "3.3. Construction of Random Feature Maps", "text": "Since S [P], the m \u00b7 n structured random matrix defined by a P model, is constructed as follows instead of the k \u00b7 n Gaussian random matrix M in Equation 1, the characteristic map for a data vector x is constructed as follows. \u2022 Pre-processing phase: Calculate x \"= D1HD0x, where H\" Rn \"n is an l2-normalized Hadamard matrix and D0, D1\" {\u2212 1, + 1} n \"independent random diagonal matrices. Note that this transformation does not alter the values of the Gaussian or Arc cosine nuclei, as they are spherically invariant. This pre-processing condenses the input data vector. \u2022 Calculate x\" \u2032 = S [P] x \"x\" x \"x\" \"Rm\" Rm. \"\u2022 Calculate x\" Rk \"by concatenating random instances of the single vector (similar to the centrifuge of the structure S\" in the upper row)."}, {"heading": "4. Theoretical results", "text": "We assume that the variance of the calculated structural approximation of the kernel is close to the unstructured one.We also present the results that specifically aim at low shifts, and show how the shift rank knob can be used to increase the budget of randomness and decrease the variance. We denote the approximation of the kernel for two vectors x, z) the approximation of the kernel for two vectors x, z) which is wrong when the P model is used. From K-G (x, z) we denote the approximation of the kernel for two vectors x, z) the completely unstructured setting is applied with really random Gaussian matrix G. We start with the following results."}, {"heading": "5. Empirical Support", "text": "In this area, we are able to search for new paths. In this area, we are looking for new paths. In this area, we are looking for new paths that we want to tread. In other areas, we are looking for new paths. In other areas, we are looking for new paths. In other areas, we are looking for new paths. In other areas, we are looking for new paths. In other areas, we are looking for new paths that we want to tread."}, {"heading": "6. Conclusions", "text": "We have theoretically justified and empirically validated the use of a broad family of structured matrices to accelerate the design of random embedding to approach various core functions. In particular, the class of toeplitz-like semi-Gaussian matrices allows our design to extend from very compact to completely random matrices."}, {"heading": "7. Appendix", "text": "We prove all the theoretical results of our structured mechanisms. We have to introduce some technical terms... From now on f is designated one of the following functions: Sin, cos, character, or a linear rectifier. We call the building blocks of the structured matrix constructed according to the P model, which is stacked vertically to produce the final structured matrix. Let us let v1, v2, Rn are two datapoints from the preprocessed input dataset D1HD0X. Let us let d be a fixed integer constant stacked vertically to produce the final structured matrix. Let us let v1, v2, Rn be two datapoints from the preprocessed input dataset dataset. Let d be a fixed integer constant. Let R = {i1, ir} be some r-element subset of the set {1,... where the number of the matrict stands in the structured one."}, {"heading": "As a corollary:", "text": "The proof for this result follows along the lines of the proof for theorem 4.1 and theorem 4.2. Let's take the formulas for si1, j1 \u00b7 si2, j2 that can be derived from the proof for theorem 7.1. Note: We want to have a sum of random variables that can be decoupled in O (1) so that variables in each subsum are independent (here we use exactly the same trick as in the proof for theorem 4.1). In each subsum, we apply the sisisisisian formula as a subsumtive that variables in each subsum are independent (here we use exactly the same trick as in the proof for theorem 4.2)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We propose a scheme for recycling Gaussian random vectors into structured matrices to approximate various kernel functions in sublinear time via random embeddings. Our framework includes the Fastfood construction of Le et al. (2013) as a special case, but also extends to Circulant, Toeplitz and Hankel matrices, and the broader family of structured matrices that are characterized by the concept of lowdisplacement rank. We introduce notions of coherence and graph-theoretic structural constants that control the approximation quality, and prove unbiasedness and low-variance properties of random feature maps that arise within our framework. For the case of low-displacement matrices, we show how the degree of structure and randomness can be controlled to reduce statistical variance at the cost of increased computation and storage requirements. Empirical results strongly support our theory and justify the use of a broader family of structured matrices for scaling up kernel methods using random features.", "creator": "LaTeX with hyperref package"}}}