{"id": "1605.09096", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change", "abstract": "Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.", "histories": [["v1", "Mon, 30 May 2016 03:54:18 GMT  (379kb,D)", "http://arxiv.org/abs/1605.09096v1", "Accepted for publication at the Association for Computational Linguistics (ACL), 2016 (this http URL) under working-title \"Diachronic word embeddings reveal laws of semantic change\""], ["v2", "Tue, 7 Jun 2016 20:24:30 GMT  (629kb,D)", "http://arxiv.org/abs/1605.09096v2", "Accepted for publication at the Association for Computational Linguistics (ACL), 2016 (this http URL)"], ["v3", "Mon, 8 Aug 2016 09:36:39 GMT  (441kb,D)", "http://arxiv.org/abs/1605.09096v3", "Association for Computational Linguistics (ACL), 2016 (this http URL)"], ["v4", "Mon, 26 Sep 2016 23:35:05 GMT  (441kb,D)", "http://arxiv.org/abs/1605.09096v4", "Association for Computational Linguistics (ACL), 2016"]], "COMMENTS": "Accepted for publication at the Association for Computational Linguistics (ACL), 2016 (this http URL) under working-title \"Diachronic word embeddings reveal laws of semantic change\"", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["william l hamilton", "jure leskovec", "dan jurafsky"], "accepted": true, "id": "1605.09096"}, "pdf": {"name": "1605.09096.pdf", "metadata": {"source": "CRF", "title": "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change", "authors": ["William L. Hamilton", "Jure Leskovec", "Dan Jurafsky"], "emails": ["wleif@stanford.edu", "jure@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "The question of the nature of semantic change, for example, is higher in some words than in others (Blank, 1999) - compared to the stable semantic history of cats (ProtoGermanic kattuz, \"cat\") to the different meanings of the English language (\"to mould,\" \"a\"). Various hypotheses have been offered about such regularities in semantic changes, such as an increasing subjectivation of meaning or the grammatization of inferences (e.g. Geeraerts, 1997; Blank, 1999; Traugott und Dasher, 2001). But many key questions about semantic change remain unanswered."}, {"heading": "2 Diachronic embedding methods", "text": "The following sections outline how we construct diachronic (historical) word embeddings by first constructing embeddings in each period and then aligning them over time, as well as the metrics we use to quantify semantic alterations.22 github.com / williamleif / historical-embeddings"}, {"heading": "2.1 Embedding algorithms", "text": "We use three methods to construct word embeddings within each period: PPMI, SVD, and SGNS (i.e. word2vec).3 These distribution methods represent each word wi by a vector wi that collects information about its co-event statistics, operationalizing the \"distribution hypothesis\" that word semantics is implicit in co-event relationships (Harris, 1954; Firth, 1957). The semantic similarity / distance between two words is approximated by the cosmic similarity / distance between their vectors (Turney and Pantel, 2010)."}, {"heading": "2.1.1 PPMI", "text": "In the PPMI representations, the vector embedding for the word wi-V contains the values of positive point-by-point mutual information (PPMI) between wi and a large number of predefined \"context words.\" The word vectors correspond to the lines of the MPPMI-R matrix | V | \u00d7 | VC | with entries from MPPMIi, j = max {log (p, cj) p (cj)) p (cj)) -\u03b1, 0}, (1) where cj-VC is a context word and \u03b1 > 0 is a negative precursor that provides a smoothing preset (Levy et al., 2015). The p vectors correspond to the smoothed empirical probabilities of word (co-) occurrences within fixed-size text windows that are gliding. Cropping the PPMI values above zero ensures that they remain finite and dramatically improve the results (Bullinaria and Levy, 2007, Levy)."}, {"heading": "2.1.2 SVD", "text": "SVD embedding corresponds to low-dimensional approximations of the PPMI embedding learned by means of singular value decomposition (Levy et al., 2015).The vector embedding for word wi is given by wSVDi = (U\u03a3 \u03b3) i, (2) where MPPMI = U\u0442V > the truncated singular value decomposition of MPPMI and \u03b3 [0, 1] is a eigenvalue weighting parameter.The determination of \u03b3 < 1 has shown that embedding qualities are dramatically improved (Turney and Pantel, 2010; Bullinaria and Levy, 2012).This SVD approach can be seen as a generalization of latent semantic analysis (Landauer and Dumais, 1997), where the term document matrix is replaced by MPPMI. Compared to PPMI, SVD representations can be more robust, as dimensionality reduction acts as a form of regulation."}, {"heading": "2.1.3 Skip-gram with negative sampling", "text": "SGNS \"neural\" embeddings are optimized to predict parallel relationships using an approximate target known as a \"negative scan skip program\" (Mikolov et al., 2013). In SGNS, each word wi is represented by two dense, low-dimensional vectors: a word vector (wSGNSi) and a context vector (cSGNSi). These embeddings are optimized by stochastic gradient descent, so that this window contains wi. SGNS optimization avoids the calculation of the normalization constant in (3) by randomly seeing \"negative\" context words, cn, for each target word in a specified text window as it contains wi."}, {"heading": "2.2 Datasets, pre-processing, and hyperparameters", "text": "We trained models using the 6 datasets taken from Google N-Grams (Lin et al., 2012) and the COHA corpus (Davies, 2010) as described in Table 1. Google N-Gram datasets are extremely large (accounting for \u2248 6% of all books ever published), but also contain many corpus artifacts, such as changing sampling distortions over time (Pechenick et al., 2015). In contrast, the COHA corpus has been carefully selected to be genre-balanced and representative of American English over the last 200 years, although it is, as a result, two orders of magnitude smaller. The COHA corpus also contains pre-extracted word lemmas that we have used to confirm that our results have both the lemma and the raw token levels. All datasets have been aggregated to the granularity of tens of thousands."}, {"heading": "2.3 Aligning historical embeddings", "text": "In order to compare word vectors from different time periods, we need to ensure that the vectors are aligned along the same coordinate axes. Of course, explicit PPMI vectors are aligned because each column simply corresponds to a context word. Low-dimensional embedding will not be naturally aligned due to the ambiguous nature of the 4The 2000s decade of Google data, as changes in the scanning method (Michel et al., 2011).SVD and the stochastic nature of SGNS. Specifically, these two methods can lead to arbitrary orthogonal transformations that do not affect any pair-wise cosmic similarities within years, but precede the comparison of the same word. 5We use orthogonal procrusts to align the learned low-dimensional embedding, defining W (t), Rd \u00d7 | V | as a matrix of word embedding that we use in the year Q > R (take into account) of SVW (4)."}, {"heading": "2.4 Time-series from historical embeddings", "text": "Diachronic word embedding can be used in two ways to quantify semantic changes: (i) we can measure changes in pairs of word similarities over time, or (ii) we can measure how the embedding of a single word shifts over time. We can quantify shifts by calculating the similarity time series (t) (wi, wj) = cos-sim (w (t) i, w (t) j) (5) between two words wi and wj over a period of time (t,..., t +). We then measure the temporal similarity time order (t) (wi, wj) = cos-sim (w (t) i, w (t) j)."}, {"heading": "3 Comparison of different approaches", "text": "We compare the different distributional approaches against a set of benchmarks to test their scientific usefulness. We evaluate both their synchronous accuracy (i.e. the ability to detect word similarities over time) and their diachronic validity (i.e. the ability to quantify semantic changes over time)."}, {"heading": "3.1 Synchronic Accuracy", "text": "We investigated the synchronous (within the period) accuracy of the methods using a modern standard scale and the 1990 part of the ENGALL data. In the MEN similarity task by Bruni et al. (2012) to compare human judgments on word similarities, SVD performed best (\u03c1 = 0.739), followed by PPMI (\u03c1 = 0.687) and SGNS (\u03c1 = 0.649). These results are consistent with the results of Levy et al. (2015), where SVD performed best on similarity tasks, while SGNS performed best on analogy tasks (which are not the focus of this work)."}, {"heading": "3.2 Diachronic Validity", "text": "We evaluate the diachronic validity of the methods on two historical semantic tasks. First, we tested whether the methods capture the known historical shifts in meaning. The goal of this task is to correctly capture the methods, whether pairs of words move closer or farther apart in semantic space during a predetermined period of time. We use a series of independently proven shifts as a basis for evaluation (Table 2). For comparison, we evaluated the methods on both large (but chaotic) ENGALLDEN data and the smaller (but clean) COHA data. For this task, all methods performed in terms of capturing the correct guidelines of shifts are evaluated."}, {"heading": "3.3 Methodological recommendations", "text": "PPMI is clearly inferior to the other two methods; it performs poorly on all benchmark tasks, is extremely sensitive to rare events, and tends to make false discoveries from global genre shifts. Overall, the results are somewhat ambiguous between SVD and SGNS, as both score best on two of the four tasks (synchronous accuracy, ENGALL detection, COHA detection, discovery). Overall, SVD performs best on the synchronous accuracy task, showing higher average accuracy on the \"detection task,\" while SGNS performs best on the \"detection task.\" These results suggest that both methods make sound decisions for studies of semantic changes, but that they each have their own trade-offs: SVD is more sensitive because it performs well even when using a small dataset, but this sensitivity also leads to false discoveries based on corpus artefacts, but is not robust enough on a corpus task."}, {"heading": "4 Statistical laws of semantic change", "text": "We will now show how diachronic embedding can be used in a large-scale linguistic analysis to uncover statistical laws that relate frequency and polysemy to semantic changes. In particular, we will analyze how the speed of semantic changes in a word, \u2206 (t) (wi) = cos-dist (w (t) i, w (t + 1) i) (6) depends on its frequency, f (t) (wi) and a measure of its polysemy, d (t) (wi) (defined in Section 4.4)."}, {"heading": "4.1 Setup", "text": "Using all four languages and all four conditions for English (ENGALL, ENGFIC and COHA with and without lemmatization), we performed regression analyses on the rates of semantic change (t) (wi); therefore, we examined one data point per word for each pair of consecutive decades and analyzed how the frequency and polysemy of a word in time t correlates with its degree of semantic shift over the next decade. To ensure the robustness of our results, we analyzed only the top 10,000 non-stop words by average historical frequency (lower words tend to have insufficient coevent data over the years) and we discarded correct nouns (changes in correct nouns usage), which are primarily driven by non-linguistic factors, such as historical events, Traugod and Dasher, 2001. We also logged the fixed semantic values we set on a fixed unit."}, {"heading": "4.2 Overview of results", "text": "We found that semantic rates of change across all languages obey a scaling relationship of the form \u2206 (wi) \u0394f (wi) \u03b2f \u00b7 d (wi) \u03b2d, (8) with \u03b2f < 0 and \u03b2d > 0. This finding implies that common words change more slowly, while polysemic words change more quickly, and that both scale these relationships as laws of power."}, {"heading": "4.3 Law of conformity: Frequently used words change at slower rates", "text": "Using the model in Equation (7), we found that the logarithm of the frequency of a word, log (f (wi)), has a significant and significant negative impact on the rates of semantic changes in all settings (Figures 2a and 3a). Given the use of log transformations in pre-processing the data, this implies that the rates of semantic changes are proportional to a negative potency (\u03b2f) of frequency, i.e. that the COHA datasets are outliers due to their relatively small sample size (Figure 3)."}, {"heading": "4.4 Law of innovation: Polysemous words change at faster rates", "text": "We quantify the polysemy of a word by measuring the local clustering coefficient of its co-occurence network (Watts and Strogatz, 1998) 9, i.e. (wi) = \u2212 \u2211 ci, cj, NPPMI (wi) I {PPMI (ci, cj) > 0} | NPPMI (wi) | (NPPMI (wi) | \u2212 1), where NPPMI (wi) = {wj: PPMI (wi, wj) > 0} According to this metric, a word has a high clustering coefficient (and thus a low polysemy score) when the words it coincides with each other. This metric and its relatives are often used in word-sense discrimination and correlate with, e.g., the number of senses in WordNet (Dorow and Witdows, 2003; Ferret, 2004)."}, {"heading": "5 Discussion", "text": "We show how distribution methods can reveal statistical laws of semantic change and perhaps provide a robust methodology for future work in this area. The two statistical laws we propose have a strong impact on future work in historical semantics. The law of conformity - frequent words change more slowly - clarifies the role of frequency in semantic changes. Future studies of semantic changes must take into account the conformity effect of frequency: when studying the interaction between some linguistic processes and semantic changes, the law of conformity should serve as a zero model in which the interaction is primarily driven by underlying frequency effects. The law of innovation - polymeric words change more quickly - quantifies the central role of polysemia in semantic change, a problem that linguists have been dealing with for more than 100 years (Bre'al, 1897). Previous work argued that semantic changes can lead to polysemantic changes (2003, Wilkins, 1993, and Hopper), but that polysemantic changes can actually explain the results."}, {"heading": "A Hyperparameter and pre-processing details", "text": "In fact, most of them are able to play by the rules that they need for their work, and they are able to play by the rules that they need for their work."}, {"heading": "C Regression analysis details", "text": "In addition to the pre-processing mentioned in the main text, we also normalized the contextual diversity values d (wi) within years by subtracting the annual median, which was necessary because there were significant changes in the median contextual diversity values over years due to changes in corpus sample sizes, etc. Data points that corresponded to words that occurred less than 1,000 times in a period were also discarded, as these points were not sufficient to calculate the change rates (this threshold, however, only took effect for the COHA data). We removed stopwords and correct nouns by removing all stopwords from the available lists in Python's NLTK package (Bird et al., 2009) and (ii) limited our analysis to words with a portion of the speech (POS) tags that corresponded to four main linguistic categories (ordinary nouns, adjectives, adjectives, and constants)."}], "references": [{"title": "Contextual diversity, not word frequency, determines word-naming and lexical decision times", "author": ["James S. Adelman", "Gordon D.A. Brown", "Jos\u00e9 F. Quesada."], "venue": "Psychological Science, 17(9):814\u2013823.", "citeRegEx": "Adelman et al\\.,? 2006", "shortCiteRegEx": "Adelman et al\\.", "year": 2006}, {"title": "Natural language processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media, Inc.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Why do new meanings occur? A cognitive typology of the motivations for lexical semantic change", "author": ["Andreas Blank."], "venue": "Peter Koch and Andreas Blank, editors, Historical Semantics and Cognition. Walter de Gruyter, Berlin, Germany.", "citeRegEx": "Blank.,? 1999", "shortCiteRegEx": "Blank.", "year": 1999}, {"title": "Culture and the Evolutionary Process", "author": ["Robert Boyd", "Peter J Richerson."], "venue": "University of Chicago Press, Chicago, IL.", "citeRegEx": "Boyd and Richerson.,? 1988", "shortCiteRegEx": "Boyd and Richerson.", "year": 1988}, {"title": "Distributional semantics in technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "NamKhanh Tran."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 136\u2013145. Asso-", "citeRegEx": "Bruni et al\\.,? 2012", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Essai de S\u00e9mantique: Science des significations", "author": ["Michel Br\u00e9al."], "venue": "Hachette, Paris, France.", "citeRegEx": "Br\u00e9al.,? 1897", "shortCiteRegEx": "Br\u00e9al.", "year": 1897}, {"title": "Extracting semantic representations from word cooccurrence statistics: A computational study", "author": ["John A. Bullinaria", "Joseph P. Levy."], "venue": "Behav. Res. Methods, 39(3):510\u2013526.", "citeRegEx": "Bullinaria and Levy.,? 2007", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2007}, {"title": "Extracting semantic representations from word cooccurrence statistics: stop-lists, stemming, and SVD", "author": ["John A. Bullinaria", "Joseph P. Levy."], "venue": "Behavior Research Methods, 44(3):890\u2013907, September.", "citeRegEx": "Bullinaria and Levy.,? 2012", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2012}, {"title": "Frequency of Use And the Organization of Language", "author": ["J.L. Bybee."], "venue": "Oxford University Press, New York City, NY.", "citeRegEx": "Bybee.,? 2007", "shortCiteRegEx": "Bybee.", "year": 2007}, {"title": "The development of polysemy and frequency use in english second language speakers", "author": ["Scott Crossley", "Tom Salsbury", "Danielle McNamara."], "venue": "Language Learning, 60(3):573\u2013605.", "citeRegEx": "Crossley et al\\.,? 2010", "shortCiteRegEx": "Crossley et al\\.", "year": 2010}, {"title": "The Corpus of Historical American English: 400 million words, 1810-2009", "author": ["Mark Davies"], "venue": null, "citeRegEx": "Davies.,? \\Q2010\\E", "shortCiteRegEx": "Davies.", "year": 2010}, {"title": "Discovering corpus-specific word senses", "author": ["Beate Dorow", "Dominic Widdows."], "venue": "Proc. 10th EACL Conf., pages 79\u201382. Association for Computational Linguistics.", "citeRegEx": "Dorow and Widdows.,? 2003", "shortCiteRegEx": "Dorow and Widdows.", "year": 2003}, {"title": "Discovering word senses from a network of lexical cooccurrences", "author": ["Olivier Ferret."], "venue": "Proc. CoLing 2004, page 1326. Association for Computational Linguistics.", "citeRegEx": "Ferret.,? 2004", "shortCiteRegEx": "Ferret.", "year": 2004}, {"title": "A Synopsis of Linguistic Theory, 1930-1955", "author": ["J.R. Firth."], "venue": "Studies in Linguistic Analysis. Special volume of the Philological Society. Basil Blackwell, Oxford, UK.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Diachronic Prototype Semantics: A Contribution to Historical Lexicology", "author": ["Dirk Geeraerts."], "venue": "Clarendon Press, Oxford, UK.", "citeRegEx": "Geeraerts.,? 1997", "shortCiteRegEx": "Geeraerts.", "year": 1997}, {"title": "Confronting multicollinearity in ecological multiple regression", "author": ["Michael H Graham."], "venue": "Ecology, 84(11):2809\u20132815.", "citeRegEx": "Graham.,? 2003", "shortCiteRegEx": "Graham.", "year": 2003}, {"title": "A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus", "author": ["Kristina Gulordava", "Marco Baroni."], "venue": "Proc. GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, pages", "citeRegEx": "Gulordava and Baroni.,? 2011", "shortCiteRegEx": "Gulordava and Baroni.", "year": 2011}, {"title": "Distributional structure", "author": ["Zellig S. Harris."], "venue": "Word, 10:146\u2013162.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Grammaticalization", "author": ["Paul J Hopper", "Elizabeth Closs Traugott."], "venue": "Cambridge University Press, Cambridge, UK.", "citeRegEx": "Hopper and Traugott.,? 2003", "shortCiteRegEx": "Hopper and Traugott.", "year": 2003}, {"title": "A framework for analyzing semantic change of words across time", "author": ["Adam Jatowt", "Kevin Duh."], "venue": "Proc. 14th ACM/IEEE-CS Conf. on Digital Libraries, pages 229\u2013238. IEEE Press.", "citeRegEx": "Jatowt and Duh.,? 2014", "shortCiteRegEx": "Jatowt and Duh.", "year": 2014}, {"title": "Principles and Methods for Historical Linguistics", "author": ["R. Jeffers", "Ilse Lehiste."], "venue": "MIT Press, Cambridge, MA.", "citeRegEx": "Jeffers and Lehiste.,? 1979", "shortCiteRegEx": "Jeffers and Lehiste.", "year": 1979}, {"title": "How dominant is the commonest sense of a word? In Text, Speech and Dialogue, pages 103\u2013111", "author": ["Adam Kilgarriff."], "venue": "Springer.", "citeRegEx": "Kilgarriff.,? 2004", "shortCiteRegEx": "Kilgarriff.", "year": 2004}, {"title": "Temporal analysis of language through neural language models", "author": ["Yoon Kim", "Yi-I. Chiu", "Kentaro Hanaki", "Darshan Hegde", "Slav Petrov."], "venue": "arXiv preprint arXiv:1405.3515.", "citeRegEx": "Kim et al\\.,? 2014", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Statistically significant detection of linguistic change", "author": ["Vivek Kulkarni", "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proc. 24th WWW Conf., pages 625\u2013635. International World Wide Web Conferences Steering Committee.", "citeRegEx": "Kulkarni et al\\.,? 2014", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2014}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K. Landauer", "Susan T. Dumais."], "venue": "Psychol. Rev., 104(2):211.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Trans. Assoc. Comput. Ling., 3.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Nonparametric econometrics: theory and practice", "author": ["Qi Li", "Jeffrey Scott Racine."], "venue": "Princeton University Press, Princeton, NJ.", "citeRegEx": "Li and Racine.,? 2007", "shortCiteRegEx": "Li and Racine.", "year": 2007}, {"title": "Quantifying the evolutionary dynamics of language", "author": ["Erez Lieberman", "Jean-Baptiste Michel", "Joe Jackson", "Tina Tang", "Martin A. Nowak."], "venue": "Nature, 449(7163):713\u2013716.", "citeRegEx": "Lieberman et al\\.,? 2007", "shortCiteRegEx": "Lieberman et al\\.", "year": 2007}, {"title": "Syntactic annotations for the google books ngram corpus", "author": ["Yuri Lin", "Jean-Baptiste Michel", "Erez Lieberman Aiden", "Jon Orwant", "Will Brockman", "Slav Petrov."], "venue": "Proceedings of the ACL 2012 system demonstrations, pages 169\u2013174. Association for", "citeRegEx": "Lin et al\\.,? 2012", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Generalized linear mixed models", "author": ["Charles E McCulloch", "John M Neuhaus."], "venue": "WileyInterscience, Hoboken, NJ.", "citeRegEx": "McCulloch and Neuhaus.,? 2001", "shortCiteRegEx": "McCulloch and Neuhaus.", "year": 2001}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A general and simple method for obtaining R 2 from generalized linear mixed-effects models", "author": ["Shinichi Nakagawa", "Holger Schielzeth."], "venue": "Methods Ecol. Evol., 4(2):133\u2013142.", "citeRegEx": "Nakagawa and Schielzeth.,? 2013", "shortCiteRegEx": "Nakagawa and Schielzeth.", "year": 2013}, {"title": "Frequency of word-use predicts rates of lexical evolution throughout Indo-European history", "author": ["Mark Pagel", "Quentin D. Atkinson", "Andrew Meade."], "venue": "Nature, 449(7163):717\u2013720.", "citeRegEx": "Pagel et al\\.,? 2007", "shortCiteRegEx": "Pagel et al\\.", "year": 2007}, {"title": "Characterizing the Google Books corpus: Strong limits to inferences of socio-cultural and linguistic evolution", "author": ["Eitan Adam Pechenick", "Christopher M. Danforth", "Peter Sheridan Dodds."], "venue": "PLoS ONE, 10(10).", "citeRegEx": "Pechenick et al\\.,? 2015", "shortCiteRegEx": "Pechenick et al\\.", "year": 2015}, {"title": "Words as alleles: connecting language evolution with Bayesian learners to models of genetic drift", "author": ["F. Reali", "T.L. Griffiths."], "venue": "Proc. R. Soc. B, 277(1680):429\u2013436.", "citeRegEx": "Reali and Griffiths.,? 2010", "shortCiteRegEx": "Reali and Griffiths.", "year": 2010}, {"title": "Tracing semantic change with latent semantic analysis", "author": ["Eyal Sagi", "Stefan Kaufmann", "Brady Clark."], "venue": "Kathryn Allan and Justyna A. Robinson, editors, Current Methods in Historical Semantics, page 161. De Gruyter Mouton, Berlin, Germany.", "citeRegEx": "Sagi et al\\.,? 2011", "shortCiteRegEx": "Sagi et al\\.", "year": 2011}, {"title": "The Lefff 2 syntactic lexicon for French: architecture, acquisition, use", "author": ["Beno\u02c6it Sagot", "Lionel Cl\u00e9ment", "Eric de La Clergerie", "Pierre Boullier."], "venue": "LREC 06, pages 1\u20134.", "citeRegEx": "Sagot et al\\.,? 2006", "shortCiteRegEx": "Sagot et al\\.", "year": 2006}, {"title": "Adding manual constraints and lexical look-up to a Brilltagger for German", "author": ["Gerold Schneider", "Martin Volk."], "venue": "Proceedings of the ESSLLI98 Workshop on Recent Advances in Corpus Annotation, Saarbr\u00fccken.", "citeRegEx": "Schneider and Volk.,? 1998", "shortCiteRegEx": "Schneider and Volk.", "year": 1998}, {"title": "A generalized solution of the orthogonal Procrustes problem", "author": ["Peter H Sch\u00f6nemann."], "venue": "Psychometrika, 31(1):1\u201310.", "citeRegEx": "Sch\u00f6nemann.,? 1966", "shortCiteRegEx": "Sch\u00f6nemann.", "year": 1966}, {"title": "Statsmodels: Econometric and statistical modeling with python", "author": ["J.S. Seabold", "J. Perktold."], "venue": "Proceedings of the 9th Python in Science Conference.", "citeRegEx": "Seabold and Perktold.,? 2010", "shortCiteRegEx": "Seabold and Perktold.", "year": 2010}, {"title": "Regularity in Semantic Change", "author": ["Elizabeth Closs Traugott", "Richard B Dasher."], "venue": "Cambridge University Press, Cambridge, UK.", "citeRegEx": "Traugott and Dasher.,? 2001", "shortCiteRegEx": "Traugott and Dasher.", "year": 2001}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "J. Artif. Intell. Res., 37(1):141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Semantics: An Introduction to the Science of Meaning", "author": ["S. Ullmann."], "venue": "Barnes & Noble, New York City, NY.", "citeRegEx": "Ullmann.,? 1962", "shortCiteRegEx": "Ullmann.", "year": 1962}, {"title": "Visualizing data using t-SNE", "author": ["Laurens Van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9(2579-2605):85.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Collective dynamics of \u2018small-world\u2019networks", "author": ["Duncan J Watts", "Steven H Strogatz."], "venue": "Nature, 393(6684):440\u2013442.", "citeRegEx": "Watts and Strogatz.,? 1998", "shortCiteRegEx": "Watts and Strogatz.", "year": 1998}, {"title": "Understanding semantic change of words over centuries", "author": ["Derry Tanti Wijaya", "Reyyan Yeniterzi."], "venue": "Proceedings of the 2011 international workshop on DETecting and Exploiting Cultural diversiTy on the social web, pages 35\u201340. ACM.", "citeRegEx": "Wijaya and Yeniterzi.,? 2011", "shortCiteRegEx": "Wijaya and Yeniterzi.", "year": 2011}, {"title": "From part to person: Natural tendencies of semantic change and the search for cognates", "author": ["David P Wilkins."], "venue": "Cognitive Anthropology Research Group at the Max Planck Institute for Psycholinguistics.", "citeRegEx": "Wilkins.,? 1993", "shortCiteRegEx": "Wilkins.", "year": 1993}, {"title": "Cognitive Factors Motivating The Evolution Of Word Meanings: Evidence From Corpora, Behavioral Data And Encyclopedic Network Structure", "author": ["B. Winter", "Graham Thompson", "Matthias Urban."], "venue": "Evolution of Language: Proceedings of", "citeRegEx": "Winter et al\\.,? 2014", "shortCiteRegEx": "Winter et al\\.", "year": 2014}, {"title": "A computational evaluation of two laws of semantic change", "author": ["Yang Xu", "Charles Kemp."], "venue": "Proc. 37th Annu. Conf. Cogn. Sci. Soc.", "citeRegEx": "Xu and Kemp.,? 2015", "shortCiteRegEx": "Xu and Kemp.", "year": 2015}, {"title": "Historical Semantic Chaining and Efficient Communication: The Case of Container Names", "author": ["Yang Xu", "Terry Regier", "Barbara C. Malt."], "venue": "Cogn. Sci.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus", "author": ["Naiwen Xue", "Fei Xia", "Fu-Dong Chiou", "Marta Palmer."], "venue": "Natural language engineering, 11(02):207\u2013238.", "citeRegEx": "Xue et al\\.,? 2005", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "The meaning-frequency relationship of words", "author": ["George Kingsley Zipf."], "venue": "The Journal of General Psychology, 33(2):251\u2013256.", "citeRegEx": "Zipf.,? 1945", "shortCiteRegEx": "Zipf.", "year": 1945}, {"title": "For the context word distributions in all methods, we used context distribution smoothing with a smoothing parameter of 0.75", "author": ["Levy"], "venue": null, "citeRegEx": "Levy,? \\Q2015\\E", "shortCiteRegEx": "Levy", "year": 2015}, {"title": "Furthermore, to improve the computational efficiency of SGNS (which works with text streams and not co-occurrence counts)", "author": ["Levy"], "venue": "Mikolov et al", "citeRegEx": "Levy,? \\Q2013\\E", "shortCiteRegEx": "Levy", "year": 2013}, {"title": "The implementations for PPMI and SVD are released with the code package", "author": ["Levy"], "venue": "For SGNS,", "citeRegEx": "Levy,? \\Q2015\\E", "shortCiteRegEx": "Levy", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Shifts in word meaning exhibit systematic regularities (Br\u00e9al, 1897; Ullmann, 1962).", "startOffset": 55, "endOffset": 83}, {"referenceID": 42, "context": "Shifts in word meaning exhibit systematic regularities (Br\u00e9al, 1897; Ullmann, 1962).", "startOffset": 55, "endOffset": 83}, {"referenceID": 2, "context": "The rate of semantic change, for example, is higher in some words than others (Blank, 1999) \u2014 compare the stable semantic history of cat (from ProtoGermanic kattuz, \u201ccat\u201d) to the varied meanings of English cast: \u201cto mould\u201d, \u201ca collection of actors\u2019, \u201ca hardened bandage\u201d, etc.", "startOffset": 78, "endOffset": 91}, {"referenceID": 2, "context": "Various hypotheses have been offered about such regularities in semantic change, such as an increasing subjectification of meaning, or the grammaticalization of inferences (e.g., Geeraerts, 1997; Blank, 1999; Traugott and Dasher, 2001).", "startOffset": 172, "endOffset": 235}, {"referenceID": 40, "context": "Various hypotheses have been offered about such regularities in semantic change, such as an increasing subjectification of meaning, or the grammaticalization of inferences (e.g., Geeraerts, 1997; Blank, 1999; Traugott and Dasher, 2001).", "startOffset": 172, "endOffset": 235}, {"referenceID": 8, "context": "Frequency plays a key role in other linguistic changes, associated sometimes with faster change\u2014sound changes like lenition occur in more frequent words\u2014and sometimes with slower change\u2014high frequency words are more resistant to morphological regularization (Bybee, 2007; Pagel et al., 2007; Lieberman et al., 2007).", "startOffset": 258, "endOffset": 315}, {"referenceID": 32, "context": "Frequency plays a key role in other linguistic changes, associated sometimes with faster change\u2014sound changes like lenition occur in more frequent words\u2014and sometimes with slower change\u2014high frequency words are more resistant to morphological regularization (Bybee, 2007; Pagel et al., 2007; Lieberman et al., 2007).", "startOffset": 258, "endOffset": 315}, {"referenceID": 27, "context": "Frequency plays a key role in other linguistic changes, associated sometimes with faster change\u2014sound changes like lenition occur in more frequent words\u2014and sometimes with slower change\u2014high frequency words are more resistant to morphological regularization (Bybee, 2007; Pagel et al., 2007; Lieberman et al., 2007).", "startOffset": 258, "endOffset": 315}, {"referenceID": 5, "context": "Words gain senses over time as they semantically drift (Br\u00e9al, 1897; Wilkins, 1993; Hopper and Traugott, 2003), and polysemous words1 occur in more diverse contexts, affecting lexical access speed (Adelman et al.", "startOffset": 55, "endOffset": 110}, {"referenceID": 46, "context": "Words gain senses over time as they semantically drift (Br\u00e9al, 1897; Wilkins, 1993; Hopper and Traugott, 2003), and polysemous words1 occur in more diverse contexts, affecting lexical access speed (Adelman et al.", "startOffset": 55, "endOffset": 110}, {"referenceID": 18, "context": "Words gain senses over time as they semantically drift (Br\u00e9al, 1897; Wilkins, 1993; Hopper and Traugott, 2003), and polysemous words1 occur in more diverse contexts, affecting lexical access speed (Adelman et al.", "startOffset": 55, "endOffset": 110}, {"referenceID": 0, "context": "Words gain senses over time as they semantically drift (Br\u00e9al, 1897; Wilkins, 1993; Hopper and Traugott, 2003), and polysemous words1 occur in more diverse contexts, affecting lexical access speed (Adelman et al., 2006) and rates of L2 learning (Crossley et al.", "startOffset": 197, "endOffset": 219}, {"referenceID": 9, "context": ", 2006) and rates of L2 learning (Crossley et al., 2010).", "startOffset": 33, "endOffset": 56}, {"referenceID": 14, "context": "But we don\u2019t know whether the diverse contextual use of polysemous words makes them more or less likely to undergo change (Geeraerts, 1997; Winter et al., 2014; Xu et al., 2015).", "startOffset": 122, "endOffset": 177}, {"referenceID": 47, "context": "But we don\u2019t know whether the diverse contextual use of polysemous words makes them more or less likely to undergo change (Geeraerts, 1997; Winter et al., 2014; Xu et al., 2015).", "startOffset": 122, "endOffset": 177}, {"referenceID": 49, "context": "But we don\u2019t know whether the diverse contextual use of polysemous words makes them more or less likely to undergo change (Geeraerts, 1997; Winter et al., 2014; Xu et al., 2015).", "startOffset": 122, "endOffset": 177}, {"referenceID": 51, "context": "Furthermore, polysemy is strongly correlated with frequency\u2014high frequency words have more senses (Zipf, 1945; \u0130lgen and Karaoglan, 2007)\u2014so understanding how polysemy relates to semantic change requires controling for word frequency.", "startOffset": 98, "endOffset": 137}, {"referenceID": 5, "context": "Answering these questions requires new methods that can go beyond the case-studies of a few words (often followed over widely different timeperiods) that are our most common diachronic data (Br\u00e9al, 1897; Ullmann, 1962; Blank, 1999; Hopper and Traugott, 2003; Traugott and Dasher, 2001).", "startOffset": 190, "endOffset": 285}, {"referenceID": 42, "context": "Answering these questions requires new methods that can go beyond the case-studies of a few words (often followed over widely different timeperiods) that are our most common diachronic data (Br\u00e9al, 1897; Ullmann, 1962; Blank, 1999; Hopper and Traugott, 2003; Traugott and Dasher, 2001).", "startOffset": 190, "endOffset": 285}, {"referenceID": 2, "context": "Answering these questions requires new methods that can go beyond the case-studies of a few words (often followed over widely different timeperiods) that are our most common diachronic data (Br\u00e9al, 1897; Ullmann, 1962; Blank, 1999; Hopper and Traugott, 2003; Traugott and Dasher, 2001).", "startOffset": 190, "endOffset": 285}, {"referenceID": 18, "context": "Answering these questions requires new methods that can go beyond the case-studies of a few words (often followed over widely different timeperiods) that are our most common diachronic data (Br\u00e9al, 1897; Ullmann, 1962; Blank, 1999; Hopper and Traugott, 2003; Traugott and Dasher, 2001).", "startOffset": 190, "endOffset": 285}, {"referenceID": 40, "context": "Answering these questions requires new methods that can go beyond the case-studies of a few words (often followed over widely different timeperiods) that are our most common diachronic data (Br\u00e9al, 1897; Ullmann, 1962; Blank, 1999; Hopper and Traugott, 2003; Traugott and Dasher, 2001).", "startOffset": 190, "endOffset": 285}, {"referenceID": 6, "context": "One promising avenue is the use of distributional semantics, in which words are embedded in vector spaces according to their co-occurrence relationships (Bullinaria and Levy, 2007; Turney and Pantel, 2010), and the embeddings of words", "startOffset": 153, "endOffset": 205}, {"referenceID": 41, "context": "One promising avenue is the use of distributional semantics, in which words are embedded in vector spaces according to their co-occurrence relationships (Bullinaria and Levy, 2007; Turney and Pantel, 2010), and the embeddings of words", "startOffset": 153, "endOffset": 205}, {"referenceID": 35, "context": "This new direction has been effectively demonstrated in a number of case-studies (Sagi et al., 2011; Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014) and used to perform largescale linguistic change-point detection (Kulkarni et al.", "startOffset": 81, "endOffset": 178}, {"referenceID": 45, "context": "This new direction has been effectively demonstrated in a number of case-studies (Sagi et al., 2011; Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014) and used to perform largescale linguistic change-point detection (Kulkarni et al.", "startOffset": 81, "endOffset": 178}, {"referenceID": 16, "context": "This new direction has been effectively demonstrated in a number of case-studies (Sagi et al., 2011; Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014) and used to perform largescale linguistic change-point detection (Kulkarni et al.", "startOffset": 81, "endOffset": 178}, {"referenceID": 19, "context": "This new direction has been effectively demonstrated in a number of case-studies (Sagi et al., 2011; Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014) and used to perform largescale linguistic change-point detection (Kulkarni et al.", "startOffset": 81, "endOffset": 178}, {"referenceID": 23, "context": ", 2011; Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014) and used to perform largescale linguistic change-point detection (Kulkarni et al., 2014) as well as to test a few specific hypotheses, such as whether English synonyms tend to change meaning in similar ways (Xu and Kemp, 2015).", "startOffset": 151, "endOffset": 174}, {"referenceID": 48, "context": ", 2014) as well as to test a few specific hypotheses, such as whether English synonyms tend to change meaning in similar ways (Xu and Kemp, 2015).", "startOffset": 126, "endOffset": 145}, {"referenceID": 17, "context": "These methods operationalize the \u2018distributional hypothesis\u2019 that word semantics are implicit in co-occurrence relationships (Harris, 1954; Firth, 1957).", "startOffset": 125, "endOffset": 152}, {"referenceID": 13, "context": "These methods operationalize the \u2018distributional hypothesis\u2019 that word semantics are implicit in co-occurrence relationships (Harris, 1954; Firth, 1957).", "startOffset": 125, "endOffset": 152}, {"referenceID": 41, "context": "The semantic similarity/distance between two words is approximated by the cosine similarity/distance between their vectors (Turney and Pantel, 2010).", "startOffset": 123, "endOffset": 148}, {"referenceID": 25, "context": "(1) where cj \u2208 VC is a context word and \u03b1 > 0 is a negative prior, which provides a smoothing bias (Levy et al., 2015).", "startOffset": 99, "endOffset": 118}, {"referenceID": 6, "context": "Clipping the PPMI values above zero ensures they remain finite and has been shown to dramatically improve results (Bullinaria and Levy, 2007; Levy et al., 2015); intuitively, this clipping ensures that the representations emphasize positive word-word correlations over negative ones.", "startOffset": 114, "endOffset": 160}, {"referenceID": 25, "context": "Clipping the PPMI values above zero ensures they remain finite and has been shown to dramatically improve results (Bullinaria and Levy, 2007; Levy et al., 2015); intuitively, this clipping ensures that the representations emphasize positive word-word correlations over negative ones.", "startOffset": 114, "endOffset": 160}, {"referenceID": 25, "context": "Synchronic applications of these three methods are reviewed in detail in Levy et al. (2015).", "startOffset": 73, "endOffset": 92}, {"referenceID": 10, "context": "5\u00d7 10 1800-1999 (Davies, 2010) ENGFIC English Fiction from Google books 7.", "startOffset": 16, "endOffset": 30}, {"referenceID": 10, "context": "5\u00d7 10 1800-1999 (Davies, 2010) COHA English Genre-balanced sample 4.", "startOffset": 16, "endOffset": 30}, {"referenceID": 10, "context": "1\u00d7 10 1810-2009 (Davies, 2010) FREALL French Google books (all genres) 1.", "startOffset": 16, "endOffset": 30}, {"referenceID": 36, "context": "9\u00d7 10 1800-1999 (Sagot et al., 2006) GERALL German Google books (all genres) 4.", "startOffset": 16, "endOffset": 36}, {"referenceID": 37, "context": "3\u00d7 10 1800-1999 (Schneider and Volk, 1998) CHIALL Chinese Google books (all genres) 6.", "startOffset": 16, "endOffset": 42}, {"referenceID": 50, "context": "0\u00d7 10 1950-1999 (Xue et al., 2005)", "startOffset": 16, "endOffset": 34}, {"referenceID": 25, "context": "SVD embeddings correspond to low-dimensional approximations of the PPMI embeddings learned via singular value decomposition (Levy et al., 2015).", "startOffset": 124, "endOffset": 143}, {"referenceID": 41, "context": "Setting \u03b3 < 1 has been shown to dramatically improve embedding qualities (Turney and Pantel, 2010; Bullinaria and Levy, 2012).", "startOffset": 73, "endOffset": 125}, {"referenceID": 7, "context": "Setting \u03b3 < 1 has been shown to dramatically improve embedding qualities (Turney and Pantel, 2010; Bullinaria and Levy, 2012).", "startOffset": 73, "endOffset": 125}, {"referenceID": 24, "context": "This SVD approach can be viewed as a generalization of Latent Semantic Analysis (Landauer and Dumais, 1997), where the term-document matrix is replaced with MPPMI.", "startOffset": 80, "endOffset": 107}, {"referenceID": 30, "context": "SGNS \u2018neural\u2019 embeddings are optimized to predict co-occurrence relationships using an approximate objective known as \u2018skip-gram with negative sampling\u2019 (Mikolov et al., 2013).", "startOffset": 153, "endOffset": 175}, {"referenceID": 22, "context": "SGNS has the benefit of allowing incremental initialization during learning, where the embeddings for time t are initialized with the embeddings from time t \u2212 \u2206 (Kim et al., 2014).", "startOffset": 161, "endOffset": 179}, {"referenceID": 28, "context": "We trained models on the 6 datasets described in Table 1, taken from Google N-Grams (Lin et al., 2012) and the COHA corpus (Davies, 2010).", "startOffset": 84, "endOffset": 102}, {"referenceID": 10, "context": ", 2012) and the COHA corpus (Davies, 2010).", "startOffset": 28, "endOffset": 42}, {"referenceID": 33, "context": ", to shifting sampling biases over time (Pechenick et al., 2015).", "startOffset": 40, "endOffset": 64}, {"referenceID": 25, "context": "We follow the recommendations of Levy et al. (2015) in setting the hyperparameters for the embedding methods, though preliminary experiments were used to tune key settings.", "startOffset": 33, "endOffset": 52}, {"referenceID": 38, "context": "The solution corresponds to the best rotational alignment and can be obtained efficiently using an application of SVD (Sch\u00f6nemann, 1966).", "startOffset": 118, "endOffset": 136}, {"referenceID": 19, "context": "Previous work circumvented this problem by either avoiding low-dimensional embeddings (e.g., Gulordava and Baroni, 2011; Jatowt and Duh, 2014) or by performing heuristic local alignments per word (Kulkarni et al.", "startOffset": 86, "endOffset": 142}, {"referenceID": 23, "context": ", Gulordava and Baroni, 2011; Jatowt and Duh, 2014) or by performing heuristic local alignments per word (Kulkarni et al., 2014).", "startOffset": 105, "endOffset": 128}, {"referenceID": 23, "context": "mean shifts (Kulkarni et al., 2014) could also be used.", "startOffset": 12, "endOffset": 35}, {"referenceID": 4, "context": "On Bruni et al. (2012)\u2019s MEN similarity task of matching human judgments of word similarities, SVD performed best (\u03c1 = 0.", "startOffset": 3, "endOffset": 23}, {"referenceID": 4, "context": "On Bruni et al. (2012)\u2019s MEN similarity task of matching human judgments of word similarities, SVD performed best (\u03c1 = 0.739), followed by PPMI (\u03c1 = 0.687) and SGNS (\u03c1 = 0.649). These results echo the findings of Levy et al. (2015), where they found SVD to perform best on similarity tasks while SGNS performed best on analogy tasks (which are not the focus of this work).", "startOffset": 3, "endOffset": 232}, {"referenceID": 23, "context": "gay homosexual, lesbian happy, showy ca 1920 (Kulkarni et al., 2014) fatal illness, lethal fate, inevitable <1800 (Jatowt and Duh, 2014) awful disgusting, mess impressive, majestic <1800 (Simpson et al.", "startOffset": 45, "endOffset": 68}, {"referenceID": 19, "context": ", 2014) fatal illness, lethal fate, inevitable <1800 (Jatowt and Duh, 2014) awful disgusting, mess impressive, majestic <1800 (Simpson et al.", "startOffset": 53, "endOffset": 75}, {"referenceID": 45, "context": ", 1989) nice pleasant, lovely refined, dainty ca 1900 (Wijaya and Yeniterzi, 2011) broadcast transmit, radio scatter, seed ca 1920 (Jeffers and Lehiste, 1979) monitor display, screen \u2014 ca 1930 (Simpson et al.", "startOffset": 54, "endOffset": 82}, {"referenceID": 20, "context": ", 1989) nice pleasant, lovely refined, dainty ca 1900 (Wijaya and Yeniterzi, 2011) broadcast transmit, radio scatter, seed ca 1920 (Jeffers and Lehiste, 1979) monitor display, screen \u2014 ca 1930 (Simpson et al.", "startOffset": 131, "endOffset": 158}, {"referenceID": 23, "context": ", 1989) record tape, album \u2014 ca 1920 (Kulkarni et al., 2014) guy fellow, man \u2014 ca 1850 (Wijaya and Yeniterzi, 2011) call phone, message \u2014 ca 1890 (Simpson et al.", "startOffset": 37, "endOffset": 60}, {"referenceID": 45, "context": ", 2014) guy fellow, man \u2014 ca 1850 (Wijaya and Yeniterzi, 2011) call phone, message \u2014 ca 1890 (Simpson et al.", "startOffset": 34, "endOffset": 62}, {"referenceID": 26, "context": "The trendlines show 95% CIs from bootstrapped kernel regressions on the ENGALL data (Li and Racine, 2007).", "startOffset": 84, "endOffset": 105}, {"referenceID": 44, "context": "We quantify the polysemy of a word by measuring the local clustering coefficient of its co-occurrence network (Watts and Strogatz, 1998)9, i.", "startOffset": 110, "endOffset": 136}, {"referenceID": 11, "context": ", number of senses in WordNet (Dorow and Widdows, 2003; Ferret, 2004); however, it does have a slight bias towards rating contextually-diverse discourse function words (e.", "startOffset": 30, "endOffset": 69}, {"referenceID": 12, "context": ", number of senses in WordNet (Dorow and Widdows, 2003; Ferret, 2004); however, it does have a slight bias towards rating contextually-diverse discourse function words (e.", "startOffset": 30, "endOffset": 69}, {"referenceID": 5, "context": "The law of innovation\u2014polysemous words change more quickly\u2014quantifies the central role polysemy plays in semantic change, an issue that has concerned linguists for more than 100 years (Br\u00e9al, 1897).", "startOffset": 184, "endOffset": 197}, {"referenceID": 46, "context": "Previous works argued that semantic change leads to polysemy (Wilkins, 1993; Hopper and Traugott, 2003), but our results suggest that polysemy may actually play a causal role in inducing semantic change.", "startOffset": 61, "endOffset": 103}, {"referenceID": 18, "context": "Previous works argued that semantic change leads to polysemy (Wilkins, 1993; Hopper and Traugott, 2003), but our results suggest that polysemy may actually play a causal role in inducing semantic change.", "startOffset": 61, "endOffset": 103}, {"referenceID": 34, "context": "The law of conformity might be a consequence of learning: perhaps people are more likely to use rare words mistakenly in novel ways, a mechanism formalizable by Bayesian models of word learning and corresponding to the biological notion of genetic drift (Reali and Griffiths, 2010).", "startOffset": 254, "endOffset": 281}, {"referenceID": 3, "context": "Or perhaps a sociocultural conformity bias makes people less likely to accept novel innovations of common words, a mechanism analogous to the biological process of purifying selection (Boyd and Richerson, 1988; Pagel et al., 2007).", "startOffset": 184, "endOffset": 230}, {"referenceID": 32, "context": "Or perhaps a sociocultural conformity bias makes people less likely to accept novel innovations of common words, a mechanism analogous to the biological process of purifying selection (Boyd and Richerson, 1988; Pagel et al., 2007).", "startOffset": 184, "endOffset": 230}, {"referenceID": 21, "context": "Highly polysemous words tend to have more rare senses (Kilgarriff, 2004), and rare senses may be unstable by the law of conformity.", "startOffset": 54, "endOffset": 72}, {"referenceID": 31, "context": "Marginal R (Nakagawa and Schielzeth, 2013).", "startOffset": 11, "endOffset": 42}], "year": 2017, "abstractText": "Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity\u2014the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation\u2014independent of frequency, words that are more polysemous have higher rates of semantic change.", "creator": "TeX"}}}