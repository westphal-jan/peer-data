{"id": "1605.09128", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Control of Memory, Active Perception, and Action in Minecraft", "abstract": "In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.", "histories": [["v1", "Mon, 30 May 2016 07:40:13 GMT  (7733kb,D)", "http://arxiv.org/abs/1605.09128v1", "ICML 2016"]], "COMMENTS": "ICML 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CV cs.LG", "authors": ["junhyuk oh", "valliappa chockalingam", "satinder p singh", "honglak lee"], "accepted": true, "id": "1605.09128"}, "pdf": {"name": "1605.09128.pdf", "metadata": {"source": "META", "title": "Control of Memory, Active Perception, and Action in Minecraft", "authors": ["Junhyuk Oh", "Valliappa Chockalingam", "Satinder Singh", "Honglak Lee"], "emails": ["JUNHYUK@UMICH.EDU", "VALLI@UMICH.EDU", "BAVEJA@UMICH.EDU", "HONGLAK@UMICH.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Related Work", "text": "Recent years have shown that you are able to learn algorithms such as copy and reverse."}, {"heading": "3. Background: Deep Q-Learning", "text": "In the DQN framework, each transition Tt = (st, st + 1, at, rt) is stored in a replay memory. For (each) iteration i, the deep neural network (with parameters \u03b8) is trained to approximate the action value function of transitions {(s, s, a, r)} by minimizing the loss functions Li (\u03b8i) as follows: Li (\u03b8) = Es, a [(yi \u2212 Q (s, a; \u03b8) 2] \u03b8Li (\u03b8) = Es, a \u0432 [(yi \u2212 Q (s, a; \u03b8) Es, a \u0441\u043e\u0441\u0442\u0432 [(yi \u2212 Q (s, a; \u0432). This is the target Q value estimated by a target Q network."}, {"heading": "4. Architectures", "text": "How important it is to retrieve a previous observation from memory depends on the current context. In the labyrinth of Figure 1, for example, where the color of the indicator block determines the desired target color, the indicator information is only important if the agent sees a potential target and must decide whether to approach it or find another target. Motivated by the absence of \"context-dependent memory retrieval\" in existing DRL architectures, we present three new memory-based architectures (Figure 3c-e) in this section. Our proposed architectures consist of revolutionary networks to extract high-level features from images (Figure 4.1), a memory that preserves a recent observation history (Figure 4.2), and a context vector that is used both to retrieve the memory and (in part) to evaluate the action value (Figure 4.3). Depending on how the context vector is constructed, we get three new network architectures (MQ), MQ (RecurQ), and FRN (FRN)."}, {"heading": "4.1. Encoding", "text": "For each time step, a raw observation (pixel) is encoded to a vector of fixed length as follows: et = enc (xt) (1), where xt-Rc \u00b7 h \u00b7 w is an h \u00b7 w image with c channels, and et-Re is the encoded attribute at the time. In this thesis, we use a CNN to encode the observation."}, {"heading": "4.2. Memory", "text": "The memory operations in the proposed architectures are similar to those in MemNN. Write. The encoded features of the last M observations are transformed linearly and stored in memory as key and value blocks, as shown in Figure 2a. Formally, two types of memory blocks are defined as follows: Mkeyt = W keyEt (2) Mvalt = W valEt (3), where Mkeyt, M val t,..., et \u2212 M] Re \u00b7 M are the concatenation of features of the last M observations, and Wkey, Wval, Rm \u00b7 e are the parameters of linear transformations for keys or values, respectively. Et = [et \u2212 1, et \u2212 2,..., et \u2212 M] Re \u00b7 M is the concatenation of features of the last M observations. Read. The memory reading mechanism is based on soft attention (Graves, 2013; Bahdanau et al., 2015) as shown in Figure 2b."}, {"heading": "4.3. Context", "text": "To this end, we present three different architectures for constructing the context vector: MQN: ht = Wcet (6) RMQN: [ht, ct] = LSTM (et, ht \u2212 1, ct \u2212 1) (8), where ht, ct Rm are a context vector and a memory cell of LSTM respectively, [et, ot \u2212 1] denotes the concatenation of the two vectors as input for LSTM. MQN is a feedback-forward architecture that constructs the context only on the basis of the current observation, which is very similar to MemNN, except that the current input is used for the memory query."}, {"heading": "5. Experiments", "text": "The experiments, baselines, and tasks are designed to examine how useful context-specific memory retrievals are to generalize invisible maps, and when memory feedback connections in FRMQN \u00b7 are helpful. Game videos can be found in the supplementary material and on the following website: https: / / sites.google.com / a / umich.edu / junhyuk-oh / icml2016-minecraft. Next, we describe aspects common to all tasks and our training methodology. Episodes end in all tasks either when the agent completes the task or after 50 steps. An agent receives a reward at each step of the time. The initial viewing direction of the agent is randomly selected between four directions: north, south, east, and west. For tasks where there is randomness (e.g. maps, spawning points), we will select an instance randomly."}, {"heading": "5.1. I-Maze: Description and Results", "text": "This year, the time has come for it to be only a matter of time before it happens, until it happens."}, {"heading": "5.2. Pattern Matching: Description and Results", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "5.3. Random Mazes: Description and Results", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "6. Discussion", "text": "In this paper, we presented three classes of cognitive-inspired tasks in Minecraft and compared the performance of two existing architectures with three architectures that we have proposed here. We emphasize that unlike most assessments of RL algorithms, we trained and evaluated architectures on disjoint sets of maps to specifically consider the applicability of learned value functions on invisible (interpolation and extrapolation) maps. In summary, our most important empirical finding is that context-dependent memory queries, especially with a feedback connection from the retrieved memory, can more effectively solve our tasks that require the control of active perception and external physical movement actions. In future work, our architectures, in particular FRQMN, also demonstrate superior capabilities compared to the base architectures, when learning value functions whose behavior is more generalized from training to invisible environments."}, {"heading": "Acknowledgement", "text": "This work has been supported by the NSF grant IIS-1526059. Any opinions, findings, conclusions or recommendations expressed herein are those of the authors and do not necessarily reflect the views of the sponsor."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A robot that reinforcement-learns to identify and memorize important previous observations", "author": ["Bakker", "Bram", "Zhumatiy", "Viktor", "Gruener", "Gabriel", "Schmidhuber", "J\u00fcrgen"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "Bakker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bakker et al\\.", "year": 2003}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, Advances in the Neural Information Processing System Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Deep learning for real-time atari game play using offline monte-carlo tree search planning", "author": ["Guo", "Xiaoxiao", "Singh", "Satinder", "Lee", "Honglak", "Lewis", "Richard L", "Wang", "Xiaoshi"], "venue": "In Advances in the Neural Information Processing System,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["Hausknecht", "Matthew", "Stone", "Peter"], "venue": "In AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents,", "citeRegEx": "Hausknecht et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bandit based monte-carlo planning", "author": ["Kocsis", "Levente", "Szepesv\u00e1ri", "Csaba"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "Kocsis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kocsis et al\\.", "year": 2006}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in the Neural Information Processing System,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Deep auto-encoder neural networks in reinforcement learning", "author": ["Lange", "Sascha", "Riedmiller", "Martin"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Lange et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lange et al\\.", "year": 2010}, {"title": "Guided policy search", "author": ["Levine", "Sergey", "Koltun", "Vladlen"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Levine et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2013}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Dharshan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dharshan et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["Narasimhan", "Karthik", "Kulkarni", "Tejas", "Barzilay", "Regina"], "venue": "In Conference on Empirical Methods on Natural Language Processing,", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["Oh", "Junhyuk", "Guo", "Xiaoxiao", "Lee", "Honglak", "Lewis", "Richard L", "Singh", "Satinder"], "venue": "In Advances in the Neural Information Processing System,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Mazes, maps, and memory", "author": ["Olton", "David S"], "venue": "American Psychologist,", "citeRegEx": "Olton and S.,? \\Q1979\\E", "shortCiteRegEx": "Olton and S.", "year": 1979}, {"title": "Universal value function approximators", "author": ["Schaul", "Tom", "Horgan", "Daniel", "Gregor", "Karol", "Silver", "David"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Deep learning in neural networks: An overview", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 2015}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["Stadie", "Bradly C", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1507.00814,", "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "Mazebase: A sandbox for learning from games", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Synnaeve", "Gabriel", "Chintala", "Soumith", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1511.07401,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Weston", "Jason", "Fergus", "Rob"], "venue": "In Advances in the Neural Information Processing System,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Temporal difference learning and tdgammon", "author": ["Tesauro", "Gerald"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "Recurrent policy gradients", "author": ["Wierstra", "Daan", "F\u00f6rster", "Alexander", "Peters", "Jan", "Schmidhuber", "J\u00fcrgen"], "venue": "Logic Journal of IGPL,", "citeRegEx": "Wierstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wierstra et al\\.", "year": 2010}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Learning simple algorithms from examples", "author": ["Zaremba", "Wojciech", "Mikolov", "Tomas", "Joulin", "Armand", "Fergus", "Rob"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Zaremba et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2016}, {"title": "Policy learning with continuous memory states for partially observed robotic control", "author": ["Zhang", "Marvin", "Levine", "Sergey", "McCarthy", "Zoe", "Finn", "Chelsea", "Abbeel", "Pieter"], "venue": "In International Conference on Robotics and Automation,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Map Generation for Pattern Matching There are a total of 512 possible visual patterns in a 3 \u00d7 3 room with blocks of two colors. We randomly picked 250 patterns and generated two maps for each pattern: one that", "author": ["Lillicrap et al", "2016). A"], "venue": null, "citeRegEx": "al. and A.2.,? \\Q2016\\E", "shortCiteRegEx": "al. and A.2.", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": ", 2015; Schmidhuber, 2015) have made advances in many lowlevel perceptual supervised learning problems (Krizhevsky et al., 2012; Girshick et al., 2014; Simonyan & Zisserman, 2015).", "startOffset": 103, "endOffset": 179}, {"referenceID": 4, "context": ", 2015; Schmidhuber, 2015) have made advances in many lowlevel perceptual supervised learning problems (Krizhevsky et al., 2012; Girshick et al., 2014; Simonyan & Zisserman, 2015).", "startOffset": 103, "endOffset": 179}, {"referenceID": 2, "context": ", 2015) architecture has been shown to successfully learn to play many Atari 2600 games in the Arcade Learning Environment (ALE) benchmark (Bellemare et al., 2013) by learning visual features useful for control directly from raw pixels using Q-Learning (Watkins & Dayan, 1992).", "startOffset": 139, "endOffset": 163}, {"referenceID": 6, "context": "Graves et al. (2014) introduced a Neural Turing Machine (NTM), a differentiable external memory architecture, and showed that it can learn algorithms such as copy and reverse.", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "Graves et al. (2014) introduced a Neural Turing Machine (NTM), a differentiable external memory architecture, and showed that it can learn algorithms such as copy and reverse. Zaremba & Sutskever (2015) proposed RL-NTM that has a nondifferentiable memory to scale up the addressing mechanism of NTM and applied policy gradient to train the architecture.", "startOffset": 0, "endOffset": 203}, {"referenceID": 6, "context": "Graves et al. (2014) introduced a Neural Turing Machine (NTM), a differentiable external memory architecture, and showed that it can learn algorithms such as copy and reverse. Zaremba & Sutskever (2015) proposed RL-NTM that has a nondifferentiable memory to scale up the addressing mechanism of NTM and applied policy gradient to train the architecture. Joulin & Mikolov (2015) implemented a stack using neural networks and demonstrated that it can infer several algorithmic patterns.", "startOffset": 0, "endOffset": 378}, {"referenceID": 6, "context": "Graves et al. (2014) introduced a Neural Turing Machine (NTM), a differentiable external memory architecture, and showed that it can learn algorithms such as copy and reverse. Zaremba & Sutskever (2015) proposed RL-NTM that has a nondifferentiable memory to scale up the addressing mechanism of NTM and applied policy gradient to train the architecture. Joulin & Mikolov (2015) implemented a stack using neural networks and demonstrated that it can infer several algorithmic patterns. Sukhbaatar et al. (2015b) proposed a Memory Network (MemNN) for Q&A and language modeling tasks, which stores all inputs and retrieves relevant memory blocks depending on the question.", "startOffset": 0, "endOffset": 511}, {"referenceID": 31, "context": "In addition, there are deep RL approaches to tasks other than Atari such as learning algorithms (Zaremba et al., 2016) and text-based games (Sukhbaatar et al.", "startOffset": 96, "endOffset": 118}, {"referenceID": 18, "context": ", 2016) and text-based games (Sukhbaatar et al., 2015a; Narasimhan et al., 2015).", "startOffset": 29, "endOffset": 80}, {"referenceID": 19, "context": "There have also been a few attempts to learn state-transition models using deep learning to improve exploration in RL (Oh et al., 2015; Stadie et al., 2015).", "startOffset": 118, "endOffset": 156}, {"referenceID": 25, "context": "There have also been a few attempts to learn state-transition models using deep learning to improve exploration in RL (Oh et al., 2015; Stadie et al., 2015).", "startOffset": 118, "endOffset": 156}, {"referenceID": 6, "context": "Guo et al. (2014) used slow Monte-Carlo Tree Search (MCTS) (Kocsis & Szepesv\u00e1ri, 2006) to generate a relatively small amount of data to train fast-playing convolutional networks in Atari games.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Guo et al. (2014) used slow Monte-Carlo Tree Search (MCTS) (Kocsis & Szepesv\u00e1ri, 2006) to generate a relatively small amount of data to train fast-playing convolutional networks in Atari games. Schulman et al. (2015), Levine et al.", "startOffset": 0, "endOffset": 217}, {"referenceID": 6, "context": "Guo et al. (2014) used slow Monte-Carlo Tree Search (MCTS) (Kocsis & Szepesv\u00e1ri, 2006) to generate a relatively small amount of data to train fast-playing convolutional networks in Atari games. Schulman et al. (2015), Levine et al. (2016), and Lillicrap et al.", "startOffset": 0, "endOffset": 239}, {"referenceID": 6, "context": "Guo et al. (2014) used slow Monte-Carlo Tree Search (MCTS) (Kocsis & Szepesv\u00e1ri, 2006) to generate a relatively small amount of data to train fast-playing convolutional networks in Atari games. Schulman et al. (2015), Levine et al. (2016), and Lillicrap et al. (2016) have successfully trained deep neural networks to directly learn policies and applied their architectures to robotics problems.", "startOffset": 0, "endOffset": 268}, {"referenceID": 6, "context": "Guo et al. (2014) used slow Monte-Carlo Tree Search (MCTS) (Kocsis & Szepesv\u00e1ri, 2006) to generate a relatively small amount of data to train fast-playing convolutional networks in Atari games. Schulman et al. (2015), Levine et al. (2016), and Lillicrap et al. (2016) have successfully trained deep neural networks to directly learn policies and applied their architectures to robotics problems. In addition, there are deep RL approaches to tasks other than Atari such as learning algorithms (Zaremba et al., 2016) and text-based games (Sukhbaatar et al., 2015a; Narasimhan et al., 2015). There have also been a few attempts to learn state-transition models using deep learning to improve exploration in RL (Oh et al., 2015; Stadie et al., 2015). Most recently, Mnih et al. (2016) proposed asynchronous DQN and showed that it can learn to explore a 3D environment similar to Minecraft.", "startOffset": 0, "endOffset": 781}, {"referenceID": 1, "context": "To deal with such a challenge, Bakker et al. (2003) used a Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1997) in an offline policy learning framework to show that a robot controlled by an LSTM network can solve T-Mazes where the robot should go to the correct destination depending on the traffic signal at the beginning of the maze.", "startOffset": 31, "endOffset": 52}, {"referenceID": 1, "context": "To deal with such a challenge, Bakker et al. (2003) used a Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1997) in an offline policy learning framework to show that a robot controlled by an LSTM network can solve T-Mazes where the robot should go to the correct destination depending on the traffic signal at the beginning of the maze. Wierstra et al. (2010) proposed a Recurrent Policy Gradient method and showed that an LSTM network trained using this method outperforms other methods in several tasks including TMazes.", "startOffset": 31, "endOffset": 377}, {"referenceID": 1, "context": "To deal with such a challenge, Bakker et al. (2003) used a Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1997) in an offline policy learning framework to show that a robot controlled by an LSTM network can solve T-Mazes where the robot should go to the correct destination depending on the traffic signal at the beginning of the maze. Wierstra et al. (2010) proposed a Recurrent Policy Gradient method and showed that an LSTM network trained using this method outperforms other methods in several tasks including TMazes. More recently, Zhang et al. (2016) introduced continuous memory states to augment the state and action space and showed it can memorize salient information through Guided Policy Search (Levine & Koltun, 2013).", "startOffset": 31, "endOffset": 575}, {"referenceID": 1, "context": "To deal with such a challenge, Bakker et al. (2003) used a Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1997) in an offline policy learning framework to show that a robot controlled by an LSTM network can solve T-Mazes where the robot should go to the correct destination depending on the traffic signal at the beginning of the maze. Wierstra et al. (2010) proposed a Recurrent Policy Gradient method and showed that an LSTM network trained using this method outperforms other methods in several tasks including TMazes. More recently, Zhang et al. (2016) introduced continuous memory states to augment the state and action space and showed it can memorize salient information through Guided Policy Search (Levine & Koltun, 2013). Hausknecht & Stone (2015) proposed Deep Recurrent QNetwork (DRQN) which consists of an LSTM on top of a CNN based on the DQN framework and demonstrated improved handling of partial observability in Atari games.", "startOffset": 31, "endOffset": 776}, {"referenceID": 1, "context": "The tasks we introduce are inspired by the T-maze experiments (Bakker et al., 2003) as well as MazeBase (Sukhbaatar et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 0, "context": "The reading mechanism of the memory is based on soft attention (Graves, 2013; Bahdanau et al., 2015) as illustrated in Figure 2b.", "startOffset": 63, "endOffset": 100}, {"referenceID": 26, "context": "In the results we report here, we used an MLP with one hidden layer as follows: gt = f ( Wht + ot ) ,qt = W gt where f is a rectified linear function (Nair & Hinton, 2010) applied only to half of the hidden units for easy optimization by following Sukhbaatar et al. (2015b). (a) I-Maze (b) Pattern Matching", "startOffset": 248, "endOffset": 274}, {"referenceID": 3, "context": "Our implementation is based on Torch7 (Collobert et al., 2011), a public DQN implementation (Mnih et al.", "startOffset": 38, "endOffset": 62}, {"referenceID": 21, "context": "More specifically, testing on {6, 8} sizes of maps and the rest of the sizes of maps can evaluate interpolation and extrapolation performance, respectively (Schaul et al., 2015).", "startOffset": 156, "endOffset": 177}], "year": 2016, "abstractText": "In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.", "creator": "LaTeX with hyperref package"}}}