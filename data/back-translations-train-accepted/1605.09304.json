{"id": "1605.09304", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks", "abstract": "Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).", "histories": [["v1", "Mon, 30 May 2016 16:22:54 GMT  (9801kb,D)", "http://arxiv.org/abs/1605.09304v1", "29 pages, 35 figures"], ["v2", "Fri, 3 Jun 2016 15:52:04 GMT  (9800kb,D)", "http://arxiv.org/abs/1605.09304v2", "29 pages, 35 figures"], ["v3", "Mon, 6 Jun 2016 17:34:59 GMT  (9801kb,D)", "http://arxiv.org/abs/1605.09304v3", "29 pages, 35 figures"], ["v4", "Thu, 27 Oct 2016 22:16:07 GMT  (9803kb,D)", "http://arxiv.org/abs/1605.09304v4", "29 pages, 35 figures, NIPS camera-ready"], ["v5", "Wed, 23 Nov 2016 18:41:12 GMT  (12783kb,D)", "http://arxiv.org/abs/1605.09304v5", "29 pages, 35 figures, NIPS camera-ready"]], "COMMENTS": "29 pages, 35 figures", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CV cs.LG", "authors": ["anh nguyen", "alexey dosovitskiy", "jason yosinski", "thomas brox", "jeff clune"], "accepted": true, "id": "1605.09304"}, "pdf": {"name": "1605.09304.pdf", "metadata": {"source": "CRF", "title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks", "authors": ["Anh Nguyen", "Alexey Dosovitskiy"], "emails": ["anguyen8@uwyo.edu", "dosovits@cs.uni-freiburg.de", "jason@geometricintelligence.com", "brox@cs.uni-freiburg.de", "jeffclune@uwyo.edu"], "sections": [{"heading": "1 Introduction and Related Work", "text": "In fact, the fact is that most people who are able to survive themselves are able to survive themselves, \"he told the German Press Agency in an interview.\" I don't think it's as if, \"he said.\" But it's as if, \"he said.\" It's not as if, \"he said,\" as if. \"\" It's as if. \"\" But it's as if. \"\" It's as if. \"\" \"It's as if.\" \"\" It's as if. \"\" It's as if. \"\" \"It's as if.\" \"\" It's as if. \"\" \"\" It's as if. \"\" \""}, {"heading": "2 Methods", "text": "We demonstrate our visualization method on a variety of different networks, which we activate directly. For reproducibility, we use prefabricated models that are freely available in Caffe or the Caffe Model Zoo [24]. Our default DNN is CaffeNet [24], a small variant of the common AlexNet architecture [28] with similar performance [24]. The last three fully interconnected layers of the 8-layer CaffeNet are called fc6, fc7 and fc8. fc8 is the last layer (pre softmax) and has 1000 outputs, one for each ImageNet Class. fc6 and fc7 have 4096 outputs each.We denote the DNN we want to visualize."}, {"heading": "3 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Comparison between priors trained to invert features from different layers", "text": "Since a generator model Gl could be trained to invert the representations of an arbitrary layer l of E, we have the same information as the encoder E (CaffeNet), but they may be different (as shown below).The Gl networks are made up [11]. For each Gl network, we chose the hyperparameter settings that yielded the best qualitative results. Optimizing codes from the convolutional layers (l = 3, 5) invent typical fragments, whereas optimizing fully connected layers produces global structures (Fig. S13). Previous studies have shown that G trained to process lower layers (smaller layers)."}, {"heading": "3.2 Does the learned prior trained on ImageNet generalize to other datasets?", "text": "We are testing whether the same DNN before (G6) that has been trained to reverse the feature representations of ImageNet images primarily enables visualization of DNNs trained on different datasets. Specifically, we are targeting the output neurons of two DNNs downloaded from the Caffe Model Zoo [24]: (1) An AlexNet DNN trained on the 2.5 million image dataset of MIT Places to classify 205 types of locations with 50.1% accuracy [30]; (2) A hybrid architecture of CaffeNet and the network in [2] created by [31] to classify actions in videos by processing each image of the video separately; the dataset consists of 13,320 videos categorized into 101 human action classes."}, {"heading": "3.3 Does the learned prior generalize to visualizing different architectures?", "text": "We have shown that if the DNN to be visualized is the same as the encoder E, the resulting visualizations are fairly realistic and recognizable (Figure 3,1). To visualize a different network architecture, one could train a new G to reverse its representation. However, creating a new G DGN for each DNN we want to visualize is mathematically costly. Here, we test whether the same DGN that was previously trained on CaffeNet (G6) can be used to visualize two state-of-the-art DNNs that are architecturally different from CaffeNet but trained on the same ImageNet dataset. Both have been downloaded from the Caffe Model Zoo and have similar accuracy values: (a) GoogLeNet is a 22-layer network and has a top-5 accuracy of 88.9% [25]; (b) ResNet is a new type of very deep architecture with Skip-26 connections, which we suspect is a top-5 architecture."}, {"heading": "3.4 Does the learned prior generalize to visualizing hidden neurons?", "text": "The visualization of hidden neurons in an ImageNet DNN. Previous visualization techniques have shown that low neurons recognize small, simple patterns such as corners and textures [2, 9, 7], middle neurons recognize individual objects such as faces and chairs, but that visualizations of hidden neurons in fully networked layers are alien and difficult to interpret [9]. Since DGN has been trained to falsify the functional representations of real, complete ImageNet images, one possibility is that these are not generalized previously to produce preferred images for such hidden neurons because they are often smaller and resemble non-real objects."}, {"heading": "3.5 Do the synthesized images teach us what the neurons prefer or what the prior prefers?", "text": "We have shown that we can create preferred images with realistic colors and textures by highlighting the properties of neurons that we have prioritized."}, {"heading": "3.6 Other applications of our proposed method", "text": "DGN-AM can also be useful for a variety of other important tasks. We briefly describe our experiments for these tasks and refer the reader to the supplementary section for more information.1. One advantage of synthesizing preferred images is that we can observe how features develop during training to better understand what happens during deep learning.1. It also tests whether what was previously learned (trained to invert features from a well-trained encoder) is generalized to visualize inadequate and exaggerated networks. Results suggest that the visualization quality indicates to some degree the validation accuracy of a DNN and that what was previously learned is not overly specialized in the well-trained encoder DNN. See Sec. S6 for more details. 2. Our method of synthesizing preferred images could, of course, be applied to synthesizing preferred videos for activity detection DNN to better understand how it works."}, {"heading": "4 Discussion and Conclusion", "text": "We have shown that activation maximization - the synthesis of the preferred input factors for neurons in neural networks - via a learned predecessor in the form of a deep generator network is a fruitful approach. DGNAM produces the most realistic, and therefore most interpretable, preferred images that have been produced to date, ensuring that it is qualitatively state-of-the-art in activation maximization; the visualizations it synthesizes from the ground up improve our ability to understand which features a neuron has learned to recognize; the images not only accurately reflect the characteristics learned by a neuron, but are also visually interesting; we have explored a variety of ways in which DGN-AM can help us understand trained DNNs. In future work, DGN-AM or its learned predecessors could dramatically improve our ability to synthesize an image from a text description of the same, (e.g. by synthesizing the image that transforms certain types of TIS into equally different types of TDN10)."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Bolei Zhou for providing images for comparison. Jeff Clune was supported by an NSF CAREER Award (CAREER: 1453549) and a hardware donation from NVIDIA Corporation and Jason Yosinski from NASA Space Technology Research Fellowship and NSF Grant 1527232."}, {"heading": "S5 Why do visualizations of some neurons not show canonical images?", "text": "In fact, it is true that most people are able to survive on their own and that they feel able to survive on their own. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "S8 Synthesizing creative art by activating two neurons instead of one", "text": "In fact, it is so that it is about a way in which people place themselves and themselves in the center of attention. (...) In fact, it is about a way in which they place themselves in the center of attention. (...) It is about a way in which they place themselves in the center of attention. (...) It is about a way in which they place themselves in the center of attention. (...) It is as if it is about a way in which they place themselves in the center of attention. (...) It is as if it is about a way in which they place themselves in the center. (...) It is as if it is about a way in which they place themselves in the center. (...) It is about a way in which it is about. (...) It is as if it is about a way in which it is about itself, in which it is about itself. (...) It is as if it is about a way in which it is about itself, in which it is about itself."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Deep neural networks (DNNs) have demonstrated state-of-the-art results on many<lb>pattern recognition tasks, especially vision classification problems. Understanding<lb>the inner workings of such computational brains is both fascinating basic science<lb>that is interesting in its own right\u2014similar to why we study the human brain\u2014and<lb>will enable researchers to further improve DNNs. One path to understanding<lb>how a neural network functions internally is to study what each of its neurons<lb>has learned to detect. One such method is called activation maximization (AM),<lb>which synthesizes an input (e.g. an image) that highly activates a neuron. Here<lb>we dramatically improve the qualitative state of the art of activation maximization<lb>by harnessing a powerful, learned prior: a deep generator network (DGN). The<lb>algorithm (1) generates qualitatively state-of-the-art synthetic images that look<lb>almost real, (2) reveals the features learned by each neuron in an interpretable<lb>way, (3) generalizes well to new datasets and somewhat well to different network<lb>architectures without requiring the prior to be relearned, and (4) can be considered<lb>as a high-quality generative method (in this case, by generating novel, creative,<lb>interesting, recognizable images).", "creator": "LaTeX with hyperref package"}}}