{"id": "1605.09346", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs", "abstract": "In this paper, we propose several improvements on the block-coordinate Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to optimize the structured support vector machine (SSVM) objective in the context of structured prediction, though it has wider applications. The key intuition behind our improvements is that the estimates of block gaps maintained by BCFW reveal the block suboptimality that can be used as an adaptive criterion. First, we sample objects at each iteration of BCFW in an adaptive non-uniform way via gapbased sampling. Second, we incorporate pairwise and away-step variants of Frank-Wolfe into the block-coordinate setting. Third, we cache oracle calls with a cache-hit criterion based on the block gaps. Fourth, we provide the first method to compute an approximate regularization path for SSVM. Finally, we provide an exhaustive empirical evaluation of all our methods on four structured prediction datasets.", "histories": [["v1", "Mon, 30 May 2016 18:15:30 GMT  (1409kb,D)", "http://arxiv.org/abs/1605.09346v1", "Appears in Proceedings of the 33rd International Conference on Machine Learning (ICML 2016). 31 pages"]], "COMMENTS": "Appears in Proceedings of the 33rd International Conference on Machine Learning (ICML 2016). 31 pages", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["anton osokin", "jean-baptiste alayrac", "isabella lukasewitz", "puneet kumar dokania", "simon lacoste-julien"], "accepted": true, "id": "1605.09346"}, "pdf": {"name": "1605.09346.pdf", "metadata": {"source": "META", "title": "Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs", "authors": ["Anton Osokin", "Jean-Baptiste Alayrac", "Isabella Lukasewitz", "Puneet K. Dokania", "Simon Lacoste-Julien"], "emails": ["FIRST.LASTNAME@INRIA.FR"], "sections": [{"heading": "1. Introduction", "text": "One of the most popular learning methods is the structural support of people who are able to survive themselves. (...) Most people who are able to survive themselves will be able to survive themselves. (...) Most people who are able to survive themselves will be able to survive themselves. (...) Most people who are able to survive themselves will be able to survive themselves. (...) Most people who are able to survive themselves will be able to survive themselves. (...) Most people who are able to survive themselves will be able to survive themselves. (...) Most people who are able to survive themselves will be able to survive themselves. \"(...) Most people who are able to survive themselves will be able to survive themselves."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Structured Support Vector Machine (SSVM)", "text": "In the structured prediction, we are confronted with an input x-X, and the goal is to predict a structured object y-y (x) (like a sequence of tags). In the default setting for structured SVM (SSVM) (SSVM) (Tsochantaridis et al., 2003; Tsochantaridis et al., 2005), we assume that the prediction is performed using a linear model hw (x) = argmaxy Y (x, y). We use the relevant information for input / output pairs below the notation and setup of Lacoste-Julien et al. (2013) Given a designated training environment D = (xi, yi)} ni = 1, the parameters w are estimated by solving a non-smooth optimization."}, {"heading": "2.2. Block Coordinate Frank-Wolfe method (BCFW)", "text": "In Alg. 1 we specify the BCFW algorithm of Lacoste-Julien et al. (2013), which is applied to problem (3). It uses the block separability of DomainM: = \u2206 | Y1 | \u00b7. \u00b7 \u2206 | Yn | for problem (3) and applies the Frank Wolfe steps sequentially to the blocks of the dual variables \u03b1 (i) \u0445 M (i): = \u0445 | Yi |. While the BCFW works on the dual (3) of the SSVM, it only explicitly maintains the primary variables on the relationship w (\u03b1). Most important is that the Frank Wolfe linear oracle on block i at Iterate \u03b1 (k) is the maximum weight vector w (k): = A\u03b1 (k) (Lacoste-Julien et al., 2013, App. B.1) the maximum oracle (2) on the corresponding weight vector w (k): Lacok-\u03b1 (the primary)."}, {"heading": "2.3. Duality gap", "text": "For each iteration, the batch Frank-Wolfe algorithm (Frank & Wolfe, 1956), (Lacoste-Julien et al., 2013, section 3) calculates the following quantity, known as the Linearization Duality Gap or Frank-Wolfe Gap: g (\u03b1): = max s-M < \u03b1 \u2212 s, 0 (\u03b1) > = < \u03b1 \u2212 s, 0 (\u03b1) >. (5) It turns out that this Frank-Wolfe Gap corresponds exactly to the Lagrange duality gap between the dual target (3) at one point \u03b1 and the urobjective (1) at the point w (\u03b1) = A\u03b1 (Lacoste-Julien et al., 2013, App. B.2) Due to the separability of M, the Frank-Wolfe Gap (5) can be used here as the sum of the block gaps gi (\u03b1), g (\u03b1) = 1 gi gap (\u03b1), with the equality of M (5) ely, ely (ely) (ely) (ely) (4) (ely) (ely) (ely) (4) (ely) (ely) (4) (ely) (ely) (4) (ely (ely) (4) (ely) (ely) (ely) (4) (ely (ely) (y) (4)."}, {"heading": "2.4. Convergence of BCFW", "text": "Lacoste-Julien et al. (2013) demonstrate the convergence of the BCFW algorithm at a rate of O (1k). Theorem 1 (Lacoste-Julien et al. (2013), theorem 2. For each k \u2265 0, the iterate3 \u03b1 (k) of Alg. 1 fulfils the IE [f (\u03b1 (k))) \u2212 f (\u03b1 (k)) \u2212 f (\u03b1). \u2212 f (Lacoste-Julien et al. (2013). Lacoste f + 2n (C'f + h0), where \"M\" is a solution to the problem (3). (h0: = f (0) \u2212 f (\u03b1) is the suboptimality at the starting point of the algorithm, \"f.\" f: = Laco n = 1 C (i) f is the sum of the curvature constants 4 of f in relation to domains M (i) \u2212 f. \""}, {"heading": "3. Block gaps in BCFW", "text": "In this section, we propose three ways to improve the BCFW algorithm: adaptive sampling (paragraph 3.1), pairwise steps and path steps (paragraph 3.2), and caching (paragraph 3.3)."}, {"heading": "3.1. Adaptive non-uniform sampling", "text": "When optimizing finite sums such as (1), it is often the case that processing some sums does not result in significant advances in the algorithms. (1) If these variables are already close to the optimum limit, then the BCFW algorithm does not make significant progress in this iteration. (6) Generally, it is difficult to determine whether processing the sums would lead to an improvement without actually performing calculations on them. BCFW algorithms obtain the block gap (6) with each iteration by quantifying the sub-optimization of the block. (an object of training sets) is selected for each iteration in such a way that the blocks with greater sub-optimization are more frequent (the sampling probability of a block is proportional to the value of the current estimate)."}, {"heading": "3.2. Pairwise and away steps", "text": "It is well known that it will be sublinear when the solution is at the limit (Wolfe, 1970), as is the case for SSVM. Several modifications have been proposed in the literature to address this problem. All these methods replace (or supplement) the FW step with a step of a different kind: it cannot be the case that all these methods have a linear rate on the target (3), even though they are not strongly convex. A common feature of these methods is the ability to remove elements of the active set (support of vectors in the case of SSVM) in order to reach the limit, as opposed to FW, which oscillates, while we expect that the solution of SSVM will be frugal."}, {"heading": "3.3. Caching", "text": "In cases where the maximum oracle is expensive enough, this step becomes a computational bottleneck. A natural idea to overcome this problem is to use a \"cheaper oracle,\" which consists most of the time in the hope that the resulting corner would be good enough to realize the results of the maximum oracle by implementing the previous calls of the maximum oracle to store potentially promising corners. The main principle of the oracle is to maintain a working environment Ci of the labels / corners for each block, in which the cache oracle defines the corner as a corner that is best aligned with the parentage direction, i.e., yci: = argmaxy @ Ci Hi (y)."}, {"heading": "4. Regularization path", "text": "According to the definition by Efron et al. (2004), a regularization path is a series of minimizers of a regularized object in the form of (1) for all possible values of the regularization parameter. Similar to LASSO and the binary SVM, the overall result of Rosset & Zhu (2007, Proposition 1) is applicable to the case of SSVM and implies that the exact regularization path is piecewise linear in 1 / \u03bb. However, restoring the exact path, to our knowledge, is intractable in the case of SSVM. In this paper, we construct an implicit regularization path, meaning that for every feasible gap we have a corresponding primary variable w that is actually approximate, i.e."}, {"heading": "5. Experiments", "text": "The experimental evaluation consists of two parts: Section 5.1 compares the different algorithms presented in Section 3; Section 5.2 evaluates our approach to estimating the regularization path. Datasets. We evaluate our methods against four datasets for different structured prediction tasks: OCR (Taskar et al., 2003) for handwritten character recognition, CoNLL (Tjong Kim Sang & Buchholz, 2000) for text tunking, HorseSeg et al. (2014) for binary image segmentation and LSP (Johnson & Everingham, 2010) for pose estimation. Models for OCR and CoNLL were provided by Lacoste-Julien et al. (2013). We build our model based on the model by Kolesnikov et al. (2014) for HorseSeg et al. (HorseSeg et al.), and the model by Felsev."}, {"heading": "5.1. Comparing the variants of BCFW", "text": "In this section, we evaluate the three modifications of the BCFW in Section 3. We compare 8 methods achieved by all combinations of three binary dimensions: gap-based vs. uniform scanning of objects, BCFW vs. BCPFW, caching oracle calls vs. no caching. We report the results of each method on 6 sets of data (including 3 sizes of HorseSeg) for three values of the regulation parameter \u03bb: the value leading to the best test performance, a smaller and larger value. For each setup, we report the duality gap against both oracle calls and elapsed time. We perform each method 5 times with different random seeds that affect the order of the sampled objects and report the medians (bold line), minimum and maximum values (shaded region). We summarize the results in Figure 3 and report the rest in App."}, {"heading": "5.2. Regularization path", "text": "In this section we evaluate our regularization path algorithm, which is illustrated in Section 4. We compare an approximate regularization path with \u03b5 = 0.1 with the standard approach of net search with / without warm start (we use a grid with 31 variables of \u03bb: 215, 214,..., 2 \u2212 15). In Figure 4 we specify the cumulative elapsed time and the cumulative number of effective exceedances of the data required by the three methods to achieve a certain value of \u03bb on the HorseSeg small dataset (based on the initialization value of the path method and the maximum values of the network for the net search methods).The methods and additional experiments are described in App. K.Interpretation. in detail. First, we observe that warm start speeds up the net search. Second, the costs of calculating the full regularization path are comparable with the costs of net search."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the MSR-Inria Joint Center and a Google Research Award."}, {"heading": "A. Related work for regularization path", "text": "Efron et al. (2004), in their groundbreaking paper, introduced the concept of regulation path and showed that the regulation path of LASSO (Tibshirani, 1996) is piecemeal linear. Hastie et al. (2004) proposed the path following method for calculating the exact regulation path for binary SVM with L2 regulation. Exact path following algorithms suffer from numerical instabilities as they repeatedly reverse a potentially poorly conditioned matrix (Allgower & Georg, 1993). Furthermore, Ga \ufffd rtner et al. (2012) show that the regulation path for binary SVM formulation contains an exponential number of breakpoints. Although the exact path following methods for SVM pieesen are still being developed (Sentelle et al al al al al al al al al al al al al al al al al al al al al al al al al., 2015), approximate methods may be more suitable for practical use cases. Karasuyama & Takeuchi (2011) suggested a path between the trade-off method and the developed method."}, {"heading": "B. Block descent lemma for BCFW", "text": "Definition 2 (Block Curvature Constant). Consider a convex function f = constant constant constant f = constant constant (1) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (n). The curvature constant C (i) \u2212 f of the function f w.r.t. The single coordination block f (i) is defined by C (i) f: = sup 2 \u03b32 (f (\u03b2) \u2212 f (\u03b1) \u2212 f (i) \u2212 lemma (i), (i) f (\u03b1) >) s.t. - M: s (i), s (i). - M: (i), p 2 (\u03b2), \u03b2 \u2212 f (s) \u2212 f (i). (13) Here s: i) - Rm and [i] - Block Rm are the zero added versions of s (i) - M (i) and M: i) - f (i)."}, {"heading": "C. Toy example for gap sampling", "text": "In this section, we construct a toy example of the structured SVM problem in which adaptive gap-based sampling (= dual gap-based sampling) is n times faster than non-adaptive sampling schemas as as uniform sampling or curvature-based sampling (the latter is the affine invariant object that requires at least K 1 visits to get the optimal parameters). We can design the example in such a way that the curvature or the Lipschitz constants are not informative about which example is hard, and which is6More general, let i is some norm defined on M (i). Then, we assume that Li is the Lipschitz continuity constant in relation to this standard (i) f (ell) varies."}, {"heading": "D. Detailed algorithms.", "text": "In this section we specify the detailed versions of our BCFW variants that apply to the SSVM warranty (= 0). (Algorithm 2 describes BCFW with adaptive gap.) We specify the block coordinate version of the paired FW algorithms as described in Lacoste-Julien & Jaggi (2015), but in the context of SSVM, which makes notation more difficult. Algorithm 5 presents the BCFW algorithms with caching. Algorithm 7 represents our method for calculating regulation and Algorithm 6 presents the initialization of regulation. Algorithm 2 Block Coordinate Frank-Wolfe (BCFW) algorithm with gap for structured SVM1:"}, {"heading": "E. Proof of Theorem 2 (convergence of BCFW with gap sampling)", "text": "Lemma 4 (Expected block parentage dilemma). Let gj (\u03b1 (k) = Frank (k)) be the block gap for block j (k). (Let\u03b1 (k + 1) is obtained by selecting a block i with the probability pi, and then performing a (block) FW step with the line search on this block, starting with the probability pi (k). Consider any number of scalars \u03b3j (0, 1], j = 1,. \u2212 n that do not depend on the selected block i. Then, in conditional expectation of the random choice of block i with the probabilities pi: It applies: IE [f (k + 1) | f (k) \u2212 s (k). \u2212 n that do not depend on the selected block i. (k) Then in conditional expectation of the random choice of block i with the probabilities pi: It applies: IE [f (k + 1)."}, {"heading": "F. Proof of Theorem 3 (convergence of BCFW with caching)", "text": "Theorem 3: K: Let's look at the fact that the combined oracle in case of an attack on the application case and the attack on the application part in the application part of the application part i: k = K = K = K = K = K: K = K = K: K = K = K = K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K:"}, {"heading": "G. Convergence of BCPFW and BCAFW", "text": "In this section we prove that the suboptimality error on (3) (\u00b7 M) (\u00b7 M) geometric (in expectation of BCPFW and BCAFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCFW) (in expectation of BCF"}, {"heading": "H. BCFW for SSVM with box constraints", "text": "H.1. Problem with check-box constraints Problem (1) can be rewritten as a quadratic program (QP) with an exponential number of constraints. < p > p > p p (QP) with an exponential number of constraints: min, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "I. Dataset description", "text": "In our experiments, we use four structured prediction datasets: OCR (Taskar et al., 2003) for character recognition; CoNLL (Tjong Kim Sang & Buchholz, 2000) for text chunking; HorseSeg (Kolesnikov et al., 2014) for binary image segmentation; LSP (Johnson & Everingham, 2010) for pose estimation. In this section, we provide the description of the datasets and the corresponding models. Table 1 summarizes quantitative statistics for all datasets. For the OCR and CoNLL datasets, the features and models described below are exactly the same as for LacosteJulien et al. (2013); we provide a detailed description for reference. For HorseSeg and LSP, we had to build the models ourselves from previous work collected in the relevant Section.I.1. OCR The Optical Character Recognition (OCR) dataset."}, {"heading": "J. Full experimental evaluation: comparing BCFW variants", "text": "Figures 5 and 6 show the detailed results of the experiments described in Section 5.1. Reminder: We compare different methods (caching versus no caching, gap sampling versus uniform sampling, paired FW steps versus regular FW steps) on different data sets in three main regimes. For each data set we use the good value of \u03bb (\"good\" means the smallest possible test error) together with its smaller and larger values. The three regimes are shown in the middle (b), top (a) and bottom (c) of each subfigure."}, {"heading": "K. Full experimental evaluation: regularization path", "text": "In this section, we evaluate the regulation method proposed in Section 4. Our experiments are organized into three stages: First, we select the two parameters for algorithm 7, which calculate the two approximate regulation paths. Second, we define and evaluate the gap in the heuristic regulation path. Finally, we compare the two approximate and heuristic paths against the number of break points and the accuracy of optimization for each break point. Algorithm 7 for calculating the approximate regulation gap has a parameter that controls how large the induction steps are in relation to \u03bb. This parameter provides the trade between the number of break points and the accuracy of optimization for each break point. We explore this trade-off in Figure 7a. For multiple values of the SSVM, we report the cumulative number of effective passes (lowest plots) and cumulative time (top plots) required to obtain a safe solution for each one."}], "references": [{"title": "Continuation and path following", "author": ["E. Allgower", "K. Georg"], "venue": "Acta Numerica,", "citeRegEx": "Allgower and Georg,? \\Q1993\\E", "shortCiteRegEx": "Allgower and Georg", "year": 1993}, {"title": "Linearly convergent away-step conditional gradient for non-strongly convex functions", "author": ["A. Beck", "S. Shtern"], "venue": null, "citeRegEx": "Beck and Shtern,? \\Q2015\\E", "shortCiteRegEx": "Beck and Shtern", "year": 2015}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Crammer and Singer,? \\Q2001\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2001}, {"title": "An optimal affine invariant smooth minimization algorithm", "author": ["A. d\u2019Aspremont", "C. Guzm\u00e1n", "M. Jaggi"], "venue": null, "citeRegEx": "d.Aspremont et al\\.,? \\Q2013\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2013}, {"title": "An exponential lower bound on the complexity of regularization paths", "author": ["B. G\u00e4rtner", "M. Jaggi", "C. Maria"], "venue": "Journal of Computational Geometry,", "citeRegEx": "G\u00e4rtner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "G\u00e4rtner et al\\.", "year": 2012}, {"title": "Approximating concavely parameterized optimization problems", "author": ["J. Giesen", "J. Mueller", "S. Laue", "S. Swiercy"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Giesen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Giesen et al\\.", "year": 2012}, {"title": "ImageNet auto-annotation with segmentation propagation", "author": ["M. Guillaumin", "D. K\u00fcttel", "V. Ferrari"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Guillaumin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guillaumin et al\\.", "year": 2014}, {"title": "The entire regularization path for the support vector machine", "author": ["T. Hastie", "S. Rosset", "R. Tibshirani", "J. Zhu"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Hastie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2004}, {"title": "On approximate solutions of systems of linear inequalities", "author": ["A.J. Hoffman"], "venue": "Journal of Research of the National Bureau of Standards,", "citeRegEx": "Hoffman,? \\Q1952\\E", "shortCiteRegEx": "Hoffman", "year": 1952}, {"title": "Revisiting Frank-Wolfe: Projection-free sparse convex optimization", "author": ["M. Jaggi"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Jaggi,? \\Q2013\\E", "shortCiteRegEx": "Jaggi", "year": 2013}, {"title": "Huberized multiclass support vector machine for microarray classification", "author": ["L.I. Jun-Tao", "J.I.A. Ying-Min"], "venue": "Acta Automatica Sinica,", "citeRegEx": "Jun.Tao and Ying.Min,? \\Q2010\\E", "shortCiteRegEx": "Jun.Tao and Ying.Min", "year": 2010}, {"title": "Suboptimal solution path algorithm for support vector machine", "author": ["M. Karasuyama", "I. Takeuchi"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Karasuyama and Takeuchi,? \\Q2011\\E", "shortCiteRegEx": "Karasuyama and Takeuchi", "year": 2011}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Characterizing the solution path of multicategory support vector machines", "author": ["Y. Lee", "Z. Cui"], "venue": "Statistica Sinica,", "citeRegEx": "Lee and Cui,? \\Q2006\\E", "shortCiteRegEx": "Lee and Cui", "year": 2006}, {"title": "Multicategory support vector machines, theory, and application to the classification of microarray data and satellite radiance data", "author": ["Y. Lee", "Y. Lin", "G. Wahba"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Lee et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2004}, {"title": "A Pylon model for semantic segmentation", "author": ["V. Lempitsky", "A. Vedaldi", "A. Zisserman"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lempitsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lempitsky et al\\.", "year": 2011}, {"title": "Distinctive image features from scaleinvariant keypoint", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Lowe,? \\Q2004\\E", "shortCiteRegEx": "Lowe", "year": 2004}, {"title": "Building a large annotated corpus of English: The Penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "CRFsuite: a fast implementation of conditional random fields (CRFs)", "author": ["N. Okazaki"], "venue": "URL http:// www.chokkan.org/software/crfsuite/", "citeRegEx": "Okazaki,? \\Q2007\\E", "shortCiteRegEx": "Okazaki", "year": 2007}, {"title": "Perceptually inspired layoutaware losses for image segmentation", "author": ["A. Osokin", "P. Kohli"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "Osokin and Kohli,? \\Q2014\\E", "shortCiteRegEx": "Osokin and Kohli", "year": 2014}, {"title": "A simple method for solving the SVM regularization path for semidefinite kernels", "author": ["C.G. Sentelle", "G.C. Anagnostopoulos", "M. Georgiopoulos"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Sentelle et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sentelle et al\\.", "year": 2015}, {"title": "Shallow parsing with conditional random fields", "author": ["F. Sha", "F. Pereira"], "venue": "In NAACL,", "citeRegEx": "Sha and Pereira,? \\Q2003\\E", "shortCiteRegEx": "Sha and Pereira", "year": 2003}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Multi-category support vector machines, feature selection and solution path", "author": ["L. Wang", "X. Shen"], "venue": "Statistica Sinica,", "citeRegEx": "Wang and Shen,? \\Q2006\\E", "shortCiteRegEx": "Wang and Shen", "year": 2006}], "referenceMentions": [{"referenceID": 9, "context": ", 1974), away step (Wolfe, 1970), fully-corrective step (Holloway, 1974) (see Lacoste-Julien & Jaggi (2015) for a recent review and the proof that all these methods have a linear rate on the objective (3) despite not being strongly convex).", "startOffset": 95, "endOffset": 108}, {"referenceID": 22, "context": "(2004), in their seminal paper, introduced the notion of regularization path and showed that the regularization path of LASSO (Tibshirani, 1996) is piecewise linear.", "startOffset": 126, "endOffset": 144}, {"referenceID": 20, "context": "Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases.", "startOffset": 70, "endOffset": 93}, {"referenceID": 5, "context": "1 of (Giesen et al., 2012) applied to the case of binary SVM.", "startOffset": 5, "endOffset": 26}, {"referenceID": 5, "context": "Another difference to (Giesen et al., 2012) consists in using the \u03bb-formulation of SVM instead of the C-formulation.", "startOffset": 22, "endOffset": 43}, {"referenceID": 5, "context": "Hastie et al. (2004) proposed the path following method to compute the exact regularization path for the binary SVM with L2-regularization.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "In addition, G\u00e4rtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points.", "startOffset": 13, "endOffset": 35}, {"referenceID": 4, "context": "In addition, G\u00e4rtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost.", "startOffset": 13, "endOffset": 342}, {"referenceID": 4, "context": "In addition, G\u00e4rtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant \u03b5approximate path with at most O(1/ \u221a \u03b5) break points and applied it, e.", "startOffset": 13, "endOffset": 470}, {"referenceID": 4, "context": "In addition, G\u00e4rtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant \u03b5approximate path with at most O(1/ \u221a \u03b5) break points and applied it, e.g., to binary SVM. In contrast to binary SVM, regularization paths for multiclass SVMs are more complex and less studied. Lee & Cui (2006) proposed a path following method for multi-class SVM in the MSVM formulation of Lee et al.", "startOffset": 13, "endOffset": 750}, {"referenceID": 4, "context": "In addition, G\u00e4rtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant \u03b5approximate path with at most O(1/ \u221a \u03b5) break points and applied it, e.g., to binary SVM. In contrast to binary SVM, regularization paths for multiclass SVMs are more complex and less studied. Lee & Cui (2006) proposed a path following method for multi-class SVM in the MSVM formulation of Lee et al. (2004). Wang & Shen (2006) analyzed the regularization path for the L1 version of MSVM.", "startOffset": 13, "endOffset": 848}, {"referenceID": 4, "context": "In addition, G\u00e4rtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant \u03b5approximate path with at most O(1/ \u221a \u03b5) break points and applied it, e.g., to binary SVM. In contrast to binary SVM, regularization paths for multiclass SVMs are more complex and less studied. Lee & Cui (2006) proposed a path following method for multi-class SVM in the MSVM formulation of Lee et al. (2004). Wang & Shen (2006) analyzed the regularization path for the L1 version of MSVM.", "startOffset": 13, "endOffset": 868}, {"referenceID": 4, "context": "In addition, G\u00e4rtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant \u03b5approximate path with at most O(1/ \u221a \u03b5) break points and applied it, e.g., to binary SVM. In contrast to binary SVM, regularization paths for multiclass SVMs are more complex and less studied. Lee & Cui (2006) proposed a path following method for multi-class SVM in the MSVM formulation of Lee et al. (2004). Wang & Shen (2006) analyzed the regularization path for the L1 version of MSVM. Finally, Jun-Tao & Ying-Min (2010) constructed the regularization path for the multi-class SVM with huberized loss.", "startOffset": 13, "endOffset": 964}, {"referenceID": 4, "context": "In addition, G\u00e4rtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant \u03b5approximate path with at most O(1/ \u221a \u03b5) break points and applied it, e.g., to binary SVM. In contrast to binary SVM, regularization paths for multiclass SVMs are more complex and less studied. Lee & Cui (2006) proposed a path following method for multi-class SVM in the MSVM formulation of Lee et al. (2004). Wang & Shen (2006) analyzed the regularization path for the L1 version of MSVM. Finally, Jun-Tao & Ying-Min (2010) constructed the regularization path for the multi-class SVM with huberized loss. We are not aware of any work computing the regularization path for SSVM or, for its predecessor multi-class SVM in the formulation of Crammer & Singer (2001). The induction step of our method is similar to Alg.", "startOffset": 13, "endOffset": 1203}, {"referenceID": 4, "context": "In addition, G\u00e4rtner et al. (2012) show that, in the worst case, the regularization path for binary SVM contains an exponential number of break points. Although the exact path following methods for SVM are still developed (Sentelle et al., 2015), approximate methods might be more suited for practical use cases. Karasuyama & Takeuchi (2011) proposed a method providing a trade-off between accuracy of the path and its computational cost. Recently, Giesen et al. (2012) have developed a general framework to construct a piecewise-constant \u03b5approximate path with at most O(1/ \u221a \u03b5) break points and applied it, e.g., to binary SVM. In contrast to binary SVM, regularization paths for multiclass SVMs are more complex and less studied. Lee & Cui (2006) proposed a path following method for multi-class SVM in the MSVM formulation of Lee et al. (2004). Wang & Shen (2006) analyzed the regularization path for the L1 version of MSVM. Finally, Jun-Tao & Ying-Min (2010) constructed the regularization path for the multi-class SVM with huberized loss. We are not aware of any work computing the regularization path for SSVM or, for its predecessor multi-class SVM in the formulation of Crammer & Singer (2001). The induction step of our method is similar to Alg. 1 of (Giesen et al., 2012) applied to the case of binary SVM. They also construct a piecewise linear \u03b5-approximate path by alternating the SVM solver and a procedure to identify the region where the output of the solver is accurate enough. In contrast to our method, Giesen et al. (2012) construct the path only for the predefined segment of the values of \u03bb.", "startOffset": 13, "endOffset": 1544}, {"referenceID": 9, "context": "Then similarly to Lemma 7 in Jaggi (2013), we have C (i) f \u2264 Li ( diam\u2016\u00b7\u2016iM (i) )2.", "startOffset": 29, "endOffset": 42}, {"referenceID": 9, "context": ", see Lemma 3 and 4 in Jaggi (2013)), hence showing that the optimization is difficult on this block.", "startOffset": 23, "endOffset": 36}, {"referenceID": 3, "context": "d\u2019Aspremont et al. (2013) suggests to use the atomic norm of the domain M for the analysis.", "startOffset": 0, "endOffset": 26}, {"referenceID": 9, "context": "We note that these two algorithms are simply the blockwise application of the PFW and AFW algorithms as described in Lacoste-Julien & Jaggi (2015), but in the context of SSVM which complicates the notation.", "startOffset": 134, "endOffset": 147}, {"referenceID": 9, "context": "We follow closely the notation and the results from Lacoste-Julien & Jaggi (2015) where the global linear convergence of the (batch) pairwise FW (PFW) and away-step FW (AFW) algorithms was shown.", "startOffset": 69, "endOffset": 82}, {"referenceID": 9, "context": "We follow closely the notation and the results from Lacoste-Julien & Jaggi (2015) where the global linear convergence of the (batch) pairwise FW (PFW) and away-step FW (AFW) algorithms was shown. The main insight to get our result is that the \u201cpairwise FW gap\u201d decomposes also as a sum of block gaps. We give our result for the following more general setting (the block-separable analog of the setup in Appendix F of Lacoste-Julien & Jaggi (2015)):", "startOffset": 69, "endOffset": 447}, {"referenceID": 9, "context": "(26) in LacosteJulien & Jaggi (2015)).", "startOffset": 24, "endOffset": 37}, {"referenceID": 9, "context": "(6) in Lacoste-Julien & Jaggi (2015) for AFW).", "startOffset": 24, "endOffset": 37}, {"referenceID": 9, "context": "(39) of Lacoste-Julien & Jaggi (2015) (\u03bc\u0303f is strictly greater than zero when q is strongly convex and M is a polytope).", "startOffset": 25, "endOffset": 38}, {"referenceID": 9, "context": "We now use the key relationship between the suboptimality hk and the PFW gap gk derived in inequality (43) of Lacoste-Julien & Jaggi (2015) (which is true for any function f by definition of \u03bc\u0303f if we allow it to be zero):", "startOffset": 127, "endOffset": 140}, {"referenceID": 8, "context": "In the more general case of problem (34) where only q is \u03bc-strongly convex, the generalized strong convexity depends both on \u03bc and the Hoffman constant (Hoffman, 1952) associated with the linear system of problem (34).", "startOffset": 152, "endOffset": 167}, {"referenceID": 8, "context": "Finally, the fact that \u03bc\u0303f > 0 when q is \u03bc-strongly convex and M is a polytope comes from the lower bound given in Theorem 10 of Lacoste-Julien & Jaggi (2015) in terms of the pyramidal width ofM (a strictly positive geometric quantity for polytopes), and the generalized strong convexity of f as defined in Lemma 9 of Lacoste-Julien & Jaggi (2015).", "startOffset": 146, "endOffset": 159}, {"referenceID": 8, "context": "Finally, the fact that \u03bc\u0303f > 0 when q is \u03bc-strongly convex and M is a polytope comes from the lower bound given in Theorem 10 of Lacoste-Julien & Jaggi (2015) in terms of the pyramidal width ofM (a strictly positive geometric quantity for polytopes), and the generalized strong convexity of f as defined in Lemma 9 of Lacoste-Julien & Jaggi (2015). The generalized strong convexity of f is simply \u03bc if f is \u03bc-strongly convex.", "startOffset": 146, "endOffset": 348}, {"referenceID": 8, "context": "In the more general case of problem (34) where only q is \u03bc-strongly convex, the generalized strong convexity depends both on \u03bc and the Hoffman constant (Hoffman, 1952) associated with the linear system of problem (34). See Lacoste-Julien & Jaggi (2015) for more details, as well as Lemma 2.", "startOffset": 135, "endOffset": 253}, {"referenceID": 8, "context": "In the more general case of problem (34) where only q is \u03bc-strongly convex, the generalized strong convexity depends both on \u03bc and the Hoffman constant (Hoffman, 1952) associated with the linear system of problem (34). See Lacoste-Julien & Jaggi (2015) for more details, as well as Lemma 2.2 of Beck & Shtern (2015).", "startOffset": 135, "endOffset": 316}, {"referenceID": 12, "context": "For a feature mapping \u03c8i(y) representing the sufficient statistics for an energy function associated with a graphical model (as for a conditional random field (Lafferty et al., 2001)), then the SSVM objective is implicitly optimizing over the marginal polytope for the graphical model (Wainwright & Jordan, 2008).", "startOffset": 159, "endOffset": 182}, {"referenceID": 8, "context": "Lacoste-Julien & Jaggi (2015) showed that the largest possible pyramidal width of a polytope in dimension m (for a fixed diameter) is achieved by the probability simplex and is \u0398(1/ \u221a m).", "startOffset": 17, "endOffset": 30}, {"referenceID": 8, "context": "By the affine invariance property of the FW-type algorithms, we can thus instead use the pyramidal width of the marginal polytope for the convergence analysis (and similarly for the Hoffman constant). Lacoste-Julien & Jaggi (2015) conjectured that the pyramidal width of a marginal polytope in dimension pwas also \u0398(1/ \u221a p), thus giving a more reasonable bound for the convergence rate of BCPFW for SSVM.", "startOffset": 182, "endOffset": 231}, {"referenceID": 17, "context": "The CoNLL dataset contains 8, 936 training English sentences extracted from the Wall Street Journal part of the Penn Treebank II (Marcus et al., 1993).", "startOffset": 129, "endOffset": 150}, {"referenceID": 17, "context": "The CoNLL dataset contains 8, 936 training English sentences extracted from the Wall Street Journal part of the Penn Treebank II (Marcus et al., 1993). Each output label yt can take up to 22 different values. We use the feature map\u03c6(x,y) proposed by Sha & Pereira (2003). First, for each position t of the input sequence x, we construct a unary feature representation, containing the local information.", "startOffset": 130, "endOffset": 271}, {"referenceID": 18, "context": "We extract the attributes with the CRFsuite library (Okazaki, 2007) and refer to its documentation for the exact list of attributes: http://www.", "startOffset": 52, "endOffset": 67}, {"referenceID": 16, "context": "The 1, 969 unary features include 1 constant feature, 512-bin histograms of densely sampled visual SIFT words (Lowe, 2004), 128-bin histograms of RGB colors, 16-bin histograms of locations (each pixel of a region of interest is matched to a cell of the 4\u00d7 4 uniform grid).", "startOffset": 110, "endOffset": 122}, {"referenceID": 6, "context": "HorseSeg contains 25, 438 training images, 147 of which are manually annotated, 5, 974 annotations are constructed from object bounding boxes by the automatic method of Guillaumin et al. (2014), while the remaining 19, 317 annotations were constructed by the same method but without any human supervision.", "startOffset": 169, "endOffset": 194}, {"referenceID": 6, "context": "HorseSeg contains 25, 438 training images, 147 of which are manually annotated, 5, 974 annotations are constructed from object bounding boxes by the automatic method of Guillaumin et al. (2014), while the remaining 19, 317 annotations were constructed by the same method but without any human supervision. The test set of HorseSeg consists of 241 images with manual annotations. In our experiments, we use training sets of three different sizes: 147 images for HorseSegsmall, 6, 121 images for HorseSeg-medium and 25, 438 for HorseSeg-large. In addition to images and their pixel-level annotations, Kolesnikov et al. (2014) released15 oversegmentations (superpixels) of the images precomputed with the SLIC algorithm (Achanta et al.", "startOffset": 169, "endOffset": 624}, {"referenceID": 6, "context": "HorseSeg contains 25, 438 training images, 147 of which are manually annotated, 5, 974 annotations are constructed from object bounding boxes by the automatic method of Guillaumin et al. (2014), while the remaining 19, 317 annotations were constructed by the same method but without any human supervision. The test set of HorseSeg consists of 241 images with manual annotations. In our experiments, we use training sets of three different sizes: 147 images for HorseSegsmall, 6, 121 images for HorseSeg-medium and 25, 438 for HorseSeg-large. In addition to images and their pixel-level annotations, Kolesnikov et al. (2014) released15 oversegmentations (superpixels) of the images precomputed with the SLIC algorithm (Achanta et al., 2012) and the unary features of each superpixel computed similarly to the work of Lempitsky et al. (2011). On average, each image contains 147 superpixels.", "startOffset": 169, "endOffset": 840}], "year": 2016, "abstractText": "In this paper, we propose several improvements on the block-coordinate Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to optimize the structured support vector machine (SSVM) objective in the context of structured prediction, though it has wider applications. The key intuition behind our improvements is that the estimates of block gaps maintained by BCFW reveal the block suboptimality that can be used as an adaptive criterion. First, we sample objects at each iteration of BCFW in an adaptive non-uniform way via gapbased sampling. Second, we incorporate pairwise and away-step variants of Frank-Wolfe into the block-coordinate setting. Third, we cache oracle calls with a cache-hit criterion based on the block gaps. Fourth, we provide the first method to compute an approximate regularization path for SSVM. Finally, we provide an exhaustive empirical evaluation of all our methods on four structured prediction datasets.", "creator": "LaTeX with hyperref package"}}}