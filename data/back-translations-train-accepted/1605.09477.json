{"id": "1605.09477", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "A Neural Autoregressive Approach to Collaborative Filtering", "abstract": "This paper proposes CF-NADE, a neural autoregressive architecture for collaborative filtering (CF) tasks, which is inspired by the Restricted Boltzmann Machine (RBM) based CF model and the Neural Autoregressive Distribution Estimator (NADE). We first describe the basic CF-NADE model for CF tasks. Then we propose to improve the model by sharing parameters between different ratings. A factored version of CF-NADE is also proposed for better scalability. Furthermore, we take the ordinal nature of the preferences into consideration and propose an ordinal cost to optimize CF-NADE, which shows superior performance. Finally, CF-NADE can be extended to a deep model, with only moderately increased computational complexity. Experimental results show that CF-NADE with a single hidden layer beats all previous state-of-the-art methods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more hidden layers can further improve the performance.", "histories": [["v1", "Tue, 31 May 2016 03:07:06 GMT  (282kb,D)", "http://arxiv.org/abs/1605.09477v1", "Accepted by ICML2016"]], "COMMENTS": "Accepted by ICML2016", "reviews": [], "SUBJECTS": "cs.IR cs.LG stat.ML", "authors": ["yin zheng", "bangsheng tang", "wenkui ding", "hanning zhou"], "accepted": true, "id": "1605.09477"}, "pdf": {"name": "1605.09477.pdf", "metadata": {"source": "META", "title": "A Neural Autoregressive Approach to Collaborative Filtering", "authors": ["Yin Zheng", "Bangsheng Tang", "Wenkui Ding", "Hanning Zhou"], "emails": ["YIN.ZHENG@HULU.COM", "BANGSHENG@HULU.COM", "WENKUI.DING@HULU.COM", "ERIC.ZHOU@HULU.COM"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that most people are able to recognize themselves and understand what they are doing. (...) Most people in the world do not know what they are doing. (...) Most people in the world do not know what they are doing. (...) They do not know what they are doing. (...) They do not know what they are doing. (...) They do not know what they are doing. (...) They do not know what they are doing. (...) They do not know what they are doing. (...) They do not know what they are doing. (...) They do not know what they are doing. (...) They do not know what they are doing. (...) They do not know what they are doing. (...) They do not know what they are doing. (...). (...) They do not know what they are doing. (...) They do not know what they are doing. (...). (...). \"\" \"\" \"..\" \"\".. \"\".. \"\".. \"\".. \"\".. \"\".. \"\".. \"\".. \"\".. \"\".. \"\" \"..\" \"..\" \"\" \"\".. \"..\" \"\" \"\".. \"\" \"..\". \".\". \"\" \"\" \"\". \"\" \"\". \"\". \"\" \"\". \"\". \"\" \".\" \"\". \"\". \"\" \"\". \"\". \"\". \"\". \".\". \".\" \"\". \".\". \".\" \"\" \".\".. \".\". \".\" \".\". \"\". \".\". \".\". \".\". \".\". \"\". \".\". \"\". \"\" \".\" \"\". \".\". \"\" \".\". \"\" \".\". \".\". \"\" \".\" \"\" \".\". \"\". \"\". \"\". \"\" \".\" \".\" \"\" \".\" \".\". \"\". \"\". \"\" \".\" \".\" \".\". \"\". \".\". \"\". \".\" \"\". \".\". \"\" \".\" \".\" \"\". \"\". \".\". \""}, {"heading": "2. Related Work", "text": "As already mentioned, some of the most successful model-based CF methods are based on matrix factorization techniques (PMF) of 2008, with a widespread assumption that the partially observed matrix occupies a low rank. Generally, MF characterizes both users and elements where the number of factors is much smaller than the number of users or objects, and the correlation between user and item factor vectors is used for recommendation tasks. Specifically, Billsus & Pazzani (1998) has proposed to apply Singular Value Decomposition (SVD) to CF tasks, which is an early work on MF-based CF tasks. Bias MF (Koren et al. 2009) is proposed to improve the performance of SVD by introducing systematic biases associated with users and items. Mnih & Salakhutdinov (2007) has expanded MF to a probable linear model with GF."}, {"heading": "3. NADE for Collaborative Filtering", "text": "This section is dedicated to CF-NADE, a NADE-based model for CF tasks. Specifically, we describe the basic model of CF-NADE in Section 3.1 and propose to improve CFNADE by sharing parameters between different assessments in Section 3.2. Finally, Section 3.3 describes a factoring version of CF-NADE to deal with large data sets."}, {"heading": "3.1. The Model", "text": "Suppose there are M-points, N-users and the ratings are integer from 1 to K (K-star scale). < M (K-star scale) A practical and dominant approach in the CF-NADE literature is that a user has the same parameters for each user and all of these models. In particular, all models have the same number of hidden units, but a user-specific model only has visible units if the user only rates D-points. Thus, each CF-NADE has only one training case, which is a vector of the ratings, but all weights and biases of these CF-NADE models are tied.In this paper, we refer to the training case for user u as ru = (rumo1, r u mo2)."}, {"heading": "3.2. Sharing Parameters Between Different Ratings", "text": "In equations 2 and 4, the connection matrices Wk, Vk and the bias bk differ for different evaluations k. In other words, CF-NADE uses different parameters for different evaluations. In practice, some evaluations can be observed much more frequently than others. As a result, the parameters associated with a rare evaluation may not be sufficiently optimized. To alleviate this problem, we propose to divide parameters between different evaluations of the same evaluation. In particular, we propose to calculate the hidden representation h (rmo < i) as follows: h (rmo < i) = g c + p < i rmoj \u0445k = 1 Wk:, moj (9) Note that given a user-rated article moj rmoj h (rmo < i) depending on all weights Wk < i) depending on Shark \u2264 rmoj."}, {"heading": "3.3. Dealing with Large-Scale Datasets", "text": "A disadvantage of CF-NADE, which we have described so far, is that the parameterization of Wk-RH-M and Vk-RM-H, where k is in the range of 1 to K, will lead to many free parameters, especially when dealing with massive datasets. For example, for the Netflix dataset (Bennett & Lanning, 2007), where H = 500 the number of free parameters of Wk and Vk would be about 89 million2. although strong overadjustment can be avoided by proper weight decomposition or abort (Srivastava et al., 2014), learning such a huge network would still be problematic. Inspired by RBM-CF (Salakhutdinov et al., 2007) and FixationNADE (Zheng et al., 2014a), we propose to address this problem by factoring Wk and Vk-M in products of two lower lengths CJ-K = 7J k 7J-7K = 7.000 k 7j-7j-7j-7j-7j-7j-7j-7j-7j-i)."}, {"heading": "4. Traing CF-NADE with Ordinal Cost", "text": "To go a step further, according to Truyen et al. (2009), we take into account the ordinary nature of a user's preference. That is, when a user reviews an item k, the user's preference for the ratings is monotonously increased from 1 to k, and the preference for the ratings from k to K. The basic CF-NADE treats different ratings as separate labels, without capturing the ordinal information. Here, we describe how to provide CF-NADE with ordal costs. <"}, {"heading": "5. Extending CF-NADE to a Deep Model", "text": "So far we have described CF-NADE with a single hidden layer. As from the recent and impressive successes of deep neural networks (Krizhevsky et al., 2012; Szegedy et al., 2014; Er et al., 2015), the extension of CF-NADE to a deep, multiple hidden model allows us better performance. (Uria et al., 2014), we propose an efficient, deep extension of CF-NADE.As mentioned in Section 3.1, a different CF-NADE model is used for each user and ordering o in r."}, {"heading": "6. Experiments", "text": "In this section, we test the performance of CF-NADE against three real benchmarks: MovieLens 1M, MovieLens 10M (Harper & Konstan, 2015) and Netflix dataset (Bennett & Lanning, 2007), each containing 106, 107 and 108 ratings. According to LLORMA (Lee et al., 2013) and AutoRec (Sedhain et al., 2015), 10% of the ratings in each of these datasets are randomly selected as a test set, so that the remaining 90% of the ratings are used as a training set. Among the ratings in the training set, 5% are used as a validation set. We use a standard rating of 3 for items without training observations. The prediction error is measured using Root Mean Squared Error (RMSE), RMSE =, 000 S i = 1 (ri \u2212 r \u0192i) 2S (22), where ri is the true rating and r i the rating predicted by the model test group, the overall benchmark of the NMA E benchmarks is averaged against some of the other benchmarks in the NMA E benchmarks."}, {"heading": "6.1. Datasets Description", "text": "The MovieLens 1M dataset contains approximately 1 million anonymous reviews of approximately 3900 movies from 6040 users, with each user rating at least 20 articles. The ratings in the MovieLens 1M dataset are based on a 5-star scale with 1-star steps. The MovieLens 10M dataset contains approximately 10 million reviews of 10681 movies from 71567 users. The users of the MovieLens 10M dataset are randomly selected and each user rates at least 20 movies. In contrast to the MovieLens 1M dataset, the ratings in the MovieLens 10M dataset are on a 5-star scale with half-star steps. Thus, the number of rating scales of MovieLens 10M is actually 10. In our experiments, we scale the ratings in MovieLens 10M to a 10-star scale with 1-star steps. The Netflix dataset comes from the Netflix challenge4. It is enormous compared to the 100 million users included in the previous 770-star ratings of the two movies."}, {"heading": "6.2. Experiments on MovieLen 1M Dataset", "text": "In this section, we will test the performance of CF-NADE on the MovieLen 1M dataset. First, we will evaluate the performance of 4The Netflix Price Challenge dataset test set is currently not available. Following Lee et al. (2013) and Sedhain et al. (2015), we will divide the available Netflix dataset test set into train, valid and test sets. Subsequently, we will compare the ordinal costs described in Section 4 with / without parameters allocated to different ratings as described in Section 3.2. Subsequently, we will compare several variants of CF-NADE with some strong baselines."}, {"heading": "6.2.1. THE PERFORMANCE OF THE ORDINAL COST", "text": "As Sedhain et al. (2015) mentioned, item-based CF outperforms user-based CF-NADE (ICF-NADE) in this section. In other words, the only difference between user-based CF-NADE (U-CF-NADE), which builds a different model for each user, as we described earlier, is that I-CF-NADE model builds a different CF-NADE model for each item. In other words, the only difference between U-CF-NADE models is that the roles of users and items are swapped. The comparison between U-CF-NADE and I-CF-NADE models is different."}, {"heading": "6.2.2. COMPARING WITH STRONG BASELINES ON", "text": "In this comparison, we compare CF-NADE-S with other hidden units on MoiveLens 1M dataset. During the comparison, the learning rate is selected based on the validation set by cross-validation at {0.001, 0.0005, 0.0002}, and the weight decay is chosen at {0.015, 0.02}. According to Section 6.2.1, the weight of the hidden units of CF-NADE-S is set at 1 and CF-NADE-S. The model is trained with Adam Optimizer and Tanh as activation function. Table 1 shows the performance of CF-NADE-S and baselines. The number of hidden units of CF-NADE is 500, just like AutoRec (Sedhain et al., 2015) for a fair comparison. It can be observed that I-CF-NADE-S exceeds the performance of CF-NADE-S by far."}, {"heading": "6.3. Experiments on MovieLens 10M Dataset", "text": "As mentioned in Section 6.1, the MovieLens 10M is much larger than the MovieLens 1M, so we opt for the factor-based version of CF-NADE, as described in Section 3.3, and set J = 50. Even with this setting, I-CF-NADE with 71567 users and 10 rating scales will still bring about 70 million free parameters, so we report only on the performance of U-CF-NADE in this experiment. As in Section 6.2.1, we train the model with Adam optimizer and use tanh as an activation feature. Other configurations are as follows: The number of hidden units is 500, the weight drop is 0.015 and \u03bb is set to 1, and the parameters are split between the ratings according to Section 6.2.1. The base learning rate is 0.0005, and we double it for the parameters of the first layer. Table 2 shows the comparison between CF-NADE and other baselines on Movieens 10S-NADS does not have lower performance already."}, {"heading": "6.4. Experiments on Netflix Dataset", "text": "Similar to Section 6.3, we use the factor version of U-CF-NADE with J = 50. The Netflix dataset is so large that we do not need to add strong regulation to avoid overadjustment and therefore set the weight degradation to 0.001. Other configurations are the same as in Section 6.3.Table 3 compares the performance of U-CF-NADE with other baselines. We can see that U-CF-NADE-S with a single hidden layer achieves an RMSE of 0.804, exceeding all baselines. Another observation from Table 3 is that using a deep CF-NADE architecture achieves a slight improvement over the flat one, with a test RMSE of 0.803."}, {"heading": "6.5. The Complexity and Running Time of CF-NADE", "text": "We implement CF-NADE using Theano (Bastien et al., 2012) and Blocks (van Merrie \ufffd nboer et al., 2015), and the code is available at https: / / github.com / Ian09 / CF-NADE. Table 4 shows the runtime of an epoch5 as well as the number of parameters used by CF-NADE. For MovieLens 1M dataset, we used the item-based CFNADE and did not use the factoring method introduced by Sec 3.3, so the number of parameters for MovieLens 1M is greater than the other two. Runtime in Table 4 includes overheads such as transferring to and from the GPU memory for each update. Note that there is still room for faster implementations 6."}, {"heading": "7. Conclusions", "text": "In this paper, we propose CF-NADE, a feed-forward, autoregressive architecture for collaborative filtering tasks. CF-NADE is inspired by the groundbreaking work of RBM-CF and the recent advances of NADE. We propose to share parameters between different evaluations to improve performance. We also describe a factor version of CF-NADE that reduces the number of parameters by factoring a large matrix through a product of two lower-ranking matrices to achieve better scalability. In addition, we take into account the neat nature of the preference and propose the neat cost of optimizing CF-NADE. Finally, following recent advances in deep learning, we extend CF-NADE to a deep model with moderate increase in computational complexity. Experimental results on three real benchmark datasets show that CF-NADE exceeds the implicit filtering methods."}, {"heading": "Acknowledgements", "text": "We thank Hugo Larochelle and the reviewers for many helpful conversations."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bouchard et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bouchard et al\\.", "year": 2012}, {"title": "The netflix prize", "author": ["Bennett", "James", "Lanning", "Stan"], "venue": "In Proceedings of KDD cup and workshop,", "citeRegEx": "Bennett et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bennett et al\\.", "year": 2007}, {"title": "Learning collaborative information filters", "author": ["Billsus", "Daniel", "Pazzani", "Michael J"], "venue": "In ICML,", "citeRegEx": "Billsus et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Billsus et al\\.", "year": 1998}, {"title": "Neural network matrix factorization", "author": ["Dziugaite", "Gintare Karolina", "Roy", "Daniel M"], "venue": "arXiv preprint arXiv:1511.06443,", "citeRegEx": "Dziugaite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dziugaite et al\\.", "year": 2015}, {"title": "Scalable recommendation with poisson factorization", "author": ["Gopalan", "Prem", "Hofman", "Jake M", "Blei", "David M"], "venue": "arXiv preprint arXiv:1311.1704,", "citeRegEx": "Gopalan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2013}, {"title": "Bayesian nonparametric poisson factorization for recommendation systems", "author": ["Gopalan", "Prem", "Ruiz", "Francisco JR", "Ranganath", "Rajesh", "Blei", "David M"], "venue": "Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Gopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2014}, {"title": "Content-based recommendations with poisson factorization", "author": ["Gopalan", "Prem K", "Charlin", "Laurent", "Blei", "David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2014}, {"title": "The movielens datasets: History and context", "author": ["Harper", "F Maxwell", "Konstan", "Joseph A"], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS),", "citeRegEx": "Harper et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Harper et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Hu", "Yifan", "Koren", "Yehuda", "Volinsky", "Chris"], "venue": "In Data Mining,", "citeRegEx": "Hu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2008}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Koren", "Yehuda", "Bell", "Robert", "Volinsky", "Chris"], "venue": null, "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A neural autoregressive topic model", "author": ["Larochelle", "Hugo", "Lauly", "Stanislas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Larochelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2012}, {"title": "The neural autoregressive distribution estimator", "author": ["Larochelle", "Hugo", "Murray", "Iain"], "venue": "In International Conference on Artificial Intelligence and Statistics, pp", "citeRegEx": "Larochelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2011}, {"title": "Non-linear matrix factorization with gaussian processes", "author": ["Lawrence", "Neil D", "Urtasun", "Raquel"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Lawrence et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lawrence et al\\.", "year": 2009}, {"title": "Local low-rank matrix approximation", "author": ["Lee", "Joonseok", "Kim", "Seungyeon", "Lebanon", "Guy", "Singer", "Yoram"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Divide-and-conquer matrix factorization", "author": ["Mackey", "Lester W", "Jordan", "Michael I", "Talwalkar", "Ameet"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mackey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mackey et al\\.", "year": 2011}, {"title": "Probabilistic matrix factorization", "author": ["Mnih", "Andriy", "Salakhutdinov", "Ruslan"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "Fast maximum margin matrix factorization for collaborative prediction", "author": ["Rennie", "Jasson DM", "Srebro", "Nathan"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Rennie et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rennie et al\\.", "year": 2005}, {"title": "Grouplens: an open architecture for collaborative filtering of netnews", "author": ["Resnick", "Paul", "Iacovou", "Neophytos", "Suchak", "Mitesh", "Bergstrom", "Peter", "Riedl", "John"], "venue": "In Proceedings of the 1994 ACM conference on Computer supported cooperative work,", "citeRegEx": "Resnick et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Resnick et al\\.", "year": 1994}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "List-wise learning to rank with matrix factorization for collaborative filtering", "author": ["Shi", "Yue", "Larson", "Martha", "Hanjalic", "Alan"], "venue": "In Proceedings of the fourth ACM conference on Recommender systems,", "citeRegEx": "Shi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2010}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Ordinal boltzmann machines for collaborative filtering", "author": ["Truyen", "Tran The", "Phung", "Dinh Q", "Venkatesh", "Svetha"], "venue": "In Proceedings of the Twenty-fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Truyen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Truyen et al\\.", "year": 2009}, {"title": "Rnade: The real-valued neural autoregressive density-estimator", "author": ["Uria", "Benigno", "Murray", "Iain", "Larochelle", "Hugo"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Uria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2013}, {"title": "A deep and tractable density estimator", "author": ["Uria", "Benigno", "Murray", "Iain", "Larochelle", "Hugo"], "venue": "JMLR: W&CP,", "citeRegEx": "Uria et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2014}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Dumoulin", "Vincent", "Serdyuk", "Dmitriy", "Warde-Farley", "David", "Chorowski", "Jan", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1506.00619,", "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Listwise approach to learning to rank: theory and algorithm", "author": ["Xia", "Fen", "Liu", "Tie-Yan", "Wang", "Jue", "Zhang", "Wensheng", "Li", "Hang"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Xia et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2008}, {"title": "A deep and autoregressive approach for topic modeling of multimodal data", "author": ["Y. Zheng", "Zhang", "Yu-Jin", "H. Larochelle"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "A neural autoregressive approach to attention-based recognition", "author": ["Zheng", "Yin", "Zemel", "Richard S", "Zhang", "Yu-Jin", "Larochelle", "Hugo"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Zheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}, {"title": "Topic modeling of multimodal data: An autoregressive approach", "author": ["Zheng", "Yin", "Zhang", "Yu-Jin", "H. Larochelle"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Zheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "The explainatbility of the recommended results as well as the easy-to-implement nature of memory-based CF ensured its popularity in early recommender systems (Resnick et al., 1994).", "startOffset": 158, "endOffset": 180}, {"referenceID": 11, "context": "(Koren et al., 2009; Salakhutdinov & Mnih, 2008; Mackey et al., 2011; Gopalan et al., 2013).", "startOffset": 0, "endOffset": 91}, {"referenceID": 17, "context": "(Koren et al., 2009; Salakhutdinov & Mnih, 2008; Mackey et al., 2011; Gopalan et al., 2013).", "startOffset": 0, "endOffset": 91}, {"referenceID": 4, "context": "(Koren et al., 2009; Salakhutdinov & Mnih, 2008; Mackey et al., 2011; Gopalan et al., 2013).", "startOffset": 0, "endOffset": 91}, {"referenceID": 12, "context": "With the recent development of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural network based CF, a subclass of model-based CF, has gained enormous attention.", "startOffset": 45, "endOffset": 109}, {"referenceID": 25, "context": "With the recent development of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural network based CF, a subclass of model-based CF, has gained enormous attention.", "startOffset": 45, "endOffset": 109}, {"referenceID": 8, "context": "With the recent development of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural network based CF, a subclass of model-based CF, has gained enormous attention.", "startOffset": 45, "endOffset": 109}, {"referenceID": 22, "context": "A prominent example is RBMbased CF (RBM-CF) (Salakhutdinov et al., 2007).", "startOffset": 44, "endOffset": 72}, {"referenceID": 27, "context": "NADE together with its variants achieved competitive results on many machine learning tasks (Larochelle & Lauly, 2012; Uria et al., 2013; Zheng et al., 2014b; Uria et al., 2014; Zheng et al., 2014a; 2015).", "startOffset": 92, "endOffset": 204}, {"referenceID": 28, "context": "NADE together with its variants achieved competitive results on many machine learning tasks (Larochelle & Lauly, 2012; Uria et al., 2013; Zheng et al., 2014b; Uria et al., 2014; Zheng et al., 2014a; 2015).", "startOffset": 92, "endOffset": 204}, {"referenceID": 26, "context": "As Truyen et al. (2009) observed, preference usually has the ordinal nature: if the true rating of an item by a user is 3 stars in a 5-star scale, then predicting 4 stars is preferred to predicting 5 stars.", "startOffset": 3, "endOffset": 24}, {"referenceID": 11, "context": "Bias MF (Koren et al., 2009) is proposed to improve the performance of SVD by introducing systematic biases associated with users and items.", "startOffset": 8, "endOffset": 28}, {"referenceID": 17, "context": "There are other MF-based CF methods such as (Rennie & Srebro, 2005; Mackey et al., 2011).", "startOffset": 44, "endOffset": 88}, {"referenceID": 8, "context": "Bias MF (Koren et al., 2009) is proposed to improve the performance of SVD by introducing systematic biases associated with users and items. Mnih & Salakhutdinov (2007) extended MF to a probabilistic linear model with Gaussian noise referred to as Probabilistic Matrix Factorization (PMF), and showed that PMF performed better than SVD.", "startOffset": 9, "endOffset": 169}, {"referenceID": 8, "context": "Bias MF (Koren et al., 2009) is proposed to improve the performance of SVD by introducing systematic biases associated with users and items. Mnih & Salakhutdinov (2007) extended MF to a probabilistic linear model with Gaussian noise referred to as Probabilistic Matrix Factorization (PMF), and showed that PMF performed better than SVD. Salakhutdinov & Mnih (2008) proposed a Bayesian treatment of PMF, which can be trained efficiently by MCMC methods.", "startOffset": 9, "endOffset": 365}, {"referenceID": 8, "context": "Bias MF (Koren et al., 2009) is proposed to improve the performance of SVD by introducing systematic biases associated with users and items. Mnih & Salakhutdinov (2007) extended MF to a probabilistic linear model with Gaussian noise referred to as Probabilistic Matrix Factorization (PMF), and showed that PMF performed better than SVD. Salakhutdinov & Mnih (2008) proposed a Bayesian treatment of PMF, which can be trained efficiently by MCMC methods. Along this line, Lawrence & Urtasun (2009) proposed a non-linear PMF using Gaussian process latent variable models.", "startOffset": 9, "endOffset": 496}, {"referenceID": 4, "context": "Recently, Poisson Matrix Factorization (Gopalan et al., 2014b;a; 2013) was proposed, replacing Gaussian assumption of PMF by Poisson distribution. Lee et al. (2013) extended the low-rank assumption by embedding locality into MF models and proposed Local Low-Rank Matrix Approximation (LLORMA) method, which achieved impressive performance on several public benchmarks.", "startOffset": 40, "endOffset": 165}, {"referenceID": 12, "context": "With the tremendous success of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural networks have found profound applications in CF tasks.", "startOffset": 45, "endOffset": 109}, {"referenceID": 25, "context": "With the tremendous success of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural networks have found profound applications in CF tasks.", "startOffset": 45, "endOffset": 109}, {"referenceID": 8, "context": "With the tremendous success of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural networks have found profound applications in CF tasks.", "startOffset": 45, "endOffset": 109}, {"referenceID": 22, "context": "RBM-CF (Salakhutdinov et al., 2007) and AutoRec (Sedhain et al.", "startOffset": 7, "endOffset": 35}, {"referenceID": 8, "context": ", 2014; He et al., 2015), neural networks have found profound applications in CF tasks. Salakhutdinov et al. (2007) proposed a variant of Restricted Boltzmann Machine (RBM) for CF tasks, which is successfully applied in Netflix prize challenge (Bennett & Lanning, 2007).", "startOffset": 8, "endOffset": 116}, {"referenceID": 8, "context": ", 2014; He et al., 2015), neural networks have found profound applications in CF tasks. Salakhutdinov et al. (2007) proposed a variant of Restricted Boltzmann Machine (RBM) for CF tasks, which is successfully applied in Netflix prize challenge (Bennett & Lanning, 2007). Recently, Sedhain et al. (2015) proposed AutoRec, an autoencoder-based CF model, which achieved the state-of-the-art performance on some benchmarks.", "startOffset": 8, "endOffset": 303}, {"referenceID": 8, "context": ", 2014; He et al., 2015), neural networks have found profound applications in CF tasks. Salakhutdinov et al. (2007) proposed a variant of Restricted Boltzmann Machine (RBM) for CF tasks, which is successfully applied in Netflix prize challenge (Bennett & Lanning, 2007). Recently, Sedhain et al. (2015) proposed AutoRec, an autoencoder-based CF model, which achieved the state-of-the-art performance on some benchmarks. RBM-CF (Salakhutdinov et al., 2007) and AutoRec (Sedhain et al., 2015) are common in that both of them build different models for different users, where all these models share the parameters. Truyen et al. (2009) proposed to apply Boltzmann Machine (BM) on CF tasks, which extends RBM-CF by integrating the correlation between users and between items.", "startOffset": 8, "endOffset": 633}, {"referenceID": 8, "context": ", 2014; He et al., 2015), neural networks have found profound applications in CF tasks. Salakhutdinov et al. (2007) proposed a variant of Restricted Boltzmann Machine (RBM) for CF tasks, which is successfully applied in Netflix prize challenge (Bennett & Lanning, 2007). Recently, Sedhain et al. (2015) proposed AutoRec, an autoencoder-based CF model, which achieved the state-of-the-art performance on some benchmarks. RBM-CF (Salakhutdinov et al., 2007) and AutoRec (Sedhain et al., 2015) are common in that both of them build different models for different users, where all these models share the parameters. Truyen et al. (2009) proposed to apply Boltzmann Machine (BM) on CF tasks, which extends RBM-CF by integrating the correlation between users and between items. Truyen et al. (2009) also extended the standard BM model so as to exploit the ordinal nature of ratings.", "startOffset": 8, "endOffset": 793}, {"referenceID": 8, "context": ", 2014; He et al., 2015), neural networks have found profound applications in CF tasks. Salakhutdinov et al. (2007) proposed a variant of Restricted Boltzmann Machine (RBM) for CF tasks, which is successfully applied in Netflix prize challenge (Bennett & Lanning, 2007). Recently, Sedhain et al. (2015) proposed AutoRec, an autoencoder-based CF model, which achieved the state-of-the-art performance on some benchmarks. RBM-CF (Salakhutdinov et al., 2007) and AutoRec (Sedhain et al., 2015) are common in that both of them build different models for different users, where all these models share the parameters. Truyen et al. (2009) proposed to apply Boltzmann Machine (BM) on CF tasks, which extends RBM-CF by integrating the correlation between users and between items. Truyen et al. (2009) also extended the standard BM model so as to exploit the ordinal nature of ratings. Recently, Dziugaite & Roy (2015) proposed Neural Network Matrix Factorization (NNMF), where the inner product between the vectors of users and items in MF is replaced by a feed-forward neural network.", "startOffset": 8, "endOffset": 910}, {"referenceID": 22, "context": "To tackle sparsity, similar to RBM-CF (Salakhutdinov et al., 2007), we use a different CF-NADE model for each user and all these models share the same parameters.", "startOffset": 38, "endOffset": 66}, {"referenceID": 27, "context": "As Uria et al. (2014) observed, we can think of the models trained with different orderings as different instantiations of CF-NADE for the same user.", "startOffset": 3, "endOffset": 22}, {"referenceID": 22, "context": "Inspired by RBM-CF (Salakhutdinov et al., 2007) and FixationNADE (Zheng et al.", "startOffset": 19, "endOffset": 47}, {"referenceID": 26, "context": "To go one step further, following Truyen et al. (2009), we take the ordinal nature of a user\u2019s preference into consideration.", "startOffset": 34, "endOffset": 55}, {"referenceID": 30, "context": "Both two products in Equation 17 can be interpreted as the likelihood loss introduced in (Xia et al., 2008) in the", "startOffset": 89, "endOffset": 107}, {"referenceID": 30, "context": "Actually, from the perspective of learning-to-rank, CF-NADE acts as a ranking function which produces rankings of ratings based on previous ratings, where sjmoi (rmo<i ) corresponds to the score function in (Xia et al., 2008) and the rankings, ydown and yup, corresponds to true rankings that we would like CF-NADE to fit.", "startOffset": 207, "endOffset": 225}, {"referenceID": 23, "context": "Put differently, the ranking loss in Equation 17 is defined on the ratings, while other learning-to-rank based CF methods, such as (Shi et al., 2010), are on items, which is the crucial difference.", "startOffset": 131, "endOffset": 149}, {"referenceID": 12, "context": "As suggested by the recent and impressive success of deep neural networks (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), extending CF-NADE to a deep, multiple hidden layers architecture could allow us to have better performance.", "startOffset": 74, "endOffset": 138}, {"referenceID": 25, "context": "As suggested by the recent and impressive success of deep neural networks (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), extending CF-NADE to a deep, multiple hidden layers architecture could allow us to have better performance.", "startOffset": 74, "endOffset": 138}, {"referenceID": 8, "context": "As suggested by the recent and impressive success of deep neural networks (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), extending CF-NADE to a deep, multiple hidden layers architecture could allow us to have better performance.", "startOffset": 74, "endOffset": 138}, {"referenceID": 31, "context": "(2014) proposed an efficient deep extension to original NADE (Larochelle & Murray, 2011) for binary vector observations, which inspires other related deep model (Zheng et al., 2015).", "startOffset": 161, "endOffset": 181}, {"referenceID": 28, "context": "Following (Uria et al., 2014), we propose a deep variant of CF-NADE.", "startOffset": 10, "endOffset": 29}, {"referenceID": 8, "context": ", 2014; He et al., 2015), extending CF-NADE to a deep, multiple hidden layers architecture could allow us to have better performance. Recently, Uria et al. (2014) proposed an efficient deep extension to original NADE (Larochelle & Murray, 2011) for binary vector observations, which inspires other related deep model (Zheng et al.", "startOffset": 8, "endOffset": 163}, {"referenceID": 27, "context": "As noticed by Uria et al. (2014) and Zheng et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 27, "context": "As noticed by Uria et al. (2014) and Zheng et al. (2015), training over all possible orderings for CF-NADE implies that for any given context rmo<i , the model performs equally well at predicting all the remaining items in rmo\u2265i , since for each item there is an ordering such that it appears at position i.", "startOffset": 14, "endOffset": 57}, {"referenceID": 28, "context": "The factors in front of the sum come from the fact that the total number of elements in the sum is D and that we are averaging over D\u2212 i+1 possible choices for the item at position i, similar to (Uria et al., 2014) and (Zheng et al.", "startOffset": 195, "endOffset": 214}, {"referenceID": 31, "context": ", 2014) and (Zheng et al., 2015).", "startOffset": 12, "endOffset": 32}, {"referenceID": 16, "context": "Following LLORMA (Lee et al., 2013) and AutoRec (Sedhain et al.", "startOffset": 17, "endOffset": 35}, {"referenceID": 16, "context": "Following Lee et al. (2013) and Sedhain et al.", "startOffset": 10, "endOffset": 28}, {"referenceID": 16, "context": "Following Lee et al. (2013) and Sedhain et al. (2015), we split the available trainset of Netflix dataset into train, valid and test sets.", "startOffset": 10, "endOffset": 54}, {"referenceID": 16, "context": ", 2015) and LLORMA (Lee et al., 2013).", "startOffset": 19, "endOffset": 37}, {"referenceID": 9, "context": "however, explicit feedback is not always available or as common as implicit feedback (watch, search, browse behaviors) in real-world recommender systems (Hu et al., 2008).", "startOffset": 153, "endOffset": 170}], "year": 2016, "abstractText": "This paper proposes CF-NADE, a neural autoregressive architecture for collaborative filtering (CF) tasks, which is inspired by the Restricted Boltzmann Machine (RBM) based CF model and the Neural Autoregressive Distribution Estimator (NADE). We first describe the basic CF-NADE model for CF tasks. Then we propose to improve the model by sharing parameters between different ratings. A factored version of CF-NADE is also proposed for better scalability. Furthermore, we take the ordinal nature of the preferences into consideration and propose an ordinal cost to optimize CF-NADE, which shows superior performance. Finally, CF-NADE can be extended to a deep model, with only moderately increased computational complexity. Experimental results show that CF-NADE with a single hidden layer beats all previous state-of-the-art methods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more hidden layers can further improve the performance.", "creator": "LaTeX with hyperref package"}}}