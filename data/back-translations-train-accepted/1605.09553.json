{"id": "1605.09553", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Attention Correctness in Neural Image Captioning", "abstract": "Attention mechanisms have recently been introduced in deep learning for various tasks in natural language processing and computer vision. But despite their popularity, the \"correctness\" of the implicitly-learned attention maps has only been assessed qualitatively by visualization of several examples. In this paper we focus on evaluating and improving the correctness of attention in neural image captioning models. Specifically, we propose a quantitative evaluation metric for how well the attention maps align with human judgment, using recently released datasets with alignment between regions in images and entities in captions. We then propose novel models with different levels of explicit supervision for learning attention maps during training. The supervision can be strong when alignment between regions and caption entities are available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing supervision of attention maps during training solidly improves both attention correctness and caption quality.", "histories": [["v1", "Tue, 31 May 2016 10:04:20 GMT  (9583kb,D)", "http://arxiv.org/abs/1605.09553v1", null], ["v2", "Wed, 23 Nov 2016 07:29:46 GMT  (2067kb,D)", "http://arxiv.org/abs/1605.09553v2", "To appear in AAAI-17. Seethis http URLfor supplementary material"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["chenxi liu", "junhua mao", "fei sha", "alan l yuille"], "accepted": true, "id": "1605.09553"}, "pdf": {"name": "1605.09553.pdf", "metadata": {"source": "CRF", "title": "Attention Correctness in Neural Image Captioning", "authors": ["Chenxi Liu", "Junhua Mao", "Fei Sha", "Alan Yuille"], "emails": ["{cxliu@,", "mjhustc@,", "feisha@cs.}ucla.edu", "alan.l.yuille@gmail.com"], "sections": [{"heading": null, "text": "In this paper, we focus on assessing and improving the accuracy of attention in neural captioning models. Specifically, we propose a quantitative measure of how well attention maps match human judgment, using recently published datasets with alignment between regions in images and units in captions. We then propose novel models with different levels of explicit monitoring for learning attention maps during training. Monitoring can be strong when alignment between regions and captions is available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing monitoring of attention maps during training solidly improves both attention correctness and caption quality."}, {"heading": "1 Introduction", "text": "In this area, the input consists of a number of vectors with the same dimension. Deep attention models address these tasks by learning a dynamic combination of these vectors. In this thesis, we focus on attention models for caption. State-of-the-art caption models [15, 18, 9, 27] incorporate Convolutional Neural Networks (CNNs) to extract image attributes and recurrent neurons (RNNs) to decode these attributes into a sentence description. These models can be interpreted within a sequence-to-sequence."}, {"heading": "2 Related Work", "text": "However, it is not clear whether the caption models really understand and recognize the objects in the image while generating the captions. [29] They have proposed an attention model and qualitatively demonstrated that the model can serve certain regions of the image by visualizing the attention maps of some images. We are building on their work and going a step further by quantitatively measuring the quality of the attention maps, which provides insights into understanding and improving current captions. Deep Attention Models Attention Mechanism is an important feature of human visual systems. [23, 7] Since deep neural networks are inspired by the structure of neurons in the human brain, the use of attention in these artificial models is natural and promising."}, {"heading": "3 Deep Attention Models for Image Captioning", "text": "In this section, we first present the attention model of Xu et al. [29], which implicitly learns attention weights and then introduces our explicitly supervised attention model."}, {"heading": "3.1 Implicit Attention Model", "text": "The model consists of three parts: the encoder, which encodes the visual information (i.e. a visual extractor), the decoder, which decodes the information into words, and the attention module, which performs the spatial attention. The aim of the decoder is to generate a caption y of length C: y = {y1,.., yC}. We use yt, \u2212 RK to represent the most uniform encoding of yt, where K is the dictionary size.In [29] an LSTM network [11] is used as decoder: it = a dynamic state (WiEyt \u2212 1 + cite + bi) (1) ft = wake."}, {"heading": "3.2 Supervised Attention Model", "text": "Deep network attention can be considered a form of alignment from the linguistic space to the image space. However, it is not clear how good this alignment is. In addition, even if the basic truth of this alignment is contained in a dataset, the model in [29] will not be able to use this information to learn a better attention function fattn (ai, ht \u2212 1). In this work, we try to force \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "3.2.1 Strong Supervision with Alignment Annotation", "text": "In the simplest case, we have a direct annotation that links the basic truth word wt with a region Rt (in the form of delimiters or segment masks) in the image. We encourage the model to \"pay attention\" to Rt by starting with \u03b2-t = {\u03b2-t-i} i = 1,.., L-by: \u03b2-t-i = {1 i-i-Rt 0 otherwise (12) Note that the resolution of the region R (e.g. 224 x 224) and the attention map \u03b1, \u03b2 (e.g. 14 x 14) may be different, so that L-t might differ from L. Therefore, we have to magnify and normalize \u03b2-t to the same resolution as \u03b1t to obtain \u03b2-t."}, {"heading": "3.2.2 Weak Supervision with Object Category Annotation", "text": "A much more general and cheaper remark is the use of Bounding Boxes or segments with object category. In this case, we get a number of regions Rj in the image with associated object classes cj, j = 1.., M, where M is the number of object delimiting boxes or segments in the image. Although these annotations are not ideal, they contain important information to draw the model's attention. For example, when generating the word \"boy,\" the model should pay attention to a person's region and pay attention to the region of a dog when generating the word \"dog.\" We can use this information to automatically find semantically related words in sentences and regions with object category labels in the image. According to this intuition, we set the probability that a word wt and a region Rj can be labeled with the image category by the similarity of wt and cj in the word embedding."}, {"heading": "4 Attention Correctness: Evaluation Metric", "text": "At each step of the implicit attention model, the LSTM not only predicts the next word yt, but also generates an attention map \u03b1t-RL across all places. However, the attention module is only an intermediate step, while the error is propagated backwards only by the word probability loss in Equation 9, which raises the question of whether this implicitly learned attention module is actually effective. Therefore, in this section, we introduce the concept of attention correctness, a value that quantitatively analyzes the quality of the attention cards generated by the attention-based model. 4.1 DefinitionFor a word yt with the attention map \u03b1t generated, let Rt be the basic truth region, then we define the word attention correctness by AC (yt) = i-Rt-Rt-Rt-Rt-Rt-Rt value in the order. \""}, {"heading": "4.2 Ground Truth Attention Region During Testing", "text": "To calculate the correctness of attention, we need to match the regions in the image and the phrases in the caption. To this end, we propose two strategies.Ground Truth Caption One option is to force the trained model to issue the basic truth by resetting yt at each step. This procedure allows us to \"decorate\" the attention module to a certain degree from the caption component and to diagnose whether the learned attention module is meaningful. Since the generated caption is exactly the same as the basic truth, we can test attention correctness for each noun phrase. Generated Caption Another option is to align the entities in the generated caption with those in the truth caption."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Implementation Details", "text": "We resize the image so that the shorter side has 25\u03b2, and then center the 224 \u00d7 224 image. We then extract the conv5 _ 4 feature of the 19-layer version of the VGG network [24], which is pre-trained on ImageNet [8]. The model is trained with stochastic gradient descent using the Adam algorithm [14]. Dropout [25] is used as regularization. We use the hyperparameters provided in publicly available Code2. Specifically, we set the number of LSTM units to 1300 for Flickr30k and 1800 for COCO.Ground Truth Attention for Strong Supervision Model We experiment with our model in Section 3.2.1 on Flickr30k Dataset [31]. We use the Flickr30k Entities Dataset [22] for fundamental1."}, {"heading": "5.2 Evaluation of Attention Correctness", "text": "In this subsection, we quantify the attention correctness of both implicit and monitored attention. All experiments are performed on the 1000 test images of Flickr30k. We compare the result with a uniform baseline that applies equally to the entire image. Therefore, the baseline value is simply the percentage of the interface size over the size of the entire image. Results are summarized in Table 1.Ground Truth Caption Result. In this setting, both implicit and monitored models are forced to produce exactly the same captions, resulting in 14566 noun formulations. We discard those that have no attention or show full image attention (such as the Match Score 1 regardless of the attention card)."}, {"heading": "5.3 Evaluation of Captioning Performance", "text": "In the previous section, we showed that monitored attention models achieve higher attention correctness than implicit attention models. Although this makes sense for tasks such as region grounding, attention is only an intermediate step for many tasks. Our intuition is that appealing dynamic weighting of the input vectors allows later components to more easily decode information. In this section, we provide experimental support by showing that the monitored attention model also provides better performance in subtitles. We report BLEU- [21] and METEOR [3] values to enable comparison with [29]. In Table 2, we show both the values reported in [29] and our implementation. Note that our implementation of [29] yields a slightly improved result compared to what they reported. We observe that BLEU- and METEOR values are continuously increasing after we introduce monitored attention for both Flickr30k EU and COsignify an increase of 0.9% or 0.7% respectively."}, {"heading": "6 Discussion", "text": "In this paper, we first try to provide a quantitative answer to the question: To what extent do attention cards correspond with human perceptions? First, we define attention correctness at both the word and phrase levels. In the context of captions, we evaluated the state-of-the-art models with implicitly trained attention modules. The quantitative evaluation results suggest that the implicit models exceed the baseline, but still have a lot of room for improvement. We then show that by introducing attention card monitoring, we can improve both the performance of captions and the quality of the attention card. Even if the truth about the reason for attention is not available, we are still able to use the segmentation masks with object categories as weak supervision of attention cards and significantly increase the performance of captions. We believe that it is necessary to see the visual gap between machine-based attention and other, similar-to-human-perception tasks that we expect."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1412.7755", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, volume 29, pages 65\u201372", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Abc-cnn: An attention based convolutional neural network for visual question answering", "author": ["K. Chen", "J. Wang", "L.-C. Chen", "H. Gao", "W. Xu", "R. Nevatia"], "venue": "arXiv preprint arXiv:1511.05960", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1411.5654", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Control of goal-directed and stimulus-driven attention in the brain", "author": ["M. Corbetta", "G.L. Shulman"], "venue": "Nature reviews neuroscience, 3(3):201\u2013215", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2625\u20132634", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "From captions to visual concepts and back. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1473\u20131482", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u2013 1780", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Framing image description as a ranking task: Data", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "models and evaluation metrics. Journal of Artificial Intelligence Research, pages 853\u2013899", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "L. Bourdev", "R. Girshick", "J. Hays", "P. Perona", "D. Ramanan", "C.L. Zitnick", "P. Doll\u00e1r"], "venue": "arXiv preprint arXiv:1405.0312", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J.R. Finkel", "S. Bethard", "D. McClosky"], "venue": "ACL (System Demonstrations), pages 55\u201360", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["V. Mnih", "N. Heess", "A. Graves"], "venue": "Recurrent models of visual attention. In Advances in Neural Information Processing Systems, pages 2204\u20132212", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models", "author": ["B.A. Plummer", "L. Wang", "C.M. Cervantes", "J.C. Caicedo", "J. Hockenmaier", "S. Lazebnik"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2641\u20132649", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "The dynamic representation of scenes", "author": ["R.A. Rensink"], "venue": "Visual cognition, 7(1-3):17\u201342", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, pages 3104\u20133112", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask", "author": ["H. Xu", "K. Saenko"], "venue": "attend and answer: Exploring question-guided spatial attention for visual question answering. arXiv preprint arXiv:1511.05234", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Show", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Image captioning with semantic attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "arXiv preprint arXiv:1603.03925", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual7w: Grounded question answering in images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1511.03416", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Attention based deep models have been proved effective at handling problems such as machine translation [2], object detection [20, 1], visual question answering [28, 4], and image captioning [29].", "startOffset": 104, "endOffset": 107}, {"referenceID": 19, "context": "Attention based deep models have been proved effective at handling problems such as machine translation [2], object detection [20, 1], visual question answering [28, 4], and image captioning [29].", "startOffset": 126, "endOffset": 133}, {"referenceID": 0, "context": "Attention based deep models have been proved effective at handling problems such as machine translation [2], object detection [20, 1], visual question answering [28, 4], and image captioning [29].", "startOffset": 126, "endOffset": 133}, {"referenceID": 27, "context": "Attention based deep models have been proved effective at handling problems such as machine translation [2], object detection [20, 1], visual question answering [28, 4], and image captioning [29].", "startOffset": 161, "endOffset": 168}, {"referenceID": 3, "context": "Attention based deep models have been proved effective at handling problems such as machine translation [2], object detection [20, 1], visual question answering [28, 4], and image captioning [29].", "startOffset": 161, "endOffset": 168}, {"referenceID": 28, "context": "Attention based deep models have been proved effective at handling problems such as machine translation [2], object detection [20, 1], visual question answering [28, 4], and image captioning [29].", "startOffset": 191, "endOffset": 195}, {"referenceID": 14, "context": "The state-of-the-art image captioning models [15, 18, 13, 9, 27] adopt Convolutional Neural Networks (CNNs) to extract image features and Recurrent Neural Networks (RNNs) to decode these features into a sentence description.", "startOffset": 45, "endOffset": 64}, {"referenceID": 17, "context": "The state-of-the-art image captioning models [15, 18, 13, 9, 27] adopt Convolutional Neural Networks (CNNs) to extract image features and Recurrent Neural Networks (RNNs) to decode these features into a sentence description.", "startOffset": 45, "endOffset": 64}, {"referenceID": 12, "context": "The state-of-the-art image captioning models [15, 18, 13, 9, 27] adopt Convolutional Neural Networks (CNNs) to extract image features and Recurrent Neural Networks (RNNs) to decode these features into a sentence description.", "startOffset": 45, "endOffset": 64}, {"referenceID": 8, "context": "The state-of-the-art image captioning models [15, 18, 13, 9, 27] adopt Convolutional Neural Networks (CNNs) to extract image features and Recurrent Neural Networks (RNNs) to decode these features into a sentence description.", "startOffset": 45, "endOffset": 64}, {"referenceID": 26, "context": "The state-of-the-art image captioning models [15, 18, 13, 9, 27] adopt Convolutional Neural Networks (CNNs) to extract image features and Recurrent Neural Networks (RNNs) to decode these features into a sentence description.", "startOffset": 45, "endOffset": 64}, {"referenceID": 25, "context": "These models can be interpreted within a sequence-to-sequence [26] or encoder-decoder [6] framework, so it is natural to apply attention mechanisms in these models [29].", "startOffset": 62, "endOffset": 66}, {"referenceID": 5, "context": "These models can be interpreted within a sequence-to-sequence [26] or encoder-decoder [6] framework, so it is natural to apply attention mechanisms in these models [29].", "startOffset": 86, "endOffset": 89}, {"referenceID": 28, "context": "These models can be interpreted within a sequence-to-sequence [26] or encoder-decoder [6] framework, so it is natural to apply attention mechanisms in these models [29].", "startOffset": 164, "endOffset": 168}, {"referenceID": 28, "context": "Although impressive visualization results of the attention maps for image captioning are shown in [29], the authors do not provide quantitative evaluations of the attention maps generated by their models.", "startOffset": 98, "endOffset": 102}, {"referenceID": 21, "context": "We use the alignment annotation between image regions and noun phrase caption entities provided in the recently released Flickr30k Entities dataset [22] as our ground truth maps.", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": "Using this metric, we show that the attention model of [29] performs better than the uniform attention baseline, but still has big room for improvement in terms of attention consistency with humans.", "startOffset": 55, "endOffset": 59}, {"referenceID": 28, "context": "We propose a quantitative evaluation metric for the quality of attention maps and find there is room for improvement of the implicitly learned attention maps of [29].", "startOffset": 161, "endOffset": 165}, {"referenceID": 21, "context": "the Flickr30k Entities dataset [22]) but also when only the object categories of image regions (which is a much cheaper type of annotations compared to [22]) are available (e.", "startOffset": 31, "endOffset": 35}, {"referenceID": 21, "context": "the Flickr30k Entities dataset [22]) but also when only the object categories of image regions (which is a much cheaper type of annotations compared to [22]) are available (e.", "startOffset": 152, "endOffset": 156}, {"referenceID": 15, "context": "MS COCO dataset [16]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 28, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 17, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 26, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 8, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 9, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 12, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 4, "context": "Image Captioning Models There has been growing interest in the field of image captioning, with lots of work demonstrating impressive results [15, 29, 18, 27, 9, 10, 13, 5].", "startOffset": 141, "endOffset": 171}, {"referenceID": 28, "context": "[29] proposed an attention model and qualitatively showed that the model can attend to specific regions of the image by visualizing the attention maps of a few images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Deep Attention Models Attention mechanism is an important property of human visual systems [23, 7].", "startOffset": 91, "endOffset": 98}, {"referenceID": 6, "context": "Deep Attention Models Attention mechanism is an important property of human visual systems [23, 7].", "startOffset": 91, "endOffset": 98}, {"referenceID": 1, "context": "In machine translation, [2] introduced an extra softmax layer in the RNN/LSTM structure that generates weights of the individual words of the sentence to be translated.", "startOffset": 24, "endOffset": 27}, {"referenceID": 28, "context": "In image captioning, [29] replaced the individual words in machine translation model by convolutional image features, allowing the model to attend to different areas of the image when generating words one by one.", "startOffset": 21, "endOffset": 25}, {"referenceID": 29, "context": "[30] proposed to target attention on a set of concepts extracted from the image to generate image captions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "In visual question answering, [4, 28, 32] proposed several models which attend to image regions or questions when generating an answer.", "startOffset": 30, "endOffset": 41}, {"referenceID": 27, "context": "In visual question answering, [4, 28, 32] proposed several models which attend to image regions or questions when generating an answer.", "startOffset": 30, "endOffset": 41}, {"referenceID": 31, "context": "In visual question answering, [4, 28, 32] proposed several models which attend to image regions or questions when generating an answer.", "startOffset": 30, "endOffset": 41}, {"referenceID": 11, "context": "Image Description Datasets For image captioning, Flickr8k [12], Flickr30k [31], and MS COCO [16] are the most commonly used benchmark datasets.", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "Image Description Datasets For image captioning, Flickr8k [12], Flickr30k [31], and MS COCO [16] are the most commonly used benchmark datasets.", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "Image Description Datasets For image captioning, Flickr8k [12], Flickr30k [31], and MS COCO [16] are the most commonly used benchmark datasets.", "startOffset": 92, "endOffset": 96}, {"referenceID": 21, "context": "[22] developed the original caption annotations in Flickr30k by providing the region to phrase correspondences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29]\u2019s attention model that learns the attention weights implicitly and then introduce our explicit supervised attention model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] was the first attempt to introduce attention models to image captioning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "In [29], an LSTM network [11] is used as the decoder:", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In [29], an LSTM network [11] is used as the decoder:", "startOffset": 25, "endOffset": 29}, {"referenceID": 28, "context": "[29]\u2019s deterministic \u201csoft\u201d attention model,", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "In [29], this function is implemented as a multilayer perceptron.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "Moreover, even if the ground truth of this alignment is provided in a dataset, the model in [29] will not be able to take advantage of this information to learn better attention function fattn(ai,ht\u22121).", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "Stanford Parser [17]), and see if there exists a word-by-word match in the set of noun phrases in the ground truth captions.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "Implicit/Supervised Attention Models All implementation details strictly follow [29].", "startOffset": 80, "endOffset": 84}, {"referenceID": 23, "context": "We then extract the conv5_4 feature of the 19 layer version of VGG net [24] pretrained on ImageNet [8].", "startOffset": 71, "endOffset": 75}, {"referenceID": 7, "context": "We then extract the conv5_4 feature of the 19 layer version of VGG net [24] pretrained on ImageNet [8].", "startOffset": 99, "endOffset": 102}, {"referenceID": 13, "context": "The model is trained using stochastic gradient descent with the Adam algorithm [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "Dropout [25] is used as regularization.", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "1 on Flickr30k dataset [31].", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "We use the Flickr30k Entities dataset [22] for generating ground", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "Ground Truth Attention for Weak Supervision Model The MS COCO dataset [16] contains instance segmentation masks of 80 classes in addition to the captions, which makes it suitable for our model with weak supervision in section 3.", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "We only construct \u03b2 t for the nouns in the captions, which we extract using the Stanford Parser [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "The similarity function is chosen to be the cosine distance between word vectors [19] pretrained on GoogleNews3, and we set an empirical threshold of 1/3.", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "To address this problem, we refer to the supplement of [16], which provides a scene category list containing key words of scenes used when collecting the dataset.", "startOffset": 55, "endOffset": 59}, {"referenceID": 28, "context": "22% for the implicit attention model [29], and 11.", "startOffset": 37, "endOffset": 41}, {"referenceID": 28, "context": "Flickr30k Implicit [29] 28.", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": "49 Implicit [29]* 29.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "COCO Implicit [29] 34.", "startOffset": 14, "endOffset": 18}, {"referenceID": 28, "context": "90 Implicit [29]* 36.", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "We report BLEU [21] and METEOR [3] scores to allow comparison with [29].", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "We report BLEU [21] and METEOR [3] scores to allow comparison with [29].", "startOffset": 31, "endOffset": 34}, {"referenceID": 28, "context": "We report BLEU [21] and METEOR [3] scores to allow comparison with [29].", "startOffset": 67, "endOffset": 71}, {"referenceID": 28, "context": "In Table 2 we show both the scores reported in [29] and our implementation.", "startOffset": 47, "endOffset": 51}, {"referenceID": 28, "context": "Note that our implementation of [29] gives slightly improved result over what they reported.", "startOffset": 32, "endOffset": 36}], "year": 2017, "abstractText": "Attention mechanisms have recently been introduced in deep learning for various tasks in natural language processing and computer vision. But despite their popularity, the \u201ccorrectness\u201d of the implicitly-learned attention maps has only been assessed qualitatively by visualization of several examples. In this paper we focus on evaluating and improving the correctness of attention in neural image captioning models. Specifically, we propose a quantitative evaluation metric for how well the attention maps align with human judgment, using recently released datasets with alignment between regions in images and entities in captions. We then propose novel models with different levels of explicit supervision for learning attention maps during training. The supervision can be strong when alignment between regions and caption entities are available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing supervision of attention maps during training solidly improves both attention correctness and caption quality.", "creator": "LaTeX with hyperref package"}}}