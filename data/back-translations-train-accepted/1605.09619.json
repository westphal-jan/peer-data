{"id": "1605.09619", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Horizontally Scalable Submodular Maximization", "abstract": "A variety of large-scale machine learning problems can be cast as instances of constrained submodular maximization. Existing approaches for distributed submodular maximization have a critical drawback: The capacity - number of instances that can fit in memory - must grow with the data set size. In practice, while one can provision many machines, the capacity of each machine is limited by physical constraints. We propose a truly scalable approach for distributed submodular maximization under fixed capacity. The proposed framework applies to a broad class of algorithms and constraints and provides theoretical guarantees on the approximation factor for any available capacity. We empirically evaluate the proposed algorithm on a variety of data sets and demonstrate that it achieves performance competitive with the centralized greedy solution.", "histories": [["v1", "Tue, 31 May 2016 13:18:30 GMT  (464kb,D)", "http://arxiv.org/abs/1605.09619v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.DC cs.DM cs.LG", "authors": ["mario lucic", "olivier bachem", "morteza zadimoghaddam", "andreas krause 0001"], "accepted": true, "id": "1605.09619"}, "pdf": {"name": "1605.09619.pdf", "metadata": {"source": "CRF", "title": "Horizontally Scalable Submodular Maximization", "authors": ["Mario Lucic", "Olivier Bachem", "Morteza Zadimoghaddam", "Andreas Krause"], "emails": ["LUCIC@INF.ETHZ.CH", "OLIVIER.BACHEM@INF.ETHZ.CH", "ZADIM@GOOGLE.COM", "KRAUSEA@ETHZ.CH"], "sections": [{"heading": "1. Introduction", "text": "As such, many important machine learning problems can be formulated as limited submodular maximization, such as influence maximization (Kempe et al., 2003), document summary (Lin & Bilmes, 2011), and active learning (Golovin & Krause, 2011). Solving these optimization problems on a massive scale is a major challenge, and the trade in computer resources and approximation quality is becoming a Paramount. Therefore, distributed computing has received a lot of interest. One of the most desirable features of dis-Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W & CP Volume 48. Copyright 2016 by the author (s).driven algorithms are horizontal scaling - scaling to larger instances by adding fixed capacity machines. Prominent approaches in distributed subdular strategies are partially based on two strategies."}, {"heading": "2. Background and Related Work", "text": "Let us put the principles with the highest marginal achievements (1 \u2212 1) to the test. We must focus on the problem of maximizing a non-negative monotonous submodular function subject to a cardinality constraint. In the face of an integer k, the maximizer of f isOPT = arg max S (S). The classical GREEDY algorithms et al., 1978), which begins with an empty sentence, we will achieve a higher degree of marginal achievements. (S) The classical GREEDY algorithms (S), which begin with an empty sentence. (S) The classical GREEDY algorithms et al. (S) The classical GREEDY algorithms et al. (S), 1978), which begin with an empty sentence and iterative achievements of the highest marginal achievements. (S) The classical GREEDY algorithms et al. (S)."}, {"heading": "3. A Practical Multi-Round Framework", "text": "In contrast to the previous approaches, we focus on scalable submodular maximization with fixed machine capacities. We first focus on cardinal restricted optimization and later extend the results to optimization under hereditary constraints. To this end, we first formally define the problem and introduce the Framework.Problem definition. Our goal is to have defined a monotonous, non-negative submodular function f that has a specified size n and an integer k. In addition, we have access to a potentially unlimited number of machines M in which each machine has limited capacities. Our goal is to calculate a solution that is not negative."}, {"heading": "3.1. Approximation Factor for Cardinality Constraints", "text": "The following theorems refer to the available capacity with the approximate guarantee of the proposed distributed multiround frame. Theorem 3.3. Let f be a monotonous, non-negative submodular function defined by the base set V of cardinality n, and k the cardinality limitation. Let A in algorithm 1 is a \u03b2-nice algorithm and the capacity of each machine. Then, algorithm 1 yields a set S of size at most k withE [f (S)] n 1 + \u03b2 f (OPT) n 2 + \u03b2) f material value if \u00b5 n 2 (OPT) f > nice algorithm and \u00b5 the capacity of each machine. (1 + \u03b2) f \u00b7 \u03b2 S the size at most k withE [f (S)."}, {"heading": "3.2. Approximation Factor for Hereditary Constraints", "text": "The constraint is hereditary if all subsets of S are also contained in I. In a submodular way, the goal is to find a workable system that captures independent sets of the Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid-Matroid"}, {"heading": "4. Experiments", "text": "We first show that the proposed algorithm scales horizontally, then we show that the available capacity has no significant effect on the approximation quality. We compare the performance of the proposed algorithm with the two-phase RANDGREEDI, the centralized GREEDY run over the entire dataset, and a randomly selected subset of size k. Finally, we investigate the use of STOCHASTIC GREEDY as a compression submethod in a series of large-scale experiments."}, {"heading": "4.1. Data Sets", "text": "CSN. The Community Seismic Network uses smartphones with accelerometers as low-cost seismometers to detect earthquakes. In Faulkner et al. (2011), 7 GB of acceleration data were recorded from volunteers who wore their phones under normal conditions (walking, speaking, on their desks, etc.). From this data, 17-dimensional feature vectors were calculated (containing frequency information, moments, etc.).TINY IMAGES. In our experiments, we used two subsets of the 32 x 32 RGB data set, each represented as a 3,072 dimensional vector (Torralba et al., 2008). We normalized the vectors to zero mean and standard. Following Mirzasoleiman et al. (2013), we selected a fixed random sub-sample of 10,000 elements for evaluation on each machine. PARKINSONS The data set consists of 5875 biomedical people with an early-onset of 22 stimuli."}, {"heading": "4.2. Objective Functions", "text": "A classic way to select a set of examples that best represent a massive dataset is to solve the k-medoid problem (Kaufman & Rousseeuw, 1987) by minimizing the sum of the pair differences between Examples A and elements of dataset V. This problem can be converted into a submodular maximization problem that is subject to a cardinality constraint, as follows: First, we define L (S) = L ({e0}) - S d (e, v), where d: V \u00b7 V \u2192 R + is a distance function that encodes the dissimilarity between elements; then, the function that measures the reduction of quantization errors by defining elements in S, f) = L ({e0}) - L (S {e0}), is monotonous submodular and maximimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimimi"}, {"heading": "4.3. Impact of Capacity on the Approximation factor", "text": "In the first set of experiments, we examine the objective function value as a function of the available capacity. We consider three basic methods: random subset of k elements, centralized GREEDY to the complete dataset and the two-round RANDGREEDI by Barbosa et al. (2015a). We use the lazy variant of the GREEDY algorithm (Minoux, 1978) as \u03b2-nice algorithm in our multiround proposal. For each algorithm, we report the ratio between the obtained function value and the average value obtained by the centralized GREEDY of more than 10 trials.Figures 2 (a) and (c) show the results of the active set selection with k = 50. Figures 2 (b) and (d) show the results of the copy-based cluster formation with k = 50. The vertical line represents the lower limit of the required capacity of the two-round algorithms with."}, {"heading": "4.4. Large-scale Experiments", "text": "In the second set of experiments, we apply both GREEDY and STOCHASTIC GREEDY (TREE and STOCHASTICTREE, respectively) as sub-procedures. We consider the complete data set of WEBSCOPE and a subset of 1,000 000TINY IMAGES. Capacity is fixed at a small percentage of the basic quantity (0.05% and 0.1%, respectively). In addition, we consider two cases of STOCHASTIC GREEDY, one with \u03b5 = 0.5 and the other with \u03b5 = 0.2, both with a capacity of 0.05% of the basic quantity. Figure 2 (e) shows the results of active quantity selection on WEBSCOPE. We note that both versions of the proposed algorithm agree with the performance of centralized GREEDY, even if the available capacity is extremely limited. Figure 2 (f) shows the results of exemplar-based clustering on TINY."}, {"heading": "5. Conclusion", "text": "Existing approaches to distributed submodular maximization are based on the implicit assumption that the capacity of each machine increases as the size of the data set increases. To our knowledge, we present the first framework for restricted distributed submodular maximization that is horizontally scalable: it scales to larger problem cases by deploying more machines with limited capacity. Our framework is based on a multi-round approach where a fraction of the elements are discarded in each round until all remaining elements fit on a machine. We offer approximation guarantees for distributed submodular maximization under cardinality constraints and extend the results to hereditary constraints. The proposed framework adapts to the available capacity: If the capacity is larger than the size of the data set, it emulates the centralized GREEDY method. If the capacity is at least one million, it is reduced to the existing two-round approaches. Otherwise, multiple rounds are preceded."}, {"heading": "Acknowledgements", "text": "We thank the reviewers for their insightful comments. This research was partially supported by ERC StG 307036 and the Zurich Information Security Centre."}, {"heading": "A. Detailed Analysis of Lemma 3.4", "text": "The submodularity property for X Y V and item x V\\ Y can now be expressed as (x, X) (x, Y) (x, Y) (x, Y) (x, Y). Leave OPT = arg max. (x, Y) (x, Y) (x, Y). Leave OPT = arbitrary subset B V, and a random partition of B into L-sets T1, T2, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 TL-Si is the set of selected items in the partial solutions OPTS def = OPT mi = 1Si mi (x).Lemma 3.4. Let Si be the output of algorithms referring to Ti i i i i i i i (x). If A is nice, for each subset C B the size is the most."}], "references": [{"title": "Fast algorithms for maximizing submodular functions", "author": ["Badanidiyuru", "Ashwinkumar", "Vondr\u00e1k", "Jan"], "venue": "In ACMSIAM Symposium on Discrete Algorithms,", "citeRegEx": "Badanidiyuru et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Badanidiyuru et al\\.", "year": 2014}, {"title": "The power of randomization: Distributed submodular maximization on massive datasets", "author": ["Barbosa", "Rafael", "Ene", "Alina", "Nguyen", "Huy L", "Ward", "Justin"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Barbosa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barbosa et al\\.", "year": 2015}, {"title": "A new framework for distributed submodular maximization", "author": ["Barbosa", "Rafael da Ponte", "Ene", "Alina", "Nguyen", "Huy L", "Ward", "Justin"], "venue": "arXiv preprint arXiv:1507.03719,", "citeRegEx": "Barbosa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barbosa et al\\.", "year": 2015}, {"title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization", "author": ["Golovin", "Daniel", "Krause", "Andreas"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Golovin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Golovin et al\\.", "year": 2011}, {"title": "Clustering by means of medoids", "author": ["Kaufman", "Leonard", "Rousseeuw", "Peter"], "venue": "Statistical Data Analysis Based on the L1-Norm and Related Methods, pp. North\u2013Holland,", "citeRegEx": "Kaufman et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Kaufman et al\\.", "year": 1987}, {"title": "Maximizing the spread of influence through a social network", "author": ["Kempe", "David", "Kleinberg", "Jon", "Tardos", "\u00c9va"], "venue": "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Kempe et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kempe et al\\.", "year": 2003}, {"title": "Submodular function maximization", "author": ["Krause", "Andreas", "Golovin", "Daniel"], "venue": "Tractability: Practical Approaches to Hard Problems,", "citeRegEx": "Krause et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2012}, {"title": "Fast greedy algorithms in mapreduce and streaming", "author": ["Kumar", "Ravi", "Moseley", "Benjamin", "Vassilvitskii", "Sergei", "Vattani", "Andrea"], "venue": "In ACM Symposium on Parallelism in Algorithms and Architectures,", "citeRegEx": "Kumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2013}, {"title": "A class of submodular functions for document summarization. In Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pp. 510\u2013520", "author": ["Lin", "Hui", "Bilmes", "Jeff"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Lin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "Accelerated greedy algorithms for maximizing submodular set functions", "author": ["Minoux", "Michel"], "venue": "In Optimization Techniques,", "citeRegEx": "Minoux and Michel.,? \\Q1978\\E", "shortCiteRegEx": "Minoux and Michel.", "year": 1978}, {"title": "Randomized composable core-sets for distributed submodular maximization", "author": ["Mirrokni", "Vahab", "Zadimoghaddam", "Morteza"], "venue": "In ACM Symposium on Theory of Computing,", "citeRegEx": "Mirrokni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mirrokni et al\\.", "year": 2015}, {"title": "Distributed submodular maximization: Identifying representative elements in massive data", "author": ["Mirzasoleiman", "Baharan", "Karbasi", "Amin", "Sarkar", "Rik", "Krause", "Andreas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mirzasoleiman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mirzasoleiman et al\\.", "year": 2013}, {"title": "Lazier than lazy greedy", "author": ["Mirzasoleiman", "Baharan", "Badanidiyuru", "Ashwinkumar", "Karbasi", "Amin", "Vondrak", "Jan", "Krause", "Andreas"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Mirzasoleiman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mirzasoleiman et al\\.", "year": 2015}, {"title": "An analysis of approximations for maximizing submodular set functions-I", "author": ["Nemhauser", "George L", "Wolsey", "Laurence A", "Fisher", "Marshall L"], "venue": "Mathematical Programming,", "citeRegEx": "Nemhauser et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Nemhauser et al\\.", "year": 1978}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Torralba", "Antonio", "Fergus", "Rob", "Freeman", "William T"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Torralba et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 1958}], "referenceMentions": [{"referenceID": 5, "context": "Examples include influence maximization (Kempe et al., 2003), document summarization (Lin & Bilmes, 2011), and active learning (Golovin & Krause, 2011).", "startOffset": 40, "endOffset": 60}, {"referenceID": 11, "context": "It was recently shown that this class of algorithms enjoys good empirical and theoretical performance, often matching those of the centralized solution (Mirzasoleiman et al., 2013).", "startOffset": 152, "endOffset": 180}, {"referenceID": 11, "context": "For the popular GREEDI algorithm, this occurs if the capacity is less than O( \u221a nk) (Mirzasoleiman et al., 2013).", "startOffset": 84, "endOffset": 112}, {"referenceID": 13, "context": "The classic GREEDY algorithm (Nemhauser et al., 1978) which starts with an empty set and iteratively adds items with highest marginal gain achieves a (1 \u2212 1/e)approximation.", "startOffset": 29, "endOffset": 53}, {"referenceID": 8, "context": "Mirzasoleiman et al. (2013) developed the first two-round algorithm whereby in the first round the data is arbitrarily partitioned to m machines and each machine runs GREEDY to select at most k items.", "startOffset": 0, "endOffset": 28}, {"referenceID": 1, "context": "Recently, Barbosa et al. (2015a) showed that instead of assuming an arbitrary partitioning of the data, one obtains an approximation factor of (1\u22121/e)/2 if the data is randomly partitioned.", "startOffset": 10, "endOffset": 33}, {"referenceID": 1, "context": "Recently, Barbosa et al. (2015a) showed that instead of assuming an arbitrary partitioning of the data, one obtains an approximation factor of (1\u22121/e)/2 if the data is randomly partitioned. Mirrokni & Zadimoghaddam (2015) prove that GREEDY computes representative subsets \u2013 coresets, and that by selecting O(k) instead of k items on each machine in the first round one obtains an approximation factor of 0.", "startOffset": 10, "endOffset": 222}, {"referenceID": 1, "context": "Recently, Barbosa et al. (2015a) showed that instead of assuming an arbitrary partitioning of the data, one obtains an approximation factor of (1\u22121/e)/2 if the data is randomly partitioned. Mirrokni & Zadimoghaddam (2015) prove that GREEDY computes representative subsets \u2013 coresets, and that by selecting O(k) instead of k items on each machine in the first round one obtains an approximation factor of 0.545. The implicit assumption in these algorithms is that that the capacity per machine \u03bc \u2265 max(n/m,mk), which implies \u03bc \u2265 \u221a nk, optimizing overm. Hence, the capacity needs to grow with the data set size! As a result, they are not truly horizontally scalable, as the maximum available memory on each machine is essentially fixed. Kumar et al. (2013) provide two multi-round algorithms for cardinality constrained submodular maximization.", "startOffset": 10, "endOffset": 755}, {"referenceID": 1, "context": "Recently, Barbosa et al. (2015a) showed that instead of assuming an arbitrary partitioning of the data, one obtains an approximation factor of (1\u22121/e)/2 if the data is randomly partitioned. Mirrokni & Zadimoghaddam (2015) prove that GREEDY computes representative subsets \u2013 coresets, and that by selecting O(k) instead of k items on each machine in the first round one obtains an approximation factor of 0.545. The implicit assumption in these algorithms is that that the capacity per machine \u03bc \u2265 max(n/m,mk), which implies \u03bc \u2265 \u221a nk, optimizing overm. Hence, the capacity needs to grow with the data set size! As a result, they are not truly horizontally scalable, as the maximum available memory on each machine is essentially fixed. Kumar et al. (2013) provide two multi-round algorithms for cardinality constrained submodular maximization. The THRESHOLDMR algorithm provides a (1/2\u2212\u03b5)approximation with high probability, inO(1/\u03b4) rounds and using O ( kn log n ) memory per machine. Furthermore, a variant of the algorithm, GREEDYSCALING, provides a (1\u22121/e)/(1 + \u03b5) approximation inO((log \u2206)/\u03b5\u03b4) rounds using O(n log(n)/\u03bc) machines with O ( kn log n ) capacity per machine, with high probability. There are four critical differences with respect to our work. Firstly, each call to the pruning procedure requires a capacity of \u221a 2nk log n, with high probability. However, given this capacity our algorithm always terminates in two rounds and we empirically observe approximation ratios very close to one. Secondly, our algorithm requires capacity of kn to complete in 1/\u03b4 rounds, while their result requires capacity of kn log n. As a result, we require capacity greater than k, while they require capacity of at least k log n which grows with n. In the case of GREEDYSCALING the number of rounds is even greater than O(1/\u03b4), especially for a small \u03b5 or a large \u2206. Thirdly, for the THRESHOLDMR algorithm one needs to try log(\u2206)/ thresholds which necessitates n(log \u2206)/(\u03bc ) machines compared to our n/\u03bc machines, which is optimal and critical in practice where small \u03b5 is desired. Finally, the pruning subprocedure that the aforementioned approach relies on has to be monotone, which is not the case for both the classic greedy and stochastic greedy. In contrast, we can use their thresholding-based algorithm as a compression subprocedure. Barbosa et al. (2015b) proved that for any sequential algorithm that achieves an approximation factor of \u03b1 and is consistent, there exists a randomized distributed algorithm that achieves a (\u03b1 \u2212 \u03b5)-approximation with constant probability in O(1/\u03b5) rounds.", "startOffset": 10, "endOffset": 2364}, {"referenceID": 12, "context": "The STOCHASTIC GREEDY algorithm (Mirzasoleiman et al., 2015) is a good choice for some problems.", "startOffset": 32, "endOffset": 60}, {"referenceID": 1, "context": "The first two cases are due to the classic analysis of GREEDY and the result of Barbosa et al. (2015a), respectively.", "startOffset": 80, "endOffset": 103}, {"referenceID": 1, "context": "The first two cases are due to the classic analysis of GREEDY and the result of Barbosa et al. (2015a), respectively. We will focus on the third case in which the limited machine capacity gives rise to multiple rounds. To estimate the quality of the compression scheme, we will track how much of OPT is pruned in each round. Clearly, losing a constant fraction would lead to an exponential decrease of the approximation quality with respect to the number of rounds. A more promising approach is based on bounding the additive loss incurred in each round. The following Lemma is a generalization of a result from Mirrokni & Zadimoghaddam (2015) in that it holds for any subset, not only OPT.", "startOffset": 80, "endOffset": 644}, {"referenceID": 1, "context": "The proof builds upon the rather elegant technique from Barbosa et al. (2015a). We start with defining the Lov\u00e1sz extension of submodular function f .", "startOffset": 56, "endOffset": 79}, {"referenceID": 11, "context": "Following Mirzasoleiman et al. (2013), we select a fixed random subsample of 10 000 elements for evaluation on each machine.", "startOffset": 10, "endOffset": 38}, {"referenceID": 1, "context": "We consider three baseline methods: random subset of k elements, centralized GREEDY on the full data set, and the two-round RANDGREEDI by Barbosa et al. (2015a). We use the lazy variant of the GREEDY algorithm (Minoux, 1978) as the \u03b2-nice algorithm in our multi-round proposal.", "startOffset": 138, "endOffset": 161}], "year": 2016, "abstractText": "A variety of large-scale machine learning problems can be cast as instances of constrained submodular maximization. Existing approaches for distributed submodular maximization have a critical drawback: The capacity \u2013 number of instances that can fit in memory \u2013 must grow with the data set size. In practice, while one can provision many machines, the capacity of each machine is limited by physical constraints. We propose a truly scalable approach for distributed submodular maximization under fixed capacity. The proposed framework applies to a broad class of algorithms and constraints and provides theoretical guarantees on the approximation factor for any available capacity. We empirically evaluate the proposed algorithm on a variety of data sets and demonstrate that it achieves performance competitive with the centralized greedy solution.", "creator": "LaTeX with hyperref package"}}}