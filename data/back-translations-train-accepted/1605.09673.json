{"id": "1605.09673", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Dynamic Filter Networks", "abstract": "In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.", "histories": [["v1", "Tue, 31 May 2016 15:29:36 GMT  (2163kb,D)", "http://arxiv.org/abs/1605.09673v1", "submitted to NIPS16"], ["v2", "Mon, 6 Jun 2016 15:39:10 GMT  (2242kb,D)", "http://arxiv.org/abs/1605.09673v2", "submitted to NIPS16; X. Jia and B. De Brabandere contributed equally to this work and are listed in alphabetical order"]], "COMMENTS": "submitted to NIPS16", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["xu jia", "bert de brabandere", "tinne tuytelaars", "luc van gool"], "accepted": true, "id": "1605.09673"}, "pdf": {"name": "1605.09673.pdf", "metadata": {"source": "CRF", "title": "Dynamic Filter Networks", "authors": ["Bert De Brabandere", "Xu Jia", "Tinne Tuytelaars"], "emails": ["firstname.lastname@esat.kuleuven.be", "vangool@vision.ee.ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "2 Related Work", "text": "In fact, it is so that most of them are able to survive themselves without being able to survive themselves. In fact, it is so that they are able to survive themselves, and that they are able to survive themselves. In fact, it is so that they are able to survive themselves, and that they are able to survive themselves. In fact, it is so that they are able to survive themselves."}, {"heading": "3.1 Filter-Generating Network", "text": "The filter generator network takes one input IA-Rh \u00b7 w \u00b7 cA, where h, w and cA are height, width and number of channels of input A. There are filters that are parameterized by the parameters \u00da Rs \u00b7 s \u00b7 cB \u00b7 n \u00b7 d, where s is the filter size, cB is the number of channels in input B and n is the number of filters. d is equal to 1 for dynamic folding and h \u00b7 w for dynamic local filtering, which we will discuss below. The filters are applied to the input IB-Rh \u00b7 w \u00b7 cB to generate an output G = F\u03b8 (IB), with G-Rh \u00b7 w \u00b7 n. The filter size s determines the receptive field and is chosen depending on the application. The size of the receptive field can also be increased by stacking several dynamic filter modules. This is useful, for example, for applications that can include large local displacements. The filter network can be implemented with any architecture."}, {"heading": "3.2 Dynamic Filtering Layer", "text": "The Dynamic Filter Layer records images or characteristic maps, which IB as input and outputs the filtered result G-Rh \u00b7 w \u00b7 n. For simplicity we consider only a single characteristic layer (cB = 1), which is filtered with a single generated filter (n = 1), but this is not required in a general setting. The Dynamic Filter Layer can be instantiated as a dynamic revolutionary layer or a dynamic local filter layer. The Dynamic Convolutionary Layer is similar to a traditional revolutionary layer, in which the same filter layer is applied at each position of the input layer IB. But unlike the traditional revolutionary layer, where the filter weights are model parameters, in a dynamic revolutionary layer the filter parameters are dynamically generated by a filter generation-generating dynamic network: G (i, j) = Filtering layer is applied at each position of the input layer dynamic, the filter parameters are dynamically generated by a filter layer."}, {"heading": "3.3 Relationship with other networks", "text": "The generic formulation of our approach makes it possible to draw parallels with other networks in the world."}, {"heading": "4.1 Learning steerable filters", "text": "First, we performed a simple experiment to illustrate the basics of the Dynamic Filter Module with a Dynamic Folding Layer. The task is to filter an input image with a steerable filter of a given orientation \u03b8. The network must learn this transformation from viewing input-output pairs consisting of randomly selected input images and angles along with their corresponding output. Here, the task of the filter-generating network is to convert an angle into a filter, which is then applied to the input image to generate the final output. We implement the filter-generating network as a pair of fully connected layers with the last layer containing 81 neurons, corresponding to the elements of a 9x9 folding filter. Figure 4 shows an example of the trained network. It has actually learned the expected filters and applies the correct transformation to the image."}, {"heading": "4.2 Video prediction", "text": "The architecture of our model is illustrated in Table 1 (right). The task is to predict the sequence of future frames that will directly follow the input systems. To solve this task, we use the conventional encoder decoder as a filter system that exploits the spatial correlation within a frame. It generates characteristics that relate to each layer. To exploit the temporary correlations, we add the temporary correlations between the frames that we use within a frame. We run the previous hidden state through two conventional layers and record the sum of each layer."}, {"heading": "4.3 Stereo prediction", "text": "We also apply the Dynamic Filter Network to the task of the stereo prediction, which we define as predicting the right view, since it is the left view of two cameras with horizontal disparity. This task is a variant of the video prediction, where the goal is to predict a new view in space, not time, from a single image and not from several. Flynn et al. [5] developed a deep network for synthesizing new views from multiple views in unrestricted environments such as museums, parks, and streets. We limit ourselves to the more structured Highway Driving dataset and a classic stereo with two views. We recycle the architecture from the previous section, but drop the recursive link that is used to model temporal correlations. Furthermore, based on the assumption that corresponding points are on the same horizontal line, we replace the square 9x9 filter with the same horizontal link as the previous 13x1 and 46x1 are evaluated on the left side of the same network."}, {"heading": "5 Conclusion", "text": "In this paper, we introduced Dynamic Filter Networks, a class of networks that apply dynamically generated filters to a sample specific image. We discussed two versions: dynamic folding and dynamic local filtering. We validated our framework in the context of controllable filters, video forecasting, and stereo forecasting. In the future, we plan to explore the potential of dynamic filter networks in other tasks, such as fine-grained image classification, where filters could learn to adjust to the object position, or image blurring, where filters can be set to adjust to the image structure."}, {"heading": "B Stereo Prediction", "text": "We create a video with stereo prediction results for the entire test sequence, which can be found at https: / / drive.google.com / file / d / 0B2k _ yg56pxkxVFFWMDVycXg3Qmc / view? usp = sharing. In each frame, the first frame is the left view, the second the visualization of filters, the third our prediction of the right view, and the last the basic truth."}], "references": [{"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["S\u00f8ren Kaae S\u00f8nderby"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Lasagne: First release", "author": ["Sander Dieleman", "Jan Schl\u00fcter", "Colin Raffel", "Eben Olson", "S\u00f8ren Kaae S\u00f8nderby", "Daniel Nouri", "Daniel Maturana", "Martin Thoma", "Eric Battenberg"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["Alexey Dosovitskiy", "Philipp Fischer", "Eddy Ilg", "Philip H\u00e4usser", "Caner Hazirbas", "Vladimir Golkov", "Patrick van der Smagt", "Daniel Cremers", "Thomas Brox"], "venue": "In ICCV,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Deepstereo: Learning to predict new views from the world\u2019s imagery", "author": ["John Flynn", "Ivan Neulander", "James Philbin", "Noah Snavely"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Learning to linearize under uncertainty", "author": ["Ross Goroshin", "Micha\u00ebl Mathieu", "Yann LeCun"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In ICCV,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Spatial transformer networks", "author": ["Max Jaderberg", "Karen Simonyan", "Andrew Zisserman", "Koray Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Deep multi-scale video prediction beyond mean square", "author": ["Micha\u00ebl Mathieu", "Camille Couprie", "Yann LeCun"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["Hyeonwoo Noh", "Paul Hongsuck Seo", "Bohyung Han"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["Junhyuk Oh", "Xiaoxiao Guo", "Honglak Lee", "Richard L. Lewis", "Satinder P. Singh"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Spatio-temporal video autoencoder with differentiable memory", "author": ["Viorica Patraucean", "Ankur Handa", "Roberto Cipolla"], "venue": "CoRR, abs/1511.06309,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["Marc\u2019Aurelio Ranzato", "Arthur Szlam", "Joan Bruna", "Micha\u00ebl Mathieu", "Ronan Collobert", "Sumit Chopra"], "venue": "CoRR, abs/1412.6604,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Convolutional LSTM network: A machine learning approach for precipitation nowcasting", "author": ["Xingjian Shi", "Zhourong Chen", "Hao Wang", "Dit-Yan Yeung", "Wai-Kin Wong", "Wang-chun Woo"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov"], "venue": "In ICML,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Training very deep networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks", "author": ["Junyuan Xie", "Ross Girshick", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1604.03650,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Weakly-supervised disentangling with recurrent transformations for 3d view synthesis", "author": ["Jimei Yang", "Scott Reed", "Ming-Hsuan Yang", "Honglak Lee"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Rotating your face using multi-task deep neural network", "author": ["Junho Yim", "Heechul Jung", "ByungIn Yoo", "Changkyu Choi", "Du-Sik Park", "Junmo Kim"], "venue": "In CVPR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": "[23] and Yang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] learn to rotate a given face to another pose.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The authors of [17, 19, 18, 16, 13] train a deep neural network to predict subsequent video frames.", "startOffset": 15, "endOffset": 35}, {"referenceID": 18, "context": "The authors of [17, 19, 18, 16, 13] train a deep neural network to predict subsequent video frames.", "startOffset": 15, "endOffset": 35}, {"referenceID": 17, "context": "The authors of [17, 19, 18, 16, 13] train a deep neural network to predict subsequent video frames.", "startOffset": 15, "endOffset": 35}, {"referenceID": 15, "context": "The authors of [17, 19, 18, 16, 13] train a deep neural network to predict subsequent video frames.", "startOffset": 15, "endOffset": 35}, {"referenceID": 12, "context": "The authors of [17, 19, 18, 16, 13] train a deep neural network to predict subsequent video frames.", "startOffset": 15, "endOffset": 35}, {"referenceID": 4, "context": "[5] use a deep network to interpolate between views separated by a wide baseline.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] propose a module called Spatial Transformer, which allows the network to actively spatially transform feature maps conditioned on themselves without explicit supervision.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] extend the Spatial Transformer by modifying the grid generator such that it has one transformation for each position, instead of a single transformation for the entire image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] introduce a dynamic parameter layer whose output is used as parameters of a fully connected layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] use an encoder-decoder framework in a way similar to language modeling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] propose a multilayer LSTM based autoencoder for both past frames reconstruction and future frames prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] who propose to use convolutional LSTM to replace the fully connected LSTM in the network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] address the problem of predicting future frames conditioned on both previous frames and actions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] manage to generate reasonably sharp frames by means of a multi-scale architecture, an adversarial training method, and an image gradient difference loss function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5]", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[21] for 2D-to-3D conversion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "None of these works adapt the filter operations of the network to the specific input sample, as we do, with the exception of [5, 21].", "startOffset": 125, "endOffset": 132}, {"referenceID": 20, "context": "None of these works adapt the filter operations of the network to the specific input sample, as we do, with the exception of [5, 21].", "startOffset": 125, "endOffset": 132}, {"referenceID": 19, "context": "Shortcut connections Our work also shares some similarity, through the use of shortcut connections, with the highway network [20] and the residual network [8, 10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 7, "context": "Shortcut connections Our work also shares some similarity, through the use of shortcut connections, with the highway network [20] and the residual network [8, 10].", "startOffset": 155, "endOffset": 162}, {"referenceID": 9, "context": "Shortcut connections Our work also shares some similarity, through the use of shortcut connections, with the highway network [20] and the residual network [8, 10].", "startOffset": 155, "endOffset": 162}, {"referenceID": 7, "context": "[8, 10] propose to reformulate layers as learning residual functions instead of learning unreferenced functions.", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[8, 10] propose to reformulate layers as learning residual functions instead of learning unreferenced functions.", "startOffset": 0, "endOffset": 7}, {"referenceID": 10, "context": "Here we discuss the relation with the spatial transformer networks [11], the deep stereo network [5, 21], and the residual networks [8, 10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "Here we discuss the relation with the spatial transformer networks [11], the deep stereo network [5, 21], and the residual networks [8, 10].", "startOffset": 97, "endOffset": 104}, {"referenceID": 20, "context": "Here we discuss the relation with the spatial transformer networks [11], the deep stereo network [5, 21], and the residual networks [8, 10].", "startOffset": 97, "endOffset": 104}, {"referenceID": 7, "context": "Here we discuss the relation with the spatial transformer networks [11], the deep stereo network [5, 21], and the residual networks [8, 10].", "startOffset": 132, "endOffset": 139}, {"referenceID": 9, "context": "Here we discuss the relation with the spatial transformer networks [11], the deep stereo network [5, 21], and the residual networks [8, 10].", "startOffset": 132, "endOffset": 139}, {"referenceID": 10, "context": "Spatial Transformer Networks The proposed dynamic filter network shares the same philosophy as the spatial transformer network proposed by [11], in that it applies a transformation conditioned on an input to a feature map.", "startOffset": 139, "endOffset": 143}, {"referenceID": 4, "context": "Deep Stereo The deep stereo network of [5] can be seen as a specific instantiation of a dynamic filter network with a local filtering layer where inputs IA and IB denote the same image, only a horizontal filter is generated and softmax is applied to each dynamic filter.", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "Residual Networks We can also draw some parallels with the recently introduced residual networks [8, 10].", "startOffset": 97, "endOffset": 104}, {"referenceID": 9, "context": "Residual Networks We can also draw some parallels with the recently introduced residual networks [8, 10].", "startOffset": 97, "endOffset": 104}, {"referenceID": 1, "context": "We use Theano [2] based library Lasagne [3] to implement all the experiments.", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "We use Theano [2] based library Lasagne [3] to implement all the experiments.", "startOffset": 40, "endOffset": 43}, {"referenceID": 18, "context": "Moving MNIST Model # params bce FC-LSTM [19] 142,667,776 341.", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "2 Conv-LSTM [18] 7,585,296 367.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "Note that we use a very simple recurrent architecture rather than the more advanced LSTM as in [19, 18].", "startOffset": 95, "endOffset": 103}, {"referenceID": 17, "context": "Note that we use a very simple recurrent architecture rather than the more advanced LSTM as in [19, 18].", "startOffset": 95, "endOffset": 103}, {"referenceID": 18, "context": "Moving MNIST We first evaluate the method on the moving MNIST dataset [19].", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "We follow the setting in [19], that is, given a sequence of 10 frames with 2 moving digits as input, we predict the following 10 frames.", "startOffset": 25, "endOffset": 29}, {"referenceID": 18, "context": "We use the code provided by [19] to generate training samples on-the-fly, and test it on the provided test set for comparison.", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "Only simple preprocessing is done to convert pixel values into the range [0,1].", "startOffset": 73, "endOffset": 78}, {"referenceID": 8, "context": "We initialize all model parameters using the method proposed in [9] and use the Adam optimizer [12] with a learning rate of 0.", "startOffset": 64, "endOffset": 67}, {"referenceID": 11, "context": "We initialize all model parameters using the method proposed in [9] and use the Adam optimizer [12] with a learning rate of 0.", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": "Our method outperforms the state-of-the-art [19, 18] and this with a much smaller model.", "startOffset": 44, "endOffset": 52}, {"referenceID": 17, "context": "Our method outperforms the state-of-the-art [19, 18] and this with a much smaller model.", "startOffset": 44, "endOffset": 52}, {"referenceID": 5, "context": "This issue could be alleviated with the methods proposed in [6, 7, 1].", "startOffset": 60, "endOffset": 69}, {"referenceID": 6, "context": "This issue could be alleviated with the methods proposed in [6, 7, 1].", "startOffset": 60, "endOffset": 69}, {"referenceID": 0, "context": "This issue could be alleviated with the methods proposed in [6, 7, 1].", "startOffset": 60, "endOffset": 69}, {"referenceID": 16, "context": "Compared to natural video like UCF101 used in [17, 13], the highway driving data is highly structured and much more predictable, making it a good testbed for video prediction.", "startOffset": 46, "endOffset": 54}, {"referenceID": 12, "context": "Compared to natural video like UCF101 used in [17, 13], the highway driving data is highly structured and much more predictable, making it a good testbed for video prediction.", "startOffset": 46, "endOffset": 54}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] developed a deep network for new view synthesis from multiple views in unconstrained settings like musea, parks and streets.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.", "creator": "LaTeX with hyperref package"}}}