{"id": "1605.09674", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "VIME: Variational Information Maximizing Exploration", "abstract": "Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.", "histories": [["v1", "Tue, 31 May 2016 15:34:36 GMT  (3212kb,D)", "http://arxiv.org/abs/1605.09674v1", null], ["v2", "Wed, 17 Aug 2016 18:25:42 GMT  (3260kb,D)", "http://arxiv.org/abs/1605.09674v2", null], ["v3", "Wed, 23 Nov 2016 12:58:44 GMT  (3454kb,D)", "http://arxiv.org/abs/1605.09674v3", "Published in \"Advances in Neural Information Processing Systems\" (NIPS 2016)"], ["v4", "Fri, 27 Jan 2017 09:26:28 GMT  (3635kb,D)", "http://arxiv.org/abs/1605.09674v4", "Published in Advances in Neural Information Processing Systems 29 (NIPS), pages 1109-1117"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO stat.ML", "authors": ["rein houthooft", "xi chen", "xi chen", "yan duan", "john schulman", "filip de turck", "pieter abbeel"], "accepted": true, "id": "1605.09674"}, "pdf": {"name": "1605.09674.pdf", "metadata": {"source": "META", "title": "Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks", "authors": ["Rein Houthooft", "Xi Chen", "Yan Duan", "John Schulman", "Filip De Turck", "Pieter Abbeel"], "emails": [], "sections": [{"heading": null, "text": "Scalable and effective exploration methods remain a key challenge in reinforcement learning (RL). While there are methods with optimal guarantees for setting discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. Therefore, most modern RL are based on simple heuristics such as greedy exploration or the addition of Gaussian noise to controls. In this paper, Variational Information Maximizing Exploration (VIME) is presented, an exploration strategy based on maximizing the information gain about the agent's belief in environmental dynamics. We propose a practical implementation using variation conclusions in neural networks that efficiently manage continuous state and action spaces. VIME modifies the MDP reward function and can be applied with several different underlying RL algorithms. We show that VIME achieves significantly better performance over multiple exploration tasks, including a very low number of exploration algorithms."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to move around without being able to play by the rules."}, {"heading": "2 Methodology", "text": "In Section 2.1, we specify the notation for the following equations. In Section 2.2, we explain the theoretical foundations of curious exploration. In Section 2.3, we describe how to adapt this idea to continuous control, and we show how to build on recent advances in the conclusion of variation for Bayesian Neural Networks (BNNs) to make this formulation practicable. Afterwards, we explicitly describe the intuitive link between improving compression and the lower limit of variation in Section 2.4. Finally, Section 2.5 describes how our method is put into practice."}, {"heading": "2.1 Preliminaries", "text": "This thesis assumes a discounted Markov decision-making process (MDP) with a finite horizon, defined by (S, A, P, R, \u03c10, \u03b3, T), where S'Rn is a state set, A'Rm is an action set, P: S \u00b7 A \u00b7 S \u2192 R \u2265 0 is a probability distribution of the transition, r: S \u00b7 A \u2192 R is a limited reward function, 0: S \u2192 R \u2265 0 is an initial state distribution, \u03b3 (0, 1] is a discount factor, and T is the horizon. States and actions that are considered random variables are abbreviated as S and A. The presented models are based on the optimization of a stochastic policy \u03c0\u03b1: S \u00d7 A \u2192 R \u2265 0, parameterized by \u03b1. Let us cite the expected discounted return: \u00b5 (\u03c0\u03b1) = E\u03c4 (\u03c0\u03b1) = E\u0442 [\u0445\u03b1) = T = 0 \u03b3tr (st, at)."}, {"heading": "2.2 Curiosity", "text": "Our method is based on the theory of curiosity-driven exploration [16, 17, 21, 22], in which the agent engages in systematic exploration by searching for regions that are relatively unexplored. The agent models environmental dynamics using a model p (st + 1 | st, at; \u03b8), but parameterized by the random variable p (p). Starting from a previous p (p), he receives a distribution via dynamic models through a distribution over the economy that is updated in Bayesian fashion (as opposed to an estimate of the point). The history of the agent up to the time of the step is referred to as [s1, a1, p]. According to curiosity-driven exploration [17], the agent should take measures that maximize the reduction of uncertainty about the dynamics. This can be called maximizing the sum of reproductions in entropy (H)."}, {"heading": "2.3 Variational Bayes", "text": "s asp (\u03b8, at, st + 1) = p (\u03b8, at) = p (namisian) p (st + 1 | 0, at) p (st + 1 | 0, at) p (st + 1 | 0, at), (4) p (\u03b8 | 0, at) = p (\u03b8 | 0, t), because actions do not affect beliefs about the environment [17]. Herein the denominator is calculated by the integralp (st + 1 | 0, at) = p (st + 1 | 0, at), because actions do not affect beliefs about the environment [17]. (Herein denominator is calculated by the integralp (st + 1 | 0, at) = p (st + 1 | 0, at), because actions do not affect beliefs about the environment [17]. (Herein denominator is calculated by the integralp (st + 1 | 0, at)."}, {"heading": "2.4 Compression", "text": "It is possible to find an interesting relationship between the improvement of compression - an intrinsic reward goal defined in [25] and the information gain of Eq. (2). In [25], the agent's curiosity is equivalent to the improvement of compression, measured by C (1) \u2212 C (2), where C (2) is the description length of the compression used as a model. Furthermore, it is known that the negative lower limit can be regarded as the description length [19]. Therefore, we can write the compression improvement as L [q; 3), like the compression improvement as p [4] \u2212 L [q; 1), as an alternative formulation of the lower limit of variation in Eq. (6), the compression improvement is stated as a significant value in Eq (D). (D) = L [q) = Compression improvement in Eq [4], Kp [5], Kp [5]."}, {"heading": "2.5 Implementation", "text": "The complete method is summarized in algorithm 1. First, we set the implementation and parameterization of the dynamics BNN. The BNN weight distribution q is given by the fully factorized Gaussian distribution. (12) Therefore, this method is described later in this section. (12) The standard deviation of the Gaussian mean vector and the covariance matrix diagonal are particularly convenient as it allows a simple analytical formulation of the KL divergence. (12) The standard deviation of the Gaussian BNN parameters is parameterized as a log. (1 + e2), with which the formation of the Dynamics BNN by optimization."}, {"heading": "3 Experiments", "text": "This year it is more than ever before in the history of the city."}, {"heading": "4 Related Work", "text": "Some of these algorithms are based on the principle of optimism under uncertainty: E3 [3], R-Max [4], UCRL [30]. An alternative approach is Bayesian learning methods of amplification, which maintain distribution over possible MDPs [1, 17, 23, 31]. Optimistic exploration strategies have been extended to continuous state spaces, for example [6, 9], but these methods do not match non-linear function approximators (as opposed to VIME). Practical RL algorithms often rely on simple exploration euristics, such as -greed and Boltzmann exploration."}, {"heading": "5 Conclusions", "text": "We have proposed Variational Information Maximizing Exploration (VIME), a curious exploration strategy for continuous control tasks. Conclusions of variation are used to approximate the posterior distribution of a Bayean neural network that represents the dynamics of the environment. Using the information gained in this learned dynamic model as an intrinsic reward allows the agent to simultaneously optimize both external reward and intrinsic surprise. Empirical results show that VIME performs significantly better than heuristic exploration methods in various continuous control tasks and algorithms. As a future work, we would like to investigate the measurement of surprise in the value function and the use of the learned dynamic model for planning."}, {"heading": "Acknowledgments", "text": "This work was partially supported by DARPA, the Berkeley Vision and Learning Center (BVLC), the Berkeley Artificial Intelligence Research (BAIR) Laboratory and Berkeley Deep Drive (BDD). Rein Houthooft is supported by a Ph.D. Fellowship of the Research Foundation - Flanders (FWO)."}, {"heading": "A Bayesian neural networks (BNNs)", "text": "We demonstrate the behavior of a BNN [1] when we are trained on simple regression data. Figure 1 shows a snapshot of the behavior of the BNN during the training. In this figure, the red dots represent the regression data, which have a 1 dimming input x and a 1 dimming output. Input to the BNN is constructed as x = [x, x2, x3, x4]. The green dots represent BNN predictions, each for a differently captured prediction value according to q (\u00b7; \u03c6). The color lines represent the output for different but fixed samples. The shaded areas represent the captured output plus minus one and two standard deviations. The figure shows that the BNN output in the training data area is very safe, while otherwise it has a high uncertainty. If we introduce data outside this training area, or data that differs significantly from the training data, they will have a high impact on the parameter q."}, {"heading": "B Experimental setup", "text": "In the case of the classic tasks CartPole, CartPoleSwingup, DoublePendulum and MountainCar, as well as in the case of the hierarchical task SwimmerGather, the dynamics BNN has a hidden layer of 32 units. For the motion tasks Walker2D and HalfCheetah, the dynamics BNN has two hidden layers of 64 units each. All hidden layers have a rectified linear unit (ReLU) nonlinearity is applied to the initial layer, while no nonlinearity is applied to the initial layer. The number of samples drawn to approximate the variable subordinate expectation is fixed at 10. The batch size for the political gradient methods is fixed at 5000 samples, except for the SwimmerGather task, where it is set at 50,000. The replay pool has a fixed size of 100,000 samples, with a minimum size of 500 samples for all but the SwimmerGather task."}], "references": [{"title": "Weight uncertainty in neural networks", "author": ["C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra"], "venue": "in ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Benchmarking deep reinforcement learning for continous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "in ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "For small tasks, this trade-off can be handled effectively through Bayesian RL [1] and PAC-MDP methods [2\u20136], which offer formal guarantees.", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "For small tasks, this trade-off can be handled effectively through Bayesian RL [1] and PAC-MDP methods [2\u20136], which offer formal guarantees.", "startOffset": 103, "endOffset": 108}, {"referenceID": 2, "context": "For small tasks, this trade-off can be handled effectively through Bayesian RL [1] and PAC-MDP methods [2\u20136], which offer formal guarantees.", "startOffset": 103, "endOffset": 108}, {"referenceID": 2, "context": "Some of these algorithms are based on the principle of optimism under uncertainty: E [3], R-Max [4], UCRL [30].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "Bayesian reinforcement learning methods, which maintain a distribution over possible MDPs [1, 17, 23, 31].", "startOffset": 90, "endOffset": 105}], "year": 2016, "abstractText": "Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as -greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent\u2019s belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.", "creator": "LaTeX with hyperref package"}}}