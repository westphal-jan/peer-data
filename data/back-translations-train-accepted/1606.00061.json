{"id": "1606.00061", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "abstract": "A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question and consequently the image via the co-attention mechanism in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN) model. Our final model outperforms all reported methods, improving the state-of-the-art on the VQA dataset from 60.4% to 62.1%, and from 61.6% to 65.4% on the COCO-QA dataset.", "histories": [["v1", "Tue, 31 May 2016 22:02:01 GMT  (4284kb,D)", "http://arxiv.org/abs/1606.00061v1", "11 pages, 7 figures, 3 tables"], ["v2", "Thu, 2 Jun 2016 01:51:13 GMT  (3549kb,D)", "http://arxiv.org/abs/1606.00061v2", "11 pages, 7 figures, 3 tables"], ["v3", "Wed, 26 Oct 2016 02:15:57 GMT  (3669kb,D)", "http://arxiv.org/abs/1606.00061v3", "11 pages, 7 figures, 3 tables"], ["v4", "Fri, 13 Jan 2017 16:18:03 GMT  (3669kb,D)", "http://arxiv.org/abs/1606.00061v4", "11 pages, 7 figures, 3 tables in 2016 Conference on Neural Information Processing Systems (NIPS)"], ["v5", "Thu, 19 Jan 2017 05:03:33 GMT  (3669kb,D)", "http://arxiv.org/abs/1606.00061v5", "11 pages, 7 figures, 3 tables in 2016 Conference on Neural Information Processing Systems (NIPS)"]], "COMMENTS": "11 pages, 7 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["jiasen lu", "jianwei yang", "dhruv batra", "devi parikh"], "accepted": true, "id": "1606.00061"}, "pdf": {"name": "1606.00061.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Co-Attention for Visual Question Answering", "authors": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "emails": ["parikh}@vt.edu"], "sections": [{"heading": "1 Introduction", "text": "In recent years, it has become clear that the problem is a global problem, that it is a global and global problem. In recent years, it has become clear that it is a global problem, that it is not only a global problem, but also a global and global problem. In recent years, it has become clear that it is a global and global problem."}, {"heading": "2 Related Work", "text": "We compare and relate our proposed co-attention mechanism with other visual and speech attention mechanisms in the literature. Image attention. Instead of directly using the holistic embedding of the overall image from the fully connected layer of a deep CNN (as in [2, 12-14]), a number of recent works have examined image attention models for VQA. Zhu et al. [25] add spatial attention to the standard LSTM model to show and establish QA. Andreas et al. [1] suggest a compositional scheme consisting of a language parser and a series of neural module networks. The language parser predicts which neuronal module authors should be inevitable to answer the question. One of these neural module networks involves participation in certain regions in an image. Some other works perform image attention calls multiple times in stacked fashion through."}, {"heading": "3 Method", "text": "In order to facilitate understanding, our complete model is described in parts. First, our hierarchical question is described in paragraph 3.2 and the proposed co-attention mechanism is described in paragraph 3.3. Finally, paragraph 3.4 shows how the questions and image functions dealt with can be combined recursively to produce answers."}, {"heading": "3.1 Notation", "text": "For a question with T-words, their representation is denoted by Q = {q1,.. qT}, where qt is the attribute vector for the t-th word. We denote qwt, q p t, and q s t as word embedding, phrase embedding, or question embedding at position. The attribute is denoted by V = {v1,..., vN}, where vn is the attribute vector at spatial position n. The attributes of the co-attention of image and question at each level in the hierarchy are denoted as v-r and q-r, where r-w, p, s}. Weights in various modules / levels are denoted by W, with corresponding sub / super scripts if necessary. In the following representation, we omit the bias term b to avoid notational disorder, but they are included in the model."}, {"heading": "3.2 Question Hierarchy", "text": "Given the 1-hot encoding of the question words Q = {q1,.., qT}, we first embed the words in a vector space (end-to-end learned) to obtain Qw = {qw1,..., qwT}. To calculate the phrase characteristics, we apply 1-D folding to the word embedding vectors. Specifically, at each word location we compute the inner product of the word vectors with filters of three window sizes: Unigram, Bigram, and Trigram. For the t-th word, the folding output with window size s is in byq-ps, t = tanh (W s c w: t + s \u2212 1), s series {1, 2, 3} (1) where W sc are the weight parameters. The word expression characteristics Q w are correspondingly 0-padded before they are inserted into bigrams and trigrams to maintain the length of the sequence according to the hierarchy."}, {"heading": "3.3 Co-Attention", "text": "We propose two co-attention mechanisms, which differ in the order in which images are generated and attention cards are questioned: the first mechanism, which we call parallel co-attention, simultaneously generates image and questions attention. The second mechanism, which we alternately call co-attention, alternates successively between the generation of image and question attention. See Fig. 2. These co-attention mechanisms are executed at all three levels of the question hierarchy."}, {"heading": "3.3.1 Parallel Co-Attention", "text": "Analogous to [22], we connect the image and the question by calculating the similarity between image and question features on all pairs of image and question points. Specifically, the affinity matrix C-RT-N is calculated by C-tanh (QTWbV) (3), where Wb-Rd-d contains the weights. After calculating this affinity matrix, one possible method of calculating image (or question) attention is simply to maximize affinity to the locations of other modality, i.e. av [n] = maxi (Ci, n) and aq [t] = maxj (Ct, j). Instead of selecting the maximum activation, we find that performance is improved when we consider this affinity matrix as a feature and the attention of the image qsoxi (Ci, n) and aq [t] = Wingj (Wt, j) = Wingv (Wingv)."}, {"heading": "3.3.2 Alternating Co-Attention", "text": "In this attention mechanism, we alternate successively between the generation of image and question attention. In short, it consists of three steps (marked in Fig. 2b): 1) we take the question in a single vector q; 2) we take care of the picture on the basis of the question summary q; 3) we take care of the question on the basis of the visited image attribute. Specifically, we define an attention operation x = A (X; g) in which the picture (or question) uses the characteristics X and the attention guidance derived from the question (or picture) g as input and outputs the treated image (or question) vector. The operation can be expressed in the following steps: H = tanh (WxX + (Wgg) 1 T) ax = softmax (wThxH) x = adapted axis xi (6), where 1: Rk is a vector with all elements."}, {"heading": "3.4 Encoding for Predicting Answers", "text": "In the following [2] we take the 1000 most common answers and treat VQA as a 1000-way classification problem. We predict the answer based on the commonly treated image and question characteristics of all three levels. We use a multi-layer perceptron (MLP) to encode the attention characteristics recursively, as shown in Figure 3 (b). Hw = tanh (Ww (q-w + v-w))))) hp = tanh (Wp [(q-p + v-p), hw]) hs = tanh (Ws [(q-s + v-s), hp]) p = softmax (Whhs) (7), where Ww, Wp, Ws and Wh are the weight parameters. [\u00b7] is the concatenation of two vectors. p is the probability of the final answer."}, {"heading": "4 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "We evaluate the proposed model using two sets of data, the VQA dataset [2] and the COCO QA dataset [14]. The dataset contains 248,349 training questions, 121,512 validation questions, and 244,302 test questions. There are three subcategories by answer type, including yes / no, number, and others. Each question has 10 answers with no answer. We use the 1000 most common answers as possible results similar [2]. This set of answers covers 86.54% of train + val answers. For testing, we train our model on VQA train + val and report the test dev and test standard results from the VQA evaluation server. We use the evaluation protocol of [2] in the experiment.COCO QA dataset is automatically extracted from the image captions in objects 736 and 711 questions in the Microsoft 78,4,000 questions category."}, {"heading": "4.2 Setup", "text": "We use the RmProp Optimizer [19] with a base learning rate of 4e-4, impulse 0.99 and weight decomposition 1e-8. We set the batch size to 300 and train up to 256 epochs with an early stop if validation accuracy has not improved over the last 5 epochs. At COCO-QA, the size of the hidden layer Ws is set to 512 and at VQA to 1024, as it is a much larger dataset. All other word embedding and hidden layers were vectors of size d = 512. We apply the dropout to each layer with a 0.5 probability. Following [23], we scale the image to 448 x 448 and then take the activation of the last pooling layer of VGGNet [17] or Deep Residual Network [6] as a feature. Our model is fully traceable, but we do not match the CNN Vasgips code: The VGGIP / Attgips is available at http: / /."}, {"heading": "4.3 Results and analysis", "text": "There are two test scenarios for VQA: open-ended and multiple-choice. The best performing method is LSTM Q + standard I of [2], which is used as the starting point. For an open-ended test scenario, we compare our method with the recently proposed SMem [22], SAN [23], FDN [9] and DMN + [21]. For multiple choice, we compare with region Sel. [16] and FDN [9]. We compare with 2-VIS + BLSTM [14], IMG-CNN [12] and SAN [23] to COCO-QA. We use Oursp to point out our parallel co-attention and Oursa for changing co-attention. Table 1 shows the results for the VQA test sets for both open and multiple-choice settings. We can see that our approach exceeds all previous results and the state of the art of 60.4% (MDN + notation]."}, {"heading": "4.4 Ablation study", "text": "In this section, we conduct ablation studies to quantify the role of each component in our model. Specifically, we train our approach by degrading certain components: \u2022 Image Attention alone, where we pay no attention to questions in a similar way to previous work [23]. The goal of this comparison is to ensure that our improvements are not the result of orthogonal contributions (say, better optimization or better CNN functionality). \u2022 W / O Conv, where no convolution or pooling is performed to represent phrases. Instead, we stack another word embedding on the top word bene. the goal of this Table 3 shows the comparison of our full approach w.r.t These deposits on the VQA validation group are not recommended to be used for such experiments. The deeper LSTM Q + Standard I baseline in [2] is also reported for comparison."}, {"heading": "4.5 Qualitative results", "text": "In this section, we visualize some maps of co-attention generated by our method in Fig. 5. From top to bottom, the rows are original images and questions, maps of co-attention at word level, maps of co-attention at phrase level and maps of co-attention at question level. At word level, our model mainly takes care of the object regions in an image, e.g. heads, birds. On the question page, attention depends on the specific question being asked. In the examples, we see that some have relatively equal attention, while some have peak attention. At phrase level, the attention of the image has different patterns in the images. In the first two images and the fourth image, attention shifts from objects to background regions. We suspect that this is caused by the different question types. In the first three images, the questions ask more about global attributes of the images, while the later two images answer more object-specific questions."}, {"heading": "5 Conclusion", "text": "We model the question hierarchically on three levels to gather information from different granularities, and the experimental results showed that our model exceeds the current state of the art on both the VQA and COCO-QA datasets. Furthermore, the ablation studies show the role of co-attention and question hierarchy in our final performance. By visualizing, we can see that our model takes into account interpretable image regions and questions to predict the answer. Although our model was evaluated after answering visual questions, it can potentially be applied to other tasks related to vision and language."}, {"heading": "6 Acknowledgements", "text": "This work has been partially supported by the Paul G. Allen Family Foundation, Google, and the Institute for Critical Technology and Applied Science (ICTAS) at Virginia Tech through awards to D.P.; and by a CAREER Award from the National Science Foundation, a YIP Award from the Army Research Office, a scholarship from the Office of Naval Research, an AWS in Education Research Grant, and NVIDIA's support of the GPU to D.B. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or recommendations of the U.S. government or any sponsor, either explicitly or implicitly."}], "references": [{"title": "Deep compositional question answering with neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In ICCV,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "A focused dynamic attention model for visual question answering", "author": ["Jiashi Feng Ilija Ilievski", "Shuicheng Yan"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In ECCV,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["Lin Ma", "Zhengdong Lu", "Hang Li"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz"], "venue": "In ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "In ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["Kevin J Shih", "Saurabh Singh", "Derek Hoiem"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Lecture 6.5 - rmsprop, coursera: Neural networks for machine learning", "author": ["T. Tieleman", "G. Hinton"], "venue": "In Technical report,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Verb semantics and lexical selection", "author": ["Z. Wu", "M. Palmer"], "venue": "In ACL,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "In ICML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "arXiv preprint arXiv:1511.05234,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou"], "venue": "In ACL,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Visual Question Answering (VQA) [2, 5, 13, 14] has emerged as a prominent multi-discipline research problem in both academia and industry.", "startOffset": 32, "endOffset": 46}, {"referenceID": 4, "context": "Visual Question Answering (VQA) [2, 5, 13, 14] has emerged as a prominent multi-discipline research problem in both academia and industry.", "startOffset": 32, "endOffset": 46}, {"referenceID": 12, "context": "Visual Question Answering (VQA) [2, 5, 13, 14] has emerged as a prominent multi-discipline research problem in both academia and industry.", "startOffset": 32, "endOffset": 46}, {"referenceID": 13, "context": "Visual Question Answering (VQA) [2, 5, 13, 14] has emerged as a prominent multi-discipline research problem in both academia and industry.", "startOffset": 32, "endOffset": 46}, {"referenceID": 15, "context": "Recently, visual attention based models [16, 21\u201323] have been explored for VQA, where the attention mechanism typically produces a spatial map highlighting image regions relevant to answering the question.", "startOffset": 40, "endOffset": 51}, {"referenceID": 20, "context": "Recently, visual attention based models [16, 21\u201323] have been explored for VQA, where the attention mechanism typically produces a spatial map highlighting image regions relevant to answering the question.", "startOffset": 40, "endOffset": 51}, {"referenceID": 21, "context": "Recently, visual attention based models [16, 21\u201323] have been explored for VQA, where the attention mechanism typically produces a spatial map highlighting image regions relevant to answering the question.", "startOffset": 40, "endOffset": 51}, {"referenceID": 22, "context": "Recently, visual attention based models [16, 21\u201323] have been explored for VQA, where the attention mechanism typically produces a spatial map highlighting image regions relevant to answering the question.", "startOffset": 40, "endOffset": 51}, {"referenceID": 1, "context": "These co-attended features are then recursively combined from word level to question level for the final answer prediction; \u2022 At the phrase level, we propose a novel convolution-pooling strategy to adaptively select the phrase sizes whose representations are passed to the question level representation; \u2022 Finally, we evaluate our proposed model on two large datasets, VQA [2] and COCO-QA [14].", "startOffset": 373, "endOffset": 376}, {"referenceID": 13, "context": "These co-attended features are then recursively combined from word level to question level for the final answer prediction; \u2022 At the phrase level, we propose a novel convolution-pooling strategy to adaptively select the phrase sizes whose representations are passed to the question level representation; \u2022 Finally, we evaluate our proposed model on two large datasets, VQA [2] and COCO-QA [14].", "startOffset": 389, "endOffset": 393}, {"referenceID": 1, "context": "A number of recent works [2, 5, 10, 13, 14] have proposed models for VQA.", "startOffset": 25, "endOffset": 43}, {"referenceID": 4, "context": "A number of recent works [2, 5, 10, 13, 14] have proposed models for VQA.", "startOffset": 25, "endOffset": 43}, {"referenceID": 9, "context": "A number of recent works [2, 5, 10, 13, 14] have proposed models for VQA.", "startOffset": 25, "endOffset": 43}, {"referenceID": 12, "context": "A number of recent works [2, 5, 10, 13, 14] have proposed models for VQA.", "startOffset": 25, "endOffset": 43}, {"referenceID": 13, "context": "A number of recent works [2, 5, 10, 13, 14] have proposed models for VQA.", "startOffset": 25, "endOffset": 43}, {"referenceID": 1, "context": "Instead of directly using the holistic entire-image embedding from the fully connected layer of a deep CNN (as in [2, 12\u201314]), a number of recent works have explored image attention models for VQA.", "startOffset": 114, "endOffset": 124}, {"referenceID": 11, "context": "Instead of directly using the holistic entire-image embedding from the fully connected layer of a deep CNN (as in [2, 12\u201314]), a number of recent works have explored image attention models for VQA.", "startOffset": 114, "endOffset": 124}, {"referenceID": 12, "context": "Instead of directly using the holistic entire-image embedding from the fully connected layer of a deep CNN (as in [2, 12\u201314]), a number of recent works have explored image attention models for VQA.", "startOffset": 114, "endOffset": 124}, {"referenceID": 13, "context": "Instead of directly using the holistic entire-image embedding from the fully connected layer of a deep CNN (as in [2, 12\u201314]), a number of recent works have explored image attention models for VQA.", "startOffset": 114, "endOffset": 124}, {"referenceID": 0, "context": "[1] propose a compositional scheme that consists of a language parser and a number of neural modules networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "In [23], the authors propose a stacked attention network, which runs multiple iterations or hops to infer the answer progressively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "[22] propose a multi-hop image attention scheme.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "In [16], the authors generate image regions with object proposals and then select the regions relevant to the question and answer choice.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "[21] augments dynamic memory network by introducing a new input fusion module.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] propose RNNSearch to learn an alignment over the input sentences.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "In [7], the authors propose an attention model to circumvent the bottleneck caused by fixed width hidden vector in text reading and comprehension.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "A more fine-grained attention mechanism is proposed in [15].", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "Also focused on modeling sentence pairs, the authors in [24] propose an attention-based bigram CNN for jointly performing attention between two CNN hierarchies.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "Our pooling method differs from those used in previous works [8] in that it adaptively selects different gram features at each time step, while preserving the original sequence length and order.", "startOffset": 61, "endOffset": 64}, {"referenceID": 21, "context": "Similar to [22], we connect the image and question by calculating the similarity between image and question features at all pairs of image-locations and question-locations.", "startOffset": 11, "endOffset": 15}, {"referenceID": 1, "context": "Following [2], we take the top-1000 frequent answers and treat VQA as 1000-way classification problem.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "We evaluate the proposed model on two datasets, the VQA dataset [2] and the COCO-QA dataset [14].", "startOffset": 64, "endOffset": 67}, {"referenceID": 13, "context": "We evaluate the proposed model on two datasets, the VQA dataset [2] and the COCO-QA dataset [14].", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "VQA dataset is the largest dataset for this problem, containing human annotated questions and answers on Microsoft COCO dataset [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 1, "context": "use the top 1000 most frequent answers as the possible outputs similar to [2].", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "We use the evaluation protocol of [2] in the experiment.", "startOffset": 34, "endOffset": 37}, {"referenceID": 10, "context": "COCO-QA dataset is automatically generated from captions in the Microsoft COCO dataset [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "We report the results both on classification accuracy and Wu-Palmer similarity (WUPS) measure [20] in Table 2.", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "We use torch [4] to develop our model.", "startOffset": 13, "endOffset": 16}, {"referenceID": 18, "context": "We use the Rmsprop optimizer [19] with a base learning rate of 4e-4, momentum 0.", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "Following [23], we rescale the image to 448\u00d7 448, and then take the activation from the last pooling layer of VGGNet [17] or Deep Residual network [6] as its feature.", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "Following [23], we rescale the image to 448\u00d7 448, and then take the activation from the last pooling layer of VGGNet [17] or Deep Residual network [6] as its feature.", "startOffset": 117, "endOffset": 121}, {"referenceID": 5, "context": "Following [23], we rescale the image to 448\u00d7 448, and then take the activation from the last pooling layer of VGGNet [17] or Deep Residual network [6] as its feature.", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "The best performing method deeper LSTM Q + norm I from [2] is used as our baseline.", "startOffset": 55, "endOffset": 58}, {"referenceID": 21, "context": "For open-ended test scenario, we compare our method with the recent proposed SMem [22], SAN [23], FDN [9] and DMN+ [21].", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "For open-ended test scenario, we compare our method with the recent proposed SMem [22], SAN [23], FDN [9] and DMN+ [21].", "startOffset": 92, "endOffset": 96}, {"referenceID": 8, "context": "For open-ended test scenario, we compare our method with the recent proposed SMem [22], SAN [23], FDN [9] and DMN+ [21].", "startOffset": 102, "endOffset": 105}, {"referenceID": 20, "context": "For open-ended test scenario, we compare our method with the recent proposed SMem [22], SAN [23], FDN [9] and DMN+ [21].", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "[16] and FDN [9].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[16] and FDN [9].", "startOffset": 13, "endOffset": 16}, {"referenceID": 13, "context": "We compare with 2-VIS+BLSTM [14], IMG-CNN [12] and SAN [23] on COCO-QA.", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "We compare with 2-VIS+BLSTM [14], IMG-CNN [12] and SAN [23] on COCO-QA.", "startOffset": 42, "endOffset": 46}, {"referenceID": 22, "context": "We compare with 2-VIS+BLSTM [14], IMG-CNN [12] and SAN [23] on COCO-QA.", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "4% (DMN+ [21]) to 62.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "2% (FDN [9]) to 66.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "Specifically, FDN [9] also uses ResidualNet [6], but Ours+Residual outperforms it by 1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 5, "context": "Specifically, FDN [9] also uses ResidualNet [6], but Ours+Residual outperforms it by 1.", "startOffset": 44, "endOffset": 47}, {"referenceID": 21, "context": "SMem [22] uses GoogLeNet [18] and the rest all use VGGNet [17], and Ours+VGG outperforms them by 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "SMem [22] uses GoogLeNet [18] and the rest all use VGGNet [17], and Ours+VGG outperforms them by 0.", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "SMem [22] uses GoogLeNet [18] and the rest all use VGGNet [17], and Ours+VGG outperforms them by 0.", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "2% on test-dev (DMN+ [21]).", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "6% (SAN(2,CNN) [23]) to 65.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "\u2022 Image Attention alone, where in a manner similar to previous works [23], we do not use any question attention.", "startOffset": 69, "endOffset": 73}, {"referenceID": 1, "context": "Open-Ended Multiple-Choice test-dev test-std test-dev test-std Method Y/N Num Other All All Y/N Num Other All All LSTM Q+I [2] 80.", "startOffset": 123, "endOffset": 126}, {"referenceID": 15, "context": "[16] - - - - - 77.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "4 SMem [22] 80.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "2 - - - - SAN [23] 79.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "9 - - - - FDN [9] 81.", "startOffset": 14, "endOffset": 17}, {"referenceID": 20, "context": "2 DMN+ [21] 80.", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "0 2-VIS+BLSTM [14] 58.", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": "6 IMG-CNN [12] 58.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "7 SAN(2, CNN) [23] 64.", "startOffset": 14, "endOffset": 18}, {"referenceID": 1, "context": "The deeper LSTM Q + norm I baseline in [2] is also reported for comparison.", "startOffset": 39, "endOffset": 42}, {"referenceID": 22, "context": "We can see that image-attention-alone does improve performance over the holistic image feature (deeper LSTM Q + norm I), which is consistent with findings of previous attention models for VQA [23, 21].", "startOffset": 192, "endOffset": 200}, {"referenceID": 20, "context": "We can see that image-attention-alone does improve performance over the holistic image feature (deeper LSTM Q + norm I), which is consistent with findings of previous attention models for VQA [23, 21].", "startOffset": 192, "endOffset": 200}, {"referenceID": 16, "context": "Table 3: Ablation study on the VQA dataset using VGGNet [17].", "startOffset": 56, "endOffset": 60}], "year": 2016, "abstractText": "A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \u201cwhere to look\u201d or visual attention, it is equally important to model \u201cwhat words to listen to\u201d or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN) model. Our final model outperforms all reported methods, improving the state-of-the-art on the VQA dataset from 60.4% to 62.1%, and from 61.6% to 65.4% on the COCO-QA dataset1.", "creator": "LaTeX with hyperref package"}}}