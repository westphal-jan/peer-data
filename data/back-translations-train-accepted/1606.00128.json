{"id": "1606.00128", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2016", "title": "Self-Paced Learning: An Implicit Regularization Perspective", "abstract": "Self-paced learning (SPL) mimics the cognitive mechanism of humans and animals that gradually learns from easy to hard samples. One key issue in SPL is to obtain better weighting strategy that is determined by the minimizer functions. Existing methods usually pursue this by artificially designing the explicit form of regularizers. In this paper, we focus on the minimizer functions, and study a group of new regularizers, named self-paced implicit regularizers that are derived from convex conjugacy. Based on the multiplicative form of half-quadratic optimization, convex and non-convex functions induced minimizer functions for the implicit regularizers are developed. And a general framework (named SPL-IR) for SPL is developed accordingly. We further analyze the relation between SPLIR and half-quadratic optimization. We implement SPL-IR to matrix factorization and multi-view clustering. Experimental results on both synthetic and real-world databases corroborate our ideas and demonstrate the effectiveness of implicit regularizers.", "histories": [["v1", "Wed, 1 Jun 2016 06:18:29 GMT  (657kb)", "https://arxiv.org/abs/1606.00128v1", "11 pages, 3 figures"], ["v2", "Fri, 3 Jun 2016 04:17:56 GMT  (657kb)", "http://arxiv.org/abs/1606.00128v2", "11 pages, 3 figures"], ["v3", "Sun, 18 Sep 2016 15:32:47 GMT  (1325kb)", "http://arxiv.org/abs/1606.00128v3", "12 pages, 3 figures"]], "COMMENTS": "11 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["yanbo fan", "ran he", "jian liang", "bao-gang hu"], "accepted": true, "id": "1606.00128"}, "pdf": {"name": "1606.00128.pdf", "metadata": {"source": "CRF", "title": "Self-Paced Learning: an Implicit Regularization Perspective", "authors": ["Yanbo Fan", "Ran He", "Jian Liang", "Bao-Gang Hu"], "emails": ["hubg}@nlpr.ia.ac.cn"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.00 128v 3 [cs.L G] 18 SE"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Self-Paced Learning via Explicit Regularizers", "text": "Given training data sets D = (xi, yi) ni = 1 with n samples, where xi-Rd is the i-th sample, yi is the optional information according to the learning objective (e.g. yi can be the label of xi in the classification model), let f (., w) denote the model learned and w be the model parameter. L (yi, f (xi, w) is the loss function of the i-th sample. By mimicking the cognitive mechanism of humans and animals, SPL aims to gradually optimize the model from simple to hard samples. The aim of SPL is to optimize the model parameter w and the latent sample weights v = [v1, v2,.) via the following minimization problem: min w, v E (w, v; \u03bb) = n-sample i = 1viL (yi, w)."}, {"heading": "2.2 Half-Quadratic Optimization", "text": "Semi-quadratic optimization [21, 5, 4] is a universally applied optimization method based on convex conjugation theory. It attempts to solve a nonlinear objective function by iteratively optimizing a series of semiquadratic reformulation problems [7, 9, 8, 6, 30]. For a differentiable function \u03c6 (t): R \u2192 R, if \u03c6 (t) continues to fulfill the conditions of the multiplicative form of HQ optimization in [20], the following equation applies to each fixed t, \u03c6 (t) = inf \u00b2 R + {1 2 pt2 + 1 p), (5), where \u0432 (p) is the dual potential function of \u03c6 (t) and R + = {t | t \u00b2."}, {"heading": "3 The Proposed Method", "text": "In this section we first define the proposed implicit regularizer and derive its minimization function based on convex conjugacy. Then we develop a general, self-determined learning system called SPL-IR based on implicit regularization. Finally, we analyze the relationships between SPL-IR and HQ optimization."}, {"heading": "3.1 Self-Paced Implicit Regularizer", "text": "Based on our analysis of SPL above, we define the self-controlled implicit regularizer as follows: 1. Self-controlled implicit regularization. A self-controlled implicit regularizer is implicitly the dual potential function of a robust loss function \u03c6 (\u03bb, t) and satisfactory1. \u03c6 (\u03bb, t) = minv \u2265 0 vt + 270 (\u03bb, v); 2. \u03c3 (\u03bb, t) is the minimizing function of \u03c6 (\u03bb, t) that satisfies the loss. (\u03bb, t) t +."}, {"heading": "3.2 Self-Paced Learning via Implicit Regularizers", "text": "By substituting the regularization term g (\u03bb, v) in (1) with a self-controlled implied regularization mechanism given in definition 1, we obtain the following SPL-IR problem, min w, v; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p;"}, {"heading": "3.3 SPL-IR and Half-Quadratic Optimization", "text": "We can develop new implicit regulatory mechanisms based on the development of robust loss functions. Specifically, we will analyze the relationships between SPL-IR and HQ optimisation and provide several self-incremental implicit regulatory mechanisms. To better demonstrate, we will first give an equivalent square form definition of the self-directed implicit regulatory function, definition 2 (square form). Self-directed implicit regulatory mechanisms will gradually be defined as dual potential function of a self-directed loss function. Definition 2 (square form), and satisfyes1. definition (square form). Definition of the self-directed implicit regulatory mechanisms will be gradually used. Definition of the double potential loss function will be defined as the double potential function of a robust loss function."}, {"heading": "4 Experiments", "text": "To illustrate the accuracy and effectiveness of the developed SPL-IR model, we apply it to three classical tasks: matrix factorization, clustering and classification. Experimental results show that the proposed, self-controlled implicit regularizers exceed basic algorithms and provide comparable or even better performance compared to the artificially designed SPL regularizers. There are two hyperparameters (\u03bb, \u00b5) that need to be adjusted in Algorithm 1. We follow a default setting in SPL [14] for all our experiments, that is, \u03bb is initialized to obtain about half of the samples, then iteratively updated to include more and more samples. Practical direction of updating depends on the specific minimizing function. For functions specified in Table 1, \u03bbT + 1 = \u03bbT / \u00b5 applies to L1-L2, while \u03bbT + 1 = T Huber is a incremental and a incremental factor > 1, whereby one is a \u00b5."}, {"heading": "4.1 Matrix Factorization", "text": "Matrix factorization (MF) is one of the fundamental problems in machine learning and data mining. It aims to factorize a possible data matrix Y into two smaller factors, U & # 160; Rm & # 160; and V & # 160; Rn & # 160; r, where r & # 160; min (m & # 160; n), so that UVT may have been successfully implemented close to Y. MF & # 160; in many applications, such as collaborative filtering [24].Here we look at the MF problem on synthetic datasets. Specifically, the data used here are generated as follows: two matrices U and V, both of which are of the size 100 x 4, are first randomly generated with each entry from the Gaussian distribution N & # 160; 1 & # 160; resulting in a soil-truth matrix-4-Y0 = UVT. Then we randomly select 40% of the entries and treat them as missing data."}, {"heading": "4.2 Multi-view Clustering", "text": "Most existing multi-view clustering algorithms fit into a non-convex model and can be stuck in poor local minima. To alleviate this, Xu et al. propose a Multiview Self-paced Learning Algorithm (MSPL) that takes into account the learning capability of both samples and views and achieves promising results in [29]. Here, we simply modify their MSPL model for comparison with various SPL regulators. UCI Handwritten Digit dataset 1 is used in this experiment. It consists of 2,000 handwritten digits classified into ten categories (0-9). Each instance is represented in relation to the following six types of characteristics (or views): Fourier coefficients of character shapes (FOU), profile correlations (FAC), Carhun-Love coefficients (KAR), pixel intersections in 2 x 3 windows (PIX), Cnices (here), and CERALM (here)."}, {"heading": "4.3 Classification", "text": "Here we perform a binary classification task. Specifically, we use the model of L2-regulated logistic regression (LR) as a starting point and integrate it for comparison with various SPL regularizers. Three real databases are considered as solvers of the LR: Breast1, Spambase1 and Svmguide1 [2]. Their statistical information is in Table 4. For each data set 1https: / / archive.ics.uci.edu / ml / datasetswe consider it without additional noise or with 20% random label noise. The 20% random label noise means that we randomly select 20% samples from training data and reverse their labels (change positive to negative and vice versa)."}, {"heading": "5 Conclusions", "text": "In this paper, we examine a group of new regularizers based on convex conjugate theory, and their analytical form may even be unknown, and their properties and corresponding minimization function can be learned directly from the latent loss function. Based on this, we then develop a general SPL framework (SPL-IR). Furthermore, we show that the learning procedure of SPL-IR is actually linked to certain latently robust loss functions and can therefore provide theoretical inspiration for the working mechanisms of SPL-IR (such as robustness against outliers or heavy noise). Later, we analyze the relationships between SPL-IR and HQ optimization and develop a group of self-step implicit regularizers accordingly. Experimental results on both supervised and unattended tasks show the correctness and effectiveness of the proposed, self-controlled regularizer."}, {"heading": "6 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Proof of Proposition 1", "text": "The proof. The proof sketch is similar to those in [20]. To simplify the representation we omit \u03bb (t) and use \u03c6 (0) = 0; H2: \u03c6 (t) is C1 and concave; H3: limt (t). According to the fennel-moreau theorem [23] the convex conjugate of the vexation of the vexation is the vexation of the vexation, i.e. the vconjugation of the vexation, i.e. the vexion conjugation of the vexation, i.e. the vconjugate of the vexation, i.e. the vconjugation of the vexposition, i.e. the vconjugation of the vexation, i.e. the vconjugation of the vexation, i.e. the vconjugation, the vconjugation of the vexposition, i.e. the vconjugation, the vconjugation, the vconjugation of the vexposition, i.e. the vexposition, the vconjugation of the vconjugation, i.e. the vexposition, the vconjugation of the vconjugation, i.e. the vexposition, the vconjugation, the vconjugation of the vconjugation, the vconjugation, i.e. the vexposition, the vexposition of the vconjugat, the vexposition, the vconjugat, the vconjugat of the vconjugat, i.e., the vexposition, the vexposition of the vexposition, the vconjugat, the vconjugat, the vexposition of the vconjugat, i.e., the"}, {"heading": "6.2 Definition 1 and Definition 2", "text": "(v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (t). (t). (t). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2) v. (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2) v. (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2) v). (v2). (v2). (v2). (v2). (v2). (v2) v). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2). (v2"}, {"heading": "6.3 Self-Paced Regularizer", "text": "Similar definitions of the self-controlled regulator (or self-controlled function) have been proposed in [13, 32, 11]; the definition in [32] is set out below. Definition 3 (Self-controlled regulator) [32]: Suppose v is a weight variable, the loss is the loss, and \u03bb is the learning tempo parameter. G (\u03bb, v) is called a self-controlled regulator, if1. g (\u03bb, v) becomes convex in relation to v [0, 1], the loss of self-controlled functions is convex, the monotonically decreasing self-analysis mechanism is w.r.t. And it states that the lim\u00e9 \u2192 0 v-controlled functions (\u03bb, v) are convex-controlled properties of the self-analysis mechanism."}], "references": [{"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Libsvm: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "JMLR, 9(Aug):1871\u20131874,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Constrained restoration and the recovery of discontinuities", "author": ["D. Geman", "G. Reynolds"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Nonlinear image recovery with half-quadratic regularization", "author": ["D. Geman", "C. Yang"], "venue": "TIP, 4(7):932\u2013946,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Two-stage sparse representation for robust recognition on large-scale database", "author": ["R. He", "B.-G. Hu", "W.-S. Zheng", "Y. Guo"], "venue": "In AAAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Robust recovery of corrupted low-rankmatrix by implicit regularizers", "author": ["R. He", "T. Tan", "L. Wang"], "venue": "TPAMI, 36(4):770\u2013783,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Maximum correntropy criterion for robust face recognition", "author": ["R. He", "W.S. Zheng", "B.G. Hu"], "venue": "TPAMI, 33(8):1561\u20131576,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Half-quadratic-based iterative minimization for robust sparse representation", "author": ["R. He", "W.-S. Zheng", "T. Tan", "Z. Sun"], "venue": "TPAMI, 36(2):261\u2013275,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Comparing partitions", "author": ["L. Hubert", "P. Arabie"], "venue": "Journal of Classification,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1985}, {"title": "Easy samples first: Self-paced reranking for zero-example multimedia search", "author": ["L. Jiang", "D. Meng", "T. Mitamura", "A.G. Hauptmann"], "venue": "In MM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Self-paced learning with diversity", "author": ["L. Jiang", "D. Meng", "S.-I. Yu", "Z. Lan", "S. Shan", "A. Hauptmann"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Self-paced curriculum learning", "author": ["L. Jiang", "D. Meng", "Q. Zhao", "S. Shan", "A.G. Hauptmann"], "venue": "In AAAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "B. Packer", "D. Koller"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Learning the easy things first: Self-paced visual category discovery", "author": ["Y.J. Lee", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Multi-objective self-paced learning", "author": ["H. Li", "M. Gong", "D. Meng", "Q. Miao"], "venue": "In AAAI,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Self-paced cross-modal subspace matching", "author": ["J. Liang", "Z. Li", "D. Cao", "R. He", "J. Wang"], "venue": "In SIGIR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Approximate computation and implicit regularization for very large-scale data analysis", "author": ["M.W. Mahoney"], "venue": "In PODS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "What objective does self-paced learning indeed optimize", "author": ["D. Meng", "Q. Zhao"], "venue": "arXiv preprint arXiv:1511.06049,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "The equivalence of half-quadratic minimization and the gradient linearization iteration", "author": ["M. Nikolova", "R.H. Chan"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Analysis of half-quadratic minimization methods for signal and image recovery", "author": ["M. Nikolova", "M.K. Ng"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Implementing regularization implicitly via approximate eigenvector computation", "author": ["L. Orecchia", "M.W. Mahoney"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Convex analysis", "author": ["R.T. Rockafellar"], "venue": "Princeton university press,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Probabilistic matrix factorization", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "In NIPS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Self-paced learning for long-term tracking", "author": ["J.S. Supancic", "D. Ramanan"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Parameter convergence for em and mm algorithms", "author": ["F. Vaida"], "venue": "Statistica Sinica, pages 831\u2013840,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "A probabilistic approach to robust matrix factorization", "author": ["N. Wang", "T. Yao", "J. Wang", "D.-Y. Yeung"], "venue": "In ECCV", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "A survey on multi-view learning", "author": ["C. Xu", "D. Tao"], "venue": "arXiv preprint arXiv:1304.5634,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Multi-view self-paced learning for clustering", "author": ["C. Xu", "D. Tao"], "venue": "In IJCAI,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Robust feature extraction via information theoretic learning", "author": ["X.-T. Yuan", "B.-G. Hu"], "venue": "In ICML,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "A self-paced multiple-instance learning framework for co-saliency detection", "author": ["D. Zhang", "D. Meng", "C. Li", "L. Jiang", "Q. Zhao", "J. Han"], "venue": "In ICCV,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Self-paced learning for matrix factorization", "author": ["Q. Zhao", "D. Meng", "L. Jiang", "Q. Xie", "Z. Xu", "A.G. Hauptmann"], "venue": "In AAAI,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "propose a new learning strategy called curriculum learning (CL) in [1], which gradually includes more and more hard samples into training process.", "startOffset": 67, "endOffset": 70}, {"referenceID": 13, "context": "propose a new learning strategy named selfpaced learning (SPL) that incorporates the curriculum updating in the process of model optimization [14].", "startOffset": 142, "endOffset": 146}, {"referenceID": 15, "context": "Due to its ability of avoiding bad local minima and improving the generalization performance, many works have been developed based on SPL [16, 17, 13, 31, 25, 15].", "startOffset": 138, "endOffset": 162}, {"referenceID": 16, "context": "Due to its ability of avoiding bad local minima and improving the generalization performance, many works have been developed based on SPL [16, 17, 13, 31, 25, 15].", "startOffset": 138, "endOffset": 162}, {"referenceID": 12, "context": "Due to its ability of avoiding bad local minima and improving the generalization performance, many works have been developed based on SPL [16, 17, 13, 31, 25, 15].", "startOffset": 138, "endOffset": 162}, {"referenceID": 30, "context": "Due to its ability of avoiding bad local minima and improving the generalization performance, many works have been developed based on SPL [16, 17, 13, 31, 25, 15].", "startOffset": 138, "endOffset": 162}, {"referenceID": 24, "context": "Due to its ability of avoiding bad local minima and improving the generalization performance, many works have been developed based on SPL [16, 17, 13, 31, 25, 15].", "startOffset": 138, "endOffset": 162}, {"referenceID": 14, "context": "Due to its ability of avoiding bad local minima and improving the generalization performance, many works have been developed based on SPL [16, 17, 13, 31, 25, 15].", "startOffset": 138, "endOffset": 162}, {"referenceID": 28, "context": "One key issue in SPL is to obtain better weighting strategy that is determined by the minimizer functions, and existing methods usually pursue this by artificially designing the explicit form of SPL regularizers [29, 32, 11, 12].", "startOffset": 212, "endOffset": 228}, {"referenceID": 31, "context": "One key issue in SPL is to obtain better weighting strategy that is determined by the minimizer functions, and existing methods usually pursue this by artificially designing the explicit form of SPL regularizers [29, 32, 11, 12].", "startOffset": 212, "endOffset": 228}, {"referenceID": 10, "context": "One key issue in SPL is to obtain better weighting strategy that is determined by the minimizer functions, and existing methods usually pursue this by artificially designing the explicit form of SPL regularizers [29, 32, 11, 12].", "startOffset": 212, "endOffset": 228}, {"referenceID": 11, "context": "One key issue in SPL is to obtain better weighting strategy that is determined by the minimizer functions, and existing methods usually pursue this by artificially designing the explicit form of SPL regularizers [29, 32, 11, 12].", "startOffset": 212, "endOffset": 228}, {"referenceID": 10, "context": "Specifically, a definition of self-paced regularizer is given in [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "One attempt in this aspect is [19], they show that the ASS method used for SPL accords with the majorization minimization [26] algorithm implemented on a latent SPL objective, and deduce the latent objective of hard, linear and mixture regulraizers.", "startOffset": 30, "endOffset": 34}, {"referenceID": 25, "context": "One attempt in this aspect is [19], they show that the ASS method used for SPL accords with the majorization minimization [26] algorithm implemented on a latent SPL objective, and deduce the latent objective of hard, linear and mixture regulraizers.", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "Since li = L(yi, f(xi,w) is constant oncew is given, the optimal value of vi is uniquely determined by the corresponding minimizer function \u03c3(\u03bb, li) that satisfies \u03c3(\u03bb, li)li + g(\u03bb, \u03c3(\u03bb, li)) \u2264 vili + g(\u03bb, vi), \u2200vi \u2208 [0, 1].", "startOffset": 217, "endOffset": 223}, {"referenceID": 13, "context": "(3) For example, if g(\u03bb, vi) = \u2212\u03bbvi [14], the optimal v\u2217 i is calculated by v\u2217 i = \u03c3(\u03bb, li) = { 1, if li \u2264 \u03bb 0, otherwise (4)", "startOffset": 36, "endOffset": 40}, {"referenceID": 28, "context": "Many efforts have been put into the learning of minimizer functions [29, 32, 11, 12, 25], and we name them as SPL with explicit regularizers as they usually require the explicit form of regularizer g(\u03bb, v) .", "startOffset": 68, "endOffset": 88}, {"referenceID": 31, "context": "Many efforts have been put into the learning of minimizer functions [29, 32, 11, 12, 25], and we name them as SPL with explicit regularizers as they usually require the explicit form of regularizer g(\u03bb, v) .", "startOffset": 68, "endOffset": 88}, {"referenceID": 10, "context": "Many efforts have been put into the learning of minimizer functions [29, 32, 11, 12, 25], and we name them as SPL with explicit regularizers as they usually require the explicit form of regularizer g(\u03bb, v) .", "startOffset": 68, "endOffset": 88}, {"referenceID": 11, "context": "Many efforts have been put into the learning of minimizer functions [29, 32, 11, 12, 25], and we name them as SPL with explicit regularizers as they usually require the explicit form of regularizer g(\u03bb, v) .", "startOffset": 68, "endOffset": 88}, {"referenceID": 24, "context": "Many efforts have been put into the learning of minimizer functions [29, 32, 11, 12, 25], and we name them as SPL with explicit regularizers as they usually require the explicit form of regularizer g(\u03bb, v) .", "startOffset": 68, "endOffset": 88}, {"referenceID": 20, "context": "Half-quadratic optimization [21, 5, 4] is a commonly used optimization method that based on the convex conjugacy theory.", "startOffset": 28, "endOffset": 38}, {"referenceID": 4, "context": "Half-quadratic optimization [21, 5, 4] is a commonly used optimization method that based on the convex conjugacy theory.", "startOffset": 28, "endOffset": 38}, {"referenceID": 3, "context": "Half-quadratic optimization [21, 5, 4] is a commonly used optimization method that based on the convex conjugacy theory.", "startOffset": 28, "endOffset": 38}, {"referenceID": 6, "context": "It tries to solve a nonlinear objective function via optimizing a series of half-quadratic reformulation problems iteratively [7, 9, 8, 6, 30].", "startOffset": 126, "endOffset": 142}, {"referenceID": 8, "context": "It tries to solve a nonlinear objective function via optimizing a series of half-quadratic reformulation problems iteratively [7, 9, 8, 6, 30].", "startOffset": 126, "endOffset": 142}, {"referenceID": 7, "context": "It tries to solve a nonlinear objective function via optimizing a series of half-quadratic reformulation problems iteratively [7, 9, 8, 6, 30].", "startOffset": 126, "endOffset": 142}, {"referenceID": 5, "context": "It tries to solve a nonlinear objective function via optimizing a series of half-quadratic reformulation problems iteratively [7, 9, 8, 6, 30].", "startOffset": 126, "endOffset": 142}, {"referenceID": 29, "context": "It tries to solve a nonlinear objective function via optimizing a series of half-quadratic reformulation problems iteratively [7, 9, 8, 6, 30].", "startOffset": 126, "endOffset": 142}, {"referenceID": 19, "context": "Given a differentiable function \u03c6(t) : R \u2192 R, if \u03c6(t) further satisfies the conditions of the multiplicative form of HQ optimization in [20], the following equation holds for any fixed t,", "startOffset": 136, "endOffset": 140}, {"referenceID": 20, "context": "More analysis about \u03c6(t) and \u03c8(p) refers to [21].", "startOffset": 44, "endOffset": 48}, {"referenceID": 19, "context": "Proposition 1 For any fixed \u03bb, if \u03c6(\u03bb, t) in Definition 1 further satisfies the conditions referred in [20], its minimizer function \u03c3(\u03bb, t) is uniquely determined by \u03c6(\u03bb, t) and the analytic form of the dual potential function \u03c8(\u03bb, v) can be even unknown during the optimization.", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "have given a definition of self-paced regularizer and derived necessary conditions of the regularizer and the corresponding minimizer function for SPL in [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 17, "context": "The benefit of implicit regularization has been analyzed in [18, 22].", "startOffset": 60, "endOffset": 68}, {"referenceID": 21, "context": "The benefit of implicit regularization has been analyzed in [18, 22].", "startOffset": 60, "endOffset": 68}, {"referenceID": 20, "context": "For HQ-welsch, standard HQ algorithm [21] is implemented with each \u03bb independently.", "startOffset": 37, "endOffset": 41}, {"referenceID": 18, "context": "The learning procedure of some existing regularizers like hard and linear [19] can also be explained under the framework of SPL-IR.", "startOffset": 74, "endOffset": 78}, {"referenceID": 8, "context": "The loss functions in Table 1 are well defined and have proven to be effective in many areas [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 13, "context": "We follow a standard setting in SPL [14] for all our experiments.", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "Similar settings are adjusted for the competing SPL regularizers, including SPL-hard [14] and SPL-mixture [32].", "startOffset": 85, "endOffset": 89}, {"referenceID": 31, "context": "Similar settings are adjusted for the competing SPL regularizers, including SPL-hard [14] and SPL-mixture [32].", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "MF has been successfully implemented in many applications, such as collaborative filtering [24].", "startOffset": 91, "endOffset": 95}, {"referenceID": 31, "context": "Similar to [32], we consider L1-norm MF problem with L2-norm regularization, and the baseline algorithm is PRMF [27].", "startOffset": 11, "endOffset": 15}, {"referenceID": 26, "context": "Similar to [32], we consider L1-norm MF problem with L2-norm regularization, and the baseline algorithm is PRMF [27].", "startOffset": 112, "endOffset": 116}, {"referenceID": 27, "context": "Multi-view clustering aims to group data with multiple views into their underlying classes [28].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "propose a multi-view self-paced learning algorithm (MSPL) that considers the learnability of both samples and views and achieves promising results in [29].", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "Five commonly used metrics are adopted to measure the clustering performances: clustering accuracy (ACC), normalized mutual information (NMI), F-score, Purity, and adjusted rand index (AR) [10].", "startOffset": 189, "endOffset": 193}, {"referenceID": 2, "context": "Liblinear [3] is used as the solver of LR.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "Three real-world databases are considered: Breast1, Spambase1 and Svmguide1 [2].", "startOffset": 76, "endOffset": 79}, {"referenceID": 19, "context": "The proof sketch is similar to that in [20].", "startOffset": 39, "endOffset": 43}, {"referenceID": 22, "context": "By the Fenchel-Moreau theorem [23], the convex conjugate of \u03b8\u2217 is \u03b8, that is \u03b8(t) = (\u03b8\u2217)\u2217(t) = supv\u22640 {vt\u2212 \u03b8\u2217(v)} = \u2212 infv\u22650 {vt+ \u03b8\u2217(\u2212v)}.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "Proposition 2 For any fixed \u03bb, if \u03c6(\u03bb, t) in Definition 2 further satisfies the conditions referred in [20], its minimizer function \u03c3(\u03bb, t) is uniquely determined by \u03c6(\u03bb, t) and the analytic form of \u03c8(\u03bb, v) can be even unknown during the optimization.", "startOffset": 103, "endOffset": 107}, {"referenceID": 19, "context": "The proof sketch is similar to that in [20].", "startOffset": 39, "endOffset": 43}, {"referenceID": 22, "context": "By the Fenchel-Moreau theorem [23], the convex conjugate of \u03b8\u2217 is \u03b8, that is \u03b8(t) = (\u03b8\u2217)\u2217(t) = supv\u22640 {vt\u2212 \u03b8\u2217(v)} = \u2212 infv\u22650 {vt+ \u03b8\u2217(\u2212v)}.", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "Similar definitions of self-paced regularizer (or self-paced function) have been proposed in [13, 32, 11].", "startOffset": 93, "endOffset": 105}, {"referenceID": 31, "context": "Similar definitions of self-paced regularizer (or self-paced function) have been proposed in [13, 32, 11].", "startOffset": 93, "endOffset": 105}, {"referenceID": 10, "context": "Similar definitions of self-paced regularizer (or self-paced function) have been proposed in [13, 32, 11].", "startOffset": 93, "endOffset": 105}, {"referenceID": 31, "context": "The definition in [32] is shown below.", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "Definition 3 (Self-Paced Regularizer) [32]: Suppose that v is a weight variable, l is the loss, and \u03bb is the learning pace parameter.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "g(\u03bb, v) is convex with respect to v \u2208 [0, 1]; 2.", "startOffset": 38, "endOffset": 44}, {"referenceID": 0, "context": "\u03bb, and it holds that lim\u03bb\u21920 v\u2217(\u03bb, l) = 0, lim\u03bb\u2192\u221e v \u2217(\u03bb, l) \u2264 1 ; where v\u2217(\u03bb, l) = argminv\u2208[0,1] vl+ g(\u03bb, v).", "startOffset": 90, "endOffset": 95}, {"referenceID": 13, "context": "[14] \u2212\u03bb\u2211ni=1 vi, \u03bb > 0 {", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11, 13] 1 2 \u03bb \u2211n i=1(v 2 i \u2212 2vi), \u03bb > 0 {", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[11, 13] 1 2 \u03bb \u2211n i=1(v 2 i \u2212 2vi), \u03bb > 0 {", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "[11, 13] n", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[11, 13] n", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "[11, 13] \u2212 \u03b6 n", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[11, 13] \u2212 \u03b6 n", "startOffset": 0, "endOffset": 8}, {"referenceID": 11, "context": "[12] \u2212\u03bb\u2211 i=1 vi \u2212 \u03b3||v||2,1, \u03bb > 0, \u03b3 > 0 { 1, li \u2264 \u03bb+ \u03b3 1 \u221a i\u2212 \u221a i\u22121 0, otherwise", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] n", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] \u2211n i=1 \u03bb\u03b3 \u03bbvi+\u03b3 , \u03bb > 0, \u03b3 > 0 \uf8f1", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] \u2212 \u03bb K", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "One attempt about the underlying working mechanism of SPL is [19].", "startOffset": 61, "endOffset": 65}, {"referenceID": 25, "context": "Starting from SPL regularizers and their minimizer functions, they show that the ASS method used for SPL accords with the majorization minimization [26] algorithm implemented on a latent SPL objective, and deduced the latent objective of hard, linear and mixture regulraizers.", "startOffset": 148, "endOffset": 152}], "year": 2016, "abstractText": "Self-paced learning (SPL) mimics the cognitive mechanism of humans and animals that gradually learns from easy to hard samples. One key issue in SPL is to obtain better weighting strategy that is determined by minimizer function. Existing methods usually pursue this by artificially designing the explicit form of SPL regularizer. In this paper, we focus on the minimizer function, and study a group of new regularizer, named self-paced implicit regularizer that is deduced from robust loss function. Based on the convex conjugacy theory, the minimizer function for self-paced implicit regularizer can be directly learned from the latent loss function, while the analytic form of the regularizer can be even known. A general framework (named SPL-IR) for SPL is developed accordingly. We demonstrate that the learning procedure of SPL-IR is associated with latent robust loss functions, thus can provide some theoretical inspirations for its working mechanism. We further analyze the relation between SPL-IR and half-quadratic optimization. Finally, we implement SPL-IR to both supervised and unsupervised tasks, and experimental results corroborate our ideas and demonstrate the correctness and effectiveness of implicit regularizers.", "creator": "LaTeX with hyperref package"}}}