{"id": "1606.00499", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2016", "title": "Generalizing and Hybridizing Count-based and Neural Language Models", "abstract": "Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.", "histories": [["v1", "Wed, 1 Jun 2016 23:26:20 GMT  (79kb,D)", "http://arxiv.org/abs/1606.00499v1", null], ["v2", "Mon, 26 Sep 2016 01:48:57 GMT  (89kb,D)", "http://arxiv.org/abs/1606.00499v2", "Presented at EMNLP2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["graham neubig", "chris dyer"], "accepted": true, "id": "1606.00499"}, "pdf": {"name": "1606.00499.pdf", "metadata": {"source": "CRF", "title": "Generalizing and Hybridizing Count-based and Neural Language Models", "authors": ["Graham Neubig", "Chris Dyer"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "These are statistical models that calculate their probability P (wI1) given a sentence wI1: = w1,.., wI. LMs are commonly used in applications such as machine translation and speech recognition, and due to their broad applicability they have also been extensively studied in literature. (The most traditional and widespread language modeling paradigm is that of numbered LMs, usually smoothed n-gramms (Witten and Bell, 1991; Chen and Goodman, 1996). Recently, there has been a focus on LMs based on neural networks (Nakamura et al, 1990; Bengio et al., 2006; Mikolov et al., 2010), which have shown impressive improvements in performance over numbered LMs. On the other hand, these neural LMs have also focused on neural networks (Nakamura et al, 1990; Goodi et al.)."}, {"heading": "2 Mixture of Distributions LMs", "text": "As mentioned above, MODLMs are LMs that take the form of equations. 1. This can be encompassed as the following matrix-vector multiplication: pc = Dc\u03bb c, where PC is a vector with the length equal to the vocabulary size in which the jth element pc, j corresponds to the probability Pk (wi = j | c), \u03bbc is a size-K vector containing the mix weights for the distributions, and Dc is a Jby-K matrix in which the element dc, j, k corresponds to the probability Pk (wi = j | c). 1. Note that all columns in D represent probability distributions and must therefore add up to one above the J words in the vocabulary, and that all \u03bb of the probability Pk must sum to 1 above the K distributions. Under this condition, the vector will also represent a well-formed probability distribution."}, {"heading": "3 Existing LMs as Linear Mixtures", "text": "The simplest form of interpolation is word sequence interpolation, where c (wii \u2212 N + 1) = 1, n-gram models often interpolate the ML distributions from length 1 to N."}, {"heading": "3.2 Neural LMs as Mixtures of Distributions", "text": "In this section, we will show how neural network LMs can also be considered an instance of the MODLM framework.Feed-forward neural network LMs: Feedforward LMs (Bengio et al., 2006; Pan, 2007), which, like n-grams, calculate the probability of the next word based on the previous words.Given the context wi \u2212 1i \u2212 N + 1, these words are converted into real word representation vectors ri \u2212 1i \u2212 N + 1, which are concatenated to a total representation vector q = (ri \u2212 1i \u2212 N + 1) in which the vector concatenation function. q is then executed by a series of affine word transformations and nonlinearity that are defined as a function NN (q) to obtain a vector h. For example, for a single-layer neural network with a tanh non-linearity that we can define."}, {"heading": "4 Novel Applications of MODLMs", "text": "This section describes how we can use this framework of MODLMs to design new varieties of LMs that take advantage of n-gram and neural network LMs.4.1 Neurally Interpolated n-gram ModelsThe first new instantiation of MODLMs that we propose is neurally interpolated n-gram models, shown in Fig. 3a. In these models, we have setD to be the same matrix used in n-gram LMs, but calculate\u03bb (c) using a neural network model. As \u03bb (c) is learned from data, this framework has the potential to learn more intelligent interpolation functions than the heuristics described in \u00a7 3.1 \u2212 Furthermore, because the neural network is only a softmax over N distributions instead of J vocabulary words, training and test efficiency of these models can be."}, {"heading": "5 Learning Mixtures of Distributions", "text": "While the MODLM formulations of the common heuristic n-gram LMs do not require learning, the other models are parameterized. In this section, the details of learning these parameters are discussed."}, {"heading": "5.1 Learning MODLMs", "text": "Like most previous work on LMs (Bengio et al., 2006), we use a negative logarithm loss, which is summed up in words wi in each sentence w in the corpusWL (W) = \u2212 \u2211 w * W * W * w logP (wi | c), where c represents all words before wi inw used in the probability calculation. As mentioned in Eq.2, P (wi = j | c) can be efficiently calculated from the distribution matrix Dc and the mixing function yield. As we can calculate the log probability, the remaining parts of the training are similar to the training for standard LMs for neural networks. As usual, we perform forward propagation to calculate the probabilities of all words in the sentence, propagate the gradients back through the computational graph and perform a variant of stochastic gradient lineage (SGD) to update the parameters."}, {"heading": "5.2 Block Dropout for Hybrid Models", "text": "While the training method described in the previous section is similar to that of other neural network models, we are making an important modification of the training process specifically tailored to the hybrid models of \u00a7 4.2. The reason for this is our observation (in detail in \u00a7 6.3) that, although the hybrid models are more meaningful than the corresponding neural network LMs, they fell into poor local minima with higher training errors than neural network LMs. This is because the count-based elements of the distribution matrix in Fig. 3b already represent good approximations of the target distribution, while the weights of the one-word distributions cannot yet provide precise probabilities. Thus, the model learns to set the mixing proportions of the elements to almost zero and relies mainly on the count-based n-gram distributions. To encourage the model to use the mixing components, we use a method called Dropout (Ammar)."}, {"heading": "5.3 Network and Training Details", "text": "Most experiments used single-layer 200-node networks and 400-node networks were used for experiments with larger training data. Word representations were the same size as the hidden layer. Larger and multi-layer networks did not bring any improvements. Training: We used ADAM (Kingma and Ba, 2015) with a learning rate of 0.001 and minibatch sizes of 512 words. This resulted in faster convergence than the standard SGD and more stable optimizations than other update rules. The models were evaluated all 500k-3M words, and the model with the best probability of development was used. In addition to block breakers of \u00a7 5.2, we used standard breakers with a rate of 0.5 for both feed models (Srivastaet al, 2014) and LM (STM networks), which are based on a slightly lower number of pseudograms than the interactive networks."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Experimental Setup", "text": "In this section, we conduct experiments to evaluate the neural interpolated n-grams (\u00a7 6.2) and the neural / n-gram hybrids (\u00a7 6.3); the ability of our models to use the information from large data sets (\u00a7 6.4); and the relative performance compared to the previous static interpolations of the already trained models (\u00a7 6.5); for the most important experiments we conduct with two different models, we evaluate two corpora: the Penn Treebank (PTB) data provided by Mikolov et al. (2010), 2 and the first 100k sentences in the English side of the ASPEC corpus (Nakazawa et al., 2015) 3 (details in Tab. 1); the PTB corpus uses the standard vocabulary of 10k words; and for the ASPEC corpus, we use a vocabu-2http: / / / rnnlm.org / simple-examptz"}, {"heading": "6.4 Results for Larger Data Sets", "text": "To study the ability of hybrid models to use counts trained on larger sets of data, we conduct experiments with two larger sets of data: WSJ: The PTB uses data from the Wall Street Journal from 1989, so we add the remaining years between 1987 and 1994 (1.81M Sents., 38.6M words).GW: News data from the English Gigaword 5th Edition (LDC2011T07, 59M Sents., 1.76G words).We incorporate this data either by training the net parameters over all the large data, or by separately training n-grams based on the number of PTB, WSJ and GW and memorizing the net parameters only on PTB data. The former have the advantage of training the network on much larger data. The latter has two main advantages: 1) If the smaller data comes from a particular domain, the mixing weights can be weighted to match those domain-specific data."}, {"heading": "6.5 Comparison with Static Interpolation", "text": "Since the proposed neural / n-gram hybrid models combine the advantages of neural and gram-based models, we finally compare them with the more standard method of training models independently, combining them with static interpolation weights tuned to the validation set using the EM algorithm. Table 4 shows perplexity regarding the combinations of a PTB-based neural standard model (or distributions based on it), and counting distributions trained on PTB, WSJ, and GW are gradually being added to the standard static and proposed LSTM interpolation methods.The results show that if only PTB data is used, the methods achieve similar results, but with the more diverse datasets overlap their static counterparts.66 In addition to better perplexities, neural / n-gram hybrids are trained in a single pass, rather than performing post-factual training that may provide other benefits for the palate (Li and Li) and other palate interpolations."}, {"heading": "7 Related Work", "text": "A number of alternative methods focus on the interpolation of LMs of multiple varieties, such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; Gu'lc-ehre et al., 2015). Perhaps most relevant is Hsu's (2007) work on learning to interpolate multiple LMs using log-linear models, which differs from our work in that it learns functions to estimate the fallback probabilities of n-gram LMs starting with n-gram probabilities in Equation 3 (Della Pietra et al., 1992; Kneser and Steinbiss, 1993; Iyer and Ostendorf, 1999) and adapts them based on the distribution of the current document, although they have better linear models, which often exhibit neural effects, compared to 2015 (Kneser and Steinbiss, 1993; Iyer and Ostendorf, 1999)."}, {"heading": "8 Conclusion and Future Work", "text": "In this paper, we proposed a language modeling framework that generalizes both neural network and number-based n-gram LMs, allowing us to learn more effective interpolation functions for number-based n-grams and to create neural LMs that incorporate information from number-based models. As the framework discussed here is general, it is also possible that it can be used for other tasks that perform sequential prediction of words such as neural machine translation (Sutskever et al., 2014) or dialog word generation (Sordoni et al., 2015)."}, {"heading": "Acknowledgements", "text": "We thank Kevin Duh, Austin Matthews, Shinji Watanabe, and anonymous critics for valuable comments on previous drafts. This work was supported in part by JSPS KAKENHI Grant Number 16H05873 and the Program for Advancing Strategic International Networks to Accelerate the Circulation of Talented Researchers."}], "references": [{"title": "Decoder integration and expected bleu training for recurrent neural network language models", "author": ["Auli", "Gao2014] Michael Auli", "Jianfeng Gao"], "venue": "In Proc. ACL,", "citeRegEx": "Auli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2014}, {"title": "Map adaptation of stochastic grammars", "author": ["Michael Riley", "Brian Roark", "Richard Sproat"], "venue": "Computer Speech and Language,", "citeRegEx": "Bacchiani et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bacchiani et al\\.", "year": 2006}, {"title": "Pragmatic neural language modelling in machine translation", "author": ["Baltescu", "Blunsom2015] Paul Baltescu", "Phil Blunsom"], "venue": "In Proc. NAACL,", "citeRegEx": "Baltescu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Baltescu et al\\.", "year": 2015}, {"title": "Neural probabilistic language models", "author": ["Bengio et al.2006] Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Large language models in machine translation", "author": ["Ashok C. Popat", "Peng Xu", "Franz J. Och", "Jeffrey Dean"], "venue": "In Proc. EMNLP,", "citeRegEx": "Brants et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brants et al\\.", "year": 2007}, {"title": "Getting more mileage from web text sources for conversational speech language modeling using class-dependent mixtures", "author": ["Bulyko et al.2003] Ivan Bulyko", "Mari Ostendorf", "Andreas Stolcke"], "venue": "In Proc. HLT,", "citeRegEx": "Bulyko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bulyko et al\\.", "year": 2003}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Chen", "Goodman1996] Stanley F. Chen", "Joshua Goodman"], "venue": "In Proc. ACL,", "citeRegEx": "Chen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1996}, {"title": "Strategies for Training Large Vocabulary Neural Language Models", "author": ["W. Chen", "D. Grangier", "M. Auli"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Adaptive language modeling using minimum", "author": ["Vincent Della Pietra", "Robert L Mercer", "Salim Roukos"], "venue": null, "citeRegEx": "Pietra et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1992}, {"title": "An empirical investigation of discounting in cross-domain language models", "author": ["Durrett", "Klein2011] Greg Durrett", "Dan Klein"], "venue": "In Proc. ACL", "citeRegEx": "Durrett et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2011}, {"title": "The population frequencies of species and the estimation of population parameters", "author": ["Irving J Good"], "venue": null, "citeRegEx": "Good.,? \\Q1953\\E", "shortCiteRegEx": "Good.", "year": 1953}, {"title": "On using monolingual corpora in neural machine translation. CoRR, abs/1503.03535", "author": ["Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Generalized linear interpolation of language models", "author": ["Bo-June Hsu"], "venue": "In Proc. ASRU,", "citeRegEx": "Hsu.,? \\Q2007\\E", "shortCiteRegEx": "Hsu.", "year": 2007}, {"title": "Modeling long distance dependence in language: Topic mixtures versus dynamic cache models", "author": ["Iyer", "Ostendorf1999] Rukmini M Iyer", "Mari Ostendorf"], "venue": "Speech and Audio Processing, IEEE Transactions", "citeRegEx": "Iyer et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 1999}, {"title": "Interpolated estimation of markov source parameters from sparse data. In Workshop on pattern recognition in practice", "author": ["Jelinek", "Mercer1980] Frederick Jelinek", "Robert Mercer"], "venue": null, "citeRegEx": "Jelinek et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Jelinek et al\\.", "year": 1980}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["Slava M Katz"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "Katz.,? \\Q1987\\E", "shortCiteRegEx": "Katz.", "year": 1987}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "Proc. ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Kneser", "Ney1995] Reinhard Kneser", "Hermann Ney"], "venue": "In Proc. ICASSP,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "On the dynamic adaptation of stochastic language models", "author": ["Kneser", "Steinbiss1993] Reinhard Kneser", "Volker Steinbiss"], "venue": "In Proc. ICASSP,", "citeRegEx": "Kneser et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1993}, {"title": "A diversitypromoting objective function for neural conversation models. CoRR, abs/1510.03055", "author": ["Li et al.2015] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Proc. InterSpeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Neural network approach to word category prediction for English texts", "author": ["Katsuteru Maruyama", "Takeshi Kawabata", "Kiyohiro Shikano"], "venue": "In Proc. COLING", "citeRegEx": "Nakamura et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Nakamura et al\\.", "year": 1990}, {"title": "On structuring probabilistic dependences in stochastic language modelling", "author": ["Ney et al.1994] Hermann Ney", "Ute Essen", "Reinhard Kneser"], "venue": "Computer Speech and Language,", "citeRegEx": "Ney et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Ney et al\\.", "year": 1994}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Pham et al.2014] Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In Proc. ICFHR,", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "A maximum entropy approach to adaptive statistical language modelling", "author": ["Ronald Rosenfeld"], "venue": "Computer Speech and Language,", "citeRegEx": "Rosenfeld.,? \\Q1996\\E", "shortCiteRegEx": "Rosenfeld.", "year": 1996}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "Computer Speech and Language,", "citeRegEx": "Schwenk.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "LSTM neural networks for language modeling", "author": ["Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In Proc. InterSpeech", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Proc. NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A Bayesian interpretation of interpolated Kneser-Ney", "author": ["Yee Whye Teh"], "venue": "Technical report, School of Computing, National Univ. of Singapore", "citeRegEx": "Teh.,? \\Q2006\\E", "shortCiteRegEx": "Teh.", "year": 2006}, {"title": "Scaling recurrent neural network language models", "author": ["Niranjani Prasad", "David Mrva", "Tom Ash", "Tony Robinson"], "venue": "In Proc. ICASSP", "citeRegEx": "Williams et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2015}, {"title": "The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression", "author": ["Witten", "Bell1991] Ian H. Witten", "Timothy C. Bell"], "venue": "IEEE Transactions on Information", "citeRegEx": "Witten et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Witten et al\\.", "year": 1991}, {"title": "Recurrent neural network regularization. CoRR, abs/1409.2329", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "Recently, there has been a focus on LMs based on neural networks (Nakamura et al., 1990; Bengio et al., 2006; Mikolov et al., 2010), which have shown impressive improvements in performance over count-based LMs.", "startOffset": 65, "endOffset": 131}, {"referenceID": 3, "context": "Recently, there has been a focus on LMs based on neural networks (Nakamura et al., 1990; Bengio et al., 2006; Mikolov et al., 2010), which have shown impressive improvements in performance over count-based LMs.", "startOffset": 65, "endOffset": 131}, {"referenceID": 21, "context": "Recently, there has been a focus on LMs based on neural networks (Nakamura et al., 1990; Bengio et al., 2006; Mikolov et al., 2010), which have shown impressive improvements in performance over count-based LMs.", "startOffset": 65, "endOffset": 131}, {"referenceID": 7, "context": "On the other hand, these neural LMs also come at the cost of increased computational complexity at both training and test time, and even the largest reported neural LMs (Chen et al., 2015; Williams et al., 2015) are trained on a fraction of the data of their count-based counterparts (Brants et al.", "startOffset": 169, "endOffset": 211}, {"referenceID": 32, "context": "On the other hand, these neural LMs also come at the cost of increased computational complexity at both training and test time, and even the largest reported neural LMs (Chen et al., 2015; Williams et al., 2015) are trained on a fraction of the data of their count-based counterparts (Brants et al.", "startOffset": 169, "endOffset": 211}, {"referenceID": 4, "context": ", 2015) are trained on a fraction of the data of their count-based counterparts (Brants et al., 2007).", "startOffset": 80, "endOffset": 101}, {"referenceID": 22, "context": "1 can be used to describe not only n-gram models, but also feed-forward (Nakamura et al., 1990; Bengio et al., 2006; Schwenk, 2007) and recurrent (Mikolov et al.", "startOffset": 72, "endOffset": 131}, {"referenceID": 3, "context": "1 can be used to describe not only n-gram models, but also feed-forward (Nakamura et al., 1990; Bengio et al., 2006; Schwenk, 2007) and recurrent (Mikolov et al.", "startOffset": 72, "endOffset": 131}, {"referenceID": 26, "context": "1 can be used to describe not only n-gram models, but also feed-forward (Nakamura et al., 1990; Bengio et al., 2006; Schwenk, 2007) and recurrent (Mikolov et al.", "startOffset": 72, "endOffset": 131}, {"referenceID": 21, "context": ", 2006; Schwenk, 2007) and recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) neural network LMs (\u00a73).", "startOffset": 37, "endOffset": 85}, {"referenceID": 29, "context": ", 2006; Schwenk, 2007) and recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) neural network LMs (\u00a73).", "startOffset": 37, "endOffset": 85}, {"referenceID": 10, "context": "Static interpolation can be improved by calculating \u03bb(c) dynamically, using heuristics based on the frequency counts of the context (Good, 1953; Katz, 1987; Witten and Bell, 1991).", "startOffset": 132, "endOffset": 179}, {"referenceID": 16, "context": "Static interpolation can be improved by calculating \u03bb(c) dynamically, using heuristics based on the frequency counts of the context (Good, 1953; Katz, 1987; Witten and Bell, 1991).", "startOffset": 132, "endOffset": 179}, {"referenceID": 23, "context": "Discounting: The widely used technique of discounting (Ney et al., 1994) defines a fixed discount d and subtracts it from the count of each word before calculating probabilities:", "startOffset": 54, "endOffset": 72}, {"referenceID": 31, "context": "Modified KN is currently the defacto standard in n-gram LMs despite occasional improvements (Teh, 2006; Durrett and Klein, 2011), and we will express it as PKN (\u00b7).", "startOffset": 92, "endOffset": 128}, {"referenceID": 3, "context": "Feed-forward neural network LMs: Feedforward LMs (Bengio et al., 2006; Schwenk, 2007) are LMs that, like n-grams, calculate the probability of the next word based on the previous words.", "startOffset": 49, "endOffset": 85}, {"referenceID": 26, "context": "Feed-forward neural network LMs: Feedforward LMs (Bengio et al., 2006; Schwenk, 2007) are LMs that, like n-grams, calculate the probability of the next word based on the previous words.", "startOffset": 49, "endOffset": 85}, {"referenceID": 21, "context": "Recurrent neural network LMs: LMs using recurrent neural networks (RNNs) (Mikolov et al., 2010) consider not the previous few words, but also maintain a hidden state summarizing the sentence up until this point by re-defining the net in Eq.", "startOffset": 73, "endOffset": 95}, {"referenceID": 29, "context": "This allows for consideration of long-distance dependencies beyond the scope of standard n-grams, and LMs using RNNs or long short-term memory (LSTM) networks (Sundermeyer et al., 2012) have posted large improvements over standard n-grams and feed-forward models.", "startOffset": 159, "endOffset": 185}, {"referenceID": 3, "context": "Like most previous work on LMs (Bengio et al., 2006), we use a negative loglikelihood loss summed over words wi in every sentence w in corpusW", "startOffset": 31, "endOffset": 52}, {"referenceID": 28, "context": "In contrast to standard dropout (Srivastava et al., 2014), which drops out single nodes or connections, block dropout randomly drops out entire subsets of network nodes.", "startOffset": 32, "endOffset": 57}, {"referenceID": 28, "context": "5 for both feed-forward (Srivastava et al., 2014) and LSTM (Pham et al.", "startOffset": 24, "endOffset": 49}, {"referenceID": 24, "context": ", 2014) and LSTM (Pham et al., 2014) nets in the neural LMs and neural/n-gram hybrids, but not in the neurally interpolated n-grams, where it resulted in slightly worse perplexities.", "startOffset": 17, "endOffset": 36}, {"referenceID": 21, "context": "For the main experiments, we evaluate on two corpora: the Penn Treebank (PTB) data set prepared by Mikolov et al. (2010),2 and the first 100k sentences in the English side of the ASPEC corpus (Nakazawa et al.", "startOffset": 99, "endOffset": 121}, {"referenceID": 34, "context": "As our main baseline, we compare to LSTMs with only \u03b4 distributions, which have reported competitive numbers on the PTB data set (Zaremba et al., 2014).", "startOffset": 129, "endOffset": 151}, {"referenceID": 20, "context": "In addition to better perplexities, neural/n-gram hybrids are trained in a single pass instead of performing post-facto interpolation, which may give advantages when training for other objectives (Auli and Gao, 2014; Li et al., 2015).", "startOffset": 196, "endOffset": 233}, {"referenceID": 5, "context": "A number of alternative methods focus on interpolating LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 121, "endOffset": 189}, {"referenceID": 1, "context": "A number of alternative methods focus on interpolating LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 121, "endOffset": 189}, {"referenceID": 11, "context": "A number of alternative methods focus on interpolating LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 121, "endOffset": 189}, {"referenceID": 25, "context": "Also conceptually similar is work on adaptation of n-gram LMs, which start with n-gram probabilities (Della Pietra et al., 1992; Kneser and Steinbiss, 1993; Rosenfeld, 1996; Iyer and Ostendorf, 1999) and adapt them based on the distribution of the current document, albeit in a linear model.", "startOffset": 101, "endOffset": 199}, {"referenceID": 7, "context": "Finally, recent works have compared n-gram and neural models, finding that neural models often perform better in perplexity, but n-grams have their own advantages such as effectiveness in extrinsic tasks (Baltescu and Blunsom, 2015) and better modeling of rare words (Chen et al., 2015).", "startOffset": 267, "endOffset": 286}, {"referenceID": 1, "context": ", 2003; Bacchiani et al., 2006; G\u00fcl\u00e7ehre et al., 2015). Perhaps most relevant is Hsu (2007)\u2019s work on learning to interpolate multiple LMs using log-linear models.", "startOffset": 8, "endOffset": 92}, {"referenceID": 30, "context": "As the framework discussed here is general, it is also possible that they could be used in other tasks that perform sequential prediction of words such as neural machine translation (Sutskever et al., 2014) or dialog response generation (Sordoni et al.", "startOffset": 182, "endOffset": 206}, {"referenceID": 27, "context": ", 2014) or dialog response generation (Sordoni et al., 2015).", "startOffset": 38, "endOffset": 60}], "year": 2017, "abstractText": "Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.", "creator": "LaTeX with hyperref package"}}}