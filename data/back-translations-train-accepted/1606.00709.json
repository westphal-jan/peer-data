{"id": "1606.00709", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization", "abstract": "Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.", "histories": [["v1", "Thu, 2 Jun 2016 14:53:33 GMT  (2657kb,D)", "http://arxiv.org/abs/1606.00709v1", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["sebastian nowozin", "botond cseke", "ryota tomioka"], "accepted": true, "id": "1606.00709"}, "pdf": {"name": "1606.00709.pdf", "metadata": {"source": "CRF", "title": "f -GAN: Training Generative Neural Samplers using Variational Divergence Minimization", "authors": ["Sebastian Nowozin", "Botond Cseke"], "emails": ["Sebastian.Nowozin@microsoft.com", "botcse@microsoft.com", "ryoto@microsoft.com"], "sections": [{"heading": null, "text": "Generative neural samplers are probabilistic models that implement the sample using feedback-forward neural networks: they take a random input vector and generate a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient calculation of samples and derivatives, but cannot be used to calculate probabilities or marginalization.The generative-adversarial training method allows to train such models by using an auxiliary discriminatory neural network. We show that the generative-adversarial approach is a special case of an existing more general approach to estimating variation divergences. We show that any f-divergence can be used to form generative neural samplers. We discuss the advantages of different options of divergence functions on the complexity of training and the quality of the obtained generative models."}, {"heading": "1 Introduction", "text": "Probabilistic generative models describe a probability distribution over a given DomainX, for example a distribution over natural language sets, natural images or recorded waveforms. Given a generative model Q from a class Q of possible models, we are generally interested in performing one or more of the following operations: \u2022 Sampling. Produce a sample from Q. By examining samples or calculating a function on a set of samples, we can gain important insights into the distribution or solve decision problems. \u2022 Estimation. Given a set of iid samples {x1, x2,., xn} from an unknown true distribution P, you will find Q that best describes the true distribution. \u2022 Punctual probability calculation evaluation by x, evaluate the probability Q (x). Generative adversarial networks (GAN) in the form that we have proposed are an expressive class of generative models that allow exact sampling and approximate estimation."}, {"heading": "2 Method", "text": "We first review the divergence estimation system of Nguyen et al. [25], which is based on f-divergences, and then expand it from divergence estimates to model estimates."}, {"heading": "2.1 The f-divergence Family", "text": "Statistical divergences such as the well-known Kullback-Leibler divergence measure the difference between two given probability distributions. A large class of different divergences are the so-called f-divergences [5, 21], also known as Ali-Silvey distances [1]. In view of two distributions P and Q, each having an absolutely continuous density function p and q in relation to a basic measure dx defined on domain X, we define f-divergence, Df (P-Q) = x q (x) f (p (x) q (x)))) dx, (2), where the generator function f: R + \u2192 R is a convex, low-semicontinuous function satisfying f (1) = 0."}, {"heading": "2.2 Variational Estimation of f -divergences", "text": "We will extend their method from merely estimating divergence for a fixed model to estimating model parameters. We call this new method variation divergence minimization (VDM) and show that generative-adversarial formation is a special case of this more general VDM framework.For completeness, we first provide a self-contained derivative of Nguyen and al's divergence estimation method. Each convex, lower semicontinuous function f has a convex conjugate function f, also known as fennel conjugate [14], which is defined as a derivative of Nguyen and al's divergence estimation method. (3) The function f \u00b2 is again convex and lower conjugate function f x x x x, and the pair f \u00b2 x x x is a dual derivative in the sense that f \u00b2 -f = f \u00b2.Therefore, we can represent a divergence."}, {"heading": "2.3 Variational Divergence Minimization (VDM)", "text": "We now use the lower limit of variation (4) on the f -divergence Df (P-Q) to estimate a generative model Q with a true distribution P. To this end, we follow the generative-adversarial approach [10] and use two neural networks, Q and T. Q is our generative model by taking a random vector as input and output a sample of interest. We parameterise Q by a vector and write Q\u03b8. T is our variation function by taking a sample as input and returning a scalar. We parameterise T with the help of a vector \u03c9 and write T. We can learn a generative model QTB by finding a saddle point of the following f -GAN objective function, where we minimise its constants and maximise F (TB) = Ex-P [TIS (x)] \u2212 Ex-Q1 (TIS (x)]]."}, {"heading": "2.4 Representation for the Variational Function", "text": "To apply the variation object (6) to different f divergences, we must respect the domain (domain) of the conjugate functions f *. To this end, we assume that the variation function T\u03c9 is represented in the form T\u03c9 (x) = gf (V\u03c9 (x)) and rewrite the saddle object (6) as follows: F (\u03b8, \u03c9) = Ex \u00b2 P [gf (V\u03c9 (x))] + Ex \u00b2 divergence. In Table 6, we propose appropriate output activation functions for the various conjugate functions f \u00b2 and their domains. [Although the choice of gf \u00b2 is somewhat arbitrary, we select them all as monotonous, increasing functions, so that a large output divergence (x) corresponds to the various conjugate functions f \u00b2 and their domains, so that the variation (domain) corresponds to the function f \u00b2 x."}, {"heading": "2.5 Example: Univariate Mixture of Gaussians", "text": "In order to demonstrate the properties of the different f-divergences and validate the variation divergence estimates, we conduct an experiment similar to that of [24]. Setup. We approach a mixture of Gaussians by learning a Gaussian distribution. We represent our model Q\u03b8 with a linear function that obtains a random z-divergence. N (0, 1) and the results of the scalar function f-divergence (z) are implemented directly, rather than evaluating the two functions in sequence; see Figure 1.We use a neural network with two hidden layers of 64 units each and concrete activations. We optimize the target F-divergence by using the one-step gradient method f."}, {"heading": "3 Algorithms for Variational Divergence Minimization (VDM)", "text": "We distinguish between two methods: firstly, the interchange method originally proposed by Goodfellow et al. [10] and secondly, a more direct single-stage optimization method. In our variation framework, the interchange gradient method can be described as a double loop method; the internal loop tightens the lower limit of divergence, while the outer loop improves the generator model. While the motivation for this method is plausible, the decision to take a single step in the inner loop is popular in practice. Goodfellow et al. [10] offer a local convergence guarantee."}, {"heading": "3.1 Single-Step Gradient Method", "text": "Motivated by the success of the alternating gradient method with a single inner step, we propose a simpler algorithm shown in Algorithm 1. The algorithm differs from the original algorithm in that there is no inner loop and the gradients in relation to 270 and 270 are calculated in a single backward propagation. (Algorithm 1 Single-Step Gradient Method 1: Function SINGLESTEPGRADIENTITERATION (P, \u03b8t, \u03c9t, B, \u03b7) 2: Example XP = {x1,.., xB} and XQ = {x \u2032 1,..., x \u2032 B}, by P and Q\u0445IENTITERATION, resp. 3: Update: perspect + 1 = Expanded + \u03c9t, B, \u03b7). (Example XP = {x1,.). 4: Update: Procedure + 1 = Procedure set in relation to the structure F (Update: + 1)."}, {"heading": "3.2 Practical Considerations", "text": "Here we discuss principled extensions of the heuristics proposed in [10] and real / fake statistics discussed by Larsen and S\u00f8nderby2. Furthermore, we discuss practical advice that slightly differs from the principle point of view. Goodfellow et al. [10] noted that the formation of GAN can be significantly accelerated by maximizing the Ex-GAN method rather than minimizing Ex-GAN (x) in order to update the generator. In the more general f-GAN algorithm (1), this means that line 4 is maintained by the update expertise + 1 = Vicente + Vicente + Civilization EX-Q\u03b8t [gf (V\u03c9t (x)]], (10) thereby maximizing generator performance. This is not only intuitively correct, but we can show that the stationary point is maintained by this change by using the same argument as in [10]."}, {"heading": "4 Experiments", "text": "We are dealing with a number of countries that are able to surpass themselves, both in the United States and in Europe. (...) We have it in our hand to surpass ourselves. (...) \"We do not have it easily.\" (...) \"We have it in our hand.\" (...) \"We have it in our hand.\" (...) \"We have it in our hand.\" (...) \"We have it in our hand.\" \"(...)\" We have it in our hand. \"(...)\" We have it in our hand. \"(...)\" We have it. \"(...)\" We have it in our hand. \"(...)\" We have it in our hand. \"(...)\" (...) \"(...)\" We have it in our hand. \"(...)\" (...) \"We have it.\" (...) \"We have it.\" (...). \"We.\" We. \"We.\" (...) \"We have it.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"(...)\" We have it in our hand. \"(...)\" We have it. \"We have it.\" (... \"We have it.\" We have it. \"(...)\" We have it. \"We have it.\" (... \"We have it.\" We have it. \"We.\" We have. \"(...\" We have it. \"We.\" We. \"We have.\" (...). \"We have it.\" We have it. (... \"We have it.\" We have it. \"We have it. (...\"). \"We have it.\" We have it. (... \"We have it.\" We have it. (. \").\" We have it. (... \").\" We have. \"We have it.\" We have it. \"We have it.\" We. (. (. \").\" We. \"We have it.\" We have it. (. \"We.\"). \"We have it.\" We have it. \"We.\" (. \"We have. (.\"). \"We.\" We have. \"We.\" We. \"We.\" We. \"We have it.\" We. \""}, {"heading": "5 Related Work", "text": "We will now discuss how our approach relates to existing work. Building generative models of real distributions is a fundamental goal of machine learning and much related work. We will only discuss the work applied to neural network models. Mixture Density Networks [2] are neural networks that directly regress the parameters of a finite parametric mixing model. In combination with a recursive neural network, this results in an impressive generative model of handwritten text [11].NADE [19] and RNADE [33] factorize output by means of a predefined and somewhat arbitrary arrangement of output dimensions. The resulting model of a variable is based on the entire history of past variables. These models provide comprehensible probability assessments and convincing results, but it is unclear how to select the factorization of results in many applications. Diffusion probabilities are defined as the result of a learned diffusion process."}, {"heading": "6 Discussion", "text": "Generative neural samplers provide a powerful way to represent complex distributions without limiting factorizing assumptions. Although the purely generative neural samplers used in this paper are interesting, their use is limited, as they cannot be conditioned on observed data after training and therefore cannot provide conclusions. We believe that the true utility of neural samplers to represent uncertainty will be found in discriminatory models, and our methods willingly extend to this case, providing additional inputs to both the generator and the variation function, as in the conditional GAN model [8]."}, {"heading": "B f -divergences and Generator-Conjugate Pairs", "text": "In Table 5 we show an extended list of f divergences Df (P-Q) together with their generators q (u) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) (q) (q) q (q) (q) q (q) (q) q (q) q) q (q) (q) (q) q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) x) x \u2212 p (x) for all divergences P) 6 = 0. Table 6 lists the conjugation functions f (t) of the generator functions f (u) in Table 5, whose domain functions f x x x) x x (x), which we obtain in the last layers of the grid mapping to use a correction of the functions."}, {"heading": "C Proof of Theorem 1", "text": "In this section we present the proof for theorem 2 from section 3 of the main text. To ensure completeness, we repeat the conditions and theory.We assume that F (1) is strongly convex and strongly concave depending on the fact that these assumptions are necessary, except for the \"strong\" part to define the type of saddle solutions that are valid solutions in our variable framework.10-1 100 101u = dP / d\u00b5dQ / d\u00b5dQ (u) f-divergence generators f (u) Squared Kullback Leibler / Chi-Square Reverse Kullback Leibler Total VariationJeffrey Neyman-Shon5 4."}, {"heading": "D Related Algorithms", "text": "Due to the recent interest in GAN models, there are attempts to derive other divergence measures and algorithms (\u03b2 \u03b2). In this section, we will summarize (some of) the current algorithms and show how they are related to each other. Note: Some algorithms use heuristics that do not match the optimization of the saddle point, that is, in the appropriate maximization and minimization steps, they optimize alternative goals that do not add up to a coherent common goal. We include a brief discussion of [13] because they are a special case of GAN.To illustrate how the discussed algorithms work, we define the objective functionF (ell, ell, \u03b2) = Ex-P."}, {"heading": "E Details of the Univariate Example", "text": "We follow the example in section 2.5 of the main text by presenting further details on the quality and behaviour of approximate values resulting from the use of different measures of divergence. For the sake of completeness, we repeat the structure and then present further results. Setup. we approach a mixture of Gaussian tails 4 by learning a Gaussian distribution. the model Q\u03b8 is represented by a linear function representing a random z \u00b2 N (0, 1) and outputG\u03b8 (z) = \u00b5 + \u03c3z, (19) although the parameters to be learned are. For the variation function T\u03c9, we use the neural networkx \u2192 linear (1.64) \u2192 linear (64.64) \u2192 linear (64.1), (20) We optimize the objective F (\u03c9, \u03b8) the exact gradient method presented in section 3.1 of the main text."}, {"heading": "F Details of the Experiments", "text": "In this section, we will present the technical setup and architectures used in the experiments described in Section 4.F.1. We will use the Deep Learning Framework Chainer [32], version 1.8.1, which runs on CUDA 7.5 with CuDNN v5 on NVIDIA GTX TITAN X.F.2 MNIST SetupMNIST Generatorz \u2192 Linear (100, 1200) \u2192 BN \u2192 Linear (1200, 1200) \u2192 BN \u2192 Linear (240,240) \u2192 Linear (1200, 784) \u2192 Sigmoid (21) All weights are initialized on a weight scale of 0.05, as in [10].MNIST \u2192 Variational Functionx \u2192 Linear (784,240) \u2192 ELU \u2192 Linear (240,240) \u2192 Linear (240,1), (22), where ELU is the exponential linear unit [4]."}], "references": [{"title": "A general class of coefficients of divergence of one distribution from another", "author": ["S.M. Ali", "S.D. Silvey"], "venue": "JRSS (B), pages 131\u2013142,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1966}, {"title": "Mixture density networks", "author": ["C.M. Bishop"], "venue": "Technical report, Aston University,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "GTM: The generative topographic mapping", "author": ["C.M. Bishop", "M. Svens\u00e9n", "C.K.I. Williams"], "venue": "Neural Computation, 10(1):215\u2013234,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Fast and accurate deep network learning by exponential linear units (ELUs)", "author": ["D.A. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "arXiv:1511.07289,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Information theory and statistics: A tutorial", "author": ["I. Csisz\u00e1r", "P.C. Shields"], "venue": "Foundations and Trends in Communications and Information Theory, 1:417\u2013528,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "NIPS, pages 2933\u20132941,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Training generative neural networks via maximum mean discrepancy optimization", "author": ["G.K. Dziugaite", "D.M. Roy", "Z. Ghahramani"], "venue": "UAI, pages 258\u2013267,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional generative adversarial nets for convolutional face generation", "author": ["J. Gauthier"], "venue": "Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester 2014,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Strictly proper scoring rules, prediction, and estimation", "author": ["T. Gneiting", "A.E. Raftery"], "venue": "JASA, 102(477): 359\u2013378,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS, pages 2672\u20132680,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv:1308.0850,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "A kernel statistical test of independence", "author": ["A. Gretton", "K. Fukumizu", "C.H. Teo", "L. Song", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "NIPS, pages 585\u2013592,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "AISTATS, pages 297\u2013304,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Fundamentals of convex analysis", "author": ["J.B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": "Springer,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "How (not) to train your generative model: scheduled sampling, likelihood, adversary", "author": ["F. Husz\u00e1r"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv:1412.6980,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv:1402.0030,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "AISTATS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Generative moment matching networks", "author": ["Y. Li", "K. Swersky", "R. Zemel"], "venue": "ICML,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "On divergences and informations in statistics and information theory", "author": ["F. Liese", "I. Vajda"], "venue": "Information Theory, IEEE, 52(10):4394\u20134412,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Bayesian neural networks and density networks", "author": ["D.J.C. MacKay"], "venue": "Nucl. Instrum. Meth. A, 354(1):73\u201380,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "Adversarial autoencoders", "author": ["A. Makhzani", "J. Shlens", "N. Jaitly", "I. Goodfellow"], "venue": "arXiv:1511.05644,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Divergence measures and message passing", "author": ["T. Minka"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization", "author": ["X. Nguyen", "M.J. Wainwright", "M.I. Jordan"], "venue": "Information Theory, IEEE, 56(11):5847\u20135861,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "On the chi-square and higher-order chi distances for approximating f-divergences", "author": ["F. Nielsen", "R. Nock"], "venue": "Signal Processing Letters, IEEE, 21(1):10\u201313,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv:1511.06434,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "ICML, pages 1278\u20131286,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep unsupervised learning using non-equilibrium thermodynamics", "author": ["J. Sohl-Dickstein", "E.A. Weiss", "N. Maheswaranathan", "S. Ganguli"], "venue": "ICML, pages 2256\u20132265,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["B.K. Sriperumbudur", "A. Gretton", "K. Fukumizu", "B. Sch\u00f6lkopf", "G. Lanckriet"], "venue": "JMLR, 11:1517\u20131561,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. v.d. Oord", "M. Bethge"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["S. Tokui", "K. Oono", "S. Hido", "J. Clayton"], "venue": "NIPS,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "RNADE: The real-valued neural autoregressive density-estimator", "author": ["B. Uria", "I. Murray", "H. Larochelle"], "venue": "NIPS, pages 2175\u20132183,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop", "author": ["F. Yu", "Y. Zhang", "S. Song", "A. Seff", "J. Xiao"], "venue": "arXiv:1506.03365,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Generative-adversarial networks (GAN) in the form proposed by [10] are an expressive class of generative models that allow exact sampling and approximate estimation.", "startOffset": 62, "endOffset": 66}, {"referenceID": 20, "context": "Such probabilistic feedforward neural network models were first considered in [22] and [3], here we call these models generative neural samplers.", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "Such probabilistic feedforward neural network models were first considered in [22] and [3], here we call these models generative neural samplers.", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "GAN is also of this type, as is the decoder model of a variational autoencoder [18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "[25] to recover the GAN training objective and generalize it to arbitrary f -divergences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] and provide a theoretical justification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] which is based on f -divergences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "A large class of different divergences are the so called f -divergences [5, 21], also known as the Ali-Silvey distances [1].", "startOffset": 72, "endOffset": 79}, {"referenceID": 19, "context": "A large class of different divergences are the so called f -divergences [5, 21], also known as the Ali-Silvey distances [1].", "startOffset": 72, "endOffset": 79}, {"referenceID": 0, "context": "A large class of different divergences are the so called f -divergences [5, 21], also known as the Ali-Silvey distances [1].", "startOffset": 120, "endOffset": 123}, {"referenceID": 23, "context": "[25] derive a general variational method to estimate f -divergences given only samples from P and Q.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Every convex, lower-semicontinuous function f has a convex conjugate function f\u2217, also known as Fenchel conjugate [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 24, "context": "Part of the list of divergences and their generators is based on [26].", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "As shown by [10] GAN is related to the Jensen-Shannon divergence through DGAN = 2DJS \u2212 log(4).", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "T , we find that under mild conditions on f [25], the bound is tight for T \u2217(x) = f \u2032 ( p(x) q(x) ) , (5)", "startOffset": 44, "endOffset": 48}, {"referenceID": 9, "context": "To this end, we follow the generative-adversarial approach [10] and use two neural networks, Q and T .", "startOffset": 59, "endOffset": 63}, {"referenceID": 22, "context": "To demonstrate the properties of the different f -divergences and to validate the variational divergence estimation framework we perform an experiment similar to the one of [24].", "startOffset": 173, "endOffset": 177}, {"referenceID": 9, "context": "[10], and second, a more direct single-step optimization procedure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] provide a local convergence guarantee.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "These conditions are similar to the assumptions made in [10] and can be formalized as follows: \u2207\u03b8F (\u03b8\u2217, \u03c9\u2217) = 0, \u2207\u03c9F (\u03b8\u2217, \u03c9\u2217) = 0, \u2207\u03b8F (\u03b8, \u03c9) \u03b4I, \u2207\u03c9F (\u03b8, \u03c9) \u2212\u03b4I.", "startOffset": 56, "endOffset": 60}, {"referenceID": 5, "context": "Note that although there could be many saddle points that arise from the structure of deep networks [6], they do not qualify as the solution of our variational framework under these assumptions.", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "Here we discuss principled extensions of the heuristic proposed in [10] and real/fake statistics discussed by Larsen and S\u00f8nderby2.", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "[10] noticed that training GAN can be significantly sped up by maximizing Ex\u223cQ\u03b8 [logD\u03c9(x)] instead of minimizing Ex\u223cQ\u03b8 [log (1\u2212D\u03c9(x))] for updating the generator.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "This is not only intuitively correct but we can show that the stationary point is preserved by this change using the same argument as in [10]; we found this useful also for other divergences.", "startOffset": 137, "endOffset": 141}, {"referenceID": 15, "context": "We found Adam [17] and gradient clipping to be useful especially in the large scale experiment on the LSUN dataset.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "We use the MNIST training data set (60,000 samples, 28-by-28 pixel images) to train the generator and variational function model proposed in [10] for various f -divergences.", "startOffset": 141, "endOffset": 145}, {"referenceID": 3, "context": "The variational function V\u03c9(x) has three linear layers with exponential linear unit [4] in between.", "startOffset": 84, "endOffset": 87}, {"referenceID": 25, "context": "As in [27] we use Adam with a learning rate of \u03b1 = 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "We also compare against variational autoencoders [18] with 20 latent dimensions.", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "We evaluate the performance using the kernel density estimation (Parzen window) approach used in [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 29, "context": "The use of the KDE approach to log-likelihood estimation has known deficiencies [31].", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": "97 Variational Autoencoder [18] 445 5.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "Through the DCGAN work [27] the generative-adversarial approach has shown real promise in generating natural looking images.", "startOffset": 23, "endOffset": 27}, {"referenceID": 25, "context": "Here we use the same architecture as as in [27] and replace the GAN objective with our more general f -GAN objective.", "startOffset": 43, "endOffset": 47}, {"referenceID": 32, "context": "We use the large scale LSUN database [34] of natural images of different categories.", "startOffset": 37, "endOffset": 41}, {"referenceID": 25, "context": "We use the generator architecture and training settings proposed in DCGAN [27].", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "The variational function is the same as the discriminator architecture in [27] and follows the structure of a convolutional neural network with batch normalization, exponential linear units [4] and one final linear layer.", "startOffset": 74, "endOffset": 78}, {"referenceID": 3, "context": "The variational function is the same as the discriminator architecture in [27] and follows the structure of a convolutional neural network with batch normalization, exponential linear units [4] and one final linear layer.", "startOffset": 190, "endOffset": 193}, {"referenceID": 1, "context": "Mixture density networks [2] are neural networks which directly regress the parameters of a finite parametric mixture model.", "startOffset": 25, "endOffset": 28}, {"referenceID": 10, "context": "When combined with a recurrent neural network this yields impressive generative models of handwritten text [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 17, "context": "NADE [19] and RNADE [33] perform a factorization of the output using a predefined and somewhat arbitrary ordering of output dimensions.", "startOffset": 5, "endOffset": 9}, {"referenceID": 31, "context": "NADE [19] and RNADE [33] perform a factorization of the output using a predefined and somewhat arbitrary ordering of output dimensions.", "startOffset": 20, "endOffset": 24}, {"referenceID": 27, "context": "Diffusion probabilistic models [29] define a target distribution as a result of a learned diffusion process which starts at a trivial known distribution.", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "Noise contrastive estimation (NCE) [13] is a method that estimates the parameters of unnormalized probabilistic models by performing non-linear logistic regression to discriminate the data from artificially generated noise.", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "The generative neural sampler models of [22] and [3] did not provide satisfactory learning methods; [22] used importance sampling and [3] expectation maximization.", "startOffset": 40, "endOffset": 44}, {"referenceID": 2, "context": "The generative neural sampler models of [22] and [3] did not provide satisfactory learning methods; [22] used importance sampling and [3] expectation maximization.", "startOffset": 49, "endOffset": 52}, {"referenceID": 20, "context": "The generative neural sampler models of [22] and [3] did not provide satisfactory learning methods; [22] used importance sampling and [3] expectation maximization.", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "The generative neural sampler models of [22] and [3] did not provide satisfactory learning methods; [22] used importance sampling and [3] expectation maximization.", "startOffset": 134, "endOffset": 137}, {"referenceID": 16, "context": "Variational auto-encoders (VAE) [18, 28] are pairs of probabilistic encoder and decoder models which map a sample to a latent representation and back, trained using a variational Bayesian learning objective.", "startOffset": 32, "endOffset": 40}, {"referenceID": 26, "context": "Variational auto-encoders (VAE) [18, 28] are pairs of probabilistic encoder and decoder models which map a sample to a latent representation and back, trained using a variational Bayesian learning objective.", "startOffset": 32, "endOffset": 40}, {"referenceID": 21, "context": "The advantage of VAEs is in the encoder model which allows efficient inference from observation to latent representation and overall they are a compelling alternative to f -GANs and recent work has studied combinations of the two approaches [23] As an alternative to the GAN training objective the work [20] and independently [7] considered the use of the kernel maximum mean discrepancy (MMD) [12, 9] as a training objective for probabilistic models.", "startOffset": 241, "endOffset": 245}, {"referenceID": 18, "context": "The advantage of VAEs is in the encoder model which allows efficient inference from observation to latent representation and overall they are a compelling alternative to f -GANs and recent work has studied combinations of the two approaches [23] As an alternative to the GAN training objective the work [20] and independently [7] considered the use of the kernel maximum mean discrepancy (MMD) [12, 9] as a training objective for probabilistic models.", "startOffset": 303, "endOffset": 307}, {"referenceID": 6, "context": "The advantage of VAEs is in the encoder model which allows efficient inference from observation to latent representation and overall they are a compelling alternative to f -GANs and recent work has studied combinations of the two approaches [23] As an alternative to the GAN training objective the work [20] and independently [7] considered the use of the kernel maximum mean discrepancy (MMD) [12, 9] as a training objective for probabilistic models.", "startOffset": 326, "endOffset": 329}, {"referenceID": 11, "context": "The advantage of VAEs is in the encoder model which allows efficient inference from observation to latent representation and overall they are a compelling alternative to f -GANs and recent work has studied combinations of the two approaches [23] As an alternative to the GAN training objective the work [20] and independently [7] considered the use of the kernel maximum mean discrepancy (MMD) [12, 9] as a training objective for probabilistic models.", "startOffset": 394, "endOffset": 401}, {"referenceID": 8, "context": "The advantage of VAEs is in the encoder model which allows efficient inference from observation to latent representation and overall they are a compelling alternative to f -GANs and recent work has studied combinations of the two approaches [23] As an alternative to the GAN training objective the work [20] and independently [7] considered the use of the kernel maximum mean discrepancy (MMD) [12, 9] as a training objective for probabilistic models.", "startOffset": 394, "endOffset": 401}, {"referenceID": 28, "context": "MMD is a particular instance of a larger class of probability metrics [30] which all take the form D(P,Q) = supT\u2208T |Ex\u223cP [T (x)]\u2212 Ex\u223cQ[T (x)]|, where the function class T is chosen in a manner specific to the divergence.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "In [16] a generalisation of the GAN objective is proposed by using an alternative Jensen-Shannon divergence that mimics an interpolation between the KL and the reverse KL divergence and has Jensen-Shannon as its mid-point.", "startOffset": 3, "endOffset": 7}], "year": 2016, "abstractText": "Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generativeadversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f -divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.", "creator": "LaTeX with hyperref package"}}}