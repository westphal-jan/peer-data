{"id": "1606.00739", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Stochastic Structured Prediction under Bandit Feedback", "abstract": "Stochastic structured prediction under bandit feedback follows a learning protocol where on each of a sequence of iterations, the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure. We introduce stochastic approximation algorithms that apply this learning scenario to probabilistic structured prediction, with a focus on asymptotic convergence and ease of elicitability of feedback. We present simulation experiments for complex natural language processing tasks, showing fastest empirical convergence and smallest empirical variance for stochastic optimization of a non-convex pairwise preference learning objective compared to stochastic optimization of related non-convex and convex objectives.", "histories": [["v1", "Thu, 2 Jun 2016 16:06:29 GMT  (25kb)", "https://arxiv.org/abs/1606.00739v1", null], ["v2", "Wed, 2 Nov 2016 16:29:42 GMT  (25kb)", "http://arxiv.org/abs/1606.00739v2", "30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["artem sokolov", "julia kreutzer", "stefan riezler", "christopher lo"], "accepted": true, "id": "1606.00739"}, "pdf": {"name": "1606.00739.pdf", "metadata": {"source": "CRF", "title": "Stochastic Structured Prediction under Bandit Feedback", "authors": ["Artem Sokolov", "Julia Kreutzer", "Christopher Lo", "Stefan Riezler"], "emails": ["sokolov@cl.uni-heidelberg.de", "kreutzer@cl.uni-heidelberg.de", "riezler@cl.uni-heidelberg.de", "chris.aa.lo@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.00 739v 2 [cs.C L] 2"}, {"heading": "1 Introduction", "text": "The algorithms apply to situations in which learning situations are predicted, in which a learning protocol is available: On each sequence of iterations, the learner receives an input, predicts an output structure, and receives partial feedback in the form of a task evaluation of the predicted structure. We present algorithms that use this feedback to \"banditize\" expected loss scenarios to approach structured approaches to structured prediction [18, 25]. The algorithms follow the structure of conducting simultaneous exploration / exploitation of output structures from a log-linear probability model, feedback on the sampled structure, and performing an update in the negative direction of the respective full information objective. The algorithms apply to situations in which learning processes are predicted, in which learning processes are performed."}, {"heading": "2 Related Work", "text": "The methods presented in this paper relate to various other machine learning processes in which predictions of large production spaces must be learned from partial information.Reinforcement learning aims to maximize the expected reward for selecting a particular state in a Markov Decision Process (MDP) model.The algorithms in this paper can be considered as a single state MDPs with context in which selecting an action corresponds to predicting a structured output. Most closely related are the recent applications of political gradients to exponential output spaces in NLP problems."}, {"heading": "3 Expected Loss Minimization for Structured Prediction", "text": "[18, 25] Introduction of the expected loss criterion for structured predictions as a minimization of the expectation of a given task loss function with respect to conditional distribution over structured outputs. Let X be a structured input space, let Y (x) be the set of possible output structures for input x, and let Y (0, 1) quantify the loss y (y) for prediction y (y) instead of the gold standard structure y. In the complete information setting, for a given (empirical) data distribution p (x, y) the learning problem is defined as if the user is losing y (x, y) pw (y) pw (y) pw (y) pw (y) | x) = minw \"Rd\" x \"x\" x \"x\" x \"x\" x \"(x, y) x\" y \"c\" y \"(x) x\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c.\""}, {"heading": "4 Stochastic Structured Prediction under Partial Feedback", "text": "Algorithm 1 shows the structure of the methods analyzed in this paper by showing different estimates and their effects. It assumes a sequence of input structures generated by a fixed, unknown distribution (line 4). For each randomly selected input structure, an output y (line 6) is collected to perform simultaneous exploitation (use the current best estimate) / exploration (get new information) about output structures (line 5). Then, a feedback to the predicted structure is achieved (line 6). An update is performed by taking a step in the negative direction of the stochastic gradient, at a rate of 8. As a post-optimization step, a list of vectors w0 is chosen.,., wT) (line 8).Given algorithm 1, we can formalize the term \"banditization\" of objective functions by presenting different estimates."}, {"heading": "5 Convergence Analysis", "text": "To analyze convergence, we describe the EL, PR, and CE algorithms as first-order stochastic methods (SFO) of the first order (SFO) within the framework of [7]. We assume that lower, differentiable objective functions J (w) with Lipschitz continuous gradient [J (w) + w. \"\u2212\" These conditions to be met are the unbiasedness of the gradient estimate E [st] = [st]. \u2212 \"The constant convergence estimate [st].\" \u2212 \"The convergence estimate [st].\" \u2212 \"The convergence estimate [st].\" \u2212 \"The convergence estimate [st].\" \u2212 \"The convergence estimate [st].\" \u2212 \"The convergence estimate [st].\""}, {"heading": "6 Experiments", "text": "In fact, it is so that it is a matter of a way in which people are able to survive themselves. (...) In fact, it is so that they are able to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) \"(...)\" (...) (() () (() () (()) (() () () () () () () () () () () ()) () () () () () () () () () () () () () ()) () () ()) () () () () () () () ()) () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () (() () (() (() () () () () ((() (() () () (() (() () ((() () ((() (() () ((() ((() (() (() () (() (() () (() () (() ((() () (((() ((() (() () () (() ((() (((((() () () (((() () ("}, {"heading": "7 Conclusion", "text": "We presented learning objectives and algorithms for stochastic structured predictions under bandit feedback. The presented methods \"bandit\" well-known approaches to probabilistic structured predictions such as expected loss minimization, pairwise preference ranking and cross entropy minimization. We presented a comparison of practical convergence criteria based on early termination with theoretically motivated convergence criteria based on the square gradient standard. Our experimental results showed the fastest convergence rate under both criteria for pairwise preference learning. Our numerical evaluation showed smallest variance for pairwise preference learning, which possibly explains the fastest convergence despite the underlying non-convex target. Moreover, since this algorithm requires only readily attainable relative preference feedback for learning, it is an attractive choice for practical interactive learning scenarios."}], "references": [{"title": "Optimal algorithms for online convex optimization with multi-point bandit feedback. In COLT", "author": ["A. Agarwal", "O. Dekel", "L. Xiao"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "A survey of preference-based online learning with bandit algorithms. In ALT", "author": ["R. Busa-Fekete", "E. H\u00fcllermeier"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning to search better than your teacher", "author": ["Chang", "K.-W", "A. Krishnamurthy", "A. Agarwal", "H. Daume", "J. Langford"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Optimal rates for zero-order convex optimization: The power of two function evaluations", "author": ["J.C. Duchi", "M.I. Jordan", "M.J. Wainwright", "A. Wibisono"], "venue": "IEEE Translactions on Information", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["C. Dyer", "A. Lopez", "J. Ganitkevitch", "J. Weese", "F. Ture", "P. Blunsom", "H. Setiawan", "V. Eidelman", "P. Resnik"], "venue": "In ACL Demo", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Stochastic first- and zeroth-order methods for nonconvex stochastic programming", "author": ["S. Ghadimi", "G. Lan"], "venue": "SIAM J. on Optimization,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["R. Herbrich", "T. Graepel", "K. Obermayer"], "venue": "In Advances in Large Margin Classifiers,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Truncated importance sampling", "author": ["E.L. Ionides"], "venue": "J. of Comp. and Graph", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "In KDD", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["J. Langford", "T. Zhang"], "venue": "In NIPS", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Learning with stochastic inputs and adversarial outputs", "author": ["A. Lazaric", "R. Munos"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Introduction to Optimization", "author": ["B.T. Polyak"], "venue": "Optimization Software, Inc.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1987}, {"title": "Sequence level training with recurrent neural networks. In ICLR", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Shallow parsing with conditional random fields", "author": ["F. Sha", "F. Pereira"], "venue": "In NAACL", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT", "author": ["P. Simianer", "S. Riezler", "C. Dyer"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Linguistic Structure Prediction", "author": ["N.A. Smith"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Learning structured predictors from bandit feedback for interactive NLP", "author": ["A. Sokolov", "J. Kreutzer", "C. Lo", "S. Riezler"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Bandit structured prediction for learning from user feedback in statistical machine translation", "author": ["A. Sokolov", "S. Riezler", "T. Urvoy"], "venue": "In MT Summit XV", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Incremental gradient algorithms with stepsizes bounded away from zero", "author": ["M.V. Solodov"], "venue": "Computational Optimization and Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D. McAllester", "S. Singh", "Y. Mansour"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "A law of comparative judgement", "author": ["L.L. Thurstone"], "venue": "Psychological Review,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1927}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Y. Yue", "T. Joachims"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Probabilistic models of vision and max-margin methods", "author": ["A. Yuille", "X. He"], "venue": "Frontiers of Electrical and Electronic Engineering,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "We present algorithms that use this feedback to \u201cbanditize\u201d expected loss minimization approaches to structured prediction [18, 25].", "startOffset": 123, "endOffset": 131}, {"referenceID": 24, "context": "We present algorithms that use this feedback to \u201cbanditize\u201d expected loss minimization approaches to structured prediction [18, 25].", "startOffset": 123, "endOffset": 131}, {"referenceID": 19, "context": "A practical example is interactive machine translation where instead of human generated reference translations only translation quality judgments on predicted translations are used for learning [20].", "startOffset": 194, "endOffset": 198}, {"referenceID": 18, "context": "[19] showed that partial feedback is indeed sufficient for optimization of feature-rich linear structured prediction over large output spaces in various natural language processing (NLP) tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "The contribution of our paper is to analyze these algorithms as stochastic first-order (SFO) methods in the framework of [7] and investigate the connection of optimization for task performance with optimization-theoretic concepts of convergence.", "startOffset": 121, "endOffset": 124}, {"referenceID": 19, "context": "Our analysis starts with revisiting the approach to stochastic optimization of a non-convex expected loss criterion presented by [20].", "startOffset": 129, "endOffset": 133}, {"referenceID": 6, "context": "The iteration complexity of stochastic optimization of a non-convex objective J(wt) can be analyzed in the framework of [7] as O(1/\u01eb) in terms of the number of iterations needed to reach an accuracy of \u01eb for the criterion E[\u2016\u2207J(wt)\u2016] \u2264 \u01eb.", "startOffset": 120, "endOffset": 123}, {"referenceID": 18, "context": "[19] attempt to improve convergence speed by introducing a cross-entropy objective that can be seen as a (strong) convexification of the expected loss objective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Lastly, we analyze the pairwise preference learning algorithm introduced by [19].", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "To our knowledge, this is the first SFO approach to stochastic learning form pairwise comparison feedback, while related approaches fall into the area of gradient-free stochastic zeroth-order (SZO) approaches [24, 1, 7, 4].", "startOffset": 209, "endOffset": 222}, {"referenceID": 0, "context": "To our knowledge, this is the first SFO approach to stochastic learning form pairwise comparison feedback, while related approaches fall into the area of gradient-free stochastic zeroth-order (SZO) approaches [24, 1, 7, 4].", "startOffset": 209, "endOffset": 222}, {"referenceID": 6, "context": "To our knowledge, this is the first SFO approach to stochastic learning form pairwise comparison feedback, while related approaches fall into the area of gradient-free stochastic zeroth-order (SZO) approaches [24, 1, 7, 4].", "startOffset": 209, "endOffset": 222}, {"referenceID": 3, "context": "To our knowledge, this is the first SFO approach to stochastic learning form pairwise comparison feedback, while related approaches fall into the area of gradient-free stochastic zeroth-order (SZO) approaches [24, 1, 7, 4].", "startOffset": 209, "endOffset": 222}, {"referenceID": 6, "context": "Convergence rate for SZO methods depends on the dimensionality d of the function to be evaluated, for example, the non-convex SZO algorithm of [7] has an iteration complexity of O(d/\u01eb).", "startOffset": 143, "endOffset": 146}, {"referenceID": 21, "context": "Most closely related are recent applications of policy gradient methods to exponential output spaces in NLP problems [22, 3, 15].", "startOffset": 117, "endOffset": 128}, {"referenceID": 2, "context": "Most closely related are recent applications of policy gradient methods to exponential output spaces in NLP problems [22, 3, 15].", "startOffset": 117, "endOffset": 128}, {"referenceID": 14, "context": "Most closely related are recent applications of policy gradient methods to exponential output spaces in NLP problems [22, 3, 15].", "startOffset": 117, "endOffset": 128}, {"referenceID": 10, "context": "Contextual one-state MDPs are also known as contextual bandits [11, 13] which operate in a scenario of maximizing the expected reward for selecting an arm of a multi-armed slot machine.", "startOffset": 63, "endOffset": 71}, {"referenceID": 12, "context": "Contextual one-state MDPs are also known as contextual bandits [11, 13] which operate in a scenario of maximizing the expected reward for selecting an arm of a multi-armed slot machine.", "startOffset": 63, "endOffset": 71}, {"referenceID": 11, "context": "In the spectrum of stochastic versus adversarial bandits, our approach is semi-adversarial in making stochastic assumptions on inputs, but not on rewards [12].", "startOffset": 154, "endOffset": 158}, {"referenceID": 7, "context": "Pairwise preference learning has been studied in the full information supervised setting [8, 10, 6] where given preference pairs are assumed.", "startOffset": 89, "endOffset": 99}, {"referenceID": 9, "context": "Pairwise preference learning has been studied in the full information supervised setting [8, 10, 6] where given preference pairs are assumed.", "startOffset": 89, "endOffset": 99}, {"referenceID": 5, "context": "Pairwise preference learning has been studied in the full information supervised setting [8, 10, 6] where given preference pairs are assumed.", "startOffset": 89, "endOffset": 99}, {"referenceID": 23, "context": "Work on stochastic pairwise learning has been formalized as derivative-free stochastic zeroth-order optimization [24, 1, 7, 4].", "startOffset": 113, "endOffset": 126}, {"referenceID": 0, "context": "Work on stochastic pairwise learning has been formalized as derivative-free stochastic zeroth-order optimization [24, 1, 7, 4].", "startOffset": 113, "endOffset": 126}, {"referenceID": 6, "context": "Work on stochastic pairwise learning has been formalized as derivative-free stochastic zeroth-order optimization [24, 1, 7, 4].", "startOffset": 113, "endOffset": 126}, {"referenceID": 3, "context": "Work on stochastic pairwise learning has been formalized as derivative-free stochastic zeroth-order optimization [24, 1, 7, 4].", "startOffset": 113, "endOffset": 126}, {"referenceID": 17, "context": "[18, 25] introduce the expected loss criterion for structured prediction as the minimization of the expectation of a given task loss function with respect to the conditional distribution over structured outputs.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[18, 25] introduce the expected loss criterion for structured prediction as the minimization of the expectation of a given task loss function with respect to the conditional distribution over structured outputs.", "startOffset": 0, "endOffset": 8}, {"referenceID": 0, "context": "Let X be a structured input space, let Y(x) be the set of possible output structures for input x, and let \u2206y : Y \u2192 [0, 1] quantify the loss \u2206y(y) suffered for predicting y instead of the gold standard structure y.", "startOffset": 115, "endOffset": 121}, {"referenceID": 19, "context": "[20] presented an algorithm that minimizes the following expected loss objective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Decomposing complex problems into a series of pairwise comparisons has been shown to be advantageous for human decision making [23].", "startOffset": 127, "endOffset": 131}, {"referenceID": 18, "context": "This idea is formalized in [19] as an expected loss objective with respect to a conditional distribution of pairs of structured outputs.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "Let P(x) = {\u3008yi, yj\u3009 |yi, yj \u2208 Y(x)} denote the set of output pairs for an input x, and let \u2206(\u3008yi, yj\u3009) : P(x) \u2192 [0, 1] denote a task loss function that specifies a dispreference of yi compared to yj .", "startOffset": 113, "endOffset": 119}, {"referenceID": 1, "context": "See [2] for an overview of bandit learning from consistent and inconsistent pairwise comparisons.", "startOffset": 4, "endOffset": 7}, {"referenceID": 18, "context": "This motivated the formalization of a convex upper bounds on expected normalized loss in [19].", "startOffset": 89, "endOffset": 93}, {"referenceID": 18, "context": "[19] thus work with an unnormalized gain function g(y) that preserves convexity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "We deal with this problem by clipping too small sampling probabilities to p\u0302wt(\u1ef9t|xt) = max{pwt(\u1ef9t|xt), k} for a constant k [9].", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "To analyze convergence, we describe Algorithms EL, PR, and CE as stochastic first-order (SFO) methods in the framework of [7].", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "Assuming \u2016\u03c6(x, y)\u2016 \u2264 R, \u2206(y) \u2208 [0, 1] and g(y) \u2208 [0, 1] for all x, y, and since the ratio g(\u1ef9t) p\u0302wt (\u1ef9t|xt) is bounded, the variance in condition (16) is bounded.", "startOffset": 31, "endOffset": 37}, {"referenceID": 0, "context": "Assuming \u2016\u03c6(x, y)\u2016 \u2264 R, \u2206(y) \u2208 [0, 1] and g(y) \u2208 [0, 1] for all x, y, and since the ratio g(\u1ef9t) p\u0302wt (\u1ef9t|xt) is bounded, the variance in condition (16) is bounded.", "startOffset": 49, "endOffset": 55}, {"referenceID": 6, "context": "Note that the analysis of [7] justifies the use of constant learning rates \u03b3t = \u03b3, t = 0, .", "startOffset": 26, "endOffset": 29}, {"referenceID": 6, "context": "For stochastic optimization of non-convex objectives, the iteration complexity with respect to this criterion is analyzed as O(1/\u01eb) in [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 13, "context": "The iteration complexity of stochastic optimization of (strongly) convex objectives has been analyzed as at best O(1/\u01eb) for the suboptimality criterion E[J(wt)] \u2212 J(w) \u2264 \u01eb for decreasing learning rates [14].", "startOffset": 202, "endOffset": 206}, {"referenceID": 18, "context": "For the purpose of measuring convergence with respect to optimal task performance, we report an evaluation of convergence speed on a fixed set of unseen data as performed in [19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 20, "context": "6 For constant learning rates, [21] show even faster convergence in the search phase of strongly-convex stochastic optimization.", "startOffset": 31, "endOffset": 35}, {"referenceID": 6, "context": "For example, these constants appear as O( \u01eb + L\u03c3 2 \u01eb ) in the complexity bound for non-convex stochastic optimization of [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 18, "context": "002 1e-4 Table 1: Test set evaluation for stochastic learning under bandit feedback from [19], for chunking under F1-score, and for machine translation under BLEU.", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "The MT experiments are based on the synchronous context-free grammar decoder cdec [5].", "startOffset": 82, "endOffset": 85}, {"referenceID": 18, "context": "The pre-processing, data splits, feature sets and tuning strategies are described in detail in [19].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "Training is distributed across 38 shards using a multitask-based feature selection algorithm [17].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "The experimental setting for chunking is the same as in [19].", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "Following [16], conditional random fields (CRF) are applied to the noun phrase chunking task on the CoNLL2000 dataset7.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "The implemented set of feature templates is a simplified version of [16] and leads to around 2M active features.", "startOffset": 68, "endOffset": 72}, {"referenceID": 18, "context": "Table 1 lists the results of the task loss evaluation for machine translation and chunking as performed in [19], together with the optimal meta-parameters and the number of iterations needed to find an optimal result on the development set.", "startOffset": 107, "endOffset": 111}], "year": 2016, "abstractText": "Stochastic structured prediction under bandit feedback follows a learning protocol where on each of a sequence of iterations, the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure. We present applications of this learning scenario to convex and non-convex objectives for structured prediction and analyze them as stochastic first-order methods. We present an experimental evaluation on problems of natural language processing over exponential output spaces, and compare convergence speed across different objectives under the practical criterion of optimal task performance on development data and the optimization-theoretic criterion of minimal squared gradient norm. Best results under both criteria are obtained for a non-convex objective for pairwise preference learning under bandit feedback.", "creator": "LaTeX with hyperref package"}}}