{"id": "1606.00776", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation", "abstract": "We introduce the multiresolution recurrent neural network, which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes: a sequence of high-level coarse tokens, and a sequence of natural language tokens. There are many ways to estimate or learn the high-level coarse tokens, but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics. Such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences. In contrast to the standard log-likelihood objective w.r.t. natural language tokens (word perplexity), optimizing the joint log-likelihood biases the model towards modeling high-level abstractions. We apply the proposed model to the task of dialogue response generation in two challenging domains: the Ubuntu technical support domain, and Twitter conversations. On Ubuntu, the model outperforms competing approaches by a substantial margin, achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study. On Twitter, the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics. Finally, our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure.", "histories": [["v1", "Thu, 2 Jun 2016 17:37:31 GMT  (1749kb,D)", "http://arxiv.org/abs/1606.00776v1", "18 pages, 2 figures, 8 tables"], ["v2", "Tue, 14 Jun 2016 02:01:16 GMT  (1750kb,D)", "http://arxiv.org/abs/1606.00776v2", "21 pages, 2 figures, 10 tables"]], "COMMENTS": "18 pages, 2 figures, 8 tables", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE stat.ML", "authors": ["iulian vlad serban", "tim klinger", "gerald tesauro", "kartik talamadupula", "bowen zhou", "yoshua bengio", "aaron c courville"], "accepted": true, "id": "1606.00776"}, "pdf": {"name": "1606.00776.pdf", "metadata": {"source": "CRF", "title": "Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation", "authors": ["Iulian Vlad Serban", "Tim Klinger", "Aaron Courville"], "emails": ["iulian.vlad.serban@umontreal.ca", "yoshua.bengio@umontreal.ca", "aaron.courville@umontreal.ca", "tklinger@us.ibm.com", "gtesauro@us.ibm.com", "krtalamad@us.ibm.com", "zhou@us.ibm.com"], "sections": [{"heading": null, "text": "We are introducing the multi-resolution recurring neural network, which expands the sequence-to-sequence framework to model the generation of natural language as two parallel discrete stochastic processes: a high-level sequence of coarse tokens and a high-level sequence of natural language tokens. There are many ways to estimate or learn the high-resolution coarse tokens, but we argue that a simple extraction method is sufficient to capture a wealth of high-level discourse semantics. Such a method enables the formation of the multi-resolution recurring neural network by maximizing the exact common log probability across both sequences. Contrary to the standard logging goal w.r.t. Natural language tokens (word perplexity), optimizing the common log probability guides the model toward modeling high-level abstractions. We are applying the proposed model to the task of challenging ugant conversations in two technical domains:"}, {"heading": "1 Introduction", "text": "These results have triggered a cascade of new neural network architectures within the deterministic sequence-to-sequence framework [15], including attention [1, 6], memory [34, 9, 15], and pointer-based mechanisms [19]. In other words, it has focused on changing the parameterization of the deterministic function mapping of input sequences to output sequences trained by maximizing the log probability of the observed output sequence-to-sequence framework. In other words, it has focused on changing the parameterization of the deterministic function mapping of input sequences to output sequences focused by maximizing the log probability of the observed output sequence-to-sequence sequence sequence sequence sequence. Instead, we are pursuing a complementary research direction aimed at generalizing sequence-to-sequence-sequencing-to-sequence-to-be-sequencing-to-level-to-sequence-to-be-generalizing the sequence sequence-to-sequence-to-to-be-sequencing-to-level-to-sequence-to-to-be-sequence-to-to-sequence-to-to-be-to-sequence-to-to-to-to-be-generalize the sequence sequence-to-to-sequence-to-to-be-sequence-to-to-sequence-to-sequence-to-to-to-be-sequence-to-level-to-to-to-be-sequence-to-to-to-be-sequence-to-to-sequence-to-be-sequence-to-to-be-to-to-sequence-to-to-be-to-to-be-sequence-to-general-to-to-be-to-be-sequence-to-be-to-be-sequence-to-sequence-to-to-be-to-to-be-sequence-to-be-to-to-be-to-to-sequence-to-to-to-be-to-to-be-to-be-to-to-to-be-to-to-to-"}, {"heading": "2 Model Architecture", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Recurrent Neural Network Language Model", "text": "We begin with the introduction of the well-established Recurrent Neural Network Language Model (RNNLM) [20, 3]. RNLM variants have been applied to various sequential tasks, including dialog modelling [28], speech synthesis [7], handwriting generation [8], and music composition [4]. Let's leave w1,..., wN is a sequence of discrete variables, so-called tokens (e.g. words), such as the wn-V for vocabulary V. The RNLM is a probabilistic generative model, with parameters that decompose probability via tokens: PTB (w1,., wN) = N-n = 1 PTB (wn,..), wn-1). (1), where the parameterized approach to the output distribubility of the model is."}, {"heading": "2.2 Hierarchical Recurrent Encoder-Decoder", "text": "Our work here builds on that of Sordoni et al. [31], who proposed the hierarchically recurring encoder decoder model (HRED). Their model uses the hierarchical structure in web queries to model a user search session as two hierarchical sequences: a sequence of queries and a sequence of words in each query. Serban et al. [28] continue in the same direction by proposing to use the temporal structure inherent in the dialogue in natural language. Their model splits a dialogue into a hierarchical sequence: a sequence of utterances, each of which is a sequence of words. Specifically, the model consists of three RNN modules: an encoder RNN, a context RNN, and a decoder RNN. A sequence of tokens (e.g. words in a sequence) encrypts the model into a real vector."}, {"heading": "2.3 Multiresolution RNN (MrRNN)", "text": "We look at the problem of generative modeling of multiple parallel sequences. Each sequence is hierarchical with the top level in terms of utterances and the lowest level in terms. We look at the problem of generative modeling of multiple parallel sequences. Each sequence is hierarchical with the top level in terms of utterances and the lowest level in terms. We look at the problem of generative modeling of multiple parallel sequences. Each sequence is hierarchical. (Unlike the second sequence also the length N,.), where zn = (1,., zn, Ln) is then'th constituent sequence of vocabulary V,. In our experiments, each sequence will consist of the words in a dialogue, and each sequence zn will contain the coarse tokens w.r.t."}, {"heading": "3 Tasks", "text": "Dialogue systems have been developed for applications ranging from technical support to language learning and entertainment [35, 30]. Dialogue systems can be divided into two different types: targeted dialog systems and non-targeted dialog systems [27]. In order to demonstrate the versatility of MrRNN, we apply it to both targeted and non-targeted dialog tasks. We focus on the task of conditional response generation. In the face of a dialog context consisting of one or more utterances, the model must generate the next response in the dialog. Ubuntu dialog task we are looking at is technical support for the Ubuntu operating system in which we use the Ubuntu dialog corpus [18]. The corpus consists of approximately 0.5 million natural speech dialogs extracted from the # Ubuntu Internet Relayed Chat (IRC) channel, the technical problem users have to enter the channel in a specific channel."}, {"heading": "4 Coarse Sequence Representations", "text": "It is the first time that a country has been able to manoeuvre itself into such a situation."}, {"heading": "5 Experiments", "text": "We optimize all models based on the training set Joint Log Probability over Coarse Sequences and Natural Language Sequences using the stochastic Gradient Optimization Method Adam [13]. We train all models with early pausing with patience over the Joint Log Probability [2]. We select our hyperparameters based on the Joint Log Probability of the validation set. We define the 20K most common words as vocabulary and the word Embedding Dimensionality to Size 300 for all models except the RNLM and HRED on Twitter, where we embed the dimensionality of Size 400. We use Gradient Clipping to keep the parameters from exploding [23]. At test time, we use a size 5 bar search to generate the model responses. For more details, see Appendix 10."}, {"heading": "5.1 Baseline Models", "text": "We compare our models with several baselines previously used in the literature; the first is the standard RNNLM with LSTM gating function [20] (LSTM), which at test date is similar to the Seq2Seq LSTM model [32]; the second baseline is the HRED model with LSTM gating function for the RNN decoder and the GRU gating function for the RNN encoder and the RNN context proposed for dialogue response generation by Serban et al. [28] The source code for both baseline models is provided upon acceptance; both Ubuntu and Twitter we specify the RNNLM model to have 2000 hidden units with the LSTM gating function; for Ubuntu we specify the HRED model to have 500, 1000 and 500 hidden units each for the RNN encoder, DecNN context, RNN and RN context."}, {"heading": "5.2 Multiresolution RNN", "text": "The rough sub-model is parameterized as the bidirectional HRED model [28] with 1000, 1000 or 2000 hidden units for the rough encoder, context and decoder RNNNs. The natural language sub-model is parameterized as the conditional HRED model with 500, 1000 or 2000 hidden units for the encoder, context and decoder RNNNs. The rough prediction code RNN GRU RNN is parameterized with 500 hidden units."}, {"heading": "5.3 Ubuntu", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to hide ourselves, and that we are able, that we are able to hide ourselves, \"he said."}, {"heading": "5.4 Twitter", "text": "Evaluation Methods For Twitter, we use the three embedding methods for textual similarity proposed by Liu et al. [17]: embedding average (average), embedding extreme (extreme) and embedding greedy (greedy). All three metrics are based on the calculation of textual similarity between the response to the basic truth and the model response to word embedding. All three metrics measure thematic similarity: If a model-generated response is based on the same topic as the response to the basic truth (e.g. contain paraphrases of the same words), the metrics receive a high score. This is a highly desirable feature for dialogue systems on an open platform such as Twitter, but is also significantly different from measuring the overall performance of the dialogue system or the appropriateness of a single answer, which would require human evaluation. Results The results on Twitter are in Table 3. Mr. RNN's responses with noun representation are better than any other models that help us learn to match this higher level of the previous model."}, {"heading": "6 Related Work", "text": "Closely related to our work is the model proposed by Ji et al. [12], which jointly models high-level natural-language text and discourse phenomena, but it models only one high-level class per sentence, which must be commented manually by humans. On the other hand, MrRNN models a sequence of automatically extracted high-level tokens. Recurring neural network models with stochastic latent variables, such as the Variational Recurrent Neural Networks by Chung et al. [7], are also closely related to our work. These models face the more difficult task of learning high-level representations while simultaneously learning to model the generative process via high-level sequences and low-level sequences, which is a more difficult optimization problem. Furthermore, such models assume that the high-level latent variables are continuous, usually Gaussian."}, {"heading": "7 Discussion", "text": "We have proposed the multi-resolution recurring neural network (MrRNN) for generative modeling of sequential data at multiple levels of abstraction. It is trained by optimizing the common log probability across sequences at each level. We apply MrRNN to generating dialog responses to two different tasks, Ubuntu technical support and Twitter conversations, and evaluate them in a human evaluation study and using automatic evaluation metrics. On Ubuntu, MrRNN shows dramatic improvements compared to competing models. On Twitter, MrRNN appears to produce more relevant and topic-specific responses. Although abstract information is implicitly present in natural speech dialogues, explicitly representing information at different levels of abstraction and collectively optimizing the generation process at all levels of abstraction, MrRNN is able to generate more fluid, relevant, and targeted responses. The results indicate that we can summarize the abstract structure with higher levels of abstraction, while making the abstract expressions available for higher levels of natural expression."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 Task Details", "text": "Ubuntu We use the Ubuntu Dialogue Corpus v2.0 extracted Jamuary, 2016: http: / / cs.mcgill.ca / ~ jpineau / datasets / ubuntu-corpus-1.0 /.Twitter We edit the record extracted with the Moses tokenizer June 2015: https: / / github.com / moses-smt / mosesdecoder / blob / master / scripts / tokenizer / tokenizer.perl.1"}, {"heading": "9 Coarse Sequence Representations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Nouns", "text": "In fact, it is the case that most of us are in a position to go to another world, in which they can go to another world, in which they find their way in another world, in which they get lost in another world, in which they live in another world, in which they live in another world, in which they live in another world, in which they live in another world, in which they find themselves in another world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "Activity-Entity Pairs", "text": "This year it is more than ever before."}, {"heading": "Stop Words for Noun-based Coarse Tokens", "text": "In fact, we are at a stage where we are at a stage where we are at a stage where we are at a stage where we are at a stage where we are at a stage where we are at a stage where we are at a stage where we are at a stage where we are at a stage where we are at a stage where we are at a stage where we are at a stage where we are at a stage where we are at a stage where we are at a stage where we are at a stage."}, {"heading": "Activities and Entities for Ubuntu Dialogue Corpus", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Ubuntu activities:", "text": "accept, activate, add, request, appoint, append, backup, boot, check, select, clean, click, comment, compare, compile, compress, modify, confirm, connect, continue, manage, copy, break, create, cut, debug, decrypt, decompress, define, describe, debug, deactivate, download, adjust, eject, connect, hide, view, execute, create, include, expand, expect, export, discover, correct, fold, freeze, get, insert, go, intervene, hash, import, include, install, interrupt, load, block, log, login, login, unsubscribe, unsubscribe, build, watch, embed, more, mount, move, navigate, open, arrange, partition, insert, insert, patch, plan, plug, insert, reactivate, delete, restore, delete, reset, reset, reset, reset, reset, reset, configure, clean, clean, clean, clean, clean, clean, add, reset, reset, reset, reset, reset, reset, reset, reset, reset, reset, reset, reset, reset, reset, reset, reset, reset, reset, reset, reset"}, {"heading": "Ubuntu entities (excerpt):", "text": "ubuntu _ 7.04, dmraid, vnc4server, tasksel, aegis, mirage, system-config-audit, uif2iso, aumix, unrar, dell, hibernate, ucoded, finger, zoneminder, ucfg, macaddress, ia32-libs, synergy, aircrack-ng, pulseaudio, gnome, kid3, bittorrent, systemsettings, fingers, xchm, pan, uwidget, vnc-java, linux-source, ucommand.com, epiphany, avanade, onboard, substance, pmount, lilypond, lilypond, pnbsp, uzw-javos, lpnpnbsp, lpnp.pnpnp.npzpnpnp.npzpzpnpnp.pznpznpnpnp.pznpznpnpnpnpnpnp.pznpznpznp.pznpznp.pznpznp.pznpznp.pz4server, taskzzzzzznp.p.pzng, tassel"}, {"heading": "Ubuntu commands:", "text": "rrrp, rp-get, aptitude, aspell, awk, basename, bc, bg, break, builtin, bzip2, cal, case, cat, cd, cfdisk, chgrp, chmod, chown, chroot, chkconfig, ckq, cmp, comm, command, continue, cp, cron, crontab, csplit, curl, cut, date, dc, dd, ddrescue, declare, df, diff, diff3, dig, dir, dircolors, dirname, dirs, dmesg, du, echo, egrep, eject, enable, env, eval, exec, exit, expect, export, expr, false, fddformat, fdisk, fg, fgrep, file, find, fmt, fold, rmesg, you, echo, egrep root, fuser, egrep, eject, enable, env, confidential, export, expr, false, fddddddddformat, fdisk, fg, fgrep, fgrep, file, find, rp fmt, root, fold, for fsck, ftp sp root, fp sp sp root, fuser, gawk, getopts, grep, grep, groupload, grocconfig, grocupload, chchchload, grochp chp chp, rp, rp chp, rp, rp sp root, rp root, rp sp sp root, rp sp sp sp root, rp sp sp sp, sp sp, sp root, rp sp sp root, sp sp sp, sp sp root, sp sp sp root, sp sp root, sp sp sp root, sp sp root, sp sp sp sp sp, sp sp sp sp, sp, sp sp sp sp sp sp, sp, sp sp, sp sp sp, sp, sp sp, sp sp"}, {"heading": "10 Model Details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Training", "text": "All models have been trained at a learning rate of 0.0002 or 0.0001, size 40 or 80 stacks and gradients are truncated at 1.0. We shorten the back propagation to stacks of 80 tokens. We validate all 5000 training packs on the entire validation set. We choose almost identical hyperparameters for the Ubuntu and Twitter models because the models seem to execute similarly different hyperparameters and because the statistics of the two data sets are comparable. We use the 20K most common words on Twitter and Ubuntu as the natural vocabulary for all models and assign all words outside the vocabulary to a special unknown token symbol. For Mr. RNN, we use a rough token vocabulary consisting of the 10K most common tokens in the rough token sequences."}, {"heading": "Generation", "text": "We calculate the cost of each bar search (candidate response) as the log probability of the tokens in the bar divided by the number of tokens it contains. LSMT model performs better when the bar search is not allowed to generate the unknown symbol, but even then it performs worse than the HRED model in all metrics except command accuracy."}, {"heading": "Baselines", "text": "Based on preliminary experiments, we found that a slightly different parameterization of the HRED base model on Twitter worked better: The RNN encoder has a bidirectional GRU RNN encoder with 1000 hidden units each for the forward and backward RNNs, as well as a context RNN and a decoder RNN with 1000 hidden units each. Furthermore, for each hidden time step, the RNN decoder calculates a 1000-dimensional real vector, which is multiplied by the source context RNN. Output is done via a single-layer forward-facing neural network with hyperbolic tangent activation function, which the RNN decoder then conditions."}, {"heading": "11 Human Evaluation", "text": "All human evaluators either study or work in an English-speaking environment and have indicated that they have some experience with a Linux operating system. Prior to starting the evaluators, a short commented example with a brief explanation of how to make comments was shown. In particular, the evaluators were instructed to use the following reference in Figure 2.The 5 evaluators gave a total of 1069 ratings. Table 6 shows the results by category."}, {"heading": "12 Model Examples", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Y. Bengio"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["K Cho"], "venue": "In Proc. of EMNLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "A recurrent latent variable model for sequential data", "author": ["J. Chung", "K. Kastner", "L. Dinh", "K. Goel", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Generating sequences with recurrent neural networks. arXiv:1308.0850", "author": ["A. Graves"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G Hinton"], "venue": "Signal Processing Magazine,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "A latent variable recurrent neural network for discourse relation language models", "author": ["Y. Ji", "G. Haffari", "J. Eisenstein"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In Proc. of ICLR", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Manual and automatic evaluation of machine translation between european languages", "author": ["P. Koehn", "C. Monz"], "venue": "In Workshop on Statistical Machine Translation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["J. Li", "M. Galley", "C. Brockett", "J. Gao", "B. Dolan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Liu", "C.-W", "R. Lowe", "I.V. Serban", "M. Noseworthy", "L. Charlin", "J. Pineau"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems", "author": ["R. Lowe", "N. Pow", "I. Serban", "J. Pineau"], "venue": "In Proc. of SIGDIAL-2015", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["M.T. Luong", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T Mikolov"], "venue": "In 11th Proceedings of INTERSPEECH,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T Mikolov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["O Owoputi"], "venue": "In Proc. of ACL", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Data-driven response generation in social media", "author": ["A. Ritter", "C. Cherry", "W.B. Dolan"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Named entity recognition in tweets: An experimental study", "author": ["A. Ritter", "S. Clark", "Mausam", "O. Etzioni"], "venue": "In Proc. of EMNLP,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["J. Schatzmann", "K. Georgila", "S. Young"], "venue": "In 6th SIGdial Workshop on DISCOURSE and DIALOGUE", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "A survey of available corpora for building data-driven dialogue systems. CoRR, abs/1512.05742", "author": ["I.V. Serban", "R. Lowe", "L. Charlin", "J. Pineau"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A.C. Courville", "J. Pineau"], "venue": "In AAAI,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["I.V. Serban", "A. Sordoni", "R. Lowe", "L. Charlin", "J. Pineau", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1605.06069", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Chatbots: are they really useful", "author": ["B.A. Shawar", "E. Atwell"], "venue": "In LDV Forum,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["A. Sordoni", "Y. Bengio", "H. Vahabi", "C. Lioma", "J.G. Simonsen", "Nie", "J.-Y"], "venue": "In Proc. of CIKM-2015", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Wen", "T.-H", "M. Gasic", "N. Mrksic", "L.M. Rojas-Barahona", "Su", "P.-H", "S. Ultes", "D. Vandyke", "S. Young"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}], "referenceMentions": [{"referenceID": 30, "context": "Recurrent neural networks (RNNs) have been gaining popularity in the machine learning community due to their impressive performance on tasks such as machine translation [32, 5] and speech recognition [10].", "startOffset": 169, "endOffset": 176}, {"referenceID": 4, "context": "Recurrent neural networks (RNNs) have been gaining popularity in the machine learning community due to their impressive performance on tasks such as machine translation [32, 5] and speech recognition [10].", "startOffset": 169, "endOffset": 176}, {"referenceID": 8, "context": "Recurrent neural networks (RNNs) have been gaining popularity in the machine learning community due to their impressive performance on tasks such as machine translation [32, 5] and speech recognition [10].", "startOffset": 200, "endOffset": 204}, {"referenceID": 13, "context": "These results have spurred a cascade of novel neural network architectures [15], including attention [1, 6], memory [34, 9, 15] and pointer-based mechanisms [19].", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "These results have spurred a cascade of novel neural network architectures [15], including attention [1, 6], memory [34, 9, 15] and pointer-based mechanisms [19].", "startOffset": 101, "endOffset": 107}, {"referenceID": 5, "context": "These results have spurred a cascade of novel neural network architectures [15], including attention [1, 6], memory [34, 9, 15] and pointer-based mechanisms [19].", "startOffset": 101, "endOffset": 107}, {"referenceID": 13, "context": "These results have spurred a cascade of novel neural network architectures [15], including attention [1, 6], memory [34, 9, 15] and pointer-based mechanisms [19].", "startOffset": 116, "endOffset": 127}, {"referenceID": 17, "context": "These results have spurred a cascade of novel neural network architectures [15], including attention [1, 6], memory [34, 9, 15] and pointer-based mechanisms [19].", "startOffset": 157, "endOffset": 161}, {"referenceID": 26, "context": "Researchers have recently observed critical problems applying end-to-end neural network architectures for dialogue response generation [28, 16].", "startOffset": 135, "endOffset": 143}, {"referenceID": 14, "context": "Researchers have recently observed critical problems applying end-to-end neural network architectures for dialogue response generation [28, 16].", "startOffset": 135, "endOffset": 143}, {"referenceID": 18, "context": "We start by introducing the well-established recurrent neural network language model (RNNLM) [20, 3].", "startOffset": 93, "endOffset": 100}, {"referenceID": 2, "context": "We start by introducing the well-established recurrent neural network language model (RNNLM) [20, 3].", "startOffset": 93, "endOffset": 100}, {"referenceID": 26, "context": "RNNLM variants have been applied to diverse sequential tasks, including dialogue modeling [28], speech synthesis [7], handwriting generation [8] and music composition [4].", "startOffset": 90, "endOffset": 94}, {"referenceID": 6, "context": "RNNLM variants have been applied to diverse sequential tasks, including dialogue modeling [28], speech synthesis [7], handwriting generation [8] and music composition [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "RNNLM variants have been applied to diverse sequential tasks, including dialogue modeling [28], speech synthesis [7], handwriting generation [8] and music composition [4].", "startOffset": 141, "endOffset": 144}, {"referenceID": 3, "context": "RNNLM variants have been applied to diverse sequential tasks, including dialogue modeling [28], speech synthesis [7], handwriting generation [8] and music composition [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 9, "context": "where f is the hidden state update function, which we will assume is either the LSTM gating unit [11] or GRU gating unit [5] throughout the rest of the paper.", "startOffset": 97, "endOffset": 101}, {"referenceID": 4, "context": "where f is the hidden state update function, which we will assume is either the LSTM gating unit [11] or GRU gating unit [5] throughout the rest of the paper.", "startOffset": 121, "endOffset": 124}, {"referenceID": 29, "context": "[31], who proposed the hierarchical recurrent encoder-decoder model (HRED).", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] continue in the same direction by proposing to exploit the temporal structure inherent in natural language dialogue.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Due to space limitations, we refer the reader to [31, 28] for additional information on the model architecture.", "startOffset": 49, "endOffset": 57}, {"referenceID": 26, "context": "Due to space limitations, we refer the reader to [31, 28] for additional information on the model architecture.", "startOffset": 49, "endOffset": 57}, {"referenceID": 29, "context": "the model parameters, and helps propagate the training signal for first-order optimization methods [31].", "startOffset": 99, "endOffset": 103}, {"referenceID": 28, "context": "Dialogue systems have been developed for applications ranging from technical support to language learning and entertainment [35, 30].", "startOffset": 124, "endOffset": 132}, {"referenceID": 25, "context": "Dialogue systems can be categorized into two different types: goal-driven dialogue systems and non-goal-driven dialogue systems [27].", "startOffset": 128, "endOffset": 132}, {"referenceID": 16, "context": "Ubuntu Dialogue Corpus The goal-driven dialogue task we consider is technical support for the Ubuntu operating system, where we use the Ubuntu Dialogue Corpus [18].", "startOffset": 159, "endOffset": 163}, {"referenceID": 22, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "We optimize all models based on the training set joint log-likelihood over coarse sequences and natural language sequences using the first-order stochastic gradient optimization method Adam [13].", "startOffset": 190, "endOffset": 194}, {"referenceID": 1, "context": "We train all models using early stopping with patience on the joint-log-likelihood [2].", "startOffset": 83, "endOffset": 86}, {"referenceID": 21, "context": "We apply gradient clipping to stop the parameters from exploding [23].", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "The first is the standard RNNLM with LSTM gating function [20] (LSTM), which at test time is similar to the Seq2Seq LSTM model [32].", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "The first is the standard RNNLM with LSTM gating function [20] (LSTM), which at test time is similar to the Seq2Seq LSTM model [32].", "startOffset": 127, "endOffset": 131}, {"referenceID": 26, "context": "[28] [31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[28] [31].", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "[29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "The coarse sub-model is parametrized as the Bidirectional-HRED model [28] with 1000, 1000 and 2000 hidden units respectively for the coarse-level encoder, context and decoder RNNs.", "startOffset": 69, "endOffset": 73}, {"referenceID": 24, "context": "Evaluation Methods It has long been known that accurate evaluation of dialogue system responses is difficult [26].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "[17] have recently shown that all automatic evaluation metrics adapted for such evaluation, including word overlap-based metrics such as BLEU and METEOR, have either very low or no correlation with human judgment of the system performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Our setup is very similar to the evaluation setup used by Koehn and Monz [14], and comparable to Liu et al [17].", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "Our setup is very similar to the evaluation setup used by Koehn and Monz [14], and comparable to Liu et al [17].", "startOffset": 107, "endOffset": 111}, {"referenceID": 26, "context": "In general, the MrRNN responses are more coherent and topic-oriented compared to the LSTM and HRED responses, which usually produce very generic responses [28].", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "[17]: Embedding Average (Average), Embedding Extrema (Extrema) and Embedding Greedy (Greedy).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12], which jointly models natural language text and high-level discourse phenomena.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7], are also closely related to our work.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "[33], are also relevant to our work.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We introduce the multiresolution recurrent neural network, which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes: a sequence of high-level coarse tokens, and a sequence of natural language tokens. There are many ways to estimate or learn the high-level coarse tokens, but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics. Such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences. In contrast to the standard loglikelihood objective w.r.t. natural language tokens (word perplexity), optimizing the joint log-likelihood biases the model towards modeling high-level abstractions. We apply the proposed model to the task of dialogue response generation in two challenging domains: the Ubuntu technical support domain, and Twitter conversations. On Ubuntu, the model outperforms competing approaches by a substantial margin, achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study. On Twitter, the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics. Finally, our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure. * This work was carried out while the first author was at IBM Research. \u25e6 Email: {iulian.vlad.serban,yoshua.bengio,aaron.courville}@umontreal.ca Email: {tklinger,gtesauro,krtalamad,zhou}@us.ibm.com \u2020 CIFAR Senior Fellow ar X iv :1 60 6. 00 77 6v 1 [ cs .C L ] 2 J un 2 01 6", "creator": "LaTeX with hyperref package"}}}