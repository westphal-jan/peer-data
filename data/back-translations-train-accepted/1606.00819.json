{"id": "1606.00819", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Matrix Factorization using Window Sampling and Negative Sampling for Improved Word Representations", "abstract": "In this paper, we propose LexVec, a new method for generating distributed word representations that uses low-rank, weighted factorization of the Positive Point-wise Mutual Information matrix via stochastic gradient descent, employing a weighting scheme that assigns heavier penalties for errors on frequent co-occurrences while still accounting for negative co-occurrence. Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks.", "histories": [["v1", "Thu, 2 Jun 2016 19:35:46 GMT  (23kb)", "https://arxiv.org/abs/1606.00819v1", null], ["v2", "Tue, 7 Jun 2016 02:20:23 GMT  (30kb)", "http://arxiv.org/abs/1606.00819v2", "Converted paper size from A4 to US Letter to avoid margin issues on arXiv"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexandre salle", "aline villavicencio", "marco idiart"], "accepted": true, "id": "1606.00819"}, "pdf": {"name": "1606.00819.pdf", "metadata": {"source": "CRF", "title": "Matrix Factorization using Window Sampling and Negative Sampling for Improved Word Representations", "authors": ["Alexandre Salle", "Marco Idiart", "Aline Villavicencio"], "emails": ["atsalle@inf.ufrgs.br,", "avillavicencio@inf.ufrgs.br,", "idiart@if.ufrgs.br"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.00 819v 2 [cs.C L] 7J un"}, {"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to, to move, to, to move, to move, to, to move, to move, to move, to move, to"}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 PPMI-SVD", "text": "With a word w and a symmetrical window with Win context words on the left and Win context words on the right, the coexistence matrix of elements Mwc is defined as the number of times a target word w and the context word c occur jointly in the corpus within the window. As this matrix is infinite in the lower limit, it is replaced in most applications by its positive definitive version, PPMI, in which negative values are set to zero. The performance of the PPMI matrix on word similarity tasks can be further improved by using context distribution smoothing (Levy et al., 2015) and subsampling of the corpus (Mikolov et al., 2013b). Since word embedding with lower dimensions can improve efficiency and generalization (Levy et al., 2015)."}, {"heading": "2.2 GloVe", "text": "GloVe (Pennington et al., 2014) takes into account the logarithm of the co-event matrix M, which takes into account the position of the context words in the window. LGloV ewc = 12 f (M-wc) (WwW + bw + b-log M-wc) 2 (3), where bw and b-c are bias terms, and f is a weighting function defined asf (x) = {(x / xmax) \u03b2 if x < xmax 1 otherwise (4) W and W \u00b2 by iterating over all non-zero (w, c) cells in the co-event matrix and minimizing eq. (3) by SGD. The weighting function (in eq. (3)) penalizes more reconstruction errors of frequent co-events, thereby improving the L2 loss, which weights all reconstruction errors in the same way as the repair errors are not taken into account in this matrix."}, {"heading": "2.3 Skip-gram with Negative Sampling (SGNS)", "text": "SGNS (Mikolov et al., 2013b) forms a neural network to predict the probability of observing a context word c with a target word w. Each observed (w, c) pair is combined with k randomly sampled sound pairs (w, wi) and used to calculate the loss function LSGNSwc = Log \u03c3 (WwW, c,) + k \u0445 i = 1Ewi \u0445 Pn (w) Log \u03c3 (\u2212 WwW, wi) (5), where Pn (w) is the distribution from which noise meters wi are sampled. We refer to this routine that SGNS uses to select (w, c) pairs of (w, c) by sliding a context window over the corpus to calculate losses and SGD as a window sampling. MicNS implicitly leads the weighting of windows factors into a frequent PM1 and 4- matrix."}, {"heading": "3 LexVec", "text": "LexVec is based on the idea of factorizing the PPMI matrix using a reconstruction loss function that does not weigh all errors equally, unlike SVD, but instead punishes errors more heavily for frequent occurrences while still treating negative side effects, unlike GloVe. Furthermore, since the use of PPMI leads to better performance than PMI for semantic tasks, we propose to maintain the SGNS weight scheme by using window samples and negative sampling, but explicitly factorizing the PPMI matrix instead of implicitly factorizing the shifted PMI matrix. The LexVec loss function has two terms LLexV ecwc = 12 (WwW-W), in which the PPMI matrix is explicitly factorized instead of implicitly factorizing the shifted PMI matrix."}, {"heading": "4 Materials", "text": "All models were placed on a rubbish pile from Wikipedia in June 2015, divided into sentences, punctuated, numbers turned into words, and wrapped lower. Words with less than 100 counts were removed, resulting in a vocabulary of 302,203 words. All models generate embedding of 300 dimensions. The PPMI * matrix is used by both PPMI-SVD and LexVec, with the PPMI * and LexVec matrix created using tactical methods used in (Levy et al., 2015) and an unweighted window of size 2. A dirty subsampling of the corpus is used for PPMI * and SGNS with the ability of PPMI * and SGNS with the threshold of 10 \u2212 5 (Mikolov et al., 2013b).2 Additionally, SGNS uses 5 negative samples (Mikolov et al., 2013b), a window of the size of the corpus is accepted for PPMI and GSNS *."}, {"heading": "5 Results", "text": "The results for word similarity and analogy tasks are given in Tables 1 and 2. Compared to PPMI-SVD, LexVec performs better on all tasks. Because they factorize the same PPMI matrix, it performs better on several word similarity tasks than SGNS, but additionally it performs better on the semantic analogy task, which comes close to GloVe. LexVec generally performs better on word similarity tasks than GloVe, possibly due to the factoring of the PPMI matrix and the weighting of window similarities compared to the mini-analogy tasks. We believe that LexVec generally performs better on word similarity tasks than GPMI."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we have introduced LexVec, a method of factorizing the PPMI matrix that generates distributed word representations, favoring low reconstruction errors in common co-occurrences while also taking negative coexistences into account, in contrast to PPMISVD, which does not weighting, and GloVe, which only takes positive co-occurrences into account. Finally, its PPMI factorization seems to better capture semantics compared to the postponed PMI factorization of SGNS. As a result, it outperforms PPMI-SVD and SGNS in a variety of word similarities and semantic analogy tasks, and outperforms GloVe in relation to similarity tasks in general. Future work will examine the use of position contexts to improve performance in syntactical analog tasks. In addition, we will further explore the hyperparameter space to find globally optimal values for Lexc, and will experiment with alternative word representations."}, {"heading": "Acknowledgments", "text": "This work was partially financed by CAPES and the AIM-WEST projects (FAPERGS-INRIA 1706-2551 / 13-7), CNPq 482520 / 2012-4, 312114 / 2015-0, \"Simplificac\" a \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" es Complexas, \"sponsored by Samsung Eletro\" nica da Amazo \"nia Ltda. under the terms of Brazilian Federal Law No. 8.248 / 91."}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Associ-", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Distributional semantics in technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Asso-", "citeRegEx": "Bruni et al\\.,? 2012", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Extracting semantic representations from word cooccurrence statistics: A computational study", "author": ["John A. Bullinaria", "Joseph P. Levy."], "venue": "Behavior research methods 39(3):510\u2013526.", "citeRegEx": "Bullinaria and Levy.,? 2007", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2007}, {"title": "Extracting semantic representations from word cooccurrence statistics: stop-lists, stemming, and svd", "author": ["John A Bullinaria", "Joseph P Levy."], "venue": "Behavior research methods 44(3):890\u2013907.", "citeRegEx": "Bullinaria and Levy.,? 2012", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2012}, {"title": "Experiments with lsa scoring: Optimal rank and basis", "author": ["John Caron."], "venue": "Proceedings of the SIAM Computational Information Retrieval Workshop. pages 157\u2013169.", "citeRegEx": "Caron.,? 2001", "shortCiteRegEx": "Caron.", "year": 2001}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Kenneth W. Church", "Patrick Hanks."], "venue": "Computational Linguistics 16(1):22\u2013", "citeRegEx": "Church and Hanks.,? 1990", "shortCiteRegEx": "Church and Hanks.", "year": 1990}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "The approximation of one matrix by another of lower rank", "author": ["C. Eckert", "G. Young."], "venue": "Psych. 1:211\u2013218.", "citeRegEx": "Eckert and Young.,? 1936", "shortCiteRegEx": "Eckert and Young.", "year": 1936}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th international conference on World Wide Web. ACM,", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics .", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computa-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Advances in Neural Information Processing Systems. pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics pages 211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg", "Israel Ramat-Gan."], "venue": "CoNLL-2014 page 171.", "citeRegEx": "Levy et al\\.,? 2014", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Automatic retrieval and clustering of similar words", "author": ["Dekang Lin."], "venue": "of the 36th and 17th , Volume 2. Montreal, Quebec, Canada, pages 768\u2013 774.", "citeRegEx": "Lin.,? 1998", "shortCiteRegEx": "Lin.", "year": 1998}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "CoNLL-2013 104.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "HLT-NAACL. pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["George A. Miller", "Walter G. Charles."], "venue": "Language and cognitive processes 6(1):1\u201328.", "citeRegEx": "Miller and Charles.,? 1991", "shortCiteRegEx": "Miller and Charles.", "year": 1991}, {"title": "Factorization of latent variables in distributional semantic models", "author": ["Arvid \u00d6sterlund", "David \u00d6dling", "Magnus Sahlgren."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "\u00d6sterlund et al\\.,? 2015", "shortCiteRegEx": "\u00d6sterlund et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A word at a time: computing word relatedness using temporal semantic analysis", "author": ["Kira Radinsky", "Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch."], "venue": "Proceedings of the 20th international conference on World wide", "citeRegEx": "Radinsky et al\\.,? 2011", "shortCiteRegEx": "Radinsky et al\\.", "year": 2011}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B. Goodenough."], "venue": "Communications of the ACM 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "ACL (1). pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics. Association", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013).", "startOffset": 107, "endOffset": 173}, {"referenceID": 6, "context": "Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013).", "startOffset": 107, "endOffset": 173}, {"referenceID": 24, "context": "Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013).", "startOffset": 107, "endOffset": 173}, {"referenceID": 0, "context": "Traditionally, word representations have been obtained using countbased methods (Baroni et al., 2014), where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 14, "context": ", 2014), where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al.", "startOffset": 78, "endOffset": 89}, {"referenceID": 5, "context": ", 2014), where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al.", "startOffset": 161, "endOffset": 185}, {"referenceID": 2, "context": ", 2014), where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al., 2014).", "startOffset": 210, "endOffset": 256}, {"referenceID": 13, "context": ", 2014), where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al., 2014).", "startOffset": 210, "endOffset": 256}, {"referenceID": 12, "context": "Techniques for generating lower-rank representations have also been employed, such as PPMI-SVD (Levy et al., 2015) and GloVe (Pennington et al.", "startOffset": 95, "endOffset": 114}, {"referenceID": 21, "context": ", 2015) and GloVe (Pennington et al., 2014), both achieving state-of-the-art performance on a variety of tasks.", "startOffset": 18, "endOffset": 43}, {"referenceID": 0, "context": "Alternatively, vector-space models can be generated with predictive methods, which generally outperform the count-based methods (Baroni et al., 2014), the most notable of which is Skip-gram with Negative Sampling (SGNS, Mikolov et al.", "startOffset": 128, "endOffset": 149}, {"referenceID": 11, "context": "It implicitly factorizes a shifted PMI matrix, and its performance has been linked to the weighting of positive and negative co-occurrences (Levy and Goldberg, 2014).", "startOffset": 140, "endOffset": 165}, {"referenceID": 0, "context": "Alternatively, vector-space models can be generated with predictive methods, which generally outperform the count-based methods (Baroni et al., 2014), the most notable of which is Skip-gram with Negative Sampling (SGNS, Mikolov et al. (2013b)), which uses a neural network to generate embeddings.", "startOffset": 129, "endOffset": 243}, {"referenceID": 2, "context": "However, since PPMI generally outperforms PMI on semantic similarity tasks (Bullinaria and Levy, 2007), rather than implicitly factorize a shifted PMI matrix (like SGNS), LexVec explicitly factorizes the PPMI matrix.", "startOffset": 75, "endOffset": 102}, {"referenceID": 12, "context": "The performance of the PPMI matrix on word similarity tasks can be further improved by using context-distribution smoothing (Levy et al., 2015) and subsampling the corpus (Mikolov et al.", "startOffset": 124, "endOffset": 143}, {"referenceID": 17, "context": ", 2015) and subsampling the corpus (Mikolov et al., 2013b).", "startOffset": 35, "endOffset": 58}, {"referenceID": 12, "context": "As word embeddings with lower dimensionality may improve efficiency and generalization (Levy et al., 2015), the improved PPMI matrix can be factorized as a product of two lower rank matrices.", "startOffset": 87, "endOffset": 106}, {"referenceID": 7, "context": "Using the truncated SVD of size d yields the factorization U\u03a3T with the lowest possible L2 error (Eckert and Young, 1936).", "startOffset": 97, "endOffset": 121}, {"referenceID": 20, "context": "Although the optimal value of p is highly task-dependent (\u00d6sterlund et al., 2015), we set p = 0.", "startOffset": 57, "endOffset": 81}, {"referenceID": 12, "context": "5 as it has been shown to perform well on the word similarity and analogy tasks we use in our experiments (Levy et al., 2015).", "startOffset": 106, "endOffset": 125}, {"referenceID": 4, "context": "Using the truncated SVD of size d yields the factorization U\u03a3T with the lowest possible L2 error (Eckert and Young, 1936). Levy et al. (2015) recommend using W = U\u03a3 as the word representations, as suggested by Bullinaria and Levy (2012), who borrowed the idea of weighting singular values from the work of Caron (2001) on Latent Semantic Analysis.", "startOffset": 98, "endOffset": 142}, {"referenceID": 2, "context": "(2015) recommend using W = U\u03a3 as the word representations, as suggested by Bullinaria and Levy (2012), who borrowed the idea of weighting singular values from the work of Caron (2001) on Latent Semantic Analysis.", "startOffset": 75, "endOffset": 102}, {"referenceID": 2, "context": "(2015) recommend using W = U\u03a3 as the word representations, as suggested by Bullinaria and Levy (2012), who borrowed the idea of weighting singular values from the work of Caron (2001) on Latent Semantic Analysis.", "startOffset": 75, "endOffset": 184}, {"referenceID": 21, "context": "GloVe (Pennington et al., 2014) factors the logarithm of the co-occurrence matrix M\u0302 that considers the position of the context words in the window.", "startOffset": 6, "endOffset": 31}, {"referenceID": 17, "context": "SGNS (Mikolov et al., 2013b) trains a neural network to predict the probability of observing a context word c given a target word w, sliding a symmetric window over a subsampled training corpus with the window size being sampled uniformly from the range [1, win].", "startOffset": 5, "endOffset": 28}, {"referenceID": 11, "context": "SGNS is implicitly performing the weighted factorization of a shifted PMI matrix (Levy and Goldberg, 2014).", "startOffset": 81, "endOffset": 106}, {"referenceID": 16, "context": "Following Mikolov et al. (2013b) it is the unigram distribution raised to the 3/4 power.", "startOffset": 10, "endOffset": 33}, {"referenceID": 12, "context": "The PPMI* matrix used by both PPMI-SVD and LexVec was constructed using smoothing of \u03b1 = 3/4 suggested in (Levy et al., 2015) and an unweighted window of size 2.", "startOffset": 106, "endOffset": 125}, {"referenceID": 17, "context": "A dirty subsampling of the corpus is adopted for PPMI* and SGNS with threshold of t = 10\u22125 (Mikolov et al., 2013b).", "startOffset": 91, "endOffset": 114}, {"referenceID": 17, "context": "2 Additionally, SGNS uses 5 negative samples (Mikolov et al., 2013b), a window of size 10 (Levy et al.", "startOffset": 45, "endOffset": 68}, {"referenceID": 12, "context": ", 2013b), a window of size 10 (Levy et al., 2015), for 5 iterations with initial learning rate set to the default 0.", "startOffset": 30, "endOffset": 49}, {"referenceID": 21, "context": "05 (Pennington et al., 2014).", "startOffset": 3, "endOffset": 28}, {"referenceID": 12, "context": "All methods generate both word and context matrices (W and W\u0303 ): W is used for SGNS, PPMI-SVD and W+W\u0303 for GloVe (following Levy et al. (2015), and W and W + W\u0303 for LexVec.", "startOffset": 124, "endOffset": 143}, {"referenceID": 17, "context": "For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Levy et al., 2014; Pennington et al., 2014; Levy et al., 2015).", "startOffset": 66, "endOffset": 152}, {"referenceID": 13, "context": "For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Levy et al., 2014; Pennington et al., 2014; Levy et al., 2015).", "startOffset": 66, "endOffset": 152}, {"referenceID": 21, "context": "For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Levy et al., 2014; Pennington et al., 2014; Levy et al., 2015).", "startOffset": 66, "endOffset": 152}, {"referenceID": 12, "context": "For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Levy et al., 2014; Pennington et al., 2014; Levy et al., 2015).", "startOffset": 66, "endOffset": 152}, {"referenceID": 8, "context": "Word similarity tasks are:3 WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001), MEN (Bruni et al.", "startOffset": 76, "endOffset": 102}, {"referenceID": 1, "context": ", 2001), MEN (Bruni et al., 2012), MTurk (Radinsky et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 22, "context": ", 2012), MTurk (Radinsky et al., 2011), RW (Luong et al.", "startOffset": 15, "endOffset": 38}, {"referenceID": 15, "context": ", 2011), RW (Luong et al., 2013), SimLex999 (Hill et al.", "startOffset": 12, "endOffset": 32}, {"referenceID": 9, "context": ", 2013), SimLex999 (Hill et al., 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 19, "context": ", 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 12, "endOffset": 38}, {"referenceID": 23, "context": ", 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 43, "endOffset": 76}, {"referenceID": 10, "context": ", 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012), calculated using cosine.", "startOffset": 87, "endOffset": 107}, {"referenceID": 16, "context": "Word analogy tasks are: Google semantic (GSem) and syntactic (GSyn) (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al.", "startOffset": 68, "endOffset": 91}, {"referenceID": 18, "context": ", 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c), using 3CosAdd and 3CosMul (Levy et al.", "startOffset": 43, "endOffset": 66}, {"referenceID": 13, "context": ", 2013c), using 3CosAdd and 3CosMul (Levy et al., 2014).", "startOffset": 36, "endOffset": 55}, {"referenceID": 12, "context": "Levy et al. (2015) obtained similar results, and suggest that using positional contexts as done by Levy et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "Levy et al. (2015) obtained similar results, and suggest that using positional contexts as done by Levy et al. (2014) might help in recovering syntactic analogies.", "startOffset": 0, "endOffset": 118}, {"referenceID": 12, "context": "This is inline with results for PPMI-SVD and SGNS models (Levy et al., 2015).", "startOffset": 57, "endOffset": 76}], "year": 2016, "abstractText": "In this paper, we propose LexVec, a new method for generating distributed word representations that uses low-rank, weighted factorization of the Positive Point-wise Mutual Information matrix via stochastic gradient descent, employing a weighting scheme that assigns heavier penalties for errors on frequent co-occurrences while still accounting for negative co-occurrence. Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}