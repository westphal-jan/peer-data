{"id": "1606.00968", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Smooth Imitation Learning for Online Sequence Prediction", "abstract": "We study the problem of smooth imitation learning for online sequence prediction, where the goal is to train a policy that can smoothly imitate demonstrated behavior in a dynamic and continuous environment in response to online, sequential context input. Since the mapping from context to behavior is often complex, we take a learning reduction approach to reduce smooth imitation learning to a regression problem using complex function classes that are regularized to ensure smoothness. We present a learning meta-algorithm that achieves fast and stable convergence to a good policy. Our approach enjoys several attractive properties, including being fully deterministic, employing an adaptive learning rate that can provably yield larger policy improvements compared to previous approaches, and the ability to ensure stable convergence. Our empirical results demonstrate significant performance gains over previous approaches.", "histories": [["v1", "Fri, 3 Jun 2016 05:25:27 GMT  (912kb,D)", "http://arxiv.org/abs/1606.00968v1", "ICML 2016"]], "COMMENTS": "ICML 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hoang minh le 0002", "andrew kang", "yisong yue", "peter carr 0001"], "accepted": true, "id": "1606.00968"}, "pdf": {"name": "1606.00968.pdf", "metadata": {"source": "META", "title": "Smooth Imitation Learning for Online Sequence Prediction", "authors": ["Hoang M. Le", "Andrew Kang", "Yisong Yue", "Peter Carr"], "emails": ["HMLE@CALTECH.EDU", "AKANG@CALTECH.EDU", "YYUE@CALTECH.EDU", "PETER.CARR@DISNEYRESEARCH.COM"], "sections": [{"heading": "1. Introduction", "text": "In many complex planning and control tasks, it is very difficult to formulate explicitly a good policy. For such tasks, the use of machine learning to automatically learn a good policy of observed expert behavior, also known as imitation of learning or learning from demonstrations, has proven to be enormously useful (Ross & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).In this paper, we examine the problem of imitated learning for a smooth online sequence in a continuous learning regime.The approach of the 33rd International Conference on Machine Learning, New York, NY, 2016. JMLR: W & CP Volume 48. Author (s).Online sequence prediction is the problem of making exogenous decisions in response to exogenous input from the environment, and a specific case of reinforcement (see section 2)."}, {"heading": "2. Problem Formulation", "text": "Let us call X \"tx1,., xT u.\" X. \"T a context sequence from the environment X, and A\" ta1,.,., aT. \"AT.,.,.,.,\".,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "2.1. Smooth Imitation Learning & Smooth Policy Class", "text": "In addition to accuracy, a key requirement of many continuous control and planning problems is smoothing (e.g., smooth tracking shots). In general, \"smoothing\" can reflect knowledge of stability characteristics or approximate balances of a dynamic system. Therefore, we define the problem of smooth imitation of learning as minimization (1) of a smooth political class (1). Most previous work on smooth policies has focused on simple political classes such as linear models (Abbeel & Ng, 2004), which can be excessively restrictive. Instead, we define a much more general smooth political class as a regulated space of complex modeling. Definition 2.1 (smooth political class). Faced with a complex model class F and a smooth regulatory class H, we define smooth political class as satisfactory: \"We define a smooth political class as satisfactory.\""}, {"heading": "3. Related Work", "text": "The most popular traditional approaches to learning expert examples focusing on the use of approximate policy iteration techniques in the MDP environment (Kakade & Langford, 2002; Bagnell et al., 2003) Most previous approaches operate in a discrete and limited space of action (Ross et al., 2009; Abbeel & Ng, 2004; Argall et al., 2009). A natural approach to addressing the more general learning situation is to reduce imitation learning to a standard overarching learning problem (Syed & Schapire, 2010; Langford & Zadrozny, 2005; Lagoudakis & Parr, 2003)."}, {"heading": "4. Smooth Imitation Learning Algorithm", "text": "\"Our learning algorithms, called SIMILE.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" We. \"\" We. \"\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\""}, {"heading": "5. Theoretical Results", "text": "All evidence will be deferred to the supplementary document."}, {"heading": "5.1. Stability Conditions", "text": "A natural smoothness is that asq \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \""}, {"heading": "5.2. Deterministic versus Stochastic", "text": "Considering two policies and an interpolation parameter \u03b2 P p0, 1q, two possibilities are considered to combine strategies: 1. stochastic strategies: \u03c0stopsq \"\u03c0stopsq\" \u03c0stopsq with probability \u03b2 and \u03c0stopsq \"\u03c0psq with probability 1\" \u03b2 2. deterministic strategies: \u03c0stopsq \"p1\" \u03b2q\u03c0psqs with probability \u03b2 and \u03c0stopsq \"\u03c0psq\" with probability 1. \"While SIMILE uses deterministic strategies, the following result shows that deterministic and stochastic interpolations result in the same expected behavior for smooth policies. Lemma 5.2 Considering each starting state s0, sequentially executed strategies and perspective strategies for achieving two separate strategies A\" tatuTt \"1 and A\" ta \"tuTt\" 1 of such strategies leading to \"the attainment of strategies."}, {"heading": "5.3. Policy Improvement", "text": "Our guarantee for the improvement of the policy is based on the analysis of SEARN (Daume \u0301 III et al., 2009 \u03b2 \u03b2 \u03b2 q), which we extend to the application of adaptive learning rates \u03b2 \u03b2 \u03b2). First, we renew the result of the most important policy improvements from thumb \u0301 III et al. (2009). Lemma 5.4 (SEARN's policy nondegradation - Lemma 1 from thumb \u2012 III et al. (2009). Let us renew \"maximum\" improvements of the policy from thumb \u0301 III et al. (2009). Lemma 5.4 (SEARN's policy nondegradation - Lemma 1 from thumb \u2012 III et al. (2009). \"Effectiveness\" Effectiveness \"Effectiveness\" Effectiveness \"Effectiveness\" Effectiveness \"Effectiveness\" Effectiveness \"Effectiveness\" Effectiveness \"Effectiveness\" Effectiveness \"Effectiveness\" Effectiveness \"Effectiveness\" Effectiveness \"Effectiveness\" Effectiveness \"Effectiveness\" 1 from thumb \u2012 III et al. (2009)."}, {"heading": "5.4. Smooth Feedback Analysis", "text": "\"We,\" it says, \"have a way of simulating.\" Alternatively, to see that simulation of a \"virtual\" feedback target does not affect training progress, we must consider the SIMILE as a powerful result in a smooth functional space (Mason et al., 1999). Define the cost functional C: \"R over policy space to be the average imitation loss over S as Cp\u03c0q.\""}, {"heading": "6. Experiments", "text": "\"We are very big.\" \"We use a smooth political class that constantly pursues a perfect persecution\" (Gaddam et al., 2015). In this setting, the time horizon of the event is multiplied by the rate of sampling. Thus, T tends to be very big. \"We use a smooth political class that maintains a smooth political control.\" \"We use a smooth political class that maintains a smooth political control.\" \"We use a smooth political class that constantly pursues a smooth persecution (Gaddam et al., 2015).\" We use a smooth political class that multiplies a smooth control of the event. \"\" Thus, T tends to be very big. \"\" We use a smooth political class that seeks smooth political control. \""}, {"heading": "7. Conclusion", "text": "We formalized the problem of smooth imitation learning for online sequence prediction, a variant of imitation learning that uses the notion of smooth policy teaching. We proposed SIMILE (Smooth IMItation LEarning), an iterative learning reduction approach for learning smooth strategies from expert demonstrations in a continuous and dynamic environment. SIMILE uses an adaptive learning rate that has been shown to facilitate much faster convergence compared to previous learning reduction approaches and also has better sample complexity than previous work because it is fully deterministic and allows virtual simulation of training labels."}, {"heading": "A. Detailed Theoretical Analysis and Proofs", "text": "\"The proof that it is a non-negative and non-negative function is based on two properties of H-smooth functions (distinguishable) in R1, as stated in the description of Lemma A.1 (self-delimitation plot of Lipschitz-smooth functions).\" Let it be: R-smooth, non-negative functions. \"Then for all P-smooth functions (distinguishable) in R1, as stated in the description of Lemma A.1 (self-delimitation plot of Lipschitz-smooth functions).\" Let it be, R-smooth, non-negative functions. \"Then for all P-smooth functions in R1, as stated in the description of Lemma A.1 (self-delimitation plot of Lipschitz-smooth functions)."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "Exploration and apprenticeship learning in reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Abbeel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2005}, {"title": "A survey of robot learning from demonstration", "author": ["Argall", "Brenna D", "Chernova", "Sonia", "Veloso", "Manuela", "Browning", "Brett"], "venue": "Robotics and autonomous systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Policy search by dynamic programming", "author": ["Bagnell", "J Andrew", "Kakade", "Sham M", "Schneider", "Jeff G", "Ng", "Andrew Y"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Bagnell et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bagnell et al\\.", "year": 2003}, {"title": "An empirical comparison of supervised learning algorithms", "author": ["Caruana", "Rich", "Niculescu-Mizil", "Alexandru"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Caruana et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Caruana et al\\.", "year": 2006}, {"title": "Mimicking human camera operators", "author": ["Chen", "Jianhui", "Carr", "Peter"], "venue": "In IEEE Winter Conference Applications of Computer Vision (WACV),", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning online smooth predictors for real-time camera planning using recurrent decision trees", "author": ["Chen", "Jianhui", "Le", "Hoang M", "Carr", "Peter", "Yue", "Yisong", "Little", "James J"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning", "author": ["Criminisi", "Antonio", "Shotton", "Jamie", "Konukoglu", "Ender"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "Criminisi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Criminisi et al\\.", "year": 2012}, {"title": "Search-based structured prediction", "author": ["Daum\u00e9 III", "Hal", "Langford", "John", "Marcu", "Daniel"], "venue": "Machine learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Imitation learning by coaching", "author": ["He", "Eisner", "Jason", "Daume", "Hal"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Learning trajectory preferences for manipulators via iterative improvement", "author": ["Jain", "Ashesh", "Wojcik", "Brian", "Joachims", "Thorsten", "Saxena", "Ashutosh"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "Reinforcement learning as classification: Leveraging modern classifiers", "author": ["Lagoudakis", "Michail", "Parr", "Ronald"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Lagoudakis et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis et al\\.", "year": 2003}, {"title": "Relating reinforcement learning performance to classification performance", "author": ["Langford", "John", "Zadrozny", "Bianca"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Langford et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2005}, {"title": "Functional gradient techniques for combining hypotheses", "author": ["Mason", "Llew", "Baxter", "Jonathan", "Bartlett", "Peter L", "Frean", "Marcus"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Mason et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Mason et al\\.", "year": 1999}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["Ratliff", "Nathan", "Silver", "David", "Bagnell", "J. Andrew"], "venue": "Autonomous Robots,", "citeRegEx": "Ratliff et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2009}, {"title": "Efficient reductions for imitation learning", "author": ["Ross", "St\u00e9phane", "Bagnell", "Drew"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "Stephane", "Gordon", "Geoff", "Bagnell", "J. Andrew"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Smoothness, low noise and fast rates", "author": ["Srebro", "Nathan", "Sridharan", "Karthik", "Tewari", "Ambuj"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "A reduction from apprenticeship learning to classification", "author": ["Syed", "Umar", "Schapire", "Robert E"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Syed et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2010}, {"title": "A study in the analysis of stationary time series, 1939", "author": ["Wold", "Herman"], "venue": null, "citeRegEx": "Wold and Herman.,? \\Q1939\\E", "shortCiteRegEx": "Wold and Herman.", "year": 1939}], "referenceMentions": [{"referenceID": 15, "context": "For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).", "startOffset": 209, "endOffset": 331}, {"referenceID": 2, "context": "For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).", "startOffset": 209, "endOffset": 331}, {"referenceID": 17, "context": "For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).", "startOffset": 209, "endOffset": 331}, {"referenceID": 10, "context": "For such tasks, the use of machine learning to automatically learn a good policy from observed expert behavior, also known as imitation learning or learning from demonstrations, has proven tremendously useful (Abbeel & Ng, 2004; Ratliff et al., 2009; Argall et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011; Jain et al., 2013).", "startOffset": 209, "endOffset": 331}, {"referenceID": 6, "context": "Our motivating example is the problem of learning smooth policies for automated camera planning (Chen et al., 2016): determining where a camera should look given environment information (e.", "startOffset": 96, "endOffset": 115}, {"referenceID": 17, "context": "To address this issue, numerous learning reduction approaches have been proposed (Daum\u00e9 III et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011), which iteratively modify the training distribution in various ways such that any supervised learning guarantees provably lift to the sequential imitation setting (potentially at the cost of statistical or computational efficiency).", "startOffset": 81, "endOffset": 146}, {"referenceID": 17, "context": ", 2009)), and not requiring data aggregation (as opposed to DAgger (Ross et al., 2011)) which can lead to super-linear training time.", "startOffset": 67, "endOffset": 86}, {"referenceID": 6, "context": "\u2022 We empirically evaluate using the setting of smooth camera planning (Chen et al., 2016), and demonstrate the performance gains of our approach.", "startOffset": 70, "endOffset": 89}, {"referenceID": 6, "context": "Consider the example of autonomous camera planning for broadcasting a sport event (Chen et al., 2016).", "startOffset": 82, "endOffset": 101}, {"referenceID": 17, "context": "Following the basic setup from (Ross et al., 2011), for any policy \u03c0 P \u03a0, let dt denote the distribution of states at time t if \u03c0 is executed for the first t \u03011 time steps.", "startOffset": 31, "endOffset": 50}, {"referenceID": 3, "context": "The most popular traditional approaches for learning from expert demonstration focused on using approximate policy iteration techniques in the MDP setting (Kakade & Langford, 2002; Bagnell et al., 2003).", "startOffset": 155, "endOffset": 202}, {"referenceID": 9, "context": "Most prior approaches operate in discrete and finite action space (He et al., 2012; Ratliff et al., 2009; Abbeel & Ng, 2004; Argall et al., 2009).", "startOffset": 66, "endOffset": 145}, {"referenceID": 15, "context": "Most prior approaches operate in discrete and finite action space (He et al., 2012; Ratliff et al., 2009; Abbeel & Ng, 2004; Argall et al., 2009).", "startOffset": 66, "endOffset": 145}, {"referenceID": 2, "context": "Most prior approaches operate in discrete and finite action space (He et al., 2012; Ratliff et al., 2009; Abbeel & Ng, 2004; Argall et al., 2009).", "startOffset": 66, "endOffset": 145}, {"referenceID": 17, "context": "State-of-the-art learning reductions for imitation learning typically take an iterative approach, where each training round uses standard supervised learning to learn a policy (Daum\u00e9 III et al., 2009; Ross et al., 2011).", "startOffset": 176, "endOffset": 219}, {"referenceID": 17, "context": ", 2009) and DAgger (Ross et al., 2011).", "startOffset": 19, "endOffset": 38}, {"referenceID": 17, "context": "First, we need not query the expert \u03c0 \u030a at every iteration (as done in DAgger (Ross et al., 2011)).", "startOffset": 78, "endOffset": 97}, {"referenceID": 9, "context": "A similar idea was proposed (He et al., 2012) for DAggertype algorithm, albeit only for linear model classes.", "startOffset": 28, "endOffset": 45}, {"referenceID": 17, "context": "Previous learning reduction approaches only use stochastic interpolation (Daum\u00e9 III et al., 2009; Ross et al., 2011), whereas SIMILE uses deterministic.", "startOffset": 73, "endOffset": 116}, {"referenceID": 17, "context": "Analyses of SEARN and other previous iterative reduction methods (Ross et al., 2011; Kakade & Langford, 2002; Bagnell et al., 2003; Syed & Schapire, 2010) rely on bounding the variation distance between d\u03c0 and d\u03c01 .", "startOffset": 65, "endOffset": 154}, {"referenceID": 3, "context": "Analyses of SEARN and other previous iterative reduction methods (Ross et al., 2011; Kakade & Langford, 2002; Bagnell et al., 2003; Syed & Schapire, 2010) rely on bounding the variation distance between d\u03c0 and d\u03c01 .", "startOffset": 65, "endOffset": 154}, {"referenceID": 7, "context": "Policy Improvement Our policy improvement guarantee builds upon the analysis from SEARN (Daum\u00e9 III et al., 2009), which we extend to using adaptive learning rates \u03b2. We first restate the main policy improvement result from Daum\u00e9 III et al. (2009). Lemma 5.", "startOffset": 95, "endOffset": 247}, {"referenceID": 7, "context": "Policy Improvement Our policy improvement guarantee builds upon the analysis from SEARN (Daum\u00e9 III et al., 2009), which we extend to using adaptive learning rates \u03b2. We first restate the main policy improvement result from Daum\u00e9 III et al. (2009). Lemma 5.4 (SEARN\u2019s policy nondegradation - Lemma 1 from Daum\u00e9 III et al. (2009)).", "startOffset": 95, "endOffset": 328}, {"referenceID": 14, "context": "To see that simulating smooth \u201cvirtual\u201d feedback target does not hurt the training progress, we alternatively view SIMILE as performing gradient descent in a smooth function space (Mason et al., 1999).", "startOffset": 180, "endOffset": 200}, {"referenceID": 6, "context": "We evaluate SIMILE in a case study of automated camera planning for sport broadcasting (Chen & Carr, 2015; Chen et al., 2016).", "startOffset": 87, "endOffset": 125}, {"referenceID": 6, "context": "2: regression tree ensembles F regularized by a class of linear autoregressor functions H (Chen et al., 2016).", "startOffset": 90, "endOffset": 109}, {"referenceID": 18, "context": "2 (1-d Case (Srebro et al., 2010)).", "startOffset": 12, "endOffset": 33}, {"referenceID": 7, "context": "Empirically, decision tree-based ensembles are among the best performing supervised machine learning method (Caruana & Niculescu-Mizil, 2006; Criminisi et al., 2012).", "startOffset": 108, "endOffset": 165}], "year": 2016, "abstractText": "We study the problem of smooth imitation learning for online sequence prediction, where the goal is to train a policy that can smoothly imitate demonstrated behavior in a dynamic and continuous environment in response to online, sequential context input. Since the mapping from context to behavior is often complex, we take a learning reduction approach to reduce smooth imitation learning to a regression problem using complex function classes that are regularized to ensure smoothness. We present a learning meta-algorithm that achieves fast and stable convergence to a good policy. Our approach enjoys several attractive properties, including being fully deterministic, employing an adaptive learning rate that can provably yield larger policy improvements compared to previous approaches, and the ability to ensure stable convergence. Our empirical results demonstrate significant performance gains over previous approaches.", "creator": "LaTeX with hyperref package"}}}