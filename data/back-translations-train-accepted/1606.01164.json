{"id": "1606.01164", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Dense Associative Memory for Pattern Recognition", "abstract": "A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used for training neural networks. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.", "histories": [["v1", "Fri, 3 Jun 2016 16:17:01 GMT  (635kb,D)", "http://arxiv.org/abs/1606.01164v1", null], ["v2", "Tue, 27 Sep 2016 16:05:36 GMT  (794kb,D)", "http://arxiv.org/abs/1606.01164v2", "Accepted for publication at NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.NE cond-mat.dis-nn cs.LG q-bio.NC stat.ML", "authors": ["dmitry krotov", "john j hopfield"], "accepted": true, "id": "1606.01164"}, "pdf": {"name": "1606.01164.pdf", "metadata": {"source": "CRF", "title": "Dense Associative Memory for Pattern Recognition", "authors": ["Dmitry Krotov", "John J Hopfield"], "emails": [], "sections": [{"heading": null, "text": "We will study a model of associative memory that stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in the field of deep learning. On the associative memory side of this duality, a family of models can be constructed that can be smoothly interpolated between two limiting cases, one called the feature matching mode of pattern recognition, and the other as a prototype regime. On the deep learning side, this family corresponds to neural networks with a hidden layer and various activation functions that transfer the activities of the visible neurons to the hidden layer. Logistics, reflected linear units, and reflected polynomials of higher degrees are among this family of activation functions. The proposed duality allows to apply energy-based intuition from associative memory to polynomial activation networks supported by polynomial higher-level activation networks."}, {"heading": "1 Introduction", "text": "This year, it is so far that it only takes one year to get there like never before."}, {"heading": "2 Associative memory with large capacity", "text": "The standard model of associative memory [1] uses a system of N binary neurons, with values \u00b1 1. A configuration of all the neurons is defined by a vector \u03c3i. The model stores K memories, which are defined as patterns that are also assumed to be binary for the moment. The model is defined by an energy function designated by E = \u2212 1 2 N \u2211 i. The basic problem is this: when the network is presented with a new pattern, it should react with a stored memory, which is input.There has been a great deal of work in the community of statistical physicists to investigate the capacity of this model, which is the maximum number of memories that the network can store and reliably retrieve. [1, 7, 8] It has been shown that in the case of random memories this value is on the order of KHamiltons, which is the maximum number of memories that the network can store and reliably retrieve."}, {"heading": "3 The case of XOR", "text": "The case of XOR is elementary, but instructive nonetheless. It is presented here for three reasons: firstly, it illustrates the construction (2) in this simplest case; secondly, it shows that the computing capacity of the network also increases with n; thirdly, it provides the simplest example of a situation where the number of memories is greater than the number of neurons, but the network functions reliably; the basic problem is this: with two inputs x \u2212 and y, an output z is produced, so that the truth table (Table 1) is fulfilled; we treat this task as an associative memory problem and will simply embed the four examples of the input output x, y, z into the memory. Therefore, the network N = 3 has identical units: Two of them are used for the inputs and one for the output, and K = 4 memories, the memories that are the four lines of the truth table."}, {"heading": "4 An example of a pattern recognition problem, the case", "text": "The visible part of this vector is treated as an \"incomplete\" pattern and associative memory is allowed to compute a completion of this pattern, which is the label of the image.Dense associative memory (2) is a recursive network in which each neuron can be updated several times. However, for digital classification purposes, this model is used in a very limited capacity, so that it can only perform an update of the classification of neurons. The network is initialized in the state in which the visible units vi are jammed to the intensities of a given image."}, {"heading": "5 Relationship to a neural network with one hidden layer", "text": "In this section, we derive a simple duality between the dense model of associative memory and a supplying neural network with a layer of hidden neurons. In other words, we show that the same computational model has two very different descriptions: one in the sense of associative memory, the other in the sense of a network with a layer of hidden units. By means of this correspondence, the family of dense memories constructed for different values of power n can be transformed into the language of the models used in deep learning. The resulting neural networks guarantee that they have computational properties of dense memories, such as the property of prototype transition.The construction is very similar to (9), except that the classification neurons are initialized in the state when all of them are equal, see Figure 4. Within the limitation, one can extend the function F in (9) so that the dominant contribution comes from the term linear in order."}, {"heading": "6 Discussion and conclusions", "text": "What is the relationship between the capacity of dense associative memory, calculated in section 2, and the neural network with a step forward, which is used for number classification? Consider the limit of very large \u03b2 in (9), so that the hyperbolic tangent is roughly equal to the drawing function, as in (4). In the limitation of sufficiently large n the network works in the prototype regime. The presented image sets the initial state of the network near a local minimum, which corresponds to one of the prototypes. In most cases, the one step updating the classification of neurons is sufficient to bring this initial state to the next local minimum, thereby completing the memory recovery, but only if the stored patterns are stable and have basins of attraction around them of at least the size of a neuron flip, which is accurate (in the case of random patterns)."}, {"heading": "Appendix. Details of experiments with MNIST.", "text": "The networks were trained using stochastic gradient pedigree with minibatches of a relatively large size, 100 digits of each class, 1000 digits in total. 3000 epochs were practiced. Initial weights were generated from a Gaussian distribution N (\u2212 0.3, 0.3). Momentum (0.6 \u2264 p \u2264 0.95) was used to balance oscillations of gradients originating from the individual minibatches. Over time, the learning rate increased accordingly to\u03b5 (t) = \u03b50f t, f = 0.998, (12) where t is the number of epochs. Typical values are 0.01 \u2264 \u03b50 \u2264 0.04. Weights (memories) were measured after each minibatch according to V \u00b5I (t \u2212 1) = pV \u00b5 I (t \u2212 1) \u2212 < the results are the same. We have the number of microunits C > \u00b5I (t) = 3000."}, {"heading": "Acknowledgments", "text": "We thank Bernard Chazelle, David Huse, Arnie Levine, Michael Mitchell, Remi Monasson, Luca Peliti, Den Raskovalov, Bingkan Xue and all members of the Simons Center for Systems Biology at IAS for useful discussions. In particular, we thank Yasser Roudi for pointing us out [12]. The work is supported by Charles L. Brown, member of the IAS."}], "references": [{"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1982}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics (pp. 315-323)", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "ImageNet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105)", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Storing infinite numbers of patterns in a spin-glass model of neural networks", "author": ["D.J. Amit", "H. Gutfreund", "H. Sompolinsky"], "venue": "Physical Review Letters,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1985}, {"title": "The capacity of the Hopfield associative memory", "author": ["R.J. McEliece", "E.C. Posner", "E.R. Rodemich", "S.S. Venkatesh"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1987}, {"title": "Associative recall of memory without errors", "author": ["I. Kanter", "H. Sompolinsky"], "venue": "Physical Review A,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1987}, {"title": "High order correlation model for associative memory", "author": ["H.H. Chen", "Y.C. Lee", "G.Z. Sun", "H.Y. Lee", "T. Maxwell", "C.L. Giles", "August"], "venue": "In Neural Networks for Computing (Vol. 151,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1986}, {"title": "Nonlinear discriminant functions and associative memories", "author": ["D. Psaltis", "C.H. Park", "August"], "venue": "In Neural networks for computing (Vol. 151,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1986}, {"title": "Number of stable points for spin-glasses and neural networks of higher orders", "author": ["P. Baldi", "S.S. Venkatesh"], "venue": "Physical Review Letters,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1987}, {"title": "Multiconnected neural network models", "author": ["E. Gardner"], "venue": "Journal of Physics A: Mathematical and General,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1987}, {"title": "Storage capacity of generalized networks", "author": ["L.F. Abbott", "Y. Arian"], "venue": "Physical Review A,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1987}, {"title": "Capacities of multiconnected memory models", "author": ["D. Horn", "M. Usher"], "venue": "Journal de Physique,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1988}, {"title": "Perceptron: an introduction to computational geometry", "author": ["M. Minsky", "S. Papert"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1969}, {"title": "August. Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "In null (p", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML-", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Pattern recognition and models of associative memory [1] are closely related.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "The standard model of associative memory works well in the limit when the number of stored patterns is much smaller than the number of neurons [1], or equivalently the number of pixels in an image.", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "These issues were largely resolved by introducing unsupervised pre-training, which made it possible to initialize the weights in such a way that the subsequent backpropagation could only gently move boundaries between the classes without destroying the feature detectors [2, 3].", "startOffset": 271, "endOffset": 277}, {"referenceID": 2, "context": "These issues were largely resolved by introducing unsupervised pre-training, which made it possible to initialize the weights in such a way that the subsequent backpropagation could only gently move boundaries between the classes without destroying the feature detectors [2, 3].", "startOffset": 271, "endOffset": 277}, {"referenceID": 3, "context": "More recently, it was realized that the use of rectified linear units (ReLU) instead of the logistic functions speeds up learning and improves generalization [4, 5, 6].", "startOffset": 158, "endOffset": 167}, {"referenceID": 4, "context": "More recently, it was realized that the use of rectified linear units (ReLU) instead of the logistic functions speeds up learning and improves generalization [4, 5, 6].", "startOffset": 158, "endOffset": 167}, {"referenceID": 5, "context": "More recently, it was realized that the use of rectified linear units (ReLU) instead of the logistic functions speeds up learning and improves generalization [4, 5, 6].", "startOffset": 158, "endOffset": 167}, {"referenceID": 0, "context": "The standard model of associative memory [1] uses a system of N binary neurons, with values \u00b11.", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "It has been demonstrated [1, 7, 8] that in case of random memories this maximal value is of the order of K \u2248 0.", "startOffset": 25, "endOffset": 34}, {"referenceID": 6, "context": "It has been demonstrated [1, 7, 8] that in case of random memories this maximal value is of the order of K \u2248 0.", "startOffset": 25, "endOffset": 34}, {"referenceID": 7, "context": "It has been demonstrated [1, 7, 8] that in case of random memories this maximal value is of the order of K \u2248 0.", "startOffset": 25, "endOffset": 34}, {"referenceID": 8, "context": "Hamiltonian (1) in a way that removes second order correlations between the stored memories, it is possible [9] to improve the capacity to K = N .", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "In the case of the polynomial function with n = 2 the network reduces to the standard model of associative memory [1].", "startOffset": 114, "endOffset": 117}, {"referenceID": 9, "context": "For the case of polynomial energy function a very similar family of models was considered in [10, 11, 12, 13, 14, 15].", "startOffset": 93, "endOffset": 117}, {"referenceID": 10, "context": "For the case of polynomial energy function a very similar family of models was considered in [10, 11, 12, 13, 14, 15].", "startOffset": 93, "endOffset": 117}, {"referenceID": 11, "context": "For the case of polynomial energy function a very similar family of models was considered in [10, 11, 12, 13, 14, 15].", "startOffset": 93, "endOffset": 117}, {"referenceID": 12, "context": "For the case of polynomial energy function a very similar family of models was considered in [10, 11, 12, 13, 14, 15].", "startOffset": 93, "endOffset": 117}, {"referenceID": 13, "context": "For the case of polynomial energy function a very similar family of models was considered in [10, 11, 12, 13, 14, 15].", "startOffset": 93, "endOffset": 117}, {"referenceID": 14, "context": "For the case of polynomial energy function a very similar family of models was considered in [10, 11, 12, 13, 14, 15].", "startOffset": 93, "endOffset": 117}, {"referenceID": 11, "context": "For higher powers n the capacity rapidly grows with N in a non-linear way, allowing the network to store and reliably retrieve many more patterns than the number of neurons that it has, in accord with [12, 13, 14, 15].", "startOffset": 201, "endOffset": 217}, {"referenceID": 12, "context": "For higher powers n the capacity rapidly grows with N in a non-linear way, allowing the network to store and reliably retrieve many more patterns than the number of neurons that it has, in accord with [12, 13, 14, 15].", "startOffset": 201, "endOffset": 217}, {"referenceID": 13, "context": "For higher powers n the capacity rapidly grows with N in a non-linear way, allowing the network to store and reliably retrieve many more patterns than the number of neurons that it has, in accord with [12, 13, 14, 15].", "startOffset": 201, "endOffset": 217}, {"referenceID": 14, "context": "For higher powers n the capacity rapidly grows with N in a non-linear way, allowing the network to store and reliably retrieve many more patterns than the number of neurons that it has, in accord with [12, 13, 14, 15].", "startOffset": 201, "endOffset": 217}, {"referenceID": 11, "context": "References [12, 13, 14] do not allow repeated indices in the products over neurons in the energy function, therefore obtain a different coefficient.", "startOffset": 11, "endOffset": 23}, {"referenceID": 12, "context": "References [12, 13, 14] do not allow repeated indices in the products over neurons in the energy function, therefore obtain a different coefficient.", "startOffset": 11, "endOffset": 23}, {"referenceID": 13, "context": "References [12, 13, 14] do not allow repeated indices in the products over neurons in the energy function, therefore obtain a different coefficient.", "startOffset": 11, "endOffset": 23}, {"referenceID": 14, "context": "In [15] the Hamiltonian coincides with ours, but the update rule is different, which, however, results in exactly the same coefficient as in (6).", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "It can also be thought of as a linear perceptron, and the inability to solve this problem represents the well known statement [16] that linear perceptrons cannot compute XOR without hidden neurons.", "startOffset": 126, "endOffset": 130}, {"referenceID": 16, "context": "6% range [17], see also controls in [18, 19].", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "6% range [17], see also controls in [18, 19].", "startOffset": 36, "endOffset": 44}, {"referenceID": 18, "context": "6% range [17], see also controls in [18, 19].", "startOffset": 36, "endOffset": 44}, {"referenceID": 5, "context": "A similar effect was reported earlier for the transition between saturating units, such as logistics or hyperbolic tangents, to ReLU [6].", "startOffset": 133, "endOffset": 136}, {"referenceID": 3, "context": "This is consistent with the previous observation that ReLU are faster in training than hyperbolic tangents and logistics [4, 5, 6].", "startOffset": 121, "endOffset": 130}, {"referenceID": 4, "context": "This is consistent with the previous observation that ReLU are faster in training than hyperbolic tangents and logistics [4, 5, 6].", "startOffset": 121, "endOffset": 130}, {"referenceID": 5, "context": "This is consistent with the previous observation that ReLU are faster in training than hyperbolic tangents and logistics [4, 5, 6].", "startOffset": 121, "endOffset": 130}, {"referenceID": 11, "context": "We especially thank Yasser Roudi for pointing out the reference [12] to us.", "startOffset": 64, "endOffset": 68}], "year": 2016, "abstractText": "A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions \u2013 the higher rectified polynomials which until now have not been used for training neural networks. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.", "creator": "LaTeX with hyperref package"}}}