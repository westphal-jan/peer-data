{"id": "1606.01245", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2016", "title": "Scalable Algorithms for Tractable Schatten Quasi-Norm Minimization", "abstract": "The Schatten-p quasi-norm $(0&lt;p&lt;1)$ is usually used to replace the standard nuclear norm in order to approximate the rank function more accurately. However, existing Schatten-p quasi-norm minimization algorithms involve singular value decomposition (SVD) or eigenvalue decomposition (EVD) in each iteration, and thus may become very slow and impractical for large-scale problems. In this paper, we first define two tractable Schatten quasi-norms, i.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, and then prove that they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively, which lead to the design of very efficient algorithms that only need to update two much smaller factor matrices. We also design two efficient proximal alternating linearized minimization algorithms for solving representative matrix completion problems. Finally, we provide the global convergence and performance guarantees for our algorithms, which have better convergence properties than existing algorithms. Experimental results on synthetic and real-world data show that our algorithms are more accurate than the state-of-the-art methods, and are orders of magnitude faster.", "histories": [["v1", "Sat, 4 Jun 2016 03:28:41 GMT  (2365kb)", "http://arxiv.org/abs/1606.01245v1", "16 pages, 5 figures, Appears in Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI), Phoenix, Arizona, USA, pp. 2016--2022, 2016"]], "COMMENTS": "16 pages, 5 figures, Appears in Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI), Phoenix, Arizona, USA, pp. 2016--2022, 2016", "reviews": [], "SUBJECTS": "cs.NA cs.AI math.OC stat.ML", "authors": ["fanhua shang", "yuanyuan liu", "james cheng"], "accepted": true, "id": "1606.01245"}, "pdf": {"name": "1606.01245.pdf", "metadata": {"source": "CRF", "title": "Scalable Algorithms for Tractable Schatten Quasi-Norm Minimization", "authors": ["Fanhua Shang", "Yuanyuan Liu", "James Cheng"], "emails": ["jcheng}@cse.cuhk.edu.hk"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.01 245v 1 [cs.N A] 4J un2 01"}, {"heading": "Introduction", "text": "This year, it has come to the point where it would never have been able to find one for as long as it has ever been in the history of the country."}, {"heading": "Notations and Background", "text": "The \"shadow p\" standard (0 < p < p > p) of a matrix X \u00b7 S (2004) is defined as \"X-Sp,\" (n-p = 1\u03c3pi (X))) 1 / p, where the \"shadow p\" standard with 0 < p < p < 1 is a better approximation than the nuclear standard (Zhang, Huang, and Zhang 2013) (analogous to the superiority of the \"shadow p\" standard with 0 < p < p < p < 1 is a better approximation than the nuclear standard (Zhang, Huang, and Zhang 2013)."}, {"heading": "Tractable Schatten Quasi-Norms", "text": "As in (Srebro, Rennie and Jaakkola 2004) the nuclear standard has the following alternative non-convex formulas. Lemma 1. For a matrix X-Rm \u00b7 n with rank (X) = r \u2264 d the following applies: EX-Rm = min U-Rm \u00b7 d, V-Rn \u00b7 d: X = UV-T-U-F-V-F = min U, V: X = UV-T-U-2F + E-V-2F."}, {"heading": "Frobenius/Nuclear Hybrid Quasi-Norm", "text": "Motivated by the equivalence relationship between the core standard and the two-dimensional spectral regulation (see (Srebro, Rennie, and Jaakkola 2004; Recht, Fazel, and Parrilo 2010), we can define a Frobenius / Nuclear Hybrid (F / N) standard as follows: 1. For each matrix X-Rm \u00b7 n with rank (X) = r \u2264 d, we can certify it into two much smaller matrices U-Rm \u00b7 d and V-Rn \u00b7 d, so that X = UV T. Then the Frobenius / Nuclear Hybrid standard is defined by X-Rm as \u0432 X-F / N: min X-UV T-shadow. In fact, the Frobenius / Nuclear hybrid standard is not a real standard, as it is not convex and does not meet the supplement D-Inequality of a standard. Similar to the well-known shadow-F-shadow-F-F-shadow-F-F."}, {"heading": "Bi-Nuclear Quasi-Norm", "text": "II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II"}, {"heading": "Optimization Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Problem Formulations", "text": "-3 / -3 / -3 / -3 / -3 / -3 / -3 / -3 / -3 / -3 / -3 / -3 / -3 / -3 / -3 / -3 / -3 / -4 / -4 / -4 / -4 / -4 / -5 / -5 / -5 / -5 / -5 / -5 / -5 / -5 / -5 / -5 / -5 / -5 / -5 / -5 / -5 / -5"}, {"heading": "Updating Lipschitz Constants", "text": "Next, we calculate the Lipschitz constants lgk + 1 and l h k + 1 in the (k + 1) iteration.Algorithm 1 solution (3) via PALM input: PALM input: PALM (D), the specified rank d and 1. Initialize: U0, V0, \u03b5 and k = 0,1: while not converged 2: Update lgk + 1 and Uk + 1 by (9) and (6), respectively. 3: Update lhk + 1 and Vk + 1 by (9) and (7), respectively. 4: Verification of the convergence condition, max."}, {"heading": "PALM Algorithms", "text": "Based on the above development, our solution algorithm (3) is presented in algorithm 1. Similarly, we also design an efficient PALM algorithm for solving (4). Runtime of algorithm 1 is dominated by performing matrix multiplications. Overall complexity of algorithm 1 and the solution algorithm (4) is O (nmd), where d \u0445 m, n."}, {"heading": "Algorithm Analysis", "text": "We now offer the guarantees of global convergence and low matrix recovery for algorithm 1, and the similar results can be obtained for the solution algorithm (4)."}, {"heading": "Global Convergence", "text": "Before examining the global convergence of algorithm 1, we first introduce the definition of the critical points of a non-convex function defined in (Bolte, Sabach and Teboulle 2014).Definition 3. Let a non-convex function f: Rn \u2192 (\u2212 \u221e, + \u221e] be a correct and lower semi-continuous function, and domf = {x \u00b2 Rn: f (x) < + \u221e}. \u2022 For each x-convex function, the Fre-convergence of f on the axis is defined as at least f (x) = {u-Rn: lim y = x-convergence y (x) \u2212 f (< u, y \u2212 x \u00b2 x \u00b2 -x \u00b2 -x \u00b2 -x \u00b2 -x \u00b2 -x \u00b2 -x \u00b2 -x \u00b2 -x \u00b2 convergence on the axis."}, {"heading": "Recovery Guarantee", "text": "In the following, we show that if a sufficient number of entries are observed, the critical point generated by our algorithms restores a low-level matrix \"close\" to the fundamental truth One. Without loss of generality, we assume that D = Z + E + Rm \u00b7 n, where Z is a true matrix, and E denotes random Gaussian noise. Theorem 5. Leave (U, V) a critical point of the problem (3) with the given rank d and m \u00b2 n. Then there is an absolute constant C1, so that probability is at least 1 \u2212 2 exp (\u2212 m), where \u03b2 = maxi, j \u00b2 T \u00b2 F \u00b2 mn (E, F \u00b2 mn + C1\u03b2 (md log (m) |) 1 / 4 + 2 \u00b2 dp (3C2), so that probability is at least 1 \u2212 2 exp (\u2212 m), and \u03b2 = maxi, j \u00b2 T \u00b2 F \u00b2 mn (D, U \u00b2 T) T."}, {"heading": "Experimental Results", "text": "All experiments were performed on an Intel Xeon E7-4830V2 2.20 GHz CPU with 64G RAM algorithms for comparison. We compared our BiN and F / N algorithms with the following state-of-the-art methods: IRucLq1 (Lai, Xu and Yin 2013): In IRucLq p varies from 0.1 to 1 with increment 0.1, and the parameters \u03bb and \u03b1 are set to 10 \u2212 6 and 0.9 respectively. Furthermore, the ranking parameter of the algorithm is dynamically updated as in (Lai, Xu and Yin 2013), i.e. it only needs the partial EVD. IRNN2 (Lu et al. 2014): We select the norm, SCAD and Mnamip."}, {"heading": "Synthetic Matrix Completion", "text": "The synthetic matrices Z'Rm \u00b7 n with rank r are generated by the following procedure: The inputs of both U'Rm \u00b7 r and V'Rn \u00b7 r are first generated as independent and identically distributed (i.e.) numbers, and then Z = UV T is mounted. Since all these algorithms have a very similar recovery performance on noiseless matrices, we only perform methods 1http: / / www.math.ucla.edu / \u02dc wotaoyin / 2https: / sites.google.com / site / canyilu / experiments on noiseless matrices with different noise levels, i.s (D) = Pig (Z + nf).E, where nf indicates the noise factor. In other worlds, the observed subset of i.i.i.d standard Gaussian noise is as in (Lu et al. 2014)."}, {"heading": "Collaborative Filtering", "text": "We tested our algorithms on three real-world referral system datasets: the MovieLens1M, MovieLens10M4 and Netflix datasets (KDDCup 2007). We randomly selected 50%, 70% and 90% as a training set and the rest as a test set, and the experimental results are reported over 10 independent runs. In addition to the methods used above, we compared our algorithms with one of the fastest existing methods, LMaFit5 (Wen, Yin and Zhang 2012). In addition, our algorithms consistently outperform the other methods in terms of predictive accuracy of all three of these datasets in Table 1, which shows that all of these methods perform significantly better than the convex nuclear standard solver, APGL, with non-convex quasi-regulated matrix. Furthermore, our algorithms consistently outperform the other methods in terms of predictive accuracy."}, {"heading": "Image Recovery", "text": "We also applied our algorithms to grayscale recovery on the 512 \u00d7 512 boot image, where 50% of the pixels in the input image were replaced by random Gaussian noise, as shown in Figure 5 (b). In addition, we used the well-known Peak Signal-to-Noise (PSNR) to measure recovery performance, and the ranking parameter of our algorithms and IRucLq was set to 100. Due to the limited space, we report only the best results (PSNR and CPU time) of APGL, LMaFit, IRucLq and IRNN-Lp in Figure 5, showing that our two algorithms perform much better recovery performance than the other methods in terms of PSNR. And impressively, our two algorithms are significantly faster than the other methods besides LMaFit and at least 70 times faster than IRucLq and IRNN-Lp."}, {"heading": "Conclusions", "text": "In this work, we defined two tractable shadow quasinstandards, i.e. the Frobenius / Nuclear Hybrid and the bi-Nuclear Quasinorms, and proved that they are essentially the shadow 2 / 3 and 1 / 2 quasinorms, respectively. Then, we designed two efficient, alternately linearized minimization algorithms to solve our shadow quasinorms minimization for matrix completion problems, and also demonstrated that each bound sequence generated by our algorithms converges globally to a critical point. In other words, our algorithms not only have better convergence properties than existing algorithms, such as IRucLq and IRNN, but also reduce the computational complexity of O (mn2) to O (mnd), where d is the estimated rank (d \u00b2 m, n). We also gave the recovery guarantee for our algorithms, which implies that they require only one observation of the host state (O)."}, {"heading": "Acknowledgements", "text": "We thank the reviewers for their constructive comments. In part, the authors are supported by the SHIAE Fund 8115048 and the Hong Kong GRF 2150851."}, {"heading": "Proofs of Theorem 1 and Property 1:", "text": "In the following, we will first prove that the Frobenius / Nuclear Hybrid Standard for all a, a1, a2, a2, and a = a1a2, we both have a quasi-standard."}, {"heading": "Proof of Theorem 5:", "text": "(V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is provided. (V) Proof is. (V) Proof is. (V) Proof is."}, {"heading": "Lower bound on C2", "text": "Finally, we have the lower limit of C2 + 1, that is, it is lower than a positive constant. (16) Algorithm 2: (4) Algorithm 2: (4) Algorithm 2: (4) Algorithm 2: (4) Algorithm 2: (4) Algorithm 2: (4) Algorithm 2: (4) Algorithm 2: (4) Algorithm 2: (4) Algorithm 2: (4) Algorithm 2: (4) Algorithm 3: (4) Algorithm 3: (4) Algorithm 3: (4) Algorithm 3: (4) Algorithm 3: (4) Algorithm 3: (4) Algorithm 3: (4) Algorithm 4: (4) Algorithm 4: (4).S: Algorithm 4: (4)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "The Schatten-p quasi-norm (0<p<1) is usually used to replace the standard nuclear norm in order to approximate the rank function more accurately. However, existing Schattenp quasi-norm minimization algorithms involve singular value decomposition (SVD) or eigenvalue decomposition (EVD) in each iteration, and thus may become very slow and impractical for large-scale problems. In this paper, we first define two tractable Schatten quasi-norms, i.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, and then prove that they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively, which lead to the design of very efficient algorithms that only need to update two much smaller factor matrices. We also design two efficient proximal alternating linearized minimization algorithms for solving representative matrix completion problems. Finally, we provide the global convergence and performance guarantees for our algorithms, which have better convergence properties than existing algorithms. Experimental results on synthetic and real-world data show that our algorithms are more accurate than the state-ofthe-art methods, and are orders of magnitude faster. Introduction In recent years, the matrix rank minimization problem arises in a wide range of applications such as matrix completion, robust principal component analysis, low-rank representation, multivariate regression and multi-task learning. To solve such problems, Fazel, Hindi, and Boyd; Cand\u00e8s and Tao; Recht, Fazel, and Parrilo (2001; 2010; 2010) have suggested to relax the rank function by its convex envelope, i.e., the nuclear norm. In fact, the nuclear norm is equivalent to the l1-norm on singular values of a matrix, and thus it promotes a low-rank solution. However, it has been shown in (Fan and Li 2001) that the l1-norm regularization over-penalizes large entries of vectors, and results in a biased solution. By realizing the intimate relationship between them, the nuclear norm penalty also over-penalizes large singular values, that is, it may make the solution deviate from the original solution as the l1-norm does (Nie, Huang, and Ding 2012; Lu et al. 2015). Compared with the nuclear norm, the Schatten-p quasi-norm for 0 < p < 1 makes a closer Copyright c \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. approximation to the rank function. Consequently, the Schatten-p quasi-norm minimization has attracted a great deal of attention in images recovery (Lu and Zhang 2014; Lu et al. 2014), collaborative filtering (Nie et al. 2012; Lu et al. 2015; Mohan and Fazel 2012) and MRI analysis (Majumdar and Ward 2011). In addition, many non-convex surrogate functions of the l0-norm listed in (Lu et al. 2014; Lu et al. 2015) have been extended to approximate the rank function, such as SCAD (Fan and Li 2001) and MCP (Zhang 2010). All non-convex surrogate functions mentioned above for low-rank minimization lead to some non-convex, nonsmooth, even non-Lipschitz optimization problems. Therefore, it is crucial to develop fast and scalable algorithms which are specialized to solve some alternative formulations. So far, Lai, Xu, and Yin (2013) proposed an iterative reweighted lease squares (IRucLq) algorithm to approximate the Schatten-p quasi-norm minimization problem, and proved that the limit point of any convergent subsequence generated by their algorithm is a critical point. Moreover, Lu et al. (2014) proposed an iteratively reweighted nuclear norm (IRNN) algorithm to solve many non-convex surrogate minimization problems. For matrix completion problems, the Schatten-p quasi-norm has been shown to be empirically superior to the nuclear norm (Marjanovic and Solo 2012). In addition, Zhang, Huang, and Zhang (2013) theoretically proved that the Schatten-p quasi-norm minimization with small p requires significantly fewer measurements than the convex nuclear norm minimization. However, all existing algorithms have to be solved iteratively and involve SVD or EVD in each iteration, which incurs high computational cost and is too expensive for solving large-scale problems (Cai and Osher 2013; Liu et al. 2014). In contrast, as an alternative non-convex formulation of the nuclear norm, the bilinear spectral regularization as in (Srebro, Rennie, and Jaakkola 2004; Recht, Fazel, and Parrilo 2010) has been successfully applied in many large-scale applications, e.g., collaborative filtering (Mitra, Sheorey, and Chellappa 2010). As the Schatten-p quasi-norm is equivalent to the lp quasi-norm on singular values of a matrix, it is natural to ask the following question: can we design equivalent matrix factorization forms for the cases of the Schatten quasi-norm, e.g., p = 2/3 or 1/2? In order to answer the above question, in this paper we<lb>first define two tractable Schatten quasi-norms, i.e., the<lb>Frobenius/nuclear hybrid and bi-nuclear quasi-norms. We<lb>then prove that they are in essence the Schatten-2/3 and 1/2<lb>quasi-norms, respectively, for solving whose minimization<lb>we only need to perform SVDs on two much smaller fac-<lb>tor matrices as contrary to the larger ones used in existing<lb>algorithms, e.g., IRNN. Therefore, our method is particu-<lb>larly useful for many \u201cbig data\u201d applications that need to<lb>deal with large, high dimensional data with missing values.<lb>To the best of our knowledge, this is the first paper to scale<lb>Schatten quasi-norm solvers to the Netflix dataset. More-<lb>over, we provide the global convergence and recovery per-<lb>formance guarantees for our algorithms. In other words, this<lb>is the best guaranteed convergence for algorithms that solve<lb>such challenging problems. Notations and Background<lb>The Schatten-p norm (0 < p < \u221e) of a matrix X \u2208 Rm\u00d7n<lb>(m \u2265 n) is defined as \u2016X\u2016Sp ,<lb>(<lb>n<lb>\u2211 i=1<lb>\u03c3<lb>i (X)<lb>)1/p", "creator": "LaTeX with hyperref package"}}}