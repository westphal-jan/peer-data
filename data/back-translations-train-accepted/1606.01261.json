{"id": "1606.01261", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Minimizing Regret on Reflexive Banach Spaces and Learning Nash Equilibria in Continuous Zero-Sum Games", "abstract": "We study a general version of the adversarial online learning problem. We are given a decision set $\\mathcal{X}$ in a reflexive Banach space $X$ and a sequence of reward vectors in the dual space of $X$. At each iteration, we choose an action from $\\mathcal{X}$, based on the observed sequence of previous rewards. Our goal is to minimize regret, defined as the gap between the realized reward and the reward of the best fixed action in hindsight. Using results from infinite dimensional convex analysis, we generalize the method of Dual Averaging (or Follow the Regularized Leader) to our setting and obtain general upper bounds on the worst-case regret that subsume a wide range of results from the literature. Under the assumption of uniformly continuous rewards, we obtain explicit anytime regret bounds in a setting where the decision set is the set of probability distributions on a compact metric space $S$ whose Radon-Nikodym derivatives are elements of $L^p(S)$ for some $p &gt; 1$. Importantly, we make no convexity assumptions on either the set $S$ or the reward functions. We also prove a general lower bound on the worst-case regret for any online algorithm. We then apply these results to the problem of learning in repeated continuous two-player zero-sum games, in which players' strategy sets are compact metric spaces. In doing so, we first prove that if both players play a Hannan-consistent strategy, then with probability 1 the empirical distributions of play weakly converge to the set of Nash equilibria of the game. We then show that, under mild assumptions, Dual Averaging on the (infinite-dimensional) space of probability distributions indeed achieves Hannan-consistency. Finally, we illustrate our results through numerical examples.", "histories": [["v1", "Fri, 3 Jun 2016 20:07:41 GMT  (3840kb,D)", "http://arxiv.org/abs/1606.01261v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["maximilian balandat", "walid krichene", "claire tomlin", "alexandre bayen"], "accepted": true, "id": "1606.01261"}, "pdf": {"name": "1606.01261.pdf", "metadata": {"source": "CRF", "title": "Minimizing Regret on Reflexive Banach Spaces and Learning Nash Equilibria in Continuous Zero-Sum Games", "authors": ["Maximilian Balandat", "Walid Krichene", "Claire Tomlin", "Alexandre Bayen"], "emails": ["BALANDAT@EECS.BERKELEY.EDU", "WALID@EECS.BERKELEY.EDU", "TOMLIN@EECS.BERKELEY.EDU", "BAYEN@BERKELEY.EDU"], "sections": [{"heading": "1. Introduction", "text": "It is a particularly interesting role that raises regret in the study of repeated games of finite games (Hart and Mas-Colell, 2001), portfolio optimization (Cover, 1991), and online optimization (Hazan et al., 2007). It is well known, for example, that in a two-player zero-sum game, when both players play according to a Hannan consistent strategy (Hannan, 1957), their (marginal) empirical distributions of the game almost certainly converge with the group of Nash equilibria of the game (Cesa-Bianchi, 2006). Furthermore, it can be shown that playing is a strategy that almost certainly triggers regret about the group of Nash equilibria of the game (Cesa-Bianchi, 2006)."}, {"heading": "2. Regret Minimization on Reflexive Banach Spaces", "text": "11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11"}, {"heading": "2.1. Preliminaries", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "2.2. Dual Averaging in Reflexive Banach Spaces", "text": "We invoke a proper convex function h = = 0 (-), which triggers a general regret (-) (-). (-) We stress that we do not assume that we will be Fre \"chet-differentiable.\" (-) Definition 1 in conjunction with Lemma 29 implies that for each regulatory function h, the supremum of a function of the form < - (-) via X, where a unique element of X will be achieved, namely Dh \"(-), the Fre\" chet gradient of h. \"(-) The dual averaging method with regulator h\" and a sequence of learning rates (-) t \"t,\" containing a sequence of decisions using the simple update rule: xt + 1 = Dh \"(-), where Ut.\""}, {"heading": "3. Online Optimization on Compact Metric Spaces with Uniformly Continuous Rewards", "text": "Motivated by Example 2, we apply our results in this section to the problem of repentance minimization in compact metric spaces with the additional assumption of uniformly continuous reward functions. It is important that we do not make assumptions about the convexity of the realizable amount or the reward functions. Essentially, this can be seen as canceling the non-convex problem, a sequence of functions over the (possibly non-convex) set S to the convex (albeit infinite) problem of minimizing a sequence of linear functions over the convex subset X of the probability measurements over the vector space of the measurements on S. This correspondence is illustrated in Figure 1."}, {"heading": "3.1. An Upper Bound on the Worst-Case Regret", "text": "Let us (S, d) be a compact metric of continuity, and let us have a Borel measurement on S. Let us suppose that the reward vectors u\u03c4 are given by elements in Lq (S, \u00b5). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S........ (S). (S). (S. (S). (S). (S....). (S. (S). (S). (S..... (S). (S). (S). (S....). (S. (S)."}, {"heading": "3.3. Analysis for Entropy Dual Averaging (The Generalized Hedge Algorithm)", "text": "If we take \u03c6 (z) = ez \u2212 1, we get f\u03c6 (x) = \u0445 x 1 \u03c6 \u2212 1 (z) dz = x log x and hence the regulator ish\u03c6 (x) = \u0445 S x (s) log x (s) d\u00b5 (s), which results in Dh * (s) = exp * (s) and exp * (s), which corresponds to a generalized version of the hedge algorithm (Arora et al., 2012; Krichene et al., 2015). The regulator h\u03c6 can be essentially strongly convex with Modulus \u03b3 (r) = 12r 2nd episode 14 (Regret Bound for Entropy Dual Averaging) Suppose that \u00b5 (S) = 1, \u00b5 r0-local Q-regular with constants c0, C0, C0 \u2212 & lt\u03b1 \u2212 2b = Callet, and that we are not (r) < Q -log, that we continue with Callet and Callet."}, {"heading": "3.4. A General Lower Bound", "text": "We also demonstrate the following general lower limit for each online algorithm: Theorem 15 (General lower limit) Let (S, d) be compact, assume acceptance 2 is valid, and leave (Z). Then for each online algorithm there is a sequence (u\u03c4) t\u03c4 = 1 of the reward vectors u\u03c4 X * with acceptance 2 and the continuity module. < Therefore, for each online algorithm there is a sequence in which w: R \u2192 R is any function with a continuity module, in such a way that w (\u00b7, s \u00b2 M and the continuity module for some s \u00b2 S, for which s \u00b2 S with d (s, s \u00b2) = DS. Maximizing the constant in (18) is of interest to compare the limit with the upper limits achieved in the previous sections. However, this problem is quite challenging, and we will postpone this analysis to future work."}, {"heading": "3.5. Consistency of Dual Averaging", "text": "It is quite intuitive to see that dual averaging would restore the greedy algorithm if the regulator h \"approaches a constant.\" In the following we make this intuition precise. Definition 17 (consistency of a sequence of regulators) A sequence (h1, h2,..) of regulators to X is consistent if there is one that hi (x) \u2192 C gives as i for all x x x x x x x x x x x x x x x x x x x x x x x x x x. Moreover, we refer to the constraint of C (s, A) = infs \"d (s, s, s\") (s, s \") d (s, s\") x x x x x. \""}, {"heading": "4. Learning in Continuous Two-Player Zero-Sum Games", "text": "In this section, we apply our essays on dual averaging to Lp spaces in the context of the repetitive play of continuous games. In particular, we focus on continuous zero-sum games for two players. In the finite case, similar results exist for games that are not zero-sum, and we believe that they can be extended to our environment, but this is outside the scope of this article (see for example (Stoltz and Lugosi, 2007) for related work on learning correlated equilibrium under additional convexity assumptions)."}, {"heading": "4.1. Static Two-Player Zero-Sum Games", "text": "Consider a zero-sum game for two players G = (S1, S2, u), in which the strategy fields S1 and S2 of player 1 and 2 are Hausdorff spaces, respectively, and u: S1 \u00b7 S2 \u2192 R is the payout function of player 1. Since the game is zero, the payout function of player 2 \u2212 u. For each i we define the natural extension u: = P (Si) the amount of Borel probabilities on Si. Let us consider S: = S1 \u00b7 S2 and P: = P1 \u00b7 P2. For a (common) mixed strategy x: P we define the natural extension u: P \u2192 R by u (x): = Ex [u] = 1 (s1, s2) dx (s1, s2), which is the expected payout of player 1 below x."}, {"heading": "4.2. Repeated Play", "text": "Considering a game G and a sequence of games (s1t) t \u2265 1 and (s 2 t) t \u2265 1, we say that player i sublinear (realized) repentance iflim sup t \u2192 \u221e 1t (sup si-Si t \u0440 = 1 ui (s i, s \u2212 i\u03c4) \u2212 t \u2211 \u03c4 = 1 ui (s i, s \u2212 i \u03c4))) = 0 (21), where we use \u2212 i to denote the other player (e.g. \u2212 1 = 2). A strategy \u03c3i for player i is, loosely spoken, a (possibly random) mapping of past observations on his actions. Of primary interest to us are Hannan consistent strategies (Hannan, 1957): Definition 21 (Hannan consistency) A strategy \u0441\u0442i of the player i is consistent when the x-consistent ability of the player is not set."}, {"heading": "4.3. Hannan-Consistent Strategies", "text": "From Theorem 24, if each player follows a Hannan-consistent strategy, then the empirical distribution of the game (= 1 function) is poorly adapted to the amount of Nash balances in the game. But, do such strategies exist? Repentance minimizing strategies are intuitive candidates, and the intimate link between repentance minimization and learning in games is well studied for specific cases such as for finite games (CesaBianchi and Lugosi, 2006) or potential games (Monderer and Shapley, 1996). Using our results from Section 3, we will show that, under an additional assumption of the underlying information structure, no-repentance learning based on dual averaging leads to Hannan consistency in our settings. Suppose that after each iteration each player i observes a partial payout function, u: Si \u2192 R, which describes their payout as a function only of their own action, si by fixing the action of the other player."}, {"heading": "5. Examples", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. A Game With Unique Mixed Strategy Equilibrium", "text": "Consider the zero sum game G1 between two players playing on the unit interval Si = [0, 1] with the payoff function, which is byu (s1, s2) = (1 + s1) (1 \u2212 s1s2) (1 + s1s2) 2 (26) Since | Dsiu | \u2264 8 for each s \u2212 i [0, 1] the payoff function Lipschitz. It can be shown that V = 4 / \u03c0 and that this game does not have a pure and unique mixed ash equilibrium, with an equilibrium density xi (s) = 2\u03c0 s (1 + s) equal for both players (Glicksberg and Gross, 1953). Note that xi is unlimited and that xi-Lp (Si, \u03bb) for any 1 \u2264 p < 2. This infinity is the reason for the slow convergence of empirical distributions close to zero that we can observe in Figure 2."}, {"heading": "5.2. A Game With Explicit Dual Averaging Updates", "text": "Consider a zero sum game G2 between two players on the unit interval with payoff function (s1, s2) = s2s2 \u2212 a1s1 \u2212 a2s2where a1 = e \u2212 2e \u2212 1 and a 2 = 1e \u2212 1. However, it is easy to verify that the pair given by x1 (s) = exp (s) e \u2212 1 and x2 (s) = exp (1 \u2212 s) e \u2212 1 (x 1, x2) represents a mixed strategy Nash balance of G2. However, for sequences (s 1) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (s) t (e) \u2212 e) e (1 \u2212 e) e) t (e) and (e) \u2212 e) (1 \u2212 e) t (e)."}, {"heading": "5.3. A Game on a Non-Convex Domain", "text": "One of the most interesting features of the dual averaging algorithms discussed in Section 3 is that they are also applicable to non-convex domains. We can therefore use them as a tool for calculating approximate Nash balances in continuous zero-sum games on non-convex domains. Consider, in particular, a game G3 in which each Si = [0, 2] 2\\ [0,4, 1] 2 is an L-shaped subset of R2. Indeed, it is easy to see that the Lebesgue measure in this sentence is Q = 2, c0 = \u03c04 and C0 = \u03c0. We define the metric d number on S1 between any two points a, b-Si as the length (at Euclidean distance) of the shortest path between a and b, which is fully contained in Si. The reward function u is considered u (s1, s2) = d (s1, s2) \u2212 110 (a game that can be viewed as a problem that can be treated as 1)."}, {"heading": "Appendix A. Review of Some Results From Convex Analysis", "text": "In this section we collect some results from the endless-dimensional convex analysis, which will play an important role in our analysis of the dual mean algorithm. (Lemma 29 (Asplund, 1968) Let f: X \u2192 (\u2212 \u043c, + \u221e) be properly lower semicontinuous. (ii) For a pair (x0, 0) the following is equivalent: (i) f * is finite and Fre \"chet\" differentiable at 0 with Fre \"chet derivative Df\" (0) = x0. (ii) For some words applies: \"L,\" f \"(L) \u2264\" (0) + < x0, 0 > + \u03b3 \"L\" (0 >), and f \"chet\" (0) is the veve \"R,\" f \"iiii\" and f \"iii\" (f) lower (iiii), f. \""}, {"heading": "Appendix B. Dual Averaging in Continuous Time", "text": "In this section, we will use ideas from Kwon and Mertikopoulos (2014) and introduce a problem of continuous regret minimization related to the problem of discrete regret minimization discussed in Section 2.2. Indeed, this analysis will be crucial to demonstrate the discrete regret limit (9) in Theorem 6."}, {"heading": "B.1. Regret Minimization in Continuous Time on Reflexive Banach Spaces", "text": "() () () () () () () () () () () () () () () () () () () () () () () () ()) () () () ()) () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () ()) () () () ()) () () () ()) () () ()) () () () ()) () () ()) () () ()) () () () ()) () () () ()) () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()"}, {"heading": "B.2. Online Optimization in Continuous Time on Compact Metric Spaces", "text": "One can also reach limits of regret in continuous time by using arguments similar to those in Section 3. Although we do not use them in the main part of this article, these limits may be of independent interest.We consider the definition of Section 3. Specifically: Allow (S, d) a compact metric space, and leave (S, d) the set of Borel measurements on S. Denote centered on B (s, r) = {s, s \u00b2 s \u00b2 s \u00b2 S: d (s, s \u00b2) < r} the open sphere of radius r. For p > 1 consider X = Lp (S, \u00b5) and X = {x \u00b2 X: x \u00b2 X: x \u00b2 measurement of probability on S, which is absolutely continuous."}, {"heading": "Appendix C. Computing the Dual Averaging Optimizer", "text": "In this section we will discuss some aspects relating to the calculation of the optimizer in the dual averaging update in the setting of online optimization on compact metric spaces with uniformly continuous rewards. The results of this section will be used to generate the Hannan consistent strategies in the repeated games in Section 5 and the numerical benchmarks of the algorithms in Appendix D. As shown in Section 3.2, it can be shown that the Fre-chet differential Dh method in this case has a simple expression in relation to the dual problem, and the problem of calculating xt + 1 = Dh roles reduced to the calculation of a scalar dual variable variable distribution. In particular, it can be shown that the following: Forecast 34 (Krichene and Balandat, 2016) Let us leave a solution with associated f-divergence potential on X."}, {"heading": "Appendix D. Numerical Results and Comparison With Other Methods", "text": "In this section, we look at some of the algorithms proposed in the literature for online convex optimization over subsets of Rn and compare them with our method of dual mean for online optimization in compact metric spaces with uniformly continuous rewards from Section 3. Such algorithms are often formulated in terms of loss functions. \"Table 1 summarizes the limits of regretting each method with the corresponding assumptions about the realizable amount and loss functions, as long as the specified S is convex and the rewards are concave and satisfy the additional assumptions of the algorithms. Table 1 summarizes the limits of regretting each method with the corresponding assumptions about the realizable amount and loss functions."}, {"heading": "D.1. Optimizing Sequences of Convex Functions over Convex Sets", "text": "He suggested that it is possible to get additional assumptions about the loss functions if the losses are H-strongly convex then GP with learning rates of 1 / 2%. Hazan et al. (2007) shows that it is possible to get logarithmic regrets under additional assumptions about the loss functions. In particular, if the losses are H-strongly convex then GP with learning rates of 1 / 2. Ht has remorse Rt. M 22H (1 + log t) They also propose methods for uniformly exp-concave losses, that is, if there is one."}, {"heading": "Appendix E. Proofs Omitted in the Main Part", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "PROOF OF THEOREM 4", "text": "Proof [Theorem 4] Fundamental Fre'chet differentiation, the characterization (7) of the Fre'chet gradient in (i) and (ii) results from theorem 30, Lemma 29 and the definition of a unified substantial strong convexity. To (8) prove, let (1), (2) and let xi = Df \"(i) = arg maxx\" X < (i), x > \u2212 f \"x\" (x). Then, after the optimality of first order, < z \u2212 1, x \u2212 xi > 0 for all zi pages (xi), i = 1, 2. Summary of these inequalities we find that < z1 \u2212 x2 \u2212 x2 \u2212 z2 > zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz"}, {"heading": "PROOF OF THEOREM 6", "text": "Proof [Theorem 6] We see the Continuous-Time-Reward and Learning Rate Processes uc >) and \u03b7c (t): = Administrative - Administrative - Administrative - Administrative - Administrative - Administrative - Administrative - Administrative - Administrative - Administrative - Administrative - Administrative - Administrative - Administrative - Administrative - Administrative - Administrative - Administrative - Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ- Administrativ-"}, {"heading": "PROOF OF COROLLARY 7", "text": "Evidence [logical sequence 7] It is easy to prove that \u03b3 \u2212 1 (\u043ej \u2212 1 2) \u2264 (2C) \u2212 1 / 2 (1) \u03b71 / 2 (1) \u04211 (1) \u2012 1 / 2 (1). If we apply for all j, then Rt (x) \u2264 h (x) \u2012 h (2C) \u2212 1 / 2 (M1 + 1 / 2) - t = 1 (1) - 1 (1) - 1 (1) - 1 (1) - 1 (1) - 1 (1) - 1 (1)."}, {"heading": "PROOF OF THEOREM 8", "text": "Proof [Theorem 8] The SpaceX = Lp (S) is uniformly convex (Clarkson, 1936) and therefore reflexive (Milman, 1938), its dual is X * = Lq (S, \u00b5) for q = pp \u2212 1 and < x, > = \"S x (s)\" for x * X and \"X\" for x * X and \"X.\" Then for all s \u00b2 S and all x \u00b2 B (s, T) < Ut, x > = \"B (s \u00b2) t (s \u00b2) x (s \u00b2) d\u00b5 (s \u00b2) x (s \u00b2, T) t (s \u00b2) d \u00b2 (s \u00b2) t (s \u00b2) x (s \u00b2) d (s \u00b2) t (s \u00b2) > (s \u00b2) t (s \u00b2) t (s \u00b2) t (s \u00b2) p (p), p (t), p (p), p (t), p (t), p (p), p (t), p (t)."}, {"heading": "PROOF OF PROPOSITION 11", "text": "Denote by 1A the indicator function of the set A, i.e. 1A (s) = > Q (s) = 1 if s-A and 1A (s) = 0 if s-6-A. In this proof we will apply the following Lemma: < 37 Let (S, d) be a compact metric space and let \u00b5 be an r0-local Q-regular measure on S. For p \u2265 1 let X p: = {x-P (S, p): x \u2265 0 a.s., x-P-1 = 1. Let us further assume that f: S \u2192 R is continuous. Consequently, S f (s) = sup x-P-P-f (s) dx-p-S f (s)."}, {"heading": "PROOF OF PROPOSITION 13", "text": "Proof [proposition 13] By convexity of f we have that h (x) = h\u03c6 (x) \u2265 f\u03c6 (\u0394S dx d\u00b5d\u00b5 (s)) = f\u03c6 (1) = 0 for all x-X and hence h = 0. Furthermore, if we select x as the uniform radon-nicodymium density w.r.t. \u00b5 to B (s, \u03d1t), i.e. x (s \u2032) = 1B (s \u2032 t) (s \u2032) \u00b5 (B (s, \u03d1t)) (s \u2032) \u00b5 (B (s \u2032) \u00b5 (B (s, \u03d1t)))) thath (x) = \u0445 Si f\u03c6 (x (s \u2032) \u00b5 (B (s, \u0445B \u2212 \u2212 t) f\u03c6 (1 \u00b5 (B (s, \u0445t)) \u00b5 (ds \u2032) \u2264 min (C0 (\u0441t) Q, \u00b5 (S), \u00b5 (S) f\u0445 (S) (s)))), applying the assumption of r0-local regularity Q number and the fact that the S\u043c (s) increases."}, {"heading": "PROOF OF COROLLARY 14", "text": "The proof [correction 14] that the logbook (r) = 2r, the logbook (x) = x log x and the logbook (r) = C\u03b1r\u03b1 was inserted in (16) shows that Rt \u2264 C0 c0 t \u03b7tlog (c \u2212 10 \u03d1 t) + C\u03b1\u03d1 t + M2t \u2211 = 1 Rt \u2264 C0 t \u03b7tlog (c \u2212 10 \u03d1 t) + C\u03b1\u03d1 t + M2t \u2211 = 1 Rt \u2264 C0 c0 t \u03b7tlog (c \u2212 Q t) + C\u03b1\u03d1 t = 1 Rt \u2264 C\u03b1tlog = 1 Rt \u2264 C\u03b1tlog = 1 Rt \u2264 C\u03b1tlog x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "PROOF OF THEOREM 15", "text": "Proof [theory] 15] Since S is compact, there is sa, sb > S so that d (sa, sb) = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D"}, {"heading": "PROOF OF PROPOSITION 16", "text": "Lemma 38 Leave C-R and 0 < \u03b2 \u2264 1. The function v: [0, \u221e) given by v (r) = Cr\u03b2 is Ho-lder continuous with a continuity module that we find with x = Cr1 \u2212 Cr2 and y = Cr2 for all r1, r2 \u2265 0 that | C-r\u03b21 \u2212 | r \u03b2 2 \u2264 | C-r1 \u2212 r2 | \u03b2. The exchange of the roles of r1 and r2 results in Cr\u03b21 \u2212 Cr\u03b21 \u2212 r1 \u2212 r2 | \u03b2. Proof [Proposition 16] with sa, sb \u2212 r1 \u2212 r2 | \u03b2. The proof for theorem 15 follows w (r) = min (C1 / r2), M d (\u00b7 sb \u2212 r2 | \u03b2."}, {"heading": "PROOF OF PROPOSITION 18", "text": "[Proposition 18] Fix t < \u221e and Ley. (Hi) hi (Hi) hi (Hi) i (Hi) i (Hi) i (Hi) i (Hi) i (Hi) i (Hi) i (Hi) i (Hi) i (Hi) i (Hi) i (Hi) i (Hi) i (V) v (H) i (H) i (H) i (H) i (H) i (H) i (H) i (H) i (H) i (H) i) i (H) i (H) i (H) i (H) i (H) i (H) (H) i (H) (H) i (H).)"}, {"heading": "PROOF OF COROLLARY 19", "text": "Proof [Conclusion 19] Let f: S \u2192 R be continuous and limited, say: f (s) | \u2264 M for all s [S] S. Let > 0. Since S is compact, f is uniformly continuous, i.e. it is such that | f (s) \u2212 f (s) \u2212 f (s) | < / 2 for all s [B]. As a result, there is j < \u00b2 so that x-i (B) c) < 4M for all i > j. This results in S-f (s) \u2212 f (s) | x-i (s) \u03bb (ds) < / 2-B-i (s) \u0445 i (s) \u00b5 (ds) + 2M (B) c x-i (s) \u00b5 (ds) < for all i > j."}, {"heading": "PROOF OF PROPOSITION 22", "text": "Proof [sentence 22] This proof uses arguments similar to Theorem 7.2 in Cesa-Bianchi and Lugosi (2006), with modifications to meet our more general function requirements for metric spaces.Since player 1 has sublinear (s) dx1 (s) dx1 (s) dx1 (s) dx1 (s) dx1 (s), s11t and [s] for each measurable f (s) dx1 (s) dx1 (s) dx1 (s) dx1 (s) dx1 (s) dx1 (s) dx1 (s) dx1 (s), so we can show equivalently that supx1, s2p) dx (s1, s2p) dx (s) dx (s1) dxb (s1) (s1) (s1) (s1), sb (sb), s1, s1 (1, s1, sxt (1) (s1, s1, sxt (s1) (sd1, s1, s1, sxt (1) (sdb), s1, s1, s1 (1, sx1, 1, sxt (1) (s1, s1, s1, s1, s1 (s1), 1, 1 (s1, 1, 1 (s1), 1 (sx1, 1, 1 (s1), 1, 1 (sx1, 1, 1 (sx1), 1 (sx1, 1, 1 (sx1), 1 (sx1, 1, 1 (sx1), 1, 1 (sx1, 1 (sx1), 1 (sx1), 1 (sx1, 1 (sx1), 1, 1, 1 (sx1, 1, 1 (sx1), 1 (sx1, 1 (sx1), 1 (sx1), 1 (sx1), 1 (sb (sb), 1, 1 (sx1), 1 (sx1, 1, 1, 1 (sx1), 1 (sx1,"}, {"heading": "PROOF OF COROLLARY 23", "text": "Proof [logical sequence 23] Based on the fact that the payoff of player 2 is the negative of player 1, we have from theorem 22 and the fact that the game has a value that proves the value thatlim inf t \u2192 \u221e 1t \u0445 \u03c4 = 1 \u2212 u (s1\u03c4, s2\u03c4) \u2265 \u2212 Vand thuslim sup t \u2192 \u221e 1t \u0445 \u03c4 = 1 u (s1\u03c4, s 2 \u0432) \u2264 VCombining this value with (22) (23)."}, {"heading": "PROOF OF THEOREM 24", "text": "(x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2) (x2) (x2) (x2) (x2) (x2) (x2) (x2) (x2) (x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2) (x2)\" (x2) \"(x2)\" (x2) (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) \"(x2)\" (x2) () \"(x2)\" () \"()\" (x2) \"(x2) ()\" () (x2) \"()\" (x2) () \"()\" (x2) \"() (x2)\" () (x2) \"()\" (x2) \"() (x2)\" () \"(x2)\" () \"(x2) (x2)\" () \"()\" (x2) (x2) \"()\" (\"() ()\" () \"(x2)\" () \"(("}, {"heading": "PROOF OF THEOREM 27", "text": "To begin with, it should be noted that for each p > 1, space X as a closed subset of Lp (S, \u00b5) is a complete metric space (S, \u00b5), and therefore between X and the Lebesgue measurement on scale [0, 1] there is a sequence of probability scales in X, it is sufficient that the player i has access to a sequence of i.i.d. marginal variables drawn from the uniform distribution to [0, 1]. Note this sequence of Zi = (Zi1, Z i 2,.).The most important observation is that if the player \u2212 i is playing a non-oblivious strategy, then the partial rewards will not be a fixed sequence of reward functions, but will depend on the history of the game. In fact, since it (S) = 1 ui (S \u2212 i.i.i.i.i.i.i.i. i."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>We study a general version of the adversarial online learning problem. We are given a decision<lb>set X in a reflexive Banach space X and a sequence of reward vectors in the dual space of X . At<lb>each iteration, we choose an action from X , based on the observed sequence of previous rewards.<lb>Our goal is to minimize regret, defined as the gap between the realized reward and the reward<lb>of the best fixed action in hindsight. Using results from infinite dimensional convex analysis, we<lb>generalize the method of Dual Averaging (or Follow the Regularized Leader) to our setting and<lb>obtain general upper bounds on the worst-case regret that subsume a wide range of results from the<lb>literature. Under the assumption of uniformly continuous rewards, we obtain explicit anytime regret<lb>bounds in a setting where the decision set is the set of probability distributions on a compact metric<lb>space S whose Radon-Nikodym derivatives are elements of L(S) for some p > 1. Importantly,<lb>we make no convexity assumptions on either the set S or the reward functions. We also prove<lb>a general lower bound on the worst-case regret for any online algorithm. We then apply these<lb>results to the problem of learning in repeated continuous two-player zero-sum games, in which<lb>players\u2019 strategy sets are compact metric spaces. In doing so, we first prove that if both players<lb>play a Hannan-consistent strategy, then with probability 1 the empirical distributions of play weakly<lb>converge to the set of Nash equilibria of the game. We then show that, under mild assumptions,<lb>Dual Averaging on the (infinite-dimensional) space of probability distributions indeed achieves<lb>Hannan-consistency. Finally, we illustrate our results through numerical examples.<lb>", "creator": "LaTeX with hyperref package"}}}