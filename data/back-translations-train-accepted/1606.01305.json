{"id": "1606.01305", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find encouraging results: zoneout gives significant performance improvements across tasks, yielding state-of-the-art results in character-level language modeling on the Penn Treebank dataset and competitive results on word-level Penn Treebank and permuted sequential MNIST classification tasks.", "histories": [["v1", "Fri, 3 Jun 2016 23:31:47 GMT  (877kb,D)", "http://arxiv.org/abs/1606.01305v1", null], ["v2", "Mon, 13 Jun 2016 18:59:48 GMT  (877kb,D)", "http://arxiv.org/abs/1606.01305v2", null], ["v3", "Wed, 18 Jan 2017 03:12:03 GMT  (1270kb,D)", "http://arxiv.org/abs/1606.01305v3", null], ["v4", "Fri, 22 Sep 2017 20:43:09 GMT  (1270kb,D)", "http://arxiv.org/abs/1606.01305v4", "David Krueger and Tegan Maharaj contributed equally to this work"]], "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["david krueger", "tegan maharaj", "j\\'anos kram\\'ar", "mohammad pezeshki", "nicolas ballas", "nan rosemary ke", "anirudh goyal", "yoshua bengio", "aaron courville", "chris pal"], "accepted": true, "id": "1606.01305"}, "pdf": {"name": "1606.01305.pdf", "metadata": {"source": "CRF", "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "authors": ["David Krueger", "Tegan Maharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville", "Chris Pal"], "emails": ["firstname.lastname@umontreal.ca.", "firstname.lastname@polymtl.ca.", "hlarochelle@twitter.com."], "sections": [{"heading": null, "text": "Like a dropout, Zoneout uses random noise to train a pseudo-ensemble, which improves generalization, but by retaining hidden units, gradient and state information spreads more easily over time, as in stochastic depth networks. We conduct an empirical study of different RNN regulators and find encouraging results: Zoneout provides significant performance improvements in various tasks and delivers state-of-the-art results in modeling the sign language based on the Penn Treebank dataset and competitive results at the word level Penn Treebank and permutated sequential MNIST classification tasks."}, {"heading": "1 Introduction", "text": "We address the problem of regulation in recursive neural networks (RNNs). Recursive networks have recently been used to achieve state-of-the-art results in many areas of machine learning, including speech recognition [Miao et al., 2015], computer vision [Yao et al., 2015], speech modelling [Kim et al., 2015], and machine translation [Bahdanau et al., 2014]. Solving machine learning tasks requires high-performance models such as RNNNs that are flexible enough to capture complex relationships in the data. Neural network regulation often leads to large performance gains when data is a limiting factor, as indicated by the frequent use of early stoppings.The use of RNNNs also requires the ability to sequentially construct fixed-length representations of arbitrary length sequences by folding new observations into their hidden state."}, {"heading": "2 Related work", "text": "At the same time as our work, Singh et al. [2016] propose zoneout for feedback networks, calling this technique SkipForward. In their experiments with feedback networks, zoneout is surpassed by stochastic depth, dropout and their proposed swapout technique, which randomly drops either identity or residual connections and performs best in image recognition tasks. In contrast, we find that zoneout exceeds stochastic depth and recurring dropouts in our experiments with RNNs."}, {"heading": "2.1 Relationship to dropout", "text": "Zoneout is named after the popular dropout technique [Srivastava et al., 2014]. In Zoneout, the units are not set to 0, but to their previous value (ht = ht \u2212 1). Zoneout, like Dropout, forms a pseudo-ensemble [Bachman et al., 2014], but uses a stochastic \"identity mask\" instead of a null mask. We suspect that Identity Mask is more suitable for RNNNs as it makes it easier for the network to retain information from earlier time steps forward, and it facilitates the flow of gradient information backwards rather than hindering it. Zoneout can also be viewed as a selective application of dropout on only some of the nodes in a modified computer graphic, as shown in Figure 1. This justifies our use of the expected identity mask at test date."}, {"heading": "2.2 Dropout in RNNs", "text": "Initially successful applications of dropouts in RNNs [Pham et al., 2013, Zaremba et al., 2014] apply only dropouts to forward-facing connections (\"up the stack\") and not to recurring connections (\"forward through time\"). However, Bayer et al. [2013] successfully apply fast dropouts [Wang and Manning, 2013], a deterministic approach of dropouts to RNNNs. We now describe several recent papers [Semeniuta et al., 2016, Moon et al., 2015] that suggest failure variants for use in RNNs. Semeniuta et al al al al. [2016] apply recurring dropouts to the updates of LSTM memory cells (or GRU states), i.e. they break off the input / update gate in LSTM / GRU. Their motivation is to prevent the loss of long-term memories that are maintained in the memory cells (or GRU states) but applied to these differences exactly (the STM states)."}, {"heading": "2.3 Relationship to Stochastic Depth", "text": "Zoneout can also be considered a per-unit version of stochastic depth [Huang et al., 2016]. To our knowledge, this method has not been extended to a recurring setting, which is equivalent to the simultaneous zoning of all units of a layer in an upstream network. In a typical RNN, there is a new input at each step of time, causing problems for a naive implementation of the hearth depth. First, Huang et al. [2016] train deep residual networks (ResNets [He et al., 2015]) and preserve the identity connections when dropping one layer, so that the inputs from the previous layer are fed into the next layer in the stack. The inclusion of such residual connections in recurring networks led to instability in our experiments, presumably due to the parameters dividing in RNNs. Zoning a whole layer means that the entire layer is entered into the corresponding NN process."}, {"heading": "3 Zoneout and preliminaries", "text": "We will now explain zoneout in detail and compare it with other forms of dropouts in RNNs. Let's start by examining recurrent neural networks (RNNs)."}, {"heading": "3.1 Recurrent Neural Networks", "text": "Recurring neural networks process data x1, x2,., xT sequentially and construct a corresponding sequence of representations, h1, h2,., hT. The transition operator T of a recursive neural network converts the current hidden state and input into a new hidden state: ht = Tt (ht \u2212 1, xt). Thus, each hidden state (implicitly) is trained to memorize and highlight all task-relevant aspects of the previous inputs and to include new inputs in its representation of the state. Another way to see zoneout is to modify the transition operator, similar to a dropout. In both methods, the masking coefficient dt is a Bernoulli random vector that is independently sampled at each time step during the training and is replaced by its expectation at the test date. Then zoneout mixes the original transition operator T with the ty identifier, as opposed to the zero operator that is used in the Zoneout:"}, {"heading": "3.2 Long short-term memory", "text": "In long-term memory RNNs (LSTMs), the hidden state is divided into memory cells (dt) identity (dt identity = 1), which is intended for internal long-term storage, and hidden state, which is used as a transient representation of the state in time step t. However, in an LSTM, ct and ht are calculated using a set of four \"gates,\" including the forgotten gate, ft, which is directly connected to the memories of the previous time step ct + \u2212 \u2212 \u2212 \u2212 sint \u2212 sint response to an elementary multiplication. Large values of the (ironically) forgotten gate cause the cell to remember most (but not all) of its previous value. The other gates control the flow of information in (it, gt) and out (ot) of the cell. Each gate has its own weight matrices and bias vector; for example, the forgotten creature has Wxf, Whbht and bf."}, {"heading": "4 Experiments and Discussion", "text": "We evaluate the performance of zoneout in the following tasks: \u2022 Classification of handwritten numerals on permutated sequential MNIST [Le et al., 2015]. \u2022 Word-level speech modeling on the Penn Treebank corpus [Marcus et al., 1993]. \u2022 Character-level speech modeling on the Penn Treebank corpusWe first examine zoneout with a common zoneout mask on cells and hide in the above tasks and compare its performance with other regulators. To better study the dynamics of memory in LSTM and the effects of zoneout on these dynamics, we also examine models with different zoneout probabilities on cells and hidden states."}, {"heading": "4.1 Sequential Permuted MNIST", "text": "In the sequential MNIST, pixels of an image representing a number [0-9] are presented to a relapsing neural network individually in lexographic order (from left to right, from top to bottom).The task is to classify the number represented in the image. In the permutated sequential MNIST (pMNIST), the pixels are displayed in a (fixed) random order.We compare recurring failure and zoneout values with a vanilla LSTM baseline for this task. All models are trained for 150 epochs using RMSProp [Tieleman and Hinton, 2012] with a decay rate of 0.5 for the moving average of the gradient standards. The learning rate is set to 0.001 and the gradients are truncated to a maximum standard of 1.0 [Pascanu et al., 2013]."}, {"heading": "4.2 Penn Treebank Language Modeling Dataset", "text": "The Penn Treebank Language Model Corpus contains 1 million words with predefined training, validation, and test parts. The model is trained to predict the next word or character, and is evaluated on the basis of word level perplexity or bits per character (BPC) for character level, as is typical for these assignments.2 These ratios are deterministic functions of negative logarithm probability (NLL). Specifically, perplexity is the exponentialized NLL, and BPC is NLL divided by the natural logarithm of 2."}, {"heading": "4.2.1 Character-level", "text": "For the character level task, we use an LSTM with a hidden layer of 1000 units. We train with non-overlapping sequences of 100 in batches of 32. We optimize the use of Adam with a learning rate of 0.002 and gradient sections with the threshold of 1. Figure 4a shows our research of different zoneout models with different probabilities for zoneout for cells and hiding places. The results are presented in 2; we find that zoneout on cells with the probability of 0.5 and zoneout on states with the probability of 0.05 surpass both relapsing exposures and lead them to our most powerful model (zc = 0.5, zh = 0.05). We compare zoneout with relapsing exposures (for p = 0.05,.2,.5,.7}), weight noise (\u03c3 = 0.075) and norm stabilizer (\u03b2 = 50, zh = 0.05).We compare zoneout with relapsing exposures (for p = 0.05,.5,.7,.7}), weight noise (\u03c3 = 0.075) and norm stabilizer (\u03b2 = 0.75)."}, {"heading": "4.2.2 Word-level", "text": "For the word-level task, we use settings similar to Semeniuta et al. [2016] and Mikolov et al. [2014]. However, unlike this previous work, we cut the data into non-overlapping sequences, and we do not use the last hidden states of a batch to initialize the hidden states of the next batch. This means that we effectively form data without overlapping, and therefore our results are not directly comparable to those reported in Semeniuta et al. [2016] The model is an LSTM with a single hidden layer of 256 units and weights uniformly initialized between -0.05 and + 0.05. We train on non-overlapping sequences of length 35, in batches of 32, and optimize using SGD with a dynamics of 0.99 and an increase threshold of 10. We start with a learning rate of 1 and decrease it by a factor of 1.5 after each epoch, with no improvement in validation performance of 0.0001 and 0.0001 in 2016."}, {"heading": "5 Conclusion", "text": "We have introduced Zoneout, a novel method of regulating RNNs. Rather than eliminating hidden units, zoneout has hidden units that stochastically replace their activation by activating one step earlier. We find that zoneout delivers significant performance improvements over permutated sequential MNIST and Penn Treebank records, outperforms alternative regulators, and delivers state-of-the-art performance in character level language modeling.We suspect that the improved generalization of zoneout is based on two key factors: \u2022 Stochasticity introduced by zoneout makes the network more resilient to hidden state changes. \u2022 Zoneout's identity connections improve the flow of information across the network.Future work could allow recurring networks to adjust the likelihood of updating different units based on the input sequence, which could be used to learn a dynamic version of the hierarchical RNN that would update the network when Hihi and Bengio proposed it in 1996 [where different dimensions were proposed]."}, {"heading": "Acknowledgments", "text": "We thank the developers of Theano [Theano Development Team, 2016] as well as Fuel and Blocks [van Merri\u00ebnboer et al., 2015] and the computer resources provided by ComputeCanada and CalculQuebec. We thank the students at MILA, in particular \u00c7a\u011flar G\u00fcl\u00e7ehre, Marcin Moczulski, Chiheb Trabelsi and Christopher Beckham for feedback and helpful discussions. We also thank IBM and Samsung for their support. This research was developed with funds from the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL). The views, opinions and / or results expressed correspond to those of the authors and should not be interpreted as official views or guidelines of the Department of Defense or the U.S. government3."}], "references": [{"title": "Learning with pseudo-ensembles", "author": ["Philip Bachman", "Ouais Alsharif", "Doina Precup"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bachman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bachman et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "On Fast Dropout and its Applicability to Recurrent Networks", "author": ["J. Bayer", "C. Osendorfer", "D. Korhammer", "N. Chen", "S. Urban", "P. van der Smagt"], "venue": null, "citeRegEx": "Bayer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bayer et al\\.", "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Yarvin Gal"], "venue": "ArXiv e-prints,", "citeRegEx": "Gal.,? \\Q2015\\E", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["Salah El Hihi", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hihi and Bengio.,? \\Q1996\\E", "shortCiteRegEx": "Hihi and Bengio.", "year": 1996}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Regularizing rnns by stabilizing activations", "author": ["David Krueger", "Roland Memisevic"], "venue": "arXiv preprint arXiv:1511.08400,", "citeRegEx": "Krueger and Memisevic.,? \\Q2015\\E", "shortCiteRegEx": "Krueger and Memisevic.", "year": 2015}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "arXiv preprint arXiv:1507.08240,", "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Learning longer memory in recurrent neural networks", "author": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "Rnndrop: A novel dropout for rnns in asr", "author": ["Taesup Moon", "Heeyoul Choi", "Hoshik Lee", "Inchul Song"], "venue": "Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Moon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moon et al\\.", "year": 2015}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1312.6026,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Dropout improves Recurrent Neural Networks for Handwriting Recognition", "author": ["V. Pham", "T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": "ArXiv e-prints,", "citeRegEx": "Pham et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2013}, {"title": "Recurrent dropout without memory loss", "author": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth"], "venue": "arXiv preprint arXiv:1603.05118,", "citeRegEx": "Semeniuta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Semeniuta et al\\.", "year": 2016}, {"title": "Swapout: Learning an ensemble of deep architectures", "author": ["S. Singh", "D. Hoiem", "D. Forsyth"], "venue": "ArXiv e-prints,", "citeRegEx": "Singh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio"], "venue": "CoRR, abs/1506.00619,", "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Fast dropout training", "author": ["Sida Wang", "Christopher Manning"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Wang and Manning.,? \\Q2013\\E", "shortCiteRegEx": "Wang and Manning.", "year": 2013}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Recurrent nets have recently been used to achieve state-of-the-art results in many machine learning domains including speech recognition [Miao et al., 2015], computer vision [Yao et al.", "startOffset": 137, "endOffset": 156}, {"referenceID": 24, "context": ", 2015], computer vision [Yao et al., 2015], language modeling [Kim et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 9, "context": ", 2015], language modeling [Kim et al., 2015] and machine translation [Bahdanau et al.", "startOffset": 27, "endOffset": 45}, {"referenceID": 1, "context": ", 2015] and machine translation [Bahdanau et al., 2014].", "startOffset": 32, "endOffset": 55}, {"referenceID": 18, "context": "In particular, we show the benefit of our approach compared with the most successful application of dropout on recurrent connections [Semeniuta et al., 2016] and other proposed regularizers for RNNs.", "startOffset": 133, "endOffset": 157}, {"referenceID": 19, "context": "Concurrent with our work, Singh et al. [2016] propose zoneout for feedforward nets, calling the technique SkipForward.", "startOffset": 26, "endOffset": 46}, {"referenceID": 0, "context": "Zoneout, like dropout, trains a pseudo-ensemble [Bachman et al., 2014], but uses a stochastic \u201cidentity-mask\u201d rather than a zero-mask.", "startOffset": 48, "endOffset": 70}, {"referenceID": 23, "context": "[2013] successfully apply fast dropout [Wang and Manning, 2013], a deterministic approximation of dropout, to RNNs.", "startOffset": 39, "endOffset": 63}, {"referenceID": 3, "context": "Whereas saturated output gates or output nonlinearities would cause recurrent dropout to suffer from the vanishing gradient problem [Bengio et al., 1994], the identity-masked units in zoneout still propagate gradients effectively in this situation.", "startOffset": 132, "endOffset": 153}, {"referenceID": 2, "context": "Bayer et al. [2013] successfully apply fast dropout [Wang and Manning, 2013], a deterministic approximation of dropout, to RNNs.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Bayer et al. [2013] successfully apply fast dropout [Wang and Manning, 2013], a deterministic approximation of dropout, to RNNs. We now describe several recent works [Semeniuta et al., 2016, Moon et al., 2015, Gal, 2015] that propose variants of dropout for use in RNNs. Semeniuta et al. [2016] apply recurrent dropout to the updates to LSTM memory cells (or GRU states), i.", "startOffset": 0, "endOffset": 295}, {"referenceID": 14, "context": "Also motivated by preventing memory loss, Moon et al. [2015] propose rnnDrop.", "startOffset": 42, "endOffset": 61}, {"referenceID": 4, "context": "Gal [2015] proposed the same technique and evaluated it on natural language processing tasks.", "startOffset": 0, "endOffset": 11}, {"referenceID": 8, "context": "3 Relationship to Stochastic Depth Zoneout can also be viewed as a per-unit version of stochastic depth [Huang et al., 2016].", "startOffset": 104, "endOffset": 124}, {"referenceID": 5, "context": "[2016] train deep residual networks (ResNets [He et al., 2015]) and preserve the identity connections when dropping a layer, so that the inputs from the previous layer are fed into the next layer in the stack.", "startOffset": 45, "endOffset": 62}, {"referenceID": 7, "context": "3 Relationship to Stochastic Depth Zoneout can also be viewed as a per-unit version of stochastic depth [Huang et al., 2016]. Recently introduced as a method to efficiently train very deep feed-forward neural networks, stochastic depth randomly drops entire layers. To our knowledge, this method has not been extended to a recurrent setting. This is equivalent to zoning out all of the units of a layer at the same a time in a feedforward network. In a typical RNN, there is a new input at each time-step, causing issues for a naive implementation of stochastic depth. Firstly, Huang et al. [2016] train deep residual networks (ResNets [He et al.", "startOffset": 105, "endOffset": 598}, {"referenceID": 18, "context": "Recently, Semeniuta et al. [2016] proposed zero-masking the input gate (which they call recurrent dropout):", "startOffset": 10, "endOffset": 34}, {"referenceID": 18, "context": "Figure 2: (a) Zoneout, vs (b) the recurrent dropout strategy of [Semeniuta et al., 2016] in an LSTM.", "startOffset": 64, "endOffset": 88}, {"referenceID": 11, "context": "We evaluate zoneout\u2019s performance on the following tasks: \u2022 Classification of hand-written digits on permuted sequential MNIST [Le et al., 2015].", "startOffset": 127, "endOffset": 144}, {"referenceID": 12, "context": "\u2022 Word-level language modeling on the Penn Treebank corpus [Marcus et al., 1993].", "startOffset": 59, "endOffset": 80}, {"referenceID": 21, "context": "All models are trained for 150 epochs using RMSProp [Tieleman and Hinton, 2012] with a decay rate of 0.", "startOffset": 52, "endOffset": 79}, {"referenceID": 16, "context": "0 [Pascanu et al., 2013].", "startOffset": 2, "endOffset": 24}, {"referenceID": 10, "context": "075), and norm stabilizer (\u03b2 = 50) [Krueger and Memisevic, 2015].", "startOffset": 35, "endOffset": 64}, {"referenceID": 17, "context": "2 Word-level For the word-level task, we use similar settings to Semeniuta et al. [2016] and Mikolov et al.", "startOffset": 65, "endOffset": 89}, {"referenceID": 14, "context": "[2016] and Mikolov et al. [2014]. Unlike these previous works, however, we slice the data into non-overlapping sequences, and we do not use the last hidden states from one batch to initialize the next batch\u2019s hidden states.", "startOffset": 11, "endOffset": 33}, {"referenceID": 18, "context": "less data, and so our results are not directly comparable to those reported in Semeniuta et al. [2016]. The model is an LSTM with a single hidden layer of 256 units and weights initialized uniformly between -0.", "startOffset": 79, "endOffset": 103}, {"referenceID": 18, "context": "less data, and so our results are not directly comparable to those reported in Semeniuta et al. [2016]. The model is an LSTM with a single hidden layer of 256 units and weights initialized uniformly between -0.05 and +0.05. We train on non-overlapping sequences of length 35, in batches of 32, and optimize using SGD with momentum of 0.99 and gradient clipping threshold of 10. We start with learning rate of 1, reducing it by a factor of 1.5 after every epoch without an improvement in validation performance after Semeniuta et al. [2016]. We perform a grid search over zoneout/dropout probabilities {0.", "startOffset": 79, "endOffset": 540}, {"referenceID": 6, "context": "This could be used to learn a dynamic version of the hierarchical RNN proposed by Hihi and Bengio [1996], where the network would learn when to update different dimensions of its hidden state, which could be used to store information about different time-scales.", "startOffset": 82, "endOffset": 105}], "year": 2016, "abstractText": "We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find encouraging results: zoneout gives significant performance improvements across tasks, yielding state-ofthe-art results in character-level language modeling on the Penn Treebank dataset and competitive results on word-level Penn Treebank and permuted sequential MNIST classification tasks.", "creator": "LaTeX with hyperref package"}}}