{"id": "1606.01323", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2016", "title": "Improving Coreference Resolution by Learning Entity-Level Distributed Representations", "abstract": "A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning to search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features.", "histories": [["v1", "Sat, 4 Jun 2016 04:08:45 GMT  (744kb,D)", "https://arxiv.org/abs/1606.01323v1", "Accepted for publication at the Association for Computational Linguistics (ACL), 2016"], ["v2", "Wed, 8 Jun 2016 21:11:13 GMT  (255kb,D)", "http://arxiv.org/abs/1606.01323v2", "Accepted for publication at the Association for Computational Linguistics (ACL), 2016"]], "COMMENTS": "Accepted for publication at the Association for Computational Linguistics (ACL), 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kevin clark", "christopher d manning"], "accepted": true, "id": "1606.01323"}, "pdf": {"name": "1606.01323.pdf", "metadata": {"source": "CRF", "title": "Improving Coreference Resolution by Learning Entity-Level Distributed Representations", "authors": ["Kevin Clark", "Christopher D. Manning"], "emails": ["kevclark@cs.stanford.edu", "manning@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "The core resolution, the task of identification mentioned in a text, relates to the same real-world entity, is essentially a cluster problem. However, many current state-of-the-art correlation systems work exclusively by linking pairs of mentions to each other (Durrett and Klein, 2013; Martschat and Strube, 2015; Wiseman et al., 2015).An alternative approach is to use agglomerative clustering, treating each mention as a singleton cluster at the beginning and then repeatedly merging clusters of mentions that relate to the same entity. Such systems can take advantage of entity-level information, i.e., functions between clusters of mentions instead of just two mentions. As an example of why this is useful is that the clusters {Bill Clinton} and {Clinton} and {Clinton} and {Clinton} do not relate to the same entity, but it is unclear whether the pair of mentions are Clinton and Bill Clinton."}, {"heading": "2 System Architecture", "text": "Our cluster ranking model is a single neural network that learns which cluster merges are desirable, but it is helpful to consider the network as consisting of different subnetworks; the mention pair encoder generates distributed representations for reference pairs by passing relevant features through a feedback neural network; the cluster pair encoder then generates distributed representations for cluster pairs by performing a pooling operation on the representations of relevant mention pairs, i.e. pairs in which there is a mention in each cluster; the cluster ranking model then evaluates cluster pairs by passing their representations through a single neural network layer; and we also train a mention ranking model that evaluates mention pairs by passing their representations through a single neural network layer; its parameters are used to initialize the cluster ranking model that produces the cluster ranking, and to consider the candidate that it will be used by the cluster ranking model to initialize the cluster model, and the results it will produce."}, {"heading": "3 Building Representations", "text": "In this section, we describe the neural networks that generate distributed representations of pairs of 1Code, and trained models are available at https: / / github.com / clarkkev / deep-coref.ertions and pairs of coreference clusters. We assume that a number of mentions have already been extracted from each document using a method like the one in Raghunathan et al. (2010)."}, {"heading": "3.1 Mention-Pair Encoder", "text": "Considering a mention m and a candidate antecedent a, the mention pair encoder produces a distributed representation of the pair rm (a, m).Rd with a feedforward neural network that is shown in Figure 2. Candidate antecedent may be any mention that occurs before m in the document or in the NA, suggesting that m has no history. We also experimented with models that recurrent neural networks on Long Short-Term Memory (Hochreiter and Schmidhuber, 1997), but found these slightly worse when used in an end-to-end coreference system that is due to the severe overmatch to the training data.Input level for each mention, the model extracts different words and groups of words that are fed into the neural network. Each word is represented by a vector wi-Rdw. Each group of words is represented by the average of word vectors of each group in the group."}, {"heading": "3.2 Cluster-Pair Encoder", "text": "Given two clusters of mentions ci = {mi1, mi2,..., mi | ci |} and cj = {m j 1, m j 2,..., m j | cj |}, the cluster pair encoder generates a distributed representation rc (ci, cj) \u0445 R2d. The encoder architecture is shown in Figure 3.The cluster pair encoder first combines the information contained in the matrix of mention pair representations Rm (ci, cj) = [rm (m i 1, m j 1), rm (m i 1, m j 2),..., rm (m i | ci |, m j | cj |) to generate rc (ci, cj) by using a pooling operation. Specifically, it links the results of the max pool and the average pool, which we considered to be slightly more effective than the use of a single: rc (ci, cltk, &ltk, &ltj \u00b7 j; max;"}, {"heading": "4 Mention-Ranking Model", "text": "There are two main advantages that result from mentioning the cluster ranking model: First, it serves as a yardstick for capturing the cluster ranking model, and the decisions are clearly wrong (these decisions can be removed, thereby significantly reducing the search space of the cluster ranking model).The mention ranking model is assignificantly reduced. (a, m) The mention ranking model assignificantly. (a, m) The mention ranking model assignificantly. (a, m) The mention model assignificantly. (a, m) The mention ranking model assignificantly."}, {"heading": "5 Cluster-Ranking Model", "text": "Although the Mention Ranking Model in itself is a strong co-reference system, it has the disadvantage that it only takes into account local information between mention pairs, and therefore cannot consolidate information at the entity level. We solve this problem by training a cluster ranking model that evaluates cluster ranking pairs instead of mention pairs. In the case of two clusters with mentions ci and cj, the cluster ranking model generates a score sc (ci, cj) = Wcrc (ci, cj) + bcc, where Wc is a 1 \u00d7 2d weight matrix. Our cluster ranking approach also uses a measure of anaphorism, or as it is likely to have a mention."}, {"heading": "5.1 Cluster-Ranking Policy Network", "text": "We see this procedure as a sequential decision process in which at each step the algorithm observes the current state x and performs an action. Specifically, we define a state x = (C, m) consisting of C = {c1, c2,...}, the set of existing core reference clusters, and m, taking into account the current mention. In the initial state, each cluster in C contains a single mention. Let cm \u00b2 C be the cluster containing m and A (m) a series of candidate precursors for m: mentions that previously occurred in the document. Then, the available actions U (x) are from x \u2022 MERGE [cm, c], where c is a cluster containing a mention in A (m), combining cm and c into a single core reference cluster."}, {"heading": "5.2 Easy-First Cluster Ranking", "text": "The last detail needed is the order in which mentions need to be taken into account. Cluster ranking models in previous work order mentions according to their positions in the document and process them from left to right (Rahman and Ng, 2011; Ma et al., 2014). Instead, we sort mentions in descending order according to their highest-rated candidate coreference link according to the mention ranking model. This results in inferences being made in a simple way, delaying hard decisions until more information is available. Simple initial alignments have shown that they improve the performance of other incremental coreference strategies (Raghunathan et al., 2010; Stoyanov and Eisner, 2012), because they reduce the problem of errors that build up during the algorithm run. We also find it beneficial to trim the set of candidate precursors A (m) for each mention, instead of using all the precursors we mention as those candidates who are significantly lower than those we mention previously."}, {"heading": "5.3 Deep Learning to Search", "text": "We are faced with a sequential prediction problem in which future observations until a final state is reached depend on costs. (This is a challenge because it violates the common i.i.d. assumptions made in machine learning.) (It is a challenge because it violates the common i.i.d. basic assumptions in machine learning. (But it is also the case that it cannot be efficient prediction work in natural language processing. (thumb III et al., 2014; algorithm 1 Deep Learning to Search for i = 1 to num epochs doalize the current training for each example (x, y).D doRun to maintain the policy of completion of Start State X. (x1, x2, xn) for each state xi, xn) for each possible action U (xi) do Execute u on xi and then the reference policy ref until a final state is reached. (x2, xn)"}, {"heading": "6 Experiments and Results", "text": "Experimental setup. We conduct experiments with the English and Chinese portions of the CoNLL 2012 shared task data (Pradhan et al., 2012), evaluating the models using three of the most popular co-reference metrics: MUC, B3 and Entity-based CEAF (CEAF\u03c64). We generally report on the average F1 score (CoNLL F1) of the three, which is common practice in co-reference evaluation. We used the latest version of the CoNLL Scorer (version 8.01), which implements the original definitions of metrics. Mention Detection. Our experiments were conducted using system-produced predicted mentions. We used the rules-based mention algorithm from Raghunathan et al. (2010), which initially extracts pronouns and maximum NP projections as candidate mentions and then filters them with rules that remove false mentions such as pronouns and pronouns."}, {"heading": "6.1 Mention-Ranking Model Experiments", "text": "The results are presented in Table 1. We find that the low number of non-embedding characteristics significantly improves the performance of the model, especially the distance and string matching characteristics. This is not surprising since the additional characteristics are not easily captured by word embedding, and such characteristics have been very important in the past for Coreference Resolvers (Bengtson and Roth, 2008).The importance of pretraining. We evaluate the benefits of the two-step pretraining for the topic ranking model and report in Table 2. In line with Wiseman et al. (2015), we find pretraining to significantly improve the precision of the model. We note in particular that the model benefits from the use of both pre-training steps in Section 4, which gently lead the model from a mention pair classification target that is easy to optimize to a maximum margin target that is better suited for a ranking task."}, {"heading": "6.2 Cluster-Ranking Model Experiments", "text": "We evaluate the importance of three key details of the cluster ranking approach: initializing it with the weights of the model mentioned, using a simple first order of mentions, and searching with the help of learning. Results are shown in Table 3.Pretrained Weights. We randomly compare the initialization of the cluster ranking model with the weights learned through the mention ranking model. Using pre-trained weights significantly improves performance. We believe that the cluster ranking model has difficulty learning effective weights from scratch because the signal coming from cluster decisions at the level (an overall bad cluster merger may still include a few cor-rect pairwise links) and the smaller amount of data used to train the cluster ranking model (many possible actions are cut off during preprocessing). We believe that the score would be even lower without cutting back the search space, which prevents the model from taking too many bad actions into account."}, {"heading": "6.3 Capturing Semantic Similarity", "text": "The use of semantic information to improve coreferential accuracy has had mixed results in previous research and has been referred to as the \"uphill battle\" in coreferential resolution (Durrett and Klein, 2013). However, word embedding is known to capture semantic relationships effectively, and we show here that neural network coreferential models can benefit from it. Perhaps the case where semantic similarities are most important is the linkage of denominations that do not match the head (e.g. \"the nation\" and \"the country\"). We compare the performance of our neural network model with our previous statistical system (Clark and Manning, 2015) in classifying mention pairs of this type as coreferent or not. In this task, the neural network shows significant improvements (18.9 F1 vs. 10.7 F1) compared to the more modest improvement it achieves in classifying mention pairs as coreferent (6F1 vs. 61), but some of the coreferential values are not shown to contribute to this improvement in example 1.2."}, {"heading": "6.4 Final System Performance", "text": "In Table 5, we compare the results of our system with modern approaches to English and Chinese. Our mention ranking model outperforms all previous systems. We attribute the improvement to the neural mention ranking by Wiseman et al. (2015) to our model, which includes a deeper neural network, pre-trained word embedding and more sophisticated pretraining. The cluster ranking model further improves results in both languages and in all valuation metrics, demonstrating the benefits of incorporating information at the entity level. The improvement is greatest at CEAF\u03c64, which is encouraging because CEAF\u03c64 is the last proposed measure designed to correct errors in the other two (Luo, 2005). We believe that entity-level information is particularly useful in preventing bad mergers between large clusters (see Figure 4 for an example). In practice, the much more complicated cluster ranking model yields relatively modest performance gains."}, {"heading": "7 Related Work", "text": "There has been extensive work on machine learning approaches to coreference resolution (Soon et al., 2001; Ng and Cardie, 2002), with mention ranking models being particularly popular (Denis and Baldridge, 2007; Durrett and Klein, 2013; Martschat and Strube, 2015).We are building a neural mention ranking model inspired by Wiseman et al. (2015) as a starting point, but then use it to prepare a cluster ranking model that benefits from information at the entity level. Wise-man et al. (2016) expand their mention ranking model by incorporating information at the entity level produced by a recursive neural network running over the candidate precursor cluster. However, this is an augmentation to a mention ranking model and not basically a cluster ranking model, as is our cluster ranking model."}, {"heading": "8 Conclusion", "text": "We introduced a core reference system that captures entity-level information with distributed representations of core cluster pairs. These learned dense, high-dimensional feature vectors give our cluster ranking core reference model a strong ability to distinguish beneficial cluster mergers from harmful ones. It is trained with a learning-to-search algorithm that allows it to learn how local decisions affect the final core reference score. We evaluate our system using the English and Chinese parts of the CoNLL 2012 Shared Task and report a significant improvement over current technology."}, {"heading": "Acknowledgments", "text": "We thank Will Hamilton, Jon Gauthier and the anonymous critics for their thoughtful comments and suggestions. This work was supported by the NSF Award IIS-1514268."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual NLP", "author": ["Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "Conference on Natural Language Learning (CoNLL),", "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Algorithms for scoring coreference chains", "author": ["Bagga", "Baldwin1998] Amit Bagga", "Breck Baldwin"], "venue": "In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference,", "citeRegEx": "Bagga et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bagga et al\\.", "year": 1998}, {"title": "Understanding the value of features for coreference resolution", "author": ["Bengtson", "Roth2008] Eric Bengtson", "Dan Roth"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Bengtson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bengtson et al\\.", "year": 2008}, {"title": "Learning structured perceptrons for coreference resolution with latent antecedents and non-local features", "author": ["Bj\u00f6rkelund", "Kuhn2014] Anders Bj\u00f6rkelund", "Jonas Kuhn"], "venue": "In Association of Computational Linguistics (ACL),", "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "Learning to search for dependencies. arXiv preprint arXiv:1503.05615", "author": ["Chang et al.2015a] Kai-Wei Chang", "He He", "Hal Daum\u00e9 III", "John Langford"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Learning to search better than your teacher", "author": ["Chang et al.2015b] Kai-Wei Chang", "Akshay Krishnamurthy", "Alekh Agarwal", "Hal Daum\u00e9 III", "John Langford"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Combining the best of two worlds: A hybrid approach to multilingual coreference resolution", "author": ["Chen", "Ng2012] Chen Chen", "Vincent Ng"], "venue": "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Con-", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Entity-centric coreference resolution with model stacking", "author": ["Clark", "Manning2015] Kevin Clark", "Christopher D. Manning"], "venue": null, "citeRegEx": "Clark et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2015}, {"title": "A large-scale exploration of effective global features for a joint entity detection and tracking model", "author": ["III Daum\u00e9", "III Marcu2005] Hal Daum\u00e9", "Marcu. Daniel"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2005}, {"title": "Search-based structured prediction", "author": ["John Langford", "Daniel Marcu"], "venue": "Machine Learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Efficient programmable learning to search. arXiv preprint arXiv:1406.1837", "author": ["John Langford", "Stephane Ross"], "venue": null, "citeRegEx": "III et al\\.,? \\Q2014\\E", "shortCiteRegEx": "III et al\\.", "year": 2014}, {"title": "A ranking approach to pronoun resolution", "author": ["Denis", "Baldridge2007] Pascal Denis", "Jason Baldridge"], "venue": "In International Joint Conferences on Artificial Intelligence (IJCAI),", "citeRegEx": "Denis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Denis et al\\.", "year": 2007}, {"title": "Easy victories and uphill battles in coreference resolution", "author": ["Durrett", "Klein2013] Greg Durrett", "Dan Klein"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Durrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2013}, {"title": "Decentralized entitylevel modeling for coreference resolution", "author": ["Durrett et al.2013] Greg Durrett", "David Leo Wright Hall", "Dan Klein"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Durrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2013}, {"title": "Latent structure perceptron with feature induction for unrestricted coreference resolution", "author": ["C\u0131\u0301cero Nogueira Dos Santos", "Ruy Luiz Milidi\u00fa"], "venue": "In Proceedings of the Joint Conference on Empirical", "citeRegEx": "Fernandes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fernandes et al\\.", "year": 2012}, {"title": "Coreference resolution in a modular, entity-centered model. In Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL), pages", "author": ["Haghighi", "Klein2010] Aria Haghighi", "Dan Klein"], "venue": null, "citeRegEx": "Haghighi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2010}, {"title": "Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["Hinton", "Tieleman2012] Geoffrey Hinton", "Tijmen Tieleman"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "A mention-synchronous coreference resolution algorithm based on the Bell tree", "author": ["Luo et al.2004] Xiaoqiang Luo", "Abe Ittycheriah", "Hongyan Jing", "Nanda Kambhatla", "Salim Roukos"], "venue": null, "citeRegEx": "Luo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2004}, {"title": "On coreference resolution performance metrics", "author": ["Xiaoqiang Luo"], "venue": "Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Luo.,? \\Q2005\\E", "shortCiteRegEx": "Luo.", "year": 2005}, {"title": "Prune-andscore: Learning for greedy coreference resolution", "author": ["Ma et al.2014] Chao Ma", "Janardhan Rao Doppa", "J Walker Orr", "Prashanth Mannem", "Xiaoli Fern", "Tom Dietterich", "Prasad Tadepalli"], "venue": null, "citeRegEx": "Ma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2014}, {"title": "Latent structures for coreference resolution. Transactions of the Association for Computational Linguistics (TACL), 3:405\u2013418", "author": ["Martschat", "Strube2015] Sebastian Martschat", "Michael Strube"], "venue": null, "citeRegEx": "Martschat et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Martschat et al\\.", "year": 2015}, {"title": "Toward conditional models of identity uncertainty with application to proper noun coreference", "author": ["McCallum", "Wellner2003] Andrew McCallum", "Ben Wellner"], "venue": "In Proceedings of the IJCAI Workshop on Information Integration on the Web", "citeRegEx": "McCallum et al\\.,? \\Q2003\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2003}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E. Hinton"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Improving machine learning approaches to coreference resolution", "author": ["Ng", "Cardie2002] Vincent Ng", "Claire Cardie"], "venue": "In Association of Computational Linguistics (ACL),", "citeRegEx": "Ng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2002}, {"title": "A joint framework for coreference resolution and mention head detection", "author": ["Peng et al.2015] Haoruo Peng", "Kai-Wei Chang", "Dan Roth"], "venue": "Conference on Natural Language Learning (CoNLL),", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Joint unsupervised coreference resolution with markov logic", "author": ["Poon", "Domingos2008] Hoifung Poon", "Pedro Domingos"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Poon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2008}, {"title": "Conll-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes", "author": ["Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang"], "venue": "In Proceedings of the Joint Conference on Empirical", "citeRegEx": "Pradhan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "A multi-pass sieve for coreference resolution", "author": ["Heeyoung Lee", "Sudarshan Rangarajan", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky", "Christopher Manning"], "venue": null, "citeRegEx": "Raghunathan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raghunathan et al\\.", "year": 2010}, {"title": "Narrowing the modeling gap: a clusterranking approach to coreference resolution", "author": ["Rahman", "Ng2011] Altaf Rahman", "Vincent Ng"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Rahman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rahman et al\\.", "year": 2011}, {"title": "A machine learning approach to coreference resolution of noun phrases", "author": ["Soon et al.2001] Wee Meng Soon", "Hwee Tou Ng", "Daniel Chung Yong Lim"], "venue": null, "citeRegEx": "Soon et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Soon et al\\.", "year": 2001}, {"title": "Easy-first coreference resolution", "author": ["Stoyanov", "Eisner2012] Veselin Stoyanov", "Jason Eisner"], "venue": "In International Conference on Computational Linguistics (COLING),", "citeRegEx": "Stoyanov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stoyanov et al\\.", "year": 2012}, {"title": "A model-theoretic coreference scoring scheme", "author": ["Vilain et al.1995] Marc Vilain", "John Burger", "John Aberdeen", "Dennis Connolly", "Lynette Hirschman"], "venue": "In Proceedings of the 6th conference on Message understanding,", "citeRegEx": "Vilain et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Vilain et al\\.", "year": 1995}, {"title": "Learning anaphoricity and antecedent ranking features for coreference resolution", "author": ["Wiseman et al.2015] Sam Wiseman", "Alexander M. Rush", "Stuart M. Shieber", "Jason Weston"], "venue": "In Association of Computational Linguistics (ACL),", "citeRegEx": "Wiseman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wiseman et al\\.", "year": 2015}, {"title": "Learning global features for coreference resolution. In Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL)", "author": ["Wiseman et al.2016] Sam Wiseman", "Alexander M. Rush", "Stuart M. Shieber"], "venue": null, "citeRegEx": "Wiseman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wiseman et al\\.", "year": 2016}, {"title": "An entity-mention model for coreference resolution with inductive logic programming", "author": ["Yang et al.2008] Xiaofeng Yang", "Jian Su", "Jun Lang", "Chew Lim Tan", "Ting Liu", "Sheng Li"], "venue": "In Association of Computational Linguistics (ACL),", "citeRegEx": "Yang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 35, "context": "However, many recent state-of-the-art coreference systems operate solely by linking pairs of mentions together (Durrett and Klein, 2013; Martschat and Strube, 2015; Wiseman et al., 2015).", "startOffset": 111, "endOffset": 186}, {"referenceID": 30, "context": "Previous work has incorporated entity-level information through features that capture hard constraints like having gender or number agreement between clusters (Raghunathan et al., 2010; Durrett et al., 2013).", "startOffset": 159, "endOffset": 207}, {"referenceID": 12, "context": "Previous work has incorporated entity-level information through features that capture hard constraints like having gender or number agreement between clusters (Raghunathan et al., 2010; Durrett et al., 2013).", "startOffset": 159, "endOffset": 207}, {"referenceID": 30, "context": "We assume that a set of mentions has already been extracted from each document using a method such as the one in Raghunathan et al. (2010).", "startOffset": 113, "endOffset": 139}, {"referenceID": 30, "context": "Speaker Features: Whether the mentions have the same speaker and whether one mention is the other mention\u2019s speaker as determined by string matching rules from Raghunathan et al. (2010).", "startOffset": 160, "endOffset": 186}, {"referenceID": 35, "context": "We train the mentionranking model with the slack-rescaled maxmargin training objective from Wiseman et al. (2015), which encourages separation between the highest scoring true and false antecedents of the current mention.", "startOffset": 92, "endOffset": 114}, {"referenceID": 24, "context": "We initialized our word embeddings with 50 dimensional ones produced by word2vec (Mikolov et al., 2013) on the Gigaword corpus for English and 64 dimensional ones provided by Polyglot (Al-Rfou et al.", "startOffset": 81, "endOffset": 103}, {"referenceID": 0, "context": ", 2013) on the Gigaword corpus for English and 64 dimensional ones provided by Polyglot (Al-Rfou et al., 2013) for Chinese.", "startOffset": 88, "endOffset": 110}, {"referenceID": 16, "context": "To regularize the network, we applied L2 regularization to the model weights and dropout (Hinton et al., 2012) with a rate of 0.", "startOffset": 89, "endOffset": 110}, {"referenceID": 35, "context": "As in Wiseman et al. (2015), we found that pretraining is crucial for the mentionranking model\u2019s success.", "startOffset": 6, "endOffset": 28}, {"referenceID": 35, "context": "As in Wiseman et al. (2015), we found that pretraining is crucial for the mentionranking model\u2019s success. We pretrained the network in two stages, minimizing the following objectives from Clark and Manning (2015):", "startOffset": 6, "endOffset": 213}, {"referenceID": 21, "context": "Cluster-ranking models in prior work order the mentions according to their positions in the document, processing them leftto-right (Rahman and Ng, 2011; Ma et al., 2014).", "startOffset": 131, "endOffset": 169}, {"referenceID": 30, "context": "Easy-first orderings have been shown to improve the performance of other incremental coreference strategies (Raghunathan et al., 2010; Stoyanov and Eisner, 2012) because they reduce the problem of errors compounding as the algorithm runs.", "startOffset": 108, "endOffset": 161}, {"referenceID": 34, "context": "Although our system evaluation also includes the MUC (Vilain et al., 1995) and CEAF\u03c64 (Luo, 2005) metrics, we do not incorporate them into the loss because MUC has the flaw of treating all errors equally and CEAF\u03c64 is slow to compute.", "startOffset": 53, "endOffset": 74}, {"referenceID": 20, "context": ", 1995) and CEAF\u03c64 (Luo, 2005) metrics, we do not incorporate them into the loss because MUC has the flaw of treating all errors equally and CEAF\u03c64 is slow to compute.", "startOffset": 19, "endOffset": 30}, {"referenceID": 29, "context": "We run experiments on the English and Chinese portions of the CoNLL 2012 Shared Task data (Pradhan et al., 2012).", "startOffset": 90, "endOffset": 112}, {"referenceID": 30, "context": "We used the rule-based mention detection algorithm from Raghunathan et al. (2010), which first extracts pronouns and maximal NP projections as candidate mentions and then filters this set with rules that remove spurious mentions such as numeric entities and pleonastic it pronouns.", "startOffset": 56, "endOffset": 82}, {"referenceID": 35, "context": "Consistent with Wiseman et al. (2015), we find pretraining to greatly improve the model\u2019s accuracy.", "startOffset": 16, "endOffset": 38}, {"referenceID": 35, "context": "We attribute its improvement over the neural mention ranker from Wiseman et al. (2015) to our model using a deeper neural network, pretrained word embeddings, and more sophisticated pretraining.", "startOffset": 65, "endOffset": 87}, {"referenceID": 20, "context": "The improvement is largest in CEAF\u03c64 , which is encouraging because CEAF\u03c64 is the most recently proposed metric, designed to correct flaws in the other two (Luo, 2005).", "startOffset": 156, "endOffset": 167}, {"referenceID": 32, "context": "There has been extensive work on machine learning approaches to coreference resolution (Soon et al., 2001; Ng and Cardie, 2002), with mentionranking models being particularly popular (Denis and Baldridge, 2007; Durrett and Klein, 2013; Martschat and Strube, 2015).", "startOffset": 87, "endOffset": 127}, {"referenceID": 35, "context": "We train a neural mention-ranking model inspired by Wiseman et al. (2015) as a starting point, but then use it to pretrain a cluster-ranking model that benefits from entity-level information.", "startOffset": 52, "endOffset": 74}, {"referenceID": 27, "context": "02 Peng et al. (2015) \u2013 \u2013 72.", "startOffset": 3, "endOffset": 22}, {"referenceID": 27, "context": "02 Peng et al. (2015) \u2013 \u2013 72.22 \u2013 \u2013 60.50 \u2013 \u2013 56.37 63.03 Wiseman et al. (2015) 76.", "startOffset": 3, "endOffset": 80}, {"referenceID": 27, "context": "02 Peng et al. (2015) \u2013 \u2013 72.22 \u2013 \u2013 60.50 \u2013 \u2013 56.37 63.03 Wiseman et al. (2015) 76.23 69.31 72.60 66.07 55.83 60.52 59.41 54.88 57.05 63.39 Wiseman et al. (2016) 77.", "startOffset": 3, "endOffset": 162}, {"referenceID": 19, "context": "Entity-level information has also been incorporated in coreference systems using joint inference (McCallum and Wellner, 2003; Poon and Domingos, 2008; Haghighi and Klein, 2010) and systems that build up coreference clusters incrementally (Luo et al., 2004; Yang et al., 2008; Raghunathan et al., 2010).", "startOffset": 238, "endOffset": 301}, {"referenceID": 37, "context": "Entity-level information has also been incorporated in coreference systems using joint inference (McCallum and Wellner, 2003; Poon and Domingos, 2008; Haghighi and Klein, 2010) and systems that build up coreference clusters incrementally (Luo et al., 2004; Yang et al., 2008; Raghunathan et al., 2010).", "startOffset": 238, "endOffset": 301}, {"referenceID": 30, "context": "Entity-level information has also been incorporated in coreference systems using joint inference (McCallum and Wellner, 2003; Poon and Domingos, 2008; Haghighi and Klein, 2010) and systems that build up coreference clusters incrementally (Luo et al., 2004; Yang et al., 2008; Raghunathan et al., 2010).", "startOffset": 238, "endOffset": 301}, {"referenceID": 21, "context": "We take the latter approach, and in particular combine the cluster-ranking (Rahman and Ng, 2011; Ma et al., 2014) and easy-first (Stoyanov and Eisner, 2012; Clark and Manning, 2015) clustering strategies.", "startOffset": 75, "endOffset": 113}, {"referenceID": 14, "context": "Other works use structured perceptron models for the same purpose (Stoyanov and Eisner, 2012; Fernandes et al., 2012; Bj\u00f6rkelund and Kuhn, 2014).", "startOffset": 66, "endOffset": 144}, {"referenceID": 8, "context": "Learning-to-search style algorithms have been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by Daum\u00e9 et al. (2005), Ma et al.", "startOffset": 167, "endOffset": 187}, {"referenceID": 8, "context": "Learning-to-search style algorithms have been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by Daum\u00e9 et al. (2005), Ma et al. (2014), and Clark and Manning (2015).", "startOffset": 167, "endOffset": 205}, {"referenceID": 8, "context": "Learning-to-search style algorithms have been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by Daum\u00e9 et al. (2005), Ma et al. (2014), and Clark and Manning (2015). Other works use structured perceptron models for the same purpose (Stoyanov and Eisner, 2012; Fernandes et al.", "startOffset": 167, "endOffset": 235}], "year": 2016, "abstractText": "A long-standing challenge in coreference resolution has been the incorporation of entity-level information \u2013 features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features.", "creator": "LaTeX with hyperref package"}}}