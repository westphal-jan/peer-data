{"id": "1606.01545", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "Neural Net Models for Open-Domain Discourse Coherence", "abstract": "Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. Yet existing models of coherence focus on individual aspects of coherence (lexical overlap, rhetorical structure, entity centering) and are trained on narrow domains. We introduce algorithms that capture diverse kinds of coherence by learning to distinguish coherent from incoherent discourse from vast amounts of open-domain training data. We propose two models, one discriminative and one generative, both using LSTMs as the backbone. The discriminative model treats windows of sentences from original human-generated articles as coherent examples and windows generated by randomly replacing sentences as incoherent examples. The generative model is a \\sts model that estimates the probability of generating a sentence given its contexts. Our models achieve state-of-the-art performance on multiple coherence evaluations. Qualitative analysis suggests that our generative model captures many aspects of coherence including lexical, temporal, causal, and entity-based coherence.", "histories": [["v1", "Sun, 5 Jun 2016 18:29:45 GMT  (209kb,D)", "http://arxiv.org/abs/1606.01545v1", null], ["v2", "Sun, 29 Jan 2017 00:21:43 GMT  (502kb,D)", "http://arxiv.org/abs/1606.01545v2", null], ["v3", "Sun, 24 Sep 2017 01:38:11 GMT  (492kb,D)", "http://arxiv.org/abs/1606.01545v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "dan jurafsky"], "accepted": true, "id": "1606.01545"}, "pdf": {"name": "1606.01545.pdf", "metadata": {"source": "CRF", "title": "Neural Net Models for Open-Domain Discourse Coherence", "authors": ["Jiwei Li", "Dan Jurafsky"], "emails": ["jiweil@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Modelling the discourse coherence of a text (the way parts of a text are linked into a coherent whole) is essential for tasks such as summaries (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997), question-and-answer (Verberne et al., 2007), and even applications such as psychiatric diagnoses (Elveva, g et al., 2007; Bedi et al., 2015). Different frameworks exist, each dealing with aspects of coherence. Lexical cohesion (Halliday and Hasan, 1System, Code and Datasets available on publication.1976; Morris and Hirst, 1991) Models of words and synonyms."}, {"heading": "2 Related Work", "text": "There are many frameworks for discourse coherence: Lexical Coherence Coherence Lis, heavily influenced by words linked by identity, synonyms or other lexical relationships that form chains between segments of discourse (Halliday and Hasan, 1976). Early models that used tools such as thesauri (Morris and Hirst, 1991) are used in later works to capture more subtle lexical relationships that may not be available in thesaurix. Structured discourse relationships such as Rhetorical Structure Theory (Mann and Thompson, 1988), a manically defined set of discourse relations between clauses, or discourse theories."}, {"heading": "3 Models", "text": "In this section, we describe the two models of coherence modelling that are suitable for different scenarios in the real world."}, {"heading": "3.1 The Discriminative Model", "text": "In fact, most of them are able to survive by themselves if they do not play by the rules. (...) Most of them are able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...)"}, {"heading": "3.2 The Generative Model", "text": "In a coherent context, a machine should be able to guess the next utterance against the previous one. Therefore, we propose to measure the degree of coherence against the probability of observing a sentence against its context. In the face of two consecutive sentences [si, si + 1], we measure the coherence by measuring the probability of generating si + 1 and generating si + 1 given si: L (si, si + 1) = 12 [log p (si, si + 1 | si)]] (4) Eq.4 measures the interdependence between the two successive sentences. Both p (si, si + 1) and p (si, si + 1 | si) can be calculated using SEQ2SEQ models."}, {"heading": "4 Experimental Results", "text": "In this section, we describe experimental results. First, we evaluate the proposed model for the task of the sentence order using two records, a standard domain-specific record (Barzilay and Lapata, 2008) and a newly constructed open domain record from Wikipedia. Next, we propose the task of paragraph reconstruction, which reconstructs an original paragraph from its components whose order has been changed."}, {"heading": "4.1 Sentence Ordering, Domain-specific Data", "text": "We first evaluate the proposed algorithms based on a widely used dataset in the sentence order and predicate based on the assumption that an article is always more conclusive than a random permutation of its sentences (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011).The corpus consists of 200 articles, each of which originates from two areas: NTSB Aircraft Accident Reports (V = 4758, 10.6 sentences / document) and AP Earthquake Reports (V = 3287, 11.5 sentences / document), divided into training and testing. For each document, pairs of permutations are generated. Each document contains the original document sequence and a random permutation of sentences from the same documentation. We use reduced versions of our two models to 4permutations, those of people.csail.edu / regherence / coherence."}, {"heading": "4.2 Evaluating Ordering on Open-domain", "text": "Since the data set presented in Barzilay and Lapata (2008) is fairly domain specific, we suggest testing consistency with a much larger, domain-free data set: Wikipedia. We created a test set by randomly selecting 984 paragraphs from Wikipedia Dump 2014, each paragraph consisting of at least 16 sets. The training set consists of 80 million sets. We ensure that there is no overlap between the training set and the test set. Based on this data set, we define the following tasks for evaluation:"}, {"heading": "4.2.1 Binary Permutation Classification", "text": "In fact, it is so that we are able to hide, and that we are able to hide, \"he said.\" We have to be able to hide, \"he said.\" We have to be able to hide. We have to be able to hide, \"he said.\" We have to be able to hide, \"he said."}, {"heading": "4.2.2 Paragraph Reconstruction", "text": "We therefore believe that it is time to move the community to a more difficult task of measuring successive sentences. We propose the task of reconstructing an original paragraph from a bag of constituent sentences previously used in the coherence assessment (Lapata, 2003). We propose the task of reconstructing an original paragraph from a bag of constituent sentences previously used in the coherence assessment. Since the discriminatory model calculates the coherence of a sentence, it cannot be applied to this task because we do not know the surrounding context. We only use the generative model: (1) The first sentence of a paragraph is given for each step, we do not compute the remaining sentence for this task because we do not know the surrounding context."}, {"heading": "4.3 Qualitative Analysis", "text": "To investigate what types of coherence the model can handle, we examine some relevant examples that comment on the (log probability) coherence of the generative model.Each of the examples above / below was selected in advance before we trained our model, so no \"cherry pickers\" were made. Case 1: Lexic coherence Pinochet was arrested. His arrest was unexpected. -4.25 Pinochet was arrested. His death was unexpected. -4.68 Mary ate some apples. She likes apples. -5.66 Mary ate some apples. She likes pears. She likes some apples. She likes Paris. -6.72 The model can handle lexical coherence, correctly favoring the 1st over the 4th examples. Case 2: Temporary order Washington was unanimously elected president in the first two national elections. He overlooked the creation of a strong, well-financed national government."}, {"heading": "5 Conclusion", "text": "We study the issue of discourse coherence, treat natural texts as coherent and permutations as incoherent, and train large neural models based on coherence, including large open domain test sets. Performance and our qualitative analysis suggest that the distributed sentence representations created by the model capture some of the implicit linguistic components of coherence. Our model outperforms the LSA baselines, suggesting that it may be good at modeling lexical coherence and seems to capture semantic coherence based on entity focus on discourse, such as temporal and causal relationships, which earlier models such as LSA and grid-based models could not capture. It also performs better than grid-based models, suggesting that it may be good at capturing coherence based on entity focus on discourse. SEQ2SEQ models have recently achieved success in many generational tasks. The fact that our model performs a key role in SE2Q in SE2Q, including SE2Q, despite its well-known SE2Q reconstruction problems (SE2Q)."}, {"heading": "6 Acknowledgement", "text": "We would like to thank Kelvin Guu, Percy Liang, Chris Manning, Sida Wang, Ziang Xie and other members of the Stanford NLP Groups for their insightful comments and suggestions. Jiwei Li is supported by the Facebook Fellowship, which we gratefully acknowledge, and this work is partially supported by the NSF Award IIS-1514268. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF or Facebook."}], "references": [{"title": "Computing locally coherent discourses", "author": ["Ernst Althaus", "Nikiforos Karamanis", "Alexander Koller."], "venue": "Proceedings of ACL 2004.", "citeRegEx": "Althaus et al\\.,? 2004", "shortCiteRegEx": "Althaus et al\\.", "year": 2004}, {"title": "Modeling local coherence: An entity-based approach", "author": ["Regina Barzilay", "Mirella Lapata."], "venue": "Computational Linguistics, 34(1):1\u201334.", "citeRegEx": "Barzilay and Lapata.,? 2008", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2008}, {"title": "Sentence fusion for multidocument news summarization", "author": ["Regina Barzilay", "Kathleen R McKeown."], "venue": "Computational Linguistics, 31(3):297\u2013328.", "citeRegEx": "Barzilay and McKeown.,? 2005", "shortCiteRegEx": "Barzilay and McKeown.", "year": 2005}, {"title": "Automated analysis of free speech predicts psychosis onset in high-risk youths", "author": ["Gillinder Bedi", "Facundo Carrillo", "Guillermo A Cecchi", "Diego Fern\u00e1ndez Slezak", "Mariano Sigman", "Nat\u00e1lia B Mota", "Sidarta Ribeiro", "Daniel C Javitt", "Mauro Copelli", "Cheryl M Corcoran"], "venue": null, "citeRegEx": "Bedi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bedi et al\\.", "year": 2015}, {"title": "Better document-level sentiment analysis from rst discourse parsing", "author": ["Parminder Bhatia", "Yangfeng Ji", "Jacob Eisenstein."], "venue": "arXiv preprint arXiv:1509.01599.", "citeRegEx": "Bhatia et al\\.,? 2015", "shortCiteRegEx": "Bhatia et al\\.", "year": 2015}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "the Journal of machine Learning research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Using entity-based features to model coherence in student essays", "author": ["Jill Burstein", "Joel Tetreault", "Slava Andreyev."], "venue": "Human language technologies: The 2010 annual conference of the North American chapter of the Association for Computational Linguistics, pages", "citeRegEx": "Burstein et al\\.,? 2010", "shortCiteRegEx": "Burstein et al\\.", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Extending the entity grid with entity-specific features", "author": ["Micha Eisner", "Eugene Charniak."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 125\u2013129. Association for", "citeRegEx": "Eisner and Charniak.,? 2011", "shortCiteRegEx": "Eisner and Charniak.", "year": 2011}, {"title": "Coreferenceinspired coherence modeling", "author": ["Micha Elsner", "Eugene Charniak."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 41\u201344. Association for Computa-", "citeRegEx": "Elsner and Charniak.,? 2008", "shortCiteRegEx": "Elsner and Charniak.", "year": 2008}, {"title": "A unified local and global model for discourse coherence", "author": ["Micha Elsner", "Joseph L Austerweil", "Eugene Charniak."], "venue": "HLT-NAACL, pages 436\u2013443.", "citeRegEx": "Elsner et al\\.,? 2007", "shortCiteRegEx": "Elsner et al\\.", "year": 2007}, {"title": "Quantifying incoherence in speech: An automated methodology and novel application to schizophrenia", "author": ["Brita Elvev\u00e5g", "Peter W Foltz", "Daniel R Weinberger", "Terry E Goldberg."], "venue": "Schizophrenia research, 93(1):304\u2013316.", "citeRegEx": "Elvev\u00e5g et al\\.,? 2007", "shortCiteRegEx": "Elvev\u00e5g et al\\.", "year": 2007}, {"title": "Extending the entity-based coherence model with multiple ranks", "author": ["Vanessa Wei Feng", "Graeme Hirst."], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 315\u2013324. Association for Computational", "citeRegEx": "Feng and Hirst.,? 2012", "shortCiteRegEx": "Feng and Hirst.", "year": 2012}, {"title": "The measurement of textual coherence with latent semantic analysis", "author": ["Peter W Foltz", "Walter Kintsch", "Thomas K Landauer."], "venue": "Discourse processes, 25(23):285\u2013307.", "citeRegEx": "Foltz et al\\.,? 1998", "shortCiteRegEx": "Foltz et al\\.", "year": 1998}, {"title": "Discourse coherence and lsa", "author": ["Peter W Foltz."], "venue": "Handbook of latent semantic analysis, pages 167\u2013184.", "citeRegEx": "Foltz.,? 2007", "shortCiteRegEx": "Foltz.", "year": 2007}, {"title": "word2vec explained: Deriving mikolov et al.\u2019s negativesampling word-embedding method", "author": ["Yoav Goldberg", "Omer Levy"], "venue": "arXiv preprint arXiv:1402.3722", "citeRegEx": "Goldberg and Levy.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy.", "year": 2014}, {"title": "Centering: A framework for modeling the local coherence of discourse", "author": ["Barbara J Grosz", "Scott Weinstein", "Aravind K Joshi."], "venue": "Computational linguistics, 21(2):203\u2013225.", "citeRegEx": "Grosz et al\\.,? 1995", "shortCiteRegEx": "Grosz et al\\.", "year": 1995}, {"title": "Graphbased local coherence modeling", "author": ["Camille Guinaudeau", "Michael Strube."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 93\u2013103.", "citeRegEx": "Guinaudeau and Strube.,? 2013", "shortCiteRegEx": "Guinaudeau and Strube.", "year": 2013}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen."], "venue": "International Conference on Artificial Intelligence and Statistics, pages 297\u2013304.", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2010}, {"title": "Cohesion in English", "author": ["M.A.K. Halliday", "Ruqaiya Hasan."], "venue": "Longman.", "citeRegEx": "Halliday and Hasan.,? 1976", "shortCiteRegEx": "Halliday and Hasan.", "year": 1976}, {"title": "Planning coherent multisentential text", "author": ["Eduard H Hovy."], "venue": "Proceedings of the 26th annual meeting on Association for Computational Linguistics, pages 163\u2013 169. Association for Computational Linguistics.", "citeRegEx": "Hovy.,? 1988", "shortCiteRegEx": "Hovy.", "year": 1988}, {"title": "Representation learning for text-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 13\u201324.", "citeRegEx": "Ji and Eisenstein.,? 2014", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in Neural Information Processing Systems, pages 3276\u2013 3284.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Probabilistic text structuring: Experiments with sentence ordering", "author": ["Mirella Lapata."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 545\u2013552. Association for Computational Linguistics.", "citeRegEx": "Lapata.,? 2003", "shortCiteRegEx": "Lapata.", "year": 2003}, {"title": "Automatic evaluation of information ordering: Kendall\u2019s tau", "author": ["Mirella Lapata."], "venue": "Computational Linguistics, 32(4):471\u2013484.", "citeRegEx": "Lapata.,? 2006", "shortCiteRegEx": "Lapata.", "year": 2006}, {"title": "Discourse relations and defeasible knowledge", "author": ["Alex Lascarides", "Nicholas Asher."], "venue": "Proceedings of the 29th annual meeting on Association for Computational Linguistics, pages 55\u201362. Association for Computational Linguistics.", "citeRegEx": "Lascarides and Asher.,? 1991", "shortCiteRegEx": "Lascarides and Asher.", "year": 1991}, {"title": "A model of coherence based on distributed sentence representation", "author": ["Jiwei Li", "Eduard Hovy."], "venue": "Proceedings of Empirical Methods in Natural Language Processing.", "citeRegEx": "Li and Hovy.,? 2014", "shortCiteRegEx": "Li and Hovy.", "year": 2014}, {"title": "Recursive deep models for discourse parsing", "author": ["Jiwei Li", "Rumeng Li", "Eduard Hovy."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2061\u20132069.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1510.03055.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Automatically evaluating text coherence using discourse relations", "author": ["Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 997\u20131006. As-", "citeRegEx": "Lin et al\\.,? 2011", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "A coherence model based on syntactic patterns", "author": ["Annie Louis", "Ani Nenkova."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1157\u20131168. Association for", "citeRegEx": "Louis and Nenkova.,? 2012", "shortCiteRegEx": "Louis and Nenkova.", "year": 2012}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["William C Mann", "Sandra A Thompson."], "venue": "Text, 8(3):243\u2013281.", "citeRegEx": "Mann and Thompson.,? 1988", "shortCiteRegEx": "Mann and Thompson.", "year": 1988}, {"title": "From local to global coherence: A bottom-up approach to text planning", "author": ["Daniel Marcu."], "venue": "AAAI/IAAI, pages 629\u2013635. Citeseer.", "citeRegEx": "Marcu.,? 1997", "shortCiteRegEx": "Marcu.", "year": 1997}, {"title": "Coh-metrix: Capturing linguistic features of cohesion", "author": ["Danielle S. McNamara", "Max M. Louwerse", "Philip M. McCarthy", "Arthur C. Graesser."], "venue": "Discourse Processes, 47(4):292\u2013330.", "citeRegEx": "McNamara et al\\.,? 2010", "shortCiteRegEx": "McNamara et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Evaluation of text coherence for electronic essay scoring systems", "author": ["Eleni Miltsakaki", "Karen Kukich."], "venue": "Natural Language Engineering, 10(01):25\u201355.", "citeRegEx": "Miltsakaki and Kukich.,? 2004", "shortCiteRegEx": "Miltsakaki and Kukich.", "year": 2004}, {"title": "Planning text for advisory dialogues", "author": ["Johanna D Moore", "Cecile L Paris."], "venue": "Proceedings of the 27th annual meeting on Association for Computational Linguis-", "citeRegEx": "Moore and Paris.,? 1989", "shortCiteRegEx": "Moore and Paris.", "year": 1989}, {"title": "Lexical cohesion computed by thesaural relations as an indicator of the structure of text", "author": ["J. Morris", "G. Hirst."], "venue": "Computational Linguistics, 17(1):21\u201348.", "citeRegEx": "Morris and Hirst.,? 1991", "shortCiteRegEx": "Morris and Hirst.", "year": 1991}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Centering: A parametric theory and its instantiations", "author": ["Massimo Poesio", "Rosemary Stevenson", "Barbara Di Eugenio", "Janet Hitzeman."], "venue": "Computational linguistics, 30(3):309\u2013363.", "citeRegEx": "Poesio et al\\.,? 2004", "shortCiteRegEx": "Poesio et al\\.", "year": 2004}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1509.00685.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Functional centering: Grounding referential coherence in information structure", "author": ["Michael Strube", "Udo Hahn."], "venue": "Computational linguistics, 25(3):309\u2013344.", "citeRegEx": "Strube and Hahn.,? 1999", "shortCiteRegEx": "Strube and Hahn.", "year": 1999}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A fast, accurate, non-projective, semantically-enriched parser", "author": ["Stephen Tratz", "Eduard Hovy."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1257\u20131268. Association for Computational Linguistics.", "citeRegEx": "Tratz and Hovy.,? 2011", "shortCiteRegEx": "Tratz and Hovy.", "year": 2011}, {"title": "Evaluating discourse-based answer extraction for why-question answering", "author": ["Suzan Verberne", "Lou Boves", "Nelleke Oostdijk", "PeterArno Coppen."], "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in informa-", "citeRegEx": "Verberne et al\\.,? 2007", "shortCiteRegEx": "Verberne et al\\.", "year": 2007}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Centering theory in discourse", "author": ["Marilyn A Walker", "Aravind Krishna Joshi", "Ellen Friedman Prince."], "venue": "Oxford University Press.", "citeRegEx": "Walker et al\\.,? 1998", "shortCiteRegEx": "Walker et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 2, "context": "Modeling the discourse coherence of a text (the way parts of a text are linked into a coherent whole) is essential for tasks like summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al.", "startOffset": 144, "endOffset": 172}, {"referenceID": 20, "context": "Modeling the discourse coherence of a text (the way parts of a text are linked into a coherent whole) is essential for tasks like summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al.", "startOffset": 188, "endOffset": 213}, {"referenceID": 32, "context": "Modeling the discourse coherence of a text (the way parts of a text are linked into a coherent whole) is essential for tasks like summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al.", "startOffset": 188, "endOffset": 213}, {"referenceID": 45, "context": "Modeling the discourse coherence of a text (the way parts of a text are linked into a coherent whole) is essential for tasks like summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al., 2007), and even applications like psychiatric diagnosis (Elvev\u00e5g et al.", "startOffset": 233, "endOffset": 256}, {"referenceID": 11, "context": ", 2007), and even applications like psychiatric diagnosis (Elvev\u00e5g et al., 2007; Bedi et al., 2015).", "startOffset": 58, "endOffset": 99}, {"referenceID": 3, "context": ", 2007), and even applications like psychiatric diagnosis (Elvev\u00e5g et al., 2007; Bedi et al., 2015).", "startOffset": 58, "endOffset": 99}, {"referenceID": 13, "context": "Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) generalize lexical cohesion via LSA embeddings of sentences.", "startOffset": 34, "endOffset": 90}, {"referenceID": 14, "context": "Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) generalize lexical cohesion via LSA embeddings of sentences.", "startOffset": 34, "endOffset": 90}, {"referenceID": 33, "context": "Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) generalize lexical cohesion via LSA embeddings of sentences.", "startOffset": 34, "endOffset": 90}, {"referenceID": 31, "context": "Relational models like RST (Mann and Thompson, 1988; Lascarides and Asher, 1991) define relations that hierarchically structure texts.", "startOffset": 27, "endOffset": 80}, {"referenceID": 25, "context": "Relational models like RST (Mann and Thompson, 1988; Lascarides and Asher, 1991) define relations that hierarchically structure texts.", "startOffset": 27, "endOffset": 80}, {"referenceID": 1, "context": "Entity grid models (Barzilay and Lapata, 2008) model the referential coherence of entities moving in and out of focus across a text.", "startOffset": 19, "endOffset": 46}, {"referenceID": 43, "context": "ments as negative examples; and a generative model that uses sequence-to-sequence models (Sutskever et al., 2014) (SEQ2SEQ) to model the likelihood of generating a sentence based on its context.", "startOffset": 89, "endOffset": 113}, {"referenceID": 1, "context": "We evaluate the models on two text-ordering datasets, one from the literature (Barzilay and Lapata, 2008), and a new larger open-domain one.", "startOffset": 78, "endOffset": 105}, {"referenceID": 1, "context": "We evaluate the models on two text-ordering datasets, one from the literature (Barzilay and Lapata, 2008), and a new larger open-domain one. The discriminative model achieves state-of-the-art performance on the domain specific dataset presented in Barzilay and Lapata (2008), pushing the state-ofthe-art result to 96% accuracy, significantly outperforming all previous models.", "startOffset": 79, "endOffset": 275}, {"referenceID": 19, "context": "Lexical Coherence Coherence is strongly cued by words: words linked by identity, synonymy or other lexical relations forming chains across discourse segments (Halliday and Hasan, 1976).", "startOffset": 158, "endOffset": 184}, {"referenceID": 38, "context": "Early models used tools like thesauri (Morris and Hirst, 1991).", "startOffset": 38, "endOffset": 62}, {"referenceID": 13, "context": "Later work used Latent Semantic Analysis (LSA) embeddings (Foltz et al., 1998; Foltz, 2007), representing sentences with LSA vectors and measuring coherence with the cosine similarity of adjacent sentences, with the goal of capturing more subtle lexical relations that might not be available in thesauri.", "startOffset": 58, "endOffset": 91}, {"referenceID": 14, "context": "Later work used Latent Semantic Analysis (LSA) embeddings (Foltz et al., 1998; Foltz, 2007), representing sentences with LSA vectors and measuring coherence with the cosine similarity of adjacent sentences, with the goal of capturing more subtle lexical relations that might not be available in thesauri.", "startOffset": 58, "endOffset": 91}, {"referenceID": 31, "context": "Structured Discourse Relations Early work used discourse relations like Rhetorical Structure Theory (Mann and Thompson, 1988), a manually defined set of discourse relations between clauses, or Discourse Representation Theory (Lascarides and Asher, 1991)) a formal semantic model of discourse contexts, coreference and scope, to create coherent paragraphs in text planning (Hovy, 1988; Moore and Paris, 1989).", "startOffset": 100, "endOffset": 125}, {"referenceID": 25, "context": "Structured Discourse Relations Early work used discourse relations like Rhetorical Structure Theory (Mann and Thompson, 1988), a manually defined set of discourse relations between clauses, or Discourse Representation Theory (Lascarides and Asher, 1991)) a formal semantic model of discourse contexts, coreference and scope, to create coherent paragraphs in text planning (Hovy, 1988; Moore and Paris, 1989).", "startOffset": 225, "endOffset": 253}, {"referenceID": 20, "context": "Structured Discourse Relations Early work used discourse relations like Rhetorical Structure Theory (Mann and Thompson, 1988), a manually defined set of discourse relations between clauses, or Discourse Representation Theory (Lascarides and Asher, 1991)) a formal semantic model of discourse contexts, coreference and scope, to create coherent paragraphs in text planning (Hovy, 1988; Moore and Paris, 1989).", "startOffset": 372, "endOffset": 407}, {"referenceID": 37, "context": "Structured Discourse Relations Early work used discourse relations like Rhetorical Structure Theory (Mann and Thompson, 1988), a manually defined set of discourse relations between clauses, or Discourse Representation Theory (Lascarides and Asher, 1991)) a formal semantic model of discourse contexts, coreference and scope, to create coherent paragraphs in text planning (Hovy, 1988; Moore and Paris, 1989).", "startOffset": 372, "endOffset": 407}, {"referenceID": 16, "context": ", on the syntactic positions in which entities appear (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004).", "startOffset": 54, "endOffset": 139}, {"referenceID": 47, "context": ", on the syntactic positions in which entities appear (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004).", "startOffset": 54, "endOffset": 139}, {"referenceID": 42, "context": ", on the syntactic positions in which entities appear (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004).", "startOffset": 54, "endOffset": 139}, {"referenceID": 40, "context": ", on the syntactic positions in which entities appear (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004).", "startOffset": 54, "endOffset": 139}, {"referenceID": 9, "context": "Entity grid models have been extended with coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), discourse relations (Lin et al.", "startOffset": 55, "endOffset": 82}, {"referenceID": 8, "context": "Entity grid models have been extended with coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), discourse relations (Lin et al.", "startOffset": 99, "endOffset": 126}, {"referenceID": 29, "context": "Entity grid models have been extended with coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), discourse relations (Lin et al., 2011), and entity graphs (Guinaudeau and Strube, 2013).", "startOffset": 148, "endOffset": 166}, {"referenceID": 17, "context": ", 2011), and entity graphs (Guinaudeau and Strube, 2013).", "startOffset": 27, "endOffset": 56}, {"referenceID": 1, "context": "The most influential such model is the entity grid model of Barzilay and Lapata (2008), in which sentences are represented by a vector of coreferent discourse entities along with their grammatical roles.", "startOffset": 60, "endOffset": 87}, {"referenceID": 21, "context": "Neural Net Models Recent work focuses instead on representing sentences as dense, real-valued vectors (Ji and Eisenstein, 2014; Bhatia et al., 2015), such as by learning sentence representations as part of supervised RST discourse parsing (Li et al.", "startOffset": 102, "endOffset": 148}, {"referenceID": 4, "context": "Neural Net Models Recent work focuses instead on representing sentences as dense, real-valued vectors (Ji and Eisenstein, 2014; Bhatia et al., 2015), such as by learning sentence representations as part of supervised RST discourse parsing (Li et al.", "startOffset": 102, "endOffset": 148}, {"referenceID": 27, "context": ", 2015), such as by learning sentence representations as part of supervised RST discourse parsing (Li et al., 2014; Ji and Eisenstein, 2014).", "startOffset": 98, "endOffset": 140}, {"referenceID": 21, "context": ", 2015), such as by learning sentence representations as part of supervised RST discourse parsing (Li et al., 2014; Ji and Eisenstein, 2014).", "startOffset": 98, "endOffset": 140}, {"referenceID": 4, "context": "Neural Net Models Recent work focuses instead on representing sentences as dense, real-valued vectors (Ji and Eisenstein, 2014; Bhatia et al., 2015), such as by learning sentence representations as part of supervised RST discourse parsing (Li et al., 2014; Ji and Eisenstein, 2014). Our proposed discriminative model extends the coherence model of Li and Hovy (2014), a neural classifier trained on small domain-specific datasets (earthquake and accidents) using negative sampling at the sentence level.", "startOffset": 128, "endOffset": 367}, {"referenceID": 4, "context": "Neural Net Models Recent work focuses instead on representing sentences as dense, real-valued vectors (Ji and Eisenstein, 2014; Bhatia et al., 2015), such as by learning sentence representations as part of supervised RST discourse parsing (Li et al., 2014; Ji and Eisenstein, 2014). Our proposed discriminative model extends the coherence model of Li and Hovy (2014), a neural classifier trained on small domain-specific datasets (earthquake and accidents) using negative sampling at the sentence level. The algorithm we present significantly outperforms the classifier of Li and Hovy (2014).", "startOffset": 128, "endOffset": 592}, {"referenceID": 43, "context": "SEQ2SEQ models have been successfully applied to a variety of NLP tasks including machine translation (Sutskever et al., 2014), dialogue generation (Vinyals and Le, 2015), and abstractive summarization (Rush et al.", "startOffset": 102, "endOffset": 126}, {"referenceID": 46, "context": ", 2014), dialogue generation (Vinyals and Le, 2015), and abstractive summarization (Rush et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 41, "context": ", 2014), dialogue generation (Vinyals and Le, 2015), and abstractive summarization (Rush et al., 2015).", "startOffset": 83, "endOffset": 102}, {"referenceID": 22, "context": "Our idea of predicting the current sentence based on the previous one is similar to skip-thought models (Kiros et al., 2015) that build an LSTM encoder-decoder model by predicting tokens in neighboring sentences.", "startOffset": 104, "endOffset": 124}, {"referenceID": 22, "context": "Our idea of predicting the current sentence based on the previous one is similar to skip-thought models (Kiros et al., 2015) that build an LSTM encoder-decoder model by predicting tokens in neighboring sentences. We use the mutual dependency between the two consecutive sequences to measure coherence. This idea of modeling the mutual dependency between two sequences for neural generation has been explored by Li et al. (2015) for dialogue generation.", "startOffset": 105, "endOffset": 428}, {"referenceID": 34, "context": "The two models we propose can also be viewed as the a kind of generalization of the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b) to the sentence level.", "startOffset": 100, "endOffset": 146}, {"referenceID": 35, "context": "The two models we propose can also be viewed as the a kind of generalization of the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b) to the sentence level.", "startOffset": 100, "endOffset": 146}, {"referenceID": 18, "context": "To model negative incoherent examples, we resort to noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010).", "startOffset": 81, "endOffset": 110}, {"referenceID": 20, "context": "The proposed model can be viewed as an extension of Li and Hovy\u2019s (2014) model but is practical at large scale3.", "startOffset": 59, "endOffset": 73}, {"referenceID": 20, "context": "Li and Hovy\u2019s (2014) recursive neural model operates on parse trees, which does not support batched computation and is therefore hard to scale up.", "startOffset": 7, "endOffset": 21}, {"referenceID": 43, "context": "Both p(si|si+1) and p(si+1|si) can be computed using SEQ2SEQ models (Sutskever et al., 2014).", "startOffset": 68, "endOffset": 92}, {"referenceID": 43, "context": "Other training details are given below, broadly aligned with Sutskever et al. (2014): LSTM parameters and embeddings are initialized from a uniform distribution in [-0.", "startOffset": 61, "endOffset": 85}, {"referenceID": 1, "context": "We first evaluate the proposed model on the task of sentence ordering using two datasets, a standard domain-specific dataset (Barzilay and Lapata, 2008) and a newly constructed open-domain dataset from Wikipedia.", "startOffset": 125, "endOffset": 152}, {"referenceID": 1, "context": "Dataset We first evaluate the proposed algorithms on a dataset widely adopted in sentence ordering and predicate on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011).", "startOffset": 214, "endOffset": 305}, {"referenceID": 30, "context": "Dataset We first evaluate the proposed algorithms on a dataset widely adopted in sentence ordering and predicate on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011).", "startOffset": 214, "endOffset": 305}, {"referenceID": 10, "context": "Dataset We first evaluate the proposed algorithms on a dataset widely adopted in sentence ordering and predicate on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011).", "startOffset": 214, "endOffset": 305}, {"referenceID": 29, "context": "Dataset We first evaluate the proposed algorithms on a dataset widely adopted in sentence ordering and predicate on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011).", "startOffset": 214, "endOffset": 305}, {"referenceID": 27, "context": "842 Recursive Neural Models (Li et al. 2014) 0.", "startOffset": 28, "endOffset": 44}, {"referenceID": 1, "context": "920 Entity Grid Model (Barzilay and Lapata, 2008) 0.", "startOffset": 22, "endOffset": 49}, {"referenceID": 30, "context": "888 HMM (Louis and Nenkova, 2012) 0.", "startOffset": 8, "endOffset": 33}, {"referenceID": 30, "context": "880 HMM+Entity (Louis and Nenkova, 2012) 0.", "startOffset": 15, "endOffset": 40}, {"referenceID": 30, "context": "876 HMM+Content (Louis and Nenkova, 2012) 0.", "startOffset": 16, "endOffset": 41}, {"referenceID": 17, "context": "847 Graph (Guinaudeau and Strube, 2013) 0.", "startOffset": 10, "endOffset": 39}, {"referenceID": 1, "context": "920 Entity Grid Model (Barzilay and Lapata, 2008) 0.904 0.872 0.888 HMM (Louis and Nenkova, 2012) 0.822 0.938 0.880 HMM+Entity (Louis and Nenkova, 2012) 0.842 0.911 0.876 HMM+Content (Louis and Nenkova, 2012) 0.742 0.953 0.847 Graph (Guinaudeau and Strube, 2013) 0.846 0.635 0.740 Foltz et al. (1998)-Glove 0.", "startOffset": 23, "endOffset": 301}, {"referenceID": 1, "context": "920 Entity Grid Model (Barzilay and Lapata, 2008) 0.904 0.872 0.888 HMM (Louis and Nenkova, 2012) 0.822 0.938 0.880 HMM+Entity (Louis and Nenkova, 2012) 0.842 0.911 0.876 HMM+Content (Louis and Nenkova, 2012) 0.742 0.953 0.847 Graph (Guinaudeau and Strube, 2013) 0.846 0.635 0.740 Foltz et al. (1998)-Glove 0.705 0.682 0.693 Foltz et al. (1998)-LDA 0.", "startOffset": 23, "endOffset": 345}, {"referenceID": 13, "context": "Baseline numbers from prior work (except for Foltz et al. (1998)) are reprinted from the best performance reported in those papers.", "startOffset": 45, "endOffset": 65}, {"referenceID": 39, "context": "We use 300 dimensional embeddings borrowed from GLOVE (Pennington et al., 2014) to initialize word embeddings.", "startOffset": 54, "endOffset": 79}, {"referenceID": 7, "context": "Word embeddings are kept fixed during training and we update LSTM parameters using AdaGrad (Duchi et al., 2011).", "startOffset": 91, "endOffset": 111}, {"referenceID": 1, "context": "(1) Entity Grid Model: The grid model (Barzilay and Lapata, 2008) represents the sentence as a column of a grid of features and applies machine learning methods (e.", "startOffset": 38, "endOffset": 65}, {"referenceID": 1, "context": "(1) Entity Grid Model: The grid model (Barzilay and Lapata, 2008) represents the sentence as a column of a grid of features and applies machine learning methods (e.g., SVM) to identify the coherent transitions based on entity features. Results are directly taken from Barzilay and Lapata\u2019s (2008) paper.", "startOffset": 39, "endOffset": 297}, {"referenceID": 30, "context": "(2) HMM: A hidden-markov model described in Louis and Nenkova (2012) models the cluster transition probability in the coherent texts.", "startOffset": 44, "endOffset": 69}, {"referenceID": 17, "context": "(3) Graph Based Approach: Guinaudeau and Strube (2013) extended the entity grid model to a graph representing the text that embeds entity transition information needed for local coherence computation (Guinaudeau and Strube, 2013).", "startOffset": 200, "endOffset": 229}, {"referenceID": 17, "context": "(3) Graph Based Approach: Guinaudeau and Strube (2013) extended the entity grid model to a graph representing the text that embeds entity transition information needed for local coherence computation (Guinaudeau and Strube, 2013).", "startOffset": 26, "endOffset": 55}, {"referenceID": 17, "context": "(3) Graph Based Approach: Guinaudeau and Strube (2013) extended the entity grid model to a graph representing the text that embeds entity transition information needed for local coherence computation (Guinaudeau and Strube, 2013). (4) Li and Hovy (2014): A recursive neural model computes sentence representations based on parse trees.", "startOffset": 26, "endOffset": 254}, {"referenceID": 39, "context": "We used this intuition, but with more modern embedding models: (1) 300-dimensional Glove word vectors (Pennington et al., 2014), embeddings for a sentence computed by averaging the embeddings of its words (2) Sentence representations obtained from LDA (Blei et al.", "startOffset": 102, "endOffset": 127}, {"referenceID": 5, "context": ", 2014), embeddings for a sentence computed by averaging the embeddings of its words (2) Sentence representations obtained from LDA (Blei et al., 2003) with 300 topics, trained on the Wikipedia dataset using Gibbs sampling.", "startOffset": 132, "endOffset": 151}, {"referenceID": 12, "context": "(5) Foltz et al. (1998) computes the semantic relatedness of two text units as the cosine similarity between their LSA vectors.", "startOffset": 4, "endOffset": 24}, {"referenceID": 20, "context": "The proposed discriminative model significantly outperforms the model presented in Li and Hovy (2014) as well as all non-neural baselines.", "startOffset": 90, "endOffset": 102}, {"referenceID": 13, "context": "The simple LSA method of calculating cosine similarity between adjacent sentences, adopted from Foltz et al. (1998), does not yield competitive results, con-", "startOffset": 96, "endOffset": 116}, {"referenceID": 1, "context": "Since the dataset presented in Barzilay and Lapata (2008) is quite domain-specific, we propose testing coherence with a much larger, open-domain dataset: Wikipedia.", "startOffset": 31, "endOffset": 58}, {"referenceID": 1, "context": "We adopt the same strategy as in Barzilay and Lapata (2008), in which we generate permutations for the original Wikipedia paragraphs.", "startOffset": 33, "endOffset": 60}, {"referenceID": 13, "context": "Baselines Our baselines consist of the Glove and LDA updates of the lexical coherence baselines (Foltz et al., 1998).", "startOffset": 96, "endOffset": 116}, {"referenceID": 1, "context": "We also implement the Entity Grid Model (Barzilay and Lapata, 2008) using the Wikipedia training set.", "startOffset": 40, "endOffset": 67}, {"referenceID": 44, "context": "We use a wikipedia dump parsed using the Fanse Parser (Tratz and Hovy, 2011).", "startOffset": 54, "endOffset": 76}, {"referenceID": 1, "context": "(Barzilay and Lapata, 2008) define two versions of the Entity Grid Model, one using full coreference and a simpler method using only exact-string coreference; Due to the difficulty of running full coreference resolution over 80 million Wikipedia sentences, we follow other researchers in using Barzilay and Lapata\u2019s simpler method (Feng and Hirst, 2012; Burstein et al.", "startOffset": 0, "endOffset": 27}, {"referenceID": 12, "context": "(Barzilay and Lapata, 2008) define two versions of the Entity Grid Model, one using full coreference and a simpler method using only exact-string coreference; Due to the difficulty of running full coreference resolution over 80 million Wikipedia sentences, we follow other researchers in using Barzilay and Lapata\u2019s simpler method (Feng and Hirst, 2012; Burstein et al., 2010; Barzilay and Lapata, 2008).", "startOffset": 331, "endOffset": 403}, {"referenceID": 6, "context": "(Barzilay and Lapata, 2008) define two versions of the Entity Grid Model, one using full coreference and a simpler method using only exact-string coreference; Due to the difficulty of running full coreference resolution over 80 million Wikipedia sentences, we follow other researchers in using Barzilay and Lapata\u2019s simpler method (Feng and Hirst, 2012; Burstein et al., 2010; Barzilay and Lapata, 2008).", "startOffset": 331, "endOffset": 403}, {"referenceID": 1, "context": "(Barzilay and Lapata, 2008) define two versions of the Entity Grid Model, one using full coreference and a simpler method using only exact-string coreference; Due to the difficulty of running full coreference resolution over 80 million Wikipedia sentences, we follow other researchers in using Barzilay and Lapata\u2019s simpler method (Feng and Hirst, 2012; Burstein et al., 2010; Barzilay and Lapata, 2008).", "startOffset": 331, "endOffset": 403}, {"referenceID": 13, "context": "686 Foltz et al. (1998)-Glove 0.", "startOffset": 4, "endOffset": 24}, {"referenceID": 13, "context": "686 Foltz et al. (1998)-Glove 0.597 Foltz et al. (1998)-LDA 0.", "startOffset": 4, "endOffset": 56}, {"referenceID": 13, "context": "Once again, purely lexical methods (Foltz et al., 1998) do not yield compelling results.", "startOffset": 35, "endOffset": 55}, {"referenceID": 1, "context": "By contrast the dataset in Barzilay and Lapata (2008) is very domain-specific, and the semantic space is thus relatively small.", "startOffset": 27, "endOffset": 54}, {"referenceID": 1, "context": "Compared with the dataset in Barzilay and Lapata (2008), overfitting is not an issue here due to the great amount of training data.", "startOffset": 29, "endOffset": 56}, {"referenceID": 23, "context": "We suggest the task of reconstructing an original paragraph from a bag of constituent sentences, which has been previously used in coherence evaluation (Lapata, 2003).", "startOffset": 152, "endOffset": 166}, {"referenceID": 23, "context": "(2) No clue is given: we employed the graph based method described in Lapata (2003). We first construct a graph where the each vertex denotes a sentence and the edge weight u\u2192 v denotes the coherence score of sentence v coming after u.", "startOffset": 70, "endOffset": 84}, {"referenceID": 23, "context": "(2) No clue is given: we employed the graph based method described in Lapata (2003). We first construct a graph where the each vertex denotes a sentence and the edge weight u\u2192 v denotes the coherence score of sentence v coming after u. Note that weight values for u \u2192 v and v \u2192 u are different. We initialize the vertex list V using all vertexes in the graph. Similar to Lapata (2003), we employ a greedy search model.", "startOffset": 70, "endOffset": 385}, {"referenceID": 23, "context": "We therefore use Kendall\u2019s Tau (Lapata, 2003; Lapata, 2006), a metric of rank correlation for evaluation.", "startOffset": 31, "endOffset": 59}, {"referenceID": 24, "context": "We therefore use Kendall\u2019s Tau (Lapata, 2003; Lapata, 2006), a metric of rank correlation for evaluation.", "startOffset": 31, "endOffset": 59}, {"referenceID": 23, "context": "We refer the readers to Lapata (2003) for more details.", "startOffset": 24, "endOffset": 38}, {"referenceID": 36, "context": "In these examples from Miltsakaki and Kukich (2004), the model successfully captures the fact that the second text is less coherent due to rough shifts.", "startOffset": 23, "endOffset": 52}], "year": 2016, "abstractText": "Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. Yet existing models of coherence focus on individual aspects of coherence (lexical overlap, rhetorical structure, entity centering) and are trained on narrow domains. We introduce algorithms that capture diverse kinds of coherence by learning to distinguish coherent from incoherent discourse from vast amounts of opendomain training data. We propose two models, one discriminative and one generative, both using LSTMs as the backbone. The discriminative model treats windows of sentences from original human-generated articles as coherent examples and windows generated by randomly replacing sentences as incoherent examples. The generative model is a SEQ2SEQ model that estimates the probability of generating a sentence given its contexts. Our models achieve state-of-the-art performance on multiple coherence evaluations. Qualitative analysis suggests that our generative model captures many aspects of coherence including lexical, temporal, causal, and entity-based coherence.1", "creator": "TeX"}}}