{"id": "1606.01549", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "Gated-Attention Readers for Text Comprehension", "abstract": "In this paper we study the problem of answering cloze-style questions over short documents. We introduce a new attention mechanism which uses multiplicative interactions between the query embedding and intermediate states of a recurrent neural network reader. This enables the reader to build query-specific representations of tokens in the document which are further used for answer selection. Our model, the Gated-Attention Reader, outperforms all state-of-the-art models on several large-scale benchmark datasets for this task---the CNN \\&amp; Dailymail news stories and Children's Book Test. We also provide a detailed analysis of the performance of our model and several baselines over a subset of questions manually annotated with certain linguistic features. The analysis sheds light on the strengths and weaknesses of several existing models.", "histories": [["v1", "Sun, 5 Jun 2016 19:30:39 GMT  (1796kb,D)", "http://arxiv.org/abs/1606.01549v1", "Under review at EMNLP 2016"], ["v2", "Thu, 1 Dec 2016 19:27:42 GMT  (7419kb,D)", "http://arxiv.org/abs/1606.01549v2", "Under review as a conference paper at ICLR 2017"], ["v3", "Fri, 21 Apr 2017 18:50:05 GMT  (7428kb,D)", "http://arxiv.org/abs/1606.01549v3", "Accepted at ACL 2017"]], "COMMENTS": "Under review at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["bhuwan dhingra", "hanxiao liu", "zhilin yang", "william w cohen", "ruslan salakhutdinov"], "accepted": true, "id": "1606.01549"}, "pdf": {"name": "1606.01549.pdf", "metadata": {"source": "CRF", "title": "Gated-Attention Readers for Text Comprehension", "authors": ["Bhuwan Dhingra", "Hanxiao Liu", "William W. Cohen", "Ruslan Salakhutdinov"], "emails": ["rsalakhu}@cs.cmu.edu"], "sections": [{"heading": null, "text": "We introduce a new attention mechanism that leverages multiplicative interactions between the embedding of the query and the intermediate states of a recurring neural network reader, allowing the reader to create query-specific representations of tokens in the document, which are then used to select the answers. Our model, the Gated Attention Reader, outperforms all current models on several large benchmark datasets for this task - CNN & Daily Mail News Stories and Children's Book Test - and provides a detailed analysis of the performance of our model and multiple baselines across a subset of hand-annotated questions with specific linguistic characteristics, highlighting the strengths and weaknesses of several existing models."}, {"heading": "1 Introduction", "text": "A current trend toward measuring progress toward machine reading is testing a system's ability to answer questions about the text it needs to comprehend. To this end, several large-format data sets on clozestyle questions about a context document have recently been introduced that allow for the formation of supervised machine learning systems (Hermann et al., 2015). The relatively unambiguous nature of the answers to clozestyle questions provides an objective yardstick for measuring a system's performance in understanding text., BD and HL contributed equally to this work. Deep learning methods have recently been demonstrated to surpass traditional shallow approaches to this task (Hermann et al., 2015). Their performance is powered by attention mechanisms borrowed from machine translation literature."}, {"heading": "2 Related Work", "text": "The Cloze-style QA task comprises tuples of the form (d, q, a), where d is a document, q is a cloze-style question about the content of that document, and a is the answer to that query. The answer comes from a fixed vocabulary A, which, depending on the data set, can consist of all the words in the vocabulary or a list of candidates from the current document. The task can then be described as follows: In a pair of documentation (d, q), you will find an \"A,\" the q.Below, we give an overview of existing neural architectures that have been applied to this problem with encouraging results."}, {"heading": "2.1 LSTMs with Attention", "text": "Several architectures introduced in (Hermann et al., 2015) use LSTM units to calculate a combined document with a query representation g (d, q). Given this representation and a query table of possible responses W (shared with the query table used by the LSTM), the probability of a response is calculated as: p (a | q), exp (W (a) g (d, q)), a). The motivation behind (1) is that g (d, q) should only contain information relevant to the task at hand - namely, to obtain the answer a and be close to the correct answer in the vector space, maximizing the inner product between these representations. Training can be performed using standard SGD on a loss function, such as cross-sectional diagnosis between true answer and predicted probability. With this formulation, there are three related architectures proposed in (Hermann et al, 2015)."}, {"heading": "2.2 Memory Networks", "text": "The storage network architecture has been proposed in (Weston et al., 2014) and applied to the data sets considered here in (Hill et al., 2015). In MemNets, each sentence in the input document is encoded to memory m. This encoding uses a simple averaging of the vectors associated with a window within the set around candidate's answers. In accordance with conventional attention, the relevance of a particular memory mi for the query is calculated by taking its dot product with the query vector q and guiding it through a Softmax layer to obtain a distribution across all memories, except for that the total memory in this first pass that is relevant to the query is then mo1 = \u2211 i \u03b1imi. This process repeats itself for several iterations, and each time the input query qk = qk \u2212 1 + mok \u2212 1 is applied, with q0 = q.The intuition behind this calculation is that we can select multiple memories to form this module query, each of which is interactive over a set of memories."}, {"heading": "2.3 Attention Sum Reader", "text": "The AS reader, represented in (Kadlec et al., 2016), receives the current state-of-the-art performance on the data sets considered here. This is a simplified version of the DeepLSTM reader, which uses a bi-directional GRU network (Cho et al., 2014) to encode the query and document each one in vectors. Subsequently, the responses of the candidates appearing in the document are evaluated by extracting their contextual representations from the bi-GRU and taking a dot product with the query vector q, resulting in a score for each possible response that is normalized by the Softmax function. Finally, multiple mentions of the same candidate unit are combined by adding their probabilities, and the token with the maximum aggregate probability is selected as the predicted response. This aggregation scheme is known as the time sum sum attention."}, {"heading": "2.4 Dynamic Entity Representations", "text": "The Dynamic Entity Representation (DER) network was recently introduced by (Kobayashi et al., 2016), which builds dynamic representations of candidates \"responses (in their case, entities) while reading the document, using a max pooling function to collect the information previously collected through an entity. Motivated by the anonymization of entities in the CNN and Daily Mail records, the model shows improved performance compared to previous approaches in the CNN dataset. Results from the Daily Mail and CBT datasets are not shown. In our model, we do not collect information about candidates\" responses when reading, but instead dynamically align their representations between levels to \"mask\" less informative features in their representation and retain the important ones."}, {"heading": "3 Gated-Attention Reader", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Motivation", "text": "Our method expands the attention sum (AS) reader and performs multiple jumps over the input document, similar to the memory network architecture (Sukhbaatar et al., 2015). We use the same attention mechanism with the pointer sum as the AS reader at the output level to obtain a distribution of the candidates \"responses. End-to-end memory networks have shown that they perform well even for synthetic QA tasks that require consideration of multiple input sets. However, a major reason for this is the multi-layered architecture that performs multiple transitions across the context (Sukhbaatar et al., 2015). Based on these results, we also include several layers in our model. However, our key contribution is a mechanism of weighted attention in which, after the first layer, attributes about the document through the query representation using an elemental combination of element inputs (Figure 1) and multiplication on layer inputs (1)."}, {"heading": "3.2 Model Details", "text": "Figure 1 illustrates the gated attention (GA) reader. Each field with the designation Bi-GRU = q = q = a = a = a = a = a = a = a = a = a = a = b = a = a = b = a = a = a = a = a = a = b = a = a = a = a = a = a = a = a = a = a = a = a = a = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = a = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = a = a = a = a = a = a = a = a = a = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = a = b = b = b = b = b = b = b = b = b = b = b = b = a = b = b = b = b = b = b = b = a = b = b = b = b = b = b = b = b = a = b = b = b = b = b = a = b = b = b = b = b = b = b = b = b = a = b = b = b = b = b = b = b = a = b = b = b = b = b = b = a = b = b = a = b = b = b = a = b = b = a = b = b = b = b = b = b = a = b = a = a = a = a = = b = a = b = b = b = b = b = b = a = b = a = a = a = a = b = b = a = a = a = a = a = b = b = a = a = b = a = a = a = a = a = a = a = a = a = a = a = = a = ="}, {"heading": "4 Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "We evaluate the GA reader using four sets of data. The first two, CNN and Daily Mail News Stories1, are from (Hermann et al., 2015) and consist of news articles from the popular websites CNN and Daily Mail. A query about each article was formed by removing an entity from the short summary that follows the article. Furthermore, the entities within each article were anonymized to make the task purely comprehensible. N-gram statistics, for example, which are calculated over the entire corpus, are no longer useful in such an anonymized corpus. The other two sets of data consist of two different subsets of the Children's Book Test (CBT) 2 (Hill et al., 2015). Documents consist of 20 connected sentences from the body of a popular children's book, and the query is formed by deleting one token from the 21st sentence."}, {"heading": "4.2 Implementation Details", "text": "Our model was implemented using the Python libraries Theano3 and Lasagne4. We used Stochastic Gradient Descent with ADAM updates for training combining classic dynamics and adaptive gradients (Kingma and Ba, 2014); the learning rate was set to 0.0005 and the lot size for each iteration to 32 for all models & datasets; we used gradient clipping with a threshold of 10 to stabilize GRU training (Pascanu et al., 2012); text embedding was set to size 128 for all models & datasets, initialized with vectors obtained by executing the word2vec toolkit5 on all documents and queries in the training set; we found empirically that this approach led to faster convergence; all other parameters were initialized to their default values as set in the lasagne library; three parameters were matched to the validation of each dataset = 25K, although the number of each dataset = 4)."}, {"heading": "4.3 Performance Comparison", "text": "Table 3 shows the performance of all the data sets discussed above on CNN, Daily Mail and CBT. Following the discontinuation of previous work, we compare both the individual best models and their ensembles. For the GA reader, the best M trained3http: / / deeplearning.net / software / theano / 4https: / / lasagne.readthedocs.io / en / latest / 5https: / / code.google.com / archive / p / word2vec / models on each data set were selected as ensemble members, with M matched to the validation set. The individual best GA readers surpass all the individual models published so far on all four data sets. Meanwhile, the combination of several GA readers results in the new state-of-the-art ensemble result on all four data sets. The improvement of the individual models is greater for Daily Mail and CNN data sets than for the CBT sets, with the CBT reader being vulnerable to the overhaul. The CBT data sets are significantly smaller for all four data sets, but the CBT data sets are smaller for both Daily Mail and CNN data sets."}, {"heading": "4.4 Visualizing Attention", "text": "Figure 2 shows two sample questions from the CNN test set with an overlay heatmap that shows attention at the output level of the GA reader: Attention is calculated as a value of st in Equation (5) for each character in the document and visualized only at st > 0.05. Both questions require the system to understand paraphrases and reason across multiple evidence object sets to arrive at the correct answer, in both cases the GA reader manages to look for the correct characters."}, {"heading": "5 Detailed Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Linguistic Feature Annotation", "text": "In order to gain insights into the behavior of neural readers at a more detailed level, we randomly selected 100 questions from CNN's test set and manually commented on their linguistic traits. 6 For each question, traits were extracted that could potentially correlate with the performance of neural readers, as shown in Table 4. In this paper, we primarily focus on two types of linguistic traits: \u2022 Simple traits that are automatically commented by scripts, such as document / query length, response frequency, etc. Most of them are simple meta information about the question. \u2022 Semantic traits that are commented manually by a human expert. For example, whether logical thinking is necessary to get the right answer. Characteristics of this kind are probably multiply.6We plan to make the commented questions publicly available if the manuscript is accepted for publication. Precise indicators of the intrinsic semantic nature of a given question."}, {"heading": "5.2 Conditional Performance", "text": "In this subsection, we examine the performance of several representative models based on a subset of questions with specific semantic characteristics. As we discussed above, the division of general accuracy into several conditional performances allows us a more comprehensive overview of the strength and weakness of different architectures. For comparison, four models are selected: (1) the word distance benchmark (WD), a simple but strong baseline based on word spacing measurements; (2) the Deep LSTM reader, a representative, LSTM-based neural architecture (Hermann et al., 2015); (3) the Attention Sum (AS) reader, which achieves the most advanced performance through multiple text comprehension tasks (Kadlec et al., 2016); (4) the GA reader, our proposed model. The results are presented in Figure 4, where the conditional performance of the GA reader dominates other baselines except for temporal thinking and exhibits a notable strength in dealing with all the sensitivity of the WAS reader over the whole spectrum of the WAS characteristics, while the WAS reader exhibits a significant sensitivity to all the weaknesses of the models."}, {"heading": "5.3 Sensitivity Analysis", "text": "With the linguistic characteristics in Table 4, each of the 100 sample CNN questions can be summarized as an 11-dimensional vector x (10 features plus an intercept). If the question has been correctly answered, let y = 1, and y = 0 otherwise. Therefore, the sensitivity of the reader's neural performance can be quantified by adjusting a linear regression model in which x and y are treated as explanatory or response variables, and the resulting regression coefficients and p values can effectively summarize the characteristics that influence model performance.7Prior to regression, all characteristics were normalized to the range [0, 1] in order to make their scales comparable.Figure 5 shows the regression coefficients associated with different linguistic characteristics, in Table 4. In addition to the known effect of DL and AF discussed in the literature, we show the involvement of logical thinking (LR), paraphrase (multiple emprical) questions (ES) are related to different architectural (multiple) characteristics (ES)."}, {"heading": "6 Conclusion", "text": "We presented the Cloze-style Gated Attention Reader via Context Documents. The model features a novel multiplicative gating mechanism and combines the benefits of both point-sum attention and multi-layered architectures of storage networks. Our model design was justified by its state-of-the-art performance across multiple large benchmark datasets under both single-best and ensemble settings, and underpinned by detailed performance analysis of questions with multiple linguistic features."}, {"heading": "Acknowledgments", "text": "The authors thank Eduard Hovy and Teruko Mitamura for useful discussions and feedback."}], "references": [{"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Holger Schwenk", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Mustafa Suleyman", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay"], "venue": "and Phil Blunsom.", "citeRegEx": "Hermann et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sumit Chopra", "author": ["Felix Hill", "Antoine Bordes"], "venue": "and Jason Weston.", "citeRegEx": "Hill et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Ondrej Bajgar", "author": ["Rudolf Kadlec", "Martin Schmid"], "venue": "and Jan Kleindienst.", "citeRegEx": "Kadlec et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Richard Zemel", "author": ["Ryan Kiros"], "venue": "and Ruslan R Salakhutdinov.", "citeRegEx": "Kiros et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Naoaki Okazaki", "author": ["Sosuke Kobayashi", "Ran Tian"], "venue": "and Kentaro Inui.", "citeRegEx": "Kobayashi et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Lapata2008] Jeff Mitchell", "Mirella Lapata"], "venue": "In ACL,", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Tomas Mikolov", "author": ["Razvan Pascanu"], "venue": "and Yoshua Bengio.", "citeRegEx": "Pascanu et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Rob Fergus", "author": ["Sainbayar Sukhbaatar", "Jason Weston"], "venue": "et al.", "citeRegEx": "Sukhbaatar et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sumit Chopra", "author": ["Jason Weston"], "venue": "and Antoine Bordes.", "citeRegEx": "Weston et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Jianfeng Gao", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He"], "venue": "and Li Deng.", "citeRegEx": "Yang et al.2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "In this paper we study the problem of answering cloze-style questions over short documents. We introduce a new attention mechanism which uses multiplicative interactions between the query embedding and intermediate states of a recurrent neural network reader. This enables the reader to build query-specific representations of tokens in the document which are further used for answer selection. Our model, the Gated-Attention Reader, outperforms all state-of-the-art models on several large-scale benchmark datasets for this task\u2014 the CNN & Daily Mail news stories and Children\u2019s Book Test. We also provide a detailed analysis of the performance of our model and several baselines over a subset of questions manually annotated with certain linguistic features. The analysis sheds light on the strengths and weaknesses of several existing models.", "creator": "LaTeX with hyperref package"}}}