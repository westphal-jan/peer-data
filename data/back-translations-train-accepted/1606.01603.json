{"id": "1606.01603", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution", "abstract": "Most existing approaches for zero pronoun resolution are supervised approaches, where annotated data are released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in zero pronoun resolution task. The existing approaches mainly face the challenge of costing manpower on labeling the extended data for better training performance and domain adaption. To alleviate the problem above, in this paper we propose a simple but novel approach to automatically produce large-scale pseudo training data for zero pronoun resolution. Furthermore, to avoid the drawbacks of the feature engineering based approaches, we proposed an attention-based LSTM model for this task. Experimental results show that our proposed approach outperforms the state-of-the-art methods significantly with an absolute improvement of 5.1% F-score in OntoNotes 5.0 corpus.", "histories": [["v1", "Mon, 6 Jun 2016 02:45:47 GMT  (534kb,D)", "https://arxiv.org/abs/1606.01603v1", null], ["v2", "Fri, 17 Jun 2016 09:18:35 GMT  (267kb,D)", "http://arxiv.org/abs/1606.01603v2", "10pages, under review at EMNLP2016, some typos have been fixed"], ["v3", "Tue, 6 Jun 2017 02:47:41 GMT  (249kb,D)", "http://arxiv.org/abs/1606.01603v3", "8+2 pages, published as a conference paper at ACL2017 (long paper)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ting liu", "yiming cui", "qingyu yin", "weinan zhang", "shijin wang", "guoping hu"], "accepted": true, "id": "1606.01603"}, "pdf": {"name": "1606.01603.pdf", "metadata": {"source": "CRF", "title": "Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution", "authors": ["Ting Liu", "Yiming Cui", "Qingyu Yin", "Weinan Zhang", "Shijin Wang", "Guoping Hu"], "emails": ["tliu@ir.hit.edu.cn", "qyyin@ir.hit.edu.cn", "wnzhang@ir.hit.edu.cn", "ymcui@iflytek.com", "sjwang3@iflytek.com", "gphu@iflytek.com"], "sections": [{"heading": "1 Introduction", "text": "\"What we have to do,\" the presiding judge said, \"is that we will be able to find a solution that is capable of finding a solution that enables us to find a solution that is capable of finding a solution that enables us to find a solution that is capable of finding a solution that is capable of finding a solution that enables us to find a solution that enables us to find a solution that is capable of finding a solution and that enables us to find a solution that we are able to find a solution.\""}, {"heading": "2 The Proposed Approach", "text": "In this section, we will describe our approach in detail: first, we will describe our method of generating large-scale pseudo-training data for the resolution of null pronouns; then, we will introduce a two-step training approach to reduce the gaps between pseudo-training and real training data; finally, we will describe the attention-based neural network model and associated unknown word processing techniques."}, {"heading": "2.1 Generating Pseudo Training Data", "text": "In order to obtain large amounts of training data for the neural network model, we propose an approach inspired by (Hermann et al., 2015) to automatically generate large-scale pseudo-training data for zero pronoun resolution. However, our approach is much simpler and more general than that of (Hermann et al., 2015). We will present the details of generating pseudo-training data for zero pronoun resolution as follows. First, we collect a large number of documents that are relevant (or in some way homogeneous) to the published OntoNote 5.0 data for zero pronoun resolution task. In our experiments, we use large-scale message data for training. In the face of a particular document D, which is composed of a set of sentences D = {s1, s2,..., sn}, we randomly choose an answer wordA in the document."}, {"heading": "2.2 Two-step Training", "text": "It should be noted that although we have generated extensive pseudo-training data for the training of neural networks, there is still a gap between pseudo-training data and the real task of resolving pronouns in terms of query style. So we should make some adjustments to our model to ideally address the problems of resolving pronouns without pronouns. In this essay, we used an effective approach to deal with the discrepancy between pseudo-training data and task-specific data for resolving pronouns without pronouns. In general, in the first stage, we use a large amount of pseudo-training data to train a basic model and select the best model according to validation accuracy. Then, we train further from the previous best model using task-specific data to solve non-pronouns tasks, which is exactly the same range and query type as the standard zero-resolution task."}, {"heading": "2.3 Attention-based Neural Network Model", "text": "Our model is primarily an attention-based neural network model similar to the Attentive Reader by (Hermann et al., 2015). Formally, if we have given a series of training programs with three < D, Q, A >, we will construct our network in the following ways: First, we project a unified representation of document D and the query Q into a continuous space with the common embedding matrix. Then, we enter these embedding into different bidirectional RNN to obtain their contextual representations. In our model, we used the bi-directional gated recurrent unit (GRU) as the RNN implementation. (Cho et al., 2014).e (x) = We embed these embedding into different bidirectional RNN to obtain their contextual representations. In our model, we used the bidirectional gated recurrent unimention (NGRU) as the implemention."}, {"heading": "2.4 Unknown Words Processing", "text": "Due to the limitation of both memory allocation and training time, it is usually suggested to use a shortlist of vocabulary in neural network training. However, we often replace the non-existent vocabulary with a unique special character, such as < unk >. But this can be an obstacle in the real world. If the model predicts the answer as < unk >, we do not know what the exact word it represents in the document is, as it may have many < unk > s in the documentation. In this paper, we propose to use a simple but effective way to handle unknown words. The idea is simple, which can be illustrated as follows. \u2022 Identify all unknown words within each < D, Q, A >; \u2022 Instead of turning all these unknown words into a unique character, we make a hash table to project these unique unknown words."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Data", "text": "In our experiments, we select a selection of public message data to generate large-scale pseudo-training data for the pre-training of our neural network model (Pre-Training Step) 1. In the adaptation step, we used the official OntoNotes Release 5.02 dataset provided by CoNLL-2012 shared task to perform our experiments.The CoNL2012 task dataset consists of three parts: a training set, a development set, and a test set. The datasets are composed of 6 different domains, namely Broadcast News (BN), Newswires (NW), Broadcast Conversations (BC), Telephone Conversations (TC), Web Blogs (WB), and Magazines (MZ).We closely follow the experimental settings such as (Kong and Zhou, 2010; Chen and Ng, 2014, 2015, 2016), where we cover the training set and the development set for the tests, as the training data and the training sets are presented in the 1."}, {"heading": "3.2 Neural Network Setups", "text": "The training details of our neural network models are listed as follows: 1The news data is available at http: / / www.sogou. com / labs / dl / cs.html2http: / / catalog.ldc.upenn.edu / LDC2013T19Docs Sentences Words AZPsTest 172 6,083 110K 1.713All models are trained on the Tesla K40 GPU. Our model is implemented with Theano (Theano Development Team, 2016) and Keras (Chollet, 2015)."}, {"heading": "3.3 Experimental results", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "4 Error Analysis", "text": "In order to better evaluate our proposed approach, we conducted a qualitative error analysis in which two major errors were uncovered by our analysis, as explained below."}, {"heading": "4.1 Effect of Unknown Words", "text": "Our approach does not fare well when there are many < unk > s in the context of ZPs, especially when the < unk > s appear near the ZP. An example is given below, where words with # are considered < unk > s in our model to have a panoramic view of the beauty of [Hong Kong Island] # and [Victoria Harbour] #. In this case, the words \"climbed\" and \"climbed\" that appear immediately after the ZP complicate the semantic information of the sentence, which in turn affects overall performance. As we model the word sequence of RNN, the < unk > s make the model more difficult to capture the semantic information of the sentence, which in turn affects the word performance, especially for words that have great significance for understanding when the ZP method is used to recognize the information, it is difficult to understand it."}, {"heading": "4.2 Long Distance Antecedents", "text": "In addition, our model makes wrong decisions when the correct ancestors of ZP are far away. As our model selects the answer from words in context, when there are many words between the ZP and its ancestor, more sound information is introduced, and it becomes more difficult to select the right answer. For example:... If we look at the relationships, we find that there are more than 30 words between the ZP and its ancestor. Although our model does not intend to fill the ZP gap only with the words near the ZP, since most of the ancestors appear only a few words before the ZP, our model prefers the closer words than correct ancestors. So, if there are many words between the ZP and its next ancestor, our ancestors should sometimes make the wrong decisions."}, {"heading": "5 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Zero pronoun resolution", "text": "Converse (2006) proposes a rules-based method to solve the null pronouns by using the Hobbs algorithm (Hobbs, 1978) in the CTB documents, followed by extensive research into supervised approaches to this task. Zhao and Ng (2007) first present a supervised machine learning approach to the identification and resolution of Chinese ZPs. Kong and Zhou (2010) develop a tree kernel-based approach to Chinese ZP resolution. More recently, uncontrolled approaches have been proposed. Chen and Ng (2014) develop an uncontrolled language-independent approach by using holistic linear programming to use ten open pronouns. Chen and Ng (2015) propose an overarching solution to Chinese ZP resolution."}, {"heading": "5.2 Cloze-style Reading Comprehension", "text": "Our neural network model is mainly motivated by recent research on the tasks of Cloze-style reading comprehension, which aims to predict a one-word response based on the document and the query. These models can be seen as a general model for exploring the relationship between document and query, so it is promising to combine these models for the specific area. A representative piece of Cloze-style reading comprehension work is being done by Hermann et al. (2015), who proposed a methodology for obtaining large quantities < D, Q, A > Tripel. Using this method, a large amount of training data can be obtained without much human intervention, enabling the formation of a reliable neural network. They used attention-based neural networks for this task. Analyses on CNN / DailyMail datasets showed that their approach is much more effective than traditional base systems. While our work is similar to Hermann et al's (2015), there are several differences that can be seen."}, {"heading": "6 Conclusion", "text": "In this study, we propose an effective method of generating and utilizing large-scale pseudo-training data for pronoun resolution tasks, the basic idea of which is to automatically generate large-scale pseudo-training data and then use an attention-based neural network model to resolve pronouns. For training purposes, a two-step training approach is used, i.e. a preliminary and adaptive step that can easily be applied to other tasks. Experimental results on OntoNote's 5.0 corpus are encouraging and show that the proposed model and accompanying approaches significantly exceed the state of the art. Future work will focus on two main aspects: firstly, since experimental results show that processing unknown words is a critical part of understanding the context, we will explore more effective ways to address the UNK problem; secondly, we will make other neural network architectures more suitable for resolving pronouns."}, {"heading": "Acknowledgements", "text": "We would like to thank the anonymous reviewers for their thorough review and thoughtful comments to improve our work, which has been supported by the National 863 Leading Technology Research Project on funding 2015AA015407, Key Projects of National Natural Science Foundation of China on funding 61632011 and the National Natural Science Youth Foundation of China on funding 61502120."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Ltp: A chinese language technology platform", "author": ["Wanxiang Che", "Zhenghua Li", "Ting Liu."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations. Association for Computational Linguistics, pages 13\u201316.", "citeRegEx": "Che et al\\.,? 2010", "shortCiteRegEx": "Che et al\\.", "year": 2010}, {"title": "Chinese zero pronoun resolution: Some recent advances", "author": ["Chen Chen", "Vincent Ng."], "venue": "EMNLP. pages 1360\u20131365.", "citeRegEx": "Chen and Ng.,? 2013", "shortCiteRegEx": "Chen and Ng.", "year": 2013}, {"title": "Chinese zero pronoun resolution: An unsupervised approach combining ranking and integer linear programming", "author": ["Chen Chen", "Vincent Ng."], "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Chen and Ng.,? 2014", "shortCiteRegEx": "Chen and Ng.", "year": 2014}, {"title": "Chinese zero pronoun resolution: A joint unsupervised discourseaware model rivaling state-of-the-art resolvers", "author": ["Chen Chen", "Vincent Ng."], "venue": "Proceedings of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natu-", "citeRegEx": "Chen and Ng.,? 2015", "shortCiteRegEx": "Chen and Ng.", "year": 2015}, {"title": "Chinese zero pronoun resolution with deep neural networks", "author": ["Chen Chen", "Vincent Ng."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics,", "citeRegEx": "Chen and Ng.,? 2016", "shortCiteRegEx": "Chen and Ng.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Pronominal anaphora resolution in chinese", "author": ["Susan P Converse"], "venue": null, "citeRegEx": "Converse.,? \\Q2006\\E", "shortCiteRegEx": "Converse.", "year": 2006}, {"title": "A computational approach to zero-pronouns in spanish", "author": ["Antonio Ferr\u00e1ndez", "Jes\u00fas Peral."], "venue": "Proceedings of the 38th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 166\u2013172.", "citeRegEx": "Ferr\u00e1ndez and Peral.,? 2000", "shortCiteRegEx": "Ferr\u00e1ndez and Peral.", "year": 2000}, {"title": "Korean zero pronouns: analysis and resolution", "author": ["Na-Rae Han."], "venue": "Ph.D. thesis, Citeseer.", "citeRegEx": "Han.,? 2006", "shortCiteRegEx": "Han.", "year": 2006}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1684\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Resolving pronoun references", "author": ["Jerry R Hobbs."], "venue": "Lingua 44(4):311\u2013338.", "citeRegEx": "Hobbs.,? 1978", "shortCiteRegEx": "Hobbs.", "year": 1978}, {"title": "Exploiting syntactic patterns as clues in zero-anaphora resolution", "author": ["Ryu Iida", "Kentaro Inui", "Yuji Matsumoto."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Compu-", "citeRegEx": "Iida et al\\.,? 2006", "shortCiteRegEx": "Iida et al\\.", "year": 2006}, {"title": "Zero-anaphora resolution by learning rich syntactic pattern features", "author": ["Ryu Iida", "Kentaro Inui", "Yuji Matsumoto."], "venue": "ACM Transactions on Asian Language Information Processing (TALIP) 6(4):1.", "citeRegEx": "Iida et al\\.,? 2007", "shortCiteRegEx": "Iida et al\\.", "year": 2007}, {"title": "A cross-lingual ilp solution to zero anaphora resolution", "author": ["Ryu Iida", "Massimo Poesio."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computa-", "citeRegEx": "Iida and Poesio.,? 2011", "shortCiteRegEx": "Iida and Poesio.", "year": 2011}, {"title": "Japanese zero pronoun resolution based on ranking rules and machine learning", "author": ["Hideki Isozaki", "Tsutomu Hirao."], "venue": "Proceedings of the 2003 conference on Empirical methods in natural language processing. Association for Computational Linguis-", "citeRegEx": "Isozaki and Hirao.,? 2003", "shortCiteRegEx": "Isozaki and Hirao.", "year": 2003}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "A tree kernelbased unified framework for chinese zero anaphora resolution", "author": ["Fang Kong", "Guodong Zhou."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,", "citeRegEx": "Kong and Zhou.,? 2010", "shortCiteRegEx": "Kong and Zhou.", "year": 2010}, {"title": "Semeval-2010 task 1: Coreference resolution in multiple languages", "author": [], "venue": null, "citeRegEx": "Recasens.,? \\Q2010\\E", "shortCiteRegEx": "Recasens.", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes", "author": [], "venue": null, "citeRegEx": "Pradhan.,? \\Q2012\\E", "shortCiteRegEx": "Pradhan.", "year": 2012}, {"title": "A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames", "author": ["Ryohei Sasano", "Sadao Kurohashi."], "venue": "IJCNLP. pages 758\u2013766.", "citeRegEx": "Sasano and Kurohashi.,? 2011", "shortCiteRegEx": "Sasano and Kurohashi.", "year": 2011}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli."], "venue": "arXiv preprint arXiv:1312.6120 .", "citeRegEx": "Saxe et al\\.,? 2013", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints abs/1605.02688. http://arxiv.org/abs/1605.02688.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Identification and resolution of chinese zero pronouns: A machine learning approach", "author": ["Shanheng Zhao", "Hwee Tou Ng."], "venue": "EMNLP-CoNLL. volume 2007, pages 541\u2013550.", "citeRegEx": "Zhao and Ng.,? 2007", "shortCiteRegEx": "Zhao and Ng.", "year": 2007}], "referenceMentions": [{"referenceID": 10, "context": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).", "startOffset": 100, "endOffset": 212}, {"referenceID": 25, "context": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).", "startOffset": 100, "endOffset": 212}, {"referenceID": 14, "context": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).", "startOffset": 100, "endOffset": 212}, {"referenceID": 18, "context": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).", "startOffset": 100, "endOffset": 212}, {"referenceID": 15, "context": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).", "startOffset": 100, "endOffset": 212}, {"referenceID": 2, "context": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).", "startOffset": 100, "endOffset": 212}, {"referenceID": 11, "context": "In order to get large quantities of training data for neural network model, we propose an approach, which is inspired by (Hermann et al., 2015), to automatically generate large-scale pseudo training data for zero pronoun resolution.", "startOffset": 121, "endOffset": 143}, {"referenceID": 11, "context": "However, our approach is much more simple and general than that of (Hermann et al., 2015).", "startOffset": 67, "endOffset": 89}, {"referenceID": 1, "context": "Note that, we restrictA to be either a noun or pronoun, where the part-of-speech is identified using LTP Toolkit (Che et al., 2010), as well as the answer word should appear at least twice in the document.", "startOffset": 113, "endOffset": 131}, {"referenceID": 11, "context": "Our model is primarily an attention-based neural network model, which is similar to Attentive Reader proposed by (Hermann et al., 2015).", "startOffset": 113, "endOffset": 135}, {"referenceID": 6, "context": "In our model, we used the bidirectional Gated Recurrent Unit (GRU) as RNN implementation (Cho et al., 2014).", "startOffset": 89, "endOffset": 107}, {"referenceID": 0, "context": "For the document, we place a soft attention over all words in document (Bahdanau et al., 2014), which indicate the degree to which part of document is attended when filling the blank in the query sentence.", "startOffset": 71, "endOffset": 94}, {"referenceID": 4, "context": "First, for a given zero pronoun, we extract a set of NPs as its candidates utilizing the same strategy as (Chen and Ng, 2015).", "startOffset": 106, "endOffset": 125}, {"referenceID": 18, "context": "We closely follow the experimental settings as (Kong and Zhou, 2010; Chen and Ng, 2014, 2015, 2016), where we treat the training set for training and the development set for testing, because only the training and development set are annotated with ZPs.", "startOffset": 47, "endOffset": 99}, {"referenceID": 23, "context": "\u2022 Hidden Layer: We use GRU with 256 units, and initialize the internal matrix by random orthogonal matrices (Saxe et al., 2013).", "startOffset": 108, "endOffset": 127}, {"referenceID": 17, "context": "\u2022 Optimization: We used ADAM update rule (Kingma and Ba, 2014) with an initial learning rate of 0.", "startOffset": 41, "endOffset": 62}, {"referenceID": 7, "context": "Our model is implemented with Theano (Theano Development Team, 2016) and Keras (Chollet, 2015).", "startOffset": 79, "endOffset": 94}, {"referenceID": 2, "context": "9 Chen and Ng (2014) 38.", "startOffset": 2, "endOffset": 21}, {"referenceID": 2, "context": "9 Chen and Ng (2014) 38.1 31.0 50.4 45.9 53.8 54.9 48.7 Chen and Ng (2015) 46.", "startOffset": 2, "endOffset": 75}, {"referenceID": 2, "context": "9 Chen and Ng (2014) 38.1 31.0 50.4 45.9 53.8 54.9 48.7 Chen and Ng (2015) 46.4 39.0 51.8 53.8 49.4 52.7 50.2 Chen and Ng (2016) 48.", "startOffset": 2, "endOffset": 129}, {"referenceID": 5, "context": "see that our model significantly outperforms the previous state-of-the-art system (Chen and Ng, 2016) by 3.", "startOffset": 82, "endOffset": 101}, {"referenceID": 20, "context": "So we use the large-scale pseudo training data for embedding training using GloVe toolkit (Pennington et al., 2014), and initialize the word embeddings in the \u201conly task-specific data\u201d system.", "startOffset": 90, "endOffset": 115}, {"referenceID": 12, "context": "Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents.", "startOffset": 104, "endOffset": 117}, {"referenceID": 8, "context": "Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents.", "startOffset": 0, "endOffset": 16}, {"referenceID": 8, "context": "Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents. Then, supervised approaches to this task have been vastly explored. Zhao and Ng (2007) first present a supervised machine learning approach to the identification and resolution of Chinese ZPs.", "startOffset": 0, "endOffset": 227}, {"referenceID": 8, "context": "Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents. Then, supervised approaches to this task have been vastly explored. Zhao and Ng (2007) first present a supervised machine learning approach to the identification and resolution of Chinese ZPs. Kong and Zhou (2010) develop a tree-kernel based approach for Chinese ZP resolution.", "startOffset": 0, "endOffset": 354}, {"referenceID": 10, "context": "Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al.", "startOffset": 80, "endOffset": 91}, {"referenceID": 16, "context": "Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011).", "startOffset": 105, "endOffset": 183}, {"referenceID": 22, "context": "Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011).", "startOffset": 105, "endOffset": 183}, {"referenceID": 2, "context": "Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information.", "startOffset": 0, "endOffset": 163}, {"referenceID": 2, "context": "Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning approaches. Ferr\u00e1ndez and Peral (2000) proposed a set of hand-crafted rules for Spanish ZP resolution.", "startOffset": 0, "endOffset": 489}, {"referenceID": 2, "context": "Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning approaches. Ferr\u00e1ndez and Peral (2000) proposed a set of hand-crafted rules for Spanish ZP resolution. Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011). Iida and Poesio (2011) developed a cross-lingual approach for Japanese and Italian ZPs where an ILPbased model was employed to zero anaphora detection and resolution.", "startOffset": 0, "endOffset": 761}, {"referenceID": 3, "context": "Even for the unsupervised approach (Chen and Ng, 2014), they also utilize a supervised pronoun resolver to resolve ZPs.", "startOffset": 35, "endOffset": 54}, {"referenceID": 10, "context": "A representative work of cloze-style reading comprehension is done by Hermann et al. (2015). They proposed a methodology for obtaining large quantities of \u3008D,Q,A\u3009 triples.", "startOffset": 70, "endOffset": 92}, {"referenceID": 11, "context": "While our work is similar to Hermann et al. (2015), there are several differences which can be illustrated as follows.", "startOffset": 29, "endOffset": 51}], "year": 2017, "abstractText": "Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task. Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.", "creator": "LaTeX with hyperref package"}}}