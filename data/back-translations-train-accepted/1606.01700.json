{"id": "1606.01700", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "Gated Word-Character Recurrent Language Model", "abstract": "We introduce a recurrent neural network language model (RNN-LM) with long short-term memory (LSTM) units that utilizes both character-level and word-level inputs. Our model has a gate that adaptively finds the optimal mixture of the character-level and word-level inputs. The gate creates the final vector representation of a word by combining two distinct representations of the word. The character-level inputs are converted into vector representations of words using a bidirectional LSTM. The word-level inputs are projected into another high-dimensional space by a word lookup table. The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words. Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-of-vocabulary words and outperforms word-level language models on several English corpora.", "histories": [["v1", "Mon, 6 Jun 2016 11:43:28 GMT  (424kb,D)", "http://arxiv.org/abs/1606.01700v1", null], ["v2", "Thu, 13 Oct 2016 03:26:43 GMT  (765kb,D)", "http://arxiv.org/abs/1606.01700v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yasumasa miyamoto", "kyunghyun cho"], "accepted": true, "id": "1606.01700"}, "pdf": {"name": "1606.01700.pdf", "metadata": {"source": "CRF", "title": "Gated Word-Character Recurrent Language Model", "authors": ["Yasumasa Miyamoto", "Kyunghyun Cho"], "emails": ["yasumasa.miyamoto@nyu.edu", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, the words are words that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are based on word levels, that are calculated at the source level."}, {"heading": "2 Model Description", "text": "The model architecture of the proposed word- and hrwt-R model is the last hidden element of forward and backward recurrent networks: xcharwt = W fhfwt + W rhrwt + b. (2) The model architecture of the proposed word- and hrwt-R model is the last hidden element of forward and backward recurrent networks: xcharwt = W fhfwt + W rhrwt + b. (2) The model architecture is the last hidden element of the forward and backward recurrent networks. (2) The model architecture of the proposed word- and hrwt-R model is the last hidden element of the forward and backward recurrent networks are linear combined."}, {"heading": "3 Experimental Settings", "text": "We are testing four different model architectures based on the three English corpora. Each model has a unique word embedding method, but all models share the same LSTM language modeling architecture, which has 2 LSTM layers with 200 hidden units, d = 200. Except for the character model, all weights are initialized with uniform random variables ranging from -0.1 to 0.1. All distortions are initialized to zero. Stochastic gradients decent (SGD) with a minicharge size of 32 are used to train the models. In the first four epochs, the learning rate is 1. After the fourth epoch, the learning rate is halved per epoch. Hyperparameters of the SGD are adjusted for each model based on the validation data. Since the standard metric for speech modeling is Perplexity (PL) to evaluate the performance of the model."}, {"heading": "3.1 Model Variations", "text": "Word Only (baseline) This is a traditional language model at the word level and is a basic model for our experiments. Character Only This is a language model in which each input word is represented as a string similar to the C2W model in (Ling et al., 2015). The bidirectional LSTM has 200 hidden units, and their weights are initialized with the Xavier initialization (Glorot and Bengio, 2010). In addition, the weights of the forget-, input- and output gates are scaled by a factor of 4. The weights in the LSTM language model are initialized with uniform random variables between -0.1 and 0.1. All distortions are initialized to zero. A learning rate is set to 0.2. Word & Characters This model simply concatenates the vector representations of a word formed from character input xxwt and word input xwt."}, {"heading": "3.2 Datasets", "text": "Penn Treebank We use the Penn Treebank Corpus (Marcus et al., 1993), which was pre-edited by Mikolov etal. (2010). We use 10k most common words and 51 characters. In the training phase, we only use sentences with less than 50 words. BBC We use the BBC Corpus, which was prepared by Greene & Cunningham (2006). We use 10k most common words and 62 characters. In the training phase, we use sentences with less than 50 words. IMDB Movie Reviews We use the IMDB Move Review Corpus, which was prepared by Maas et al. (2011). We use 30k most common words and 74 characters. In the training phase, we use sentences with less than 50 words. In the validation and test phase, we use sentences with less than 500 characters."}, {"heading": "3.3 Pre-training", "text": "For the word-sign hybrid models, we used a training method to encourage the model to use both representations; the entire model is trained only on the word-level input for the first m-epochs and only on the sign-level input for the next m-epochs; in the first m-epochs, a learning rate is set to 1, and a lower learning rate of 0.1 is used in the next m-epoch; after the 2nd epoch, both the sign-level and word-level input are used; we use m = 2 for PTB and BBC, m = 1 for IMDB."}, {"heading": "4 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Perplexity", "text": "Table 1 compares the models on each dataset: on the PTB and BBC, the Gated Word & Character model achieves the least helplessness, on the IMDB Movie Review, the Gated Word & Character model without pre-training achieves the least helplessness, closely followed by the Gated Word & Character model initiated with pre-training."}, {"heading": "4.2 Values of Word\u2013Character Gate", "text": "The BBC and IMDB datasets contain words outside the vocabulary (OOV), while the OOV words in the Penn Treebank dataset have been replaced by. In the BBC and IMDB datasets, our model assigns a significantly high gating value to the unknown word mark UNK compared to the other words. As can be seen in Figure 2, the gating value is generally higher for rarer words, implying that the recurring language model has learned to use the spelling of a word if its word vector could not be correctly estimated. We suspect that this flexibility of modulating between word level and character level leads to better language modeling. Overall, the gating values are small. However, this does not mean that the model does not use character level input. We observed that the word vectors constructed from character level input generally have a greater L2 standard than the word vectors constructed from the word level."}, {"heading": "5 Conclusion", "text": "We introduced a recurrent neural network speech model with LSTM units and a word-sign gate. In our model, it was found empirically that character-level input is used, especially when the model meets rare words. Experimental results suggest that the gate can be trained efficiently so that the model can find a good balance between word-character input."}, {"heading": "Acknowledgments", "text": "This work is part of the DS-GA 1010-001 Independent Study in Data Science course at the Center for Data Science of New York University. KC thanks Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016). YM thanks Kentaro Hanaki, Israel Malkin and Tian Wang for their helpful feedback."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Alternative structures for character-level rnns", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": null, "citeRegEx": "Bojanowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2015}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["dos Santos", "Bianca Zadrozny"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Bengio2010] Xavier Glorot", "Yoshua Bengio"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Practical solutions to the problem of diagonal dominance in kernel document clustering", "author": ["Greene", "Cunningham2006] Derek Greene", "Padraig Cunningham"], "venue": "In Machine Learning, Proceedings of the Twenty-Third International Conference (ICML", "citeRegEx": "Greene et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Greene et al\\.", "year": 2006}, {"title": "Exploring the limits of language modeling. CoRR, abs/1602.02410", "author": ["Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": null, "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "Mandarin word-character hybridinput neural network language model", "author": ["Kang et al.2011] Moonyoung Kang", "Tim Ng", "Long Nguyen"], "venue": "INTERSPEECH", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Character-aware neural language models. CoRR, abs/1508.06615", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Tiago Lu\u00eds", "Lu\u00eds Marujo", "R\u00e1mon Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models. CoRR, abs/1604.00788", "author": ["Luong", "Manning2016] Minh-Thang Luong", "Christopher D. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas et al.2011] Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": "In The 49th Annual Meeting of the Association for Computational Linguistics:", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Subword language modeling with neural networks", "author": ["Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Recurrent neural network regularization. CoRR, abs/1409.2329", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Simple, fast noise-contrastive estimation for large rnn vocabularies", "author": ["Zoph et al.2016] Barret Zoph", "Ashish Vaswani", "Jonathan May", "Kevin Knight"], "venue": null, "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Recurrent neural networks (RNNs) achieve state-ofthe-art performance on fundamental tasks of natural language processing (NLP) such as language modeling (RNN-LM) (J\u00f3zefowicz et al., 2016; Zoph et al., 2016).", "startOffset": 162, "endOffset": 206}, {"referenceID": 16, "context": "Recurrent neural networks (RNNs) achieve state-ofthe-art performance on fundamental tasks of natural language processing (NLP) such as language modeling (RNN-LM) (J\u00f3zefowicz et al., 2016; Zoph et al., 2016).", "startOffset": 162, "endOffset": 206}, {"referenceID": 14, "context": "RNN-LMs are usually based on the wordlevel information or subword-level information such as characters (Mikolov et al., 2012), and predictions are made at either word level or subword level respectively.", "startOffset": 103, "endOffset": 125}, {"referenceID": 0, "context": "(2011) apply a word\u2013character hybrid language model on Chinese using a neural network language model (Bengio et al., 2003).", "startOffset": 101, "endOffset": 122}, {"referenceID": 5, "context": "Ling et al. (2015) introduce the compositional character-to-word (C2W) model that takes as input character-level representation of a word and generates vector representation of the word using a bidirectional LSTM (Graves and Schmidhuber, 2005).", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Kim et al. (2015) propose a convolutional neural network (CNN) based character-level language model and achieve the state-of-the-art perplexity on the PTB dataset with a significantly fewer parameters.", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "Kang et al. (2011) apply a word\u2013character hybrid language model on Chinese using a neural network language model (Bengio et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "(2011) apply a word\u2013character hybrid language model on Chinese using a neural network language model (Bengio et al., 2003). Santos and Zadrozny (2014) produce high performance part-of-speech taggers using a deep neural network that learns character-level representation of words and associates them with usual word representations.", "startOffset": 102, "endOffset": 151}, {"referenceID": 0, "context": "(2011) apply a word\u2013character hybrid language model on Chinese using a neural network language model (Bengio et al., 2003). Santos and Zadrozny (2014) produce high performance part-of-speech taggers using a deep neural network that learns character-level representation of words and associates them with usual word representations. Bojanowski et al. (2015) investigate RNN models that predict characters based on the character and word level inputs.", "startOffset": 102, "endOffset": 357}, {"referenceID": 0, "context": "(2011) apply a word\u2013character hybrid language model on Chinese using a neural network language model (Bengio et al., 2003). Santos and Zadrozny (2014) produce high performance part-of-speech taggers using a deep neural network that learns character-level representation of words and associates them with usual word representations. Bojanowski et al. (2015) investigate RNN models that predict characters based on the character and word level inputs. Luong and Manning (2016) present word\u2013character hybrid neural machine translation systems that consult the character-level information for rare words.", "startOffset": 102, "endOffset": 475}, {"referenceID": 15, "context": "We use the architecture similar to the nonregularized LSTM model by Zaremba et al. (2014). One step of LSTM computation corresponds to", "startOffset": 68, "endOffset": 90}, {"referenceID": 9, "context": "Character Only This is a language model where each input word is represented as a character sequence similar to the C2W model in (Ling et al., 2015).", "startOffset": 129, "endOffset": 148}, {"referenceID": 12, "context": "Penn Treebank We use the Penn Treebank Corpus (Marcus et al., 1993) preprocessed by Mikolov et", "startOffset": 46, "endOffset": 67}, {"referenceID": 11, "context": "IMDB Movie Reviews We use the IMDB Move Review Corpus prepared by Maas et al. (2011). We use 30k most frequent words and 74 characters.", "startOffset": 66, "endOffset": 85}], "year": 2016, "abstractText": "We introduce a recurrent neural network language model (RNN-LM) with long shortterm memory (LSTM) units that utilizes both character-level and word-level inputs. Our model has a gate that adaptively finds the optimal mixture of the character-level and wordlevel inputs. The gate creates the final vector representation of a word by combining two distinct representations of the word. The character-level inputs are converted into vector representations of words using a bidirectional LSTM. The word-level inputs are projected into another high-dimensional space by a word lookup table. The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words. Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-ofvocabulary words and outperforms word-level language models on several English corpora.", "creator": "LaTeX with hyperref package"}}}