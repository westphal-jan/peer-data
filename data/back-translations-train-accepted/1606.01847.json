{"id": "1606.01847", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding", "abstract": "Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise multiplication or addition, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.", "histories": [["v1", "Mon, 6 Jun 2016 17:59:56 GMT  (2194kb,D)", "http://arxiv.org/abs/1606.01847v1", null], ["v2", "Thu, 23 Jun 2016 19:52:41 GMT  (3358kb,D)", "http://arxiv.org/abs/1606.01847v2", "Added qualitative and quantitative results"], ["v3", "Sat, 24 Sep 2016 01:58:59 GMT  (3443kb,D)", "http://arxiv.org/abs/1606.01847v3", "Accepted to EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["akira fukui", "dong huk park", "daylen yang", "anna rohrbach", "trevor darrell", "marcus rohrbach"], "accepted": true, "id": "1606.01847"}, "pdf": {"name": "1606.01847.pdf", "metadata": {"source": "CRF", "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding", "authors": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Most approaches require linking the two vector representations (Donahue et al., 2013; Er et al., 2015). For tasks such as VQA, most approaches require linking the two modalities. To combine the two vector representations (multimodal merging), current approaches in VQA or grounding vectors or applying element-wise completion or multiplication are used. While this generates a common representation, it may not be meaningful enough to capture the complex associations between the two modalities."}, {"heading": "2 Related Work", "text": "Simpler models such as iBOWIMG Baseline (Zhou et al., 2015) use concatenation and fully connected layers to combine the image and the questions. Stacked Attention Networks (Yang et al., 2015) and Spatial Memory Networks (Xu et al., 2015) use soft attention to the image functions, but ultimately use element-wise sums to merge modalities. D-NMN et al., 2016a) have REINFORCE to create a network and use elements-wise."}, {"heading": "3 Multimodal Compact Bilinear Pooling for Visual and Textual Embeddings", "text": "For the task of visual response to questions (VQA) or visual grounding, we need to predict the most likely answer or place where a picture x and a question or phrase q are located, which can be formulated as follows: A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = B. In this section, we are interested in a good common representation by combining the two representations. Multimodal pooling (x, q), which encodes the relationship between x and q well, makes it easier to learn a classification of equation (1). In this section, we will first discuss our multimodal pooling to combine representations from different modalities into a single representation (Figure 3.1) and then our architectures for VQA (Figure 3.2) and a further explanation (Figure 3.3) with a visual representation."}, {"heading": "3.1 Multimodal Compact Bilinear Pooling (MCB)", "text": "Bilinear models (Tenenbaum and Freeman, 2000) take the outer product of two vectors q = 48 sqm and q = Rn2 and learn a model W (here linear), i.e. z = W [x q], where it denotes the equivalent product (xqT) and [] denotes the linearized matrix in a vector. As discussed in the introduction, bilinear pooling is interesting because it allows all elements of both vectors to interact with each other in a multiplicative way. However, it denotes the high-dimensional representation algorithm 1 multimodal compact bilinear 1: Input: v1 x-dimensional pooling is interesting because it allows all elements of both vectors to interact with each other."}, {"heading": "3.2 Architectures for VQA", "text": "In fact, the fact is that most of them are able to move, to move and to move, and that they are able to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "3.3 Architecture for Visual Grounding", "text": "Our grounding approach is based on the fully supervised version of GroundeR (Rohrbach et al., 2016).The overview of our model is shown in Figure 5. Input into the model is a query phrase and an image along with multiple bounding fields for suggestions. The goal is to predict a bounding field corresponding to the query phrase. We replace the concatenation of the visual representation and the coded phrase in Grounder with MCB to combine the two modalities. Unlike Rohrbach et al. (2016), we include a linear embedding of the visual representation and an L2 normalization of both input modalities instead of a batch normalization (Ioffe and Szegedy, 2015), which we considered advantageous for the grounding task."}, {"heading": "4 Evaluation on Visual Question Answering", "text": "We evaluate the usefulness of MCB using a multitude of ablations based on two visual questions that respond to data sets."}, {"heading": "4.1 Datasets", "text": "The Visual Question Answering Real-Image Dataset (Antol et al., 2015) consists of approximately 200,000 MSCOCO images (Lin et al., 2014), with 3 questions per image and 10 answers per question. There are 3 data splits: train (80K images), validation (40K images), and test (80K images). In addition, there is a 25% subset of tests called test-dev. Ac-curacies for ablation experiments in this paper. We use the VQA tool provided by Antol et al. (2015) for evaluation. We performed most of our experiments with the open real-image task. In Table 4 we also report on our multiple-choice real-image scores. The Visual Genome Dataset (Krishna et al., 2016) uses visual images from the intersection of YFCC100M (Thomeet al., 2015) and COMW-7W pairs of questions each."}, {"heading": "4.2 Experimental Setup", "text": "We use the Adam solver with = 0.0007, \u03b21 = 0.9, \u03b22 = 0.999. We use dropout after the LSTM layer and in fully connected layers. For all experiments, we train on the VQA train split, validate on the VQA validation split, and report on the results on the VQA test split. We use the same hyperparameters and allocation settings as in the VQA experiments: If the validation score for 50,000 iterations does not improve, we interrupt the training and evaluate the best iteration on the test dev. For the Visual7W task, we use the same hyperparameters and allocation settings as in the VQA experiments. We use the splits off (Zhu et al., 2016) to train, validate and test our models. Accuracy was calculated using the evaluation codes published by (Zhu et al., 2016)."}, {"heading": "4.3 Ablation Results", "text": "We compare the performance of non-bilinear and bilinear pooling methods in Table 1. We see that MCB pooling outperforms all non-bilinear pooling methods, such as the core sum, the concatenation, and the core product.It could be argued that the compact bilinear method simply has more parameters than the non-bilinear pooling methods that contribute to its performance. We compensated for this by using fully connected layers (with 4096 units per layer, ReLU activation, and failures). According to the non-bilinear pooling methods, in order to increase their number of parameters. However, even with similar parameter budgets, non-bilinear methods could not achieve the same accuracy as the MCB method. For example, the \"Concat + FC + FC\" pooling method shows approximately 40962 + 4096 x 3000 x 46 million parameters available in MCB."}, {"heading": "4.4 Comparison to State-of-the-Art", "text": "Table 4 compares our approach to the state of the art. Our best performance comes from the interplay of seven models using MCB pooling with lathing. In addition, we have added images and QA pairs from the Visual Genome dataset to our training data, which is 4.4 points above the next best approach for the open VQA task and 4 points above the next best approach for the multiplechoice task (the test standard). It is important to note that even without ensembles and additional training data, our \"MCB + Att.\" model still outperforms the previous state of the art by 2.4 points, with an accuracy of 64.2% versus 61.8% for the open task (Test-dev)."}, {"heading": "5 Evaluation on Visual Grounding", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets", "text": "We evaluate our visual grounding approach using two challenging datasets. The first is Flickr30k Entities (Plummer et al., 2015), which consists of 31K images from the Flickr30k dataset (Hodosh et al., 2014) with 244K phrases localized with delimiting fields. We follow the experimental setup of previous work, e.g. 1Plummer et al. (2016), and achieve a higher accuracy of 50.89% taking into account box size and color. We believe that our approach would also benefit from such additional features. We use the same selective search (Uijlings et al., 2013) object suggestions and the Fast R-CNN (Girshick, 2015) fine-tuned features of the VGG16 (Simonyan and Zisserman, 2014) as Rohrbach et al. (2016). The second dataset is reference game (Kazemzadeh et, Seboth) of the IA20K-ASPR dataset (2011K-Box)."}, {"heading": "5.2 Experimental Setup", "text": "In all experiments, we use Adam Solver (Kingma and Ba, 2014) with a learning rate of = 0.0001. The embedding size used in our model is 500 for both visual and linguistic embedding. In the following, we used = 2048 for MCB pooling, which we believe works best for the visual grounding task. Accuracy is measured as a percentage of correctly localized query phrases. If the predicted bounding box overlaps with the ground truth bounding box by more than 50%, the phrase will be correctly localized."}, {"heading": "5.3 Results", "text": "Tables 5 and 6 summarize our results in the visual grounding task. We present several ablations of our proposed architecture. First, we replace the MCB with a simple concatenation of the embedded visual function and the embedded phrase, resulting in 46.5% for the Flickr30k entities and 25.48% for the ReferItGame datasets. Results can be improved by replacing the concatenation with the elementary product of the two embedded features (47.41% and 27.80%). We can further increase performance by introducing additional 2048-D folds after the elementary product (47.86% and 27.98%), but even with fewer parameters, our MCB pooling improves significantly above this baseline for both datasets, achieving a state-of-the-art accuracy of 48.69% for Flickr30k entities and 28.91% for reference game datasets. Figure 6 (right) shows an improved phralocalization example."}, {"heading": "6 Conclusion", "text": "To answer visual questions, our attentive architecture and multiple MCBs provide significant improvements over state-of-the-art in two VQA datasets. In the visual grounding task, the introduction of MCB pooling results in improved phrase localization accuracy, indicating better interaction between the representation of query phrases and the visual representation of request delimitation fields."}, {"heading": "Acknowledgments", "text": "This work was supported by DARPA, AFRL, DoD MURI award N000141110688, NSF awards IIS1427425 and IIS-1212798 and the Berkeley Vision and Learning Center."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Neural module networks", "author": ["Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Finding frequent items in data streams", "author": ["Kevin Chen", "Martin Farach-Colton"], "venue": "In Automata, languages and programming,", "citeRegEx": "Charikar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2002}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Donahue et al.2013] Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": null, "citeRegEx": "Donahue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2013}, {"title": "The segmented and annotated iapr tc-12", "author": ["Carlos A Hern\u00e1ndez", "Jesus A Gonzalez", "Aurelio L\u00f3pez-L\u00f3pez", "Manuel Montes", "Eduardo F Morales", "L Enrique Sucar", "Luis Villase\u00f1or", "Michael Grubinger"], "venue": null, "citeRegEx": "Escalante et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Escalante et al\\.", "year": 2010}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome et al.2013] Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Compact bilinear pooling", "author": ["Gao et al.2016] Yang Gao", "Oscar Beijbom", "Ning Zhang", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Gao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2016}, {"title": "Fast R-CNN", "author": ["Ross Girshick"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV)", "citeRegEx": "Girshick.,? \\Q2015\\E", "shortCiteRegEx": "Girshick.", "year": 2015}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Gong et al.2014] Yunchao Gong", "Liwei Wang", "Micah Hodosh", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV)", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "The iapr tc-12 benchmark: A new evaluation resource for visual information systems", "author": ["Paul Clough", "Henning M\u00fcller", "Thomas Deselaers"], "venue": "In International Workshop OntoImage,", "citeRegEx": "Grubinger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Grubinger et al\\.", "year": 2006}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["Sandor Szedmak", "John Shawe-Taylor"], "venue": "Neural computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Deep residual learning for image recognition", "author": ["He et al.2015] Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Hodosh et al.2014] Peter Hodosh", "Alice Young", "Micah Lai", "Julia Hockenmaier"], "venue": null, "citeRegEx": "Hodosh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2014}, {"title": "2016a. Segmentation from natural language expressions", "author": ["Hu et al.2016a] Ronghang Hu", "Marcus Rohrbach", "Trevor Darrell"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Natural language object retrieval", "author": ["Hu et al.2016b] Ronghang Hu", "Huazhe Xu", "Marcus Rohrbach", "Jiashi Feng", "Kate Saenko", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "A focused dynamic attention model for visual question answering", "author": ["Shuicheng Yan", "Jiashi Feng"], "venue": null, "citeRegEx": "Ilievski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ilievski et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Szegedy2015] Sergey Ioffe", "Christian Szegedy"], "venue": null, "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Fei-Fei2015] Andrej Karpathy", "Li FeiFei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Referit game: Referring to objects in photographs of natural scenes", "author": ["Vicente Ordonez", "Mark Matten", "Tamara L. Berg"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Kazemzadeh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kazemzadeh et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["Kiros et al.2014] Ryan Kiros", "Ruslan Salakhutdinov", "Rich Zemel"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Fisher vectors derived from hybrid gaussian-laplacian mixture models for image annotation", "author": ["Klein et al.2015] Benjamin Klein", "Guy Lev", "Gil Sadeh", "Lior Wolf"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Klein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar et al.2015] Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin et al.2014] Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV)", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Bilinear cnn models for finegrained visual recognition", "author": ["Lin et al.2015] Tsung-Yu Lin", "Aruni RoyChowdhury", "Subhransu Maji"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV)", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Hierarchical Co-Attention for Visual Question Answering", "author": ["Lu et al.2016] J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": null, "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering", "author": ["M. Rohrbach", "M. Fritz"], "venue": null, "citeRegEx": "Malinowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2016}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["Mao et al.2015] Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Multimodal deep learning", "author": ["Ngiam et al.2011] Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["Noh et al.2015] Hyeonwoo Noh", "Paul Hongsuck Seo", "Bohyung Han"], "venue": null, "citeRegEx": "Noh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2015}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["Pham", "Pagh2013] Ninh Pham", "Rasmus Pagh"], "venue": "In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Pham et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2013}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models", "author": ["Liwei Wang", "Chris Cervantes", "Juan Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": null, "citeRegEx": "Plummer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2015}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models. arXiv:1505.04870v3", "author": ["Liwei Wang", "Chris Cervantes", "Juan Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": null, "citeRegEx": "Plummer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2016}, {"title": "Grounding of textual phrases in images", "author": ["Marcus Rohrbach", "Ronghang Hu", "Trevor Darrell", "Bernt Schiele"], "venue": null, "citeRegEx": "Rohrbach et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Grounded compositional semantics for finding", "author": ["Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. V Le"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Separating style and content with bilinear models", "author": ["Tenenbaum", "Freeman2000] Joshua B Tenenbaum", "William T Freeman"], "venue": "Neural computation,", "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "The new data and new challenges in multimedia research", "author": ["Thomee et al.2015] Bart Thomee", "David A. Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li"], "venue": null, "citeRegEx": "Thomee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thomee et al\\.", "year": 2015}, {"title": "Selective search for object recognition", "author": ["Koen EA van de Sande", "Theo Gevers", "Arnold WM Smeulders"], "venue": null, "citeRegEx": "Uijlings et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uijlings et al\\.", "year": 2013}, {"title": "Learning deep structure-preserving image-text embeddings", "author": ["Wang et al.2016] Liwei Wang", "Yin Li", "Svetlana Lazebnik"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Weston et al.2011] Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "In IJCAI,", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources", "author": ["Wu et al.2016] Qi Wu", "Peng Wang", "Chunhua Shen", "Anton van den Hengel", "Anthony Dick"], "venue": "In Proc. IEEE Conf. Computer Vision Pattern Recognition", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Xiong et al.2016] Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": null, "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Ask, attend and answer: Exploring questionguided spatial attention for visual question answering", "author": ["Xu", "Saenko2015] Huijuan Xu", "Kate Saenko"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "Proceedings of the International Conference on Machine", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Yang et al.2015] Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Visual7W: Grounded Question Answering in Images", "author": ["Zhu et al.2016] Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["Zitnick", "Doll\u00e1r2014] C Lawrence Zitnick", "Piotr Doll\u00e1r"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "Zitnick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zitnick et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "2015), and convolutional neural networks (CNNs) are often used to represent images (Donahue et al., 2013; He et al., 2015).", "startOffset": 83, "endOffset": 122}, {"referenceID": 11, "context": "2015), and convolutional neural networks (CNNs) are often used to represent images (Donahue et al., 2013; He et al., 2015).", "startOffset": 83, "endOffset": 122}, {"referenceID": 24, "context": "Bilinear pooling models (Tenenbaum and Freeman, 2000) have recently been shown to be beneficial for finegrained classification for vision only tasks (Lin et al., 2015).", "startOffset": 149, "endOffset": 167}, {"referenceID": 2, "context": "As shown in Figure 1, Multimodal Compact Bilinear pooling (MCB) is approximated by randomly projecting the image and text representations to a higher dimensional space (using Count Sketch (Charikar et al., 2002)) and then convolving both vectors efficiently by using element-wise multiplication in Fast Fourier Transform (FFT) space.", "startOffset": 188, "endOffset": 211}, {"referenceID": 5, "context": "In this paper, we adopt the idea from Gao et al. (2016) which shows how to efficiently compress bilinear pooling for a single modality.", "startOffset": 38, "endOffset": 56}, {"referenceID": 46, "context": "Stacked Attention Networks (Yang et al., 2015) and Spatial Memory Networks (Xu et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 44, "context": ", 2015) and Spatial Memory Networks (Xu et al., 2015) use LSTMs or extract soft-attention on the image features, but ultimately use element-wise product or element-wise sums to merge modalities.", "startOffset": 36, "endOffset": 53}, {"referenceID": 43, "context": "Dynamic Memory Networks (DMN) (Xiong et al., 2016) pool image and question with element-wise multiplication and addition, attending to part of image and question with a Episodic Memory Module (Kumar et al.", "startOffset": 30, "endOffset": 50}, {"referenceID": 22, "context": ", 2016) pool image and question with element-wise multiplication and addition, attending to part of image and question with a Episodic Memory Module (Kumar et al., 2015).", "startOffset": 149, "endOffset": 169}, {"referenceID": 29, "context": "DPPnet (Noh et al., 2015) create a Parameter Prediction Network which learns to predict the parameters of the second last visual recognition layer dynamically from the question.", "startOffset": 7, "endOffset": 25}, {"referenceID": 0, "context": "D-NMN (Andreas et al., 2016a) introduced REINFORCE to dynamically create a network and use element-wise multiplication to joint attentions, and elementwise sum to predict answers. Dynamic Memory Networks (DMN) (Xiong et al., 2016) pool image and question with element-wise multiplication and addition, attending to part of image and question with a Episodic Memory Module (Kumar et al., 2015). DPPnet (Noh et al., 2015) create a Parameter Prediction Network which learns to predict the parameters of the second last visual recognition layer dynamically from the question. Lu et al. (2016) recently proposed a model that extracts multiple co-attentions on the image and question and combines the co-attentions in a hierarchical manner using all of element-wise sums, concatenation, and fully connected layers.", "startOffset": 7, "endOffset": 589}, {"referenceID": 13, "context": "Similarly, Hu et al. (2016a) concatenate phrase embeddings with visual features at different spatial locations to obtain a segmentation.", "startOffset": 11, "endOffset": 29}, {"referenceID": 22, "context": "Lin et al. (2015) use two CNNs to extract features from an image and combine the resulting vectors using an outer product, which is fully connected to an output layer.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Gao et al. (2016) address the space and time complexity of bilinear features by viewing the bilinear transformation as a polynomial kernel.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Gao et al. (2016) address the space and time complexity of bilinear features by viewing the bilinear transformation as a polynomial kernel. Pham and Pagh (2013) describes a method to approximate the polynomial kernel using Count Sketches and convo-", "startOffset": 0, "endOffset": 161}, {"referenceID": 10, "context": "Some of such embeddings are based on Canonical Correlation Analysis (Hardoon et al., 2004) e.", "startOffset": 68, "endOffset": 90}, {"referenceID": 8, "context": "(Gong et al., 2014; Klein et al., 2015; Plummer et al., 2015), linear models with ranking loss (Frome et al.", "startOffset": 0, "endOffset": 61}, {"referenceID": 21, "context": "(Gong et al., 2014; Klein et al., 2015; Plummer et al., 2015), linear models with ranking loss (Frome et al.", "startOffset": 0, "endOffset": 61}, {"referenceID": 31, "context": "(Gong et al., 2014; Klein et al., 2015; Plummer et al., 2015), linear models with ranking loss (Frome et al.", "startOffset": 0, "endOffset": 61}, {"referenceID": 5, "context": ", 2015), linear models with ranking loss (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Socher et al., 2014; Weston et al., 2011) or non-linear deep learning models (Kiros et al.", "startOffset": 41, "endOffset": 131}, {"referenceID": 35, "context": ", 2015), linear models with ranking loss (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Socher et al., 2014; Weston et al., 2011) or non-linear deep learning models (Kiros et al.", "startOffset": 41, "endOffset": 131}, {"referenceID": 41, "context": ", 2015), linear models with ranking loss (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Socher et al., 2014; Weston et al., 2011) or non-linear deep learning models (Kiros et al.", "startOffset": 41, "endOffset": 131}, {"referenceID": 20, "context": ", 2011) or non-linear deep learning models (Kiros et al., 2014; Mao et al., 2015; Ngiam et al., 2011).", "startOffset": 43, "endOffset": 101}, {"referenceID": 27, "context": ", 2011) or non-linear deep learning models (Kiros et al., 2014; Mao et al., 2015; Ngiam et al., 2011).", "startOffset": 43, "endOffset": 101}, {"referenceID": 28, "context": ", 2011) or non-linear deep learning models (Kiros et al., 2014; Mao et al., 2015; Ngiam et al., 2011).", "startOffset": 43, "endOffset": 101}, {"referenceID": 2, "context": "(2016) for a single modality, we rely on the count sketch projection function \u03a8 (Charikar et al., 2002), which projects a vector v \u2208 Rn to y \u2208 Rd.", "startOffset": 80, "endOffset": 103}, {"referenceID": 5, "context": "As suggested by Gao et al. (2016) for a single modality, we rely on the count sketch projection function \u03a8 (Charikar et al.", "startOffset": 16, "endOffset": 34}, {"referenceID": 11, "context": "We extract image features using a 152-layer Residual Network (He et al., 2015) that is pretrained on ImageNet data (Deng et al.", "startOffset": 61, "endOffset": 78}, {"referenceID": 44, "context": "Explored by (Xu et al., 2015) for image captioning and by (Xu and Saenko, 2015) and (Yang et al.", "startOffset": 12, "endOffset": 29}, {"referenceID": 46, "context": ", 2015) for image captioning and by (Xu and Saenko, 2015) and (Yang et al., 2015) for VQA, the soft attention mechanism can be easily integrated to our model.", "startOffset": 62, "endOffset": 81}, {"referenceID": 33, "context": "We base our grounding approach on the fullysupervised version of GroundeR (Rohrbach et al., 2016).", "startOffset": 74, "endOffset": 97}, {"referenceID": 33, "context": "We base our grounding approach on the fullysupervised version of GroundeR (Rohrbach et al., 2016). The overview of our model is shown in Figure 5. The input to the model is a query natural language phrase and an image along with multiple proposal bounding boxes. The goal is to predict a bounding box which corresponds to the query phrase. We replace the concatenation of the visual representation and the encoded phrase in Grounder with MCB to combine both modalities. In contrast to Rohrbach et al. (2016), we include a linear embedding of the visual representation and L2 normalization of both input modalities, instead of batch normalization (Ioffe and Szegedy, 2015), which we found to be beneficial when using MCB for the grounding task.", "startOffset": 75, "endOffset": 508}, {"referenceID": 23, "context": ", 2015) consists of approximately 200,000 MSCOCO images (Lin et al., 2014), with 3 questions per image and 10 answers per question.", "startOffset": 56, "endOffset": 74}, {"referenceID": 38, "context": ", 2016) uses 108,249 images from the intersection of YFCC100M (Thomee et al., 2015) and MSCOCO.", "startOffset": 62, "endOffset": 83}, {"referenceID": 47, "context": "The Visual7W dataset (Zhu et al., 2016) is a part of the Visual Genome.", "startOffset": 21, "endOffset": 39}, {"referenceID": 47, "context": "We use the splits from (Zhu et al., 2016) to train, validate, and test our models.", "startOffset": 23, "endOffset": 41}, {"referenceID": 47, "context": "The accuracy was computed using the evaluation code released by (Zhu et al., 2016).", "startOffset": 64, "endOffset": 82}, {"referenceID": 25, "context": "HieCoAtt (Lu et al., 2016) 79.", "startOffset": 9, "endOffset": 26}, {"referenceID": 43, "context": "1 DMN+ (Xiong et al., 2016) 80.", "startOffset": 7, "endOffset": 27}, {"referenceID": 15, "context": "4 FDA (Ilievski et al., 2016) 81.", "startOffset": 6, "endOffset": 29}, {"referenceID": 42, "context": "4 AMA (Wu et al., 2016) 81.", "startOffset": 6, "endOffset": 23}, {"referenceID": 46, "context": "4 SAN (Yang et al., 2015) 79.", "startOffset": 6, "endOffset": 25}, {"referenceID": 26, "context": "7 AYN (Malinowski et al., 2016) 78.", "startOffset": 6, "endOffset": 31}, {"referenceID": 29, "context": "1 DPPnet (Noh et al., 2015) 80.", "startOffset": 9, "endOffset": 27}, {"referenceID": 31, "context": "The first is Flickr30k Entities (Plummer et al., 2015) which consists of 31K images from Flickr30k dataset (Hodosh et al.", "startOffset": 32, "endOffset": 54}, {"referenceID": 12, "context": ", 2015) which consists of 31K images from Flickr30k dataset (Hodosh et al., 2014) with 244K phrases localized with bounding boxes.", "startOffset": 60, "endOffset": 81}, {"referenceID": 13, "context": "30 Hu et al. (2016b) 27.", "startOffset": 3, "endOffset": 21}, {"referenceID": 13, "context": "30 Hu et al. (2016b) 27.80 Plummer et al. (2016)1 43.", "startOffset": 3, "endOffset": 49}, {"referenceID": 13, "context": "30 Hu et al. (2016b) 27.80 Plummer et al. (2016)1 43.84 Wang et al. (2016) 43.", "startOffset": 3, "endOffset": 75}, {"referenceID": 13, "context": "30 Hu et al. (2016b) 27.80 Plummer et al. (2016)1 43.84 Wang et al. (2016) 43.89 Rohrbach et al. (2016) 47.", "startOffset": 3, "endOffset": 104}, {"referenceID": 39, "context": "we use the same Selective Search (Uijlings et al., 2013) object proposals and the Fast R-CNN (Girshick, 2015) fine-tuned VGG16 features (Simonyan and Zisserman, 2014), as Rohrbach et al.", "startOffset": 33, "endOffset": 56}, {"referenceID": 7, "context": ", 2013) object proposals and the Fast R-CNN (Girshick, 2015) fine-tuned VGG16 features (Simonyan and Zisserman, 2014), as Rohrbach et al.", "startOffset": 44, "endOffset": 60}, {"referenceID": 18, "context": "The second dataset is ReferItGame (Kazemzadeh et al., 2014), which contains 20K images from IAPR TC12 dataset (Grubinger et al.", "startOffset": 34, "endOffset": 59}, {"referenceID": 9, "context": ", 2014), which contains 20K images from IAPR TC12 dataset (Grubinger et al., 2006) with segmented regions from SAIAPR-12 dataset (Escalante et al.", "startOffset": 58, "endOffset": 82}, {"referenceID": 6, "context": ", 2013) object proposals and the Fast R-CNN (Girshick, 2015) fine-tuned VGG16 features (Simonyan and Zisserman, 2014), as Rohrbach et al. (2016). The second dataset is ReferItGame (Kazemzadeh et al.", "startOffset": 45, "endOffset": 145}, {"referenceID": 13, "context": "For ReferItGame we follow the experimental setup of Hu et al. (2016b) and rely on their ground-truth bounding boxes extracted around the segmentation masks.", "startOffset": 52, "endOffset": 70}, {"referenceID": 13, "context": "For ReferItGame we follow the experimental setup of Hu et al. (2016b) and rely on their ground-truth bounding boxes extracted around the segmentation masks. We use the Edge Box (Zitnick and Doll\u00e1r, 2014) object proposals and visual features (VGG16 combined with the spatial features, which encode bounding box relative position) from Hu et al. (2016b).", "startOffset": 52, "endOffset": 352}], "year": 2017, "abstractText": "Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise multiplication or addition, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.", "creator": "LaTeX with hyperref package"}}}