{"id": "1606.01868", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "Unifying Count-Based Exploration and Intrinsic Motivation", "abstract": "We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use sequential density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary sequential density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.", "histories": [["v1", "Mon, 6 Jun 2016 19:21:32 GMT  (2153kb,D)", "http://arxiv.org/abs/1606.01868v1", null], ["v2", "Mon, 7 Nov 2016 21:16:21 GMT  (2091kb,D)", "http://arxiv.org/abs/1606.01868v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["marc g bellemare", "sriram srinivasan", "georg ostrovski", "tom schaul", "david saxton", "r\u00e9mi munos"], "accepted": true, "id": "1606.01868"}, "pdf": {"name": "1606.01868.pdf", "metadata": {"source": "CRF", "title": "Unifying Count-Based Exploration and Intrinsic Motivation", "authors": ["Marc G. Bellemare", "Sriram Srinivasan", "Georg Ostrovski"], "emails": ["bellemare@google.com", "srsrinivasan@google.com", "ostrovski@google.com", "schaul@google.com", "saxton@google.com", "munos@google.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2 Notation", "text": "We consider an alphabet to be X \u00b7 This alphabet can be finite, e.g. the binary alphabet = 1 > chain = 1 \u00b2. (All grayscale images of certain fixed dimensions or the set of English letters; or it can be countable, e.g. the set of English sentences. We denote a length sequence n from this alphabet with x1: n \u00b2 Xn, the set of finite sequences from X with X \u00b2, write x1: nx to mean the concatenation of x1: n and a symbol x \u00b2 X. We denote the empty string. A sequential density model of X is a mapping of X \u00b2 to probability distributions over X \u00b2. That is, for each x1: n \u00b2 Xn - the model delivers a probability distribution denotedn (x; x1: n). If the model produces a probability distribution from X \u00b2 and not simply a mapping from sequences to distributions."}, {"heading": "2.1 Markov Decision Processes and models over joint alphabets.", "text": "Consequently, the alphabet X will sometimes play the role of state space in a Markov decision-making process (X, A, P, R, \u03b3) with action set A, transition function P, reward function R, and discount factor \u03b3. Of course, there is the need for a sequential density model over a common alphabet X \u00b7 Y, where Y a) the action setA or b) the setZ of the attainable finite horizon returns (undiscounted reward sums). If Y is small and finite and we get a sequential density model over X, we can construct a reasonable sequential density model over the common alphabet X \u00b7 Y using a chain-like construct (Veness et al., 2015). For y-Y, we define y1: n-Yn the index theorem (y): = 1-Yen-Yen distribution model: = {t: yt = y}."}, {"heading": "2.2 Count-based exploration.", "text": "Number-based exploration algorithms use the state-action visit countNn (x, a) to explore the principle of optimism in the face of uncertainty. We are now reviewing two such algorithms to illustrate how visit volumes can be used to advance exploration. Model-based interval estimation (MBIE; Strehl and Littman, 2008) explores by acting optimistically with respect to the agent's empirical model, which maintains the empirical estimates P and R of the transition and reward functions. Optimism takes the form of an L1 confidence interval over both P and R intervals; this confidence interval is derived from Nn (x, a). A closely related algorithm, UCRL2, shows low regret within the framework of average reward and reward functions (Jaksch et al, 2010). Optimism takes the form of an interval over both LP and LP steps."}, {"heading": "3 From Predictions to Counts", "text": "In the introduction, we argued that in many practical constellations, states are rarely revisited, which precludes answering the question: \"How new is this state?\" with the empirical count, which is almost always zero. Also, the problem is not solved by a Bayesian approach: even variable letter estimators (e.g. Friedman and Singer, 1999; Hutter, 2015) must assign a small, diminishing probability to states not yet seen. In large spaces, counting is (in the usual sense) irrelevant; to estimate the certainty of an acting knowledge, we must instead look for a quantity that is generalized across states. In this section, we derive such a quantity because it expands the familiar notion of Bayesian estimates. As a functioning example, we consider the following scenario: A commuter has recently landed in a major metropolis, and every morning at the train station she observes the weather (x1)."}, {"heading": "3.1 An approximation to Equation (2).", "text": "We close this section with an approximation of the pseudo-counting described by (2). We begin by solving (1) somewhat differently: N-N (x) + 1 N-N (x) = N-N (x) (x) (x) n-N (x) n-1 N-N (x) = B-N (x) (n + 1) n (x) n-N (x) = (n) n (x) n (x) n (x) n-1) \u2212 1 N (x) n (x) n (x) n (x) - \u03c1n (x), having made the approximation n-N (x) + 1. The investigation of (2) shows that this is equivalent to the assumption that \"n (x) \u2248 0,\" which is typical of large alphabets, corresponds: the most common English word, \"the,\" has a relative frequency of 4.7%; while the twentieth, \"at,\" has a relative frequency of barely 0.2%."}, {"heading": "4 (Pseudo-)Counting Salient Events", "text": "It is a question of what the future of humanity is like, and it is a question of what the future of humanity is like. (...) It is a question of what the future of humanity is like. (...) It is a question of what the future of humanity is like. (...) It is a question of what the future of humanity is like. (...) It is a question of what the future of humanity is like. (...) It is a question of the future of humanity. (...) It is a question of the future of humanity. (...) It is a question of the future of humanity. (...) It is a question of the future of humanity. (...) It is a question of the future of humanity. (...) It is a question of the future of humanity. (...) It is a question of the future of humanity. (...) It is a question of the future of humanity. \""}, {"heading": "5 Properties of Pseudo-Counts", "text": "In this section, we outline interesting properties of the pseudo-number N-N. First, we provide a consistency result that describes the limiting behavior of the ratio N-N / Nn. Then, we instantiate this result for a broad class of models, including the CTS model used in the previous section. Finally, we reveal a relationship between our pseudo-number and a density-based approach to value matching called Compress and Control (Veness et al., 2015)."}, {"heading": "5.1 Relation of pseudo-count to empirical count.", "text": "We define the boundary of a sequence of functions (f (x; x1: n): n in relation to the length n of the sub-sequence n \u2212 n: n. We also assume that the empirical distribution (n) n (n) points to a distribution (f (x; x1: n): n (n) in relation to the length n of the sub-sequence n \u2212 n: n. We start with two assumptions about our sequential density model. Assumption 1. The boundaries (a) r (x): = lim n (n) in relation to the distribution n (x) n (x) in relation to the x-side (x). We start with two assumptions regarding our sequential density model.Assumption 1. The boundaries (a) r (x): lim n (x) in relation to the x-side (x), x) in relation to the x-side (x)."}, {"heading": "5.2 Directed graphical models as sequential density models.", "text": "Next, we show that directed graphical models (Wainwright and Jordan, 2008) fulfill assumption 1 (xi \u00b2). A directed graphical model describes a probability distribution using a factored alphabet. A parental sentence \u03c0 (i) is assigned to the ith factor xi (i) {1,.., i \u2212 1}. Let x\u03c0 (i) denote the value of the factors in the parental set. The ith factor model is assigned to (x i; x\u03c0 (i)): = \u03c1i (xi; x1: n): = k \u00b2 in (x i; x\u03c0 (i))), understanding that it is allowed to make a different prediction for each value of x\u03c0 (i). The symbol x \u2212 x is assigned to the common probability model GM (x; x1: n): = x \u00b2 in (x i; x\u03c0 (i)).Frequent decisions for district decisions (xi) x (1) contain the conditional distribution and the estimator."}, {"heading": "If N\u0302n is the pseudo-count corresponding to \u03c1n then N\u0302n(x)/Nn(x)\u2192 1 for all x with \u00b5(x) > 0.", "text": "In the appendix, we prove a somewhat stronger result that allows \u03c6 (x) to vary with x1: n; the above case is a special case of this result. Therefore, pseudo-counts derived from atomic (i.e. one-factorial) sequential density models exhibit the correct behavior: they correspond asymptotically with the empirical counts."}, {"heading": "5.3 Relationship to a kind of dual value function.", "text": "The Compress and Control Method (Veness et al., 2015) applies sequential density models to the problem of amplification. It uses these models to learn a form of dual value function based on stationary distributions (Wang et al., 2008). In the Markov Decision Making (MDP) setting is a policy of mapping states to distributions via actions. The finite horizon function for this policy is the expected sum of rewards over a horizon H: N: V \u03c0 (x) = E\u03c0 (H) = 1 r (xt, at).Consider the set of all achievable returns Z: = (z).Consider the set of all achievable returns Z: Pr (xt) = 1 r \u00b2 > 0}. Let us assume that Z is finite (e.g. r x, a) assumes a finite number of values."}, {"heading": "In particular, if N\u0302n(x) > 0 then", "text": "Proposition 2 shows that the compression and control value function (4) is similar, with an additional distortion factor N-n / N-zn applied to the pseudo-empirical distribution, a distortion factor derived from the potential discrepancy in learning rates between yield-related density models that must occur when the distortion factors are not uniform. If the distortion factor is asymptotic 1, we can use Theorem 1 (under normal occupational conditions) to derive relative upper and lower limits to the asymptotic approximation value function V-n (x): = lim n-z-z-zn (x). Proposition 2 immediately suggests a new algorithm calculation that immediately avoids the distortion of V-n (x)."}, {"heading": "6 The Connection to Intrinsic Motivation", "text": "Intrinsic motivation algorithms attempt to explain what drives behavior in absence or even as opposed to extrinsic reward (Barto, 2013). To reflect the terminology of machine learning, we can imagine intrinsically motivated behavior as unsupervised exploration. However, as we now show, there is a surprisingly close link between signals typical of intrinsic motivation algorithms and number-based explorations. Here, the information gain refers to the change in posterior within a mixture model defined by a classM of sequential density models. A mixture model itself is a universal sequential density model that predicts according to a weighted combination of models."}, {"heading": "7 Pseudo-Counts for Exploration", "text": "In this section, we will demonstrate the use of pseudo-count in guiding the exploration. We will return to the Arcade Learning Environment, but will now use the same CTS sequence density model to extend the environmental reward with a numbered exploration bonus. Unless otherwise stated, all our agents will be trained on the stochastic version of the Arcade Learning Environment."}, {"heading": "7.1 Exploration in hard games.", "text": "Out of 60 games available through the Arcade Learning Environment, we have identified those for which exploration is difficult, in the sense that a greedy policy is clearly inefficient (a rough taxonomy is given in the appendix). While we continue to optimistically select five games for each constellation, we can adequately represent the value function, and our CTS density model can adequately represent empirical distribution across states. We will pay particular attention to MONTEZUMA'S REVENGE, one of the most difficult Atari 2600 games available through ALE. MONTEZUMA'S REVENGE is notorious for its hostile, unforgiving environment: the agent must navigate a performance of different rooms, each filled with a number of traps. The rewards are far and few in between, making it almost impossible for the best exploration programs to succeed."}, {"heading": "7.2 Exploration in MONTEZUMA\u2019S REVENGE.", "text": "MONTEZUMA'S REVENGE is divided into three levels, each of which consists of 24 pyramid-shaped rooms (Figure 3). As described above, each room presents a series of challenges: To escape from the very first room, the agent must climb ladders, dodge a creature, pick up a key, and then open one of two doors backwards. Therefore, the number of rooms reached by an agent is a good measure of his ability. By accessing game RAM, we tracked the agent's location at every step during training. [6] We calculated the number of visits to each room averaged over periods of one million frames. From this information, we created a map of the agent's \"known world,\" i.e., all rooms visited were viewed at least once. [Figure 4] paints a clear picture: after 50 million frames, the agent saw a total of 15 rooms with exploration bonuses, while the agent saw two without bonuses."}, {"heading": "7.3 Improving exploration for actor-critic methods.", "text": "Next, we turn to the methods of the actor-critic, in particular the A3C algorithm (asynchronous actor-critic) by Mnih et al. (2016). One attraction of the methods of the actor-critic is that their explicit separation of political and Q function parameters allows for a richer behavioral space. However, it is precisely this separation that often leads to insufficient exploration: in order to achieve reasonable results, the A3C policy parameters must be regulated with entropy costs (Mnih et al., 2016). As we now show, our count-based exploration bonus leads to a significantly improved A3C performance. We first trained A3C on 60 Atari 2600 games, with and without the exploration bonus given by (6). We refer to our advanced algorithm as A3C +. From a surplus of parameters over 5 training games, we found the parameter \u03b2 = 01 to work best."}, {"heading": "7.4 Comparing exploration bonuses.", "text": "Next, we compare the effect of using different exploration bonuses derived from our density model. We consider the following variants: \u2022 no exploration bonus, \u2022 N-N (x) \u2212 1 / 2, as per MBIE-EB (Strehl and Littman, 2008); \u2022 N-N (x) \u2212 1, as per BEB (Kolter and Ng, 2009); and \u2022 PGn (x), as in connection with compression progress (Schmidhuber, 2008).The exact form of these bonuses is analogous to (6).We compare these variants after 10, 50, 100 and 200 million frames of training, using the same experimental setup as in the previous section. To compare the results over 60 games, we use interalgorithmic score distributions (Bellemare et al., 2013).Interalgorithm results are standardized so that 0 corresponds to the worst score of a game, and 1, the best."}, {"heading": "8 Future Directions", "text": "In this work, we have reconciled counting, the fundamental unit of certainty, with prediction-based heuristics and intrinsic motivation. In describing our approach, we have deliberately omitted the question of where the generalization comes from. It seems plausible that the choice of the sequential density model induces a metric about the state space. A better understanding of this induced metric should allow us to shape the density model to accelerate exploration. Universal density models such as the Solomonoff induction (Hutter, 2005) form a value that we falsify."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Laurent Orseau, Alex Graves, Joel Veness, Charles Blundell, Shakir Mohamed, Ivo Danihelka, Ian Osband, Matt Hoffman, Greg Wayne and Will Dabney for their excellent early and late feedback."}, {"heading": "A Proof of Proposition 1", "text": "By hypothesizing that the two are the two (x; x1: n). \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 n (x) = < < < (x) < (x) < (x) < (x) > Identity (x) and (x) (x; xp (i) < (x; xp (i) < (x; xp (i) < (xp (x) \u2212 n (x)). \u2212 (x) \u2212 (x) \u2212 (x). \u2212 (x). \u2212 (x) \u2212 (x). \u2212 (x). \u2212 (x). \u2212 (x). \u2212 (x). \u2212 (x). \u2212 (x; xp)."}, {"heading": "B Proof of Lemma 2", "text": "We rewrite the rule (5) to show that for each \"N\" and each \"X\" X, \"N\" (x) = \"N\" (x) = \"N\" (x).Write \"N\": = \"N.\" NowPGn (x) = \"N\" (x) = \"N\" (x) = \"N\" (x) = \"N\" (x) = \"N\" (x) = \"N\" (x) = \"N\" (n) = \"N\" (n) = \"N\" (n) = \"N\" (n) = \"N\" (n) + \"N\" (n) = \"N\" (n)."}, {"heading": "C Proof of Corollary 1", "text": "We will prove the following, which includes sequence 1 as a special case. Lemma 3. Consider \u03c6: X \u00b7 X \u0443 \u2192 R +. Let us assume that for all (xn: n: N) and each x-X1. sedan n \u2192 \u221e 1 n \u2211 x-X \u03c6 (x, x1: n) = 0, and 2. sedan n \u2192 \u221e (\u03c6 (x, x1: nx) \u2212 \u03c6 (x, x1: n) = 0.Let \u03c1n (x) be the count-based estimator: n (x) = Nn (x) + \u03c6 (x, x1: n) n + \u2211 x-shaped (x, x1: n)."}, {"heading": "If N\u0302n is the pseudo-count corresponding to \u03c1n then N\u0302n(x)/Nn(x)\u2192 1 for all x with \u00b5(x) > 0.", "text": "Condition 2 is fulfilled if \u03c6n (x, x1: n) = un (x) throughn (n) throughn (n) throughn (n) throughn (n) throughn (n), which is the case with most atomic sequential density models. Proof. We will demonstrate that the condition with respect to the rate of change required under Proposition 1 is fulfilled under the stated conditions. Let \u03c6n (x): = \u03c6 (x, x1: n), throughn (x): = throughandrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighrighri"}, {"heading": "D Experimental Methods", "text": "In fact, it is such that most of them will be able to put themselves into another world, in which they are able to put themselves into another world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which"}], "references": [{"title": "Near-optimal BRL using optimistic local transitions", "author": ["M. Araya-L\u00f3pez", "V. Thomas", "O. Buffet"], "venue": "Proceedings of the 29th International Conference on Machine Learning.", "citeRegEx": "Araya.L\u00f3pez et al\\.,? 2012", "shortCiteRegEx": "Araya.L\u00f3pez et al\\.", "year": 2012}, {"title": "Speedy Q-learning", "author": ["M.G. Azar", "R. Munos", "M. Gavamzadeh", "H.J. Kappen"], "venue": "Advances in Neural Information Processing Systems 24.", "citeRegEx": "Azar et al\\.,? 2011", "shortCiteRegEx": "Azar et al\\.", "year": 2011}, {"title": "Intrinsic motivation and reinforcement learning", "author": ["A.G. Barto"], "venue": "Intrinsically Motivated Learning in Natural and Artificial Systems, pages 17\u201347. Springer.", "citeRegEx": "Barto,? 2013", "shortCiteRegEx": "Barto", "year": 2013}, {"title": "Skip context tree switching", "author": ["M. Bellemare", "J. Veness", "E. Talvitie"], "venue": "Proceedings of the 31st International Conference on Machine Learning, pages 1458\u20131466.", "citeRegEx": "Bellemare et al\\.,? 2014", "shortCiteRegEx": "Bellemare et al\\.", "year": 2014}, {"title": "Count-based frequency estimation using bounded memory", "author": ["M.G. Bellemare"], "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence.", "citeRegEx": "Bellemare,? 2015", "shortCiteRegEx": "Bellemare", "year": 2015}, {"title": "The Arcade Learning Environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279.", "citeRegEx": "Bellemare et al\\.,? 2013", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["M.G. Bellemare", "G. Ostrovski", "A. Guez", "P.S. Thomas", "R. Munos"], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence.", "citeRegEx": "Bellemare et al\\.,? 2016", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Investigating contingency awareness using Atari 2600 games", "author": ["M.G. Bellemare", "J. Veness", "M. Bowling"], "venue": "Proceedings of the 26th AAAI Conference on Artificial Intelligence.", "citeRegEx": "Bellemare et al\\.,? 2012", "shortCiteRegEx": "Bellemare et al\\.", "year": 2012}, {"title": "Dynamic programming", "author": ["R.E. Bellman"], "venue": "Princeton University Press, Princeton, NJ.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Neuro-Dynamic Programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific.", "citeRegEx": "Bertsekas and Tsitsiklis,? 1996", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "R-max - a general polynomial time algorithm for near optimal reinforcement learning", "author": ["R. Brafman", "M. Tennenholtz"], "venue": "Journal of Machine Learning Research, 3:213\u2013231.", "citeRegEx": "Brafman and Tennenholtz,? 2002", "shortCiteRegEx": "Brafman and Tennenholtz", "year": 2002}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Machine Learning, 5(1):1\u2013122.", "citeRegEx": "Bubeck and Cesa.Bianchi,? 2012", "shortCiteRegEx": "Bubeck and Cesa.Bianchi", "year": 2012}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "John Wiley & Sons.", "citeRegEx": "Cover and Thomas,? 1991", "shortCiteRegEx": "Cover and Thomas", "year": 1991}, {"title": "Bayesian Q-learning", "author": ["R. Dearden", "N. Friedman", "S. Russell"], "venue": "Proceedings of the Fifteenth National Conference on Artificial Intelligence, pages 761\u2013768.", "citeRegEx": "Dearden et al\\.,? 1998", "shortCiteRegEx": "Dearden et al\\.", "year": 1998}, {"title": "An object-oriented representation for efficient reinforcement learning", "author": ["C. Diuk", "A. Cohen", "M.L. Littman"], "venue": "Proceedings of the 25th International Conference on Machine Learning, pages 240\u2013247. ACM.", "citeRegEx": "Diuk et al\\.,? 2008", "shortCiteRegEx": "Diuk et al\\.", "year": 2008}, {"title": "Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes", "author": ["M.O. Duff"], "venue": "PhD thesis, University of Massachusetts Amherst.", "citeRegEx": "Duff,? 2002", "shortCiteRegEx": "Duff", "year": 2002}, {"title": "Convergence of optimistic and incremental Q-learning", "author": ["E. Even-Dar", "Y. Mansour"], "venue": "Advances in Neural Information Proceesing Systems 14.", "citeRegEx": "Even.Dar and Mansour,? 2001", "shortCiteRegEx": "Even.Dar and Mansour", "year": 2001}, {"title": "Efficient bayesian parameter estimation in large discrete domains", "author": ["N. Friedman", "Y. Singer"], "venue": "Advances in Neural Information Processing Systems 11.", "citeRegEx": "Friedman and Singer,? 1999", "shortCiteRegEx": "Friedman and Singer", "year": 1999}, {"title": "Universal artificial intelligence: Sequential decisions based on algorithmic probability", "author": ["M. Hutter"], "venue": "Springer.", "citeRegEx": "Hutter,? 2005", "shortCiteRegEx": "Hutter", "year": 2005}, {"title": "Sparse adaptive dirichlet-multinomial-like processes", "author": ["M. Hutter"], "venue": "Proceedings of the Conference on Online Learning Theory.", "citeRegEx": "Hutter,? 2013", "shortCiteRegEx": "Hutter", "year": 2013}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "Journal of Machine Learning Research, 11:1563\u20131600.", "citeRegEx": "Jaksch et al\\.,? 2010", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Near-bayesian exploration in polynomial time", "author": ["Z.J. Kolter", "A.Y. Ng"], "venue": "Proceedings of the 26th International Conference on Machine Learning.", "citeRegEx": "Kolter and Ng,? 2009", "shortCiteRegEx": "Kolter and Ng", "year": 2009}, {"title": "PAC bounds for discounted MDPs", "author": ["T. Lattimore", "M. Hutter"], "venue": "Proceedings of the Conference on Algorithmic Learning Theory.", "citeRegEx": "Lattimore and Hutter,? 2012", "shortCiteRegEx": "Lattimore and Hutter", "year": 2012}, {"title": "State of the Art Control of Atari Games Using Shallow Reinforcement Learning", "author": ["Y. Liang", "M.C. Machado", "E. Talvitie", "M.H. Bowling"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems.", "citeRegEx": "Liang et al\\.,? 2016", "shortCiteRegEx": "Liang et al\\.", "year": 2016}, {"title": "Domain-independent optimistic initialization for reinforcement learning", "author": ["M.C. Machado", "S. Srinivasan", "M. Bowling"], "venue": "arXiv preprint arXiv:1410.4604.", "citeRegEx": "Machado et al\\.,? 2014", "shortCiteRegEx": "Machado et al\\.", "year": 2014}, {"title": "Hierarchical optimistic region selection driven by curiosity", "author": ["Maillard", "O.-A."], "venue": "Advances in Neural Information Processing Systems 25.", "citeRegEx": "Maillard and O..A.,? 2012", "shortCiteRegEx": "Maillard and O..A.", "year": 2012}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783.", "citeRegEx": "Mnih et al\\.,? 2016", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Variational information maximisation for intrinsically motivated reinforcement learning", "author": ["S. Mohamed", "D.J. Rezende"], "venue": "Advances in Neural Information Processing Systems 28.", "citeRegEx": "Mohamed and Rezende,? 2015", "shortCiteRegEx": "Mohamed and Rezende", "year": 2015}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "Advances in Neural Information Processing Systems, pages 2845\u20132853.", "citeRegEx": "Oh et al\\.,? 2015", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Universal knowledge-seeking agents for stochastic environments", "author": ["L. Orseau", "T. Lattimore", "M. Hutter"], "venue": "Proceedings of the Conference on Algorithmic Learning Theory.", "citeRegEx": "Orseau et al\\.,? 2013", "shortCiteRegEx": "Orseau et al\\.", "year": 2013}, {"title": "Intrinsic motivation systems for autonomous mental development", "author": ["P. Oudeyer", "F. Kaplan", "V. Hafner"], "venue": "IEEE Transactions on Evolutionary Computation, 11(2):265\u2013286.", "citeRegEx": "Oudeyer et al\\.,? 2007", "shortCiteRegEx": "Oudeyer et al\\.", "year": 2007}, {"title": "Efficient PAC-optimal exploration in concurrent, continuous state MDPs with delayed updates", "author": ["J. Pazis", "R. Parr"], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence.", "citeRegEx": "Pazis and Parr,? 2016", "shortCiteRegEx": "Pazis and Parr", "year": 2016}, {"title": "An analytic solution to discrete bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "Proceedings of the 23rd International Conference on Machine Learning.", "citeRegEx": "Poupart et al\\.,? 2006", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "CHILD: A first step towards continual learning", "author": ["M.B. Ring"], "venue": "Machine Learning, 28(1):77\u2013104.", "citeRegEx": "Ring,? 1997", "shortCiteRegEx": "Ring", "year": 1997}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Schaul et al\\.,? 2016", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "Curiosity-driven optimization", "author": ["T. Schaul", "Y. Sun", "D. Wierstra", "F. Gomez", "J. Schmidhuber"], "venue": "IEEE Congress on Evolutionary Computation.", "citeRegEx": "Schaul et al\\.,? 2011", "shortCiteRegEx": "Schaul et al\\.", "year": 2011}, {"title": "A possibility for implementing curiosity and boredom in model-building neural controllers", "author": ["J. Schmidhuber"], "venue": "From animals to animats: proceedings of the first international conference on simulation of adaptive behavior.", "citeRegEx": "Schmidhuber,? 1991", "shortCiteRegEx": "Schmidhuber", "year": 1991}, {"title": "Driven by compression progress", "author": ["J. Schmidhuber"], "venue": "Knowledge-Based Intelligent Information and Engineering Systems. Springer.", "citeRegEx": "Schmidhuber,? 2008", "shortCiteRegEx": "Schmidhuber", "year": 2008}, {"title": "Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "An intrinsic reward mechanism for efficient exploration", "author": ["O. Simsek", "A.G. Barto"], "venue": "Proceedings of the 23rd International Conference on Machine Learning.", "citeRegEx": "Simsek and Barto,? 2006", "shortCiteRegEx": "Simsek and Barto", "year": 2006}, {"title": "Intrinsically motivated reinforcement learning", "author": ["S. Singh", "A.G. Barto", "N. Chentanez"], "venue": "Advances in Neural Information Processing Systems 16.", "citeRegEx": "Singh et al\\.,? 2004", "shortCiteRegEx": "Singh et al\\.", "year": 2004}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["B.C. Stadie", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1507.00814.", "citeRegEx": "Stadie et al\\.,? 2015", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "An information-theoretic approach to curiosity-driven reinforcement learning", "author": ["S. Still", "D. Precup"], "venue": "Theory in Biosciences, 131(3):139\u2013148.", "citeRegEx": "Still and Precup,? 2012", "shortCiteRegEx": "Still and Precup", "year": 2012}, {"title": "An analysis of model-based interval estimation for Markov decision processes", "author": ["A.L. Strehl", "M.L. Littman"], "venue": "Journal of Computer and System Sciences, 74(8):1309 \u2013 1331.", "citeRegEx": "Strehl and Littman,? 2008", "shortCiteRegEx": "Strehl and Littman", "year": 2008}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["I. Szita", "C. Szepesv\u00e1ri"], "venue": "Proceedings of the 27th International Conference on Machine Learning.", "citeRegEx": "Szita and Szepesv\u00e1ri,? 2010", "shortCiteRegEx": "Szita and Szepesv\u00e1ri", "year": 2010}, {"title": "The role of exploration in learning control", "author": ["S.B. Thrun"], "venue": "Handbook of Intelligent Control: Neural, Fuzzy and Adaptive Approaches, pages 1\u201327.", "citeRegEx": "Thrun,? 1992", "shortCiteRegEx": "Thrun", "year": 1992}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Compress and control", "author": ["J. Veness", "M.G. Bellemare", "M. Hutter", "A. Chua", "G. Desjardins"], "venue": "Proceedings of the 29th AAAI Conference on Artificial Intelligence.", "citeRegEx": "Veness et al\\.,? 2015", "shortCiteRegEx": "Veness et al\\.", "year": 2015}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning, 1(1-2):1\u2013305.", "citeRegEx": "Wainwright and Jordan,? 2008", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Dual representations for dynamic programming", "author": ["T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans"], "venue": "Journal of Machine Learning Research, pages 1\u201329.", "citeRegEx": "Wang et al\\.,? 2008", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Motivation reconsidered: the concept of competence", "author": ["R.W. White"], "venue": "Psychological review, 66(5):297.", "citeRegEx": "White,? 1959", "shortCiteRegEx": "White", "year": 1959}, {"title": "The CTS sequential density model treats x \u2208 X as a factored observation, where each (i, j) pixel corresponds to a factor x . The parents of this factor are its upper-left neighbours, i.e. pixels (i \u2212 1, j)", "author": ["Bellemare"], "venue": null, "citeRegEx": "Bellemare,? \\Q2015\\E", "shortCiteRegEx": "Bellemare", "year": 2015}, {"title": "tain a sufficiently good approximation to the value function, and performance quickly deteriorates. Comparable results using the A3C method provide another example of the practical importance of eligibility traces and return-based methods in reinforcement learning", "author": ["Mnih"], "venue": null, "citeRegEx": "Mnih,? \\Q2016\\E", "shortCiteRegEx": "Mnih", "year": 2016}, {"title": "Our experiments involved the stochastic version of the Arcade Learning Environment (ALE) without a terminal signal for life loss, which is now the default ALE setting. Briefly, the stochasticity is achieved by accepting the agent action at each frame with probability 1\u2212 p and using the agents previous action during rejection. We used the ALE\u2019s default value of p = 0.25", "author": ["Bellemare"], "venue": null, "citeRegEx": "Bellemare,? \\Q2016\\E", "shortCiteRegEx": "Bellemare", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Most Bayesian exploration schemes use counts in their posterior estimates, for example when the uncertainty over transition probabilities is captured by an exponential family prior (Dearden et al., 1998; Duff, 2002; Poupart et al., 2006).", "startOffset": 181, "endOffset": 237}, {"referenceID": 15, "context": "Most Bayesian exploration schemes use counts in their posterior estimates, for example when the uncertainty over transition probabilities is captured by an exponential family prior (Dearden et al., 1998; Duff, 2002; Poupart et al., 2006).", "startOffset": 181, "endOffset": 237}, {"referenceID": 33, "context": "Most Bayesian exploration schemes use counts in their posterior estimates, for example when the uncertainty over transition probabilities is captured by an exponential family prior (Dearden et al., 1998; Duff, 2002; Poupart et al., 2006).", "startOffset": 181, "endOffset": 237}, {"referenceID": 11, "context": "There are near-optimal algorithms for multi-armed bandits (Bubeck and Cesa-Bianchi, 2012); the hardness of exploration in Markov Decision Processes (MDPs) is well-understood (Jaksch et al.", "startOffset": 58, "endOffset": 89}, {"referenceID": 20, "context": "There are near-optimal algorithms for multi-armed bandits (Bubeck and Cesa-Bianchi, 2012); the hardness of exploration in Markov Decision Processes (MDPs) is well-understood (Jaksch et al., 2010; Lattimore and Hutter, 2012).", "startOffset": 174, "endOffset": 223}, {"referenceID": 22, "context": "There are near-optimal algorithms for multi-armed bandits (Bubeck and Cesa-Bianchi, 2012); the hardness of exploration in Markov Decision Processes (MDPs) is well-understood (Jaksch et al., 2010; Lattimore and Hutter, 2012).", "startOffset": 174, "endOffset": 223}, {"referenceID": 39, "context": "By stark contrast, contemporary practical successes in reinforcement learning (e.g. Mnih et al., 2015; Silver et al., 2016) still rely on simple forms of exploration, for example -greedy policies \u2013 what Thrun (1992) calls undirected exploration.", "startOffset": 78, "endOffset": 123}, {"referenceID": 32, "context": "However, save for some recent work in continuous state spaces for which a good metric is given (Pazis and Parr, 2016), there has not yet been a thoroughly convincing attempt at generalizing counts.", "startOffset": 95, "endOffset": 117}, {"referenceID": 37, "context": "Intrinsic motivation (Schmidhuber, 1991; Oudeyer et al., 2007; Barto, 2013) offers a different perspective on exploration.", "startOffset": 21, "endOffset": 75}, {"referenceID": 31, "context": "Intrinsic motivation (Schmidhuber, 1991; Oudeyer et al., 2007; Barto, 2013) offers a different perspective on exploration.", "startOffset": 21, "endOffset": 75}, {"referenceID": 2, "context": "Intrinsic motivation (Schmidhuber, 1991; Oudeyer et al., 2007; Barto, 2013) offers a different perspective on exploration.", "startOffset": 21, "endOffset": 75}, {"referenceID": 10, "context": "There are near-optimal algorithms for multi-armed bandits (Bubeck and Cesa-Bianchi, 2012); the hardness of exploration in Markov Decision Processes (MDPs) is well-understood (Jaksch et al., 2010; Lattimore and Hutter, 2012). By stark contrast, contemporary practical successes in reinforcement learning (e.g. Mnih et al., 2015; Silver et al., 2016) still rely on simple forms of exploration, for example -greedy policies \u2013 what Thrun (1992) calls undirected exploration.", "startOffset": 59, "endOffset": 441}, {"referenceID": 51, "context": "rogates for extrinsic rewards \u2013 to drive curiosity within an agent, influenced by classic ideas from psychology (White, 1959).", "startOffset": 112, "endOffset": 125}, {"referenceID": 41, "context": "To sketch out some recurring themes, these novelty signals might be prediction error (Singh et al., 2004; Stadie et al., 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015).", "startOffset": 85, "endOffset": 126}, {"referenceID": 42, "context": "To sketch out some recurring themes, these novelty signals might be prediction error (Singh et al., 2004; Stadie et al., 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015).", "startOffset": 85, "endOffset": 126}, {"referenceID": 40, "context": ", 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015).", "startOffset": 21, "endOffset": 45}, {"referenceID": 37, "context": ", 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015).", "startOffset": 65, "endOffset": 84}, {"referenceID": 43, "context": ", 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015).", "startOffset": 108, "endOffset": 159}, {"referenceID": 28, "context": ", 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015).", "startOffset": 108, "endOffset": 159}, {"referenceID": 34, "context": "The idea also finds roots in continual learning (Ring, 1997).", "startOffset": 48, "endOffset": 60}, {"referenceID": 5, "context": "We bring them to bear on Atari 2600 games from the Arcade Learning Environment (Bellemare et al., 2013), focusing on games where myopic exploration fails.", "startOffset": 79, "endOffset": 103}, {"referenceID": 44, "context": "We extract our pseudo-counts from a simple sequential model and use them within a variant of model-based interval estimation with bonuses (Strehl and Littman, 2008).", "startOffset": 138, "endOffset": 164}, {"referenceID": 2, "context": ", 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015). The idea also finds roots in continual learning (Ring, 1997). In Thrun\u2019s taxonomy, intrinsic motivation methods fall within the category of error-based exploration. However, while in toto a promising approach, intrinsic motivation has so far yielded few theoretical guarantees, save perhaps for the work of Maillard (2012) and the universally curious agent of Orseau et al.", "startOffset": 33, "endOffset": 484}, {"referenceID": 2, "context": ", 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015). The idea also finds roots in continual learning (Ring, 1997). In Thrun\u2019s taxonomy, intrinsic motivation methods fall within the category of error-based exploration. However, while in toto a promising approach, intrinsic motivation has so far yielded few theoretical guarantees, save perhaps for the work of Maillard (2012) and the universally curious agent of Orseau et al. (2013). We provide what we believe is the first formal evidence that intrinsic motivation and count-based exploration are but two sides of the same coin.", "startOffset": 33, "endOffset": 542}, {"referenceID": 48, "context": "When Y is small and finite and we are given a sequential density model \u03c1 over X , we can construct a reasonable sequential density model over the joint alphabet X \u00d7Y by means of a chain rule-like construct (Veness et al., 2015).", "startOffset": 206, "endOffset": 227}, {"referenceID": 44, "context": "Model-based interval estimation (MBIE; Strehl and Littman, 2008) explores by acting optimistically with respect to the agent\u2019s empirical model of the environment.", "startOffset": 32, "endOffset": 64}, {"referenceID": 20, "context": "A closely related algorithm, UCRL2, exhibits low regret in the average-reward setting (Jaksch et al., 2010).", "startOffset": 86, "endOffset": 107}, {"referenceID": 8, "context": "Model-based interval estimation with exploratory bonus (MBIE-EB) is a simplification which augments the estimated reward function at every state and solves Bellman\u2019s equation (Bellman, 1957):", "startOffset": 175, "endOffset": 190}, {"referenceID": 21, "context": "Bayesian Exploration Bonus (BEB; Kolter and Ng, 2009) replaces the empirical estimates P\u0302 and R\u0302 with Dirichlet estimators.", "startOffset": 27, "endOffset": 53}, {"referenceID": 19, "context": "Nor is the problem solved by a Bayesian approach: even variable-alphabet estimators (e.g. Friedman and Singer, 1999; Hutter, 2013; Bellemare, 2015) must assign a small, diminishing probability to yet-unseen states.", "startOffset": 84, "endOffset": 147}, {"referenceID": 4, "context": "Nor is the problem solved by a Bayesian approach: even variable-alphabet estimators (e.g. Friedman and Singer, 1999; Hutter, 2013; Bellemare, 2015) must assign a small, diminishing probability to yet-unseen states.", "startOffset": 84, "endOffset": 147}, {"referenceID": 12, "context": "The term \u201crecoding\u201d is inspired from the statistical compression literature, where coding costs are inversely related to probabilities (Cover and Thomas, 1991).", "startOffset": 135, "endOffset": 159}, {"referenceID": 5, "context": "We use the Arcade Learning Environment (Bellemare et al., 2013).", "startOffset": 39, "endOffset": 63}, {"referenceID": 9, "context": "Echoing the pioneering work of Diuk et al. (2008) on the Atari 2600, we define the \u201csalient event\u201d to occur whenever the agent is located in the area rightwards from the starting screen.", "startOffset": 31, "endOffset": 50}, {"referenceID": 3, "context": "We use a simplified version of the CTS sequential density model for Atari 2600 frames proposed by Bellemare et al. (2014) and used for density estimation proper by Veness et al.", "startOffset": 98, "endOffset": 122}, {"referenceID": 3, "context": "We use a simplified version of the CTS sequential density model for Atari 2600 frames proposed by Bellemare et al. (2014) and used for density estimation proper by Veness et al. (2015). While the CTS model is rather impoverished in comparison to state-of-the-art modelling algorithms (e.", "startOffset": 98, "endOffset": 185}, {"referenceID": 48, "context": "Finally, we expose a relationship between our pseudo-count and a density-based approach to value function approximation called Compress and Control (Veness et al., 2015).", "startOffset": 148, "endOffset": 169}, {"referenceID": 49, "context": "We next show that directed graphical models (Wainwright and Jordan, 2008) satisfy Assumption 1.", "startOffset": 44, "endOffset": 73}, {"referenceID": 48, "context": "The Compress and Control method (Veness et al., 2015) applies sequential density models to the problem of reinforcement learning.", "startOffset": 32, "endOffset": 53}, {"referenceID": 50, "context": "It uses these models to learn a form of dual value function based on stationary distributions (Wang et al., 2008).", "startOffset": 94, "endOffset": 113}, {"referenceID": 1, "context": "We believe this relationship is the natural extension of the close relationship between tabular value estimation using a decaying step-size (e.g. Even-Dar and Mansour, 2001; Azar et al., 2011) and count-based exploration in the tabular setting (e.", "startOffset": 140, "endOffset": 192}, {"referenceID": 44, "context": ", 2011) and count-based exploration in the tabular setting (e.g. Brafman and Tennenholtz, 2002; Strehl and Littman, 2008; Jaksch et al., 2010; Szita and Szepesv\u00e1ri, 2010).", "startOffset": 59, "endOffset": 170}, {"referenceID": 20, "context": ", 2011) and count-based exploration in the tabular setting (e.g. Brafman and Tennenholtz, 2002; Strehl and Littman, 2008; Jaksch et al., 2010; Szita and Szepesv\u00e1ri, 2010).", "startOffset": 59, "endOffset": 170}, {"referenceID": 45, "context": ", 2011) and count-based exploration in the tabular setting (e.g. Brafman and Tennenholtz, 2002; Strehl and Littman, 2008; Jaksch et al., 2010; Szita and Szepesv\u00e1ri, 2010).", "startOffset": 59, "endOffset": 170}, {"referenceID": 48, "context": "which Veness et al. (2015) showed converges to V (x) when (x, z)1:n is drawn from the ergodic Markov chain jointly induced by a finite-state MDP and \u03c0.", "startOffset": 6, "endOffset": 27}, {"referenceID": 2, "context": "Intrinsic motivation algorithms seek to explain what drives behaviour in the absence of, or even contrary to, extrinsic reward (Barto, 2013).", "startOffset": 127, "endOffset": 140}, {"referenceID": 30, "context": "Information gain is a frequently appealed-to quantity in the intrinsic motivation literature (e.g. Schaul et al., 2011; Orseau et al., 2013).", "startOffset": 93, "endOffset": 140}, {"referenceID": 30, "context": "In a Bayesian sense, expected information gain is the appropriate measure of exploration when extrinsic rewards are ignored: an agent which maximizes long-term expected information gain is \u201coptimally curious\u201d (Orseau et al., 2013).", "startOffset": 209, "endOffset": 230}, {"referenceID": 21, "context": "Since an exploration bonus proportional to Nn(x, a) \u22121 leads to PAC-BAMDP algorithms (Kolter and Ng, 2009; Araya-L\u00f3pez et al., 2012), we hypothesize that similar bounds can be derived for our pseudo-counts.", "startOffset": 85, "endOffset": 132}, {"referenceID": 0, "context": "Since an exploration bonus proportional to Nn(x, a) \u22121 leads to PAC-BAMDP algorithms (Kolter and Ng, 2009; Araya-L\u00f3pez et al., 2012), we hypothesize that similar bounds can be derived for our pseudo-counts.", "startOffset": 85, "endOffset": 132}, {"referenceID": 35, "context": "To date, the best agents require hundreds of millions of frames to attain nontrivial performance levels, and visit at best two or three rooms out of 72; most published agents (e.g. Bellemare et al., 2012; van Hasselt et al., 2016; Schaul et al., 2016; Mnih et al., 2016; Liang et al., 2016) fail to match even a human beginner\u2019s performance.", "startOffset": 175, "endOffset": 290}, {"referenceID": 26, "context": "To date, the best agents require hundreds of millions of frames to attain nontrivial performance levels, and visit at best two or three rooms out of 72; most published agents (e.g. Bellemare et al., 2012; van Hasselt et al., 2016; Schaul et al., 2016; Mnih et al., 2016; Liang et al., 2016) fail to match even a human beginner\u2019s performance.", "startOffset": 175, "endOffset": 290}, {"referenceID": 23, "context": "To date, the best agents require hundreds of millions of frames to attain nontrivial performance levels, and visit at best two or three rooms out of 72; most published agents (e.g. Bellemare et al., 2012; van Hasselt et al., 2016; Schaul et al., 2016; Mnih et al., 2016; Liang et al., 2016) fail to match even a human beginner\u2019s performance.", "startOffset": 175, "endOffset": 290}, {"referenceID": 24, "context": "We also compare using exploration bonuses to the optimistic initialization trick proposed by (Machado et al., 2014).", "startOffset": 93, "endOffset": 115}, {"referenceID": 26, "context": "This very separation, however, often leads to deficient exploration: to produce any sensible results, the A3C policy parameters must be regularized with an entropy cost (Mnih et al., 2016).", "startOffset": 169, "endOffset": 188}, {"referenceID": 5, "context": "To demonstrate the benefits of augmenting A3C with our exploration bonus, we computed a baseline score (Bellemare et al., 2013) for A3C+ over time.", "startOffset": 103, "endOffset": 127}, {"referenceID": 21, "context": "We next turn our attention to actor-critic methods, specifically the A3C (asynchronous actor-critic) algorithm of Mnih et al. (2016). One appeal of actor-critic methods is that their explicit separation of policy and Q-function parameters allows for a richer behaviour space.", "startOffset": 114, "endOffset": 133}, {"referenceID": 44, "context": "We consider the following variants: \u2022 no exploration bonus, \u2022 N\u0302n(x), as per MBIE-EB (Strehl and Littman, 2008); \u2022 N\u0302n(x), as per BEB (Kolter and Ng, 2009); and \u2022 PGn(x), related to compression progress (Schmidhuber, 2008).", "startOffset": 85, "endOffset": 111}, {"referenceID": 21, "context": "We consider the following variants: \u2022 no exploration bonus, \u2022 N\u0302n(x), as per MBIE-EB (Strehl and Littman, 2008); \u2022 N\u0302n(x), as per BEB (Kolter and Ng, 2009); and \u2022 PGn(x), related to compression progress (Schmidhuber, 2008).", "startOffset": 134, "endOffset": 155}, {"referenceID": 38, "context": "We consider the following variants: \u2022 no exploration bonus, \u2022 N\u0302n(x), as per MBIE-EB (Strehl and Littman, 2008); \u2022 N\u0302n(x), as per BEB (Kolter and Ng, 2009); and \u2022 PGn(x), related to compression progress (Schmidhuber, 2008).", "startOffset": 203, "endOffset": 222}, {"referenceID": 5, "context": "compare scores across 60 games, we use inter-algorithm score distributions (Bellemare et al., 2013).", "startOffset": 75, "endOffset": 99}, {"referenceID": 18, "context": "Universal density models such as Solomonoff induction (Hutter, 2005) learn the structure of the state space at a much greater rate than the empirical estimator, violating Assumption 1b.", "startOffset": 54, "endOffset": 68}], "year": 2016, "abstractText": "We consider an agent\u2019s uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use sequential density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary sequential density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult MONTEZUMA\u2019S REVENGE.", "creator": "LaTeX with hyperref package"}}}