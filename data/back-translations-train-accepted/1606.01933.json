{"id": "1606.01933", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "A Decomposable Attention Model for Natural Language Inference", "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.", "histories": [["v1", "Mon, 6 Jun 2016 20:30:57 GMT  (61kb,D)", "http://arxiv.org/abs/1606.01933v1", "5 pages, 1 figure"], ["v2", "Sun, 25 Sep 2016 23:52:45 GMT  (63kb,D)", "http://arxiv.org/abs/1606.01933v2", "7 pages, 1 figure, Proceeedings of EMNLP 2016"]], "COMMENTS": "5 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ankur p parikh", "oscar t\u00e4ckstr\u00f6m", "dipanjan das 0001", "jakob uszkoreit"], "accepted": true, "id": "1606.01933"}, "pdf": {"name": "1606.01933.pdf", "metadata": {"source": "CRF", "title": "A Decomposable Attention Model for Natural Language Inference", "authors": ["Ankur P. Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das"], "emails": ["aparikh@google.com", "oscart@google.com", "dipanjand@google.com", "uszkoreit@google.com"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Work", "text": "Our method is motivated by the central role of alignment in machine translation (Koehn, 2009) and previous approaches to modelling sentence similarities (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010; Fader et al., 2013). The neural counterpart to alignment, attention (Bahdanau et al., 2015), which is a central component of our approach, was originally proposed and has been used mainly in conjunction with LSTMs (Rockta \ufffd schel et al., 2016; Wang and Jiang, 2015) and to a lesser extent with CNNs (Yin et al., 2016)."}, {"heading": "3 Approach", "text": "Let a = (a1,.., a'a) and b = (b1,.., b'b) be the two input sentences of length \"a\" and \"b.\" We assume that each ai, bj \"Rd\" is a word embedding a vector of dimension d, and that each sentence is preceded by a \"NULL\" character. Our training data comes in the form of designated pairs {a (n), b (n), y (n)} Nn = 1, where y (n) 1,..., y (n) C \"is an indicator vector that encodes the label and C is the number of output classes. At the test date, we get a sentence pair (a, b) Nn = 1, and our goal is to predict the correct designation y.input representation."}, {"heading": "3.1 Attend", "text": "We first get unnormalized attention weights eij, calculated by a function F, \"which decomposes as follows: eij: = F\" (a-i, b-j): = F (a-i) TF (b-j). (1) This decomposition avoids the quadratic complexity that would be associated with a separate application of F \"a-b times. Instead, only\" a + \"b applications of F. We assume F as a leading neural network with ReLU activations (Glorot et al., 2011). These attention weights are normalized as follows: \u03b2i: =\" b-j = 1 Exp (eij)."}, {"heading": "3.2 Compare", "text": "Next, we compare the aligned phrases {(a-i, \u03b2i) \"ai = 1 and {(b-i, \u03b1j)\" b-j = 1 using a function G, which in this work is again a forward-directed network: v1, i: = G ([a-i, \u03b2i]) as well as [1,...], [a], v2, j: = G ([b-j, \u03b1j]) and [1,... \"b]. (3) where the brackets denote concatenation. Since there is only a linear number of terms in this case, we do not need to apply decomposition as in the previous step. Thus, G can consider both a-i and \u03b2i together."}, {"heading": "3.3 Aggregate", "text": "We now have two sets of comparison vectors {v1, i}'ai = 1 and {v2, j}'b = 1. We first aggregate each quantity by adding up: v1 = \"a-a-i = 1 v1, i, v2 = 'b-j = 1 v2, j. (4) and feed the result through a definitive feedback forward network classifier H: y = H ([v1, v2]), (5) where y-RC represents the predicted (unnormalized) values for each class and consequently the predicted class by y = argmaxiy = i. For training we use a cross-class entropy loss with drop-out regulation (Srivastava et al., 2014): L (advantageF, advantageG, prevH) = 1N N-N-N-N-1 C-c = 1 y (n) c-entropy loss with drop-out regulation (prevava, 2014), prevaable, F-dividable (Sstava, F)."}, {"heading": "3.4 Intra-Sentence Attention (Optional)", "text": "In the model above, the input representations are simple word embeddings. Similar to Equations 1 and 2, however, we can extend this input representation to include intrasentence attention to encode compositional relationships between words within each sentence, as proposed by Cheng et al. (2016). Similar to Eqs 1 and 2, we define Fintra: = Fintra (ai) TFintra (aj), (6) where Fintra is a forward-looking network. We then generate the self-aligned phrases i: = 'a \u2211 j = 1 Exp (fij + di \u2212 j) \u2211'a = 1 Exp (fik + di \u2212 j) aj. (7) The distance-dependent bias terms di \u2212 j \u2032 R provide the model with a minimal amount of sequence information while remaining parallelizable. These terms are coupled together so that all distances greater than 10 words share the same preference. The input representation for the following steps is analogous to i [i \u00b2] and i [i]."}, {"heading": "4 Computational Complexity", "text": "We will now discuss the asymptotic complexity of our approach and how it provides a higher degree of parallelism than LSTM-based approaches. Let's remember that d means the embedding of dimension and \"sentence length. For simplicity's sake, we assume that all hidden dimensions are d and that the complexity of the matrix (d \u00b7 d) vector (d \u00b7 1) multiplication is O (d2). A key assumption of our analysis is that\" < d \"is what we consider reasonable and applies to the SNLI dataset (Bowman et al., 2015) where\" < 80, whereas newer LSTM-based approaches have used d \u2265 300. This assumption allows us to limit the complexity of calculating the \"2 Attention. Complexity of LSTMs. An LSTM cell is O (d2), resulting in a complexity (2) of one (dO)."}, {"heading": "5 Experiments", "text": "We evaluate our approach using the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Faced with a pair of sentences (a, b), the task is to predict whether b contradicts a, b contradicts a, or whether their relationship is neutral. Following Bowman et al. (2015), we remove examples labeled \"-\" (not a gold label) from the dataset that leaves 549,367 pairs for training, 9,842 for development, and 9,824 for testing. We use the tokenized sentences specified in the dataset as input into our methodology. We use 300-dimensional GloVe embeddings (Pennington et al., 2014) to represent words projected down to 200 dimensions, a number determined by hyperparameter tuning. OOV words are hacked onto one of 100 random embeddings (embeddings). All inserts remain fixed during the project matrix, but the training is performed."}, {"heading": "6 Conclusion", "text": "We presented a simple, attention-based approach to natural language conclusions that can be trivially parallelized, and this approach surpasses much more complex neural methods aimed at understanding text. Our results suggest that, for at least this task, pair-by-pair comparisons are relatively more important than global sentence-level representations."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Abadi et al.2015] Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A brief history of natural logic. College Publications", "author": ["Johan van Benthem"], "venue": null, "citeRegEx": "Benthem.,? \\Q2008\\E", "shortCiteRegEx": "Benthem.", "year": 2008}, {"title": "Recognising textual entailment with logical inference", "author": ["Bos", "Markert2005] Johan Bos", "Katja Markert"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Bos et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bos et al\\.", "year": 2005}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "Proceedings of EMNLP", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts"], "venue": "Proceedings of ACL", "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Discriminative learning over constrained latent representations", "author": ["Chang et al.2010] Ming-Wei Chang", "Dan Goldwasser", "Dan Roth", "Vivek Srikumar"], "venue": "In Proceedings of HLT-NAACL", "citeRegEx": "Chang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2010}, {"title": "Long short-term memory-networks for machine reading", "author": ["Cheng et al.2016] Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Paraphrase identification as probabilistic quasisynchronous recognition", "author": ["Das", "Smith2009] Dipanjan Das", "Noah A. Smith"], "venue": "In Proceedings of ACLIJCNLP", "citeRegEx": "Das et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Das et al\\.", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Fader et al.2013] Anthony Fader", "Luke S Zettlemoyer", "Oren Etzioni"], "venue": "In Proceedings of ACL", "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of AISTATS", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Robust textual inference via graph matching", "author": ["Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of HLTNAACL", "citeRegEx": "Haghighi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2005}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "In Advances in NIPS", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Statistical machine translation", "author": ["Philipp Koehn"], "venue": null, "citeRegEx": "Koehn.,? \\Q2009\\E", "shortCiteRegEx": "Koehn.", "year": 2009}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["LeCun et al.1990] Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Advances in NIPS", "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "An extended model of natural logic", "author": ["MacCartney", "Manning2009] Bill MacCartney", "Christopher D. Manning"], "venue": "In Proceedings of the IWCS", "citeRegEx": "MacCartney et al\\.,? \\Q2009\\E", "shortCiteRegEx": "MacCartney et al\\.", "year": 2009}, {"title": "Natural language inference by tree-based convolution and heuristic matching", "author": ["Mou et al.2015] Lili Mou", "Men Rui", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin"], "venue": "In Proceedings of ACL (short papers)", "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "GloVe: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "In Proceedings of ICLR", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Order-embeddings of images and language", "author": ["Vendrov et al.2015] Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun"], "venue": "In Proceedings of ICLR", "citeRegEx": "Vendrov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vendrov et al\\.", "year": 2015}, {"title": "Learning natural language inference with LSTM", "author": ["Wang", "Jiang2015] Shuohang Wang", "Jing Jiang"], "venue": "In Proceedings of NAACL", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "ABCNN: Attentionbased convolutional neural network for modeling sentence pairs", "author": ["Yin et al.2016] Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou"], "venue": "In Transactions of the Association of Computational Linguistics", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "NLI is a central problem in language understanding (Katz, 1972; Bos and Markert, 2005; Benthem, 2008; MacCartney and Manning, 2009) and recently the large SNLI corpus of 570K sentence pairs was created for this task (Bowman et al.", "startOffset": 51, "endOffset": 131}, {"referenceID": 4, "context": "NLI is a central problem in language understanding (Katz, 1972; Bos and Markert, 2005; Benthem, 2008; MacCartney and Manning, 2009) and recently the large SNLI corpus of 570K sentence pairs was created for this task (Bowman et al., 2015).", "startOffset": 216, "endOffset": 237}, {"referenceID": 1, "context": "Given two sentences, where each word is represented by an embedding vector, we first create a soft alignment matrix using neural attention (Bahdanau et al., 2015).", "startOffset": 139, "endOffset": 162}, {"referenceID": 7, "context": "In addition, we optionally apply intra-sentence attention (Cheng et al., 2016) to endow the model with a richer encoding of substructures prior to the alignment step.", "startOffset": 58, "endOffset": 78}, {"referenceID": 15, "context": "Our method is motivated by the central role played by alignment in machine translation (Koehn, 2009) and previous approaches to sentence similarity modeling (Haghighi et al.", "startOffset": 87, "endOffset": 100}, {"referenceID": 12, "context": "Our method is motivated by the central role played by alignment in machine translation (Koehn, 2009) and previous approaches to sentence similarity modeling (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010; Fader et al., 2013).", "startOffset": 157, "endOffset": 241}, {"referenceID": 6, "context": "Our method is motivated by the central role played by alignment in machine translation (Koehn, 2009) and previous approaches to sentence similarity modeling (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010; Fader et al., 2013).", "startOffset": 157, "endOffset": 241}, {"referenceID": 10, "context": "Our method is motivated by the central role played by alignment in machine translation (Koehn, 2009) and previous approaches to sentence similarity modeling (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010; Fader et al., 2013).", "startOffset": 157, "endOffset": 241}, {"referenceID": 1, "context": "The neural counterpart to alignment, attention (Bahdanau et al., 2015), which is a key part of our approach, was originally proposed and has been predominantly used in conjunction with LSTMs (Rockt\u00e4schel et al.", "startOffset": 47, "endOffset": 70}, {"referenceID": 20, "context": ", 2015), which is a key part of our approach, was originally proposed and has been predominantly used in conjunction with LSTMs (Rockt\u00e4schel et al., 2016; Wang and Jiang, 2015) and to a lesser extent with CNNs (Yin et al.", "startOffset": 128, "endOffset": 176}, {"referenceID": 24, "context": ", 2016; Wang and Jiang, 2015) and to a lesser extent with CNNs (Yin et al., 2016).", "startOffset": 63, "endOffset": 81}, {"referenceID": 1, "context": "First, soft-align the elements of \u0101 and b\u0304 using a variant of neural attention (Bahdanau et al., 2015) and decompose the problem into the comparison of aligned subphrases.", "startOffset": 79, "endOffset": 102}, {"referenceID": 11, "context": "We take F to be a feed-forward neural network with ReLU activations (Glorot et al., 2011).", "startOffset": 68, "endOffset": 89}, {"referenceID": 21, "context": "For training, we use multi-class cross-entropy loss with dropout regularization (Srivastava et al., 2014):", "startOffset": 80, "endOffset": 105}, {"referenceID": 7, "context": "However, we can augment this input representation with intra-sentence attention to encode compositional relationships between words within each sentence, as proposed by Cheng et al. (2016). Similar to Eqs.", "startOffset": 169, "endOffset": 189}, {"referenceID": 4, "context": "A key assumption of our analysis is that ` < d, which we believe is reasonable and is true of the SNLI dataset (Bowman et al., 2015) where ` < 80, whereas recent LSTM-based approaches have used d \u2265 300.", "startOffset": 111, "endOffset": 132}, {"referenceID": 20, "context": "Adding attention as in Rockt\u00e4schel et al. (2016) increases this complexity to O(`d2 + `2d).", "startOffset": 23, "endOffset": 49}, {"referenceID": 4, "context": "Lexicalized Classifier (Bowman et al., 2015) 99.", "startOffset": 23, "endOffset": 44}, {"referenceID": 5, "context": "300D LSTM LSTM RNN encoders (Bowman et al., 2016) 83.", "startOffset": 28, "endOffset": 49}, {"referenceID": 22, "context": "0M 1024D pretrained GRU encoders (Vendrov et al., 2015) 98.", "startOffset": 33, "endOffset": 55}, {"referenceID": 18, "context": "0M 300D Tree-based CNN encoders (Mou et al., 2015) 83.", "startOffset": 32, "endOffset": 50}, {"referenceID": 5, "context": "5M 300D SPINN-NP encoders (Bowman et al., 2016) 89.", "startOffset": 26, "endOffset": 47}, {"referenceID": 20, "context": "100D LSTM with attention (Rockt\u00e4schel et al., 2016) 85.", "startOffset": 25, "endOffset": 51}, {"referenceID": 7, "context": "9M 450D LSTMN with deep attention fusion (Cheng et al., 2016) 88.", "startOffset": 41, "endOffset": 61}, {"referenceID": 7, "context": "of the non-peer reviewed approach of Cheng et al. (2016) is included for reference, noting that this result may be preliminary.", "startOffset": 37, "endOffset": 57}, {"referenceID": 4, "context": "We evaluate our approach on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).", "startOffset": 83, "endOffset": 104}, {"referenceID": 19, "context": "We use 300 dimensional GloVe embeddings (Pennington et al., 2014) to represent words, projected down to 200 dimensions, a number determined via hyperparameter tuning.", "startOffset": 40, "endOffset": 65}, {"referenceID": 4, "context": "We evaluate our approach on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Given a sentences pair (a,b), the task is to predict whether b is entailed by a, b contradicts a, or whether their relationship is neutral. Following Bowman et al. (2015), we remove examples labeled \u201c\u2013\u201d (no gold label) from the dataset, which leaves 549,367 pairs for training, 9,842 for development, and 9,824 for testing.", "startOffset": 84, "endOffset": 277}, {"referenceID": 0, "context": "plemented in TensorFlow (Abadi et al., 2015).", "startOffset": 24, "endOffset": 44}, {"referenceID": 9, "context": "Each hyperparameter setting was run on a single machine with 10 asynchronous gradient-update threads, using Adagrad (Duchi et al., 2011) for optimization.", "startOffset": 116, "endOffset": 136}, {"referenceID": 7, "context": "Our approach, which ignores word order, achieves state-of-the-art results with almost an order of magnitude fewer parameters than the LSTMN of Cheng et al. (2016). Adding intra-sentence attention gives a considerable improvement of 0.", "startOffset": 143, "endOffset": 163}], "year": 2016, "abstractText": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.", "creator": "LaTeX with hyperref package"}}}