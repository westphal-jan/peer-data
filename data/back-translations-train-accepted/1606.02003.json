{"id": "1606.02003", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Memory-enhanced Decoder for Neural Machine Translation", "abstract": "We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memory-enhanced RNN decoder is called \\textsc{MemDec}. At each time during decoding, \\textsc{MemDec} will read from this memory and write to this memory once, both with content-based addressing. Unlike the unbounded memory in previous work\\cite{RNNsearch} to store the representation of source sentence, the memory in \\textsc{MemDec} is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step. Our empirical study on Chinese-English translation shows that it can improve by $4.8$ BLEU upon Groundhog and $5.3$ BLEU upon on Moses, yielding the best performance achieved with the same training set.", "histories": [["v1", "Tue, 7 Jun 2016 02:28:19 GMT  (148kb,D)", "http://arxiv.org/abs/1606.02003v1", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mingxuan wang", "zhengdong lu", "hang li", "qun liu"], "accepted": true, "id": "1606.02003"}, "pdf": {"name": "1606.02003.pdf", "metadata": {"source": "CRF", "title": "Memory-enhanced Decoder for Neural Machine Translation", "authors": ["Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Qun Liu"], "emails": ["wangmingxuan@ict.ac.cn", "liuqun@ict.ac.cn", "Lu.Zhengdong@huawei.com", "HangLi.HL@huawei.com"], "sections": [{"heading": "1 Introduction", "text": "The introduction of external memory has significantly expanded the representational ability of a neural network to model sequences (Graves et al., 2014) by providing flexible ways of storing and accessing information. Specifically, in neural machine translation, there has been a major improvement in the source code in a sentence memory and dynamic access to the relevant sequences (such as decrypting content). The success of RNsearch demonstrated the advantage of storing in a whole set of arrangements."}, {"heading": "2 Neural machine translation with attention", "text": "Our work is based on an attention-based NMT (Bahdanau et al., 2014), which represents the source set as a sequence of vectors after it has been processed by RNN or bi-directional RNNNs, and then dynamically aligns and generates the target set with another RNN at the same time. (Attention-based NMT, with RNNsearch as its most popular representative, generalizes the conventional term of the encoder decoder when using an unlimited amount of memory for the interim representation of the source set and the content-based addressing read in decoding (Figure 1.). More precisely, RNNN search first receives context vector ct after reading the source representation MS, which is then used to update the state and generate the word yt (along with the current hidden state st), and the previously generated word yi 1., formal \u2212 input sequence [xx1]."}, {"heading": "2.1 Improved Attention Model", "text": "The alignment model \u03b1t, j evaluates how well the output at position t matches the input around position j based on st \u2212 1 and hj. It is intuitively advantageous to use the information from yt \u2212 1 when reading MS that is missing from the implementation of attention-based NMT in (Bahdanau et al., 2014). In this thesis, we build a more effective alignment path by feeding both the previous hidden state st \u2212 1 and the context word yt \u2212 1 into the attention model, inspired by the recent implementation of attention-based NMT1. Formally, the calculation of et, j becomeset, j = v T a tanh (What amount \u2212 1 + Uahj) is found, where 1github.com / nyu-dl / dl4mt-tutorial / tree / master / session2 \u2022 s effort \u2212 1 is found as an intermediate state \u2212 1 (which is \u2212 1 for the \u2212 H, but \u2212 1 for the \u2212 1)."}, {"heading": "3 Decoder with External Memory", "text": "In this section, we will discuss the proposed memory-extended decoder MEMDEC = = predictive factor 1. In addition to the source memory MS, MEMDEC has a buffer memory MB as an extension of the conventional state vector. Figure 3 contrasts MEMDEC with the decoder in RNNsearch (Figure 1) at a high level. In the rest of the paper, we refer to the conventional state as vector state (referred to as st) and its memory expansion as memory state (referred to as MBt). Both states are updated interweaving at each step, while the output symbol yt is based exclusively on the vector state (together with ct and yt \u2212 1). \u2212 The diagram of this memory-extended decoder is given in Figure 2.Vector state update. In the time t, the vector state st st-1 is used to read the state MBrt \u2212 1. \u2212 The read state \u2212 1 is then predicted as B1 (the prediction state st-1)."}, {"heading": "3.1 Reading Memory-State", "text": "Before updating the vector state at this time, the output of the measured value rt is specified with byrt = j = n \u2211 j = 1 wRt (j) M B t \u2212 1 (j), where wRt \u0109Rn specifies the normalized weights assigned to the cells in MBt. Similar to reading from MS (a.k.a. Attention Model), we use a content-based addressing when determining wRt. More specifically, wRt is also used from the previous time w R t \u2212 1 aswRt = g R t \u2212 1 + (1 \u2212 gRt) w-Rt, (10) where \u2022 gRt = \u03c3 (wRgst) is the gate function with the parameters wRamew = Rm; \u2022 w \u00b7 t gives the contribution based on the current vector (R > Rv = 1 ftm) (Rv = 1 ftm)."}, {"heading": "3.2 Writing to Memory-State", "text": "There are two types of operations when writing to the memory state: ERASE and ADD. Erasion is similar to the forget-me-not in LSTM or GRU, which determines the content to be removed from the memory cells. Specifically, the vector \u00b5ERSt-Rm specifies the values that must be removed in each dimension in the memory cells and which are then assigned to each cell by normalized weights. Formally, the memory state according to ERASE is parameterized by M-Bt (i) = M-B-t-1 (i) (1 \u2212 wWt (i) \u00b7 \u00b5ERSt) (13) i = 1 \u00b7, n, where \u2022 \u00b5ERSt = \u03c3 (WERSst) is parameterized with WERS-Rm-m; \u2022 wWt (i) specifies the weight that resembles the ith cell in the same parametric form as in Equation (10) - (12) with generally different parameters AD.Update-D \u00b7 S-MD (ADm) information."}, {"heading": "3.3 Some Analysis", "text": "The write process in Equation (13) in due course t can be considered a non-linear way to combine the previous memory state MBt \u2212 1 and the newly updated vector state st, with nonlinearity starting from both content-based addressing and gating. This is somewhat similar to updating states in regular RNN, while we suspect that the addressing strategy in MEMDEC makes it easier to selectively update some content (e.g. the relatively short-term content) while other content is less modified (e.g. the relatively long-term content).The read process in Equation (10) can \"extract\" the content from MBt, which is relevant for alignment (reading from MS) and the prediction task at a given time. This is in contrast to the regular RNN decoder including its gated variants, which requires the entire state vector for this purpose. As an advantage, although only one part of the information is used in the later state, the MBt is used for the entire time, the other part of the memory state can be used for the update."}, {"heading": "4 Experiments on Chinese-English Translation", "text": "We test the memory-extended decoder for the task of translating from Chinese to English, using MEMDEC as in on the encoder (Bahdanau et al., 2014)."}, {"heading": "4.1 Datasets and Evaluation metrics", "text": "Our training data for the translation task consists of 1.25M pairs of sentences extracted from the LDC corpora2, with 27.9M Chinese and 34.5m English words respectively. We select the NIST 2002 (MT02) data set and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) and 2006 (MT06) data sets as test sets. We use the case-insensitive 4 gram NIST BLEU score (Papineni et al., 2002) as evaluation metric."}, {"heading": "4.2 Experiment settings", "text": "In fact, it is the case that most people who are able to move, to move, to move, to move, to fight back, to fight back, to fight back, to fight back, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "4.3 Comparison systems", "text": "We compare our method to three state-of-the-art systems: \u2022 Moses: an open source phrase-based translation system 3: with default configuration and a 4 gram language model trained on the target portion of the training data. \u2022 RNNSearch: an attention-based NMT model with default settings. We use the open source GroundHog system as an NMT baseline4. \u2022 Coverage model: a state-of-the-art variant of the attention-based NMT model (Tu et al., 2016), which improves the attention mechanism by modelling soft coverage based on source representation."}, {"heading": "4.4 Results", "text": "The main results of various models are listed in Table 1. MEMDEC clearly leads to a remarkable improvement over Moses (+ 5.28 BLEU) and Groundhog (+ 4.78 BLEU). Attention to feedback gains + 1.06 BLEU points over Groundhog on average, while along with the drop-out another + 0.83 BLEU is added, which represents the 1.89 BLEU gain of RNNSearch? over Groundhog. Compared to RNSearch?, MEMDEC + 2.89 BLEU points is higher, showing the modeling performance gained from external memory. Finally, we also compare MEMDEC to the state-of-the-art attention-based NMT3http: / / www.statmt.org / moses / 4https: / / github.com / lisa-GroundHog / GroundHogwith COVERAGE mechanism (Tu et al., 2016), which is about 2 BLEU above the published result after quick attention and abandonment."}, {"heading": "4.5 Model selection", "text": "Pre-training plays an important role in optimising the memory model. As can be seen in Tab.2, the pre-training value improves on average compared to our baseline + 1.11 BLEU, but even without pre-training, our model still averages + 1.04 BLEU points. Our model is quite robust in terms of memory size: with just four cells, our model will be over 2 BLEU higher than RNNsearch?. This confirms our assumption that the external memory is mainly used to store part of the source and history of the target."}, {"heading": "4.6 Case study", "text": "In Table 5, we show sample translations from Chinese to English, mainly comparing MEMDEC and the RNNsearch model for its pre-training. It is interesting to observe that MEMDEC provides smoother translation results and a better understanding of the sentence's semantic information."}, {"heading": "5 Related Work", "text": "There is a long workbook aimed at improving the ability of RNN to remember long sequences, with short-term memory RNN (LSTM) (Hochreiter and Schmidhuber, 1997) being the most prominent examples, and GRU (Cho et al., 2014) the most recent. This work focuses on designing the dynamics of the RNN through new dynamic operators and appropriate gating, while maintaining vector-form RNN states. In addition to gated RNN, MEMDEC explicitly adds matrix format memories that are equipped with content-based addressing to the system, thereby greatly improving the performance of the RNN decoder in displaying the information important for the translation task. MEMDEC is obviously related to recent efforts to attach an external memory to neural networks, with two outstanding examples of Neural Turing Machine (NTM) and Working Memory Network (MENC) translation being significant in 2014, MENC-specific terms."}, {"heading": "6 Conclusion", "text": "We propose to improve the RNN decoder in a Neural Machine Translator (NMT) with external memory. Our empirical study of Chinese-English translation shows that it can significantly improve the performance of NMT."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Long short-term memory-networks for machine reading", "author": ["Cheng et al.2016] Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Context-dependent translation selection using convolutional neural network", "author": ["Hu et al.2015] Baotian Hu", "Zhaopeng Tu", "Zhengdong Lu", "Hang Li"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A deep memorybased architecture for sequence-to-sequence learning", "author": ["Meng et al.2015] Fandong Meng", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Qun Liu"], "venue": "arXiv preprint arXiv:1506.06442", "citeRegEx": "Meng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "How to construct deep recurrent neural networks. arXiv preprint arXiv:1312.6026", "author": ["Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Modeling coverage for neural machine translation", "author": ["Tu et al.2016] Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": null, "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "A novel dependency-to-string model for statistical machine translation", "author": ["Xie et al.2011] Jun Xie", "Haitao Mi", "Qun Liu"], "venue": null, "citeRegEx": "Xie et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2011}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Unlike the unbounded memory in previous work(Bahdanau et al., 2014) to store the representation of source sentence, the memory in MEMDEC is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step.", "startOffset": 44, "endOffset": 67}, {"referenceID": 0, "context": "More specifically, in neural machine translation, one great improvement came from using an array of vectors to represent the source in a sentence-level memory and dynamically accessing relevant segments of them (alignment) (Bahdanau et al., 2014) through content-based addressing (Graves et al.", "startOffset": 223, "endOffset": 246}, {"referenceID": 0, "context": "This is in contrast to the set of hidden states of the entire source sentence (which can viewed as another form of memory) in (Bahdanau et al., 2014) for attentive read, but can be combined with it to greatly improve the performance of neural machine translator.", "startOffset": 126, "endOffset": 149}, {"referenceID": 12, "context": "We apply our model on English-Chinese translation tasks, achieving performance superior to any published results, SMT or NMT, on the same training data (Xie et al., 2011; Meng et al., 2015; Tu et al., 2016; Hu et al., 2015) Our contributions are mainly two-folds ar X iv :1 60 6.", "startOffset": 152, "endOffset": 223}, {"referenceID": 8, "context": "We apply our model on English-Chinese translation tasks, achieving performance superior to any published results, SMT or NMT, on the same training data (Xie et al., 2011; Meng et al., 2015; Tu et al., 2016; Hu et al., 2015) Our contributions are mainly two-folds ar X iv :1 60 6.", "startOffset": 152, "endOffset": 223}, {"referenceID": 11, "context": "We apply our model on English-Chinese translation tasks, achieving performance superior to any published results, SMT or NMT, on the same training data (Xie et al., 2011; Meng et al., 2015; Tu et al., 2016; Hu et al., 2015) Our contributions are mainly two-folds ar X iv :1 60 6.", "startOffset": 152, "endOffset": 223}, {"referenceID": 6, "context": "We apply our model on English-Chinese translation tasks, achieving performance superior to any published results, SMT or NMT, on the same training data (Xie et al., 2011; Meng et al., 2015; Tu et al., 2016; Hu et al., 2015) Our contributions are mainly two-folds ar X iv :1 60 6.", "startOffset": 152, "endOffset": 223}, {"referenceID": 0, "context": "2 Neural machine translation with attention Our work is built on attention-based NMT(Bahdanau et al., 2014), which represents the source sentence as a sequence of vectors after being processed by RNN or bi-directional RNNs, and then conducts dynamic alignment and generation of the target sentence with another RNN simultaneously.", "startOffset": 84, "endOffset": 107}, {"referenceID": 2, "context": "(2) where g(\u00b7) can be an be any activation function, here we adopt a more sophisticated dynamic operator as in Gated Recurrent Unit (GRU, (Cho et al., 2014)).", "startOffset": 138, "endOffset": 156}, {"referenceID": 0, "context": "This is called automatic alignment (Bahdanau et al., 2014) or attention model (Luong et al.", "startOffset": 35, "endOffset": 58}, {"referenceID": 7, "context": ", 2014) or attention model (Luong et al., 2015), but it is essentially reading with content-based addressing defined in (Graves et al.", "startOffset": 27, "endOffset": 47}, {"referenceID": 0, "context": "It is intuitively beneficial to exploit the information of yt\u22121 when reading from M, which is missing from the implementation of attention-based NMT in (Bahdanau et al., 2014).", "startOffset": 152, "endOffset": 175}, {"referenceID": 0, "context": "Prediction As illustrated in Figure 6, the prediction model is same as in (Bahdanau et al., 2014), where the score for word y is given by", "startOffset": 74, "endOffset": 97}, {"referenceID": 0, "context": "We test the memory-enhanced decoder to task of Chinese-to-English translation, where MEMDEC is put on the top of encoder same as in (Bahdanau et al., 2014).", "startOffset": 132, "endOffset": 155}, {"referenceID": 9, "context": "We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric as our evaluation metric (Papineni et al., 2002).", "startOffset": 101, "endOffset": 124}, {"referenceID": 13, "context": "Adadelta (Zeiler, 2012) is used to automatically adapt the learning rate of each parameter ( = 10\u22126 and \u03c1 = 0.", "startOffset": 9, "endOffset": 23}, {"referenceID": 10, "context": "0 was normalized to the threshold (Pascanu et al., 2013).", "startOffset": 34, "endOffset": 56}, {"referenceID": 4, "context": "Dropout we also use dropout for our NMT baseline model and MEMDEC to avoid over-fitting (Hinton et al., 2012).", "startOffset": 88, "endOffset": 109}, {"referenceID": 11, "context": "\u2022 Coverage model: a state-of-the-art variant of attention-based NMT model (Tu et al., 2016) which improves the attention mechanism through modelling a soft coverage on the source representation.", "startOffset": 74, "endOffset": 91}, {"referenceID": 11, "context": "The coverage model on top of RNNsearch? has significantly improved upon its published version (Tu et al., 2016), which achieves the best published result on this training set.", "startOffset": 94, "endOffset": 111}, {"referenceID": 11, "context": "with COVERAGE mechanism(Tu et al., 2016), which is about 2 BLEU over than the published result after adding fast attention and dropout.", "startOffset": 23, "endOffset": 40}, {"referenceID": 2, "context": "5 Related Work There is a long thread of work aiming to improve the ability of RNN in remembering long sequences, with the long short-term memory RNN (LSTM) (Hochreiter and Schmidhuber, 1997) being the most salient examples and GRU (Cho et al., 2014) being the most recent one.", "startOffset": 232, "endOffset": 250}, {"referenceID": 1, "context": "Our work is also related to the recent work on machine reading (Cheng et al., 2016), in which the machine reader is equipped with a memory tape, enabling the model to directly read all the previous hidden state with an attention mechanism.", "startOffset": 63, "endOffset": 83}, {"referenceID": 8, "context": "In (Meng et al., 2015), Meng et.", "startOffset": 3, "endOffset": 22}], "year": 2016, "abstractText": "We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memoryenhanced RNN decoder is called MEMDEC. At each time during decoding, MEMDEC will read from this memory and write to this memory once, both with content-based addressing. Unlike the unbounded memory in previous work(Bahdanau et al., 2014) to store the representation of source sentence, the memory in MEMDEC is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step. Our empirical study on Chinese-English translation shows that it can improve by 4.8 BLEU upon Groundhog and 5.3 BLEU upon on Moses, yielding the best performance achieved with the same training set.", "creator": "LaTeX with hyperref package"}}}