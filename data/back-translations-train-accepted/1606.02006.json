{"id": "1606.02006", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Incorporating Discrete Translation Lexicons into Neural Machine Translation", "abstract": "Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.", "histories": [["v1", "Tue, 7 Jun 2016 02:40:42 GMT  (157kb)", "https://arxiv.org/abs/1606.02006v1", "Submitted to EMNLP 2016"], ["v2", "Wed, 5 Oct 2016 02:46:39 GMT  (158kb)", "http://arxiv.org/abs/1606.02006v2", "Accepted at EMNLP 2016"]], "COMMENTS": "Submitted to EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["philip arthur", "graham neubig", "satoshi nakamura 0001"], "accepted": true, "id": "1606.02006"}, "pdf": {"name": "1606.02006.pdf", "metadata": {"source": "CRF", "title": "Incorporating Discrete Translation Lexicons into Neural Machine Translation", "authors": ["Philip Arthur", "Graham Neubig", "Satoshi Nakamura"], "emails": ["philip.arthur.om0@is.naist.jp", "gneubig@cs.cmu.edu", "s-nakamura@is.naist.jp"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.02 006v 2 [cs.C L] 5O ct"}, {"heading": "1 Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to put ourselves at the top, and that we are able to assert ourselves, that we are able to put ourselves at the top, \"he said."}, {"heading": "2 Neural Machine Translation", "text": "The goal of machine translation is to translate a sequence of source words F = f | F | 1 into a sequence of words. (These words belong to the source vocabulary Vf \u2212 \u2212 \u2212 and the target vocabulary Ve \u2212 a. This is done by encoding the context < F, ei \u2212 1 > a conditional probability pm (ei | F, i \u2212 1) of the target word ei \u2212 1), calculating the probability as follows: pm (ei \u2212 F, e \u2212 1) = softmax words ei \u2212 11. This is done by encoding the context < F, ei \u2212 1 > a fixed width vector. The exact variety of the NMT model depends on how we use it as input."}, {"heading": "3 Integrating Lexicons into NMT", "text": "In \u00a7 2, we have described how traditional NMT models calculate the probability of the next target word pm (ei | e i \u2212 1 1, F). Our goal in this essay is to improve the accuracy of this probability estimate by incorporating information from discrete probability icons. We assume that we have a lexicon that assigns a probability pl (e | f) to the target word e with a source word f. In the case of a source word f, this probability will generally not be zero for a small number of translation candidates and zero for the majority of words in VE. In this section, we will first describe how we include these probabilities in NMT and explain how we actually obtain the pl (e | f) probabilities in \u00a7 4."}, {"heading": "3.1 Converting Lexicon Probabilities into Conditioned Predictive Proabilities", "text": "First, we must convert the lexical probabilities pl (e | f) for the individual words in the source sentence F into a form that can be used together with pm (ei | e \u2212 1 1, F). If we use the input sentence F, we can construct a matrix in which each column corresponds to a word in the input sentence, each line corresponds to a word in the VE, and the input corresponds to the corresponding lexical probability: LF = pl (e = 1 | f1). \u00b7 \u00b7 pl (e = 1 | f |)....... pl (e = | Ve | f1) \u00b7 \u00b7 \u00b7 pl (e = | Ve | f |).This matrix can be pre-calculated during the coding phase, because it only requires information about the source sentence F. Next, we convert this matrix e to a predictive probability about the next word: pl (ei | F \u2212 1 \u2212 1)."}, {"heading": "3.2 Combining Predictive Probabilities", "text": "After calculating the lexiconic predictive probability pl (ei | e i \u2212 1 1, F), we must next integrate this probability with the probability pm (ei | e i \u2212 1 1 1, F) of the NMT model by examining two methods: (1) addition as bias and (2) linear interpolation."}, {"heading": "3.2.1 Model Bias", "text": "In our first biased method, we use pl (\u00b7) to distort the probability distribution calculated by the vanilla NMT model. Specifically, we add a small constant to pl (\u00b7), take the logarithm and add this adjusted logarithm probability to the Softmax input as follows: pb (ei | F, e i \u2212 1 1 1) = Softmax (Ws\u03b7i + bs + log (pl (ei | F, e \u2212 1 1 1 1) +))). We take the logarithm of pl (\u00b7) so that the values are still in the probability range even after the Softmax is calculated, and add the hyperparameter to prevent the zero probabilities from becoming \u2212 xi after the protocol is included. If the model is small, it is more distorted to use the lexicon, and if it is larger, the lexicon probabilities are less weighted."}, {"heading": "3.2.2 Linear Interpolation", "text": "We also try to integrate the two probabilities by linear interpolation between the standard NMT probability model pm (\u00b7) and the lexicon probability pl (\u00b7). We will call this the linear method and define it as follows: po (ei | F, e i \u2212 1 1) = pl (ei = 1 | F, e i \u2212 1 1) pm (e = 1 | F, e i \u2212 1 1 1)...... pl (ei = | Ve | | F, e i \u2212 1 1) pm (e = | Ve | F, e i \u2212 1 1 1) [\u03bb1 \u2212 \u03bb], where \u03bb is an interpolation coefficient that is the result of the sigmoid function \u03bb = sig (x) = 11 + e \u2212 x.x is a learnable parameter, and the sigmoid function ensures that the final interpolation level falls between 0 and 1. We choose x = 0 (\u03bb = 0.5) at the beginning of the training.This notation is partially inspired by a sentence that 2016 is a standard copy of all probabilities."}, {"heading": "4 Constructing Lexicon Probabilities", "text": "In the previous section, we have defined some ways to use predictive probabilities pl (ei | F, e i \u2212 1 1) based on word-to-word lexical probabilities pl (e | f). Next, we define three ways to construct these lexical probabilities using automatically learned lexicographs, handmade lexicographs, or a combination of both."}, {"heading": "4.1 Automatically Learned Lexicons", "text": "In traditional SMT systems, lexical translation probabilities are generally learned directly from parallel data > unsupervised using a model such as the IBM models (Brown et al., 1993; Och and Ney, 2003), which can be used to estimate the alignments and lexical translation probabilities pl (e | f) between the tokens of the two languages using the Expectation Maximization Algorithm (EM). In the maximization step, lexical probabilities are calculated by dividing the expected number by all possible counts: pl, a (e | f) = c (f, e). IBM models differ in the degree of refinement, with Model 1 relying exclusively on these lexical probabilities, and the latter IBM models (Models 2, 3, 4, 5)."}, {"heading": "4.2 Manual Lexicons", "text": "In contrast to automatically learned dictionaries, handmade dictionaries generally do not contain translation probabilities. To construct the probability of pl (e | f), we define the translation quantities Kf for a specific starting word f in the dictionary and assume a uniform distribution over these words: pl, m (e | f) = {1 | Kf | if e Kf 0 otherwise. According to Equation (5) unknown source words assign their probability mass to the < unk > tag."}, {"heading": "4.3 Hybrid Lexicons", "text": "Handmade lexicographs have a wide range of words, but their probabilities may not be as accurate as those acquired, especially if the automatic lexicon is built on in-domain data. Therefore, we are also testing a hybrid method in which we use the handmade lexicographs to supplement the automatically learned lexicograph.2 3 Specifically, inspired by filling in phrase tables in PBMT systems (Bisazza et al., 2011), we use the probability of automatically learned lexicographs pl, a by default, and use the handmade lexicographs pl, m only for uncovered words: pl, h (e | f) = {pl, a (e | f) if f is covered pl, m (e | f) otherwise (6)."}, {"heading": "5 Experiment & Result", "text": "In this section we describe experiments with which we evaluate our proposed methods."}, {"heading": "5.1 Settings", "text": "We have tried to compare the results with the fill-up method so that we apply the refill method to the refill method. We are conducting experiments so that we can imagine a method where we combine the training data and dictionaries with each other before we create the word alignments for the word selection method. We tried this, and the results were similar to or worse than the fill-up method, so that we will use the refill method for the paper's refill method.3While most words are learned in Japanese dictionaries, we are covered by the cover method. We have tried, and the results were similar to or worse than the refill method for the paper's refill method.3While most words are learned in Japanese dictionaries, we are covered by the cover method."}, {"heading": "5.2 Effect of Integrating Lexicons", "text": "In this section we first have a detailed examination of the usefulness of the proposed approach, when it is applicable: / / eijiro.jpwith the car or hyb lexicons, which empirically provide the best results, and perform a comparison between the other methods of lexical integration in the following section. Table 2 shows the results of these methods, along with the corresponding baselines. First, compared with the baseline attn, our approach consistently achieves higher values on both test categories. In particular, the gains on the more difficult KFTT set are large, up to 2.44 NIST, and 30% Recall, showing the utility of the proposed method in the face of more content and less high frequency words."}, {"heading": "5.3 Comparison of Integration Methods", "text": "Finally, we perform a full comparison between the various methods for integrating lexicon into the translation process, with results shown in Table 4. In general, the bias method improves accuracy for the auto- and hyb-lexicon, but is less effective for the man lexicon. This is probably due to the fact that the manual lexicon, although it has broad coverage, does not sufficiently cover target domain words (the coverage of unique words in the source lexicon was 35.3% and 9.7% for BTEC and KFTT, respectively).Interestingly, the trend in the linear method is reversed, improving human systems, but leads to decreases in the use of auto and hyb-lexicon, suggesting that the linear method is better suited for cases where the lexicon does not closely correspond to the target domain, and plays a more complementary role. Compared to the log-linear modeling of bible, the limitations imposed by the lexicon in 1998, the lexicon is generally the interdisciplinary one, which is the reason for the lexicon's distribution."}, {"heading": "6 Additional Experiments", "text": "To test whether the proposed method is useful for larger datasets, we also conducted follow-up experiments with the larger Japanese-English ASPEC dataset (Nakazawa et al., 2016), which consists of 2 million training examples, 63 million tokens and 81,000 vocabulary sizes. We achieved an improvement in the BLEU score from 20.82 using the attn baseline to 22.66 using the proposed method with autobias. This experiment shows that our method scales to larger datasets."}, {"heading": "7 Related Work", "text": "Since the beginning of the work on NMT, unknown words that do not exist in the vocabulary of the system have been focused as a weakness of these systems. Early methods of dealing with these unknown words have replaced them with appropriate words in the target vocabulary (Jean et al., 2015; Luong et al., 2015b) according to a lexicon similar to the one used in this work. In contrast to our work, these vocabulary only treat unknown words and do not incorporate information from the lexicon into the learning process. There have also been other approaches that include models that learn when words like these are copied into the target language (Allamanis et al., 2016; Guet al., 2016; Gu \u00bc lc erhre et al., 2016). These models are similar to the linear approach of \u00a7 3.2.2, but apply only to words that can be asis copied into the target language. In fact, these models can be considered as a subclass of the proposed approach."}, {"heading": "8 Conclusion & Future Work", "text": "In this paper, we have proposed a method of incorporating discrete probability lexicons into NMT systems to solve the difficulties that low-frequency word NMT systems have shown. As a result, we have achieved significant increases in BLEU- (2.0-2.3) and NIST-values (0.13-0.44) and observed qualitative improvements in the translation of content words.For future work, we are interested in conducting experiments on more extensive translation tasks. We also plan a subjective evaluation, as we expect that improvements in text translation content are crucial for the subjective impression of the translation results. Finally, we are also interested in improvements in the linear method, where \u03bb is calculated based on context, rather than using a fixed value."}, {"heading": "Acknowledgment", "text": "We thank Makoto Morishita and Yusuke Oda for their help in this project. We also thank the lecturers of the AHC Laboratory for their support and suggestions. This work was supported by grants from the Japanese Ministry of Education, Culture, Sport, Science and Technology and partly by the JSPS KAKENHI grant number 16H05873."}], "references": [{"title": "A convolutional attention network for extreme summarization of source code", "author": ["Miltiadis Allamanis", "Hao Peng", "Charles Sutton."], "venue": "Proceedings of the 33th International Conference on Machine Learning (ICML).", "citeRegEx": "Allamanis et al\\.,? 2016", "shortCiteRegEx": "Allamanis et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 4th International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "Journal of Machine Learning Research, pages 1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Fill-up versus interpolation methods for phrasebased SMT adaptation", "author": ["Arianna Bisazza", "Nick Ruiz", "Marcello Federico."], "venue": "Proceedings of the 2011 International Workshop on Spoken Language Translation (IWSLT), pages 136\u2013143.", "citeRegEx": "Bisazza et al\\.,? 2011", "shortCiteRegEx": "Bisazza et al\\.", "year": 2011}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer."], "venue": "Computational Linguistics, pages 263\u2013311.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Computational Linguistics, pages 201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of the Workshop on Syntax and Structure in Statistical Translation (SSST), pages", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1693\u20131703.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["Marta R. Costa-Juss\u00e0", "Jos\u00e9 A.R. Fonollosa."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 357\u2013361.", "citeRegEx": "Costa.Juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.Juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "author": ["George Doddington."], "venue": "Proceedings of the Second International Conference on Human Language Technology Research, pages 138\u2013145.", "citeRegEx": "Doddington.,? 2002", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "A simple, fast, and effective reparameterization of IBM model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Felix A. Gers", "J\u00fcrgen A. Schmidhuber", "Fred A. Cummins."], "venue": "Neural Computation, pages 2451\u20132471.", "citeRegEx": "Gers et al\\.,? 2000", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Incorporating copying mechanism in sequenceto-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1631\u20131640.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 140\u2013149.", "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? 2016", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, pages 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "KyungHyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Creating corpora for speech-to-speech translation", "author": ["Gen-ichiro Kikui", "Eiichiro Sumita", "Toshiyuki Takezawa", "Seiichi Yamamoto."], "venue": "8th European Conference on Speech Communication and Technology, EU-", "citeRegEx": "Kikui et al\\.,? 2003", "shortCiteRegEx": "Kikui et al\\.", "year": 2003}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Log-linear interpolation of language models", "author": ["Dietrich Klakow."], "venue": "Proceedings of the 5th International Conference on Speech and Language Processing (ICSLP).", "citeRegEx": "Klakow.,? 1998", "shortCiteRegEx": "Klakow.", "year": 1998}, {"title": "Statistical phrase-based translation", "author": ["Phillip Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 48\u2013", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Alignment by agreement", "author": ["Percy Liang", "Ben Taskar", "Dan Klein."], "venue": "Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 104\u2013111.", "citeRegEx": "Liang et al\\.,? 2006", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black."], "venue": "CoRR.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1054\u20131063.", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics (ACL) and the", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "The Kyoto free translation task", "author": ["Graham Neubig"], "venue": null, "citeRegEx": "Neubig.,? \\Q2011\\E", "shortCiteRegEx": "Neubig.", "year": 2011}, {"title": "Travatar: A forest-to-string", "author": ["Graham Neubig"], "venue": null, "citeRegEx": "Neubig.,? \\Q2013\\E", "shortCiteRegEx": "Neubig.", "year": 2013}, {"title": "Bleu: A method for automatic", "author": ["Jing Zhu"], "venue": null, "citeRegEx": "Zhu.,? \\Q2002\\E", "shortCiteRegEx": "Zhu.", "year": 2002}], "referenceMentions": [{"referenceID": 25, "context": "NMT has recently gained popularity due to its ability to model the translation process end-to-end using a single probabilistic model, and for its state-of-the-art performance on several language pairs (Luong et al., 2015a; Sennrich et al., 2016).", "startOffset": 201, "endOffset": 245}, {"referenceID": 15, "context": "Neural machine translation (NMT, \u00a72; Kalchbrenner and Blunsom (2013), Sutskever et al.", "startOffset": 37, "endOffset": 69}, {"referenceID": 15, "context": "Neural machine translation (NMT, \u00a72; Kalchbrenner and Blunsom (2013), Sutskever et al. (2014)) is a variant of statistical machine translation (SMT; Brown et al.", "startOffset": 37, "endOffset": 94}, {"referenceID": 4, "context": "(2014)) is a variant of statistical machine translation (SMT; Brown et al. (1993)), using neural networks.", "startOffset": 62, "endOffset": 82}, {"referenceID": 20, "context": "This is in contrast to more traditional SMT methods such as phrase-based machine translation (PBMT; Koehn et al. (2003)), which represent translations as discrete pairs of word strings in the source and target languages.", "startOffset": 100, "endOffset": 120}, {"referenceID": 1, "context": "1) into a predictive probability for the next word by utilizing attention vectors from attentional NMT models (Bahdanau et al., 2015).", "startOffset": 110, "endOffset": 133}, {"referenceID": 1, "context": "While there are many methods to perform this modeling, we opt to use attentional models (Bahdanau et al., 2015), which focus on particular words in the source sentence when calculating the probability of ei.", "startOffset": 88, "endOffset": 111}, {"referenceID": 1, "context": "While there are many methods to perform this modeling, we opt to use attentional models (Bahdanau et al., 2015), which focus on particular words in the source sentence when calculating the probability of ei. These models represent the current state of the art in NMT, and are also convenient for use in our proposed method. Specifically, we use the method of Luong et al. (2015a), which we describe briefly here and refer readers to the original paper for details.", "startOffset": 89, "endOffset": 380}, {"referenceID": 2, "context": "Here the embed(\u00b7) function maps the words into a representation (Bengio et al., 2003), and enc(\u00b7) is a stacking long short term memory (LSTM) neural network (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 14, "context": ", 2003), and enc(\u00b7) is a stacking long short term memory (LSTM) neural network (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Sutskever et al., 2014).", "startOffset": 79, "endOffset": 156}, {"referenceID": 11, "context": ", 2003), and enc(\u00b7) is a stacking long short term memory (LSTM) neural network (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Sutskever et al., 2014).", "startOffset": 79, "endOffset": 156}, {"referenceID": 25, "context": "sim(\u00b7) can be an arbitrary similarity function, which we set to the dot product, following Luong et al. (2015a). We then normalize this into an attention vector, which weights the amount of focus that we put on each word in the source sentence", "startOffset": 91, "endOffset": 112}, {"referenceID": 0, "context": "This notation is partly inspired by Allamanis et al. (2016) and Gu et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 0, "context": "This notation is partly inspired by Allamanis et al. (2016) and Gu et al. (2016) who use linear interpolation to merge a standard attentional model with a \u201ccopy\u201d operator that copies a source word as-is into the target sentence.", "startOffset": 36, "endOffset": 81}, {"referenceID": 4, "context": "In traditional SMT systems, lexical translation probabilities are generally learned directly from parallel data in an unsupervised fashion using a model such as the IBM models (Brown et al., 1993; Och and Ney, 2003).", "startOffset": 176, "endOffset": 215}, {"referenceID": 22, "context": "\u201cgarbage collecting\u201d effects (Liang et al., 2006)), traditional SMT systems generally achieve better translation accuracies of lowfrequency words than NMT systems (Sutskever et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 15, "context": "Note that in many cases, NMT limits the target vocabulary (Jean et al., 2015) for training speed or memory constraints, resulting in rare words not being covered by the NMT vocabulary VE .", "startOffset": 58, "endOffset": 77}, {"referenceID": 3, "context": "2 3 Specifically, inspired by phrase table fill-up used in PBMT systems (Bisazza et al., 2011), we use the probability of the automatically learned lexicons pl,a by default, and fall back to the handmade lexicons pl,m only for uncovered words:", "startOffset": 72, "endOffset": 94}, {"referenceID": 27, "context": "Dataset: We perform experiments on two widelyused tasks for the English-to-Japanese language pair: KFTT (Neubig, 2011) and BTEC (Kikui et al.", "startOffset": 104, "endOffset": 118}, {"referenceID": 17, "context": "Dataset: We perform experiments on two widelyused tasks for the English-to-Japanese language pair: KFTT (Neubig, 2011) and BTEC (Kikui et al., 2003).", "startOffset": 128, "endOffset": 148}, {"referenceID": 18, "context": "We train the system using the Adam (Kingma and Ba, 2014) optimization method with the default settings: \u03b1 = 1e\u22123, \u03b21 = 0.", "startOffset": 35, "endOffset": 56}, {"referenceID": 6, "context": "Finally, because NMT models tend to give higher probabilities to shorter sentences (Cho et al., 2014), we discount the probability of \u3008EOS\u3009 token by 10% to correct for this bias.", "startOffset": 83, "endOffset": 101}, {"referenceID": 20, "context": "Traditional SMT Systems: We also prepare two traditional SMT systems for comparison: a PBMT system (Koehn et al., 2003) using Moses5 (Koehn et al.", "startOffset": 99, "endOffset": 119}, {"referenceID": 5, "context": ", 2007), and a hierarchical phrase-based MT system (Chiang, 2007) using Travatar6 (Neubig, 2013), Systems are built using the default settings, with models trained on the training data, and weights tuned on the development data.", "startOffset": 51, "endOffset": 65}, {"referenceID": 28, "context": ", 2007), and a hierarchical phrase-based MT system (Chiang, 2007) using Travatar6 (Neubig, 2013), Systems are built using the default settings, with models trained on the training data, and weights tuned on the development data.", "startOffset": 82, "endOffset": 96}, {"referenceID": 16, "context": "We train the system using the Adam (Kingma and Ba, 2014) optimization method with the default settings: \u03b1 = 1e\u22123, \u03b21 = 0.9, \u03b22 = 0.999, \u01eb = 1e\u22128. Additionally, we add dropout (Srivastava et al., 2014) with drop rate r = 0.2 at the last layer of each stacking LSTM unit to prevent overfitting. We use a batch size of B = 64 and we run a total of N = 14 iterations for all data sets. All of the experiments are conducted on a single GeForce GTX TITAN X GPU with a 12 GB memory cache. At test time, we use beam search with beam size b = 5. We follow Luong et al. (2015b) in replacing every unknown token at position i with the target token that maximizes the probability pl,a(ei|fj).", "startOffset": 36, "endOffset": 568}, {"referenceID": 9, "context": "Additionally, we also use NIST (Doddington, 2002), which is a measure that puts a particular focus on low-frequency word strings, and thus is sensitive to the low-frequency words we are focusing on in this paper.", "startOffset": 31, "endOffset": 49}, {"referenceID": 21, "context": "We measure the statistical significant differences between systems using paired bootstrap resampling (Koehn, 2004) with 10,000 iterations and measure statistical significance at the p < 0.", "startOffset": 101, "endOffset": 114}, {"referenceID": 10, "context": "It also takes an additional 297 minutes to train the lexicon with GIZA++, but this can be greatly reduced with more efficient training methods (Dyer et al., 2013).", "startOffset": 143, "endOffset": 162}, {"referenceID": 19, "context": "Compared to the log-linear modeling of bias, which strictly enforces constraints imposed by the lexicon distribution (Klakow, 1998), linear interpolation is intuitively more appropriate for integrating this type of complimentary information.", "startOffset": 117, "endOffset": 131}, {"referenceID": 12, "context": "Gu et al. (2016) have recently developed methods to use the context information from the decoder to calculate the different interpolation coefficients for every decoding step, and it is possible that introducing these methods would improve our results.", "startOffset": 0, "endOffset": 17}, {"referenceID": 15, "context": "Early methods to handle these unknown words replaced them with appropriate words in the target vocabulary (Jean et al., 2015; Luong et al., 2015b) according to a lexicon similar to the one used in this work.", "startOffset": 106, "endOffset": 146}, {"referenceID": 26, "context": "Early methods to handle these unknown words replaced them with appropriate words in the target vocabulary (Jean et al., 2015; Luong et al., 2015b) according to a lexicon similar to the one used in this work.", "startOffset": 106, "endOffset": 146}, {"referenceID": 23, "context": "Finally, there have been a number of recent works that improve accuracy of low-frequency words using character-based translation models (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 136, "endOffset": 208}, {"referenceID": 8, "context": "Finally, there have been a number of recent works that improve accuracy of low-frequency words using character-based translation models (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 136, "endOffset": 208}, {"referenceID": 7, "context": "Finally, there have been a number of recent works that improve accuracy of low-frequency words using character-based translation models (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 136, "endOffset": 208}, {"referenceID": 7, "context": ", 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016). However, Luong and Manning (2016) have found that even when using character-based models, incorporating information about words allows for gains in translation accuracy, and it is likely that our lexicon-based method could result in improvements in these hybrid systems as well.", "startOffset": 41, "endOffset": 96}], "year": 2016, "abstractText": "Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.1", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}