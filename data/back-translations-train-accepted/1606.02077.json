{"id": "1606.02077", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Regret Bounds for Non-decomposable Metrics with Missing Labels", "abstract": "We consider the problem of recommending relevant labels (items) for a given data point (user). In particular, we are interested in the practically important setting where the evaluation is with respect to non-decomposable (over labels) performance metrics like the $F_1$ measure, and the training data has missing labels. To this end, we propose a generic framework that given a performance metric $\\Psi$, can devise a regularized objective function and a threshold such that all the values in the predicted score vector above and only above the threshold are selected to be positive. We show that the regret or generalization error in the given metric $\\Psi$ is bounded ultimately by estimation error of certain underlying parameters. In particular, we derive regret bounds under three popular settings: a) collaborative filtering, b) multilabel classification, and c) PU (positive-unlabeled) learning. For each of the above problems, we can obtain precise non-asymptotic regret bound which is small even when a large fraction of labels is missing. Our empirical results on synthetic and benchmark datasets demonstrate that by explicitly modeling for missing labels and optimizing the desired performance metric, our algorithm indeed achieves significantly better performance (like $F_1$ score) when compared to methods that do not model missing label information carefully.", "histories": [["v1", "Tue, 7 Jun 2016 10:00:30 GMT  (124kb)", "http://arxiv.org/abs/1606.02077v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nagarajan natarajan", "prateek jain 0002"], "accepted": true, "id": "1606.02077"}, "pdf": {"name": "1606.02077.pdf", "metadata": {"source": "CRF", "title": "Regret Bounds for Non-decomposable Metrics with Missing Labels", "authors": ["Prateek Jain", "Nagarajan Natarajan"], "emails": ["prajain@microsoft.com", "t-nanata@microsoft.com", "precision@k;"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.02 077v 1 [cs.L G] 7J unKeywords: Non-decomposable losses, regret limits, multi-label learning"}, {"heading": "1. Introduction", "text": "It is not as if this kind of learning can only be applied on the basis of previous labels (e.g. existing likes / dislikes for different labels / items). In the presence of characteristics, the problem of standard multi-level classification is problematic. Design and analysis of algorithms for such tasks should meet two fundamental challenges: a) in practical scenarios, desired performance metrics for our predictions are typically complex non-decomposable functions such as F1 Score or Precision @ k; standard metrics such as hamming loss or RMSE over the labels may not be useful, and b) any realistic system in this area should be able to deal with missing labels."}, {"heading": "2. Problem Setup and Background", "text": "Let us ask whether it would not be better if we were able to observe the distribution effects. (...) Let us examine the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects, the distribution effects of the distribution effects of the distribution effects of the distribution effects, the distribution effects of the distribution effects of the distribution effects of the distribution effects, the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects, the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects, the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects,"}, {"heading": "3. Algorithm", "text": "Our approach is based on estimating real-rated predictions and then the threshold of predictions, which could essentially result in an improvement in the spatial structure. (W) The approach is based on estimating real-rated predictions and then maximizing the threshold of predictions optimally. (W) The approach to the approach is demonstrably consistently asymptotic, but it is not clear whether it allows for a useful regret; in particular, we would characterize the behavior in the finite samples. (W) In the case of the sampling model (1), the approach translates to learning columns of the parameter matrix W independently. In many cases, W exhibits a certain structure, such as low ranking, reflective correlation between labels (Yu et al., 2014; Zhong et al., 2015; Davenport et al., 2014)."}, {"heading": "4. Analysis: Regret Bounds", "text": "In this section, we will first show that this regret can be limited by regretting a certain loss. Then, using different sample models that relate to different settings such as 1-bit matrix completion, multi-label learning, and positive-unlabeled learning, we will show that regret can be limited by restoring the underlying parameter matrix that governs P (yij | xi)."}, {"heading": "4.1 Low \u2113-regret implies low \u03a8-regret", "text": "Our first major result combines regret with regret regarding a strongly correct loss function (Agarwal, 2014). Canonical examples of strongly correct losses include logistical loss (t, y) = log (1 + exp (\u2212 yt))), exponential loss (t, y) = exp (\u2212 yt) and squared loss (t, y) = (1 \u2212 yt) 2. Define the regret of Z Rn \u00b7 L as: Reg (Z) = E [(Zij, Yij). \u2212 min Z \u00b2 the loss of Rn \u00b7 L (, Yij), with the expectation derived from the threshold and shared distribution across instances and labels.Theorem 1 (main result 1). Let us be a performance metric as defined in (3), (4) or (5)."}, {"heading": "4.2 Bounding \u2113-regret", "text": "Below, we present the desired remorse in three different settings."}, {"heading": "4.2.1 Collaborative Filtering", "text": "Consider the 1-bit matrix sampling model in (2), then (6) is reduced to the optimization problem considered by Lafond (2015). We regret the 1-bit matrix sampling model Z = W obtained in step 2 of algorithm 1 (note that X is only treated as identity in this setting).Theorem 2. Assuming \u03c0 is uniform, let's look at the 1-bit matrix sampling model (2).Suppose that a 1-lipschitz, highly correct loss (appears in (7), and Z denotes the output in step 2 of algorithm 1. With a probability of at least 1 \u2212 \u043c applies: regulation (Z)."}, {"heading": "4.2.2 Multi-label Learning", "text": "Consider the sampling model (1) with its characteristics. We have the following regret for the estimator Z = XW = XW = XW = XW = XW = XZ = XZ = XZ = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z Z = Z = Z = Z = Z Z = Z = Z = Z Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z"}, {"heading": "4.2.3 PU Learning", "text": "In many collaborative filter and multi-level learning tasks, only the positive entries (yij = 1) are observed. In this context, we can apply the approach of (Hsieh et al., 2015), where they consider a two-level sample model: example yij with (2) for all i, j [n] \u00b7 [L] (or with (1) if features are available), and then flip a fraction of the collected 1s to 0's, resulting in Y's. We would then use the unbiased estimator for losses in (6)."}, {"heading": "5. Experiments", "text": "We focus on multi-label datasets for experimental studies. The aim is to show that convergence occurs as theory suggests, and that the proposed algorithm performs well on real datasets. To solve (6), we use an alternate minimization method by forming W = W1W T 2 so that W1, Rd \u00b7 k and W2, RL \u00b7 k, where k, the rank of W, is an input parameter."}, {"heading": "5.1 Synthetic data", "text": "We generate multi-stage data as follows: We fix n = 1000, L = 100 and d = 10. First, we generate X-Rn \u00b7 d using samples from multivariable Gaussian N (0, I). Then we generate W * of rank 5. We obtain the identification matrix Y through the threshold XW * at \u03b8 * = 0, i.e. yij = sign (< xi, w * j >). In this noise-free setting, we expect that our algorithm would accurately restore both W * and T * when it sees more and more observations. Results to maximize micro-F1 and accuracy metrics are presented in Figure 1. As the sampling ratio increases, we observe that the proposed estimator achieves optimal performance in both cases. Furthermore, we find that even if only 10% of the observations are detected, the proposed method achieves very high F1 and accuracy values, compared to learning the columns of W (followed indirectly by a plugin)."}, {"heading": "5.2 Real-world data", "text": "We look at five real multi-label datasets widely used as benchmarks (Bhatia et al., 2015a; Yu et al., 2014).1 CAL500: a music dataset with 400 training and 100 test instances, L = 174, d = 68.2. Corel5k: an image dataset with 4500 training and 500 test instances, L = 374, d = 499.3. Bibtex: a text dataset with 4,880 training and 2,515 test instances, L = 159, d = 1, 836.4. Compphys dataset with 161 training and 40 test instances, L = 208, d = 33, 284 and 5. Autofood dataset with 4,880 training and 2,515 test instances, L = 162, d = 1, 836, nLwas set to 20% for training."}, {"heading": "6. Conclusions", "text": "We presented a framework for optimizing general performance indicators applicable to both multi-label and collaborative filter settings. Our work complements current findings in this direction: On the theoretical front, we draw strong boundaries for practically applied indicators such as F-measure, and on the algorithmic front, we offer a simple and efficient process that works well in practice."}, {"heading": "Appendix A. Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Theorem 1", "text": "The proof of the technique is based on (Kot\u0142owski and Dembczy\u0144ski, 2015), deriving similar bindings in the binary classification setting. First, we refer to \"remorse\" as weighted 0-1 loss regrets. Let's define \"remorse\" (weighted 0-1 losses): \"remorse\" (R): \"remorse\" (0, 1): \"remorse\" (y): \"remorse\" (y): \"remorse\" (y): \"risk\" (y): \"risk\" (y): \"risk\" (y): \"risk\"): \"risk\" (f): \"risk\" (n): \"risk.\""}, {"heading": "A.1.1 Proof of Lemma 2", "text": "F (F). F (F). F (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F).).). (F. (F.). (F.). (F.).). (F.). (F.). (F.).). (F.). (F.). (F.).). (F.).). (F.). (F.)."}, {"heading": "A.1.2 Proof of Lemma 3", "text": "Notice that Risk (f) is defined as follows: Risk (f) = E [amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160 amp # 160; amp # 160 amp # 160; amp # 160 amp # 160; amp # 160 amp # 160; amp # 160; amp # 160; amp # 160; amp # 160 amp # 160; amp # 160 amp # 160; amp # 160; amp # 160; amp # 160 amp # 160 amp # 160"}, {"heading": "A.1.3 Proof of Lemma 4", "text": "For the second part we can apply the same arguments as in Lemma 9 by Koyejo et al. (2014)."}, {"heading": "A.2 Proof of Theorem 2", "text": "The following theorem limits the error of the estimator W-Rn-L in this model, on the result of Lafond (2015).Theorem 5 (Lafond (2015).Let us assume \u03c0 is uniform, and let us consider the 1-bit matrix scanning model (2).Let us use W-Log (n + L) as a solution to the optimization problem regulated by the trace norm (6) by considering logistical losses for (with input X as identity matrix of size n), number of observations, min (n + L) min (n, L) max (c-Log) 2 (c-Log) (c-3 (n, L), 1 / 9) and setting of the regulation parameter (n + L) min (n, L)."}, {"heading": "A.3 Weakness of using Lafond (2015) for Multi-label Learning", "text": "In the multi-level learning model (1), one might hope to apply Lafond's analysis (2015) directly to the restoration of the XW standard. Instead of the problem (6), we would then solve the optimization problem in Lafond (2015): W \u00b2 = arg min W = arg min W: 0 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (i, j) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (algorithm 1). The following conclusion of Theorem 5 provides a limit to the restoration error of W \u00b2. The following conclusion: 1. Assumption 1, \u03c0 is uniform and takes the sampling model into account (1). Let W \u00b2 provide the solution to the restoration problem 5. The following conclusion results in the restoration error of W \u00b2. The following conclusion: 1. Assumption 1, \u03c0 is uniform, and takes the sampling model into account (1). Let W \u00b2 provide the solution to the restoration problem regulated optimization problem 12."}, {"heading": "A.4 Proof of Theorem 3", "text": "The statement is a logical consequence of the more general theorem 8 shown in Appendix B. We can calculate the constants for the logistic loss as follows: \u03c3 \u0445 \u03b3 \u2264 1 and \u03c3\u03b3 \u2265 (1 + e \u03b3) 2e \u2212 \u03b3, over the domain [\u2212 \u03b3, \u03b3]."}, {"heading": "A.5 Proof of Theorem 4", "text": "The following result of (Hsieh et al., 2015) indicates the recovery limit for the resulting estimator W \u0442. Theorem 6 (((Hsieh et al., 2015)) described in the text (Section 4.2.3) with a probability of at least 1 \u2212 2 (n + L) \u2212 1, where C is an absolute constant and vice versa. The proof is complete by using the same argument for 1-Lipschitz as for the proof for Theorem 2."}, {"heading": "Appendix B. Sampling from Exponential Distribution", "text": "We now look at the generalized matrix completion problem when the values iid are taken from an exponential distribution distributed by the input characters x, wj, exph, G (xi, wj): = h (yij) exp (< xi, wj > yij \u2212 G (< xi, wj >)). (13) where < xi, wj >, i = 1, 2,..., n and j = 1, 2,... L are the canonical parameters, h and G are the baseline measures and log partition functions associated with this canonical representation."}, {"heading": "Maximum Log-likelihood Estimator.", "text": "We consider the negative log probability of observations given by the following definition: < xi, wj, wj, wj, wj, wj, wj, wj, wj, wj, wj, wj, wj, wj, wj, w, as follows: W, arg, min, w, so that there are constants. < xi, wy, wj, wj, wj, wj, w, whereby the function G (x) is doubly differentiated and strongly convex, so that there are constants."}, {"heading": "Writing W \u2208 Rd\u00d7L as W = W\u0303+P\u22a5", "text": "W (W \u2212 W) + P \u2212 W (W \u2212 W), Lemma 16 (i) of (Lafond, 2015) and triangular inequality together result in: \"W,\" \"K,\" \"K,\" \"K,\" \"K,\" \"K,\" \"K,\" \"K.\" (W \u2212 W), \"K,\" \"K,\" \"K,\" \"\" K, \"\" \"K,\" \"\" K, \"\" K, \"\" K, \"\" \"K,\" \"\" K, \"\" \"K,\" \"\" K, \"\" \"K,\" \"\" K, \"\" \"K,\" \"\" K, \"\" \"K,\" \"\" K, \"\" \"K,\" \"\" K, \"\" \"K,\" \"\" K, \"\" \"K,\" \",\" K, \"\" \"K,\" \"K,\" \"\" K, \"\" \"K,\" \"\" K, \"\" \"K,\" \"\" K, \"\" K, \"\" \"K,\" \"\" K, \"\" \"K,\" \"\" K, \"\" \"\" K, \"\" \"\" K, \"\" \"\" \"\" K, \"\" \"\" \"\" K, \"\" \"\" \"\" \"\" K, \"\" \"\" \"\" \"\" \"\" K, \"\" \"\" \"\" \"\" \"K,\" \"\" \"\" \"\" \"K,\" \"\" \"\" \"\" \"\" \"\" \"\" K, \"\" \"\" \"\" \"\" \"K,\" \"\" \"\" \"\" \"\" K, \"\" \"\" \"\" \"\" \"\" \"\" K, \"\" \"\" \"\" \"\" \"\" \"K,\" \"\" \"\" \"\" \"\" \"\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W \"W,\" W \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W\" W, \"W,\" W \"W"}, {"heading": "Proof of Lemma 1", "text": "Let H denote the matrix with hij = yij \u2212 G \u00b2 (< xi, w \u00b2 j >); let hi denote the ith series of H. Let P \u00b2 (H) denote the projection of H on the observed indices; let i denote the observed indices in row i of Y. For a vector v, let i denote its projection on the observed indices."}], "references": [{"title": "Surrogate regret bounds for bipartite ranking via strongly proper losses", "author": ["Shivani Agarwal"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Agarwal.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal.", "year": 2014}, {"title": "Sparse local embeddings for extreme multi-label classification", "author": ["Kush Bhatia", "Himanshu Jain", "Purushottam Kar", "Manik Varma", "Prateek Jain"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bhatia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhatia et al\\.", "year": 2015}, {"title": "Robust regression via hard thresholding", "author": ["Kush Bhatia", "Prateek Jain", "Purushottam Kar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bhatia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhatia et al\\.", "year": 2015}, {"title": "A max-norm constrained minimization approach to 1-bit matrix completion", "author": ["Tony Cai", "Wen-Xin Zhou"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Cai and Zhou.,? \\Q2013\\E", "shortCiteRegEx": "Cai and Zhou.", "year": 2013}, {"title": "1-bit matrix completion", "author": ["Mark A Davenport", "Yaniv Plan", "Ewout van den Berg", "Mary Wootters"], "venue": "Information and Inference,", "citeRegEx": "Davenport et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Davenport et al\\.", "year": 2014}, {"title": "Consistent multilabel ranking through univariate losses", "author": ["Krzysztof Dembczynski", "Wojciech Kotlowski", "Eyke H\u00fcllermeier"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Dembczynski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2012}, {"title": "On the consistency of multi-label learning", "author": ["Wei Gao", "Zhi-Hua Zhou"], "venue": "Artificial Intelligence,", "citeRegEx": "Gao and Zhou.,? \\Q2013\\E", "shortCiteRegEx": "Gao and Zhou.", "year": 2013}, {"title": "Pu learning for matrix completion", "author": ["Cho-jui Hsieh", "Nagarajan Natarajan", "Inderjit Dhillon"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Hsieh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2015}, {"title": "Provable inductive matrix completion", "author": ["Prateek Jain", "Inderjit S Dhillon"], "venue": "arXiv preprint arXiv:1306.0626,", "citeRegEx": "Jain and Dhillon.,? \\Q2013\\E", "shortCiteRegEx": "Jain and Dhillon.", "year": 2013}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky"], "venue": null, "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Surrogate regret bounds for generalized classification performance metrics", "author": ["Wojciech Kot\u0142owski", "Krzysztof Dembczy\u0144ski"], "venue": "arXiv preprint arXiv:1504.07272,", "citeRegEx": "Kot\u0142owski and Dembczy\u0144ski.,? \\Q2015\\E", "shortCiteRegEx": "Kot\u0142owski and Dembczy\u0144ski.", "year": 2015}, {"title": "Consistent binary classification with generalized performance metrics", "author": ["Oluwasanmi O Koyejo", "Nagarajan Natarajan", "Pradeep K Ravikumar", "Inderjit S Dhillon"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Koyejo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koyejo et al\\.", "year": 2014}, {"title": "Consistent multilabel classification", "author": ["Oluwasanmi O Koyejo", "Nagarajan Natarajan", "Pradeep K Ravikumar", "Inderjit S Dhillon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Koyejo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Koyejo et al\\.", "year": 2015}, {"title": "Low rank matrix completion with exponential family noise", "author": ["Jean Lafond"], "venue": "arXiv preprint arXiv:1502.06919,", "citeRegEx": "Lafond.,? \\Q2015\\E", "shortCiteRegEx": "Lafond.", "year": 2015}, {"title": "Fastxml: a fast, accurate and stable tree-classifier for extreme multi-label learning", "author": ["Yashoteja Prabhu", "Manik Varma"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Prabhu and Varma.,? \\Q2014\\E", "shortCiteRegEx": "Prabhu and Varma.", "year": 2014}, {"title": "Composite binary losses", "author": ["Mark D Reid", "Robert C Williamson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Reid and Williamson.,? \\Q2010\\E", "shortCiteRegEx": "Reid and Williamson.", "year": 2010}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin.", "year": 2010}, {"title": "Large-scale multilabel learning with missing labels", "author": ["Hsiang-Fu Yu", "Prateek Jain", "Purushottam Kar", "Inderjit Dhillon"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Ranking via robust binary classification", "author": ["Hyokun Yun", "Parameswaran Raman", "S Vishwanathan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yun et al\\.", "year": 2014}, {"title": "Efficient matrix sensing using rank-1 gaussian measurements", "author": ["Kai Zhong", "Prateek Jain", "Inderjit S. Dhillon"], "venue": "In International Conference on Algorithmic Learning Theory (ALT),", "citeRegEx": "Zhong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2015}, {"title": "\u03bb-strongly proper composite loss (Reid and Williamson, 2010), such as the squared loss or the logistic. Given real-valued predictions Z \u2208 Rn\u00d7L, we now argue that there exists a thresholding Thr\u03b8\u2217(Z) \u2208 {0, 1}n\u00d7L such that Risk\u03b1(Thr\u03b8\u2217(Z),Y) is bounded by the l-regret of a strongly proper loss", "author": ["R \u00d7 R"], "venue": null, "citeRegEx": "\u2192,? \\Q2010\\E", "shortCiteRegEx": "\u2192", "year": 2010}, {"title": "Sampling from Exponential Distribution We now consider the generalized matrix completion problem when the values are sampled iid from an exponential distribution parameterized by the input features x \u2208 R. This setting extends that of Lafond (2015)", "author": ["Jain", "Natarajan Appendix B"], "venue": "Let yij \u2208 R", "citeRegEx": "Jain and B.,? \\Q2015\\E", "shortCiteRegEx": "Jain and B.", "year": 2015}, {"title": "The first term in the RHS of above inequality can be bounded first using Lemma", "author": ["Jain", "Natarajan"], "venue": null, "citeRegEx": "Jain and Natarajan,? \\Q2015\\E", "shortCiteRegEx": "Jain and Natarajan", "year": 2015}, {"title": "To bound the first term in the above equation, we can apply Lemma 16-(ii) of Lafond (2015). Lemma 5 gives a bound for the second term", "author": ["\u2016\u0174 \u2212 W"], "venue": null, "citeRegEx": "W.\u2217..,? \\Q2015\\E", "shortCiteRegEx": "W.\u2217..", "year": 2015}, {"title": "2015b), we have for any \u03b4 > 0, with probability at least 1\u2212 \u03b4, each of the following statements", "author": ["Bhatia"], "venue": null, "citeRegEx": "Bhatia,? \\Q2015\\E", "shortCiteRegEx": "Bhatia", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Introduction Predicting relevant labels/items for a given data point is by now a standard task with applications in several domains like recommendation systems (Koren et al., 2009), document tagging, image tagging (Prabhu and Varma, 2014), etc.", "startOffset": 160, "endOffset": 180}, {"referenceID": 14, "context": ", 2009), document tagging, image tagging (Prabhu and Varma, 2014), etc.", "startOffset": 41, "endOffset": 65}, {"referenceID": 7, "context": "Furthermore, often the location of missing labels may not be available like in the positive-unlabeled learning setting (Hsieh et al., 2015).", "startOffset": 119, "endOffset": 139}, {"referenceID": 13, "context": "On the other hand, most of the existing collaborative filtering/matrix completion methods only focus on decomposable losses like RMSE, sum of logistic loss (Lafond, 2015; Yu et al., 2014), which are not effective in real-world systems with large number of labels (Prabhu and Varma, 2014).", "startOffset": 156, "endOffset": 187}, {"referenceID": 17, "context": "On the other hand, most of the existing collaborative filtering/matrix completion methods only focus on decomposable losses like RMSE, sum of logistic loss (Lafond, 2015; Yu et al., 2014), which are not effective in real-world systems with large number of labels (Prabhu and Varma, 2014).", "startOffset": 156, "endOffset": 187}, {"referenceID": 14, "context": ", 2014), which are not effective in real-world systems with large number of labels (Prabhu and Varma, 2014).", "startOffset": 83, "endOffset": 107}, {"referenceID": 10, "context": "Our framework is motivated by a simple observation that has been used in other contexts as well (Kot\u0142owski and Dembczy\u0144ski, 2015; Koyejo et al., 2015): for a large class of metrics \u03a8, simply thresholding the class probability vector leads to bayes-optimal estimators.", "startOffset": 96, "endOffset": 150}, {"referenceID": 12, "context": "Our framework is motivated by a simple observation that has been used in other contexts as well (Kot\u0142owski and Dembczy\u0144ski, 2015; Koyejo et al., 2015): for a large class of metrics \u03a8, simply thresholding the class probability vector leads to bayes-optimal estimators.", "startOffset": 96, "endOffset": 150}, {"referenceID": 13, "context": "For general multilabel setting, a direct application of existing results, such as (Lafond, 2015) leads to weak bounds.", "startOffset": 82, "endOffset": 96}, {"referenceID": 8, "context": "In fact, our result strictly generalizes the result by Lafond (2015), which is for general matrix completion with exponential family noise, to the general inductive matrix completion setting (Jain and Dhillon, 2013) with exponential family noise.", "startOffset": 191, "endOffset": 215}, {"referenceID": 8, "context": "For example, Koyejo et al. (2015) establish that for a large class of performance metrics, the optimal solution is to compute a score vector over all the labels and selecting all the labels whose score is greater than a constant.", "startOffset": 13, "endOffset": 34}, {"referenceID": 8, "context": "Our framework is motivated by a simple observation that has been used in other contexts as well (Kot\u0142owski and Dembczy\u0144ski, 2015; Koyejo et al., 2015): for a large class of metrics \u03a8, simply thresholding the class probability vector leads to bayes-optimal estimators. Hence, the goal would be to estimate per-label class probabilities accurately. To this end, we show that by using a \u03bb-strongly proper loss along with appropriate thresholding leads to bounded regret wrt. \u03a8 (Theorem 1). Note that the threshold can be learned using cross-validation over a small fraction of the training data. Moreover, \u03bb-strong convexity of the loss function ensures that by minimizing a nuclearnorm regularized ERM (with risk measured by the selected loss function) wrt. a parameter matrix W \u2208 Rd\u00d7L, we can bound the regret in \u03a8 by regret in estimation of the optimal W (Theorem 1); here, d is the dimensionality of the data and is equal to number of users in case of recommender system. Hence, this result allows us to focus on estimation of W in various different settings such as: a) one-bit matrix completion (Theorem 2), popularly used in recommender systems with only like/dislike information, b) one-bit matrix completion with PU learning (Theorem 4) applicable to recommender systems where only \u201clikes\" or positive feedback is observed, and c) general multi-label learning with missing labels (Theorem 3). For one-bit matrix completion (and the related PU setting), we obtain our final regret bound by adapting existing results from Lafond (2015) and Hsieh et al.", "startOffset": 97, "endOffset": 1540}, {"referenceID": 7, "context": "For one-bit matrix completion (and the related PU setting), we obtain our final regret bound by adapting existing results from Lafond (2015) and Hsieh et al. (2015), respectively.", "startOffset": 145, "endOffset": 165}, {"referenceID": 7, "context": "For one-bit matrix completion (and the related PU setting), we obtain our final regret bound by adapting existing results from Lafond (2015) and Hsieh et al. (2015), respectively. For general multilabel setting, a direct application of existing results, such as (Lafond, 2015) leads to weak bounds. A main technical contribution of our work is to analyze the parameter estimation problem in this setting and provide tight regret bounds. In fact, our result strictly generalizes the result by Lafond (2015), which is for general matrix completion with exponential family noise, to the general inductive matrix completion setting (Jain and Dhillon, 2013) with exponential family noise.", "startOffset": 145, "endOffset": 506}, {"referenceID": 7, "context": "For one-bit matrix completion (and the related PU setting), we obtain our final regret bound by adapting existing results from Lafond (2015) and Hsieh et al. (2015), respectively. For general multilabel setting, a direct application of existing results, such as (Lafond, 2015) leads to weak bounds. A main technical contribution of our work is to analyze the parameter estimation problem in this setting and provide tight regret bounds. In fact, our result strictly generalizes the result by Lafond (2015), which is for general matrix completion with exponential family noise, to the general inductive matrix completion setting (Jain and Dhillon, 2013) with exponential family noise. Hence, it should have applications beyond our framework as well. Finally, we illustrate our framework and algorithms on synthetic as well as real-world datasets. Our method exhibits significant improvement over a natural extension of the method by Koyejo et al. (2015) that optimizes \u03a8 directly but ignores label correlations, hence does not handle missing labels in a principled manner.", "startOffset": 145, "endOffset": 953}, {"referenceID": 7, "context": "For one-bit matrix completion (and the related PU setting), we obtain our final regret bound by adapting existing results from Lafond (2015) and Hsieh et al. (2015), respectively. For general multilabel setting, a direct application of existing results, such as (Lafond, 2015) leads to weak bounds. A main technical contribution of our work is to analyze the parameter estimation problem in this setting and provide tight regret bounds. In fact, our result strictly generalizes the result by Lafond (2015), which is for general matrix completion with exponential family noise, to the general inductive matrix completion setting (Jain and Dhillon, 2013) with exponential family noise. Hence, it should have applications beyond our framework as well. Finally, we illustrate our framework and algorithms on synthetic as well as real-world datasets. Our method exhibits significant improvement over a natural extension of the method by Koyejo et al. (2015) that optimizes \u03a8 directly but ignores label correlations, hence does not handle missing labels in a principled manner. For example, our method achieves 12% higher F1-measure on a benchmark dataset than that by Koyejo et al. (2015).", "startOffset": 145, "endOffset": 1184}, {"referenceID": 6, "context": "Gao and Zhou (2013) study consistency and surrogate losses for", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "Existing theoretical guarantees for 1-bit matrix completion methods used in recommender systems focus solely on RMSE or 0-1 loss (Lafond, 2015; Hsieh et al., 2015).", "startOffset": 129, "endOffset": 163}, {"referenceID": 7, "context": "Existing theoretical guarantees for 1-bit matrix completion methods used in recommender systems focus solely on RMSE or 0-1 loss (Lafond, 2015; Hsieh et al., 2015).", "startOffset": 129, "endOffset": 163}, {"referenceID": 5, "context": "Dembczynski et al. (2012) consider expected pairwise ranking loss in multilabel learning, show that the problem decomposes into independent binary problems, and provide regret bound for the same.", "startOffset": 0, "endOffset": 26}, {"referenceID": 5, "context": "Dembczynski et al. (2012) consider expected pairwise ranking loss in multilabel learning, show that the problem decomposes into independent binary problems, and provide regret bound for the same. Yun et al. (2014) consider the learning to rank problem, where the goal is to rank the relevant labels for a given instance.", "startOffset": 0, "endOffset": 214}, {"referenceID": 17, "context": "Following the low-rank inductive matrix completion model (Yu et al., 2014; Zhong et al., 2015), we let W \u2217 \u2208 Rd\u00d7L be the parameter matrix and gj(xi;W) = g(\u3008xi,w j \u3009) where w\u2217 j is the jth column of W corresponding to the jth label, for some differentiable function g : R \u2192 [0, 1].", "startOffset": 57, "endOffset": 94}, {"referenceID": 19, "context": "Following the low-rank inductive matrix completion model (Yu et al., 2014; Zhong et al., 2015), we let W \u2217 \u2208 Rd\u00d7L be the parameter matrix and gj(xi;W) = g(\u3008xi,w j \u3009) where w\u2217 j is the jth column of W corresponding to the jth label, for some differentiable function g : R \u2192 [0, 1].", "startOffset": 57, "endOffset": 94}, {"referenceID": 3, "context": "When we do not observe feature vectors x, as in the classical recommender system or matrix completion setting, the above model (1) reduces to the widely studied 1-bit matrix completion model (Cai and Zhou, 2013; Davenport et al., 2014):", "startOffset": 191, "endOffset": 235}, {"referenceID": 4, "context": "When we do not observe feature vectors x, as in the classical recommender system or matrix completion setting, the above model (1) reduces to the widely studied 1-bit matrix completion model (Cai and Zhou, 2013; Davenport et al., 2014):", "startOffset": 191, "endOffset": 235}, {"referenceID": 12, "context": "In this work, we consider a large family of non-decomposable metrics (Koyejo et al., 2015) that constitutes linear-fractional functions of (multi-label analogues of) true positives, false positives, false negatives and true negatives defined below.", "startOffset": 69, "endOffset": 90}, {"referenceID": 11, "context": "Koyejo et al. (2015) showed that the Bayes optimal \u03a8\u2217 thresholds the conditional probability of each label j, i.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "In many cases, W exhibits some structure, such as low-rankness, reflecting correlation between labels (Yu et al., 2014; Zhong et al., 2015; Davenport et al., 2014).", "startOffset": 102, "endOffset": 163}, {"referenceID": 19, "context": "In many cases, W exhibits some structure, such as low-rankness, reflecting correlation between labels (Yu et al., 2014; Zhong et al., 2015; Davenport et al., 2014).", "startOffset": 102, "endOffset": 163}, {"referenceID": 4, "context": "In many cases, W exhibits some structure, such as low-rankness, reflecting correlation between labels (Yu et al., 2014; Zhong et al., 2015; Davenport et al., 2014).", "startOffset": 102, "endOffset": 163}, {"referenceID": 10, "context": "Koyejo et al. (2015) proposed a simple consistent plug-in estimator algorithm, which first computes conditional marginals P(yj|x) independently for each label j, and then estimates a threshold jointly to optimize \u03a8.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": ", 2015; Davenport et al., 2014). Statistically, capturing correlations via a low-rank structure could help improve the sample complexity for recovery, and computationally, it would help reduce space and time complexity of the learning procedure. Our proposed algorithm is presented in Algorithm 1. In Step 1, we solve a traceregularized minimization problem to estimate the parameter matrix W, where the function l can be any bounded loss such as the squared, the logistic or the squared Hinge loss. In particular, using the logistic loss corresponds to the maximum likelihood estimation of the sampling model (1). Yu et al. (2014) also solve essentially the same objective as (6), except for the additional bound constraint on entries of XW.", "startOffset": 8, "endOffset": 632}, {"referenceID": 12, "context": "The definitions in (Koyejo et al., 2015) do not include general sampling distribution \u03c0, but the results can be generalized in a straight-forward manner.", "startOffset": 19, "endOffset": 40}, {"referenceID": 0, "context": "1 Low l-regret implies low \u03a8-regret Our first main result connects \u03a8-regret to regret with respect to a strongly proper loss function l (Agarwal, 2014).", "startOffset": 136, "endOffset": 151}, {"referenceID": 10, "context": "Proof technique is based on (Kot\u0142owski and Dembczy\u0144ski, 2015), where they derive similar bound in the binary classification setting.", "startOffset": 28, "endOffset": 61}, {"referenceID": 13, "context": "Then (6) reduces to the optimization problem considered by Lafond (2015). We have the following regret bound for the estimator Z = \u0174 obtained in Step 2 of Algorithm 1 (Note that X is just treated as identity in this setting).", "startOffset": 59, "endOffset": 73}, {"referenceID": 13, "context": "Remark 4 (Comparing (Lafond, 2015)).", "startOffset": 20, "endOffset": 34}, {"referenceID": 13, "context": "If we directly apply the method and the analysis of (Lafond, 2015), the resulting bounds are very weak; in fact, when n \u2265 L and |\u03a9| = O(n), which is quite common in the multi-label scenario, the ensuing bound suggests that the estimator is not even consistent, even when \u03c0 is uniform.", "startOffset": 52, "endOffset": 66}, {"referenceID": 12, "context": "Remark 5 (Comparing (Koyejo et al., 2015)).", "startOffset": 20, "endOffset": 41}, {"referenceID": 12, "context": "The plugin-in estimator algorithm of (Koyejo et al., 2015) estimates w\u2217 j for each label j independently, and learns a common threshold as in Algorithm 1.", "startOffset": 37, "endOffset": 58}, {"referenceID": 13, "context": "Our proof sketch is based on Lafond (2015), but requires bounding certain quantities carefully.", "startOffset": 29, "endOffset": 43}, {"referenceID": 7, "context": "In this setting, we can use the approach of (Hsieh et al., 2015), where they consider a two-stage sampling model: sample yij using (2) for all i, j \u2208 [n] \u00d7 [L] (or using (1) when features are available), and then flip a fraction \u03c1 of the sampled 1\u2019s to 0\u2019s, resulting in \u1ef8.", "startOffset": 44, "endOffset": 64}, {"referenceID": 7, "context": "Let Z = X\u0174, where \u0174 is obtained by solving the unbiased estimator objective of Hsieh et al. (2015). With probability at least 1\u2212 \u03b4, there exists absolute constant C such that: Regl(Z) \u2264 \u221a 6 \u221a log(2/\u03b4) \u221a nL(1\u2212 \u03c1) + 2C .", "startOffset": 79, "endOffset": 99}, {"referenceID": 14, "context": "This PU learning result is particularly very useful in extreme classification setting (Bhatia et al., 2015a; Prabhu and Varma, 2014); where there are too many labels and is unrealistic to get feedback on every label, but possible to obtain a small subset of relevant labels for instances.", "startOffset": 86, "endOffset": 132}, {"referenceID": 12, "context": "Furthermore, even when only 10% of the observations are revealed, we observe that the proposed method achieves very high F1 as well as accuracy values, compared to learning the columns of W independently via the plugin estimator method proposed by (Koyejo et al., 2015) (followed by learning a threshold).", "startOffset": 248, "endOffset": 269}, {"referenceID": 17, "context": "2 Real-world data We consider five real-world multi-label datasets widely used as benchmarks (Bhatia et al., 2015a; Yu et al., 2014).", "startOffset": 93, "endOffset": 132}, {"referenceID": 11, "context": "Dataset Koyejo et al. (2015) Algorithm 1 Koyejo et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 11, "context": "Dataset Koyejo et al. (2015) Algorithm 1 Koyejo et al. (2015) Algorithm 1 micro F1 micro F1 Accuracy Accuracy CAL500 0.", "startOffset": 8, "endOffset": 62}, {"referenceID": 12, "context": "Table 1: Comparison of proposed algorithm and plugin-estimator method of (Koyejo et al., 2015) on multi-label micro F1 and Hamming (i.", "startOffset": 73, "endOffset": 94}, {"referenceID": 12, "context": "The learned model is much more compact than that of (Koyejo et al., 2015) (k(d+L) vs dL parameters).", "startOffset": 52, "endOffset": 73}], "year": 2016, "abstractText": "We consider the problem of recommending relevant labels (items) for a given data point (user). In particular, we are interested in the practically important setting where the evaluation is with respect to non-decomposable (over labels) performance metrics like the F1 measure, and the training data has missing labels. To this end, we propose a generic framework that given a performance metric \u03a8, can devise a regularized objective function and a threshold such that all the values in the predicted score vector above and only above the threshold are selected to be positive. We show that the regret or generalization error in the given metric \u03a8 is bounded ultimately by estimation error of certain underlying parameters. In particular, we derive regret bounds under three popular settings: a) collaborative filtering, b) multilabel classification, and c) PU (positive-unlabeled) learning. For each of the above problems, we can obtain precise non-asymptotic regret bound which is small even when a large fraction of labels is missing. Our empirical results on synthetic and benchmark datasets demonstrate that by explicitly modeling for missing labels and optimizing the desired performance metric, our algorithm indeed achieves significantly better performance (like F1 score) when compared to methods that do not model missing label information carefully.", "creator": "LaTeX with hyperref package"}}}