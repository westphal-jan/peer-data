{"id": "1606.02206", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "A Minimax Approach to Supervised Learning", "abstract": "Given a task of predicting $Y$ from $X$, a loss function $L$, and a set of probability distributions $\\Gamma$, what is the optimal decision rule minimizing the worst-case expected loss over $\\Gamma$? In this paper, we address this question by introducing a generalization of the principle of maximum entropy. Applying this principle to sets of distributions with a proposed structure, we develop a general minimax approach for supervised learning problems, that reduces to the maximum likelihood problem over generalized linear models. Through this framework, we develop two classification algorithms called the minimax SVM and the minimax Brier classifier. The minimax SVM, which is a relaxed version of the standard SVM, minimizes the worst-case 0-1 loss over the structured set of distribution, and by our numerical experiments can outperform the SVM. We also explore the application of the developed framework in robust feature selection.", "histories": [["v1", "Tue, 7 Jun 2016 16:39:09 GMT  (205kb,D)", "http://arxiv.org/abs/1606.02206v1", null], ["v2", "Mon, 1 Aug 2016 01:20:30 GMT  (207kb,D)", "http://arxiv.org/abs/1606.02206v2", null], ["v3", "Thu, 4 Aug 2016 23:03:43 GMT  (207kb,D)", "http://arxiv.org/abs/1606.02206v3", null], ["v4", "Sun, 6 Nov 2016 23:19:58 GMT  (284kb,D)", "http://arxiv.org/abs/1606.02206v4", null], ["v5", "Tue, 4 Jul 2017 01:56:04 GMT  (855kb,D)", "http://arxiv.org/abs/1606.02206v5", null]], "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["farzan farnia", "david tse"], "accepted": true, "id": "1606.02206"}, "pdf": {"name": "1606.02206.pdf", "metadata": {"source": "CRF", "title": "A Minimax Approach to Supervised Learning", "authors": ["Farzan Farnia", "David Tse"], "emails": ["farnia@stanford.edu", "dntse@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us are able to decide for ourselves what we want to do and what we want to do."}, {"heading": "2 Principle of Maximum Conditional Entropy", "text": "In this section, we present a conditional version of the key definitions and results developed in [5]. In Figure 1, we propose the principle of maximum conditional entropy to divide step 3 into 3a and 3b. Furthermore, we define and characterize Baye's decision rules under various loss functions to address step 3b."}, {"heading": "2.1 Decision Problems, Bayes Decision Rules, Conditional Entropy", "text": "Consider a decision problem. Here, the decision-maker observes X-X - Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-"}, {"heading": "2.2 Examples", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1 Logarithmic Loss", "text": "For each y-Y and the distribution QY is defined Llog (y, QY) = \u2212 logQY (y). (7) It can be seen that below the logarithmic loss is Hlog (Y), Hlog (Y | X), Ilog (X; Y) the well-known unconditional Shannon entropy and mutual information [11]. The divergence measure is the well-known KL divergence. Also the Bayes decision rule for each distribution PX, Y is given by \"Bayes (x) = PY | X (\u00b7 x).\" (8)"}, {"heading": "2.2.2 0-1 loss function", "text": "The 0-1 loss function is defined for each y, y-Y as L0-1 (y, y-Y) = I (y-6 = y). Then we can represent H0-1 (Y) = 1 \u2212 max y-Y PY (y), H0-1 (Y | X) = 1 \u2212 \u2211 x-X max y-Y PX, Y (x, y).Under the 0-1 loss function, the Bayes decision rule for a distribution PX, Y is the known maximum a posteriori (MAP), i.e., Bayes (x) = argmax y-YPY | X (y | x). (9)"}, {"heading": "2.2.3 Quadratic loss function", "text": "The quadratic loss function is defined as L2 (y, y) = (y \u2212 y) 2. It can be regarded as H2 (Y) = Var (Y), H2 (Y | X) = E [Var (Y | X)], I2 (X; Y) = Var (E [Y | X]). Furthermore, the Bayes decision rule for each PX, Y is the known Minimum Middle Square Error (MMSE), which is called \"Bayes (x) = E [Y | X = x].\" (10)"}, {"heading": "2.2.4 Brier loss function", "text": "In contrast to logarithmic losses and 0-1 loss functions, the quadratic loss function does not make perfect sense of a discrete variable Y. The Brier loss function [12] is an adapted version of the quadratic loss function, which aims at a discrete Y distribution, where for each distribution QY stands for Y and a result y-Y, LBR (y, QY) = equy \u2212 qY-22. (11) Here \u03b4y denotes a vector of size | Y | X, 1 for index y and 0 elsewhere, and qY stands for the vector of probabilities for QY. Then HBR (Y) = 1 \u2212 equance pY-22, HBR (Y | X) = 1 \u2212 E [PY | X)]]]] in view of the distribution PX, Y is the Bayes decision rule of uniquelylated Bayes (x) = equelyconditioned Bayes (x) = equelyconditioned variability (x)."}, {"heading": "2.3 Principle of Maximum Conditional Entropy & Robust Bayes decision rules", "text": "In view of a distribution theorem, we consider the following Minimax problem to find a decision rule that minimizes the expected loss in the worst-case scenario via distribution. (16) Note that by a randomized mapping we mean a random selection of members of F, the space of deterministic functions from X to A, according to a certain distribution. We call any solution to the above problem a robust Bayes decision rule against distribution P. If it is convex, the following theorem guarantees the existence of a saddle point for (16), under certain mild conditions. Therefore, Theorem 2 motivates a generalization of the maximum entropy principle to find robust Bayes decision rules. Theorem 2 assumes that Bayes that Bayes is convex and that there is a Bayes rule under each P point."}, {"heading": "3 Prediction via Maximum Conditional Entropy Principle", "text": "Consider a prediction task with target variables Y and feature vector X = (X1,., Xd). Note that we do not have to consider the variables discrete. (D) As discussed above, the maximum conditional entropy principle reduces to (16) to (17). Note, however, that (16) and (17) the formulation of steps 3 and 3a in Figure 1, respectively, (17) a general formulation of the (17) common distribution PX, Y leads to exponential computational complexity in the feature dimension. Therefore, the key question is which structures of (P) in Figure 2 we can efficiently solve. (17) In this section, we propose a specific structure for distribution, under which we provide an efficient solution for steps 3a and 3b in Figure 1."}, {"heading": "3.1 Logarithmic Loss: Logistic Regression", "text": "For the classification of Y-Y = {1,.., t + 1}, we allow \u03b8 (Y) to be the most uniform encoding of the variable Y, i.e. \u03b8i (Y) = I (Y = i) for 1 \u2264 i \u2264 t. In view of this, we exclude i = t + 1 as I (Y = t + 1) = 1 \u2212 \u2211 t i = 1 I (Y = i). In view of this assumption, the logarithmic lossF\u03b8 (z) = log (1 + t \u0445 j = 1 exp (zj)) results in the multinomic logistic regression model [21]. Also, the RHS of (20) would be the regularized maximum probability problem for this specific GLM. This discussion is well studied in the literature and simply based on the duality presented in [8]."}, {"heading": "3.2 0-1 Loss: Minimax SVM", "text": "(Consider the same classification setting and the formation described at the beginning of the last subsection. Since we show in the appendix that we can calculate the gradient of F\u03b8 for the 0-1 loss using the following procedure. In view of z-Rt, let us leave z-Rt = (z, 0). Let us see the permutation so that z-Rt is sorted in descending order, i.e. i-Rt = z-Rt (j). We find the smallest k, at x-Rt (i) \u2212 z-Rt (k + 1) > 1. If this does not apply to any k, let us leave k = t + 1. Then we find the smallest k, at x-Rt (z) i = {1 / k, if x-K (i) \u2264 k, 0 otherwise. (27) Let us know that F\u03b8 (0) = t / (t + 1), which is the formation 0-1."}, {"heading": "3.3 Brier Loss: Minimax Brier Classifier", "text": "Consider the same classification setting and the problems defined in the last two subparagraphs. In the Annex, we show that we can characterize the gradient of the Brier classification by repeating the same procedure as in the Minimax classification with two modifications. (1) First, we change the level for searching for the smallest k as a minimum classification (1). (2) The maximum Y classification as a minimum classification (2). (2). (2) The maximum Y classification (2). (2). (2). (2). (2). (2). (3). (3). (3). (3). (3). (3). (3). (3). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (3). (3). (3). (3). (3.........). (3. (........). (3.)...... (......... (3. (.....)...... (.....). (3...... (.....)...... (3......). (.....). (.....)....... (3...........). (.....)... (..........). (3................)...... (.....).......................... (.....)......).. (..........)..... (......).....).... (..... (.....).....)..). (..... (.....).....).). (.....). (.....). (.....).). (.....). (.....). (.....). (.....). (.....).). (.....). (.....).). (.....).). (.....).). (..... (.....). (.....).). (.....).).). ("}, {"heading": "3.4 Quadratic Loss: Linear Regression", "text": "Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-"}, {"heading": "4 Robust Feature Selection", "text": "Using a Minimax criterion across a number of distributions, we solve the following problem to select the most informative subset of k features. Here, we evaluate a feature subset based on its minimum worst-case loss over the indices in S. Theorem 2, where XS limits the feature vector X to the indices in S. Theorem 2. (37) Reducing the least favorable generalized information I (XS; Y) to the maximum P number H (Y | XS), (38) which assumes H (Y) is fixed across all distributions, becomes equivalent to selecting a subset S to maximize the least favorable generalized information I (XS; Y) to the least favorable generalized information I (i.e.argmax | S | k min P (XS; Y)."}, {"heading": "5 Numerical Experiments", "text": "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification data sets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete R\u00e9nyi Classifiers (DRC) [4]. The results are summarized in Table 1, where the numbers indicate the error percentage in the classification task. We implemented the three mmSVM, mmBC, and mmRBC, and mmRBC, using the gradient lineage to solve the RHS of (20) with an additional regulator."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Meisam Razaviyayn for the helpful discussions on the topic and the evidence."}, {"heading": "6 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Proof of Theorem 1", "text": "In this proof, we use a known result for \u03c1m (X; Z) for a BernoulliZ-Z = {0, 1} with the probabilities p0, p1 [32]. For the sake of simplicity, we use px, z and pz | x to denote PX, Z (x, z) and PZ | X (z | x), respectively. Then, we use \u03c12m (X; Z) = 1p0p1 \u2211 x [p0p 2 x, 1 + p1p 2 x, 0 px] \u2212 1 = 1p0p1 \u2211 x [px (p0p 2 1 | x + p1p 2 0 | x)] \u2212 1 = 1p0p1 \u2211 x [px (1 \u2212 p21 | x + p 2 0 | x) + p0 \u2212 p1 \u00b2 (p21 | p20 | x)] \u2212 1 = 12p0p0p0p1 \u2212 pppppppp1 \u2212 p \u2212 \u2212 \u2212 xp.p.pi = 1pi = 1pY (p20 | x)."}, {"heading": "6.2 Proof of Theorem 2", "text": "Let us first recall the assumptions of theorem 2: \u2022 \u2022 \u2022 For each distribution (P) there is a Bayes decision rule. \u2022 We assume continuity in Bayes decision rules, i.e. if a sequence of distributions (Qn) \u00b2 n = 1 converges with the corresponding Bayes decision rules n = 1. Let it be a Bayes decision rule for P. We must show that the Bayes decision rule is a robust Bayes decision rule against the expected loss of n. To show this, it is sufficient to show that (P) is the (P) decision on inequality (P)."}, {"heading": "6.3 Proof of Theorem 3", "text": "Let us remember the definition of the target set (Q) = \"Problem\" (Q) = \"Problem\" (X) = \"Problem\" (X) = \"Problem\" (X) = \"Problem\" (X) = \"Problem\" (X) = \"Problem\" (X) = \"Problem\" (X) = \"Problem\" (X) = \"Problem\" (X) = \"Problem\" (X) = \"Problem\" (X) = \"Problem\" (X). Definition \"Problem\" (X): \"EQ\" (Y) \"X\" (Y), \"EP\" (Y), \"EQX\" (Y), \"EQX\" (X), \"X\"), \"Problem\" (X), \"X\" X \"(X),\" Problem \"X\" (X), \"X\" X \"(X),\" Problem \"(X)."}, {"heading": "6.4 F\u03b8 derivation for the 0-1 Loss, minimax SVM", "text": "Here we derive the optimal solution for the 0-1 loss function, where \"k\" is the described unit encoding, which is \"i\" (Y) = I (Y = i) for 1 \u2264 i \u2264 t. If P (Y = i) = Pi for 1 \u2264 i \u2264 t + 1, then \"H\" (Y) + E [\u03b8 (Y)] T = 1 \u2212 max 1 \u2264 i \u2264 t + 1 pi + 1 pizi. (54) To solve the above problem, we define \"F\" (z) = argmax p \"Rt + 1: p \u2265 0,1Tp = 1t\" p \"i = 1 pizi \u2212 max 1 pizi + 1 pizi (55) and rewrite the objective\" z \"(z, 0) and the objective\" z \"i.\""}, {"heading": "6.5 F\u03b8 derivation for the Brier Loss, minimax Brier classifer", "text": "Similar to the proof given for the 0-1 loss, we derive for the Brier loss function with the described unit encoding. If P (Y = i) = Pi for 1 \u2264 i \u2264 t + 1, then H (Y) + E [\u03b8 (Y)] returns T = 1 \u2212 t + 1 \u2211 i = 1 [p2i] + t \u2211 i = 1 [pizi]. (61) Consequently, we redefine z \u2212 t + 1 (z, 0) and rewrite the target ast + 1 \u0445 (z) = argmax p Rt + 1: p \u2265 0,1Tp = 1t \u2211 i = 1 [pizi \u2212 p2i] \u2212 pizi + 1 (62)."}, {"heading": "6.6 Quadratic Loss: Linear Regression", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.6.1 F\u03b8 derivation", "text": "Here we find F\u03b8 (z) = maxP, PY H (Y) + E [\u03b8 (Y)] T z for \u03b8 (Y) = Y and PY = {PY: E [Y 2] \u2264 \u03c12}. Since for the square loss H (Y) = Var (Y) = E [Y] -E [Y] [Y] 2 the problem is equivalent to F\u03b8 (z) = max E [Y 2] \u2264 \u03c12E [Y 2] \u2212 E [Y] 2 + zE [Y] (66) as E [Y] 2 \u2264 E [Y 2], it can be seen for the solution EP [Y 2] = \u03c12, and therefore we solve F\u03b8 (z) = max | E [Y] | \u04452 \u2212 E [Y] 2 + zE [Y] = {\u03c12 + z2 / 4, if | z / 2 | \u2264."}, {"heading": "6.6.2 Applying Theorem 3 while restricting PY", "text": "For the quadratic loss, we first change PY = {PY: E [Y] \u2264 \u03c12} and then apply Theorem 3. Note that by modifying F\u03b8 on the basis of the new PY, we also solve a modified version of the maximum conditional entropy problem P: PX, Y-Q (Q) \u0441x: PY | X = x-PYH (Y | X) (68). In the case of PY = {PY: E [Y] \u2264 \u03c12}, Theorem 3 retains its validity in view of the above modification of the maximum conditional entropy problem. This is because the inequality condition E [Y 2 | X = x] \u2264 \u03c12 is linear in PY | X = x, and thus the problem remains convex and strongly dual. Even if we shift the limitations of wi = EP [EP) to the objective function, we get a similar double problem in Asup X | w-PY (X-PY), X-PY (X-PY) and PY-PY (next)."}, {"heading": "6.6.3 Derivation of group lasso", "text": "In order to derive the group lasso, we slightly change the structure of the problem. In view of the resolution of the subsets I1,.., Ik, we consider a series of distributions that have the following structure: GL (Q) = {PX, Y: PX = QX, (70) = 1 \u2264 j \u2264 k: EP [YXIj] \u2212 EQ [YXIj] \u2264 j. (71) To prove this identity, we can apply the same proof provided for Theorem 3. We only need E-j = EQ [YXIj] = EQ [Y\u03b1TX] and Cj = {u-u \u2212 E-j."}, {"heading": "6.7 Proof of Theorem 5", "text": "Since entropy measures the expected loss in a distribution, it is sufficient to show that the rate of entropy misclassification is in any case twice as high as entropy in any case due to the maximum entropy in Brier compared to entropy in any case. To show the first part, it should be noted that for each sequence (ai) ni = 1, and for each sequence (ai) ni = 2aj \u2264 a2i + 2 = j a2i. Therefore, since the conditions of theorem 2 apply, for each distribution P-Y (X) 6 = Y + 2 = Y = 2, x px, i (1 \u2212 p \u00b2), i (2 \u2212 2 \u2212 P \u00b2)."}], "references": [{"title": "A robust minimax approach to classification", "author": ["Gert RG Lanckriet", "Laurent El Ghaoui", "Chiranjib Bhattacharyya", "Michael I Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Discrete chebyshev classifiers", "author": ["Elad Eban", "Elad Mezuman", "Amir Globerson"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Discrete r\u00e9nyi classifiers", "author": ["Meisam Razaviyayn", "Farzan Farnia", "David Tse"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Game theory, maximum entropy, minimum discrepancy and robust bayesian decision theory", "author": ["Peter D. Gr\u00fcnwald", "Philip Dawid"], "venue": "The Annals of Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Information theory and statistical mechanics", "author": ["Edwin T Jaynes"], "venue": "Physical review,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1957}, {"title": "The minimum information principle for discriminative learning", "author": ["Amir Globerson", "Naftali Tishby"], "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "A maximum entropy approach to natural language processing", "author": ["Adam L Berger", "Vincent J Della Pietra", "Stephen A Della Pietra"], "venue": "Computational linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Agnostic learning of monomials by halfspaces is hard", "author": ["Vitaly Feldman", "Venkatesan Guruswami", "Prasad Raghavendra", "Yi Wu"], "venue": "SIAM Journal on Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Coherent measures of discrepancy, uncertainty and dependence, with applications to bayesian predictive experimental design", "author": ["Philip Dawid"], "venue": "Technical Report 139,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Verification of forecasts expressed in terms of probability", "author": ["Glenn W Brier"], "venue": "Monthly weather review,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1950}, {"title": "A connection between correlation and contingency", "author": ["H.O. Hirschfeld"], "venue": "In Mathematical Proceedings of the Cambridge Philosophical Society,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1935}, {"title": "Das statistische problem der korrelation als variations-und eigenwertproblem und sein zusammenhang mit der ausgleichsrechnung", "author": ["H. Gebelein"], "venue": "ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift fu\u0308r Angewandte Mathematik und Mechanik,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1941}, {"title": "On measures of dependence", "author": ["A. R\u00e9nyi"], "venue": "Acta mathematica hungarica,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1959}, {"title": "Unifying divergence minimisation and statistical inference via convex duality", "author": ["Yasemin Altun", "Alexander Smola"], "venue": "In Learning Theory: Conference on Learning Theory COLT 2006,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Maximum entropy density estimation with generalized regularization and an application to species distribution modeling", "author": ["Miroslav Dud\u00edk", "Steven J Phillips", "Robert E Schapire"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Semi-supervised learning via generalized maximum entropy", "author": ["AN Erkan", "Y Altun", "Teh M Titterington"], "venue": "In Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Generalized linear models, volume 37", "author": ["Peter McCullagh", "John A Nelder"], "venue": "CRC press,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1989}, {"title": "Fast rates for regularized objectives", "author": ["Karthik Sridharan", "Shai Shalev-Shwartz", "Nathan Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "The elements of statistical learning, volume 1", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Pattern recognition and machine learning", "author": ["Christopher M Bishop"], "venue": "springer,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1996}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["Ming Yuan", "Yi Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Robust regression and lasso", "author": ["Huan Xu", "Constantine Caramanis", "Shie Mannor"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "A unified robust regression model for lasso-like algorithms", "author": ["Wenzhuo Yang", "Huan Xu"], "venue": "In Proceedings of The International Conference on Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Normalized mutual information feature selection", "author": ["Pablo Est\u00e9vez", "Michel Tesmer", "Claudio Perez", "Jacek M Zurada"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["CK Chow", "CN Liu"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1968}, {"title": "On maximal correlation, hypercontractivity, and the data processing inequality studied by Erkip and Cover", "author": ["V. Anantharam", "A. Gohari", "S. Kamath", "C. Nair"], "venue": "arXiv preprint arXiv:1304.6133,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Some special cases of this minimax approach, which are based on learning a prediction rule from low-order marginal/moments, have been addressed in the literature: [2] solves a robust minimax classification problem for continuous settings with fixed first and second-order moments; [3] develops a classification approach by minimizing the worst-case hinge loss subject to fixed low-order marginals; And [4] fits a model minimizing the maximal correlation under fixed pairwise marginals to design a robust classification scheme.", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "Some special cases of this minimax approach, which are based on learning a prediction rule from low-order marginal/moments, have been addressed in the literature: [2] solves a robust minimax classification problem for continuous settings with fixed first and second-order moments; [3] develops a classification approach by minimizing the worst-case hinge loss subject to fixed low-order marginals; And [4] fits a model minimizing the maximal correlation under fixed pairwise marginals to design a robust classification scheme.", "startOffset": 281, "endOffset": 284}, {"referenceID": 2, "context": "Some special cases of this minimax approach, which are based on learning a prediction rule from low-order marginal/moments, have been addressed in the literature: [2] solves a robust minimax classification problem for continuous settings with fixed first and second-order moments; [3] develops a classification approach by minimizing the worst-case hinge loss subject to fixed low-order marginals; And [4] fits a model minimizing the maximal correlation under fixed pairwise marginals to design a robust classification scheme.", "startOffset": 402, "endOffset": 405}, {"referenceID": 1, "context": "To formulate Step 3 in Figure 1, given a general loss function L and set of distribution \u0393(P\u0302 ) we generalize the problem formulation discussed at [3] to argmin \u03c8\u2208\u03a8 max P\u2208\u0393(P\u0302 ) E [ L ( Y, \u03c8(X) ) ] .", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "If we have to predict Y with no access to X, (1) will reduce to the formulation studied at [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "There, the authors propose to use the principle of maximum entropy [6], for a generalized definition of entropy, to find the optimal prediction minimizing the worst-case loss.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "In [7], the authors propose a conditional version of the principle of maximum entropy, for the specific case of Shannon entropy, and draw the principle\u2019s connection to (1).", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "More importantly, by applying the described idea for the generalized conditional entropy we provide a generalization of the duality derived in [8] between maximum conditional (Shannon) entropy and maximum likelihood for logistic regression.", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "Note that ERM with the 0-1 loss is known to be NP-hard [9].", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "In this section, we provide a conditional version of the key definitions and results developed in [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "We can also define an (unconditional) entropy [5]", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "In [10], the author has defined the same concept to which he calls a coherent dependence measure.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "It can be seen that I(X;Y ) = EPX [D(PY |X , PY ) ] where D is the divergence measure corresponding to the loss L, defined for any two probability distributions PY , QY with Bayes actions aP , aQ as [5] D(PY , QY ) := EP [L(Y, aQ)]\u2212 EP [L(Y, aP )] = EP [L(Y, aQ)]\u2212HP (Y ).", "startOffset": 199, "endOffset": 202}, {"referenceID": 9, "context": "(7) It can be seen that under the logarithmic loss Hlog(Y ), Hlog(Y |X), Ilog(X;Y ) are the well-known unconditional, conditional Shannon entropy and mutual information [11].", "startOffset": 169, "endOffset": 173}, {"referenceID": 10, "context": "The Brier loss function [12] is an adjusted version of the quadratic loss function targeting a discrete Y , where for any distribution QY on Y and an outcome y \u2208 Y , LBR(y,QY ) = \u2016\u03b4 \u2212 qY \u20162.", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "To extend this correlation measure to a measure for non-linear dependence, the HGR (HirschfeldGebelein-R\u00e9nyi) maximal correlation has been proposed in the probability literature [13\u201315].", "startOffset": 178, "endOffset": 185}, {"referenceID": 12, "context": "To extend this correlation measure to a measure for non-linear dependence, the HGR (HirschfeldGebelein-R\u00e9nyi) maximal correlation has been proposed in the probability literature [13\u201315].", "startOffset": 178, "endOffset": 185}, {"referenceID": 13, "context": "To extend this correlation measure to a measure for non-linear dependence, the HGR (HirschfeldGebelein-R\u00e9nyi) maximal correlation has been proposed in the probability literature [13\u201315].", "startOffset": 178, "endOffset": 185}, {"referenceID": 13, "context": "In [15], it has been shown the maximal correlation satisfies several interesting properties.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "To that end, we apply the Fenchel\u2019s duality technique, also used at [16\u201318] to address f-divergence minimization problems.", "startOffset": 68, "endOffset": 75}, {"referenceID": 15, "context": "To that end, we apply the Fenchel\u2019s duality technique, also used at [16\u201318] to address f-divergence minimization problems.", "startOffset": 68, "endOffset": 75}, {"referenceID": 16, "context": "To that end, we apply the Fenchel\u2019s duality technique, also used at [16\u201318] to address f-divergence minimization problems.", "startOffset": 68, "endOffset": 75}, {"referenceID": 17, "context": "We make the key observation that the problem in the RHS of (20), when i = 0 for all i\u2019s, is equivalent to minimizing the negative log-likelihood for fitting a generalized linear model [19] given by \u2022 An exponential family distribution p(y|\u03b7) = h(y) exp ( \u03b7\u03b8(y)\u2212 F\u03b8(\u03b7) ) with the log-partition function F\u03b8 and the sufficient statistic \u03b8(Y ), \u2022 A linear predictor , \u03b7(X) = AX, \u2022 A link function such that E[\u03b8(Y )|X = x] = \u2207F\u03b8(\u03b7(x)).", "startOffset": 184, "endOffset": 188}, {"referenceID": 18, "context": "Then, the theorem is an immediate consequence of Theorem 1 in [20].", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "that gives the multinomial logistic regression model [21].", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "This discussion is well-studied in the literature and straightforward using the duality result shown in [8].", "startOffset": 104, "endOffset": 107}, {"referenceID": 20, "context": "Since max{0, 1\u2212z 2 ,\u2212z} \u2264 max{0, 1\u2212 z}, if we replace \u2016\u03b1\u2016\u2217 with \u03bb\u2016\u03b1\u2016 2 2 in (29), we get a relaxation of the standard SVM formulated with the hinge loss [22].", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "This binary classification problem is the same classification problem formulated with the modified Huber loss function at [24].", "startOffset": 122, "endOffset": 126}, {"referenceID": 2, "context": "Assuming some extra conditions, [4] solves the minimax problem of finding the maximal correlation-minimizing distribution, but for a larger class of distributions where only pairwise marginals are fixed.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "In [4], the authors also show for a binary prediction problem over a convex set of distributions \u0393, there exists a randomized prediction rule based on the maximal correlation-minimizing distribution, achieving a worst-case misclassification rate of at most twice the minimum worst-case misclassification rate.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "The above theorem suggests the minimax Brier classification as a natural extension of the results proven in [4] for the binary case.", "startOffset": 108, "endOffset": 111}, {"referenceID": 22, "context": "\u2013 Lasso [25] when \u2016 \u00b7 \u2016/\u2016 \u00b7 \u2016\u2217 is the `\u221e/`1 pair.", "startOffset": 8, "endOffset": 12}, {"referenceID": 19, "context": "\u2013 Ridge regression [21] when \u2016 \u00b7 \u2016 is the `2-norm.", "startOffset": 19, "endOffset": 23}, {"referenceID": 23, "context": "\u2013 Group lasso [26] with the `1,p regularizer when we adjust \u0393(Q)\u2019s definition for disjoint subsets I1, .", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "Another type of minimax, but non-probabilistic, justification of the robustness of lasso and group lasso as regression algorithms can be found in [27, 28].", "startOffset": 146, "endOffset": 154}, {"referenceID": 25, "context": "Another type of minimax, but non-probabilistic, justification of the robustness of lasso and group lasso as regression algorithms can be found in [27, 28].", "startOffset": 146, "endOffset": 154}, {"referenceID": 22, "context": "It is noteworthy that for the quadratic loss and identity \u03b8, (41) is the same as the lasso [25].", "startOffset": 91, "endOffset": 95}, {"referenceID": 26, "context": "Hence, the `1-regularized logistic regression maximizes the worst-case mutual information over \u0393(Q), which seems superior to the heuristic techniques for maximizing an approximation of the mutual information I(XS ;Y ) in the literature [29, 30].", "startOffset": 236, "endOffset": 244}, {"referenceID": 27, "context": "Hence, the `1-regularized logistic regression maximizes the worst-case mutual information over \u0393(Q), which seems superior to the heuristic techniques for maximizing an approximation of the mutual information I(XS ;Y ) in the literature [29, 30].", "startOffset": 236, "endOffset": 244}, {"referenceID": 1, "context": "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete R\u00e9nyi Classifiers (DRC) [4].", "startOffset": 313, "endOffset": 316}, {"referenceID": 0, "context": "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete R\u00e9nyi Classifiers (DRC) [4].", "startOffset": 354, "endOffset": 357}, {"referenceID": 28, "context": "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete R\u00e9nyi Classifiers (DRC) [4].", "startOffset": 392, "endOffset": 396}, {"referenceID": 2, "context": "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete R\u00e9nyi Classifiers (DRC) [4].", "startOffset": 435, "endOffset": 438}], "year": 2017, "abstractText": "Given a task of predicting Y from X , a loss function L, and a set of probability distributions \u0393, what is the optimal decision rule minimizing the worst-case expected loss over \u0393? In this paper, we address this question by introducing a generalization of the principle of maximum entropy. Applying this principle to sets of distributions with a proposed structure, we develop a general minimax approach for supervised learning problems, that reduces to the maximum likelihood problem over generalized linear models. Through this framework, we develop two classification algorithms called the minimax SVM and the minimax Brier classifier. The minimax SVM, which is a relaxed version of the standard SVM, minimizes the worst-case 0-1 loss over the structured set of distribution, and by our numerical experiments can outperform the SVM. We also explore the application of the developed framework in robust feature selection.", "creator": "LaTeX with hyperref package"}}}