{"id": "1606.02270", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Natural Language Comprehension with the EpiReader", "abstract": "We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model's response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children's Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin.", "histories": [["v1", "Tue, 7 Jun 2016 19:27:04 GMT  (92kb,D)", "http://arxiv.org/abs/1606.02270v1", "8 pages plus references. Submitted to EMNLP 2016"], ["v2", "Fri, 10 Jun 2016 15:43:51 GMT  (92kb,D)", "http://arxiv.org/abs/1606.02270v2", "8 pages plus references. Submitted to EMNLP 2016"]], "COMMENTS": "8 pages plus references. Submitted to EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["adam trischler", "zheng ye", "xingdi yuan", "philip bachman", "alessandro sordoni", "kaheer suleman"], "accepted": true, "id": "1606.02270"}, "pdf": {"name": "1606.02270.pdf", "metadata": {"source": "CRF", "title": "Natural Language Comprehension with the EpiReader", "authors": ["Adam Trischler", "Zheng Ye jeff.ye", "Xingdi Yuan eric.yuan", "Kaheer Suleman"], "emails": ["k.suleman@maluuba.com"], "sections": [{"heading": "1 Introduction", "text": "When people think about the world, they tend to formulate a multitude of hypotheses and opposites, then in their turn they are seized by physical or mental experiments."}, {"heading": "2 Problem definition, notation, datasets", "text": "The task of the EpiReader is to answer a Cloze-style question by reading and understanding a supportive passage of text. Training and evaluation data consist of tuples (Q, T, a \u0445, A), where Q is the question (a sequence of words {q1,... q | Q |}), T is the text (a sequence of words {t1,..., t | T |}), A is a series of possible answers {a1,..., a | A |}, and A is the correct answer. All words come from a vocabulary V and A. In each question there is a placeholder token indicating the missing word to be filled in."}, {"heading": "2.1 Datasets", "text": "The articles themselves form the text passages, and the questions are synthetically generated from short summarizing statements that accompany each article. These summarizing points are (presumably) written by human authors; each question arises by replacing a named unit in a summarizing point with a placeholder; all the named units in the articles and questions are replaced by anonymized tokens that are shuffled for each (Q, T) pair, forcing the model to rely only on the text, rather than learning the world's knowledge of the units during the training; the CNN corpus (henceforth CNN) was presented by Hermann et al. (2015) Children's Book Test This corpus is structured similarly to CNN, but from children's books available through Project Gutenberg; instead of articles, the text passages are taken from book excerpts that consist of 20 sentences; and since no summaries are provided, a question is generated by generating a single word in the next (i.e. 21st) sentence."}, {"heading": "3 The EpiReader", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Overview and intuition", "text": "The EpiReader explicitly uses the observation that the answer to a question is often a word or phrase from the relevant passage of text, a mechanism described in detail in Section 3.2 and previously used by the Attention Sum Reader (Kadlec et al., 2016). Pointing to the candidates \"answers eliminates the need to apply a soft maximum across the entire vocabulary, as in Section 3.2 and 2014, which is more expensive in terms of computing and uses less direct information about the context of a predicted answer in the supporting text. EpiReader's second module, the Reasoner, begins by formulating hypotheses using the extracted answer candidates. It generates each hypothesis by replacing the placeholder with an answer candidate."}, {"heading": "3.2 The Extractor", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3.3 The Reasoner", "text": "RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR"}, {"heading": "3.4 Combining components", "text": "Finally, we combine the evidence from the bAbI dataset (Weston et al. 2015) with the probability from the extractor. < Q = weighted question. We calculate the production probability of each hypothesis, \u03c0k, according to the produkt\u03c0k-ekpk, (4) whereby the evidence of the reasonder can be interpreted as a correction to the extractor probabilities, applied as an additive shift in the logspace. We experimented with other combinations of the extractor and reasonder, but we found the multiplicative approach to achieve the best performance.After combining the results from the extractor and reasoner to obtain the probabilities described in Eq. 4, we optimize the parameters of the complete EpiReader to minimize a probable answer, LE and LR. The first term is a standard negative logging object that favors the correct answer over other hypothetical responses."}, {"heading": "4 Related Work", "text": "The impatient and attentive reader models were proposed by Hermann et al. (2015). Rather, the attentive reader applies bidirectional recurring encoders to the question and the supporting text. Subsequently, he uses the attention mechanism described in Bahdanau et al. (2014) to calculate a fixed-length representation of the text based on a weighted sum of the text encoder output, guided by comparing the question and the representation at each location in the text. Finally, a common representation of the question and the supporting text is formed by selecting their separate representations by a prepackaged MLP and an answer by comparing the MLP output with a representation of any possible answer. The impatient reader works similarly, but forces attention on the text after processing each successive word of the question. The two models achieved a similar performance on CNN and Daily Mail datasets.Jeory Networks were first proposed by Weston et al (2014) and later applied to the machine-based model of Hill."}, {"heading": "5 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Implementation and training details", "text": "To train our model, we used the stochastic gradient drop with the ADAM optimizer (Kingma and Ba, 2014), with an initial learning rate of 0.0001. The word embedding was randomly initialized by relying on the uniform distribution over [\u2212 0.05, 0.05). We used lots with 32 examples and stopped early with a patience of 2 epochs. Our model was implemented in Theano (Bergstra et al., 2010) using the Keras framework (Chollet, 2015). The results shown below for the epiReader were obtained by searching over a small grid of hyperparameter settings. We chose the model that achieved on each dataset the maximum accuracy of the validation set on the test set, and then evaluated it on the test set. The best settings for each dataset are shown in Table 1. As before, we train separate models on BT Cbenter Canntit\u00e4t = 0.00000Entity of all 00001 (all models used in common with NBT)."}, {"heading": "5.2 Results", "text": "In Table 2, we compare the performance of the EpiReader with that of several baselines, based on the validation and test rates of the CBT and CNN corporations. We measure the performance of the EpiReader by the performance of both the extractor and the reader. The EpiReader achieves state-of-the-art performance for both sets of data. On CNN, we achieve 2.2% higher scores than the best previous model by Chen et al. (2016). Interestingly, an analysis of the CNN data set by Chen et al. (2016) suggests that approximately 25% of the test examples contain co-ference errors or questions that are \"ambiguous / hard\" even for a human analyst. If this estimate is correct, then the EpiReader, which achieves an absolute test accuracy of 74.0%, works close to the expected human performance. On the other hand, it is unlikely that the ambiguity of the model is evenly distributed across the units, so that a good model, i.e., the reader should achieve a better performance, i.e., if the answer to the 1.1% question is the one where the reader is unsure."}, {"heading": "5.3 Analysis", "text": "Apart from the fact that it is about a state, in which it is about a state, in which people are able to survive themselves, and in which it is about a state, in which they see themselves in a position to survive themselves, and in which they feel themselves in a position to survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they feel themselves in a position to survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they survive themselves, in which they live themselves, in which they live themselves, in which they live in what they live, in what they, in what they, what they, in what they, what they, in what they, what they, in what they, what they, in which they, in which they, in which they, in which they, they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they,"}, {"heading": "6 Conclusion", "text": "In this article, we introduced the novel EpiReader framework for machine understanding and evaluated it on two large, complex datasets: CNN and CBT. Our model achieves state-of-the-art results for these corpora that exceed all previous approaches. In future work, we plan to expand our framework to include a more powerful model for natural language conclusions and to investigate the effects of pre-training such a model specifically on an inference task. We also plan to try to simplify the model by reusing the biGRU coding of the extractor in the reasoner."}], "references": [{"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "R\u00e9jean Ducharme", "author": ["Yoshua Bengio"], "venue": "and Pascal Vincent.", "citeRegEx": "Bengio et al.2000", "shortCiteRegEx": null, "year": 2000}, {"title": "and Y", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley"], "venue": "Bengio.", "citeRegEx": "Bergstra et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Christopher D", "author": ["Danqi Chen", "Jason Bolton"], "venue": "Manning.", "citeRegEx": "Chen et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "keras. https: //github.com/fchollet/keras", "author": ["Fran\u00e7ois Chollet"], "venue": null, "citeRegEx": "Chollet.,? \\Q2015\\E", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Oren Glickman", "author": ["Ido Dagan"], "venue": "and Bernardo Magnini.", "citeRegEx": "Dagan et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "John Prager", "author": ["David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A Kalyanpur", "Adam Lally", "J William Murdock", "Eric Nyberg"], "venue": "et al.", "citeRegEx": "Ferrucci et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Mustafa Suleyman", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay"], "venue": "and Phil Blunsom.", "citeRegEx": "Hermann et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sumit Chopra", "author": ["Felix Hill", "Antoine Bordes"], "venue": "and Jason Weston.", "citeRegEx": "Hill et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Ondrej Bajgar", "author": ["Rudolf Kadlec", "Martin Schmid"], "venue": "and Jan Kleindienst.", "citeRegEx": "Kadlec et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Edward Grefenstette", "author": ["Nal Kalchbrenner"], "venue": "and Phil Blunsom.", "citeRegEx": "Kalchbrenner et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Richard Socher", "author": ["Jeffrey Pennington"], "venue": "and Christopher D Manning.", "citeRegEx": "Pennington et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Christopher JC Burges", "author": ["Matthew Richardson"], "venue": "and Erin Renshaw.", "citeRegEx": "Richardson et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling relational information in question-answer pairs with convolutional neural networks. arXiv preprint arXiv:1604.01178", "author": ["Severyn", "Moschitti2016] Aliaksei Severyn", "Alessandro Moschitti"], "venue": null, "citeRegEx": "Severyn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2016}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["Wilson L Taylor"], "venue": "Journalism and Mass Communication Quarterly,", "citeRegEx": "Taylor.,? \\Q1953\\E", "shortCiteRegEx": "Taylor.", "year": 1953}, {"title": "Philip Bachman", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Jing He"], "venue": "and Kaheer Suleman.", "citeRegEx": "Trischler et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "2015", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly"], "venue": "Pointer networks. In Advances in Neural Information Processing Systems, pages 2674\u2013", "citeRegEx": "Vinyals et al.2015", "shortCiteRegEx": null, "year": 2682}, {"title": "Learning natural language inference with lstm", "author": ["Wang", "Jiang2015] Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1512.08849", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Sumit Chopra", "author": ["Jason Weston"], "venue": "and Antoine Bordes.", "citeRegEx": "Weston et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Sumit Chopra", "author": ["Jason Weston", "Antoine Bordes"], "venue": "and Tomas Mikolov.", "citeRegEx": "Weston et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model\u2019s response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children\u2019s Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin.", "creator": "LaTeX with hyperref package"}}}