{"id": "1606.02342", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Optimizing Spectral Learning for Parsing", "abstract": "We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods. Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods, parsing results significantly improve if the number of latent states for each nonterminal is globally optimized, while taking into account interactions between the different nonterminals. In addition, we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages.", "histories": [["v1", "Tue, 7 Jun 2016 21:58:41 GMT  (51kb)", "https://arxiv.org/abs/1606.02342v1", "11 pages, ACL 2016"], ["v2", "Thu, 9 Jun 2016 08:34:12 GMT  (51kb)", "http://arxiv.org/abs/1606.02342v2", "11 pages, ACL 2016"], ["v3", "Tue, 14 Jun 2016 13:10:41 GMT  (51kb)", "http://arxiv.org/abs/1606.02342v3", "11 pages, ACL 2016"]], "COMMENTS": "11 pages, ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shashi narayan", "shay b cohen"], "accepted": true, "id": "1606.02342"}, "pdf": {"name": "1606.02342.pdf", "metadata": {"source": "CRF", "title": "Optimizing Spectral Learning for Parsing", "authors": ["Shashi Narayan"], "emails": ["snaraya2@inf.ed.ac.uk", "scohen@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.02 342v 3 [cs.C L] 14 Jun 20"}, {"heading": "1 Introduction", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance,"}, {"heading": "2 Background and Notation", "text": "We have a series of holistic principles (1,., n). An L-PCFG is a 5-tuple (N, I, P, f, n), in which it is a question of whether it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, and in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way, and in which it is about a way in which it is about a way, and in which it is about a way in which it is about a way, and in which it is about a way in which it is a way and in which it is a way in which it is a way and in which it is a way in which it is a way and in which it is a way in which it is a way and in which it is a way in which it is a way and in which it is a way in which it is a way and in which it is a way in which it is a way and in which it is a way in which it is a way and in which it is a way it is a way and in which it is a way in which it is a way and in which it is a way in which it is a way it is a way and in which it is a way it is a way and in which it is a way in which it is a way and in which it is a way in which it is a way it is a way and in which it is a way in which it is a way and in which it is a way in which it is a way and in which it is a way it is a"}, {"heading": "3 Optimizing Spectral Estimation", "text": "In this section we describe our optimization algorithm and its motivation."}, {"heading": "3.1 Spectral Learning of L-PCFGs and Model Size", "text": "The family of spectral algorithms for latent variable PCFGs is based on trait functions defined for both inside and outside trees. In the case of a tree, the inner tree for a node contains the entire subtree below that node; the outer tree contains everything in the tree except the inner tree. Figure 1 shows an example of inside and outside trees for the nonterminal VP in the parsection tree of the sentence \"the mouse chased after the cat.\" With L-PCFGs, the model dictates that an inner tree and an outer tree connected to a node are statistically independent of each other, since the node designation and the latent state associated with it. As such, the distribution over the latent states for a given nonterminal value can be identified by using the cross-covariance matrix of the inner and outer tree connected to a node. For more information on the definition of this cross-variance matrix, see Cohen and Cohen (2015 and Cohen)."}, {"heading": "3.2 Optimizing the Number of Latent States", "text": "In practice, it is the case that most people are able to determine for themselves what they want and what they do not want. (...) In practice, it is the case that most people are able to decide what they want and what they do not want. (...) In practice, it is the case that most of them do not know what they want. (...) In practice, it is the case that they do not know what they want. (...) It is the case that they do not know what they want. (...) It is the case that they do not know what they want. (...) It is the case that they do not know what they want. (...) It is the case that they do not know what they want. (...) It is the case that they do not know what they want. (...) It is the case that they do not know what they do not want. (...) It is the case that they do not know what they do not want. (...)"}, {"heading": "4 Experiments", "text": "In this section we describe our setup for parsing experiments in different languages."}, {"heading": "4.1 Experimental Setup", "text": "In fact, most of them are able to determine for themselves what they want and what they don't want."}, {"heading": "4.2 Results", "text": "In fact, we have the results for other parsers, such as the parsers of Hall et al. (2014), Fernandez \"lez and Martins (2015) and Crabbe.\" In line with Bjo \ufffd rkelund et al. (2013), our preliminary experiments with the treatment of rare words suggest that morphological features are useful for all SPMRL languages, except the French. (11) See more at http: / cohort.ed.ac.uk / lpcfg, for Basques, Hungarians and Koreans, improvements are significantly greater."}, {"heading": "4.3 Further Analysis", "text": "In addition to the basic parsing results, we also wanted to check the size of the parsing models when comparing the results of the optimization with the algorithm compared to the vanilla models. Table 4 gives this analysis. In this table, we see that in most cases, on average, the optimization algorithm wants to increase the number of latent states. However, for Deutsch-T and Korean, for example, the optimization algorithm actually chooses a smaller model than the original vanilla model. We further investigated the behavior of the optimization algorithm for the preterminals in Deutsch-N, for which the optimal model (on average) chose a larger number of latent states. Table 5 describes this analysis. We see that in most cases the optimization algorithm wanted to reduce the number of latent states for the different preterminals, but in some cases the number of latent states increases significantly. 13Our experiments dispose another \"common\" wisdom about learning and training data."}, {"heading": "5 Conclusion", "text": "We have shown that careful selection of the number of latent states in a latent-variable PCFG with spectral estimation has a significant impact on the accuracy of L-PCFG analysis. We have described a search method to perform this type of optimization, and described the results of the analysis for eight languages (with nine datasets). Our results show that when comparing expectation maximization with coarse to fine techniques with our spectral algorithm with latent state optimization, spectral learning outperforms in six of the datasets."}, {"heading": "Acknowledgments", "text": "The authors thank David McClosky for his help in running the BLLIP parser and his comments on the work, as well as the three anonymous reviewers for their helpful comments. We also thank Eugene Charniak, DK Choe and Geoff Gordon for their useful discussions. Finally, we thank Djame'Seddah for providing the SPMRL datasets and Thomas Mueller and Anders Bjo \ufffd rkelund for providing the MarMot models. This research was supported by an EPSRC grant (EP / L02411X / 1) and an EU grant for H2020 (688139 / H2020-ICT-2015; SUMMA)."}], "references": [{"title": "A spectral approach for probabilistic grammatical inference on trees", "author": ["Rapha\u00ebl Bailly", "Amaury Habrard", "Fran\u00e7ois Denis."], "venue": "Proceedings of International Conference on Algorithmic Learning Theory.", "citeRegEx": "Bailly et al\\.,? 2010", "shortCiteRegEx": "Bailly et al\\.", "year": 2010}, {"title": "Re)ranking meets morphosyntax: State-of-the-art results from the SPMRL 2013 shared task", "author": ["Anders Bj\u00f6rkelund", "\u00d6zlem \u00c7etino\u011flu", "Rich\u00e1rd Farkas", "Thomas M\u00fceller", "Wolfgang Seeker."], "venue": "Proceedings of the Fourth Workshop on Statistical Pars-", "citeRegEx": "Bj\u00f6rkelund et al\\.,? 2013", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2013}, {"title": "Introducing the IMS-Wroc\u0142aw-Szeged-CIS entry at the SPMRL 2014 shared task: Reranking and morphosyntax", "author": ["Anders Bj\u00f6rkelund", "\u00d6zlem \u00c7etino\u011flu", "Agnieszka Fale\u0144ska", "Rich\u00e1rd Farkas", "Thomas M\u00fcller", "Wolfgang Seeker", "Zsolt Sz\u00e1nt\u00f3"], "venue": null, "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "A procedure for quantitatively comparing the syntactic coverage of English grammars", "author": ["torini", "Tomek Strzalkowski"], "venue": "In Proceedings of DARPA Workshop on Speech and Natural Language", "citeRegEx": "torini and Strzalkowski.,? \\Q1991\\E", "shortCiteRegEx": "torini and Strzalkowski.", "year": 1991}, {"title": "TIGER: Linguistic interpretation of a German corpus", "author": ["Sabine Brants", "Stefanie Dipper", "Peter Eisenberg", "Silvia Hansen-Schirra", "Esther K\u00f6nig", "Wolfgang Lezius", "Christian Rohrer", "George Smith", "Hans Uszkoreit."], "venue": "Research on Language and Com-", "citeRegEx": "Brants et al\\.,? 2004", "shortCiteRegEx": "Brants et al\\.", "year": 2004}, {"title": "Coarseto-fine n-best parsing and maxent discriminative reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of ACL.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "A provably correct learning algorithm for latent-variable PCFGs", "author": ["Shay B. Cohen", "Michael Collins."], "venue": "Proceedings of ACL.", "citeRegEx": "Cohen and Collins.,? 2014", "shortCiteRegEx": "Cohen and Collins.", "year": 2014}, {"title": "Spectral learning of latent-variable PCFGs", "author": ["Shay B. Cohen", "Karl Stratos", "Michael Collins", "Dean F. Foster", "Lyle Ungar."], "venue": "Proceedings of ACL.", "citeRegEx": "Cohen et al\\.,? 2012", "shortCiteRegEx": "Cohen et al\\.", "year": 2012}, {"title": "Experiments with spectral learning of latent-variable PCFGs", "author": ["Shay B. Cohen", "Karl Stratos", "Michael Collins", "Dean P. Foster", "Lyle Ungar."], "venue": "Proceedings of NAACL.", "citeRegEx": "Cohen et al\\.,? 2013", "shortCiteRegEx": "Cohen et al\\.", "year": 2013}, {"title": "Head-Driven Statistical Models for Natural Language Parsing", "author": ["Michael Collins."], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "Collins.,? 1999", "shortCiteRegEx": "Collins.", "year": 1999}, {"title": "Multilingual discriminative lexicalized phrase structure parsing", "author": ["Benoit Crabb\u00e9."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Crabb\u00e9.,? 2015", "shortCiteRegEx": "Crabb\u00e9.", "year": 2015}, {"title": "Parsing as reduction", "author": ["Daniel Fern\u00e1ndez-Gonz\u00e1lez", "Andr\u00e9 F.T. Martins."], "venue": "Proceedings of ACLIJCNLP.", "citeRegEx": "Fern\u00e1ndez.Gonz\u00e1lez and Martins.,? 2015", "shortCiteRegEx": "Fern\u00e1ndez.Gonz\u00e1lez and Martins.", "year": 2015}, {"title": "Less grammar, more features", "author": ["David Hall", "Greg Durrett", "Dan Klein."], "venue": "Proceedings of ACL.", "citeRegEx": "Hall et al\\.,? 2014", "shortCiteRegEx": "Hall et al\\.", "year": 2014}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["Daniel Hsu", "Sham M. Kakade", "Tong Zhang."], "venue": "Proceedings of COLT.", "citeRegEx": "Hsu et al\\.,? 2009", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "TurboParsers: Dependency parsing by approximate variational inference", "author": ["Andr\u00e9 F.T. Martins", "Noah A. Smith", "Eric P. Xing", "M\u00e1rio A.T. Figueiredo", "Pedro M.Q. Aguiar."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Martins et al\\.,? 2010", "shortCiteRegEx": "Martins et al\\.", "year": 2010}, {"title": "Probabilistic CFG with latent annotations", "author": ["Takuya Matsuzaki", "Yusuke Miyao", "Junichi Tsujii."], "venue": "Proceedings of ACL.", "citeRegEx": "Matsuzaki et al\\.,? 2005", "shortCiteRegEx": "Matsuzaki et al\\.", "year": 2005}, {"title": "Efficient higher-order CRFs for morphological tagging", "author": ["Thomas M\u00fceller", "Helmut Schmid", "Hinrich Sch\u00fctze."], "venue": "Proceedings of EMNLP.", "citeRegEx": "M\u00fceller et al\\.,? 2013", "shortCiteRegEx": "M\u00fceller et al\\.", "year": 2013}, {"title": "Diversity in spectral learning for natural language parsing", "author": ["Shashi Narayan", "Shay B. Cohen."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Narayan and Cohen.,? 2015", "shortCiteRegEx": "Narayan and Cohen.", "year": 2015}, {"title": "A spectral algorithm for latent junction trees", "author": ["Ankur P. Parikh", "Le Song", "Mariya Ishteva", "Gabi Teodoru", "Eric P. Xing."], "venue": "Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence.", "citeRegEx": "Parikh et al\\.,? 2012", "shortCiteRegEx": "Parikh et al\\.", "year": 2012}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein."], "venue": "Proceedings of COLING-ACL.", "citeRegEx": "Petrov et al\\.,? 2006", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Products of random latent variable grammars", "author": ["Slav Petrov."], "venue": "Proceedings of HLT-NAACL.", "citeRegEx": "Petrov.,? 2010", "shortCiteRegEx": "Petrov.", "year": 2010}, {"title": "Head-driven PCFGs with latent-head statistics", "author": ["Detlef Prescher."], "venue": "Proceedings of IWPT.", "citeRegEx": "Prescher.,? 2005", "shortCiteRegEx": "Prescher.", "year": 2005}, {"title": "Low-rank approximation of weighted tree automata", "author": ["Guillaume Rabusseau", "Borja Balle", "Shay B. Cohen."], "venue": "Proceedings of The 19th International Conference on Artificial Intelligence and Statistics.", "citeRegEx": "Rabusseau et al\\.,? 2016", "shortCiteRegEx": "Rabusseau et al\\.", "year": 2016}, {"title": "An annotation scheme for free word order languages", "author": ["Wojciech Skut", "Brigitte Krenn", "Thorsten Brants", "Hans Uszkoreit."], "venue": "Proceedings of ANLP.", "citeRegEx": "Skut et al\\.,? 1997", "shortCiteRegEx": "Skut et al\\.", "year": 1997}, {"title": "Special techniques for constituent parsing of morphologically rich languages", "author": ["Zsolt Sz\u00e1nt\u00f3", "Rich\u00e1rd Farkas."], "venue": "Proceedings of EACL.", "citeRegEx": "Sz\u00e1nt\u00f3 and Farkas.,? 2014", "shortCiteRegEx": "Sz\u00e1nt\u00f3 and Farkas.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "They were introduced in the NLP community by Matsuzaki et al. (2005) and Prescher (2005), with Matsuzaki et al.", "startOffset": 45, "endOffset": 69}, {"referenceID": 15, "context": "They were introduced in the NLP community by Matsuzaki et al. (2005) and Prescher (2005), with Matsuzaki et al.", "startOffset": 45, "endOffset": 89}, {"referenceID": 15, "context": "They were introduced in the NLP community by Matsuzaki et al. (2005) and Prescher (2005), with Matsuzaki et al. using the expectation-maximization (EM) algorithm to estimate them. Their performance on syntactic parsing of English at that stage lagged behind state-of-the-art parsers. Petrov et al. (2006) showed that one of the reasons that the EM algorithm does not estimate state-of-the-art parsing models for English is that the EM algorithm does not control well for the model size used in the parser \u2013 the number of latent states associated with the various nonterminals in the grammar.", "startOffset": 45, "endOffset": 305}, {"referenceID": 7, "context": "In more recent work, Cohen et al. (2012) described a different family of estimation algorithms for L-PCFGs.", "startOffset": 21, "endOffset": 41}, {"referenceID": 7, "context": "In a sense, the relationship between our work and the work of Cohen et al. (2013) is analogous to the relationship between the work by Petrov et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 7, "context": "In a sense, the relationship between our work and the work of Cohen et al. (2013) is analogous to the relationship between the work by Petrov et al. (2006) and the work by Matsuzaki et al.", "startOffset": 62, "endOffset": 156}, {"referenceID": 7, "context": "In a sense, the relationship between our work and the work of Cohen et al. (2013) is analogous to the relationship between the work by Petrov et al. (2006) and the work by Matsuzaki et al. (2005): we suggest a technique for optimizing the number of latent states for spectral algorithms, and test it on eight", "startOffset": 62, "endOffset": 196}, {"referenceID": 19, "context": "parsing models the spectral algorithms yield perform significantly better than the vanilla-estimated models, and for most of the languages \u2013 better than the Berkeley parser of Petrov et al. (2006).", "startOffset": 176, "endOffset": 197}, {"referenceID": 15, "context": "L-PCFGs, in their symbolic form, are related to regular tree grammars, an old grammar formalism, but they were introduced as statistical models for parsing with latent heads more recently by Matsuzaki et al. (2005) and Prescher (2005).", "startOffset": 191, "endOffset": 215}, {"referenceID": 15, "context": "L-PCFGs, in their symbolic form, are related to regular tree grammars, an old grammar formalism, but they were introduced as statistical models for parsing with latent heads more recently by Matsuzaki et al. (2005) and Prescher (2005). Earlier work about L-PCFGs by Matsuzaki et al.", "startOffset": 191, "endOffset": 235}, {"referenceID": 15, "context": "L-PCFGs, in their symbolic form, are related to regular tree grammars, an old grammar formalism, but they were introduced as statistical models for parsing with latent heads more recently by Matsuzaki et al. (2005) and Prescher (2005). Earlier work about L-PCFGs by Matsuzaki et al. (2005) used the expectation-maximization (EM) algorithm to estimate the grammar probabilities.", "startOffset": 191, "endOffset": 290}, {"referenceID": 15, "context": "L-PCFGs, in their symbolic form, are related to regular tree grammars, an old grammar formalism, but they were introduced as statistical models for parsing with latent heads more recently by Matsuzaki et al. (2005) and Prescher (2005). Earlier work about L-PCFGs by Matsuzaki et al. (2005) used the expectation-maximization (EM) algorithm to estimate the grammar probabilities. Indeed, given that the latent states are not observed, EM is a good fit for L-PCFG estimation, since it aims to do learning from incomplete data. This work has been further extended by Petrov et al. (2006) to use EM in a coarse-to-fine fashion: merging and splitting nonterminals using the latent states to optimize the number of latent states for each nonterminal.", "startOffset": 191, "endOffset": 584}, {"referenceID": 17, "context": "The family of L-PCFG spectral learning algorithms was further extended by Narayan and Cohen (2015). They presented a simplified version", "startOffset": 74, "endOffset": 99}, {"referenceID": 7, "context": "of the algorithm of Cohen et al. (2012) that estimates sparse grammars and assigns probabilities (instead of weights) to the rules in the grammar, and as such does not suffer from the problem of negative probabilities that arise with the", "startOffset": 20, "endOffset": 40}, {"referenceID": 7, "context": "original spectral algorithm (see discussion in Cohen et al., 2013). In this paper, we use the algorithms by Narayan and Cohen (2015) and Cohen", "startOffset": 47, "endOffset": 133}, {"referenceID": 22, "context": "See also (Rabusseau et al., 2016).", "startOffset": 9, "endOffset": 33}, {"referenceID": 0, "context": "A related algorithm for weighted tree automata (WTA) was developed by Bailly et al. (2010). However, the conversion from L-PCFGs to WTA is not straightforward, and information is lost in this conversion.", "startOffset": 70, "endOffset": 91}, {"referenceID": 19, "context": "(2012), and we compare them against stateof-the-art L-PCFG parsers such as the Berkeley parser (Petrov et al., 2006).", "startOffset": 95, "endOffset": 116}, {"referenceID": 12, "context": "We also compare our algorithms to other state-of-the-art parsers where elaborate linguistically-motivated feature specifications (Hall et al., 2014), annotations (Crabb\u00e9, 2015) and formalism conversions (Fern\u00e1ndezGonz\u00e1lez and Martins, 2015) are used.", "startOffset": 129, "endOffset": 148}, {"referenceID": 10, "context": ", 2014), annotations (Crabb\u00e9, 2015) and formalism conversions (Fern\u00e1ndezGonz\u00e1lez and Martins, 2015) are used.", "startOffset": 21, "endOffset": 35}, {"referenceID": 7, "context": "For more information on the definition of this crosscovariance matrix, see Cohen et al. (2012) and", "startOffset": 75, "endOffset": 95}, {"referenceID": 7, "context": "As a matter of fact, in addition to neglecting small singular values, the spectral methods of Cohen et al. (2013)", "startOffset": 94, "endOffset": 114}, {"referenceID": 17, "context": "and Narayan and Cohen (2015) also cap the number of latent states for each nonterminal to an up-", "startOffset": 4, "endOffset": 29}, {"referenceID": 15, "context": "(2006) improves over the estimation described in Matsuzaki et al. (2005) by taking into account the interactions between the nonter-", "startOffset": 49, "endOffset": 73}, {"referenceID": 7, "context": "gorithms of Cohen et al. (2013) and Narayan and Cohen (2015).", "startOffset": 12, "endOffset": 32}, {"referenceID": 7, "context": "gorithms of Cohen et al. (2013) and Narayan and Cohen (2015). These methods, in their default setting, use a function fS which maps each nonterminal a to a fixed number of latent states ma it uses.", "startOffset": 12, "endOffset": 61}, {"referenceID": 7, "context": "Cohen et al. (2013) estimate the parameters of the L-PCFG up to a linear transformation using f(a) non-zero singular values of \u03a9a, whereas Narayan and Cohen (2015) use the feature representations induced from the SVD step to cluster instances of nonterminal a in the training data into f(a) clusters; these clusters are then treated as latent states that are \u201cobserved.", "startOffset": 0, "endOffset": 20}, {"referenceID": 7, "context": "Cohen et al. (2013) estimate the parameters of the L-PCFG up to a linear transformation using f(a) non-zero singular values of \u03a9a, whereas Narayan and Cohen (2015) use the feature representations induced from the SVD step to cluster instances of nonterminal a in the training data into f(a) clusters; these clusters are then treated as latent states that are \u201cobserved.", "startOffset": 0, "endOffset": 164}, {"referenceID": 15, "context": "An important point to make is that the learning algorithms of Narayan and Cohen (2015) and Cohen et al.", "startOffset": 62, "endOffset": 87}, {"referenceID": 7, "context": "An important point to make is that the learning algorithms of Narayan and Cohen (2015) and Cohen et al. (2013) are relatively fast,2 in comparison to the EM algorithm.", "startOffset": 91, "endOffset": 111}, {"referenceID": 17, "context": "See, for example, Parikh et al. (2012). and Swedish) are taken from the workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL; Seddah et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 1, "context": "This is in line with Bj\u00f6rkelund et al. (2013).4 For Korean, there are", "startOffset": 21, "endOffset": 46}, {"referenceID": 1, "context": "Bj\u00f6rkelund et al. (2013) have shown that the morphological signatures for rare words are useful to improve the performance of the Berkeley parser.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "In their experiments Bj\u00f6rkelund et al. (2013) found that fine tags were not useful for Basque also; they did not find a proper explanation for that.", "startOffset": 21, "endOffset": 46}, {"referenceID": 1, "context": "We follow Bj\u00f6rkelund et al. (2013) and consider a word to be rare if it occurs less than 20 times in the training data.", "startOffset": 10, "endOffset": 35}, {"referenceID": 15, "context": "Spectral algorithms: subroutine choices The latent state optimization algorithm will work with either the clustering estimation algorithm of Narayan and Cohen (2015) or the spectral algorithm of Cohen et al.", "startOffset": 141, "endOffset": 166}, {"referenceID": 7, "context": "Spectral algorithms: subroutine choices The latent state optimization algorithm will work with either the clustering estimation algorithm of Narayan and Cohen (2015) or the spectral algorithm of Cohen et al. (2013). In our setup, we first run the latent state optimization algorithm with the clustering algorithm.", "startOffset": 195, "endOffset": 215}, {"referenceID": 17, "context": "We use the same features for the spectral methods as in Narayan and Cohen (2015) for GermanN.", "startOffset": 56, "endOffset": 81}, {"referenceID": 17, "context": "We use the kmeans function in Matlab to do the clustering for the spectral algorithm of Narayan and Cohen (2015). We experimented with several versions of k-means, and discovered that the version that works best in a set of preliminary experiments is hard k-means.", "startOffset": 88, "endOffset": 113}, {"referenceID": 14, "context": "We tag the German-N data using the Turbo Tagger (Martins et al., 2010).", "startOffset": 48, "endOffset": 70}, {"referenceID": 14, "context": "We tag the German-N data using the Turbo Tagger (Martins et al., 2010). For the languages in the SPMRL data we use the MarMot tagger of M\u00fceller et al. (2013) to jointly predict the POS and morphological tags.", "startOffset": 49, "endOffset": 158}, {"referenceID": 9, "context": "prm (Collins, 1999) for the German-N data and the SPMRL parameter file, spmrl.", "startOffset": 4, "endOffset": 19}, {"referenceID": 17, "context": "Following Narayan and Cohen (2015), we further improve our results by using multiple spec-", "startOffset": 10, "endOffset": 35}, {"referenceID": 1, "context": "See Bj\u00f6rkelund et al. (2013) for the performance of the MarMot tagger on the SPMRL datasets.", "startOffset": 4, "endOffset": 29}, {"referenceID": 19, "context": "\u201cBk\u201d makes use of the Berkeley parser with its coarse-to-fine mechanism to optimize the number of latent states (Petrov et al., 2006).", "startOffset": 112, "endOffset": 133}, {"referenceID": 20, "context": "\u201cBk multiple\u201d shows the best results with the multiple models using product-of-grammars procedure (Petrov, 2010) and discriminative reranking (Charniak and Johnson, 2005).", "startOffset": 98, "endOffset": 112}, {"referenceID": 5, "context": "\u201cBk multiple\u201d shows the best results with the multiple models using product-of-grammars procedure (Petrov, 2010) and discriminative reranking (Charniak and Johnson, 2005).", "startOffset": 142, "endOffset": 170}, {"referenceID": 17, "context": "\u201cCl multiple\u201d gives the results with multiple models generated using the noise induction and decoded using the hierarchical decoding (Narayan and Cohen, 2015).", "startOffset": 133, "endOffset": 158}, {"referenceID": 11, "context": "\u201cBk\u201d makes use of the Berkeley parser with its coarse-to-fine mechanism to optimize the number of latent states (Petrov et al., 2006). For Bk, \u201cvan\u201d uses the vanilla treatment of rare words using signatures defined by Petrov et al. (2006), whereas \u201crep.", "startOffset": 113, "endOffset": 239}, {"referenceID": 10, "context": "\u201cCl\u201d uses the algorithm of Narayan and Cohen (2015) and \u201cSp\u201d uses the algorithm of Cohen et al.", "startOffset": 27, "endOffset": 52}, {"referenceID": 4, "context": "\u201cCl\u201d uses the algorithm of Narayan and Cohen (2015) and \u201cSp\u201d uses the algorithm of Cohen et al. (2013). In Cl, \u201cvan (pos)\u201d and \u201cvan (rep)\u201d are vanilla estimations (i.", "startOffset": 83, "endOffset": 103}, {"referenceID": 1, "context": "For others, we report Bk results from Bj\u00f6rkelund et al. (2013). We also include results from Hall et al.", "startOffset": 38, "endOffset": 63}, {"referenceID": 1, "context": "For others, we report Bk results from Bj\u00f6rkelund et al. (2013). We also include results from Hall et al. (2014) and Crabb\u00e9 (2015).", "startOffset": 38, "endOffset": 112}, {"referenceID": 1, "context": "For others, we report Bk results from Bj\u00f6rkelund et al. (2013). We also include results from Hall et al. (2014) and Crabb\u00e9 (2015).", "startOffset": 38, "endOffset": 130}, {"referenceID": 17, "context": "For the German-N data, Bk results are taken from Petrov (2010). \u201cCl van\u201d shows the performance of the best vanilla models from Table 2 on the test set.", "startOffset": 49, "endOffset": 63}, {"referenceID": 10, "context": "We also include results from Hall et al. (2014), Crabb\u00e9 (2015) and Fern\u00e1ndez-Gonz\u00e1lez and Martins (2015).", "startOffset": 29, "endOffset": 48}, {"referenceID": 10, "context": "(2014), Crabb\u00e9 (2015) and Fern\u00e1ndez-Gonz\u00e1lez and Martins (2015).", "startOffset": 8, "endOffset": 22}, {"referenceID": 10, "context": "(2014), Crabb\u00e9 (2015) and Fern\u00e1ndez-Gonz\u00e1lez and Martins (2015).", "startOffset": 8, "endOffset": 64}, {"referenceID": 5, "context": "To decode with multiple noisy models, we train the MaxEnt reranker of Charniak and Johnson (2005).10 Hi-", "startOffset": 70, "endOffset": 98}, {"referenceID": 17, "context": "See Narayan and Cohen (2015) for more de-", "startOffset": 4, "endOffset": 29}, {"referenceID": 15, "context": "We only use the algorithm of Narayan and Cohen (2015) for the noisy model estimation.", "startOffset": 29, "endOffset": 54}, {"referenceID": 7, "context": "They have shown that decoding with noisy models performs better with their sparse estimates than the dense estimates of Cohen et al. (2013). Implementation: https://github.", "startOffset": 120, "endOffset": 140}, {"referenceID": 19, "context": "11 Our main focus is on comparing the coarse-to-fine Berkeley parser (Petrov et al., 2006)", "startOffset": 69, "endOffset": 90}, {"referenceID": 11, "context": "However, for the sake of completeness, we also present results for other parsers, such as parsers of Hall et al. (2014), Fern\u00e1ndezGonz\u00e1lez and Martins (2015) and Crabb\u00e9 (2015).", "startOffset": 101, "endOffset": 120}, {"referenceID": 11, "context": "However, for the sake of completeness, we also present results for other parsers, such as parsers of Hall et al. (2014), Fern\u00e1ndezGonz\u00e1lez and Martins (2015) and Crabb\u00e9 (2015).", "startOffset": 101, "endOffset": 158}, {"referenceID": 10, "context": "(2014), Fern\u00e1ndezGonz\u00e1lez and Martins (2015) and Crabb\u00e9 (2015).", "startOffset": 49, "endOffset": 63}, {"referenceID": 1, "context": "In line with Bj\u00f6rkelund et al. (2013), our preliminary experiments with the treatment of rare words suggest that morphological features are useful for all SPMRL languages except French.", "startOffset": 13, "endOffset": 38}, {"referenceID": 15, "context": "It is also interesting to compare the clustering algorithm of Narayan and Cohen (2015) to the spectral algorithm of Cohen et al.", "startOffset": 62, "endOffset": 87}, {"referenceID": 7, "context": "It is also interesting to compare the clustering algorithm of Narayan and Cohen (2015) to the spectral algorithm of Cohen et al. (2013). In the vanilla version, the spectral algorithm does better in most cases.", "startOffset": 116, "endOffset": 136}, {"referenceID": 1, "context": "They include transformations of the treebanks such as with unary rules (Bj\u00f6rkelund et al., 2013), a more careful handling of unknown words and better use of morphological informa-", "startOffset": 71, "endOffset": 96}, {"referenceID": 2, "context": "tion such as decorating preterminals with such information (Bj\u00f6rkelund et al., 2014; Sz\u00e1nt\u00f3 and Farkas, 2014), with careful feature specifications (Hall et al.", "startOffset": 59, "endOffset": 109}, {"referenceID": 24, "context": "tion such as decorating preterminals with such information (Bj\u00f6rkelund et al., 2014; Sz\u00e1nt\u00f3 and Farkas, 2014), with careful feature specifications (Hall et al.", "startOffset": 59, "endOffset": 109}, {"referenceID": 12, "context": ", 2014; Sz\u00e1nt\u00f3 and Farkas, 2014), with careful feature specifications (Hall et al., 2014) and head-annotations (Crabb\u00e9, 2015), and other techniques.", "startOffset": 70, "endOffset": 89}, {"referenceID": 10, "context": ", 2014) and head-annotations (Crabb\u00e9, 2015), and other techniques.", "startOffset": 29, "endOffset": 43}], "year": 2016, "abstractText": "We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods. Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods, parsing results significantly improve if the number of latent states for each nonterminal is globally optimized, while taking into account interactions between the different nonterminals. In addition, we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}