{"id": "1606.02421", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Gossip Dual Averaging for Decentralized Optimization of Pairwise Functions", "abstract": "In decentralized networks (of sensors, connected objects, etc.), there is an important need for efficient algorithms to optimize a global cost function, for instance to learn a global model from the local data collected by each computing unit. In this paper, we address the problem of decentralized minimization of pairwise functions of the data points, where these points are distributed over the nodes of a graph defining the communication topology of the network. This general problem finds applications in ranking, distance metric learning and graph inference, among others. We propose new gossip algorithms based on dual averaging which aims at solving such problems both in synchronous and asynchronous settings. The proposed framework is flexible enough to deal with constrained and regularized variants of the optimization problem. Our theoretical analysis reveals that the proposed algorithms preserve the convergence rate of centralized dual averaging up to an additive bias term. We present numerical simulations on Area Under the ROC Curve (AUC) maximization and metric learning problems which illustrate the practical interest of our approach.", "histories": [["v1", "Wed, 8 Jun 2016 07:01:47 GMT  (607kb,D)", "http://arxiv.org/abs/1606.02421v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.DC cs.LG cs.SY", "authors": ["igor colin", "aur\u00e9lien bellet", "joseph salmon", "st\u00e9phan cl\u00e9men\u00e7on"], "accepted": true, "id": "1606.02421"}, "pdf": {"name": "1606.02421.pdf", "metadata": {"source": "META", "title": "Gossip Dual Averaging for Decentralized Optimization of  Pairwise Functions", "authors": ["Igor Colin", "Aur\u00e9lien Bellet"], "emails": ["IGOR.COLIN@TELECOM-PARISTECH.FR", "AURELIEN.BELLET@INRIA.FR", "JOSEPH.SALMON@TELECOM-PARISTECH.FR", "STEPHAN.CLEMENCON@TELECOM-PARISTECH.FR"], "sections": [{"heading": "1. Introduction", "text": "The growing popularity of large-scale and fully decentralized computer architectures, fueled for example by the approach of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W & CP Volume 48. Copyright 2016 by the author (s).the advent of the \"Internet of Things,\" encourages the development of efficient optimization algorithms adapted to this setting. An important application is machine learning in wired and wireless networks of agents (sensors, networked objects, mobile phones, etc.), where agents seek to minimize a global learning goal that depends on the locally collected data of each agent. In such networks, it is typically impossible to efficiently centralize data or use globally aggregated interim results: agents can communicate only with their immediate neighbors (e.g. agents within a short distance), often in a completely asynchronous manner."}, {"heading": "2. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Definitions and Notation", "text": "For each integer p > 0 we denote by [p] the set {1,.., p} and by | F | the cardinality of each finite set F. We denote an undirected graph by G = (V, E), where V = [n] is the set of vertices and E V \u00b7 V is the set of edges. A node i-V has degrees di = | {j: (i, j), E} |. G is connected if for all (i, j), V 2 there is a path connecting i and j; it is divided in two if there is S, T-V, so that S-T = V, S-T = and E-T (S \u00b7 T), (T \u00b7 S). The graph laplan of G is denoted by L (G) = D (G) \u2212 A (G), where D (G) and A (G) each have degrees and adjacance matrices of G. The transpose of a matrix-n is denoted by the size (G) and by the size x-x-x-x-x-x."}, {"heading": "2.2. Problem Statement", "text": "We represent a network of n agents as an undirected graph G = (n], E), where each node i [n] corresponds to an agent and (i, j) \"E\" if nodes i and j can exchange information directly (i.e., they are neighbors).For the simplicity of exposure, we assume that each node i [n] has a single data point xi [n].Although this assumption can be mitigated easily in practice, it would lead to more technical details to handle the memory size without changing the general analysis (see supplementary material for details).Given d > 0, we leave f > X \u00b7 X \u2192 R a differentiated and convex function with respect to the first variable. We assume that for each (x) x \u00b2 there is Lf > 0 that f (x), x \u00b2) is."}, {"heading": "2.3. Centralized Dual Averaging", "text": "In this section, we review the stochastic dual optimization algorithm (Nesterov, 2009; Xiao, 2010) to solve the problem (2) in the centralized environment (where all data is on the same machine), which is indeed the root of our gossip algorithms, for reasons clarified in Section 3. To explain the main idea behind the dual averaging, let us first look at the iterations of the stochastic gradient (SGD), assuming that simplicity is a 0 (t) \u2212 g (t) \u2212 g), where E [t) is the iteration of the gradient (t) - n (t) - and (t) - a non-negative, non-increasing step sequence. For SGD, the convergence convergence is optimal, the step size sequence must be fulfilled."}, {"heading": "3. Pairwise Gossip Dual Averaging", "text": "We now turn to our main goal, which is to develop efficient gossip algorithms to solve problem (2) in a decentralized environment. The methods we propose are based on dual averaging (see Section 2.3). This choice is guided by the fact that the structure of the updates makes dual averaging in the distributed environment much easier to analyze than the parentage of subgradients when the problem is limited or regulated. This is because the dual averaging maintains a simple sum of subgradients, while the (nonlinear) smoothing operator is applied separately. Our work builds on the analysis by Duchi et al. (2012), which proposed a distributed dual averaging algorithm to optimize an average of universal functions f (\u00b7 xi). In its algorithm, each node computes i unbiased estimates of its local function f (\u00b7 xi), while those in our network are averaged iteratively."}, {"heading": "3.1. Synchronous Setting", "text": "In the synchronous setting, we assume that each node has access to a global clock, so that each node can be updated at each node at the same time. Although not very realistic, this setting allows for easier analysis. We assume that the scaling sequence (s) t is the same for each node. At any time, each node has the following quantities in its local memory register: a variable number of data (the gradient accumulator), its original observation xi, and an auxiliary observation yi, which is initialized at xi, but will change throughout the algorithm as a result of the data propagation."}, {"heading": "3.2. Asynchronous Setting", "text": "There is a need for a common time scale to perform the appropriate reduction. In this section, we place ourselves in a completely asynchronous environment where each node has a local clock that is ticking, independent of the others. This is equivalent to a global clock that is ticking. (See Boyd et al., 2006, for details on time modeling). With this in mind, algorithm 2 must be adapted to this setting."}, {"heading": "4. Numerical Simulations", "text": "In this section, we present numerical experiments on two popular machine learning problems with paired functions: Area Under the ROC Curve (AUC) maximization and metric learning. Our results show that our algorithms converge and that the bias term disappears very quickly with the number of iterations. To investigate the influence of network topology, we run our simulations on three types of networks (see Table 1 for the corresponding spectral gap values): \u2022 Full graph: All nodes are connected to each other. It is the ideal situation within our framework, as each pair of nodes can communicate directly. In this setting, the bias of the gradient estimates should be very small, as you have for all k values and all t values."}, {"heading": "5. Conclusion", "text": "In this paper, we have introduced new synchronous and asynchronous gossip algorithms to optimize functions based on pairs of data points distributed over a network; the proposed methods are based on dual averaging and can easily incorporate various popular regularization terms; we have presented an analysis that shows they behave similarly to the central dual average algorithm, with additional terms reflecting network connectivity and gradient bias; and finally, we have proposed some numerical experiments on AUC maximization and metric learning that illustrate the performance of the proposed algorithms and the impact of network topology; a challenging line of future research is the development and analysis of novel adaptive gossip schemes where the communication scheme is dynamic and depends on the characteristics of the network connection and the local information transported from each node."}, {"heading": "A. Outline of the Supplementary Material", "text": "The supplementary material is structured as follows: In Section B, we recall the standard proof of convergence rate for (centralized) dual averaging. Subsequently, in Section C, we improve the analysis of the decentralized version of the dual averaging algorithm for simple sums of functions and provide insights into the case of the sum of paired functions. Our asynchronous variant is examined in Section D. Technical details on how to extend our framework to the case with multiple points per node are discussed in Section E. Finally, additional numerical results are discussed in Section F."}, {"heading": "B. Centralized Dual Averaging", "text": "B.1. Deterministic SettingWe present the dual averaging algorithm for minimizing the sum f + 1 (in a context where f + 1 is convex and smooth), in a context where f + 1 is convex and smooth (in a context where f + 1 is convex and smooth), in a context where f + 1 is convex and smooth (in a context where f + 1 is convex, non-convex and possibly non-smooth, easy to calculate with a proximity operator). In the centralized context, this algorithm is as follows: (9) for each t + 1 in which a scale factor similar to a gradient step size is used in standard gradient derivation algorithms, and g (t) is a sequence of gradient lineage taken from f (t). In addition, we initialize the function f, which we explicitly consider here in the form f + n."}, {"heading": "C. Convergence Proof for Synchronous Pairwise Gossip Dual Averaging", "text": "In (Duchi et al., 2012) the following convergence rate for distributed dual averaging is set: Rn (2001) \u2212 Rn (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 2000) \u2212 Rp (2000) \u2212 Rp (2000) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 \u2212 Rp (2001) \u2212 \u2212 \u2212 \u2212 \u2212 Rp (2001) \u2212 \u2212 \u2212 \u2212 \u2212) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 (2001) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 2000) \u2212 Rp (2001) \u2212 Rp (2001) \u2212 2000) \u2212 2000) \u2212 Rp (2000) \u2212 2000) \u2212 2000 (Rp (2000) \u2212 2000)"}, {"heading": "D. Asynchronous Distributed Setting", "text": "In this section, we focus on a completely asynchronous setting in which each node has a local clock. We assume for simplicity that each node has a clock that ticks at a Poisson rate (see Boyd et al., 2006). Under this assumption, we can specify a method used in Algorithm 3.The main difficulty in the asynchronous setting is that each node must use a time estimate mi instead of the global clock reference (which is no longer available in such a context). Even if the time estimate is unbiased, its variance represents an additional error rate in the convergence rate. However, for an iteration rate T large enough, these estimates can be bellow.Lemma 5. There is T1 > 0 one that exists for any k rate."}, {"heading": "E. Extension to Multiple Points per Node", "text": "For a simple representation, we have assumed in the work that each node i has a single data point. (In this section, we discuss simple extensions of our results to the case where each node has the same number of points. (G) So it is easy to see that our results are still exchanged at each node, as in the algorithms proposed in the main text. (G) The idea is to view each \"physical\" node as a set of virtual \"nodes, each of which holds a single observation. (G) These nodes are all interconnected, as suggested in the algorithms proposed in the main text. (G) The idea is to view each\" physical \"node as a set of virtual\" nodes. (G) These nodes are all interconnected, as well as with the neighbors of i in the initial graphs G and their virtual nodes."}, {"heading": "F. Additional experiments", "text": "In this section, we will present additional results of decentralized metric learning. First, we will discuss the comparison to the unbiased baseline for metric learning on which in Section 4. Then we will analyze numerical experiments of decentralized metric learning on the breast cancer Wisconsin dataset3.3https: / / archive.ics.uci.edu / ml / datasets / Breast + Cancer + Wisconsin + (original) synthetic dataset In Section 4, we will discuss the results of decentralized metric learning on a synthetic dataset of n = 1,000 points generated from a mixture of 10 Gaussians in R40, so that all Gaussian averages are contained in a 5d subspace.We will compare the logistic loss associated with the iteration of our algorithm with the loss associated with the following baseline: Instead of the addition of f (t), xi, yi (t))))), to its dual metric function (random learning process [n] that is already in [n]."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the Chair \"Machine Learning for Big Data\" of Te'le \u0301 com ParisTech and by a scholarship from the CPER Nord-Pas de Calais / FEDER DATA Advanced data science and technologies 2015-2020."}], "references": [{"title": "Metric Learning", "author": ["Bellet", "Aur\u00e9lien", "Habrard", "Amaury", "Sebban", "Marc"], "venue": null, "citeRegEx": "Bellet et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bellet et al\\.", "year": 2015}, {"title": "Convergence of a Multi-Agent Projected Stochastic Gradient Algorithm for Non-Convex Optimization", "author": ["Bianchi", "Pascal", "Jakubowicz", "J\u00e9r\u00e9mie"], "venue": "IEEE Trans. Autom. Control,", "citeRegEx": "Bianchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bianchi et al\\.", "year": 2013}, {"title": "Statistical Inference on Graphs", "author": ["Biau", "G\u00e9rard", "Bleakley", "Kevin"], "venue": "Statistics & Decisions,", "citeRegEx": "Biau et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Biau et al\\.", "year": 2006}, {"title": "Modern Graph Theory, volume 184", "author": ["Bollob\u00e1s", "B\u00e9la"], "venue": null, "citeRegEx": "Bollob\u00e1s and B\u00e9la.,? \\Q1998\\E", "shortCiteRegEx": "Bollob\u00e1s and B\u00e9la.", "year": 1998}, {"title": "Randomized gossip algorithms", "author": ["Boyd", "Stephen", "Ghosh", "Arpita", "Prabhakar", "Balaji", "Shah", "Devavrat"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "Boyd et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2006}, {"title": "Spectral Graph Theory, volume 92", "author": ["Chung", "Fan"], "venue": "Amer. Math. Soc.,", "citeRegEx": "Chung and Fan.,? \\Q1997\\E", "shortCiteRegEx": "Chung and Fan.", "year": 1997}, {"title": "Ranking and Empirical Minimization of U-statistics", "author": ["Cl\u00e9men\u00e7on", "St\u00e9phan", "Lugosi", "G\u00e0bor", "Vayatis", "Nicolas"], "venue": "Ann. Stat.,", "citeRegEx": "Cl\u00e9men\u00e7on et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cl\u00e9men\u00e7on et al\\.", "year": 2008}, {"title": "Extending Gossip Algorithms to Distributed Estimation of U-Statistics", "author": ["I. Colin", "A. Bellet", "J. Salmon", "S. Cl\u00e9men\u00e7on"], "venue": "In NIPS,", "citeRegEx": "Colin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Colin et al\\.", "year": 2015}, {"title": "Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling", "author": ["Duchi", "John", "Agarwal", "Alekh", "Wainwright", "Martin"], "venue": "IEEE Trans. Autom. Control,", "citeRegEx": "Duchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2012}, {"title": "Distances of probability measures and random variables", "author": ["Dudley", "Richard M"], "venue": "Selected Works of RM Dudley, pp", "citeRegEx": "Dudley and M.,? \\Q2010\\E", "shortCiteRegEx": "Dudley and M.", "year": 2010}, {"title": "Algebra connectivity of graphs", "author": ["Fiedler", "Miroslav"], "venue": "Czechoslovake Mathematical Journal,", "citeRegEx": "Fiedler and Miroslav.,? \\Q1973\\E", "shortCiteRegEx": "Fiedler and Miroslav.", "year": 1973}, {"title": "Asynchronous Distributed Optimization using a Randomized Alternating Direction Method of Multipliers", "author": ["Iutzeler", "Franck", "Bianchi", "Pascal", "Ciblat", "Philippe", "Hachem", "Walid"], "venue": "In IEEE CDC,", "citeRegEx": "Iutzeler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Iutzeler et al\\.", "year": 2013}, {"title": "Regularized Distance Metric Learning: Theory and Algorithm", "author": ["R. Jin", "S. Wang", "Y. Zhou"], "venue": "In NIPS, pp", "citeRegEx": "Jin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2009}, {"title": "A Randomized Incremental Subgradient Method for Distributed Optimization in Networked Systems", "author": ["Johansson", "Bj\u00f6rn", "Rabi", "Maben", "Mikael"], "venue": "SIAM J. Optimiz.,", "citeRegEx": "Johansson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Johansson et al\\.", "year": 2010}, {"title": "Gossip-Based Computation of Aggregate Information", "author": ["Kempe", "David", "Dobra", "Alin", "Gehrke", "Johannes"], "venue": "In FOCS, pp", "citeRegEx": "Kempe et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kempe et al\\.", "year": 2003}, {"title": "A Binary Classification Framework for Two-Stage Multiple Kernel Learning", "author": ["Kumar", "Abhishek", "Niculescu-Mizil", "Alexandru", "K. Kavukcuoglu", "Daum\u00e9", "Hal"], "venue": "In ICML,", "citeRegEx": "Kumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2012}, {"title": "Decentralized online optimization with global objectives and local communication", "author": ["Lee", "Soomin", "Nedi\u0107", "Angelia", "Raginsky", "Maxim"], "venue": "arXiv preprint arXiv:1508.07933,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Asynchronous broadcast-based convex optimization over a network", "author": ["Nedi\u0107", "Angelia"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Nedi\u0107 and Angelia.,? \\Q2011\\E", "shortCiteRegEx": "Nedi\u0107 and Angelia.", "year": 2011}, {"title": "Distributed Subgradient Methods for Multi-Agent Optimization", "author": ["Nedi\u0107", "Angelia", "Ozdaglar", "Asuman E"], "venue": "IEEE Trans. Autom. Control,", "citeRegEx": "Nedi\u0107 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nedi\u0107 et al\\.", "year": 2009}, {"title": "Primal-dual subgradient methods for convex problems", "author": ["Nesterov", "Yurii"], "venue": "Math. Program.,", "citeRegEx": "Nesterov and Yurii.,? \\Q2009\\E", "shortCiteRegEx": "Nesterov and Yurii.", "year": 2009}, {"title": "Gossip Algorithms for Computing U-Statistics", "author": ["Pelckmans", "Kristiaan", "Suykens", "Johan"], "venue": "In NecSys, pp", "citeRegEx": "Pelckmans et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pelckmans et al\\.", "year": 2009}, {"title": "Distributed Stochastic Subgradient Projection Algorithms for Convex Optimization", "author": ["S. Ram", "Nedi\u0107", "Angelia", "V. Veeravalli"], "venue": "J. Optimiz. Theory. App.,", "citeRegEx": "Ram et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ram et al\\.", "year": 2010}, {"title": "Push-Sum Distributed Dual Averaging for convex optimization", "author": ["Tsianos", "Konstantinos", "Lawlor", "Sean", "Rabbat", "Michael"], "venue": "In IEEE CDC,", "citeRegEx": "Tsianos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsianos et al\\.", "year": 2015}, {"title": "Problems in decentralized decision making and computation", "author": ["Tsitsiklis", "John"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Tsitsiklis and John.,? \\Q1984\\E", "shortCiteRegEx": "Tsitsiklis and John.", "year": 1984}, {"title": "Collective dynamics of \u2018small-world\u2019networks", "author": ["Watts", "Duncan J", "Strogatz", "Steven H"], "venue": "Nature, 393(6684):440\u2013442,", "citeRegEx": "Watts et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Watts et al\\.", "year": 1998}, {"title": "Distributed Alternating Direction Method of Multipliers", "author": ["Wei", "Ermin", "Ozdaglar", "Asuman"], "venue": "In IEEE CDC,", "citeRegEx": "Wei et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2012}, {"title": "On the O(1/k) Convergence of Asynchronous Distributed Alternating Direction Method of Multipliers", "author": ["Wei", "Ermin", "Ozdaglar", "Asuman"], "venue": "In IEEE GlobalSIP,", "citeRegEx": "Wei et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2013}, {"title": "Dual averaging method for regularized stochastic learning and online optimization", "author": ["Xiao", "Lin"], "venue": "In NIPS, pp", "citeRegEx": "Xiao and Lin.,? \\Q2009\\E", "shortCiteRegEx": "Xiao and Lin.", "year": 2009}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["Xiao", "Lin"], "venue": "JMLR, 11:2543\u20132596,", "citeRegEx": "Xiao and Lin.,? \\Q2010\\E", "shortCiteRegEx": "Xiao and Lin.", "year": 2010}, {"title": "Distributed dual averaging method for multi-agent optimization with quantized communication", "author": ["Yuan", "Deming", "Xu", "Shengyuan", "Zhao", "Huanyu", "Rong", "Lina"], "venue": "Systems & Control Letters,", "citeRegEx": "Yuan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2012}, {"title": "Online AUC Maximization", "author": ["Zhao", "Peilin", "Hoi", "Steven", "Jin", "Rong", "Yang", "Tianbao"], "venue": "In ICML,", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 4, "context": "In contrast, gossip algorithms (Tsitsiklis, 1984; Boyd et al., 2006; Kempe et al., 2003; Shah, 2009) are tailored to this setting because they only rely on simple peer-to-peer communication: each agent only exchanges information with one neighbor at a time.", "startOffset": 31, "endOffset": 100}, {"referenceID": 14, "context": "In contrast, gossip algorithms (Tsitsiklis, 1984; Boyd et al., 2006; Kempe et al., 2003; Shah, 2009) are tailored to this setting because they only rely on simple peer-to-peer communication: each agent only exchanges information with one neighbor at a time.", "startOffset": 31, "endOffset": 100}, {"referenceID": 13, "context": "The most popular algorithms are based on (sub)gradient descent (Johansson et al., 2010; Nedi\u0107 & Ozdaglar, 2009; Ram et al., 2010; Bianchi & Jakubowicz, 2013), ADMM (Wei & Ozdaglar, 2012; 2013; Iutzeler et al.", "startOffset": 63, "endOffset": 157}, {"referenceID": 21, "context": "The most popular algorithms are based on (sub)gradient descent (Johansson et al., 2010; Nedi\u0107 & Ozdaglar, 2009; Ram et al., 2010; Bianchi & Jakubowicz, 2013), ADMM (Wei & Ozdaglar, 2012; 2013; Iutzeler et al.", "startOffset": 63, "endOffset": 157}, {"referenceID": 11, "context": ", 2010; Bianchi & Jakubowicz, 2013), ADMM (Wei & Ozdaglar, 2012; 2013; Iutzeler et al., 2013) or dual averaging (Duchi et al.", "startOffset": 42, "endOffset": 93}, {"referenceID": 8, "context": ", 2013) or dual averaging (Duchi et al., 2012; Yuan et al., 2012; Lee et al., 2015; Tsianos et al., 2015), some of which can also accommodate constraints or regularization on \u03b8.", "startOffset": 26, "endOffset": 105}, {"referenceID": 29, "context": ", 2013) or dual averaging (Duchi et al., 2012; Yuan et al., 2012; Lee et al., 2015; Tsianos et al., 2015), some of which can also accommodate constraints or regularization on \u03b8.", "startOffset": 26, "endOffset": 105}, {"referenceID": 16, "context": ", 2013) or dual averaging (Duchi et al., 2012; Yuan et al., 2012; Lee et al., 2015; Tsianos et al., 2015), some of which can also accommodate constraints or regularization on \u03b8.", "startOffset": 26, "endOffset": 105}, {"referenceID": 22, "context": ", 2013) or dual averaging (Duchi et al., 2012; Yuan et al., 2012; Lee et al., 2015; Tsianos et al., 2015), some of which can also accommodate constraints or regularization on \u03b8.", "startOffset": 26, "endOffset": 105}, {"referenceID": 30, "context": ", Area Under the ROC Curve (AUC) maximization (Zhao et al., 2011), distance/similarity learning (Bellet et al.", "startOffset": 46, "endOffset": 65}, {"referenceID": 0, "context": ", 2011), distance/similarity learning (Bellet et al., 2015), ranking (Cl\u00e9men\u00e7on et al.", "startOffset": 38, "endOffset": 59}, {"referenceID": 6, "context": ", 2015), ranking (Cl\u00e9men\u00e7on et al., 2008), supervised graph inference (Biau & Bleakley, 2006) and multiple kernel learning (Kumar et al.", "startOffset": 17, "endOffset": 41}, {"referenceID": 15, "context": ", 2008), supervised graph inference (Biau & Bleakley, 2006) and multiple kernel learning (Kumar et al., 2012), to name a few.", "startOffset": 89, "endOffset": 109}, {"referenceID": 7, "context": "Although gossip algorithms have recently been introduced to evaluate such pairwise functions for a fixed \u03b8 (Pelckmans & Suykens, 2009; Colin et al., 2015), to the best of our knowledge, efficiently finding the optimal solution \u03b8 in a decentralized way remains an open challenge.", "startOffset": 107, "endOffset": 154}, {"referenceID": 30, "context": "For instance, in AUC maximization (Zhao et al., 2011), binary labels (`1, .", "startOffset": 34, "endOffset": 53}, {"referenceID": 0, "context": "Other popular instances of Problem (2) include metric learning (Bellet et al., 2015), ranking (Cl\u00e9men\u00e7on et al.", "startOffset": 63, "endOffset": 84}, {"referenceID": 6, "context": ", 2015), ranking (Cl\u00e9men\u00e7on et al., 2008), supervised graph inference (Biau & Bleakley, 2006) and multiple kernel learning (Kumar et al.", "startOffset": 17, "endOffset": 41}, {"referenceID": 15, "context": ", 2008), supervised graph inference (Biau & Bleakley, 2006) and multiple kernel learning (Kumar et al., 2012).", "startOffset": 89, "endOffset": 109}, {"referenceID": 7, "context": "To go around this problem, we rely on a gossip data propagation step (Pelckmans & Suykens, 2009; Colin et al., 2015) so that the nodes are able to compute biased estimates of \u2207fi(\u00b7) while keeping the communication and memory overhead to a small level for each node.", "startOffset": 69, "endOffset": 116}, {"referenceID": 7, "context": "Our work builds upon the analysis of Duchi et al. (2012), who proposed a distributed dual averaging algorithm to optimize an average of univariate functions f(\u00b7;xi).", "startOffset": 37, "endOffset": 57}, {"referenceID": 8, "context": "Note that C2(T ) corresponds to the network dependence for the distributed dual averaging algorithm of Duchi et al. (2012) while the term C3(T ) comes from the bias of our partial gradient estimates.", "startOffset": 103, "endOffset": 123}, {"referenceID": 8, "context": "In the asynchronous setting, no convergence rate was known even for the distributed dual averaging algorithm of Duchi et al. (2012), which deals with the simpler problem of minimizing univariate functions.", "startOffset": 112, "endOffset": 132}, {"referenceID": 12, "context": "following criterion (Jin et al., 2009):", "startOffset": 20, "endOffset": 38}, {"referenceID": 8, "context": "Convergence Proof for Synchronous Pairwise Gossip Dual Averaging In (Duchi et al., 2012), the following convergence rate for distributed dual averaging is established:", "startOffset": 68, "endOffset": 88}, {"referenceID": 8, "context": "As denoted in (Duchi et al., 2012), the update rule for Z can be expressed as follows:", "startOffset": 14, "endOffset": 34}, {"referenceID": 4, "context": "We assume for simplicity that each node has a clock ticking at a Poisson rate equals to 1, so it is equivalent to a global clock ticking at a Poisson rate of n, and then drawing an edge uniformly at random (see (Boyd et al., 2006) for more details).", "startOffset": 211, "endOffset": 230}], "year": 2016, "abstractText": "In decentralized networks (of sensors, connected objects, etc.), there is an important need for efficient algorithms to optimize a global cost function, for instance to learn a global model from the local data collected by each computing unit. In this paper, we address the problem of decentralized minimization of pairwise functions of the data points, where these points are distributed over the nodes of a graph defining the communication topology of the network. This general problem finds applications in ranking, distance metric learning and graph inference, among others. We propose new gossip algorithms based on dual averaging which aims at solving such problems both in synchronous and asynchronous settings. The proposed framework is flexible enough to deal with constrained and regularized variants of the optimization problem. Our theoretical analysis reveals that the proposed algorithms preserve the convergence rate of centralized dual averaging up to an additive bias term. We present numerical simulations on Area Under the ROC Curve (AUC) maximization and metric learning problems which illustrate the practical interest of our approach.", "creator": "LaTeX with hyperref package"}}}