{"id": "1606.02461", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Learning Semantically and Additively Compositional Distributional Representations", "abstract": "This paper connects a vector-based composition model to a formal semantics, the Dependency-based Compositional Semantics (DCS). We show theoretical evidence that the vector compositions in our model conform to the logic of DCS. Experimentally, we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors, achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification; meanwhile, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence completion task and report a new state-of-the-art.", "histories": [["v1", "Wed, 8 Jun 2016 09:12:17 GMT  (163kb,D)", "http://arxiv.org/abs/1606.02461v1", "to appear in ACL2016"]], "COMMENTS": "to appear in ACL2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ran tian", "naoaki okazaki", "kentaro inui"], "accepted": true, "id": "1606.02461"}, "pdf": {"name": "1606.02461.pdf", "metadata": {"source": "CRF", "title": "Learning Semantically and Additively Compositional Distributional Representations", "authors": ["Ran Tian"], "emails": ["inui}@ecei.tohoku.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "An essential goal of semantic processing is to map natural language expressions into representations that facilitate the calculation of meanings, the execution of commands and / or the conclusion of knowledge. Formal semantics supports such representations by defining words as some functional units and combining them by a specific logic. A simple and illustrative example is the definition of the term \"compositional semantics\" (DCS), in which the terminology of things to which the words apply applies applies; let's say, the denotations of the concept \"drug and the event ban\" are shown in Figure 1b, where drugs are a list of drug names and prohibitions, which are a list of subject complement pairs in each prohibition event; then a list of prohibited drugs can be constructed by first taking the COMP column of all data sets in prohibition (projection), and then intersectioning."}, {"heading": "2 DCS Trees", "text": "A \"thing\" (i.e., an element of a designation) is represented by a tuple of characteristics of the form Field = Value, each tuple recording participants in a prohibited event (e.g. Canada, which prohibits thalidomide); operations are applied to sets of things to generate new denotations, modelling the semantic composition; an example is the intersection of pets and fish, which gives the denotation of \"pet fish.\" Another necessary operation is projection; by \"N\" we mean a function that assigns a tuple to its value of the field N. For example, \"DCOMP\" (Prohibition) is the value of the COMP fields consisting of forbidden objects (i.e. \"D.\" G \"), where we set a tuple to its value of the field x."}, {"heading": "D2 := drug \u2229 \u03c0\u22121ARG(\u03c0COMP(ban)),", "text": "Hence the following designation:"}, {"heading": "D3 := sell \u2229 D1 \u2229 \u03c0\u22121COMP(\u03c0ARG(D2))", "text": "The DCS tree for the sentence \"a man sells prohibited drugs\" is represented in Figure 2. Formally, a DCS tree is defined as a rooted tree in which nodes are names for substantive words and edges marked by fields at each end. Suppose a node x has children y1,., yn, and the edges (x, y1),., (x, yn) are denominated by names such as (P1, L1),., (Pn, Ln), respectively, the names x, and the edges (x, y1),., (x, yn) are denominations for trees, the names of (P1, L1),. (Pn, Ln), respectively. Then, the naming [x]] of the subtree rooted in x is recursively calculated as [x]."}, {"heading": "3 Vector-based DCS", "text": "For each content word w, we use a query vector vw to model its designation, and a response vector uw to model a prototypical element in that designation. Query vector v and response vector u are learned in such a way that exp (v \u00b7 u) is proportional to the probability of answering the query v. The learning source is a collection of DCS trees based on the idea that the DCS tree of a declarative sentence usually has a non-empty denotation. For example, \"Playing children\" means that there is a child answering the query. Consequently, any element in game denotation belongs to \u03c0 \u2212 1SUBJ (Child). This signal is a signal to increase the dot product of uplay and the query vector vector vector vector vector vector-1SUBJ (Child)."}, {"heading": "4 Training", "text": "In fact, it is as if it were a matter of a way in which it is a matter of a way in which people are able to understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand."}, {"heading": "5 Experiments", "text": "\"We must abide by the rules we have made our own,\" he says. \"We must abide by the rules.\" \"We must abide by the rules.\" \"We must abide by the rules.\" \"We must abide by the rules.\" \"We must abide by the rules.\" \"We must abide by the rules.\" \"We must abide by the rules.\" \"We must abide by the rules.\" \"We must abide by the rules.\" \"We must abide by the rules.\" \"We must abide by the rules.\" \"We must abide by the rules.\" \"\" We must abide by the rules. \"\" \"We must abide by the rules.\" \"\" \"We must abide by the rules.\""}, {"heading": "5.1 Qualitative Analysis", "text": "We observe several specific properties of the vectors and matrices trained by our model. Words are bundled by POS. Unlike the vectors trained by GloVe or \"no matrix\" (Table 1), word vectors are bundled by POS tags. Matrices show semantic regularity Matrices learned for ARG, SUBJ and COMP are exactly orthogonal, and some of the most common prepositions 6 are remarkably close. For these matrices, the corresponding M \u2212 1 coincide exactly with their inversion. It points to regularities in semantic space, especially because orthogonal matrices retain cosmic similarity - if MN is orthogonal, two words x, y and their predictions, Decn (y) have the same similarity."}, {"heading": "5.2 Phrase Similarity", "text": "To test whether vecDCS has the ability to calculate similar things to similar vectors, we perform an evaluation on a wide range of phrase similarity tasks. (In these tasks, a system calculates similarity values for two types of phrases, and the performance is evaluated as their correlation with human annotations, measured by Spearman's analog annotations. (eg, \"tax burden\" and \"interest rate\") and verb objects (eg, \"battle for two types of words\" and \"dark eye\"), compound nouns (eg, \"tax burden\" and \"interest rate\") and verb objects (VO). Each dataset consists of 108 pairs and each pair is made up of 18 people (i.e., 1,944 scores in total)."}, {"heading": "5.3 Relation Classification", "text": "In the relationship between the two forests, we must seek a better solution than a solution."}, {"heading": "5.4 Sentence Completion", "text": "In this context, it should be noted that the solution of the problem is not the solution of a problem, but the solution of a problem that is a problem that cannot be solved. (...) It is noteworthy, however, that despite a common word shared by the formulations, their response lists are largely different, indicating that the composition can indeed be done. (...) Moreover, some words are actually able to answer queries9https. (...) It is noteworthy, however, that despite a common word shared by the formulations, their responses are largely different. (...)"}, {"heading": "6 Discussion", "text": "We have shown a way to combine a vector composition with formal semantics that combines the strength of vector representations to calculate phrase similarities, and the strength of formal semantics to build structured queries. In this section, we will discuss several lines of previous research that relate to this work.10http: / / research.microsoft.com / en-us / projects / scc / Logic and Distributional Semantics Logic is necessary to implement the functional aspects of meaning and organization in a structured and unambiguous way.10In contrast, distributional semantics provides an elegant methodology for evaluating semantic similarity and is well suited to learning from data. Repeated calls for combining the strength of these two approaches (Coecke et al., 2010; Baroni et al., 2014; Liang and Potts, 2015) and several systems (Lewis and Steedman, 2014; Tial) have transformed this remarkable direction."}], "references": [{"title": "Broad-coverage ccg semantic parsing with amr", "author": ["Artzi et al.2015] Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Artzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Coupling ccg and hybrid logic dependency semantics", "author": ["Baldridge", "Kruijff2002] Jason Baldridge", "Geert-Jan Kruijff"], "venue": "In Proceedings of ACL", "citeRegEx": "Baldridge et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Baldridge et al\\.", "year": 2002}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Baroni", "Lenci2010] Marco Baroni", "Alessandro Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Baroni", "Zamparelli2010] Marco Baroni", "Roberto Zamparelli"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Frege in space: A program for compositional distributional semantics", "author": ["Baroni et al.2014] Marco Baroni", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "Linguistic Issues in Language Technology,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Probabilistic soft logic for semantic textual similarity", "author": ["Katrin Erk", "Raymond Mooney"], "venue": "In Proceedings of ACL", "citeRegEx": "Beltagy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Beltagy et al\\.", "year": 2014}, {"title": "Semantic parsing via paraphrasing", "author": ["Berant", "Liang2014] Jonathan Berant", "Percy Liang"], "venue": "In Proceedings of ACL", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["Blacoe", "Lapata2012] William Blacoe", "Mirella Lapata"], "venue": "In Proceedings of EMNLP-CoNLL", "citeRegEx": "Blacoe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe et al\\.", "year": 2012}, {"title": "Wide-coverage semantic representations from a ccg parser", "author": ["Bos et al.2004] Johan Bos", "Stephen Clark", "Mark Steedman", "James R. Curran", "Julia Hockenmaier"], "venue": "In Proceedings of ICCL", "citeRegEx": "Bos et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bos et al\\.", "year": 2004}, {"title": "Stochastic gradient descent tricks", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q2012\\E", "shortCiteRegEx": "Bottou.", "year": 2012}, {"title": "On the decidability of query containment under constraints", "author": ["Giuseppe De Giacomo", "Maurizio Lenzerini"], "venue": "In Proceedings of the 17th ACM SIGACT SIGMOD SIGART Symposium on Principles of Database Sys-", "citeRegEx": "Calvanese et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Calvanese et al\\.", "year": 1998}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Coecke et al.2010] Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark"], "venue": null, "citeRegEx": "Coecke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "An algebra for semantic construction in constraint-based grammars", "author": ["Alex Lascarides", "Dan Flickinger"], "venue": "In Proceedings of ACL", "citeRegEx": "Copestake et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Copestake et al\\.", "year": 2001}, {"title": "Minimal recursion semantics: An introduction", "author": ["Dan Flickinger", "Carl Pollard", "Ivan A. Sag"], "venue": "Research on Language and Computation,", "citeRegEx": "Copestake et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of ACL-IJCNLP", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Introduction to Montague Semantics", "author": ["Dowty et al.1981] David R. Dowty", "Robert E. Wall", "Stanley Peters"], "venue": null, "citeRegEx": "Dowty et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Dowty et al\\.", "year": 1981}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Grefenstette et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2011}, {"title": "Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics", "author": ["Edward Grefenstette"], "venue": "PhD thesis", "citeRegEx": "Grefenstette.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette.", "year": 2013}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Edward Grefenstette"], "venue": "In Proceedings of *SEM", "citeRegEx": "Grefenstette.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette.", "year": 2013}, {"title": "Dependency language models for sentence completion", "author": ["Gubbins", "Vlachos2013] Joseph Gubbins", "Andreas Vlachos"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Gubbins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gubbins et al\\.", "year": 2013}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Gutmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2012}, {"title": "Traversing knowledge graphs in vector space", "author": ["Guu et al.2015] Kelvin Guu", "John Miller", "Percy Liang"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Guu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Jointly learning word representations and composition functions using predicate-argument structures", "author": ["Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Hashimoto et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2014}, {"title": "Semeval-2010 task 8: Multi-way classification", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "From Discourse to Logic", "author": ["Kamp", "Reyle1993] Hans Kamp", "Uwe Reyle"], "venue": null, "citeRegEx": "Kamp et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Kamp et al\\.", "year": 1993}, {"title": "A study of entanglement in a categorical framework of natural language", "author": ["Kartsaklis", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL)", "citeRegEx": "Kartsaklis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2014}, {"title": "Fast exact inference with a factored model for natural language parsing", "author": ["Klein", "Manning2003] Dan Klein", "Christopher D. Manning"], "venue": "In Advances in NIPS", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Composing and updating verb argument expectations: A distributional semantic model", "author": ["Alessandro Lenci"], "venue": "In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics", "citeRegEx": "Lenci.,? \\Q2011\\E", "shortCiteRegEx": "Lenci.", "year": 2011}, {"title": "Dependency-based word embeddings", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of ACL", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Levy et al.2015] Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Transactions of ACL,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Combined distributional and logical semantics", "author": ["Lewis", "Steedman2013] Mike Lewis", "Mark Steedman"], "venue": "Transactions of ACL,", "citeRegEx": "Lewis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2013}, {"title": "Bringing machine learning and compositional semantics together", "author": ["Liang", "Potts2015] Percy Liang", "Christopher Potts"], "venue": "Annual Review of Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2015}, {"title": "Learning dependency-based compositional semantics", "author": ["Liang et al.2013] Percy Liang", "Michael I. Jordan", "Dan Klein"], "venue": "Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "Modeling semantic containment and exclusion in natural language inference", "author": ["MacCartney", "Manning2008] Bill MacCartney", "Christopher D. Manning"], "venue": null, "citeRegEx": "MacCartney et al\\.,? \\Q2008\\E", "shortCiteRegEx": "MacCartney et al\\.", "year": 2008}, {"title": "Efficient estimation of word representations in vector space. arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "Advances in NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Higher-order logical inference with compositional semantics", "author": ["Pascual Mart\u0131\u0301nez-G\u00f3mez", "Yusuke Miyao", "Daisuke Bekki"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Mineshima et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mineshima et al\\.", "year": 2015}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Lapata2010] Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Mnih", "Teh2012] Andriy Mnih", "Yee Whye Teh"], "venue": "Proceedings of ICML", "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "Dependency-based construction of semantic space models", "author": ["Pad\u00f3", "Lapata2007] Sebastian Pad\u00f3", "Mirella Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Pad\u00f3 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2007}, {"title": "A practical and linguistically-motivated approach to compositional distributional semantics", "author": ["Nghia The Pham", "Marco Baroni"], "venue": "In Proceedings of ACL", "citeRegEx": "Paperno et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model", "author": ["Pham et al.2015] Nghia The Pham", "Germ\u00e1n Kruszewski", "Angeliki Lazaridou", "Marco Baroni"], "venue": "Proceedings of ACL", "citeRegEx": "Pham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "Unsupervised semantic parsing", "author": ["Poon", "Domingos2009] Hoifung Poon", "Pedro Domingos"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Poon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2009}, {"title": "Low-dimensional embeddings of logic", "author": ["Matko Bosnjak", "Sameer Singh", "Sebastian Riedel"], "venue": "In ACL Workshop on Semantic Parsing (SP\u201914)", "citeRegEx": "Rocktaeschel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rocktaeschel et al\\.", "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H. Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y. Ng"], "venue": "Advances in NIPS", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Taking Scope - The Natural Semantics of Quantifiers", "author": ["Mark Steedman"], "venue": null, "citeRegEx": "Steedman.,? \\Q2012\\E", "shortCiteRegEx": "Steedman.", "year": 2012}, {"title": "Logical inference on dependency-based compositional semantics", "author": ["Tian et al.2014] Ran Tian", "Yusuke Miyao", "Takuya Matsuzaki"], "venue": "In Proceedings of ACL", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "The mechanism of additive composition", "author": ["Tian et al.2015] Ran Tian", "Naoaki Okazaki", "Kentaro Inui"], "venue": null, "citeRegEx": "Tian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2015}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Semantic relation classification via convolutional neural networks with simple negative sampling", "author": ["Xu et al.2015] Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao"], "venue": "Proceedings of EMNLP", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Computational approaches to sentence completion", "author": ["Zweig et al.2012] Geoffrey Zweig", "John C. Platt", "Christopher Meek", "Christopher J.C. Burges", "Ainur Yessenalina", "Qiang Liu"], "venue": "Proceedings of ACL", "citeRegEx": "Zweig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 32, "context": "A simple and illustrative example is the Dependency-based Compositional Semantics (DCS) (Liang et al., 2013).", "startOffset": 88, "endOffset": 108}, {"referenceID": 32, "context": "DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al.", "startOffset": 48, "endOffset": 68}, {"referenceID": 48, "context": ", 2013) and textual entailment recognition (Tian et al., 2014).", "startOffset": 43, "endOffset": 62}, {"referenceID": 29, "context": "Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al.", "startOffset": 132, "endOffset": 176}, {"referenceID": 46, "context": ", 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014).", "startOffset": 98, "endOffset": 255}, {"referenceID": 40, "context": ", 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014).", "startOffset": 98, "endOffset": 255}, {"referenceID": 22, "context": ", 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014).", "startOffset": 98, "endOffset": 255}, {"referenceID": 49, "context": "First, recent research has shown that additive composition of word vectors is an approximation to the situation where two words have overlapping context (Tian et al., 2015); therefore, it is suitable to implement an \u201cand\u201d or intersection operation (Section 3).", "startOffset": 153, "endOffset": 172}, {"referenceID": 41, "context": "Experimentally, we show that vectors and matrices learned by our model exhibit favorable characteristics as compared with vectors trained by GloVe (Pennington et al., 2014) or those learned from syntactic dependencies (Section 5.", "startOffset": 147, "endOffset": 172}, {"referenceID": 52, "context": "We quantitatively evaluate this utility on sentence completion task (Zweig et al., 2012) and report a new state-of-the-art (Section 5.", "startOffset": 68, "endOffset": 88}, {"referenceID": 32, "context": "DCS can be further extended to handle phenomena such as quantifiers or superlatives (Liang et al., 2013; Tian et al., 2014).", "startOffset": 84, "endOffset": 123}, {"referenceID": 48, "context": "DCS can be further extended to handle phenomena such as quantifiers or superlatives (Liang et al., 2013; Tian et al., 2014).", "startOffset": 84, "endOffset": 123}, {"referenceID": 32, "context": "DCS trees can be learned from question-answer pairs and a given database of denotations (Liang et al., 2013), or they can be extracted from dependency trees if no database is specified, by taking advantage of the observation that DCS trees are similar to dependency trees (Tian et al.", "startOffset": 88, "endOffset": 108}, {"referenceID": 48, "context": ", 2013), or they can be extracted from dependency trees if no database is specified, by taking advantage of the observation that DCS trees are similar to dependency trees (Tian et al., 2014).", "startOffset": 171, "endOffset": 190}, {"referenceID": 48, "context": "An effect of adding up two skip-gram vectors is further analyzed in Tian et al. (2015). Namely, the target vector vw can be regarded as encoding the distribution of context words surrounding w.", "startOffset": 68, "endOffset": 87}, {"referenceID": 9, "context": "We use stochastic gradient descent (Bottou, 2012) for training.", "startOffset": 35, "endOffset": 49}, {"referenceID": 17, "context": "17 Grefenstette and Sadrzadeh (2011) - 0.", "startOffset": 3, "endOffset": 37}, {"referenceID": 17, "context": "17 Grefenstette and Sadrzadeh (2011) - 0.21 Blacoe and Lapata (2012):RAE 0.", "startOffset": 3, "endOffset": 69}, {"referenceID": 17, "context": "17 Grefenstette and Sadrzadeh (2011) - 0.21 Blacoe and Lapata (2012):RAE 0.31 0.30 0.28 Grefenstette (2013a) - 0.", "startOffset": 3, "endOffset": 109}, {"referenceID": 17, "context": "17 Grefenstette and Sadrzadeh (2011) - 0.21 Blacoe and Lapata (2012):RAE 0.31 0.30 0.28 Grefenstette (2013a) - 0.27 Paperno et al. (2014) - 0.", "startOffset": 3, "endOffset": 138}, {"referenceID": 17, "context": "17 Grefenstette and Sadrzadeh (2011) - 0.21 Blacoe and Lapata (2012):RAE 0.31 0.30 0.28 Grefenstette (2013a) - 0.27 Paperno et al. (2014) - 0.36 Hashimoto et al. (2014):Waddnl 0.", "startOffset": 3, "endOffset": 169}, {"referenceID": 17, "context": "17 Grefenstette and Sadrzadeh (2011) - 0.21 Blacoe and Lapata (2012):RAE 0.31 0.30 0.28 Grefenstette (2013a) - 0.27 Paperno et al. (2014) - 0.36 Hashimoto et al. (2014):Waddnl 0.48 0.40 0.39 0.34 Kartsaklis and Sadrzadeh (2014) - 0.", "startOffset": 3, "endOffset": 228}, {"referenceID": 41, "context": "Additionally, we compare with the GloVe (6B, 300d) vector5 (Pennington et al., 2014).", "startOffset": 59, "endOffset": 84}, {"referenceID": 17, "context": "The dataset GS11 created by Grefenstette and Sadrzadeh (2011) (100 pairs, 25 annotators) is also of the form SVO, but in each pair only the verbs are different (e.", "startOffset": 28, "endOffset": 62}, {"referenceID": 17, "context": "The dataset GS12 described in Grefenstette (2013a) (194 pairs, 50 annotators) is of the form Adjective-Noun-Verb-AdjectiveNoun (e.", "startOffset": 30, "endOffset": 51}, {"referenceID": 21, "context": "The GS11 dataset appears to favor models that can learn from interactions between the subject and object arguments, such as the non-linear model Waddnl in Hashimoto et al. (2014) and the entanglement model in Kartsaklis and Sadrzadeh (2014).", "startOffset": 155, "endOffset": 179}, {"referenceID": 21, "context": "The GS11 dataset appears to favor models that can learn from interactions between the subject and object arguments, such as the non-linear model Waddnl in Hashimoto et al. (2014) and the entanglement model in Kartsaklis and Sadrzadeh (2014). However, these models do not show particular advantages on other datasets.", "startOffset": 155, "endOffset": 241}, {"referenceID": 21, "context": "The GS11 dataset appears to favor models that can learn from interactions between the subject and object arguments, such as the non-linear model Waddnl in Hashimoto et al. (2014) and the entanglement model in Kartsaklis and Sadrzadeh (2014). However, these models do not show particular advantages on other datasets. The recursive autoencoder (RAE) proposed in Socher et al. (2011) shares an aspect with vecDCS as to construct meanings from parse trees.", "startOffset": 155, "endOffset": 382}, {"referenceID": 21, "context": "The GS11 dataset appears to favor models that can learn from interactions between the subject and object arguments, such as the non-linear model Waddnl in Hashimoto et al. (2014) and the entanglement model in Kartsaklis and Sadrzadeh (2014). However, these models do not show particular advantages on other datasets. The recursive autoencoder (RAE) proposed in Socher et al. (2011) shares an aspect with vecDCS as to construct meanings from parse trees. It is tested by Blacoe and Lapata (2012) for compositionality, where vecDCS appears to be better.", "startOffset": 155, "endOffset": 495}, {"referenceID": 21, "context": "The GS11 dataset appears to favor models that can learn from interactions between the subject and object arguments, such as the non-linear model Waddnl in Hashimoto et al. (2014) and the entanglement model in Kartsaklis and Sadrzadeh (2014). However, these models do not show particular advantages on other datasets. The recursive autoencoder (RAE) proposed in Socher et al. (2011) shares an aspect with vecDCS as to construct meanings from parse trees. It is tested by Blacoe and Lapata (2012) for compositionality, where vecDCS appears to be better. NeverthevecDCS 81.2 -no matrix 69.2 -no inverse 79.7 vecUD 69.2 GloVe 74.1 Socher et al. (2012) 79.", "startOffset": 155, "endOffset": 648}, {"referenceID": 14, "context": "4 dos Santos et al. (2015) 84.", "startOffset": 6, "endOffset": 27}, {"referenceID": 14, "context": "4 dos Santos et al. (2015) 84.1 Xu et al. (2015) 85.", "startOffset": 6, "endOffset": 49}, {"referenceID": 23, "context": "Dataset We use the dataset of SemEval-2010 Task 8 (Hendrickx et al., 2009), in which 9 directed relations (e.", "startOffset": 50, "endOffset": 74}, {"referenceID": 51, "context": "VecDCS scores moderately lower than the state-of-the-art (Xu et al., 2015), however we note that these results are achieved by adding additional features and training task-specific neural networks (dos Santos et al.", "startOffset": 57, "endOffset": 74}, {"referenceID": 51, "context": ", 2015), however we note that these results are achieved by adding additional features and training task-specific neural networks (dos Santos et al., 2015; Xu et al., 2015).", "startOffset": 130, "endOffset": 172}, {"referenceID": 14, "context": ", 2015), however we note that these results are achieved by adding additional features and training task-specific neural networks (dos Santos et al., 2015; Xu et al., 2015). Our method only uses features constructed from unlabeled corpora. From this point of view, it is comparable to the MV-RNN model (without features) in Socher et al. (2012), and vecDCS actually does better.", "startOffset": 135, "endOffset": 345}, {"referenceID": 50, "context": "tw/ \u0303cjlin/ libsvm/ vecDCS 50 -no matrix 60 -no inverse 46 vecUD 31 N-gram (Various) 39-41 Zweig et al. (2012) 52 Mnih and Teh (2012) 55 Gubbins and Vlachos (2013) 50 Mikolov et al.", "startOffset": 91, "endOffset": 111}, {"referenceID": 50, "context": "tw/ \u0303cjlin/ libsvm/ vecDCS 50 -no matrix 60 -no inverse 46 vecUD 31 N-gram (Various) 39-41 Zweig et al. (2012) 52 Mnih and Teh (2012) 55 Gubbins and Vlachos (2013) 50 Mikolov et al.", "startOffset": 91, "endOffset": 134}, {"referenceID": 50, "context": "tw/ \u0303cjlin/ libsvm/ vecDCS 50 -no matrix 60 -no inverse 46 vecUD 31 N-gram (Various) 39-41 Zweig et al. (2012) 52 Mnih and Teh (2012) 55 Gubbins and Vlachos (2013) 50 Mikolov et al.", "startOffset": 91, "endOffset": 164}, {"referenceID": 34, "context": "(2012) 52 Mnih and Teh (2012) 55 Gubbins and Vlachos (2013) 50 Mikolov et al. (2013a) 55", "startOffset": 63, "endOffset": 86}, {"referenceID": 52, "context": "The task can be viewed as a coarse-grained question answering or an evaluation for language models (Zweig et al., 2012).", "startOffset": 99, "endOffset": 119}, {"referenceID": 11, "context": "There have been repeated calls for combining the strength of these two approaches (Coecke et al., 2010; Baroni et al., 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al.", "startOffset": 82, "endOffset": 147}, {"referenceID": 4, "context": "There have been repeated calls for combining the strength of these two approaches (Coecke et al., 2010; Baroni et al., 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al.", "startOffset": 82, "endOffset": 147}, {"referenceID": 5, "context": ", 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al., 2014; Tian et al., 2014) have contributed to this direction.", "startOffset": 52, "endOffset": 119}, {"referenceID": 48, "context": ", 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al., 2014; Tian et al., 2014) have contributed to this direction.", "startOffset": 52, "endOffset": 119}, {"referenceID": 49, "context": "Furthermore, by using additive composition we enjoy a learning guarantee (Tian et al., 2015).", "startOffset": 73, "endOffset": 92}, {"referenceID": 17, "context": "Vector-based Logic Models This work also shares the spirit with Grefenstette (2013b) and Rocktaeschel et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 17, "context": "Vector-based Logic Models This work also shares the spirit with Grefenstette (2013b) and Rocktaeschel et al. (2014), in exploring vector calculations that realize logic operations.", "startOffset": 64, "endOffset": 116}, {"referenceID": 32, "context": "Formal Semantics Our model implements a fragment of logic capable of semantic composition, largely due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013).", "startOffset": 171, "endOffset": 191}, {"referenceID": 15, "context": "It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al.", "startOffset": 53, "endOffset": 111}, {"referenceID": 12, "context": ", 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al.", "startOffset": 122, "endOffset": 170}, {"referenceID": 13, "context": ", 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al.", "startOffset": 122, "endOffset": 170}, {"referenceID": 8, "context": ", 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015).", "startOffset": 16, "endOffset": 123}, {"referenceID": 47, "context": ", 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015).", "startOffset": 16, "endOffset": 123}, {"referenceID": 0, "context": ", 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015).", "startOffset": 16, "endOffset": 123}, {"referenceID": 36, "context": ", 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015).", "startOffset": 16, "endOffset": 123}, {"referenceID": 10, "context": "We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic.", "startOffset": 89, "endOffset": 113}, {"referenceID": 21, "context": "We also borrow ideas from previous work, for example our training scheme is similar to Guu et al. (2015) in using paths and composition of matrices, and our method is similar to Poon and Domingos (2009) in building structured knowledge from clustering syntactic parse of unlabeled data.", "startOffset": 87, "endOffset": 105}, {"referenceID": 21, "context": "We also borrow ideas from previous work, for example our training scheme is similar to Guu et al. (2015) in using paths and composition of matrices, and our method is similar to Poon and Domingos (2009) in building structured knowledge from clustering syntactic parse of unlabeled data.", "startOffset": 87, "endOffset": 203}, {"referenceID": 42, "context": "Unlike several previous models (Pad\u00f3 and Lapata, 2007; Levy and Goldberg, 2014; Pham et al., 2015), our approach learns matrices at the same time that can extract the information according to different syntactic-semantic roles.", "startOffset": 31, "endOffset": 98}, {"referenceID": 27, "context": "A related application is selectional preference (Baroni and Lenci, 2010; Lenci, 2011; Van de Cruys, 2014), wherein our model might has potential for smoothly handling composition.", "startOffset": 48, "endOffset": 105}], "year": 2016, "abstractText": "This paper connects a vector-based composition model to a formal semantics, the Dependency-based Compositional Semantics (DCS). We show theoretical evidence that the vector compositions in our model conform to the logic of DCS. Experimentally, we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors, achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification; meanwhile, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence completion task and report a new state-of-the-art.", "creator": "LaTeX with hyperref package"}}}