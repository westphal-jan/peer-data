{"id": "1606.02492", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Convolutional Neural Fabrics", "abstract": "Despite the success of convolutional neural networks, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a $\"$fabric$\"$ that embeds an exponentially large number of CNN architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of the model (nr. of channels and layers) are not critical for performance. While individual CNN architectures can be recovered as paths in the trellis, the trellis can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. The trellis parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.", "histories": [["v1", "Wed, 8 Jun 2016 10:17:51 GMT  (655kb,D)", "https://arxiv.org/abs/1606.02492v1", null], ["v2", "Thu, 9 Jun 2016 16:21:57 GMT  (663kb,D)", "http://arxiv.org/abs/1606.02492v2", "Added Supplementary Material"], ["v3", "Fri, 28 Oct 2016 13:10:05 GMT  (1786kb,D)", "http://arxiv.org/abs/1606.02492v3", "Added Final version (To appear at NIPS16 )"], ["v4", "Mon, 30 Jan 2017 12:28:29 GMT  (1770kb,D)", "http://arxiv.org/abs/1606.02492v4", "Corrected typos (In proceedings of NIPS16 )"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["shreyas saxena", "jakob verbeek"], "accepted": true, "id": "1606.02492"}, "pdf": {"name": "1606.02492.pdf", "metadata": {"source": "META", "title": "Convolutional Neural Fabrics", "authors": ["Shreyas Saxena", "Jakob Verbeek", "Jean Kuntzmann"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who stand up for people's rights are not aware of themselves and their rights."}, {"heading": "2 Related work", "text": "In fact, most people who fight for the rights of women and men have the same rights as them, have the same rights as men, have the same rights and obligations as women, have the same rights and obligations as men, have the same rights and obligations as men, have the same rights and obligations as women, have the same rights and obligations as men, have the same rights and obligations as men and women, have the same rights and obligations as women, have the same rights and obligations as men, have the same rights and obligations as men, have the same rights and obligations as men, have the same rights and obligations."}, {"heading": "3 The fabric of convolutional neural networks", "text": "In this section we give an accurate definition of wavy neural tissues and show in Section 3.2 that most architectural network design decisions become irrelevant for sufficiently large tissues. Finally, we analyze the number of response cards, parameters and activations of tissues in Section 3.3."}, {"heading": "3.1 Weaving the convolutional neural fabric", "text": "Each node in the fabric represents a response card with the same dimension D as the input signal (D = 1 for audio, D = 2 for images, D = 3 for video); the fabric above the node is spanned by three axes; a layer axis preceded by all channels that exclude all cycles and analogous to the depth axis of a CNN; a scale axis along which response cards of different resolutions are organized is separated by a factor of two; a channel axis along which different response cards of the same scale and layer are organized; we use S = 1 + log2 N scales when processing inputs of size ND, for example, for 32 x 32 images that we use to obtain a scale-equal resolution."}, {"heading": "3.2 Stitching convolutional neural networks on the fabric", "text": "In fact, most of them will be able to play by the rules they have set themselves in order to play by the rules."}, {"heading": "3.3 Analysis of the number of parameters and activations", "text": "For our analysis, we ignore boundary effects and treat each node as an internal problem. The top row of Table 1 indicates the total number of response cards in the entire tissue, and the number of parameters when the channels are sparsely or not connected at all is just as high as the number of activations recorded in the meantime. The second line indicates an exponential number of architectures in the number of layers L and channels C."}, {"heading": "4 Experimental evaluation results", "text": "In this section, we first present the data sets used in our experiments, followed by evaluation results."}, {"heading": "4.1 Datasets and experimental protocol", "text": "This dataset [10] consists of 2,927 facial images from the LFW dataset [8], with annotations at the pixel level in the classes hair, skin and background. We use the standard evaluation protocol, which specifies training, validation and test kits of 1,500, 500 and 927 images, respectively. We report on accuracy at the pixel level and superpixel level. For superpixels, we use the class probabilities of the included pixels. We used horizontal flipping for data augmentation. MNIST. This dataset [16] consists of 28 x 28 pixel images of the handwritten digits 0,.., 9. We use the standard division of the dataset into 50k training samples, 10k validation samples and 10k test samples. The pixel values are standardized to [0, 1] by dividing them by 255 pixels. We extend the retraction data 10, 10 by randomly placing the 10 10 10 x 32 x x x x x x 32 x x x x x x 10."}, {"heading": "4.2 Experimental results", "text": "For all three, we are able to achieve the best results."}, {"heading": "5 Conclusion", "text": "We presented Convolutionary Neural tissues: homogeneous and locally connected trellises via response cards. tissues subsume a large class of Convolutionary Networks. They make it possible to bypass the arduous process of specifying, training and testing individual network architectures to find the best ones. While tissues use more parameters, memory and computation than are required for each of the embedded architectures, this is far less costly than the resources required to test all embedded architectures individually. tissues have only two essential hyperparameters: the number of layers and the number of channels. In practice, their setting is not decisive: we only need a sufficiently large fabric with sufficient capacity. We propose variants with dense channel connectivity and with channel duplication over scales. The latter suggests a very attractive capacity / memory contraction."}, {"heading": "A Supplementary Material", "text": "In fact, the fact is that most of them will be able to demonstrate that they are able to play by the rules and that they are able to achieve their goals."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Despite the success of CNNs, selecting the optimal architecture for a given task<lb>remains an open problem. Instead of aiming to select a single optimal architecture,<lb>we propose a \u201cfabric\u201d that embeds an exponentially large number of architectures.<lb>The fabric consists of a 3D trellis that connects response maps at different layers,<lb>scales, and channels with a sparse homogeneous local connectivity pattern. The<lb>only hyper-parameters of a fabric are the number of channels and layers. While<lb>individual architectures can be recovered as paths, the fabric can in addition<lb>ensemble all embedded architectures together, sharing their weights where their<lb>paths overlap. Parameters can be learned using standard methods based on back-<lb>propagation, at a cost that scales linearly in the fabric size. We present benchmark<lb>results competitive with the state of the art for image classification on MNIST and<lb>CIFAR10, and for semantic segmentation on the Part Labels dataset.", "creator": "LaTeX with hyperref package"}}}