{"id": "1606.02647", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Safe and Efficient Off-Policy Reinforcement Learning", "abstract": "In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace($\\lambda$), with three desired properties: (1) low variance; (2) safety, as it safely uses samples collected from any behaviour policy, whatever its degree of \"off-policyness\"; and (3) efficiency, as it makes the best use of samples collected from near on-policy behaviour policies. We analyse the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. To our knowledge, this is the first return-based off-policy control algorithm converging a.s. to $Q^*$ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q($\\lambda$), which was still an open problem. We illustrate the benefits of Retrace($\\lambda$) on a standard suite of Atari 2600 games.", "histories": [["v1", "Wed, 8 Jun 2016 17:34:13 GMT  (150kb,D)", "http://arxiv.org/abs/1606.02647v1", null], ["v2", "Mon, 7 Nov 2016 21:26:31 GMT  (184kb,D)", "http://arxiv.org/abs/1606.02647v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["r\u00e9mi munos", "tom stepleton", "anna harutyunyan", "marc g bellemare"], "accepted": true, "id": "1606.02647"}, "pdf": {"name": "1606.02647.pdf", "metadata": {"source": "CRF", "title": "Safe and efficient off-policy reinforcement learning", "authors": ["R\u00e9mi Munos", "Tom Stepleton", "Anna Harutyunyan", "Marc G. Bellemare"], "emails": ["munos@google.com", "stepleton@google.com", "anna.harutyunyan@vub.ac.be", "bellemare@google.com"], "sections": [{"heading": null, "text": "A fundamental trade-off in amplification policy is the definition of the updating target: should one estimate how high the TB returns are or how high the gain from an existing Q function is? Return-oriented methods (where the return refers to the sum of discounted returns) offer some advantages over other methods: they behave better when combined with functional approximation, and quickly propagate the fruits of exploration (Sutton, 1996). On the other hand, value neutralization methods are more easily applicable to non-political data, a common use case. In this paper, we show that learning from returns does not necessarily have to take place with non-political learning goals. We start with the recent work by Harutyunyan et al al al. (2016), which shows that naive non-political policy assessment without correction of \"non-political policies\" still adapts to the desired QE value function."}, {"heading": "1 Notation", "text": "We consider an actor that interacts with a Markov decision-making process (X, A, Q, Q, Q, Q, r). X is a finite state space, A is the action space, \u03b3 is the discount factor, P is the transition function mapping state action pairs (x, a) Q \u00b7 A to distributions over X, and r: X \u00b7 A \u2192 [\u2212 RMAX, RMAX] is the reward function. \u2212 For notation simplification \u2212 Q maps we consider a finite action pair (x, a) to a value in R, but the case of infinite - possibly continuous - action space can also be handled by the Retrace (\u03bb) algorithm. A political action space is a mapping of X to a distribution over A. Q function Q maps we consider each state action pair (x, a) to a value in R; in particular the reward r is a Q function. For a policy measure we define the operator P\u03c0 (x, a)."}, {"heading": "2 Off-Policy Algorithms", "text": "We are interested in two related off-policy learning problems = if t = 0. In the policy evaluation setting, we are not given a fixed policy approach (Q = 2001), the value of which we want to estimate on the basis of examples drawn from a policy of conduct. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &"}, {"heading": "3 Analysis of Retrace(\u03bb)", "text": "For our part, we will analyze both the non-political evaluation and the control attitudes and show that R is a contraction map in both attitudes (with a mild additional assumption for the control case)."}, {"heading": "3.1 Policy Evaluation", "text": "To simplify the exposure, we consider a rigid behavior policy \u00b5 = Q = Q = Q = Q (Q = Q = Q = Q = Q = Q (Q = Q = Q = Q = Q = Q = Q = Q = Q (Q = Q = Q = Q = Q = Q = Q = Q (Q = Q = Q = Q = Q = Q)) (to emphasize that cs can be a function of the entire history Fs) \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 t (as | xs) \u00b5 (as | xs).Theorem 1. The operator R (as (as (4) has a unique fixed point Q\u03c0. Furthermore, if we have a function of the entire history Fs for each point Fs, we have cs = cs = cs (as, Fs) \u00b5 (as | xs) \u00b5 (as | xs) \u00b5 (as | xs) \u00b5 (as | x.Theorem 1. The point has a unique (1) as an operator (4)."}, {"heading": "3.2 Control", "text": "In the control setting, the single target policy is replaced by a sequence of measures that depend on the Q-Q policy. While most previous work has focused on strictly greedy measures, here we look at the larger class of increasingly greedy measures. We are now making this idea precise. Definition 1. We say that a sequence of measures (\u03c0k: k-N) is increasingly greedy. This means that each measure is at least as greedy as the previous measure (Qk: k-N) of the Q functions if the following property applies to all: P\u03c0k + 1Qk + 1Q. Intuitively, this means that each measure is at least as greedy as the previous measure. Many natural sequences of measures are increasingly greedy, including non-increasing measures (with non-increasing QE-Q-K) and softmax measures (with non-increasing temperature). See the proofs in the plant. We assume that cs = cs (as, xs) Markoviv is in place."}, {"heading": "3.3 Online algorithms", "text": "We analyze the algorithms in each form of visit (Sutton and Barto, 1998), which is the more practical generalization of the first form of visit. In this section, however, we will consider only the rerace (\u03bb) algorithm defined with the coefficient c = \u03bbmin (1, \u03c0xi / \u00b5). Therefore, let us rewrite the operator P c\u00b5 as \u03bbP\u03c0 (x, a), in the prace \u00b5Q (x, a): The rerace operator P c\u00b5, in the pig-Q (x, a): the results y (b | y), \u00b5 (b)), and we write the rerace operator RQ + (I) - The rerace operator P c\u00b5, in the pig-Q (a): the results y (b), \u00b5 (b), and we write the rerace operator RQ + (I) - The rerace operator RQ + (I)"}, {"heading": "4 Discussion of the results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Choice of the trace coefficients cs", "text": "Theorems 1 and 2 ensure that convergence to Q\u03c0 and Q * is greater for any trace coefficient cs (0, \u03c0 (as | xs) \u00b5 (as | xs)). However, in order to make the best choice of cs, we must take into account the speed of convergence, which depends both (1) on the deviation of the online coefficient, which indicates how many online updates are required in a single iteration of R, and (2) the contraction coefficient of R. Variance The deviation of the estimate depends greatly on the deviation of the product trace (c1. contract), which is not easy to control, since the (cs) are not normally independent. However, assuming independence and stationary nature of (cs), we have the deviation from the deviation from the deviation of the product trace (c1.... contract) is at least an appropriateness of t-2 tV (c) and c), which is < then, if an algorithm is finite (V)."}, {"heading": "4.2 Other topics of discussion", "text": "The crucial point of Theorem 2 is that convergence to Q * TB occurs for any behavioural policies. Therefore, the online result of Theorem 3 does not require that behavioural policies at the boundary of infinite exploration become greedy (i.e. the GLIE assumption, Singh et al., 2000). We believe that Theorem 3 provides the first convergence result to Q * for a consistent return (with \u03bb > 0) algorithm that does not require this (difficult to satisfy) assumption, unlike Watkins \"Q (\u03bb). As a logical consequence of Theorem 3 in selecting our target policies that are to be greedy. Qk (i.e. \u03b5k = 0) we conclude from this that Watkins\" Q \"(\u03bb) (e.g. Watkins, 1989; Sutton and Barto, 1998) convergence a.s. to Q * (assuming that Mikrok connects with greed policies."}, {"heading": "5 Experimental Results", "text": "To validate our theoretical results, we use Retrace (\u03bb) in an empirical comparison (Lin, 1993), in which sampling transitions are stored within a large but limited replay memory and then repeated as if they were new experiences. Of course, older data in memory is usually derived from a policy that differs from current policy, and provides an excellent point of comparison for those in Section 2. Our agent adapts the DQN architecture of Mnih et al. (2015) to render short sequences from memory (details in Appendix F) instead of individual transitions. The Q function target for a sample sequence xt, at, rt, \u00b7, Application + k is \u2206 Q (xt, at) = k \u2212 1 \u00b2 s = t \u00b2 s \u2212 t (s \u00b2 i = t + 1 ci) [r (xs, as) and the target for a sample sequence xt (xs + 1, \u00b7) \u2212 Q (xs, as)."}, {"heading": "A Proof of Lemma 1", "text": "Proof (Lemma 1). Let's start by rewriting Q (Q) (4): RQ (x, a) = p (x, a) = p (x, a) = p (x, a) = p (x, a) = p (x, a) = p (x, b) = p (x, b) = p (x, b) = p (x, a) = p (x, a) = p (x, a) = p (x, a) = p (x, a) = p (x, b) (t, c) (x, b) (x) = p (x, b). Since Qp (x) is the fixed point of R, we have Qp (x, a) = p (x, a) = p (t, c)."}, {"heading": "C Proof of Theorem 2", "text": "As mentioned in the main K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K"}, {"heading": "D Proof of Theorem 3", "text": "We first prove the convergence of the general online algorithm. Theorem 4: Consider the algorithm mQk + 1 (x, a) = (1 \u2212 \u03b1k (x, a)) Qk (x, a) (RkQk (x, a) + 2) (x, a) + 2 (x, a), (18) and assume that (1) p \u2212 \u2212 \u2212 P (x, a) is a constant convergence, Fk \u2212 2) (RkQk \u2212 P (x, a) + 2) (x, a) + 1). Then, under the same assumptions as in Theorem 3, we have that Qk \u2192 Q \"almost surely.Proof. We writeR forRk.\" Let us prove the result in three steps."}, {"heading": "E Asymptotic commutativity of P \u03c0k and P \u03c0k\u2227\u00b5k", "text": "4. Let us apply (\u03c0k) and (\u00b5k) two sequences of politics. If there is such a policy (\u03c0k), then the transition matrix applies to all x, min (\u03c0k (a | x), p (\u03c0k) + o (1), (29) then the transition matrix (\u03c0k) applies. For each Q we have (\u03b1 (\u03b1) the transition matrices P\u03c0k (x), p (x), p (x), p (y), p (x), p (b), p (b), p (p), p (c), p), p), p (z), p (c), p), p (p), p (z), p (c), p)."}, {"heading": "F Experimental Methods", "text": "Although our learning problem closely matches the DQN setting used by Mnih et al. (2015) (i.e. a single thread outside the political learning process with large repetitive memory), we conducted our experiments in the multithreaded, CPU-based framework of Mnih et al. (2016), obtaining extensive results data from affordable CPU resources. Key differences from the DQN are as follows: Sixteen private-environment threads are used simultaneously; each thread includes and finds progression phases w.r.t. a local copy of network parameters is then updated and local copies are refreshed; target network parameters are simply shared globally; each thread has a private repetitive memory with 62,500 transitions (1 / 16th of DQN total repetitive capacity); the optimizer is unchanged from (Mnih et al., 2016): \"Shared RMSprop\" with a step size from 3 to x0."}], "references": [{"title": "The Arcade Learning Environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279.", "citeRegEx": "Bellemare et al\\.,? 2013", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Neuro-Dynamic Programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific.", "citeRegEx": "Bertsekas and Tsitsiklis,? 1996", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Off-policy learning with eligibility traces: A survey", "author": ["M. Geist", "B. Scherrer"], "venue": "The Journal of Machine Learning Research, 15(1):289\u2013333.", "citeRegEx": "Geist and Scherrer,? 2014", "shortCiteRegEx": "Geist and Scherrer", "year": 2014}, {"title": "Generalized emphatic temporal difference learning: Bias-variance analysis", "author": ["A. Hallak", "A. Tamar", "R. Munos", "S. Mannor"], "venue": "arXiv:1509.05172.", "citeRegEx": "Hallak et al\\.,? 2015", "shortCiteRegEx": "Hallak et al\\.", "year": 2015}, {"title": "Q(\u03bb) with off-policy corrections", "author": ["A. Harutyunyan", "M.G. Bellemare", "T. Stepleton", "R. Munos"], "venue": "arXiv:1602.04951.", "citeRegEx": "Harutyunyan et al\\.,? 2016", "shortCiteRegEx": "Harutyunyan et al\\.", "year": 2016}, {"title": "Bias-variance error bounds for temporal difference updates", "author": ["M.J. Kearns", "S.P. Singh"], "venue": "Conference on Computational Learning Theory, pages 142\u2013147.", "citeRegEx": "Kearns and Singh,? 2000", "shortCiteRegEx": "Kearns and Singh", "year": 2000}, {"title": "Scaling up reinforcement learning for robot control", "author": ["L. Lin"], "venue": "Machine Learning: Proceedings of the Tenth International Conference, pages 182\u2013189.", "citeRegEx": "Lin,? 1993", "shortCiteRegEx": "Lin", "year": 1993}, {"title": "Off-policy learning based on weighted importance sampling with linear computational complexity", "author": ["A.R. Mahmood", "R.S. Sutton"], "venue": "Conference on Uncertainty in Artificial Intelligence.", "citeRegEx": "Mahmood and Sutton,? 2015", "shortCiteRegEx": "Mahmood and Sutton", "year": 2015}, {"title": "Emphatic temporal-difference learning", "author": ["A.R. Mahmood", "H. Yu", "M. White", "R.S. Sutton"], "venue": "arXiv:1507.01569.", "citeRegEx": "Mahmood et al\\.,? 2015", "shortCiteRegEx": "Mahmood et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv:1602.01783.", "citeRegEx": "Mnih et al\\.,? 2016", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["D. Precup", "R.S. Sutton", "S. Dasgupta"], "venue": "International Conference on Machine Laerning, pages 417\u2013424.", "citeRegEx": "Precup et al\\.,? 2001", "shortCiteRegEx": "Precup et al\\.", "year": 2001}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "Proceedings of the Seventeenth International Conference on Machine Learning.", "citeRegEx": "Precup et al\\.,? 2000", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": "John Wiley & Sons, Inc., New York, NY, USA, 1st edition.", "citeRegEx": "Puterman,? 1994", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Schaul et al\\.,? 2016", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "Convergence results for singlestep on-policy reinforcement-learning algorithms", "author": ["S. Singh", "T. Jaakkola", "M.L. Littman", "C. Szepesv\u00e1ri"], "venue": "Machine Learning, 38(3):287\u2013308.", "citeRegEx": "Singh et al\\.,? 2000", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Reinforcement learning: An introduction, volume 116", "author": ["R. Sutton", "A. Barto"], "venue": "Cambridge Univ Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine learning, 3(1):9\u201344.", "citeRegEx": "Sutton,? 1988", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["R.S. Sutton"], "venue": "Advances in Neural Information Processing Systems 8.", "citeRegEx": "Sutton,? 1996", "shortCiteRegEx": "Sutton", "year": 1996}, {"title": "On the convergence of optimistic policy iteration", "author": ["J.N. Tsitsiklis"], "venue": "Journal of Machine", "citeRegEx": "Tsitsiklis,? 2003", "shortCiteRegEx": "Tsitsiklis", "year": 2003}, {"title": "Learning from Delayed Rewards", "author": ["Watkins", "C.J.C. H"], "venue": "Learning Research,", "citeRegEx": "Watkins and H.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and H.", "year": 1989}], "referenceMentions": [{"referenceID": 18, "context": "One fundamental trade-off in reinforcement learning lies in the definition of the update target: should one estimate Monte Carlo returns or bootstrap from an existing Q-function? Return-based methods (where return refers to the sum of discounted rewards \u2211 t \u03b3 rt) offer some advantages over value bootstrap methods: they are better behaved when combined with function approximation, and quickly propagate the fruits of exploration (Sutton, 1996).", "startOffset": 431, "endOffset": 445}, {"referenceID": 12, "context": "Alternatively, the Tree-backup (TB) (\u03bb) algorithm (Precup et al., 2000) tolerates arbitrary target/behavior discrepancies by scaling information (here called traces) from future temporal differences by the product of target policy probabilities.", "startOffset": 50, "endOffset": 71}, {"referenceID": 6, "context": "Retrace(\u03bb) can learn from full returns retrieved from past policy data, as in the context of experience replay (Lin, 1993), which has returned to favour with advances in deep reinforcement learning (Mnih ar X iv :1 60 6.", "startOffset": 111, "endOffset": 122}, {"referenceID": 4, "context": "We start from the recent work of Harutyunyan et al. (2016), who show that naive off-policy policy evaluation, without correcting for the \u201coff-policyness\u201d of a trajectory, still converges to the desired Q value function provided the behavior \u03bc and target \u03c0 policies are not too far apart (the maximum allowed distance depends on the \u03bb parameter).", "startOffset": 33, "endOffset": 59}, {"referenceID": 15, "context": "To the best of our knowledge, this is the first online return-based off-policy control algorithm which does not require the GLIE (Greedy in the Limit with Infinite Exploration) assumption (Singh et al., 2000).", "startOffset": 188, "endOffset": 208}, {"referenceID": 16, "context": "In addition, we provide as a corollary the first proof of convergence of Watkins\u2019 Q(\u03bb) (see, e.g., Watkins, 1989; Sutton and Barto, 1998).", "startOffset": 87, "endOffset": 137}, {"referenceID": 0, "context": "Finally, we illustrate the significance of Retrace(\u03bb) in a deep learning setting by applying it to the suite of Atari 2600 games provided by the Arcade Learning Environment (Bellemare et al., 2013).", "startOffset": 173, "endOffset": 197}, {"referenceID": 13, "context": "(3) Its fixed point is Q\u2217, the unique optimal value function (Puterman, 1994).", "startOffset": 61, "endOffset": 77}, {"referenceID": 17, "context": "Return-based Operators: The \u03bb-return extension (Sutton, 1988) of both (2) and (3) considers exponentially weighted sums of n-step returns: T \u03c0 \u03bb Q := (1\u2212 \u03bb) \u2211", "startOffset": 47, "endOffset": 61}, {"referenceID": 16, "context": "At one extreme (\u03bb = 0) we have the Bellman operator T \u03c0 \u03bb=0Q = T Q, while at the other (\u03bb = 1) we have the policy evaluation operator T \u03c0 \u03bb=1Q = Q which can be estimated using Monte Carlo methods (Sutton and Barto, 1998).", "startOffset": 196, "endOffset": 220}, {"referenceID": 5, "context": "Intermediate values of \u03bb trade off estimation bias with sample variance (Kearns and Singh, 2000).", "startOffset": 72, "endOffset": 96}, {"referenceID": 16, "context": "By extension of the idea of eligibility traces (Sutton and Barto, 1998), we informally call the coefficients (cs) the traces of the operator.", "startOffset": 47, "endOffset": 71}, {"referenceID": 2, "context": "Importance sampling is the simplest way to correct for the discrepancy between \u03bc and \u03c0 when learning from off-policy returns (Precup et al., 2000, 2001; Geist and Scherrer, 2014).", "startOffset": 125, "endOffset": 178}, {"referenceID": 7, "context": "It is well known that IS estimates can suffer from large \u2013 even possibly infinite \u2013 variance (mainly due to the variance of the product \u03c0(a1|x1) \u03bc(a1|x1) \u00b7 \u00b7 \u00b7 \u03c0(at|xt) \u03bc(at|xt) ), which has motivated further variance reduction techniques such as (Mahmood and Sutton, 2015; Mahmood et al., 2015; Hallak et al., 2015).", "startOffset": 247, "endOffset": 316}, {"referenceID": 8, "context": "It is well known that IS estimates can suffer from large \u2013 even possibly infinite \u2013 variance (mainly due to the variance of the product \u03c0(a1|x1) \u03bc(a1|x1) \u00b7 \u00b7 \u00b7 \u03c0(at|xt) \u03bc(at|xt) ), which has motivated further variance reduction techniques such as (Mahmood and Sutton, 2015; Mahmood et al., 2015; Hallak et al., 2015).", "startOffset": 247, "endOffset": 316}, {"referenceID": 3, "context": "It is well known that IS estimates can suffer from large \u2013 even possibly infinite \u2013 variance (mainly due to the variance of the product \u03c0(a1|x1) \u03bc(a1|x1) \u00b7 \u00b7 \u00b7 \u03c0(at|xt) \u03bc(at|xt) ), which has motivated further variance reduction techniques such as (Mahmood and Sutton, 2015; Mahmood et al., 2015; Hallak et al., 2015).", "startOffset": 247, "endOffset": 316}, {"referenceID": 2, "context": ", 2000, 2001; Geist and Scherrer, 2014). The off-policy correction uses the product of the likelihood ratios between \u03c0 and \u03bc. Notice that the RQ operator (4) defined with this choice of (cs) yields Q for any Q. For Q = 0 we recover the basic IS estimate \u2211 t\u22650 \u03b3 t (\u220ft s=1 cs ) rt, thus (4) can be seen as a variance reduction technique (with a baseline Q). It is well known that IS estimates can suffer from large \u2013 even possibly infinite \u2013 variance (mainly due to the variance of the product \u03c0(a1|x1) \u03bc(a1|x1) \u00b7 \u00b7 \u00b7 \u03c0(at|xt) \u03bc(at|xt) ), which has motivated further variance reduction techniques such as (Mahmood and Sutton, 2015; Mahmood et al., 2015; Hallak et al., 2015). Off-policy Q(\u03bb) and Q\u2217(\u03bb): cs = \u03bb. A recent alternative proposed by Harutyunyan et al. (2016) introduces an off-policy correction based on a Q-baseline (instead of correcting the probability of the sample path like in IS).", "startOffset": 14, "endOffset": 769}, {"referenceID": 2, "context": ", 2000, 2001; Geist and Scherrer, 2014). The off-policy correction uses the product of the likelihood ratios between \u03c0 and \u03bc. Notice that the RQ operator (4) defined with this choice of (cs) yields Q for any Q. For Q = 0 we recover the basic IS estimate \u2211 t\u22650 \u03b3 t (\u220ft s=1 cs ) rt, thus (4) can be seen as a variance reduction technique (with a baseline Q). It is well known that IS estimates can suffer from large \u2013 even possibly infinite \u2013 variance (mainly due to the variance of the product \u03c0(a1|x1) \u03bc(a1|x1) \u00b7 \u00b7 \u00b7 \u03c0(at|xt) \u03bc(at|xt) ), which has motivated further variance reduction techniques such as (Mahmood and Sutton, 2015; Mahmood et al., 2015; Hallak et al., 2015). Off-policy Q(\u03bb) and Q\u2217(\u03bb): cs = \u03bb. A recent alternative proposed by Harutyunyan et al. (2016) introduces an off-policy correction based on a Q-baseline (instead of correcting the probability of the sample path like in IS). This approach, called Q(\u03bb) and Q\u2217(\u03bb) for policy evaluation and control, respectively, corresponds to the choice cs = \u03bb. It offers the advantage of avoiding the blow-up of the variance of the product of ratios encountered with IS. Interestingly, this operator contracts around Q provided that \u03bc and \u03c0 are sufficiently close to each other. Defining \u03b5 := maxx \u2016\u03c0(\u00b7|x)\u2212\u03bc(\u00b7|x)\u20161 the amount of \u201coff-policyness\u201d, the authors prove that the operator defined by (4) with cs = \u03bb is a contraction mapping around Q for \u03bb < 1\u2212\u03b3 \u03b3\u03b5 , and around Q \u2217 for the worst case of \u03bb < 1\u2212\u03b3 2\u03b3 . Unfortunately, Q(\u03bb) requires knowledge of \u03b5, and the condition for Q\u2217(\u03bb) is very conservative. Neither Q(\u03bb), nor Q\u2217(\u03bb) are safe as they do not guarantee convergence for arbitrary \u03c0 and \u03bc. Tree-backup (TB) (\u03bb): cs = \u03bb\u03c0(as|xs). The TB(\u03bb) algorithm of Precup et al. (2000) corrects for the target/behaviour discrepancy by multiplying each term of the sum by the product of target policy probabilities.", "startOffset": 14, "endOffset": 1738}, {"referenceID": 16, "context": "We analyze the algorithms in the every visit form (Sutton and Barto, 1998), which is the more practical generalization of the first-visit form.", "startOffset": 50, "endOffset": 74}, {"referenceID": 1, "context": "The proof extends similar convergence proofs of TD(\u03bb) by Bertsekas and Tsitsiklis (1996) and of optimistic policy iteration by Tsitsiklis (2003), and is provided in the appendix.", "startOffset": 57, "endOffset": 89}, {"referenceID": 1, "context": "The proof extends similar convergence proofs of TD(\u03bb) by Bertsekas and Tsitsiklis (1996) and of optimistic policy iteration by Tsitsiklis (2003), and is provided in the appendix.", "startOffset": 57, "endOffset": 145}, {"referenceID": 16, "context": "\u03b5k = 0), we deduce that Watkins\u2019 Q(\u03bb) (e.g., Watkins, 1989; Sutton and Barto, 1998) converges a.", "startOffset": 38, "endOffset": 83}, {"referenceID": 6, "context": "To validate our theoretical results, we employ Retrace(\u03bb) in an experience replay (Lin, 1993) setting, where sample transitions are stored within a large but bounded replay memory and subsequently replayed as if they were new experience.", "startOffset": 82, "endOffset": 93}, {"referenceID": 6, "context": "To validate our theoretical results, we employ Retrace(\u03bb) in an experience replay (Lin, 1993) setting, where sample transitions are stored within a large but bounded replay memory and subsequently replayed as if they were new experience. Naturally, older data in the memory is usually drawn from a policy which differs from the current policy, offering an excellent point of comparison for the algorithms presented in Section 2. Our agent adapts the DQN architecture of Mnih et al. (2015) to replay short sequences from the memory (details in Appendix F) instead of single transitions.", "startOffset": 83, "endOffset": 489}, {"referenceID": 0, "context": "We compare our algorithms\u2019 performance on 60 different Atari 2600 games in the Arcade Learning Environment (Bellemare et al., 2013) using Bellemare et al.", "startOffset": 107, "endOffset": 131}], "year": 2016, "abstractText": "In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace(\u03bb), with three desired properties: (1) low variance; (2) safety, as it safely uses samples collected from any behaviour policy, whatever its degree of \u201coff-policyness\u201d; and (3) efficiency, as it makes the best use of samples collected from near on-policy behaviour policies. We analyse the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. To our knowledge, this is the first return-based off-policy control algorithm converging a.s. to Q\u2217 without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins\u2019 Q(\u03bb), which was still an open problem. We illustrate the benefits of Retrace(\u03bb) on a standard suite of Atari 2600 games. One fundamental trade-off in reinforcement learning lies in the definition of the update target: should one estimate Monte Carlo returns or bootstrap from an existing Q-function? Return-based methods (where return refers to the sum of discounted rewards \u2211 t \u03b3 rt) offer some advantages over value bootstrap methods: they are better behaved when combined with function approximation, and quickly propagate the fruits of exploration (Sutton, 1996). On the other hand, value bootstrap methods are more readily applied to off-policy data, a common use case. In this paper we show that learning from returns need not be at cross-purposes with off-policy learning. We start from the recent work of Harutyunyan et al. (2016), who show that naive off-policy policy evaluation, without correcting for the \u201coff-policyness\u201d of a trajectory, still converges to the desired Q value function provided the behavior \u03bc and target \u03c0 policies are not too far apart (the maximum allowed distance depends on the \u03bb parameter). Their Q(\u03bb) algorithm learns from trajectories generated by \u03bc simply by summing discounted off-policy corrected rewards at each time step. Unfortunately, the assumption that \u03bc and \u03c0 are close is restrictive, as well as difficult to uphold in the control case, where the target policy is always greedy with respect to the current Q-function. In that sense this algorithm is not safe: it does not handle the case of arbitrary \u201coff-policyness\u201d. Alternatively, the Tree-backup (TB) (\u03bb) algorithm (Precup et al., 2000) tolerates arbitrary target/behavior discrepancies by scaling information (here called traces) from future temporal differences by the product of target policy probabilities. TB(\u03bb) is not efficient in the \u201cnear on-policy\u201d case (similar \u03bc and \u03c0), though, as traces may be cut prematurely, blocking learning from full returns. In this work, we express several off-policy, return-based algorithms in a common form. From this we derive an improved algorithm, Retrace(\u03bb), which is both safe and efficient, enjoying convergence guarantees for off-policy policy evaluation and \u2013 more importantly \u2013 for the control setting. Retrace(\u03bb) can learn from full returns retrieved from past policy data, as in the context of experience replay (Lin, 1993), which has returned to favour with advances in deep reinforcement learning (Mnih ar X iv :1 60 6. 02 64 7v 1 [ cs .L G ] 8 J un 2 01 6 et al., 2015; Schaul et al., 2016). Off-policy learning is also desirable for exploration, since it allows the agent to deviate from the target policy currently under evaluation. To the best of our knowledge, this is the first online return-based off-policy control algorithm which does not require the GLIE (Greedy in the Limit with Infinite Exploration) assumption (Singh et al., 2000). In addition, we provide as a corollary the first proof of convergence of Watkins\u2019 Q(\u03bb) (see, e.g., Watkins, 1989; Sutton and Barto, 1998). Finally, we illustrate the significance of Retrace(\u03bb) in a deep learning setting by applying it to the suite of Atari 2600 games provided by the Arcade Learning Environment (Bellemare et al., 2013). 1 Notation We consider an agent interacting with a Markov Decision Process (X ,A, \u03b3, P, r). X is a finite state space, A the action space, \u03b3 \u2208 [0, 1) the discount factor, P the transition function mapping stateaction pairs (x, a) \u2208 X \u00d7A to distributions over X , and r : X \u00d7A \u2192 [\u2212RMAX, RMAX] is the reward function. For notational simplicity we will consider a finite action space, but the case of infinite \u2013 possibly continuous \u2013 action space can be handled by the Retrace(\u03bb) algorithm as well. A policy \u03c0 is a mapping from X to a distribution over A. A Q-function Q maps each state-action pair (x, a) to a value in R; in particular, the reward r is a Q-function. For a policy \u03c0 we define the operator P: (PQ)(x, a) := \u2211", "creator": "TeX"}}}