{"id": "1606.02785", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "Neural Network-Based Abstract Generation for Opinions and Arguments", "abstract": "We study the problem of generating abstractive summaries for opinionated text. We propose an attention-based neural network model that is able to absorb information from multiple text units to construct informative, concise, and fluent summaries. An importance-based sampling method is designed to allow the encoder to integrate information from an important subset of input. Automatic evaluation indicates that our system outperforms state-of-the-art abstractive and extractive summarization systems on two newly collected datasets of movie reviews and arguments. Our system summaries are also rated as more informative and grammatical in human evaluation.", "histories": [["v1", "Thu, 9 Jun 2016 00:15:23 GMT  (162kb,D)", "http://arxiv.org/abs/1606.02785v1", "NAACL 2016"]], "COMMENTS": "NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lu wang", "wang ling"], "accepted": true, "id": "1606.02785"}, "pdf": {"name": "1606.02785.pdf", "metadata": {"source": "CRF", "title": "Neural Network-Based Abstract Generation for Opinions and Arguments", "authors": ["Lu Wang", "Wang Ling"], "emails": ["luwang@ccs.neu.edu", "lingwang@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to survive themselves without there being a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, and in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process. \""}, {"heading": "2 Data Collection", "text": "We have compiled two sets of data for film reviews and arguments on controversial topics with gold standard abstractions.1 Rotten Tomatoes (www. rottentomatoes.com) is a film review site that summarizes both professional critics and user-generated reviews (henceforth RottenTomatoes). For each film, an editor constructs a one-sentence critic consensus to summarize the opinions of professional critics. We have compiled 246,164 critics and their consensus of opinion for 3,731 films (i.e. an average of about 66 reviews per movie). We select 2,458 films for training, 536 films for validation, and 737 films for testing. The consensus of opinion is treated as a gold standard summary. We also collect an argumentation dataset from idebate.org (henceforth Idebate), which is a Wikipedia-style website where pros and cons arguments on controversial topics are collected. The arguments can be downloaded from any debate (or dictionary)."}, {"heading": "3 The Neural Network-Based Abstract Generation Model", "text": "In this section, we first define our problem in Section 3.1, followed by a model description. In general, we use a long-term short-term memory network to generate summaries (Section 3.2) from a latent representation calculated by an attention-based encoder (Section 3.3), which is designed to search for relevant information from inputs to better inform the abstract generation process. We also discuss a meaning-based scanning method to allow the encoder to integrate information from an important subset of inputs (Section 3.4 and 3.5), and post-processing (Section 3.6) to rearrange the generations and select the best as the final summary."}, {"heading": "3.1 Problem Formulation", "text": "In summary, the goal is to generate a summary y consisting of the sequence of the words y1,..., | y |. Unlike previous neural encoder decoder approaches, which decode from only one input, our input consists of an arbitrary number of reviews or arguments (henceforth text units where there is no ambiguity) called x = {x1,..., xM}. Each text unit xk consists of a sequence of words xk1,..., x k | xk |. Each word takes the form of a representation vector initialized randomly or by pre-trained embeddings (Mikolov et al., 2013) and is updated during the training. The summary task is defined as finding y, which is the most likely sequence of words y... 1,..., y \u00b2 N, in this way: y = argmaxy logP (y | x) (1), with the probability that we will reproduce the text (logty) in the given sections of P."}, {"heading": "3.2 Decoder", "text": "Similar to the previous paper (Sutskever et al., 2014b; Bahdanau et al., 2014), we split logP (y | x) into a sequence of word-level predictions: logP (y \u2212 x) = \u2211 j = 1,..., | logP (yj | y1,..., yj \u2212 1, x) (2), where each word yj is predicted on the previously generated y1,..., yj \u2212 1 and input x. Probability is estimated by the default word softmax: p (yj | y1,..., yj \u2212 1, x) = softmax (hj) = softmax (hj) hj is the recurring Neural Networks (RNNs) state variable at timestamp j, which is modeled as: hj = g (yj \u2212 1, hj \u2212 1, s)."}, {"heading": "3.3 Encoder", "text": "The representation of the input text units s is calculated on the basis of an attention model (Bahdanau et al., 2014). For a single text unit x1,..., x | x | and the previous state hj, the model s generates a weighted sum: \u2211 i = 1,..., | x | aibi (6), where ai is the attention coefficient obtained for the word xi, and bi is the context-dependent representation of xi. In our work, we construct bi by putting a bidirectional LSTM over the entire input sequence x1,..., x | and then combining the forward and backward states. Formally, we use the LSTM formulation from Equation 5 to generate the forward states hf1,..., h f | x | by setting uj = xj (the projection word xj using a word search table). Similarly, the backward state hb | x,... combination b is inserted using 1 reverse order Lv = 1."}, {"heading": "3.4 Attention Over Multiple Inputs", "text": "An important distinction between our model and existing sequence-to-sequence models (Sutskever et al., 2014b; Bahdanau et al., 2014) is that our input consists of several separate text units. If we input N units of text, i.e. {xk1,..., xk | xk |} N k = 1, a simple extension would be to link them to a sequence as z = x11,..., x 1 | xx1 |, SEG, x 2 1,..., x 2 |, SEG, x N 1,..., x N | xN | |, where SEG is a special mark that limits input. However, there are two problems with this approach. First, the model is sensitive to the order of text units. In addition, z can contain thousands of words, which will become a bottleneck for our model with a training time of O (N | z |), since the attention coefficients for all input words must be calculated to generate each output."}, {"heading": "3.5 Importance Estimation", "text": "In general, we start with a Q regression model and add a regulator to force the separation of summary text units from others. In view of a cluster of text units {x1,..., xM} and their summary y, we calculate the number of overlapping content words between each text unit and summary y as the gold standard valuation. The ratings are uniformly normalized to [0, 1]. Each text unit xk is represented as a d-dimensional feature vector rk, called lk. Therefore, text units in the training data are designated with a feature matrix R and a mark vector L. We aim at learning f (xk) = rk \u00b7 w by minimizing the feature units."}, {"heading": "3.6 Post-processing", "text": "For the test phase, we reclassify the n-best summaries according to their cosinal similarity to the input text units. The one with the greatest similarity will be included in the final summary. Future work will examine the use of more sophisticated re-ranking methods (Charniak and Johnson, 2005; Konstas and Lapata, 2012)."}, {"heading": "4 Experimental Setup", "text": "Data preprocessing. We process the data sets with Stanford CoreNLP (Manning et al., 2014) for tokenization and extraction of POS tags and dependency relationships. For RottenTomatoes data set, we replace movie titles in training with a generic label and replace them with the movie name if there is a generic label generated in the test. Preschooled embeddings and features. The size of the word representation is set at 300, for both input and output words. These can be randomly initialized or with preschooled embeddings learned from Google messages (Mikolov et al., 2013). We extend our model with additional features described in Table 2. Discrete features, such as POS tags, are mapped in word representations using reference tables. For continuous features (e.g. TF-IDF values), they are appended to word vectors as additional values, such as stop and hyper parameter."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Importance Estimation Evaluation", "text": "First, we evaluate the component of appreciation described in Section 3.5. We compare it with the Support Vector Regression (SVR) (Smola and Vapnik, 1997) and two baselines: (1) a length baseline that classifies text units based on their length, and (2) a centric baseline that classifies text units according to their centrality, which is calculated as a cosinal similarity between a text unit and the center of the combined cluster (Erkan and Radev, 2004). We evaluate text units based on the mean reciprocal rank (MRR) and a normalized discounted cumulative increase in the upper 3 and 5 returned results (NDCG @ 3). Text units are considered relevant if they have at least one overlapping content word with the gold standard summary."}, {"heading": "5.2 Automatic Summary Evaluation", "text": "We are considering three common metrics for automatic evaluation."}, {"heading": "5.3 Human Evaluation on Summary Quality", "text": "For human evaluation, we consider three aspects: informativeness, which indicates how much significant information is contained in the summary; grammar, which measures whether a summary is grammatical; and compactness, which indicates whether a summary contains unnecessary information; each aspect is rated on a scale from 1 to 5 (5 is the best). Judges are 2We do not run this model on the basis of Idebate, because it relies on high redundancy to detect repetitive expressions that are not observed on Idebate.We were also asked to rank all summary variations according to their overall quality.We randomly evaluated 40 films from the RottenTomatoes test set, each of which was rated by 5 different human judges. We hired 10 competent English speakers for the evaluation; three system summaries (LexRank, Opinosis, and our system); and humanly written summaries of summaries were presented as summaries."}, {"heading": "5.4 Sampling Effect", "text": "We will further investigate whether the use of inputs taken from distributions estimated by importance produces models with better performance than those obtained by fixed inputs or uniformly sampled inputs. Let's remember that we evaluate K text units based on their importance (importance-based sampling).In this context, we will consider two other constellations: One is the uniform sampling of K text units from input (uniform sampling), another is the selection of K text units with the highest values (top K).We will try different K values. The results in Figure 4 show that import-based sampling can produce comparable BLEU values with Top-K methods, while both exceed uniform sampling."}, {"heading": "5.5 Further Discussion", "text": "Finally, we discuss some other observations and potential improvements. First, applying the re-ranking component after the model has generated n-best abstracts leads to better performance. Preliminary experiments show that selecting the top-1 gene-3We achieve similar results on the Idebate datasetations, rather than re-evaluating them with simple heuristics. This suggests that the current models ignore some task-specific problems, such as informativeness. Post-processing is necessary to make better use of the candidates for the summary. For example, future work may study other complex re-ranking algorithms (Charniak and Johnson, 2005; Konstas and Lapata, 2012). In addition, we also consider the difficult cases where our summaries are evaluated to have a lower informativeness. They are often much shorter than the gold standard human abstracts, limiting the information coverage. In other cases, some generations contain incorrect information about domain-specific facts, such as a director being used as a summary unit in a film, etc."}, {"heading": "6 Related Work", "text": "Our work belongs to the field of summarizing opinions. The construction of fluent natural language summaries has mainly focused on product reviews (Hu and Liu, 2004; Lerman et al., 2009), answers to community questions (Wang et al., 2014), and editorials (Paul et al., 2010). Extractive summary techniques are used to identify summary sentences. As far as we know, we are also the first to investigate the creation of arguments. Recently, there has been a growing interest in generalizing abstract summaries for news articles (Bing et al., 2015), spoken meetings (Wang and Cardie, 2013), and product reviews (Ganesan et al concept. al., 2014; Di Fabbrizio et al al al al al al al. 2014), summaries of human articles (Bing et al., 2015), spoken meetings to construct natural language summaries."}, {"heading": "7 Conclusion", "text": "In this paper, we presented a neural approach to create abstract summaries for idiosyncratic text. We used an attention-based method that finds important information from different input text units to create an informative and concise summary. To cope with the large number of input texts, we used an important scanning mechanism for model training. Experiments showed that our system achieved state-of-the-art results through both automatic evaluation and human evaluation."}], "references": [{"title": "A simple domain-independent probabilistic approach to generation", "author": ["Angeli et al.2010] Gabor Angeli", "Percy Liang", "Dan Klein"], "venue": null, "citeRegEx": "Angeli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Angeli et al\\.", "year": 2010}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Abstractive multi-document summarization via phrase selection and merging", "author": ["Bing et al.2015] Lidong Bing", "Piji Li", "Yi Liao", "Wai Lam", "Weiwei Guo", "Rebecca Passonneau"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Bing et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bing et al\\.", "year": 2015}, {"title": "Coarse-to-fine n-best parsing and maxent discriminative reranking", "author": ["Charniak", "Johnson2005] Eugene Charniak", "Mark Johnson"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Charniak et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Charniak et al\\.", "year": 2005}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Chiang.,? \\Q2005\\E", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation", "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "A hybrid approach to multi-document summarization of opinions in reviews", "author": ["Amanda J Stent", "Robert Gaizauskas"], "venue": "INLG", "citeRegEx": "Fabbrizio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fabbrizio et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["Erkan", "Radev2004] G\u00fcnes Erkan", "Dragomir R. Radev"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Erkan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Erkan et al\\.", "year": 2004}, {"title": "Sentence compression by deletion with lstms", "author": ["Enrique Alfonseca", "Carlos A. Colmenares", "Lukasz Kaiser", "Oriol Vinyals"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions", "author": ["ChengXiang Zhai", "Jiawei Han"], "venue": "In Proceedings of the 23rd international conference on computational linguistics,", "citeRegEx": "Ganesan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ganesan et al\\.", "year": 2010}, {"title": "Abstractive summarization of product reviews using discourse structure", "author": ["Gerani et al.2014] Shima Gerani", "Yashar Mehdad", "Giuseppe Carenini", "Raymond T. Ng", "Bita Nejat"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural", "citeRegEx": "Gerani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gerani et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": null, "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Mining and summarizing customer reviews", "author": ["Hu", "Liu2004] Minqing Hu", "Bing Liu"], "venue": "In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Optimizing search engines using clickthrough data", "author": ["Thorsten Joachims"], "venue": "In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "Joachims.,? \\Q2002\\E", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Fei-Fei2014] Andrej Karpathy", "Li FeiFei"], "venue": "arXiv preprint arXiv:1412.2306", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Concept-to-text generation via discriminative reranking", "author": ["Konstas", "Lapata2012] Ioannis Konstas", "Mirella Lapata"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Konstas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Konstas et al\\.", "year": 2012}, {"title": "Sentiment summarization: Evaluating and learning user preferences", "author": ["Lerman et al.2009] Kevin Lerman", "Sasha BlairGoldensohn", "Ryan McDonald"], "venue": "In Proceedings of the 12th Conference of the European Chapter of the Association", "citeRegEx": "Lerman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lerman et al\\.", "year": 2009}, {"title": "Structure-aware review mining and summarization", "author": ["Li et al.2010] Fangtao Li", "Chao Han", "Minlie Huang", "Xiaoyan Zhu", "Ying-Ju Xia", "Shu Zhang", "Hao Yu"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Automatic evaluation of summaries using ngram co-occurrence statistics", "author": ["Lin", "Hovy2003] Chin-Yew Lin", "Eduard Hovy"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Summarizing contrastive viewpoints in opinionated text", "author": ["Paul et al.2010] Michael J. Paul", "ChengXiang Zhai", "Roxana Girju"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Paul et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2010}, {"title": "Experiments in single and multidocument summarization using mead", "author": ["Dragomir R. Radev"], "venue": "First Document Understanding Conference", "citeRegEx": "Radev.,? \\Q2001\\E", "shortCiteRegEx": "Radev.", "year": 2001}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Large-margin learning of submodular summarization models", "author": ["Sipos et al.2012] Ruben Sipos", "Pannaga Shivaswamy", "Thorsten Joachims"], "venue": "In Proceedings of the 13th Conference of the European Chapter", "citeRegEx": "Sipos et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sipos et al\\.", "year": 2012}, {"title": "Support vector regression machines. Advances in neural information processing", "author": ["Smola", "Vapnik1997] Alex Smola", "Vladimir Vapnik"], "venue": null, "citeRegEx": "Smola et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Smola et al\\.", "year": 1997}, {"title": "The General Inquirer: A Computer Approach to Content Analysis", "author": ["Dexter C. Dunphy", "Marshall S. Smith", "Daniel M. Ogilvie"], "venue": null, "citeRegEx": "Stone et al\\.,? \\Q1966\\E", "shortCiteRegEx": "Stone et al\\.", "year": 1966}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks. CoRR, abs/1409.3215", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Domain-independent abstract generation for focused meeting summarization", "author": ["Wang", "Cardie2013] Lu Wang", "Claire Cardie"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Query-focused opinion summarization for user-generated content", "author": ["Wang et al.2014] Lu Wang", "Hema Raghavan", "Claire Cardie", "Vittorio Castelli"], "venue": "In Proceedings of COLING", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["Janyce Wiebe", "Paul Hoffmann"], "venue": "In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Pro-", "citeRegEx": "Wilson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Movie review mining and summarization", "author": ["Zhuang et al.2006] Li Zhuang", "Feng Jing", "Xiao-Yan Zhu"], "venue": "In Proceedings of the 15th ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Zhuang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhuang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 19, "context": "methods, where phrases or sentences from the original documents are selected for inclusion in the summary (Hu and Liu, 2004; Lerman et al., 2009).", "startOffset": 106, "endOffset": 145}, {"referenceID": 36, "context": "summarization work (Zhuang et al., 2006; Li et al., 2010).", "startOffset": 19, "endOffset": 57}, {"referenceID": 20, "context": "summarization work (Zhuang et al., 2006; Li et al., 2010).", "startOffset": 19, "endOffset": 57}, {"referenceID": 2, "context": "Existing abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (Bing et al., 2015; Ganesan et al., 2010).", "startOffset": 157, "endOffset": 198}, {"referenceID": 10, "context": "Existing abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (Bing et al., 2015; Ganesan et al., 2010).", "startOffset": 157, "endOffset": 198}, {"referenceID": 11, "context": "For example, Gerani et al. (2014)", "startOffset": 13, "endOffset": 34}, {"referenceID": 1, "context": "An attention-based model (Bahdanau et al., 2014) is deployed to allow the encoder to automatically search for salient information within context.", "startOffset": 25, "endOffset": 48}, {"referenceID": 24, "context": "Automatic evaluation by BLEU (Papineni et al., 2002) indicates that our system outperforms the state-of-the-art extract-based and abstractbased methods on both tasks.", "startOffset": 29, "endOffset": 52}, {"referenceID": 10, "context": "72 by an abstractive opinion summarization system from Ganesan et al. (2010). ROUGE evaluation (Lin and Hovy, 2003) also indicates that our system summaries have reasonable information coverage.", "startOffset": 55, "endOffset": 77}, {"referenceID": 23, "context": "Each word takes the form of a representation vector, which is initialized randomly or by pre-trained embeddings (Mikolov et al., 2013), and updated during training.", "startOffset": 112, "endOffset": 134}, {"referenceID": 1, "context": "Similar as previous work (Sutskever et al., 2014b; Bahdanau et al., 2014), we decompose logP (y|x)", "startOffset": 25, "endOffset": 73}, {"referenceID": 1, "context": "The representation of input text units s is computed using an attention model (Bahdanau et al., 2014).", "startOffset": 78, "endOffset": 101}, {"referenceID": 1, "context": "isting sequence-to-sequence models (Sutskever et al., 2014b; Bahdanau et al., 2014) is that our input consists of multiple separate text units.", "startOffset": 35, "endOffset": 83}, {"referenceID": 15, "context": "Furthermore, pairwise preference constraints have been utilized for learning ranking models (Joachims, 2002).", "startOffset": 92, "endOffset": 108}, {"referenceID": 30, "context": "- num of words - category in General Inquirer - unigram (Stone et al., 1966) - num of POS tags - num of positive/negative/neutral - num of named entities words (General Inquirer, - centroidness (Radev, 2001) MPQA (Wilson et al.", "startOffset": 56, "endOffset": 76}, {"referenceID": 26, "context": ", 1966) - num of POS tags - num of positive/negative/neutral - num of named entities words (General Inquirer, - centroidness (Radev, 2001) MPQA (Wilson et al.", "startOffset": 125, "endOffset": 138}, {"referenceID": 35, "context": ", 1966) - num of POS tags - num of positive/negative/neutral - num of named entities words (General Inquirer, - centroidness (Radev, 2001) MPQA (Wilson et al., 2005)) - avg/max TF-IDF scores", "startOffset": 144, "endOffset": 165}, {"referenceID": 22, "context": "We pre-process the datasets with Stanford CoreNLP (Manning et al., 2014) for tokenization and extracting POS tags and dependency relations.", "startOffset": 50, "endOffset": 72}, {"referenceID": 23, "context": "from Google news (Mikolov et al., 2013).", "startOffset": 17, "endOffset": 39}, {"referenceID": 7, "context": "Training is performed via Adagrad (Duchi et al., 2011).", "startOffset": 34, "endOffset": 54}, {"referenceID": 24, "context": "We use BLEU (up to 4-grams) (Papineni et al., 2002) as evaluation met-", "startOffset": 28, "endOffset": 51}, {"referenceID": 4, "context": "generation systems (Chiang, 2005; Angeli et al., 2010; Karpathy and Fei-Fei, 2014).", "startOffset": 19, "endOffset": 82}, {"referenceID": 0, "context": "generation systems (Chiang, 2005; Angeli et al., 2010; Karpathy and Fei-Fei, 2014).", "startOffset": 19, "endOffset": 82}, {"referenceID": 10, "context": "For comparisons, we first compare with an abstractive summarization method presented in Ganesan et al. (2010) on the RottenTomatoes dataset.", "startOffset": 88, "endOffset": 110}, {"referenceID": 28, "context": "PageRank algorithm; (2) Sipos et al. (2012) propose a supervised SUBMODULAR summarization model which is trained with Support Vector Machines.", "startOffset": 24, "endOffset": 44}, {"referenceID": 19, "context": "Constructing fluent natural language opinion summaries has mainly considered product reviews (Hu and Liu, 2004; Lerman et al., 2009), community question answering (Wang et al.", "startOffset": 93, "endOffset": 132}, {"referenceID": 34, "context": ", 2009), community question answering (Wang et al., 2014), and", "startOffset": 38, "endOffset": 57}, {"referenceID": 25, "context": "editorials (Paul et al., 2010).", "startOffset": 11, "endOffset": 30}, {"referenceID": 25, "context": "editorials (Paul et al., 2010). Extractive summarization approaches are employed to identify summaryworthy sentences. For example, Hu and Liu (2004) first identify the frequent product features and then attach extracted opinion sentences to the corre-", "startOffset": 12, "endOffset": 149}, {"referenceID": 2, "context": "Recently, there has been a growing interest in generating abstractive summaries for news articles (Bing et al., 2015), spoken meetings (Wang and Cardie, 2013), and product reviews (Ganesan et al.", "startOffset": 98, "endOffset": 117}, {"referenceID": 2, "context": "Most approaches are based on phrase extraction, from which an algorithm concatenates them into sentences (Bing et al., 2015; Ganesan et al., 2010).", "startOffset": 105, "endOffset": 146}, {"referenceID": 10, "context": "Most approaches are based on phrase extraction, from which an algorithm concatenates them into sentences (Bing et al., 2015; Ganesan et al., 2010).", "startOffset": 105, "endOffset": 146}, {"referenceID": 2, "context": "Most approaches are based on phrase extraction, from which an algorithm concatenates them into sentences (Bing et al., 2015; Ganesan et al., 2010). Nevertheless, the output summaries are not guaranteed to be grammatical. Gerani et al. (2014)", "startOffset": 106, "endOffset": 242}, {"referenceID": 9, "context": "sequence-to-sequence paradigm, RNNs-based models have been investigated for compression (Filippova et al., 2015) and summarization (Filippova et al.", "startOffset": 88, "endOffset": 112}, {"referenceID": 9, "context": ", 2015) and summarization (Filippova et al., 2015; Rush et al., 2015; Hermann et al., 2015) at sentence-level.", "startOffset": 26, "endOffset": 91}, {"referenceID": 27, "context": ", 2015) and summarization (Filippova et al., 2015; Rush et al., 2015; Hermann et al., 2015) at sentence-level.", "startOffset": 26, "endOffset": 91}, {"referenceID": 12, "context": ", 2015) and summarization (Filippova et al., 2015; Rush et al., 2015; Hermann et al., 2015) at sentence-level.", "startOffset": 26, "endOffset": 91}, {"referenceID": 1, "context": "Built on the attention-based translation model in Bahdanau et al. (2014), Rush et al.", "startOffset": 50, "endOffset": 73}], "year": 2016, "abstractText": "We study the problem of generating abstractive summaries for opinionated text. We propose an attention-based neural network model that is able to absorb information from multiple text units to construct informative, concise, and fluent summaries. An importance-based sampling method is designed to allow the encoder to integrate information from an important subset of input. Automatic evaluation indicates that our system outperforms state-ofthe-art abstractive and extractive summarization systems on two newly collected datasets of movie reviews and arguments. Our system summaries are also rated as more informative and grammatical in human evaluation.", "creator": "LaTeX with hyperref package"}}}