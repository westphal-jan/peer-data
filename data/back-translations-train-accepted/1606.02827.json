{"id": "1606.02827", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "Variational Information Maximization for Feature Selection", "abstract": "Feature selection is one of the most fundamental problems in machine learning. An extensive body of work on information-theoretic feature selection exists which is based on maximizing mutual information between subsets of features and class labels. Practical methods are forced to rely on approximations due to the difficulty of estimating mutual information. We demonstrate that approximations made by existing methods are based on unrealistic assumptions. We formulate a more flexible and general class of assumptions based on variational distributions and use them to tractably generate lower bounds for mutual information. These bounds define a novel information-theoretic framework for feature selection, which we prove to be optimal under tree graphical models with proper choice of variational distributions. Our experiments demonstrate that the proposed method strongly outperforms existing information-theoretic feature selection approaches.", "histories": [["v1", "Thu, 9 Jun 2016 05:19:23 GMT  (542kb,D)", "http://arxiv.org/abs/1606.02827v1", "15 pages, 9 figures"]], "COMMENTS": "15 pages, 9 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["shuyang gao", "greg ver steeg", "aram galstyan"], "accepted": true, "id": "1606.02827"}, "pdf": {"name": "1606.02827.pdf", "metadata": {"source": "CRF", "title": "Variational Information Maximization for Feature Selection", "authors": ["Shuyang Gao", "Greg Ver Steeg", "Aram Galstyan"], "emails": ["gaos@usc.edu,", "gregv@isi.edu,", "galstyan@isi.edu"], "sections": [{"heading": "1 Introduction", "text": "In this case, it is often advantageous to select a smaller group of characteristics in order to speed up the calculation, or simply to improve the interpretability of the results. As a rule, the selection of characteristics depends on the use of the characteristics."}, {"heading": "2 Information-Theoretic Feature Selection Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Mutual Information-Based Feature Selection", "text": "Consider a supervised learning scenario in which x = {x1, x2,..., q} is a D-dimensional input characteristic vector, and y is the output label. In filtering methods, the task of selecting information-based characteristics is to select T characteristics in such a way that the mutual information between xS and y is maximized. Formally, the S-xi \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 selection method is a greedy method in which characteristics are selected step by step, one attribute at a time. Let St \u2212 1 = {xf1, xf2,..., xft \u2212 1) maximize the selected attribute selection (H \u2212 xi \u2212 \u2212 xi). Many MI \u2212 based attribute selection methods use a greedy method in which characteristics are selected step by step."}, {"heading": "2.2 Limitations of Previous MI-Based Feature Selection Methods", "text": "Most of them are unable to abide by the rules they have imposed on themselves. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...)"}, {"heading": "3 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Variational Mutual Information Lower Bound", "text": "Let p (x, y) be the common distribution of input (x) and output (y) variables. Barber & Agkov [13] derived the following lower limit for mutual information I (x: y) by using the nonnegativity of the KL divergence, i.e., x (x | y) log p (x | y) q (x | y) \u2265 0: I (x: y) \u2265 H (x) + < ln q (x | y) > p (x, y) (6), where brackets represent averages and q (x | y) is an arbitrary variation distribution. This limit becomes exact when q (x | y) \u2261 p (x | y).It is worth noting that in the context of uncontrolled display learning p (y | x) and q (x | y) can be considered as encoders or decoders."}, {"heading": "3.2 Variational Information Maximization for Feature Selection", "text": "Of course, in terms of information theory feature selection, we could also try to optimize the variable lower limit in Eq.6 by selecting a subset of features in x, so that the H (xS) term in Eq.7 is still insoluble if xS is very high dimensional. Nevertheless, noting that the variable y is the class designation that is normally discrete, and that henceforth H (y) is fixed and traceable, we symmetrically change x and y in Eq.6 and rewrite the lower limit as follows: I (x: y) \u2265 H (y) + < ln q (y | x) > p (x, y) = ln (q (y | y).p (8) Equality in equality is achieved in equality (g)."}, {"heading": "3.2.1 Choice of Variational Distribution", "text": "We need to select q (y | xS) to be as general as possible, while continuing to use the term < ln q (y) > p (xS, y) p (y) p (y) p (y) p (y) p (y) p (y) p (y) p (xS) p (y) p (y) p (y) p (y) p (y) p (10) p (y) p (y) p (y) p (y) p (y) p (y) p (y) p (y) p (10) p) p (xS) asq (y). We can verify that Eq (xS) p (y) p (n) p (y) p (y) p (c) p (p) p (c) p (c) p (p) p (n) p (n) x) p (c)."}, {"heading": "3.2.2 Estimating Lower Bound From Data", "text": "Assuming either naive Bayes Q distribution or pairs of Q distributions, it is convenient to estimate q (xS | y) and q (xS) in Equation 12 using plug-in probability estimates for discrete data or a one- / two-dimensional density estimate for continuous data. We also use the sample mean to approximate the expectation term in Equation 12. Our final estimate for ILB (xS: y) is as follows: I-LB (xS: y) = 1N value x (k), y (k) ln q value (x (k) S | y (k)) q value (x (k) S) (17), where {x (k), y (k)} are samples from data and q (\u00b7) denotes the estimate for q (\u00b7)."}, {"heading": "3.2.3 Variational Forward Feature Selection Under Auto-Regressive Decomposition", "text": "This year, \"he said,\" we've never lost that much time, \"he said.\" We're not there yet, \"he said.\" We're not there yet. We're not there yet, \"he said.\" We're not there yet as far as we imagined. \""}, {"heading": "4 Experiments", "text": "We start with the experiments on a synthetic model according to the tree structure shown in the left part of Fig. 3. The detailed process of data generation is illustrated in supplementary section D. The root node Y is a binary variable, while other variables are continuous. We use VMInaive to optimize the lower limit of the ILB (x: y). 5000 samples are used to generate the synthetic data, and the varying Q distributions are estimated by a grain density estimator. We can see from the diagram in the right part of Fig. 3 that our algorithm, VMInaive, selects x1, x2, x3 as the first three characteristics, although x2 and x3 are only weakly correlated with y. If we continue to add deeper characteristics {x4,..., x9}, the lower limit decreases. For comparison, we also illustrate the reciprocal information between the individual characteristics xi and x3. We can select them as relevant in table 2. We can see from the top of table x2 and x1 that they are x1."}, {"heading": "4.1 Real-World Data", "text": "We compare our VMInaive and VMIpairwise algorithms with other popular information theory selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12]. We use 17 known data sets in previous feature selection studies [5, 12] (all data discredited); the data sets are presented in supplementary sec. C. We use the average error rate for cross-validation in the range of 10 to 100 features to compare different algorithms in the same environment as [12]. 10-fold cross-validation is used for data sets with the number of samples N \u2265 100 and the deviation from the number of cross-validations otherwise used."}, {"heading": "5 Related Work", "text": "Most of these methods are based on combinations of so-called relevant, redundant, and complementary information. Such combinations, which represent approximations of low-order mutual information, are derived from two assumptions, and it has proved unrealistic to expect both assumptions to be true. Inspired by group tests [21], more scalable methods of attribute selection have been developed, but this method also requires the calculation of high-dimensional mutual information as a basic scoring function. Estimating mutual information from data requires a large number of observations - especially when dimensionality is high. The proposed lower limit of variation can be considered a method of estimating mutual information between a high-dimensional continuous variable and a discrete variable."}, {"heading": "6 Conclusion", "text": "Although there is a large number of information theory methods, they are rather limited and are based on mutually conflicting assumptions about the underlying data distribution. We introduced a unifying variation of mutual information that is lower to address these problems. We demonstrated that automatic regressive decomposition can be used to predict the selection of characteristics by gradually maximizing the lower limit. In addition, we presented two concrete methods using Naive Bayes and pairwise Q distributions that significantly exceed existing methods. VMInaive assumes only a Naive Bayes model, but even this simple model exceeds existing information theory methods and demonstrates the effectiveness of our variable information maximization framework. We hope that our framework will inspire new mathematically rigorous algorithms for information theory feature selection, such as the powerful variation optimization for more global approaches."}, {"heading": "A Detailed Algorithm for Variational Forward Feature Selection", "text": "We describe the detailed algorithm for our approach. We also provide the open source code that implements VMInaive (Q = Q = Q = 1 Q = 1 Q = 1 Q (Q = 1 Q = 1 Q). Specifically, we assume that the class name y is discrete and L has different values {y1, y2,..., yL}; then we define the distribution q (xSt | y) vector Q (k) of size L (k) for each sample (x), y \u2212 n (k)) at step t: Q (k) t = [q) St (x (k) St (x (k) St (x) St (x) St (x) Y (x), q (k), q (k), and y (k) at size L (x)] T (x), where x (k) St (k) is projected."}, {"heading": "B Optimality under Tree Graphical Models", "text": "We must then take a step T > 0 so that the following three conditions for the use of VMInaive or VMIpairwise: Condition I: Condition II: Condition II: Condition II: Condition II: Condition II: Condition III: Condition III: Condition III: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition III: Condition III: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II: Condition II"}, {"heading": "C Datasets and Results", "text": "(6.7) 6.7 (6.7) 6.6 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.6 (6.7) 6.6 (6.7) 6.6 (6.7) 6.7 (6.7) 6.6 (6.7) 6.7 (6.7) 6.7 (6.7) 6.6 (6.7) 6.6 (6.7) 6.6 (6.7) 6.6 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7 (6.7) 6.7) 6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7 (6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7 (6.7) 6.7) 6.7 (6.7) 6.7"}, {"heading": "D Generating Synthetic Data", "text": "Here is a detailed generation process for synthetic tree model data in experiment.Drawing y \u0445 Bernoulli (0.5) Drawing x1 \u0445 Gauss (\u03c3 = 1.0, \u00b5 = y) Drawing x2 \u0445 Gauss (\u03c3 = 1.0, \u00b5 = y / 1.5) Drawing x3 \u0445 Gauss (\u03c3 = 1.0, \u00b5 = y / 2.25) Drawing x4 \u0445 Gauss (\u03c3 = 1.0, \u00b5 = x1) Drawing x5 \u0445 Gauss (\u03c3 = 1.0, \u00b5 = x1) Drawing x6 \u0445 Gauss (\u03c3 = 1.0, \u00b5 = x2) Drawing x7 \u0445 Gauss (\u03c3 = 1.0, \u00b5 = x2) Drawing x8 \u0445 Gauss (\u03c3 = 1.0, \u00b5 = x3) Drawing x9 \u0445 Gauss (\u03c3 = 1.0, \u00b5 = x3) Drawing x9 \u0445 Gauss (\u03c3 = 1.0, \u00b5 = x3)"}], "references": [{"title": "Feature selection for classification", "author": ["Manoranjan Dash", "Huan Liu"], "venue": "Intelligent data analysis,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Feature selection for knowledge discovery and data mining, volume 454", "author": ["Huan Liu", "Hiroshi Motoda"], "venue": "Springer Science & Business Media,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Wrappers for feature subset selection", "author": ["Ron Kohavi", "George H John"], "venue": "Artificial intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Conditional likelihood maximisation: a unifying framework for information theoretic feature selection", "author": ["Gavin Brown", "Adam Pocock", "Ming-Jie Zhao", "Mikel Luj\u00e1n"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["Roberto Battiti"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "Data visualization and feature selection: New algorithms for nongaussian data", "author": ["Howard Hua Yang", "John E Moody"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Fast binary feature selection with conditional mutual information", "author": ["Fran\u00e7ois Fleuret"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Quadratic programming feature selection", "author": ["Irene Rodriguez-Lujan", "Ramon Huerta", "Charles Elkan", "Carlos Santa Cruz"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Effective global approaches for mutual information based feature selection", "author": ["Xuan Vinh Nguyen", "Jeffrey Chan", "Simone Romano", "James Bailey"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "The im algorithm: a variational approach to information maximization", "author": ["David Barber", "Felix Agakov"], "venue": "In Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection", "author": ["Abhimanyu Das", "David Kempe"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Feature selection and feature extraction for text categorization", "author": ["David D Lewis"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Conditional infomax learning: an integrated framework for feature extraction and fusion", "author": ["Dahua Lin", "Xiaoou Tang"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Variational information maximisation for intrinsically motivated reinforcement learning", "author": ["Shakir Mohamed", "Danilo Jimenez Rezende"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "On the feature selection criterion based on an approximation of multidimensional mutual information", "author": ["Kiran S Balagani", "Vir V Phoha"], "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Can high-order dependencies improve mutual information based feature selection", "author": ["Nguyen Xuan Vinh", "Shuo Zhou", "Jeffrey Chan", "James Bailey"], "venue": "Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Conditional mutual information-based feature selection analyzing for synergy and redundancy", "author": ["Hongrong Cheng", "Zhiguang Qin", "Chaosheng Feng", "Yong Wang", "Fagen Li"], "venue": "ETRI Journal,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Parallel feature selection inspired by group testing", "author": ["Yingbo Zhou", "Utkarsh Porwal", "Ce Zhang", "Hung Q Ngo", "Long Nguyen", "Christopher R\u00e9", "Venu Govindaraju"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Mutual information between discrete and continuous data sets", "author": ["Brian C Ross"], "venue": "PloS one,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization", "author": ["XuanLong Nguyen", "Martin J Wainwright", "Michael I Jordan"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Minimum redundancy feature selection from microarray gene expression data", "author": ["Chris Ding", "Hanchuan Peng"], "venue": "Journal of bioinformatics and computational biology,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Feature selection is one of the fundamental problems in machine learning research [1, 2].", "startOffset": 82, "endOffset": 88}, {"referenceID": 1, "context": "Feature selection is one of the fundamental problems in machine learning research [1, 2].", "startOffset": 82, "endOffset": 88}, {"referenceID": 2, "context": "Feature selection approaches are usually categorized into three groups: wrapper, embedded and filter [3, 4, 5].", "startOffset": 101, "endOffset": 110}, {"referenceID": 3, "context": "Feature selection approaches are usually categorized into three groups: wrapper, embedded and filter [3, 4, 5].", "startOffset": 101, "endOffset": 110}, {"referenceID": 4, "context": "Feature selection approaches are usually categorized into three groups: wrapper, embedded and filter [3, 4, 5].", "startOffset": 101, "endOffset": 110}, {"referenceID": 5, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 170, "endOffset": 191}, {"referenceID": 7, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 170, "endOffset": 191}, {"referenceID": 8, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 170, "endOffset": 191}, {"referenceID": 9, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 170, "endOffset": 191}, {"referenceID": 10, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 170, "endOffset": 191}, {"referenceID": 11, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 170, "endOffset": 191}, {"referenceID": 4, "context": "Since mutual information (MI) is a general measure of dependence with several unique properties [6], many MI-based scoring functions have been proposed as filter methods [7, 8, 9, 10, 11, 12]; see [5] for an exhaustive list.", "startOffset": 197, "endOffset": 200}, {"referenceID": 12, "context": "To address the above shortcomings, in this paper we introduce a novel feature selection method based on variational lower bound on mutual information; a similar bound was previously studied within the Infomax learning framework [13].", "startOffset": 228, "endOffset": 232}, {"referenceID": 5, "context": "where I(\u00b7) denotes the mutual information [6].", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "As shown in [5], the mutual information term in Eq.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "where H(\u00b7) denotes the entropy [6].", "startOffset": 31, "endOffset": 34}, {"referenceID": 13, "context": "The greedy learning algorithm has been analyzed in [14].", "startOffset": 51, "endOffset": 55}, {"referenceID": 4, "context": "A general family of methods rely on the following approximations [5]:", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "5 become exact under the following two assumptions [5]:", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "As we mentioned above, most existing methods implicitly or explicitly adopt both assumptions or their stronger versions as shown in [5], including mutual information maximization (MIM) [15], joint mutual information (JMI) [8], conditional mutual information maximization (CMIM) [9], maximum relevance minimum redundancy (mRMR) [10], conditional infomax feature extraction (CIFE) [16], etc.", "startOffset": 132, "endOffset": 135}, {"referenceID": 14, "context": "As we mentioned above, most existing methods implicitly or explicitly adopt both assumptions or their stronger versions as shown in [5], including mutual information maximization (MIM) [15], joint mutual information (JMI) [8], conditional mutual information maximization (CMIM) [9], maximum relevance minimum redundancy (mRMR) [10], conditional infomax feature extraction (CIFE) [16], etc.", "startOffset": 185, "endOffset": 189}, {"referenceID": 7, "context": "As we mentioned above, most existing methods implicitly or explicitly adopt both assumptions or their stronger versions as shown in [5], including mutual information maximization (MIM) [15], joint mutual information (JMI) [8], conditional mutual information maximization (CMIM) [9], maximum relevance minimum redundancy (mRMR) [10], conditional infomax feature extraction (CIFE) [16], etc.", "startOffset": 222, "endOffset": 225}, {"referenceID": 8, "context": "As we mentioned above, most existing methods implicitly or explicitly adopt both assumptions or their stronger versions as shown in [5], including mutual information maximization (MIM) [15], joint mutual information (JMI) [8], conditional mutual information maximization (CMIM) [9], maximum relevance minimum redundancy (mRMR) [10], conditional infomax feature extraction (CIFE) [16], etc.", "startOffset": 278, "endOffset": 281}, {"referenceID": 9, "context": "As we mentioned above, most existing methods implicitly or explicitly adopt both assumptions or their stronger versions as shown in [5], including mutual information maximization (MIM) [15], joint mutual information (JMI) [8], conditional mutual information maximization (CMIM) [9], maximum relevance minimum redundancy (mRMR) [10], conditional infomax feature extraction (CIFE) [16], etc.", "startOffset": 327, "endOffset": 331}, {"referenceID": 15, "context": "As we mentioned above, most existing methods implicitly or explicitly adopt both assumptions or their stronger versions as shown in [5], including mutual information maximization (MIM) [15], joint mutual information (JMI) [8], conditional mutual information maximization (CMIM) [9], maximum relevance minimum redundancy (mRMR) [10], conditional infomax feature extraction (CIFE) [16], etc.", "startOffset": 379, "endOffset": 383}, {"referenceID": 10, "context": "Approaches based on global optimization of mutual information, such as quadratic programming feature selection (QPFS) [11] and state-of-the-art conditional mutual informationbased spectral method (SPECCMI) [12], are derived from the previous greedy methods and therefore also implicitly rely on those two assumptions.", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "Approaches based on global optimization of mutual information, such as quadratic programming feature selection (QPFS) [11] and state-of-the-art conditional mutual informationbased spectral method (SPECCMI) [12], are derived from the previous greedy methods and therefore also implicitly rely on those two assumptions.", "startOffset": 206, "endOffset": 210}, {"referenceID": 12, "context": "Barber & Agkov [13] derived the following lower bound for mutual information I(x : y) by using the non-negativity of KL-divergence, i.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "6 by iteratively adjusting the parameters of the encoder and decoder, such as [13, 17].", "startOffset": 78, "endOffset": 86}, {"referenceID": 16, "context": "6 by iteratively adjusting the parameters of the encoder and decoder, such as [13, 17].", "startOffset": 78, "endOffset": 86}, {"referenceID": 9, "context": "Note that the most-cited MI-based feature selection method mRMR [10] also assumes conditional independence given the class label y as shown in [5, 18, 19], but they make additional stronger independence assumptions among only feature variables.", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "Note that the most-cited MI-based feature selection method mRMR [10] also assumes conditional independence given the class label y as shown in [5, 18, 19], but they make additional stronger independence assumptions among only feature variables.", "startOffset": 143, "endOffset": 154}, {"referenceID": 17, "context": "Note that the most-cited MI-based feature selection method mRMR [10] also assumes conditional independence given the class label y as shown in [5, 18, 19], but they make additional stronger independence assumptions among only feature variables.", "startOffset": 143, "endOffset": 154}, {"referenceID": 18, "context": "Note that the most-cited MI-based feature selection method mRMR [10] also assumes conditional independence given the class label y as shown in [5, 18, 19], but they make additional stronger independence assumptions among only feature variables.", "startOffset": 143, "endOffset": 154}, {"referenceID": 9, "context": "As shown in Table 1, our methods VMInaive and VMIpairwise have the same time complexity as mRMR [10], while state-of-the-art global optimization method SPECCMI [12] is required to precompute the pairwise mutual information matrix, which gives an time complexity of O(ND).", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "As shown in Table 1, our methods VMInaive and VMIpairwise have the same time complexity as mRMR [10], while state-of-the-art global optimization method SPECCMI [12] is required to precompute the pairwise mutual information matrix, which gives an time complexity of O(ND).", "startOffset": 160, "endOffset": 164}, {"referenceID": 14, "context": "We can see from Table 2 that it would choose x1, x4 and x5 as the top three features by using the maximum relevance criteria [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "We compare our algorithms VMInaive and VMIpairwise with other popular information-theoretic feature selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12].", "startOffset": 134, "endOffset": 138}, {"referenceID": 7, "context": "We compare our algorithms VMInaive and VMIpairwise with other popular information-theoretic feature selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12].", "startOffset": 144, "endOffset": 147}, {"referenceID": 14, "context": "We compare our algorithms VMInaive and VMIpairwise with other popular information-theoretic feature selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12].", "startOffset": 153, "endOffset": 157}, {"referenceID": 8, "context": "We compare our algorithms VMInaive and VMIpairwise with other popular information-theoretic feature selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12].", "startOffset": 164, "endOffset": 167}, {"referenceID": 15, "context": "We compare our algorithms VMInaive and VMIpairwise with other popular information-theoretic feature selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12].", "startOffset": 174, "endOffset": 178}, {"referenceID": 11, "context": "We compare our algorithms VMInaive and VMIpairwise with other popular information-theoretic feature selection methods, including mRMR [10], JMI [8], MIM [15], CMIM [9], CIFE [16], and SPECCMI [12].", "startOffset": 192, "endOffset": 196}, {"referenceID": 4, "context": "We use 17 well-known datasets in previous feature selection studies [5, 12] (all data are discretized).", "startOffset": 68, "endOffset": 75}, {"referenceID": 11, "context": "We use 17 well-known datasets in previous feature selection studies [5, 12] (all data are discretized).", "startOffset": 68, "endOffset": 75}, {"referenceID": 11, "context": "We use the average cross-validation error rate on the range of 10 to 100 features to compare different algorithms under the same setting as [12].", "startOffset": 140, "endOffset": 144}, {"referenceID": 4, "context": "The 3-Nearest-Neighbor classifier is used for Gisette and Madelon, following [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 10, "context": "While for the remaining datasets, the classifier is chosen to be Linear SVM, following [11, 12].", "startOffset": 87, "endOffset": 95}, {"referenceID": 11, "context": "While for the remaining datasets, the classifier is chosen to be Linear SVM, following [11, 12].", "startOffset": 87, "endOffset": 95}, {"referenceID": 4, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 6, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 7, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 8, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 9, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 14, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 10, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 11, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 19, "context": "There has been a significant amount of work on information-theoretic feature selection in the past twenty years: [5, 7, 8, 9, 10, 15, 11, 12, 20], to name a few.", "startOffset": 113, "endOffset": 145}, {"referenceID": 20, "context": "Inspired by group testing [21], more scalable feature selection methods have been developed, but this method also requires the calculation of high-dimensional mutual information as a basic scoring function.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "Only a few examples exist in literature [22] under this setting.", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": "We hope our method will shed light on new ways to estimate mutual information, similar to estimating divergences in [23].", "startOffset": 116, "endOffset": 120}], "year": 2016, "abstractText": "Feature selection is one of the most fundamental problems in machine learning. An extensive body of work on information-theoretic feature selection exists which is based on maximizing mutual information between subsets of features and class labels. Practical methods are forced to rely on approximations due to the difficulty of estimating mutual information. We demonstrate that approximations made by existing methods are based on unrealistic assumptions. We formulate a more flexible and general class of assumptions based on variational distributions and use them to tractably generate lower bounds for mutual information. These bounds define a novel information-theoretic framework for feature selection, which we prove to be optimal under tree graphical models with proper choice of variational distributions. Our experiments demonstrate that the proposed method strongly outperforms existing information-theoretic feature selection approaches.", "creator": "LaTeX with hyperref package"}}}