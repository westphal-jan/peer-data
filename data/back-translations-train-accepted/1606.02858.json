{"id": "1606.02858", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "abstract": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding current state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.", "histories": [["v1", "Thu, 9 Jun 2016 08:19:16 GMT  (93kb,D)", "http://arxiv.org/abs/1606.02858v1", "ACL 2016"], ["v2", "Mon, 8 Aug 2016 21:21:19 GMT  (247kb,D)", "http://arxiv.org/abs/1606.02858v2", "ACL 2016, updated results"]], "COMMENTS": "ACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["danqi chen", "jason bolton", "christopher d manning"], "accepted": true, "id": "1606.02858"}, "pdf": {"name": "1606.02858.pdf", "metadata": {"source": "CRF", "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "authors": ["Danqi Chen", "Christopher D. Manning"], "emails": ["danqi@cs.stanford.edu", "jebolton@cs.stanford.edu", "manning@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to fight, to move, to move, to fight, to move, to fight, to move, to move, to move, to move, to fight, to fight, to fight, to move, to move, to fight, to move, to move, to move, to fight, to move, to fight, to move, to move, to fight, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2 The Reading Comprehension Task", "text": "The RC datasets introduced in (Hermann et al., 2015) probably stem from articles on the CNN and Daily Mail news websites that use articles and their summaries of keywords.3 Figure 1 demonstrates 3The datasets are available at http feat.: / / github.com / deepmind / rc-data.an Example 4: It consists of a passage p, a question q and an answer a where the passage is a news article, the question is a cloze-style task in which one of the item's points has been replaced by a placeholder, and the answer is this questioned entity. The goal is to find the missing entity (answer a) from all possible entities that appear in the passage. A news article is usually associated with a few (e.g. 3-5) spherical points, and each of them highlights an aspect of its contents."}, {"heading": "3 Our Systems", "text": "In this section, we describe two systems that we have implemented - a conventional entity-centric classifier and an end-to-end neural parser. While Hermann et al. (2015) provide several baselines for the performance of the RC task, we suspect that their baselines are not as strong. They are trying to use a frame semantic parser, and we believe that the poor coverage of this parser undermines the results and is not representative of what a simple NLP system - based on standard approaches to answering factoid question and relation extraction developed over the last 15 years - can achieve. In fact, their frame semantic model is clearly inferior to another baseline they provide, a heuristic word-distance model. Currently, only two papers are available that present results to this RC task that present both neural network approaches: (Hermann et al., 2015 and (Hill et al.), the latter is packed in the end-to-to-end language."}, {"heading": "3.1 Entity-Centric Classifier", "text": "We first build a conventional trait-based classifier aimed at identifying which features are effective for this task, similar in spirit to the one (Wang et al., 2015) currently performing very competitively on the MCTest RC dataset (Richardson et al., 2013).The set-up of this system consists in designing a trait vector fp, q (e) for each candidate unit e and learning a weight vector in such a way that the correct answer a is likely to rank higher than any other candidate unit: Phenomenfp, q (a) > \u03b8 fp, q (e), q (e), and q (e), as well as the following feature templates: 1. Whether entity e in the passage.2. Whether entity e in the question.3. Whether entity e in the question.3. The frequency of entity e in the passage.4. The first position of the occurrence of the text of the 5th entity matches exactly with the older passage.3."}, {"heading": "3.2 End-to-end Neural Network", "text": "We have to ask ourselves what we can do when we go looking for something in the next three steps (see Figure 2). (See Figure 2) We have to ask ourselves how we can behave in the next three steps. (See Figure 2) We have to ask ourselves what we are doing. (See Figure 3) We have to ask ourselves what we are doing. (See Figure 5) We have to ask ourselves how we are behaving in the next three steps. (See Figure 5) We have to ask ourselves what we are doing. (See Figure 5) We have to ask ourselves what we are doing. (See Figure 5). (See Figure 5). (See Figure 5). (See Figure 5). (See Figure 5). (See 5). (See Figure 5). (See 5). (See 5). (See 5). (See Figure 5.) (See 5.) (See 5. (See 5.). (See 5.) (See 5.) (See 5.). (See 5.). (See 5.). (See 5.) (See 5. (Figure 5.). (See 5.). (Figure 5.). (See 5.)"}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Training Details", "text": "For the training of our conventional classifier, we use the implementation of LambdaMART (Wu et al., 2010) in the RankLib package. [5] We use this ranking algorithm because, of course, our problem is a ranking problem, and forests with increased decision trees have been very successful recently (as in many recent Kaggle contests). We do not use all the characteristics of LambdaMART, as we only lose 1 / 0 on the first proposal, instead of using an IR-like metric to achieve ranking results. We use Stanford's neural dependency parser (Chen and Manning, 2014) to analyze all of our document and question texts, and all other characteristics can be extracted without additional tools. For the training of our neural networks, we keep only the most common words (including entity and placeholder markers), and all other words that are calculated to a minimum."}, {"heading": "4.2 Main Results", "text": "Table 2 presents our most important results. The conventional function-based classifier achieves an accuracy of 67.9% on the CNN test set, which not only exceeds one of the symbolic approaches reported in (Hermann et al., 2015), but also exceeds all neural network systems from their work and the best single system result to date (Hill et al., 2016), suggesting that the task may not be as difficult as assumed, and a simple functionality may cover many of the cases. Table 3 presents a feature ablation analysis of our entity-centric classifier on the development part of the CNN data set, showing that n-gram match and frequency of entities are the two most important performance classes. More drastically, our single neural network exceeds previous results by a large margin (over 5%), bringing the state-of-the-the-art dataset to 72.4% and 75.8%, respectively, of the overall performance classes."}, {"heading": "5 Data Analysis", "text": "In this section, we will do an in-depth analysis and answer the following questions: (i) Since the dataset was created automatically and heuristically, how many of the questions are trivial to answer, and how many are loud and unanswerable? (ii) What have these models learned? What prospects are there for further improving them? To investigate this, we randomly selected 100 examples from the Dev part of the CNN dataset for analysis (see Appendix A for more details)."}, {"heading": "5.1 Breakdown of the Examples", "text": "iSe rf\u00fc ide eeisrrteeeeSrtee\u00fccnlhsrtee\u00fccnlrllhsrteeee\u00fccnlrfhsrteeee\u00fccnlrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrlrrrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrrrrrrrrrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrl"}, {"heading": "5.2 Per-category Performance", "text": "Now we are analyzing the predictions of our two systems on the basis of the above categorization. As shown in Table 6, we have the following observations: (i) The exact similarities are quite simple and both systems get 100% correct. (ii) In cases of ambiguity / hard and entity-linking errors that meet our expectations, both systems perform poorly. (iii) The two systems differ mainly in cases of paraphrasing and some cases of \"partial reference.\" This clearly shows how neural networks are better able to learn semantic similarities where the two sentences are paraphrased or lexically varied. (iv) We believe that in all cases the neural network system already performs roughly optimally with one sentence and unambiguous cases. There does not seem to be much useful scope to explore more complex approaches to understanding natural language in this dataset."}, {"heading": "6 Related Tasks", "text": "We briefly review other tasks related to reading comprehension. MCTest (Richardson et al., 2013) is an open-ended reading comprehension task in the form of fictitious short stories accompanied by multiple choice questions. It has been carefully crowd sourced and aims at a 7-year reading comprehension level. On the other hand, this data set has a high demand for various argumentation capabilities: over 50% of the questions require multiple sentences to be answered, and the questions also come in different categories (what, why, whose, what, etc.). On the other hand, the complete data set has only 660 paragraphs in total (each paragraph is linked to 4 questions), making the formation of statistical models (particularly complex) very difficult. The best solutions (Sachan et al., 2015; Wang et al., 2015) are still heavily reliant on manually curated syntactic / setic features."}, {"heading": "7 Conclusion", "text": "In this paper, we carefully examined the recent CNN / Daily Mail reading comprehension task. Our systems showed state-of-the-art results, but more importantly, we carefully analyzed the data set by hand. Overall, we think that the CNN / Daily Mail data sets are valuable data sets that offer a promising way to build effective statistical models for reading comprehension tasks. Nevertheless, we argue that: (i) this data set is still quite noisy due to its method of data generation and correlation errors; (ii) current neural networks have almost reached the performance limit for this data set; and (iii) the level of reasoning and reasoning required for this dataset is still quite simple. As for future work, we need to consider how to use these data sets (and the models trained on them) to solve more complex RC reasoning problems (with less commented data)."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their thoughtful feedback. Stanford University thanks the Defense Advanced Research Projects Agency (DARPA) for its support of the Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract number FA8750-13-2-0040. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, AFRL or the U.S. government."}], "references": [{"title": "Modeling biological processes for reading comprehension", "author": ["Jonathan Berant", "Vivek Srikumar", "Pei-Chun Chen", "Abby Vander Linden", "Brittany Harding", "Brad Huang", "Peter Clark", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing", "citeRegEx": "Berant et al\\.,? 2014", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Towards the machine comprehension of text: An essay", "author": ["Christopher J.C. Burges."], "venue": "Technical report, Microsoft Research Technical Report MSR-TR-2013125.", "citeRegEx": "Burges.,? 2013", "shortCiteRegEx": "Burges.", "year": 2013}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 1684\u20131692.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Dynamic entity representation with max-pooling improves machine reading", "author": ["Sosuke Kobayashi", "Ran Tian", "Naoaki Okazaki", "Kentaro Inui."], "venue": "North American Association for Computational Linguistics (NAACL).", "citeRegEx": "Kobayashi et al\\.,? 2016", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2016}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher."], "venue": "International Conference on", "citeRegEx": "Kumar et al\\.,? 2016", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Reasoning in vector space: An exploratory study", "author": ["Moontae Lee", "Xiaodong He", "Wen-tau Yih", "Jianfeng Gao", "Li Deng", "Paul Smolensky"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A Unified Theory of Inference for Text Understanding", "author": ["Peter Norvig."], "venue": "Ph.D. thesis, University of California, Berkeley.", "citeRegEx": "Norvig.,? 1978", "shortCiteRegEx": "Norvig.", "year": 1978}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 193\u2013203.", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Learning answerentailing structures for machine comprehension", "author": ["Mrinmaya Sachan", "Kumar Dubey", "Eric Xing", "Matthew Richardson."], "venue": "Association for Computational Linguistics and International Joint Conference on Natural Language", "citeRegEx": "Sachan et al\\.,? 2015", "shortCiteRegEx": "Sachan et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "arthur szlam", "Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Machine comprehension with syntax, frames, and semantics", "author": ["Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester."], "venue": "Association for Computational Linguistics and International Joint Conference on Natural Language Processing", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Weston et al\\.,? 2016", "shortCiteRegEx": "Weston et al\\.", "year": 2016}, {"title": "Adapting boosting for information retrieval measures", "author": ["Qiang Wu", "Christopher J. Burges", "Krysta M. Svore", "Jianfeng Gao."], "venue": "Information Retrieval, pages 254\u2013270.", "citeRegEx": "Wu et al\\.,? 2010", "shortCiteRegEx": "Wu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 3, "context": "Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": ", (Norvig, 1978)).", "startOffset": 2, "endOffset": 16}, {"referenceID": 1, "context": "Human reading comprehension is often tested by asking questions that require interpretive understanding of a passage, and the same approach has been suggested for testing computers (Burges, 2013).", "startOffset": 181, "endOffset": 195}, {"referenceID": 12, "context": "In recent years, there have been several strands of work which attempt to collect human-labeled data for this task \u2013 in the form of document, question and answer triples \u2013 and to learn machine learning models directly from it (Richardson et al., 2013; Berant et al., 2014; Wang et al., 2015).", "startOffset": 226, "endOffset": 291}, {"referenceID": 0, "context": "In recent years, there have been several strands of work which attempt to collect human-labeled data for this task \u2013 in the form of document, question and answer triples \u2013 and to learn machine learning models directly from it (Richardson et al., 2013; Berant et al., 2014; Wang et al., 2015).", "startOffset": 226, "endOffset": 291}, {"referenceID": 15, "context": "In recent years, there have been several strands of work which attempt to collect human-labeled data for this task \u2013 in the form of document, question and answer triples \u2013 and to learn machine learning models directly from it (Richardson et al., 2013; Berant et al., 2014; Wang et al., 2015).", "startOffset": 226, "endOffset": 291}, {"referenceID": 3, "context": "Recently, researchers at DeepMind (Hermann et al., 2015) had the appealing, original idea of exploiting the fact that the abundant news articles of CNN and Daily Mail are accompanied by bullet point summaries in order to heuristically create large-scale supervised training data for the reading comprehension task.", "startOffset": 34, "endOffset": 56}, {"referenceID": 3, "context": "The RC datasets introduced in (Hermann et al., 2015) are made from articles on the news websites CNN and Daily Mail, utilizing articles and their bullet point summaries.", "startOffset": 30, "endOffset": 52}, {"referenceID": 3, "context": "Hermann et al. (2015) argue convincingly that such a strategy is necessary to ensure that systems approach this task by understanding the passage in front of them, rather than by using world knowledge or a language model to answer questions without needing to understand the passage.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "At present just two papers are available presenting results on this RC task, both presenting neural network approaches: (Hermann et al., 2015) and (Hill et al.", "startOffset": 120, "endOffset": 142}, {"referenceID": 4, "context": ", 2015) and (Hill et al., 2016).", "startOffset": 12, "endOffset": 31}, {"referenceID": 3, "context": "While Hermann et al. (2015) do provide several baselines for performance on the RC task, we suspect that their baselines are not that strong.", "startOffset": 6, "endOffset": 28}, {"referenceID": 15, "context": "This is similar in spirit to (Wang et al., 2015), which at present has very competitive performance on the MCTest RC dataset (Richardson et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 12, "context": ", 2015), which at present has very competitive performance on the MCTest RC dataset (Richardson et al., 2013).", "startOffset": 84, "endOffset": 109}, {"referenceID": 3, "context": "Our neural network system is based on the AttentiveReader model proposed by (Hermann et al., 2015).", "startOffset": 76, "endOffset": 98}, {"referenceID": 3, "context": "Differences from (Hermann et al., 2015).", "startOffset": 17, "endOffset": 39}, {"referenceID": 9, "context": "The effectiveness of the simple bilinear attention function has been shown previously for neural machine translation by (Luong et al., 2015).", "startOffset": 120, "endOffset": 140}, {"referenceID": 3, "context": "In contrast, the original model in (Hermann et al., 2015) combined o and the question embedding q via another non-linear layer before making final predictions.", "startOffset": 35, "endOffset": 57}, {"referenceID": 4, "context": "Window-based MemN2Ns (Hill et al., 2016).", "startOffset": 21, "endOffset": 40}, {"referenceID": 4, "context": "Another recent neural network approach proposed by (Hill et al., 2016) is based on a memory network architecture (Weston et al.", "startOffset": 51, "endOffset": 70}, {"referenceID": 16, "context": ", 2016) is based on a memory network architecture (Weston et al., 2015).", "startOffset": 50, "endOffset": 71}, {"referenceID": 18, "context": "For training our conventional classifier, we use the implementation of LambdaMART (Wu et al., 2010) in the RankLib package.", "startOffset": 82, "endOffset": 99}, {"referenceID": 2, "context": "We use Stanford\u2019s neural network dependency parser (Chen and Manning, 2014) to parse all our document and question text, and all other features can be extracted without additional tools.", "startOffset": 51, "endOffset": 75}, {"referenceID": 11, "context": "We choose word embedding size d = 100, and use the 100-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014) for initialization.", "startOffset": 101, "endOffset": 126}, {"referenceID": 3, "context": "Not only does this significantly outperform any of the symbolic approaches reported in (Hermann et al., 2015), it also outperforms all the neural network systems from their paper and the best single-system result reported so far from (Hill et al.", "startOffset": 87, "endOffset": 109}, {"referenceID": 4, "context": ", 2015), it also outperforms all the neural network systems from their paper and the best single-system result reported so far from (Hill et al., 2016).", "startOffset": 132, "endOffset": 151}, {"referenceID": 3, "context": "Results marked \u2020 are from (Hermann et al., 2015) and results marked \u2021 are from (Hill et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 4, "context": ", 2015) and results marked \u2021 are from (Hill et al., 2016).", "startOffset": 38, "endOffset": 57}, {"referenceID": 4, "context": "Due to resource constraints, we have not had a chance to investigate ensembles of models, which generally can bring further gains, as demonstrated in (Hill et al., 2016) and many other papers.", "startOffset": 150, "endOffset": 169}, {"referenceID": 5, "context": "Concurrently with our paper, Kadlec et al. (2016) and Kobayashi et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 5, "context": "Concurrently with our paper, Kadlec et al. (2016) and Kobayashi et al. (2016) also experiment on these two datasets and report competitive results.", "startOffset": 29, "endOffset": 78}, {"referenceID": 3, "context": "Additionally, only 2 examples require multiple sentences for inference \u2013 this is a lower rate than we expected and Hermann et al. (2015) suggest.", "startOffset": 115, "endOffset": 137}, {"referenceID": 12, "context": "MCTest (Richardson et al., 2013) is an opendomain reading comprehension task, in the form of fictional short stories, accompanied by multiplechoice questions.", "startOffset": 7, "endOffset": 32}, {"referenceID": 13, "context": "Up to now, the best solutions (Sachan et al., 2015; Wang et al., 2015) are still heavily relying on manually curated syntactic/semantic features, with the aid of additional knowledge (e.", "startOffset": 30, "endOffset": 70}, {"referenceID": 15, "context": "Up to now, the best solutions (Sachan et al., 2015; Wang et al., 2015) are still heavily relying on manually curated syntactic/semantic features, with the aid of additional knowledge (e.", "startOffset": 30, "endOffset": 70}, {"referenceID": 4, "context": "Children Book Test (Hill et al., 2016) was developed in a similar spirit to the CNN/Daily Mail datasets.", "startOffset": 19, "endOffset": 38}, {"referenceID": 4, "context": "According to the first study on this dataset (Hill et al., 2016), a language model (an n-gram model or a recurrent neural network) with local context is sufficient for predicting verbs or prepositions; however, for named entities or common nouns, it improves performance to scan through the whole paragraph to make predictions.", "startOffset": 45, "endOffset": 64}, {"referenceID": 17, "context": "bAbI (Weston et al., 2016) is a collection of artificial datasets, consisting of 20 different reasoning types.", "startOffset": 5, "endOffset": 26}, {"referenceID": 14, "context": "\u201d Various types of memory networks (Sukhbaatar et al., 2015; Kumar et al., 2016) have been shown effective on these tasks, and Lee et al.", "startOffset": 35, "endOffset": 80}, {"referenceID": 7, "context": "\u201d Various types of memory networks (Sukhbaatar et al., 2015; Kumar et al., 2016) have been shown effective on these tasks, and Lee et al.", "startOffset": 35, "endOffset": 80}, {"referenceID": 4, "context": "Children Book Test (Hill et al., 2016) was developed in a similar spirit to the CNN/Daily Mail datasets. It takes any consecutive 21 sentences from a children\u2019s book \u2013 the first 20 sentences are used as the passage, and the goal is to infer a missing word in the 21st sentence (question and answer). The questions are also categorized by the type of the missing word: named entity, common noun, preposition or verb. According to the first study on this dataset (Hill et al., 2016), a language model (an n-gram model or a recurrent neural network) with local context is sufficient for predicting verbs or prepositions; however, for named entities or common nouns, it improves performance to scan through the whole paragraph to make predictions. So far, the best published results are reported by window-based memory networks. bAbI (Weston et al., 2016) is a collection of artificial datasets, consisting of 20 different reasoning types. It encourages the development of models with the ability to chain reasoning, induction/ deduction, etc., so that they can answer a question like \u201cThe football is in the playground\u201d after reading a sequence of sentences \u201cJohn is in the playground; Bob is in the office; John picked up the football; Bob went to the kitchen.\u201d Various types of memory networks (Sukhbaatar et al., 2015; Kumar et al., 2016) have been shown effective on these tasks, and Lee et al. (2016) show that vector space models based on extensive problem analysis can obtain near-perfect accuracies on all the categories.", "startOffset": 20, "endOffset": 1403}], "year": 2016, "abstractText": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding current state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.1", "creator": "LaTeX with hyperref package"}}}