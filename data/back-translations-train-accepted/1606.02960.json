{"id": "1606.02960", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "Sequence-to-Sequence Learning as Beam-Search Optimization", "abstract": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.", "histories": [["v1", "Thu, 9 Jun 2016 13:29:34 GMT  (54kb,D)", "http://arxiv.org/abs/1606.02960v1", null], ["v2", "Thu, 10 Nov 2016 03:45:30 GMT  (223kb,D)", "http://arxiv.org/abs/1606.02960v2", "EMNLP 2016 camera-ready"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE stat.ML", "authors": ["sam wiseman", "alexander m rush"], "accepted": true, "id": "1606.02960"}, "pdf": {"name": "1606.02960.pdf", "metadata": {"source": "CRF", "title": "Sequence-to-Sequence Learning as Beam-Search Optimization", "authors": ["Sam Wiseman"], "emails": ["swiseman@seas.harvard.edu", "srush@seas.harvard.edu"], "sections": [{"heading": "1 Introduction", "text": "Sequence-to-sequence learning using deep neural networks (in this, seq2seq) However, beyond requiring impressive results for machine translation (Sutskever et al., 2014), about the same model and training has also proved useful for sentence compression (Filippova et al., 2015), and dialog systems (Sutskever et al., 2016), among other things tasksksksksksksksksks.The dominant approach to the formation of a seq2seq system is as a conditional language model, with the training maximizing the likelihood of each successive word on the input sequence and gold history of target words. Thus, the training uses a strictly cross-word loss."}, {"heading": "2 Related Work", "text": "The issues of exposure bias and label bias have received a lot of attention from authors in the structured prediction community, and we have briefly reviewed some of this work here. A prominent approach to combating exposure bias is that of SEARN (Daume) III et al., 2009), a meta-training algorithm that requires a search method in the form of a cost-sensitive classifier generated from examples of an interpolation of one's own policies and the current (learned) policies of the model. Therefore, SEARN explicitly aims at the discrepancy between oracular education and non-oracular (often greedy) test time inference by training on the outcomes of the model itself. DAgger et al., 2011) is a similar approach that differs from how training examples are generated and aggregated, and there have been additional imported training to this style of education in recent years (Chang et al al al al)."}, {"heading": "3 Background and Notation", "text": "In the simplest seq2seq scenery of a target sequence V, in which especially the term sequences are generated, we have the opportunity to explore and explore the term sequences. (The term sequence is) The term sequence of the individual terms (and non-sequential) of the individual term sequences of the different source domains has proven effective. (In this work we are agnostic in the form of the coding model, and simply take an abstract source representation x. As soon as the input sequence is encoded, we generate a target sequence with a decoder.) The decoder is with a target sequence of words from a target sequence."}, {"heading": "4 Optimizing for Beam Search", "text": "We start with a small change to the seq2seq modeling frame. Instead of regressing about the probability of the next word, we learn instead to generate (non-probabilistic) values for precedence sequences. Define the value of a sequence that examines the current hidden state of the relevant RNN at a time t followed by a single word w asscore (w1: t, w), f (w, ht, x), (1) where f is a parameterized function that examines the current hidden state of the relevant RNN at a time t as well as entering the representation x. In experiments, our f will have an identical shape to g, but without the final Softmax transformation, which converts unnormed values into probabilities. This scoring change allows the model to avoid problems related to the label bias problem. More importantly, we also modify how this model is trained. Ideally, we would train by comparing the gold sequence with the highscore sequence."}, {"heading": "4.1 Search-Based Loss", "text": "& & # 222; K & # 222; K & # 222; K & # 222; K & # 222; K & # 222; K & # 222; K & # 222; K & # 222; K & # 822; K & # 822; K & # 822; K & # 222; K & # 222; K & # 822; K & # 222; K & # 222; K & # 222; K & # 222; K & # 222; K # 222; 822; K & # 222; 822; 822; K # 222; 822; K # 222; K & # 222; 822; K # 222; 822; K # 222; 822; K # 222; K # 222; K # 222; K # 222; K # 222; K & # 222; K # 222; K & # 22; 822; K & # 22; 822; K # 22; 822; K & # 22; 822; K & # 22; 822; K # 22; 822; K & # 22; 822; K & # 22; 822; K & # 22; 822; 822; K & # 22; 822; 822; 822; K & # 22; 822; 822; 822; 822; 822; K & # 22; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822; 822"}, {"heading": "4.2 Forward: Find Violations", "text": "iDe rf\u00fc ide eeisrcnh-eaeaeaJnlhsrdcnlhSrc\u00fceaeSrlh-eaSrcnlhsrteaeaeaeaeaeSrlhc ni red eeisrcnh-eaeSrcnlhsrteaeSrcnh-eaeSrcnh-eaeSrcnnlhsrteaeSrteee\u00fccnlcnlllrcehnlrrcehnlrcehnlrcehnlrcehnlrf\u00fc eeisde rf\u00fc ide eeisrcnlcehnlrcnlrcehnllrcehnllrcehnlrllllluiiiiiiiiu"}, {"heading": "4.3 Backward: Merge Sequences", "text": "Suppose a margin violation occurs in the time step t between the predicted history y (K) 1: t and the gold history y1: t. As in the standard seq2seq training, we must propagate this error over time; however, unlike seq2seq, we also have a gradient for the incorrectly predicted history. In the worst case, there is an infringement in each time step that could lead to T-independent sequences. Since we need to call BRNO (T) times for each sequence, naively executing this infringement could result in an O (T) reverse, rather than the O (T) time required for the standard seq2seq. Fortunately, our combination of search strategy and loss is able to efficiently divide BRNN operations."}, {"heading": "5 Data and Methods", "text": "We conduct experiments on three different tasks and compare our approach with the seq2seq baseline and other relevant baselines."}, {"heading": "5.1 Model", "text": "While the method we describe applies to seq2seq RNNs in general, for all experiments we use the global attention model of Luong et al. (2015) - which consists of an LSTM (Hochreiter and Schmidhuber, 1997) encoder and an LSTM decoder with a global attention model - both as the base model seq2seq (i.e. as the model that calculates the g in Section 3) and as the model that uses our sequence results for. As in Luong et al. (2015), we also use \"input feeding,\" in which the attention distribution from the previous time step is fed into the current step. As far as we3We also note that since algorithm 1 does not update the parameters until the entire target sequence is sought, our training method differs slightly from LaSO (which is online), and in this aspect is essentially equivalent to the \"Delayed LaSO-Sq 2014 and S1-Iniq-1 procedures.\""}, {"heading": "15: /*BACKWARD*/", "text": "16: 00 00 00: 00 00 00: 00 00: 00 00 00: 00 00 00: 00 00 00: 00 00 00: 00 00 00 00 00: 00 00 00: 00 00 00: 00 00 00 00: 00 00 00 00: 00 00 00: 00 00 00 00 00: 00 00 00: 00 00 00 00 00: 00 00 00 00: 00 00 00 00: 00 00 00 00: 00 00 00 00: 00 00 00 00: 00 00 00 00: 00 00 00: 00 00 00: 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00: 00 00 00 00: 00 00 00 00: 00 00 00 00: 00 00 00: 00 00 00: 00 00 00: 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00: 00 00 00 00 00 00 00: 00 00 00 00 00 00 00 00: 00 00 00 00 00 00 00: 00 00 00 00 00 00 00: 00 00 00 00 00 00 00: 00 00 00 00 00 00 00: 00 00 00 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00 00 00: 00 00 00 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00: 00 00 00 00: 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00 00 00: 00 00 00 00 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00 00: 00 00 00 00 00 00 00 00"}, {"heading": "5.2 Training and Test", "text": "First, we found that the model did not learn if it was from an accidental initial.4 We therefore considered it necessary to train the model with a standard, literal entropy loss, as described in Section 3. The need for pre-training in this case is consistent with the results of other authors who train non-local neural models (Kingsbury, 2009; Sak et al., 2016; Ranzato et al.).5 Likewise, it is clear that the less space the model has to make erroneous predictions without exceeding the margin, the less the beam is used in training."}, {"heading": "5.3 Tasks and Results", "text": "We are primarily there to assess the effectiveness of radiation sickness, which requires us to be able to outdo ourselves, and as such we are conducting experiments with the same model on three very different problems: the arrangement of individuals, the arrangement of individuals, the arrangement of individuals, the arrangement of individuals, the arrangement of individuals, the way they have dealt with each other and the way they have dealt with each other, the way they have dealt with each other, the way they have dealt with each other, the way they have dealt with each other, the way they have dealt with each other, the way they have dealt with each other, the way they have dealt with each other, the way they have dealt with each other."}, {"heading": "6 Conclusion", "text": "We have introduced a variant of seq2seq and an associated beam detection program that addresses both exposure bias and label bias, as well as enabling training with both sequence level cost functions and harsh constraints. We show that this training method improves on a powerful seq2seq baseline for multiple NLP tasks."}, {"heading": "Acknowledgments", "text": "We thank Yoon Kim for providing helpful discussions and the first seq2seq code on which our implementations are based, and thank him for his support with a Google Research Award."}], "references": [{"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Learning structured perceptrons for coreference Resolution with Latent Antecedents and Non-local Features", "author": ["Bj\u00f6rkelund", "Kuhn2014] Anders Bj\u00f6rkelund", "Jonas Kuhn"], "venue": null, "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "Efficient programmable learning to search. In Arxiv", "author": ["Chang et al.2015] Kai-Wei Chang", "Hal Daum\u00e9 III", "John Langford", "Stephane Ross"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "A systematic comparison of smoothing techniques for sentence-level bleu", "author": ["Chen", "Cherry2014] Boxing Chen", "Colin Cherry"], "venue": "ACL", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Collins", "Roark2004] Michael Collins", "Brian Roark"], "venue": "In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Collins et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2004}, {"title": "Learning as search optimization: approximate large margin methods for structured prediction", "author": ["III Daum\u00e9", "III Marcu2005] Hal Daum\u00e9", "Marcu. Daniel"], "venue": "In Proceedings of the Twenty-Second International Conference on Machine Learning (ICML", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2005}, {"title": "Search-based structured prediction", "author": ["John Langford", "Daniel Marcu"], "venue": "Machine Learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Sentence compression by deletion with lstms", "author": ["Enrique Alfonseca", "Carlos A Colmenares", "Lukasz Kaiser", "Oriol Vinyals"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Structured perceptron with inexact search", "author": ["Huang et al.2012] Liang Huang", "Suphan Fayong", "Yang Guo"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["Brian Kingsbury"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "Kingsbury.,? \\Q2009\\E", "shortCiteRegEx": "Kingsbury.", "year": 2009}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando C.N. Pereira"], "venue": "In Proceedings of the Eighteenth International Conference on Machine", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Transition-based syntactic linearization", "author": ["Liu et al.2015] Yijia Liu", "Yue Zhang", "Wanxiang Che", "Bing Qin"], "venue": "In Proceedings of NAACL", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Pham et al.2014] Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In Frontiers in Handwriting Recognition (ICFHR),", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Sequence level training with recurrent neural networks. ICLR", "author": ["Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross et al.2011] St\u00e9phane Ross", "Geoffrey J. Gordon", "Drew Bagnell"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Sequence discriminative distributed training of long short-term memory recurrent neural networks", "author": ["Sak et al.2014] Hasim Sak", "Oriol Vinyals", "Georg Heigold", "Andrew W. Senior", "Erik McDermott", "Rajat Monga", "Mark Z. Mao"], "venue": "INTERSPEECH", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Word ordering without syntax", "author": ["Alexander M Rush", "Stuart M Shieber"], "venue": "arXiv preprint arXiv:1604.08633", "citeRegEx": "Schmaltz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schmaltz et al\\.", "year": 2016}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Pineau."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pages 3776\u20133784.", "citeRegEx": "Pineau.,? 2016", "shortCiteRegEx": "Pineau.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["James Martens", "Geoffrey E Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML),", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Sequence-discriminative training of recurrent neural networks", "author": ["Patrick Doetsch", "Simon Wiesler", "Ralf Schluter", "Hermann Ney"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Voigtlaender et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Voigtlaender et al\\.", "year": 2015}, {"title": "Transition-based neural constituent parsing", "author": ["Watanabe", "Sumita2015] Taro Watanabe", "Eiichiro Sumita"], "venue": "Proceedings of ACL-IJCNLP", "citeRegEx": "Watanabe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watanabe et al\\.", "year": 2015}, {"title": "Incremental recurrent neural network dependency parser with search-based discriminative training", "author": ["Yazdani", "Henderson2015] Majid Yazdani", "James Henderson"], "venue": "In Proceedings of the 19th Conference on Computational Natural Language Learning,", "citeRegEx": "Yazdani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yazdani et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization. CoRR, abs/1409.2329", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Syntax-based grammaticality improvement using ccg and guided search", "author": ["Zhang", "Clark2011] Yue Zhang", "Stephen Clark"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "Discriminative syntax-based word ordering for text generation", "author": ["Zhang", "Clark2015] Yue Zhang", "Stephen Clark"], "venue": "Computational Linguistics,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "A neural probabilistic structured-prediction", "author": ["Zhou et al.2015] Hao Zhou", "Yue Zhang", "Jiajun Chen"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "Sequence-to-Sequence learning with deep neural networks (herein, seq2seq) (Sutskever et al., 2011; Sutskever et al., 2014) has rapidly become a very useful and surprisingly general-purpose tool for natural language processing.", "startOffset": 74, "endOffset": 122}, {"referenceID": 26, "context": "Sequence-to-Sequence learning with deep neural networks (herein, seq2seq) (Sutskever et al., 2011; Sutskever et al., 2014) has rapidly become a very useful and surprisingly general-purpose tool for natural language processing.", "startOffset": 74, "endOffset": 122}, {"referenceID": 26, "context": "In addition to demonstrating impressive results for machine translation (Sutskever et al., 2014), roughly the same model and training has also proven to be useful for sentence compression (Filippova et al.", "startOffset": 72, "endOffset": 96}, {"referenceID": 9, "context": ", 2014), roughly the same model and training has also proven to be useful for sentence compression (Filippova et al., 2015), parsing (Vinyals et al.", "startOffset": 99, "endOffset": 123}, {"referenceID": 27, "context": ", 2015), parsing (Vinyals et al., 2015), and dialogue systems (Serban et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 26, "context": "In practice, generation is accomplished by searching over output sequences greedily or with beam search (Sutskever et al., 2014).", "startOffset": 104, "endOffset": 128}, {"referenceID": 19, "context": "In this context, Ranzato et al. (2016) note", "startOffset": 17, "endOffset": 39}, {"referenceID": 17, "context": "Loss-Evaluation Mismatch: training uses a word-level loss, while at test-time we target improving sequence-level evaluation metrics, such as BLEU (Papineni et al., 2002).", "startOffset": 146, "endOffset": 169}, {"referenceID": 13, "context": "We might additionally add the concern of label bias (Lafferty et al., 2001) to the list, since wordprobabilities at each time-step are locally normalized, guaranteeing that successors of incorrect hisar X iv :1 60 6.", "startOffset": 52, "endOffset": 75}, {"referenceID": 15, "context": "ferent problems: word ordering, syntactic parsing, and machine translation, and compare to a highlytuned seq2seq system with attention (Luong et al., 2015).", "startOffset": 135, "endOffset": 155}, {"referenceID": 20, "context": "DAgger (Ross et al., 2011) is a similar approach, which differs in terms of how training examples are generated and aggregated, and there have additionally been important refinements to this style of training over the past several years (Chang et al.", "startOffset": 7, "endOffset": 26}, {"referenceID": 2, "context": ", 2011) is a similar approach, which differs in terms of how training examples are generated and aggregated, and there have additionally been important refinements to this style of training over the past several years (Chang et al., 2015).", "startOffset": 218, "endOffset": 238}, {"referenceID": 0, "context": "When it comes to training RNNs, SEARN/DAgger has been applied under the name \u201cscheduled sampling\u201d (Bengio et al., 2015), which involves training an RNN to generate the t+ 1\u2019st token in a target sequence after consuming either the true t\u2019th token, or, with probability that increases throughout training, the predicted t\u2019th token.", "startOffset": 98, "endOffset": 119}, {"referenceID": 34, "context": "forward setting by Zhou et al. (2015) and Andor et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 34, "context": "forward setting by Zhou et al. (2015) and Andor et al. (2016). Our work differs in that we adopt a LaSO-like paradigm (with some minor modifications), and apply it to the training of seq2seq RNNs (rather than feed-forward networks).", "startOffset": 19, "endOffset": 62}, {"referenceID": 11, "context": "that Watanabe and Sumita (2015) apply maximumviolation training (Huang et al., 2012), which is similar to early-update, to a parsing model with recurrent components, and that Yazdani and Henderson (2015) use beam-search in training a discriminative, locally normalized dependency parser with recurrent components.", "startOffset": 64, "endOffset": 84}, {"referenceID": 11, "context": "that Watanabe and Sumita (2015) apply maximumviolation training (Huang et al., 2012), which is similar to early-update, to a parsing model with recurrent components, and that Yazdani and Henderson (2015) use beam-search in training a discriminative, locally normalized dependency parser with recurrent components.", "startOffset": 65, "endOffset": 204}, {"referenceID": 19, "context": "Ranzato et al. (2016) follow this approach to train RNN decoders in a seq2seq model, and they obtain consistent improvements in performance, even over models trained with scheduled sampling.", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "Ranzato et al. (2016) follow this approach to train RNN decoders in a seq2seq model, and they obtain consistent improvements in performance, even over models trained with scheduled sampling. As Daum\u00e9 III and Marcu (2005) note, however, techniques such as LaSO are similar to reinforcement learning, except that they do not require blind \u201cexploration\u201d in the same way.", "startOffset": 0, "endOffset": 221}, {"referenceID": 21, "context": "We also note that there are some (non-seq2seq) exceptions to the trend of locally normalized RNNs, such as the work of Sak et al. (2014) and Voigtlaen-", "startOffset": 119, "endOffset": 137}, {"referenceID": 34, "context": "(2016) and Zhou et al. (2015), though in training those models are simply not updated when the gold sequence remains on the beam.", "startOffset": 11, "endOffset": 30}, {"referenceID": 15, "context": "While the method we describe applies to seq2seq RNNs in general, for all experiments we use the global attention model of Luong et al. (2015) \u2014 which consists of an LSTM (Hochreiter and Schmidhuber, 1997) encoder and an LSTM decoder with a global attention model \u2014 as both the baseline seq2seq model (i.", "startOffset": 122, "endOffset": 142}, {"referenceID": 15, "context": "While the method we describe applies to seq2seq RNNs in general, for all experiments we use the global attention model of Luong et al. (2015) \u2014 which consists of an LSTM (Hochreiter and Schmidhuber, 1997) encoder and an LSTM decoder with a global attention model \u2014 as both the baseline seq2seq model (i.e., as the model that computes the g in Section 3) and as the model that computes our sequence-scores f . As in Luong et al. (2015), we also use \u201cinput feeding,\u201d which involves feeding the attention distribution from the previous time-step into the decoder at the current step.", "startOffset": 122, "endOffset": 435}, {"referenceID": 12, "context": "The necessity of pre-training in this instance is consistent with the findings of other authors who train non-local neural models (Kingsbury, 2009; Sak et al., 2014; Andor et al., 2016; Ranzato et al., 2016).", "startOffset": 130, "endOffset": 207}, {"referenceID": 21, "context": "The necessity of pre-training in this instance is consistent with the findings of other authors who train non-local neural models (Kingsbury, 2009; Sak et al., 2014; Andor et al., 2016; Ranzato et al., 2016).", "startOffset": 130, "endOffset": 207}, {"referenceID": 19, "context": "The necessity of pre-training in this instance is consistent with the findings of other authors who train non-local neural models (Kingsbury, 2009; Sak et al., 2014; Andor et al., 2016; Ranzato et al., 2016).", "startOffset": 130, "endOffset": 207}, {"referenceID": 24, "context": "Finally, it has been established that dropout (Srivastava et al., 2014) regularization improves the per-", "startOffset": 46, "endOffset": 71}, {"referenceID": 18, "context": "formance of LSTMs (Pham et al., 2014; Zaremba et al., 2014), and in our experiments we run beam search under dropout.", "startOffset": 18, "endOffset": 59}, {"referenceID": 31, "context": "formance of LSTMs (Pham et al., 2014; Zaremba et al., 2014), and in our experiments we run beam search under dropout.", "startOffset": 18, "endOffset": 59}, {"referenceID": 8, "context": "For all experiments, we trained both seq2seq and BSO models with mini-batch Adagrad (Duchi et al., 2011) (using batches of size 64), and we renormalized all gradients so they did not exceed 5 before taking the gradient step.", "startOffset": 84, "endOffset": 104}, {"referenceID": 14, "context": "For experiments, we use the same PTB dataset (with the standard training, development, and test splits) and evaluation procedure as in Zhang and Clark (2015), Liu et al. (2015), and Schmaltz et al.", "startOffset": 159, "endOffset": 177}, {"referenceID": 14, "context": "For experiments, we use the same PTB dataset (with the standard training, development, and test splits) and evaluation procedure as in Zhang and Clark (2015), Liu et al. (2015), and Schmaltz et al. (2016), with performance reported in terms of BLEU score with the correctly ordered sentences.", "startOffset": 159, "endOffset": 205}, {"referenceID": 16, "context": "We replace singleton words in the training set with an UNK token, normalize digits to a single symbol, and initialize word embeddings for both source and target words from the publicly available word2vec (Mikolov et al., 2013) embeddings.", "startOffset": 204, "endOffset": 226}, {"referenceID": 19, "context": "We evaluate on the machine translation dataset used in Ranzato et al. (2016), which uses data from the German-toEnglish portion of the IWSLT 2014 machine translation evaluation campaign (Cettolo et al.", "startOffset": 55, "endOffset": 77}, {"referenceID": 0, "context": "DAD trains seq2seq with scheduled sampling (Bengio et al., 2015).", "startOffset": 43, "endOffset": 64}, {"referenceID": 18, "context": "Table 4: Machine translation experiments on test set; results below middle line are from MIXER model of Ranzato et al. (2016). SB-\u2206 indicates sentence BLEU costs are used in defining \u2206.", "startOffset": 104, "endOffset": 126}, {"referenceID": 19, "context": "We use the same preprocessing and dataset splits as Ranzato et al. (2016), and like them we also use a single-layer LSTM decoder with 256 units.", "startOffset": 52, "endOffset": 74}, {"referenceID": 19, "context": "We note, however, that while our decoder LSTM is of the same size as that of Ranzato et al. (2016), our results are not directly comparable, because we use an LSTM encoder (rather than a convolutional encoder as they do), a slightly different", "startOffset": 77, "endOffset": 99}, {"referenceID": 19, "context": "In Table 4 we show our final results and those from Ranzato et al. (2016). While we start with an improved baseline, we see similarly large increases in accuracy as those obtained by DAD and MIXER, in particular when Kte > 1.", "startOffset": 52, "endOffset": 74}], "year": 2016, "abstractText": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an important generalpurpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and training scheme, based on the work of Daum\u00e9 III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highlyoptimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.", "creator": "LaTeX with hyperref package"}}}