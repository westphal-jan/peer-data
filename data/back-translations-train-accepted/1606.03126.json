{"id": "1606.03126", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "Key-Value Memory Networks for Directly Reading Documents", "abstract": "Directly reading documents and being able to answer questions from them is a key problem. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs suffer from often being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, MovieQA, a QA dataset in the domain of movies. Our method closes the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark.", "histories": [["v1", "Thu, 9 Jun 2016 21:33:55 GMT  (110kb,D)", "http://arxiv.org/abs/1606.03126v1", null], ["v2", "Mon, 10 Oct 2016 20:14:10 GMT  (122kb,D)", "http://arxiv.org/abs/1606.03126v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander h miller", "adam fisch", "jesse dodge", "amir-hossein karimi", "antoine bordes", "jason weston"], "accepted": true, "id": "1606.03126"}, "pdf": {"name": "1606.03126.pdf", "metadata": {"source": "CRF", "title": "Key-Value Memory Networks for Directly Reading Documents", "authors": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston"], "emails": ["ahm@fb.com", "afisch@fb.com", "jessedodge@fb.com", "ahkarimi@fb.com", "abordes@fb.com", "jase@fb.com"], "sections": [{"heading": null, "text": "A key problem is reading documents directly and answering questions from them. To avoid their inherent difficulty, answering questions (QA) has instead been focused on the use of knowledge bases (KBs), which have proven effective. Unfortunately, KBs often suffer from overly restrictive methods because the schema cannot support certain types of answers and is too sparse, e.g. Wikipedia contains much more information than Freebase. In this paper, we present a new method, Key Value Memory Networks, which makes reading documents more viable by using different encodings during the addressing and output phase of the memory reading process. To compare KBs, information extraction or Wikipedia documents directly in a single framework, we construct an analysis tool, MOVIEQA, a QA dataset in the field of movies. Our method closes the gap between all three settings. In addition, it achieves state-of-the-art results on the existing WIQIKA benchmark."}, {"heading": "1 Introduction", "text": "The question we must ask ourselves is whether it is a \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"old,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" new, \"\" new, \"new,\" new, \"new,\" new, \"new,\" \"new,\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"\" new, \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" new, \"\" new, \"new,\" \"new,\" new, \"\" new, \"new,\" \"new,\" \"new,\" new, \"\" new, \"new,\" \"new,\" \"new,\" \"new,\" new, \"\" new, \"\" new, \"new,\" \"\" new, \"\" new, \"new,\" \"new,\" new, \"new,\" \"new,\" \"new,\" new, \"\", \"new,\" new, \"new,\" \"new,\" \"\" \"\" new, \"new,\" new, \"new,\" \"new,\" \",\" \"\" \"new,\" new, \"new,\" \"new,\" \",\", \"\" \"new,\" \"new,\" new, \"new,\" \"new,\" \",\" new, \",\" \"\" \"\" \"\" new, \"new,\" new, \"\" \"\" new, \",\", \"\" \""}, {"heading": "2 Related Work", "text": "Early QA systems were based on information retrieval and were designed to return snippets of text containing an answer (Voorhees and Tice, 2000; Banko et al., 2002), with limitations in terms of question complexity and answer coverage. The creation of large-scale KBs (Auer et al., 2007; Bollacker et al., 2015) has led to the development of a new class of QA methods based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2014; Yih et al., 2015) that can return precise answers to complicated compositional questions. Due to the scarcity of KB data, the main challenge is shifting from searching for answers to efficient information extraction methods in order to populate KBs automatically (Craven et al., 2000; Carlson et al., 2010) - which is not an easy problem."}, {"heading": "3 Key-Value Memory Networks", "text": "The Key Value Memory Network model is based on the Memory Network model (MemNNs) (Weston et al., 2015a; Sukhbaatar et al., 2015), which has proven useful for a variety of document reading and answering questions: for reading children's books and answering questions about them (Hill et al., 2015), for complex thinking about simulated stories (Weston et al., 2015b), and for using KBs to answer questions (Bordes et al., 2015).Key value paired memories are a generalization of the way contexts (e.g. knowledge databases or documents to be read) are stored in memory. The search phase (addressing) is based on the key memory and the reading phase (resulting in the returned result) is the value memory. This gives both (i) the user greater flexibility so that the user's prior knowledge of the task can match the value with the value of the key (ii) while the key may not match the value of the task (ii)."}, {"heading": "3.1 Model Description", "text": "Our model is based on the end-to-end storage network architecture of Sukhbaatar et al. (2015). A high-level view of both models is as follows: You define a memory that may be a very large array of storage locations that can encode both long-term and short-term decisions. At test date, you get a query (e.g. the question in QA tasks) that is used for iterative addressing and read from memory (these iterations are also called \"hops\") that search for relevant information to answer the question. Finally, after gathering information from memory, the original queries and the retrieved memory contexts are combined as characteristics to predict the final answer. Figure 1 illustrates the KV-MemNN model of architecture. In KV-MemNNNs, we define the storage slots as pairs of vectors (k1, v1). (kM, vM) and the denote question."}, {"heading": "3.2 Key-Value Memories", "text": "There are a variety of ways to generate key values that can have important effects on overall performance. The ability to encode previous knowledge in this way is an important component in our experiments, and we are free to define \u03a6X, \u03a6S, and \u03a6V for the query, answer, key, and values. We will now describe several possible variants of \u03a6K and \u03a6V that we have tried in our experiments for the simplicity we have been able to function as a marsupial. Knowledge entries have a structure of three \"subject relationships\" (see Table 1 for examples). The representation we are looking at is simple: the key is fixed by the left side entity and the relationship, and the value is the right side entity (object)."}, {"heading": "4 The MovieQA Benchmark", "text": "The MOVIEQA benchmark consists of film question pairs. It was developed with the following objectives in mind: (i) machine learning techniques should have sufficient training examples for learning; and (ii) it is easy to analyze the performance of different knowledge representations and break down the results by question type. It is available for download at http: / / fb.ai / babi."}, {"heading": "4.1 Knowledge Representations", "text": "In fact, it is as if most of us are able to keep to the rules that they have imposed on ourselves. (...) It is not as if they are able to understand the rules. (...) It is not as if they keep to the rules. (...) It is not as if they keep to the rules. (...) It is as if they keep to the rules. (...) It is not as if they keep to the rules. (...) It is as if they keep to the rules. \"(...) It is as if they keep to the rules.\" (...) It is as if they keep to the rules. \"(...) It is as if they keep to the rules.\" (...) It is as if they keep to the rules. \"(...) It is as if they keep to the rules."}, {"heading": "4.2 Question-Answer Pairs", "text": "Within the more than 100,000 question pairs in the dataset, we distinguish 13 question classes that correspond to different types of edges in our KB, ranging from specific (actor to movie: \"In which movies did Harrison Ford star?,\" movie to actor: \"Who starred in Blade Runner?\") to more general (day to movie: \"Which movies can be described by dystopias?\"); see Table 4 for the full list. For each question type, there are a number of possible answers. Using SimpleQuestions (Bordes et al., 2015), an existing open domain question based on Freebase, we identified the subset of questions asked by human commentators covering our question types. We expanded this set to cover all of our KB by replacing the entities in these questions to apply them to other questions, such as, \"In which movies did Harrison Ford star?\""}, {"heading": "5 Experiments", "text": "This section describes our experiments on MOVIEQA and WIKIQA."}, {"heading": "5.1 MovieQA", "text": "In fact, it is in such a way that we see ourselves in a position to put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in the same world, in which we are in another world, in which we put ourselves in the same world, in which we, in which we, in which we, in which we, in which we in which we, in which we are in the same world, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we,"}, {"heading": "5.2 WikiQA", "text": "WIKIQA (Yang et al., 2015) is an existing data set for answering the question set selection using Wikipedia as a source of knowledge. The task is to select the sentence that comes from a Wikipedia document that best answers the question, measuring performance with average average precision (MAP) and average reciprocal ranking (MRR) of the order of answers. The data set uses a pre-built information retrieval step and therefore provides a fixed set of candidate sets per question, so systems do not need to consider all of Wikipedia. Unlike MOVIEQA, the educational size is small (\u0445 1000 examples), while the topic is much broader than Wikipedia, rather than just movies, and the questions can only be answered by reading the documents, so no comparison can be made to the use of KBs."}, {"heading": "6 Conclusion", "text": "We investigated the problem of direct reading of documents to answer questions, and focused our analysis on the gap between such direct methods and the use of human commented or automatically constructed KBs. We presented a new model, Key-Value Memory Networks, which helps bridge this gap by outperforming several other methods in two sets of data, MOVIEQA and WIKIQA. However, there is still a gap in performance. MOVIEQA serves as an analysis tool to shed some light on the causes. Future work should try to close this gap even further. Key-Value Memory Networks are a versatile tool for reading and answering questions about documents or KBs - it enables pre-knowledge of the respective task to be encoded in the key and value stores. These models could also be applied to storing and reading memories for other tasks, and future work should try them in other areas, such as a full dialog box."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z. Ives"], "venue": "Springer.", "citeRegEx": "Auer et al\\.,? 2007", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Askmsr: Question answering using the worldwide web", "author": ["M. Banko", "E. Brill", "S. Dumais", "J. Lin"], "venue": "Proceedings of 2002 AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases, pages 7\u20139.", "citeRegEx": "Banko et al\\.,? 2002", "shortCiteRegEx": "Banko et al\\.", "year": 2002}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": "EMNLP, pages 1533\u2013 1544.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Question answering with subgraph embeddings", "author": ["A. Bordes", "S. Chopra", "J. Weston"], "venue": "Proc. EMNLP.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1506.02075.", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Toward an architecture for never-ending language learning", "author": ["A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E.R. Hruschka Jr", "T.M. Mitchell"], "venue": "AAAI, volume 5, page 3.", "citeRegEx": "Carlson et al\\.,? 2010", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "J. Mach. Learn. Res., 12, 2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Learning to construct knowledge bases from the world wide web", "author": ["M. Craven", "D. DiPasquo", "D. Freitag", "A. McCallum", "T. Mitchell", "K. Nigam", "S. Slattery"], "venue": "Artificial intelligence, 118(1), 69\u2013113.", "citeRegEx": "Craven et al\\.,? 2000", "shortCiteRegEx": "Craven et al\\.", "year": 2000}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "Proceedings of 20th SIGKDD Conference on Knowledge Discovery and Data Mining (KDD\u201914), New York City, USA.", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["F. Hill", "A. Bordes", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1511.02301.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["T. Kwiatkowski", "E. Choi", "Y. Artzi", "L. Zettlemoyer"], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP\u201913), Seattle, USA.", "citeRegEx": "Kwiatkowski et al\\.,? 2013", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "Association for Computational Linguistics (ACL) System Demonstrations, pages 55\u201360.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Neural variational inference for text processing", "author": ["Y. Miao", "L. Yu", "P. Blunsom"], "venue": "arXiv preprint arXiv:1511.06038.", "citeRegEx": "Miao et al\\.,? 2015", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Attentive pooling networks", "author": ["Santos", "C. d.", "M. Tan", "B. Xiang", "B. Zhou"], "venue": "arXiv preprint arXiv:1602.03609.", "citeRegEx": "Santos et al\\.,? 2016", "shortCiteRegEx": "Santos et al\\.", "year": 2016}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "Proceedings of NIPS.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Webbased question answering: Revisiting askmsr", "author": ["C. Tsai", "Yih", "W.-t.", "C. Burges"], "venue": "Technical report, Technical Report MSR-TR-201520, Microsoft Research.", "citeRegEx": "Tsai et al\\.,? 2015", "shortCiteRegEx": "Tsai et al\\.", "year": 2015}, {"title": "The trec-8 question answering track report", "author": ["Voorhees", "E. M"], "venue": "Trec, volume 99, pages 77\u201382.", "citeRegEx": "Voorhees and M,? 1999", "shortCiteRegEx": "Voorhees and M", "year": 1999}, {"title": "Building a question answering test collection", "author": ["E.M. Voorhees", "D.M. Tice"], "venue": "Proceedings of the 23rd annual international ACM SIGIR", "citeRegEx": "Voorhees and Tice,? 2000", "shortCiteRegEx": "Voorhees and Tice", "year": 2000}, {"title": "What is the jeopardy model? a quasi-synchronous grammar for qa", "author": ["M. Wang", "N.A. Smith", "T. Mitamura"], "venue": "EMNLP-CoNLL, volume 7, pages 22\u201332.", "citeRegEx": "Wang et al\\.,? 2007", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Sentence similarity learning by lexical decomposition and composition", "author": ["Z. Wang", "H. Mi", "A. Ittycheriah"], "venue": "arXiv preprint arXiv:1602.07019.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Weston et al\\.,? 2015a", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Towards ai-complete question answering: a set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint arXiv:1502.05698.", "citeRegEx": "Weston et al\\.,? 2015b", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Y. Yang", "Yih", "W.-t.", "C. Meek"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2013\u20132018. Citeseer.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Yih", "W.-t.", "Chang", "M.-W.", "X. He", "J. Gao"], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Yih et al\\.,? 2015", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "Convolutional neural network for paraphrase identification", "author": ["W. Yin", "H. Sch\u00fctze"], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages", "citeRegEx": "Yin and Sch\u00fctze,? 2015", "shortCiteRegEx": "Yin and Sch\u00fctze", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Question answering (QA) has been a long standing research problem in natural language processing, with the first systems attempting to answer questions by directly reading documents (Voorhees and Tice, 2000).", "startOffset": 182, "endOffset": 207}, {"referenceID": 4, "context": "The development of large-scale Knowledge Bases (KBs) such as Freebase (Bollacker et al., 2008) helped organize information into structured forms, prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such databases (Berant et al.", "startOffset": 70, "endOffset": 94}, {"referenceID": 3, "context": ", 2008) helped organize information into structured forms, prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such databases (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014).", "startOffset": 196, "endOffset": 263}, {"referenceID": 12, "context": ", 2008) helped organize information into structured forms, prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such databases (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014).", "startOffset": 196, "endOffset": 263}, {"referenceID": 10, "context": ", 2008) helped organize information into structured forms, prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such databases (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014).", "startOffset": 196, "endOffset": 263}, {"referenceID": 9, "context": "Since information extraction (IE) (Craven et al., 2000), in-", "startOffset": 34, "endOffset": 55}, {"referenceID": 16, "context": "In this work we propose the Key-Value Memory Network (KV-MemNN), a new neural network architecture that generalizes the original Memory Network (Sukhbaatar et al., 2015) and can work with either knowledge source.", "startOffset": 144, "endOffset": 169}, {"referenceID": 24, "context": "our findings on WIKIQA (Yang et al., 2015), another Wikipedia-based QA benchmark where no KB is available, where we demonstrate that KV-MemNN can reach state-of-the-art results \u2013 surpassing the most recent attention-based neural network models.", "startOffset": 23, "endOffset": 42}, {"referenceID": 19, "context": "Early QA systems were based on information retrieval and were designed to return snippets of text containing an answer (Voorhees and Tice, 2000; Banko et al., 2002), with limitations in terms of question complexity and response coverage.", "startOffset": 119, "endOffset": 164}, {"referenceID": 2, "context": "Early QA systems were based on information retrieval and were designed to return snippets of text containing an answer (Voorhees and Tice, 2000; Banko et al., 2002), with limitations in terms of question complexity and response coverage.", "startOffset": 119, "endOffset": 164}, {"referenceID": 0, "context": "The creation of large-scale KBs (Auer et al., 2007; Bollacker et al., 2008) have lead to the developement of a new class of QA methods based on semantic parsing (Berant et al.", "startOffset": 32, "endOffset": 75}, {"referenceID": 4, "context": "The creation of large-scale KBs (Auer et al., 2007; Bollacker et al., 2008) have lead to the developement of a new class of QA methods based on semantic parsing (Berant et al.", "startOffset": 32, "endOffset": 75}, {"referenceID": 3, "context": ", 2008) have lead to the developement of a new class of QA methods based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014; Yih et al., 2015) that can return precise answers to complicated compositional questions.", "startOffset": 93, "endOffset": 178}, {"referenceID": 12, "context": ", 2008) have lead to the developement of a new class of QA methods based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014; Yih et al., 2015) that can return precise answers to complicated compositional questions.", "startOffset": 93, "endOffset": 178}, {"referenceID": 10, "context": ", 2008) have lead to the developement of a new class of QA methods based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014; Yih et al., 2015) that can return precise answers to complicated compositional questions.", "startOffset": 93, "endOffset": 178}, {"referenceID": 25, "context": ", 2008) have lead to the developement of a new class of QA methods based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014; Yih et al., 2015) that can return precise answers to complicated compositional questions.", "startOffset": 93, "endOffset": 178}, {"referenceID": 9, "context": "Due to the sparsity of KB data, however, the main challenge shifts from finding answers to developing efficient information extraction methods to populate KBs automatically (Craven et al., 2000; Carlson et al., 2010) \u2013 which is not an easy problem.", "startOffset": 173, "endOffset": 216}, {"referenceID": 7, "context": "Due to the sparsity of KB data, however, the main challenge shifts from finding answers to developing efficient information extraction methods to populate KBs automatically (Craven et al., 2000; Carlson et al., 2010) \u2013 which is not an easy problem.", "startOffset": 173, "endOffset": 216}, {"referenceID": 20, "context": "For this reason, recent initiatives are returning to the original setting of directly answering from text using datasets like TRECQA (Wang et al., 2007), based on classical TREC resources (Voorhees et al.", "startOffset": 133, "endOffset": 152}, {"referenceID": 24, "context": ", 1999), and WIKIQA (Yang et al., 2015), extracted from Wikipedia.", "startOffset": 20, "endOffset": 39}, {"referenceID": 2, "context": "Even though standard pipeline QA systems like AskMR (Banko et al., 2002) have been recently revisited (Tsai et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 17, "context": ", 2002) have been recently revisited (Tsai et al., 2015), the best published results on TRECQA and WIKIQA have been obtained by either convolutional neural networks (Santos et al.", "startOffset": 37, "endOffset": 56}, {"referenceID": 14, "context": ", 2016) or recurrent neural networks (Miao et al., 2015) \u2013 both usually with attention mechanisms inspired by (Bahdanau et al.", "startOffset": 37, "endOffset": 56}, {"referenceID": 1, "context": ", 2015) \u2013 both usually with attention mechanisms inspired by (Bahdanau et al., 2015).", "startOffset": 61, "endOffset": 84}, {"referenceID": 22, "context": "The Key-Value Memory Network model is based on the Memory Network (MemNNs) model (Weston et al., 2015a; Sukhbaatar et al., 2015) which has proven useful for a variety of document reading and question answering tasks: for reading children\u2019s books and answering questions about them (Hill et al.", "startOffset": 81, "endOffset": 128}, {"referenceID": 16, "context": "The Key-Value Memory Network model is based on the Memory Network (MemNNs) model (Weston et al., 2015a; Sukhbaatar et al., 2015) which has proven useful for a variety of document reading and question answering tasks: for reading children\u2019s books and answering questions about them (Hill et al.", "startOffset": 81, "endOffset": 128}, {"referenceID": 11, "context": ", 2015) which has proven useful for a variety of document reading and question answering tasks: for reading children\u2019s books and answering questions about them (Hill et al., 2015), for complex reasoning over simulated stories (Weston et al.", "startOffset": 160, "endOffset": 179}, {"referenceID": 23, "context": ", 2015), for complex reasoning over simulated stories (Weston et al., 2015b) and for utilizing KBs to answer questions (Bordes et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 6, "context": ", 2015b) and for utilizing KBs to answer questions (Bordes et al., 2015).", "startOffset": 51, "endOffset": 72}, {"referenceID": 16, "context": "Our model is based on the end-to-end Memory Network architecture of Sukhbaatar et al. (2015). A high-level view of both models is as follows: one defines a memory, which is a possibly very large array of slots which can encode both long-term and shortterm context.", "startOffset": 68, "endOffset": 93}, {"referenceID": 16, "context": "To obtain the standard End-To-End Memory Network of Sukhbaatar et al. (2015) one can simply set the key and value to be the same for all memories.", "startOffset": 52, "endOffset": 77}, {"referenceID": 23, "context": "identical to a standard MemNN and this approach has been used in several papers (Weston et al., 2015b; Dodge et al., 2016).", "startOffset": 80, "endOffset": 122}, {"referenceID": 11, "context": "Window representations for MemNNs have been shown to work well previously (Hill et al., 2015).", "startOffset": 74, "endOffset": 93}, {"referenceID": 13, "context": "We first employ coreference resolution via the Stanford NLP Toolkit (Manning et al., 2014) to reduce ambiguity by replacing pronominal (\u201che\u201d, \u201cit\u201d) and", "startOffset": 68, "endOffset": 90}, {"referenceID": 8, "context": "Next we use the SENNA semantic role labeling tool (Collobert et al., 2011) to uncover the grammatical structure of each sentence and pair verbs with their arguments.", "startOffset": 50, "endOffset": 74}, {"referenceID": 6, "context": "Using SimpleQuestions (Bordes et al., 2015), an existing open-domain question answering dataset based on Freebase, we identified the subset of questions posed by human annotators that covered our question types.", "startOffset": 22, "endOffset": 43}, {"referenceID": 5, "context": "(Bordes et al., 2014) QA system 93.", "startOffset": 0, "endOffset": 21}, {"referenceID": 24, "context": "for example, the WIKIQA dataset (Yang et al., 2015) for which we also conduct experiments in Sec.", "startOffset": 32, "endOffset": 51}, {"referenceID": 3, "context": "(2014) that performs well on existing datasets WebQuestions (Berant et al., 2013) and SimpleQuestions (Bordes et al.", "startOffset": 60, "endOffset": 81}, {"referenceID": 6, "context": ", 2013) and SimpleQuestions (Bordes et al., 2015) that use KBs only; (ii) supervised embeddings that do not make use of a KB at all but learn question to answer embeddings directly and hence act as a sanity check (Dodge et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 4, "context": "We compare four approaches: (i) the QA system of Bordes et al. (2014) that performs well on existing datasets WebQuestions (Berant et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 5, "context": "The QA system of Bordes et al. (2014) outperforms Supervised Embeddings and Memory Networks for KB", "startOffset": 17, "endOffset": 38}, {"referenceID": 24, "context": "5132 2-gram CNN (Yang et al., 2015) 0.", "startOffset": 16, "endOffset": 35}, {"referenceID": 15, "context": "6652 AP-CNN (Santos et al., 2016) 0.", "startOffset": 12, "endOffset": 33}, {"referenceID": 14, "context": "6957 Attentive LSTM (Miao et al., 2015) 0.", "startOffset": 20, "endOffset": 39}, {"referenceID": 26, "context": "7069 Attentive CNN (Yin and Sch\u00fctze, 2015) 0.", "startOffset": 19, "endOffset": 42}, {"referenceID": 21, "context": "(Wang et al., 2016) 0.", "startOffset": 0, "endOffset": 19}, {"referenceID": 24, "context": "WIKIQA (Yang et al., 2015) is an existing dataset for answer sentence selection using Wikipedia as the knowledge source.", "startOffset": 7, "endOffset": 26}, {"referenceID": 24, "context": "Due to the size of the training set, following many other works (Yang et al., 2015; Santos et al., 2016; Miao et al., 2015) we pre-trained the word vectors (matrices A and B which are constrained to be identical) before training KV-MemNNs.", "startOffset": 64, "endOffset": 123}, {"referenceID": 15, "context": "Due to the size of the training set, following many other works (Yang et al., 2015; Santos et al., 2016; Miao et al., 2015) we pre-trained the word vectors (matrices A and B which are constrained to be identical) before training KV-MemNNs.", "startOffset": 64, "endOffset": 123}, {"referenceID": 14, "context": "Due to the size of the training set, following many other works (Yang et al., 2015; Santos et al., 2016; Miao et al., 2015) we pre-trained the word vectors (matrices A and B which are constrained to be identical) before training KV-MemNNs.", "startOffset": 64, "endOffset": 123}, {"referenceID": 26, "context": "Finally, again following other successful methods (Yin and Sch\u00fctze, 2015), we combine our approach with exact matching word features between question and answers.", "startOffset": 50, "endOffset": 73}], "year": 2016, "abstractText": "Directly reading documents and being able to answer questions from them is a key problem. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs suffer from often being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, MOVIEQA, a QA dataset in the domain of movies. Our method closes the gap between all three settings. It also achieves state-of-the-art results on the existing WIKIQA benchmark.", "creator": "LaTeX with hyperref package"}}}