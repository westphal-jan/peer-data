{"id": "1606.03137", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "Cooperative Inverse Reinforcement Learning", "abstract": "For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as {\\em cooperative inverse reinforcement learning} (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.", "histories": [["v1", "Thu, 9 Jun 2016 22:39:54 GMT  (223kb,D)", "http://arxiv.org/abs/1606.03137v1", null], ["v2", "Tue, 5 Jul 2016 18:25:07 GMT  (220kb,D)", "http://arxiv.org/abs/1606.03137v2", null], ["v3", "Sat, 12 Nov 2016 20:33:43 GMT  (579kb,D)", "http://arxiv.org/abs/1606.03137v3", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["dylan hadfield-menell", "stuart j russell", "pieter abbeel", "anca d dragan"], "accepted": true, "id": "1606.03137"}, "pdf": {"name": "1606.03137.pdf", "metadata": {"source": "CRF", "title": "Cooperative Inverse Reinforcement Learning", "authors": ["Dylan Hadfield-Menell", "Anca Dragan", "Pieter Abbeel", "Stuart Russell"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "As Norbert Wiener (1960) wrote in one of the earliest explanations of the problems that arise when robots operate with objectives that are different from those of humans, this value-alignment problem is not trivial to solve. Humans tend to misrepresent their objectives, which can lead to unexpected implementations, as King Midas has found out. Russell & Norvig (2010) give the example of a reward function for a vacuum robot: when we reward the action of cleaning up dirt, which seems reasonable, the robot repeatedly cleans up the same dirt and maximizes its reward. A solution to the value-alignment of systems has long-term effects on the future of AI and its relationship with humanity (which we are more likely to use in 2014 and beyond)."}, {"heading": "2 Related Work", "text": "We divide this work into three categories: in reverse, multiple learning, optimal teaching, and principal agent models.Inverse Reinforcement Learning. Ng & Russell (2000) defines an inverse reinforcement of learning (IRL) as follows: \"Given the measurements of a [actor]'s behavior over time, we consider the reward function to be optimized.\" The central assumption that this behavior is optimal in the sense that the observed reward maximizes the sum of rewards is what we call the demonstration-by-expert (DBE) assumption. One of our contributions is to prove that this can be be1A coordination problems when there are several optimal policy pairs; we shift this problem to future work. Ng & Russell (2000), Abbeel & Nell (2004), and Ratliff et al. (2006) compute constraints that characterize the observed behavior of reward functions."}, {"heading": "3 Cooperative Inverse Reinforcement Learning", "text": "This section describes CIRL as a Markov game for two players with identical payouts, proves that the problem of calculating an optimal balance for a CIRL game is less complex than the near limit of the Dec-POMDP's suggests, and characterizes apprentice learning as a subclass of CIRL games."}, {"heading": "3.1 CIRL Formulation", "text": "Definition 1. A cooperative inverse reinforcement learning (CIRL) game M is a Markov game for two players with identical rewards between a human or a principle, H, and a robot or agent, R. The game is described by a tuple, M = < S, {AH, AR}, T (\u00b7, \u00b7, \u00b7), (\u00b7, \u00b7; \u00b7), P0 (\u00b7, \u00b7 >, (1) with the following definitions: S a set of world states: s, S. AH a set of actions for H: aH, AH. AR a set of actions for the actors of the first world. AR a set of actions for the actors of the second world. T (\u00b7, \u00b7, \u00b7 \u00b7) a conditional distribution to the next world state, given previous states and actions for both actors: T (s, aH, aR) a set of actions for the first world."}, {"heading": "3.2 Structural Results for Optimal Equilibrium Computation", "text": "The analogy in CIRL for calculating an optimal policy for an MDP is the problem of calculating an optimal policy pair. This is a policy pair that maximizes the expected sum of discounted rewards. This is not the same as the \"solution\" of a CIRL game, since a real implementation of a CIRL agent must take into account coordination problems and strategic uncertainty (Boutilier, 1999). The optimal policy pair represents the best H- and R-behavior if they can coordinate perfectly before H observes a problem. Calculating an optimal common policy for a collaborative game is the solution for a decentralized, partially observed Markov decision process (Dec-POMDP). Unfortunately, Dec-POMDP's NEXPcomplete (Bernstein et al, 2000) are so that general Dec-POMDP algorithms have a computational complexity that is twice exponential. Fortunately, CIRL games have special weighting structure that makes an optimal equilibrium."}, {"heading": "3.3 A Formal Model of Apprenticeship Learning", "text": "In this paradigm, a human gives demonstrations of a robot to an example task and the robot is asked to imitate it in a subsequent task. In what follows, we formulate apprentice reward as speed-based reward, and thus IRL is generally a suboptimal approach to learning processes and a deployment phase. We characterize IRL as the best response to a demonstration action by experts for H. We also show that this policy is generally not a balance reward and therefore represents a suboptimal approach to learning processes. Definition 2. (ACIRL) An apprenticeship cooperative enhancement of learning (ACIRL) is a two-phase game: a learning phase in which the human and robot perform rotations, and an deployment phase in which the robot independently.Example."}, {"heading": "3.4 Approximate Best Response to Feature Matching", "text": "If we limit our attention to the case of linear reward functions2, we can develop an efficient approximate algorithm to calculate an optimal response. Specifically, we consider the case where the reward for a state (s) is defined as a linear combination of state characteristics for a particular function: R (s, aH, aR; tel) =. Standard results from the IRL literature show that measures with the same expected property have the same value (Abbeel Ng, 2004) > if a particular function has the same value."}, {"heading": "4 Experiments", "text": "In this section, we present a learning experiment for 2D mobile robot navigation. We compare the approach from Section 3.4 with IRL. Specifically, we examine the case where R is implemented with IRL, and measure the value of different guidelines for H: expert, which agree with the IRL assumption, and best-responder, which calculates the best response to IRL."}, {"heading": "4.1 Cooperative Learning for Mobile Robot Navigation", "text": "In the deployment phase, R controls the robot and tries to maximize the reward. We use a limited horizon H and let the first H2 timeframe be the learning phase. There are N\u03c6 state characteristics defined as basic radial functions in which the centers of general knowledge are. Rewards are linear in these characteristics and there is an additional no-op state that each actor deterministically executes on the other side of the map. We use a uniform distribution on [\u2212 1, 1] N\u03c6 for the previous phase. The actions move in one of the four directions {N, S, E, W} and there is an additional no-op that each actor deterministically executes on the other side of the map. Figure 1 shows an example comparison between demonstration-by-expert and the approximate best reaction policy in Section 3.4. The most left picture is the ground reward function. Next, demonstration areas are compared with a reward function that are immediately connected by both reward functions."}, {"heading": "4.2 Demonstration-by-Expert vs Best Responder", "text": "Hypothesis. If R plays an IRL algorithm that matches characteristics, H prefers the best response policy from Section 3.4 to \u03c0E: the best response policy will significantly outperform the expert demonstration policy.2 Where linearity is in terms of effectiveness. Manipulated variables. Our experiment consists of 2 factors: H policy and Num characteristics. We assume that R uses an IRL algorithm to calculate his estimate of effectiveness during learning and maximizes the reward below that estimate during use. We use maximum entropy IRL (Ziebart et al., 2008) to implement the policy of R. H policy varies the strategy of H: demonstration-by-experts and best-responders. At demonstration-by-experts level H, H adheres to the DBE assumption and maximizes the reward during the demonstration."}, {"heading": "5 Conclusion and Future Work", "text": "The key to this model is that the robot knows that it is in a shared environment and tries to maximize human reward (rather than estimating the human reward function and adopting it as its own), which leads to cooperative learning behavior and provides a framework in which HRI algorithms can be designed and the incentives of both actors in a learning environment can be analyzed. We have reduced the problem of calculating an optimal pair of actions to solving a POMDP. This is a useful theoretical tool and can be used to develop new algorithms, but it is clear that optimal pairs of actions are only part of the story. In particular, when it performs a centralized calculation, the reduction assumes that we can effectively program both actors to follow a set coordination policy. This may not be practical in reality, although it can still be helpful to train people to be better teachers."}, {"heading": "6 Appendix: Supplementary Material", "text": "This section contains supplementary material and corrections for paper. Some parts of the main text are repeated for completeness."}, {"heading": "6.1 CIRL Formulation", "text": "Definition 3. A cooperative inverse reinforcement learning (CIRL) is a two-player Markov game with identical rewards between a human or principles, H and a robot or agent, R. The game is described by a tuple, M = < S, {AH, AR}, T (\u00b7 \u00b7, \u00b7), R (\u00b7, \u00b7 \u00b7), P0 (\u00b7, \u00b7 \u00b7), 3) with the following definitions: S a set of world states: s, S. AH a set of measures for H: aH a set of measures for H: aH. AR a set of measures for R: aR."}, {"heading": "6.2 Apprenticeship CIRL", "text": "An example of an educational task in which R must help create office supplies. H and R can make paper clips and staples and the unobserved staples, and the unnoticed staples describe H's preference for paper clips. We model the problem as ACIRL, in which the learning and deployment phase each consists of an individual action. The world state in this problem is a tuple (ps, qs, t) in which the number of paper clips and staples H's is each the round number. An action is a tuple (pa, qa) that produces pa paper clips and qa staples. Man can make 2 items in total: AH = {0, 2), (1), (2, 0). The robot has different abilities. He can select 50 units of each item or it to make 90 of a single item: AR = (0, 90, 0), (90, 0)."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P Abbeel", "A. Ng"], "venue": "In ICML,", "citeRegEx": "Abbeel and Ng,? \\Q2004\\E", "shortCiteRegEx": "Abbeel and Ng", "year": 2004}, {"title": "Recent developments in algorithmic teaching", "author": ["F Balbach", "T. Zeugmann"], "venue": "In Language and Automata Theory and Applications. Springer,", "citeRegEx": "Balbach and Zeugmann,? \\Q2009\\E", "shortCiteRegEx": "Balbach and Zeugmann", "year": 2009}, {"title": "The complexity of decentralized control of Markov decision processes", "author": ["D Bernstein", "S Zilberstein", "N. Immerman"], "venue": "In UAI,", "citeRegEx": "Bernstein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2000}, {"title": "Superintelligence: Paths, dangers, strategies", "author": ["N. Bostrom"], "venue": null, "citeRegEx": "Bostrom,? \\Q2014\\E", "shortCiteRegEx": "Bostrom", "year": 2014}, {"title": "Sequential optimality and coordination in multiagent systems", "author": ["Boutilier", "Craig"], "venue": "In IJCAI,", "citeRegEx": "Boutilier and Craig.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier and Craig.", "year": 1999}, {"title": "Algorithmic and human teaching of sequential decision tasks", "author": ["M Cakmak", "M. Lopes"], "venue": "In AAAI,", "citeRegEx": "Cakmak and Lopes,? \\Q2012\\E", "shortCiteRegEx": "Cakmak and Lopes", "year": 2012}, {"title": "Generating legible motion", "author": ["A Dragan", "S. Srinivasa"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "Dragan and Srinivasa,? \\Q2013\\E", "shortCiteRegEx": "Dragan and Srinivasa", "year": 2013}, {"title": "Incentives in organizations", "author": ["R. Gibbons"], "venue": "Technical report, National Bureau of Economic Research,", "citeRegEx": "Gibbons,? \\Q1998\\E", "shortCiteRegEx": "Gibbons", "year": 1998}, {"title": "On the complexity of teaching", "author": ["S Goldman", "M. Kearns"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Goldman and Kearns,? \\Q1995\\E", "shortCiteRegEx": "Goldman and Kearns", "year": 1995}, {"title": "Learning binary relations and total orders", "author": ["S Goldman", "R Rivest", "R. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Goldman et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Goldman et al\\.", "year": 1993}, {"title": "A game-theoretic approach to generating spatial descriptions", "author": ["D Golland", "P Liang", "D. Klein"], "venue": "In EMNLP, pp", "citeRegEx": "Golland et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Golland et al\\.", "year": 2010}, {"title": "Aggregation and linearity in the provision of intertemporal incentives", "author": ["B Holmstrom", "P. Milgrom"], "venue": null, "citeRegEx": "Holmstrom and Milgrom,? \\Q1987\\E", "shortCiteRegEx": "Holmstrom and Milgrom", "year": 1987}, {"title": "Multitask principal-agent analyses: Incentive contracts, asset ownership, and job design", "author": ["B Holmstrom", "P. Milgrom"], "venue": "Journal of Law, Economics, & Organization,", "citeRegEx": "Holmstrom and Milgrom,? \\Q1991\\E", "shortCiteRegEx": "Holmstrom and Milgrom", "year": 1991}, {"title": "Theory of the firm: Managerial behavior, agency costs and ownership structure", "author": ["M Jensen", "W. Meckling"], "venue": "Journal of Financial Economics,", "citeRegEx": "Jensen and Meckling,? \\Q1976\\E", "shortCiteRegEx": "Jensen and Meckling", "year": 1976}, {"title": "On the folly of rewarding A, while hoping for B", "author": ["S. Kerr"], "venue": "Academy of Management Journal,", "citeRegEx": "Kerr,? \\Q1975\\E", "shortCiteRegEx": "Kerr", "year": 1975}, {"title": "Inverse game theory", "author": ["V Kuleshov", "O. Schrijvers"], "venue": "Web and Internet Economics,", "citeRegEx": "Kuleshov and Schrijvers,? \\Q2015\\E", "shortCiteRegEx": "Kuleshov and Schrijvers", "year": 2015}, {"title": "An investigation of the Therac-25 accidents", "author": ["N Leveson", "C. Turner"], "venue": "IEEE Computer,", "citeRegEx": "Leveson and Turner,? \\Q1993\\E", "shortCiteRegEx": "Leveson and Turner", "year": 1993}, {"title": "Multi-agent inverse reinforcement learning", "author": ["S Natarajan", "G Kunapuli", "K Judah", "P Tadepalli", "Kersting", "J. Kand Shavlik"], "venue": "In Int\u2019l Conference on Machine Learning and Applications,", "citeRegEx": "Natarajan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Natarajan et al\\.", "year": 2010}, {"title": "Decentralized stochastic control with partial history sharing: A common information approach", "author": ["A Nayyar", "A Mahajan", "D. Teneketzis"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Nayyar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nayyar et al\\.", "year": 2013}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A Ng", "S. Russell"], "venue": "In ICML,", "citeRegEx": "Ng and Russell,? \\Q2000\\E", "shortCiteRegEx": "Ng and Russell", "year": 2000}, {"title": "Bayesian inverse reinforcement learning", "author": ["D Ramachandran", "E. Amir"], "venue": "In IJCAI,", "citeRegEx": "Ramachandran and Amir,? \\Q2007\\E", "shortCiteRegEx": "Ramachandran and Amir", "year": 2007}, {"title": "Maximum margin planning", "author": ["N Ratliff", "J Bagnell", "M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Ratliff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2006}, {"title": "Learning agents for uncertain environments (extended abstract)", "author": ["Russell", "Stuart J"], "venue": "In COLT,", "citeRegEx": "Russell and J.,? \\Q1998\\E", "shortCiteRegEx": "Russell and J.", "year": 1998}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["R Smallwood", "E. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Computational rationalization: The inverse equilibrium problem", "author": ["K Waugh", "B Ziebart", "J. Bagnell"], "venue": "In ICML,", "citeRegEx": "Waugh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Waugh et al\\.", "year": 2011}, {"title": "Some moral and technical consequences of automation", "author": ["N. Wiener"], "venue": "Science, 131,", "citeRegEx": "Wiener,? \\Q1960\\E", "shortCiteRegEx": "Wiener", "year": 1960}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B Ziebart", "A Maas", "J Bagnell", "A. Dey"], "venue": "In AAAI,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 25, "context": "\u201d So wrote Norbert Wiener (1960) in one of the earliest explanations of the problems that arise when autonomous systems operate with objectives that differ from those of humans.", "startOffset": 19, "endOffset": 33}, {"referenceID": 25, "context": "\u201d So wrote Norbert Wiener (1960) in one of the earliest explanations of the problems that arise when autonomous systems operate with objectives that differ from those of humans. This value alignment problem is not trivial to solve. Humans are prone to mis-stating their objectives, which can lead to unexpected implementations, as King Midas found out. Russell & Norvig (2010) give the example of specifying a reward function for a vacuum robot: if we reward the action of cleaning up dirt, which seems reasonable, the robot repeatedly dumps and cleans up the same dirt to maximize its reward.", "startOffset": 19, "endOffset": 377}, {"referenceID": 3, "context": "A solution to the value alignment problem has long-term implications for the future of AI and its relationship to humanity (Bostrom, 2014) and short-term utility for the design of usable AI systems.", "startOffset": 123, "endOffset": 138}, {"referenceID": 3, "context": "A solution to the value alignment problem has long-term implications for the future of AI and its relationship to humanity (Bostrom, 2014) and short-term utility for the design of usable AI systems. Giving robots the right objectives and enabling them to make the right trade-offs is crucial for self-driving cars, personal assistants, and human\u2013robot interaction more broadly. Value alignment problems are not unique to artificial systems. Economic systems often involve multiple agents with distinct objectives (e.g., employees and employers) and for whom incentive schemes (e.g., wages) must be designed. Kerr (1975) explains several examples of value misalignment in this context.", "startOffset": 124, "endOffset": 620}, {"referenceID": 2, "context": "Moreover, the complexity of solving the POMDP is exponentially lower than the NEXP-hard bound that (Bernstein et al., 2000) obtains for the natural reduction of the problem to a general Dec-POMDP.", "startOffset": 99, "endOffset": 123}, {"referenceID": 20, "context": "Ng & Russell (2000), Abbeel & Ng (2004), and Ratliff et al. (2006) compute constraints that characterize the set of reward functions so that the observed behavior maximizes reward.", "startOffset": 45, "endOffset": 67}, {"referenceID": 20, "context": "Ng & Russell (2000), Abbeel & Ng (2004), and Ratliff et al. (2006) compute constraints that characterize the set of reward functions so that the observed behavior maximizes reward. In general, there will be many reward functions consistent with this constraint. They use a max-margin heuristic to select a single reward function from this set as their estimate. In CIRL, the constraints they compute characterize R\u2019s belief about \u03b8 under the DBE assumption. Ramachandran & Amir (2007) and Ziebart et al.", "startOffset": 45, "endOffset": 485}, {"referenceID": 20, "context": "Ng & Russell (2000), Abbeel & Ng (2004), and Ratliff et al. (2006) compute constraints that characterize the set of reward functions so that the observed behavior maximizes reward. In general, there will be many reward functions consistent with this constraint. They use a max-margin heuristic to select a single reward function from this set as their estimate. In CIRL, the constraints they compute characterize R\u2019s belief about \u03b8 under the DBE assumption. Ramachandran & Amir (2007) and Ziebart et al. (2008) consider the case where \u03c0 is \u201cnoisily expert,\u201d i.", "startOffset": 45, "endOffset": 511}, {"referenceID": 20, "context": "Ng & Russell (2000), Abbeel & Ng (2004), and Ratliff et al. (2006) compute constraints that characterize the set of reward functions so that the observed behavior maximizes reward. In general, there will be many reward functions consistent with this constraint. They use a max-margin heuristic to select a single reward function from this set as their estimate. In CIRL, the constraints they compute characterize R\u2019s belief about \u03b8 under the DBE assumption. Ramachandran & Amir (2007) and Ziebart et al. (2008) consider the case where \u03c0 is \u201cnoisily expert,\u201d i.e., \u03c0is a Boltzmann distribution where actions or trajectories are selected in proportion to the exponent of their value. Ramachandran & Amir (2007) adopt a Bayesian approach and place an explicit prior on rewards.", "startOffset": 45, "endOffset": 709}, {"referenceID": 20, "context": "Ng & Russell (2000), Abbeel & Ng (2004), and Ratliff et al. (2006) compute constraints that characterize the set of reward functions so that the observed behavior maximizes reward. In general, there will be many reward functions consistent with this constraint. They use a max-margin heuristic to select a single reward function from this set as their estimate. In CIRL, the constraints they compute characterize R\u2019s belief about \u03b8 under the DBE assumption. Ramachandran & Amir (2007) and Ziebart et al. (2008) consider the case where \u03c0 is \u201cnoisily expert,\u201d i.e., \u03c0is a Boltzmann distribution where actions or trajectories are selected in proportion to the exponent of their value. Ramachandran & Amir (2007) adopt a Bayesian approach and place an explicit prior on rewards. Ziebart et al. (2008) places a prior on reward functions indirectly by assuming a uniform prior over trajectories.", "startOffset": 45, "endOffset": 797}, {"referenceID": 17, "context": "Natarajan et al. (2010) introduce an extension to IRL where R observes multiple actors that cooperate to maximize a common reward function.", "startOffset": 0, "endOffset": 24}, {"referenceID": 17, "context": "Natarajan et al. (2010) introduce an extension to IRL where R observes multiple actors that cooperate to maximize a common reward function. This is a different type of cooperation than we consider, as the reward function is common knowledge and R is a passive observer. Waugh et al. (2011) and Kuleshov & Schrijvers (2015) consider the problem of inferring payoffs from observed behavior in a general (i.", "startOffset": 0, "endOffset": 290}, {"referenceID": 17, "context": "Natarajan et al. (2010) introduce an extension to IRL where R observes multiple actors that cooperate to maximize a common reward function. This is a different type of cooperation than we consider, as the reward function is common knowledge and R is a passive observer. Waugh et al. (2011) and Kuleshov & Schrijvers (2015) consider the problem of inferring payoffs from observed behavior in a general (i.", "startOffset": 0, "endOffset": 323}, {"referenceID": 9, "context": "Because CIRL incentivizes the human to teach, as opposed to maximizing reward in isolation, our work is related to optimal teaching: finding examples that optimally train a learner (Balbach & Zeugmann, 2009; Goldman et al., 1993; Goldman & Kearns, 1995).", "startOffset": 181, "endOffset": 253}, {"referenceID": 10, "context": "Examples include finding a motion that best communicates an agent\u2019s intention (Dragan & Srinivasa, 2013), or finding a natural language utterance that best communicates a particular grounding (Golland et al., 2010).", "startOffset": 192, "endOffset": 214}, {"referenceID": 13, "context": "Kerr (1975) describes a wide variety of misaligned incentives in the aptly titled \u201cOn the folly of rewarding A, while hoping for B.", "startOffset": 0, "endOffset": 12}, {"referenceID": 7, "context": "Gibbons (1998) provides a useful survey of principal\u2013agent models and their applications.", "startOffset": 0, "endOffset": 15}, {"referenceID": 7, "context": "Gibbons (1998) provides a useful survey of principal\u2013agent models and their applications. Holmstrom & Milgrom (1987) gives structural results on optimal incentive schemes in linear principal\u2013agent models.", "startOffset": 0, "endOffset": 117}, {"referenceID": 7, "context": "Gibbons (1998) provides a useful survey of principal\u2013agent models and their applications. Holmstrom & Milgrom (1987) gives structural results on optimal incentive schemes in linear principal\u2013agent models. From the perspective of AI research, one of the most interesting lines of research in this literature studies the impacts of distorted incentives. Holmstrom & Milgrom (1991) develop a multi-task model where some tasks are more easily measured and rewarded than others.", "startOffset": 0, "endOffset": 379}, {"referenceID": 2, "context": "Unfortunately, Dec-POMDPs are NEXPcomplete (Bernstein et al., 2000) so general Dec-POMDP algorithms have a computational complexity that is doubly exponential.", "startOffset": 43, "endOffset": 67}, {"referenceID": 2, "context": "Unfortunately, Dec-POMDPs are NEXPcomplete (Bernstein et al., 2000) so general Dec-POMDP algorithms have a computational complexity that is doubly exponential. Fortunately, CIRL games have special structure that makes optimal equilibrium computation more efficient. Nayyar et al. (2013) shows that a Dec-POMDP can be reduced to a coordination-POMDP.", "startOffset": 44, "endOffset": 287}, {"referenceID": 26, "context": "We use Maximum-Entropy IRL (Ziebart et al., 2008) to implement R\u2019s policy.", "startOffset": 27, "endOffset": 49}], "year": 2017, "abstractText": "For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partialinformation game with two agents, human and robot; both are rewarded according to the human\u2019s reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.", "creator": "LaTeX with hyperref package"}}}