{"id": "1606.03203", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Causal Bandits: Learning Good Interventions via Causal Inference", "abstract": "We study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment. Our formalism combines multi-arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information.", "histories": [["v1", "Fri, 10 Jun 2016 06:19:32 GMT  (1110kb,D)", "http://arxiv.org/abs/1606.03203v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["finnian lattimore", "tor lattimore", "mark d reid"], "accepted": true, "id": "1606.03203"}, "pdf": {"name": "1606.03203.pdf", "metadata": {"source": "CRF", "title": "Causal Bandits: Learning Good Interventions via Causal Inference", "authors": ["Finnian Lattimore", "Mark D. Reid"], "emails": ["finn.lattimore@gmail.com", "tor.lattimore@gmail.com", "mark.reid@anu.edu.au"], "sections": [{"heading": null, "text": "We are investigating the problem of using causal models to improve the speed at which good interventions can be learned online in a stochastic environment. Our formalism combines multi-armed bandits and causal conclusions to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits causal feedback and demonstrates a limit to its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information."}, {"heading": "1 Introduction", "text": "Within this framework, individual actions (also referred to as interventions) are repeated by a predetermined set to evaluate their effectiveness using a single, real-world evaluated reward signal. We propose generalizing the standard model by assuming that, in addition to the student's reward signal, the values of a number of covariates drawn from a probable causal model (Pearl, 2000) are observed. Causal models are often used in disciplines where explicit experiments can be difficult, such as social science, demography, and economics. For example, when we predict the impact of changes in childcare on participation, or school choice to a degree. Results from causal inferences relate observations to interventional distributions that predict the outcome of an intervention without being able to explicitly predict it without explicitly implementing meaningful actions."}, {"heading": "2 Problem Setup", "text": "We introduce a novel class of stochastic sequential decision problems, which we call causal bandit problems. In these problems, rewards are given for repeated interventions on a specified causal model. (Following the terminology and notation in Koller and Friedman (2009), a causal model is given by a directed acyclic graph G over a series of random variables X = {X1,., XN} and a common distribution P over X factoring over G. We assume that each variable only takes a limited number of different values. A margin of variable Xi to Xj is interpreted so that a change in the value of Xi can directly lead to a change in the value of Xj. The parents of a variable Xi, called PaXi, is the set of all variables Xj that there is an edge from Xj to Xi in G. An intervention or action (of size n) associated with values {x1."}, {"heading": "3 Regret Bounds for Parallel Bandit", "text": "In this section we propose and analyze an algorithm to achieve optimal regret in a natural special case of the causal bandit problem, which we call parallel bandit. It is simple enough to allow a thorough analysis, but rich enough to model the type of problem discussed in \u00a7 1, including the reward example. It is also sufficient to observe the gap between algorithms, the use of causal models and those that note.The causal model for this class of problems has N binary variables {X1,., XN} where each Xi example {0, 1} independent causes for a reward variable Y variable are {0, 1}, as shown in Figure 1a. All variables are observable and the amount of actions allowed are all interventions: A = {do (Xi = j) {do (Xi = j): 1 \u2264 i."}, {"heading": "4 Regret Bounds for General Graphs", "text": "For general graphs, let us assume that Q = j = 6 = P {Y} 6 = P {2} do (Xi = j)} (correlation is not a causal distribution). However, if all variables are observable, any causal distribution P {X1... XN | do (Xi = j)} do (Xi = j)} do can be expressed in terms of observational distributions (XS = 2), where PaXk denotes the parents of Xk and the Dirac delta function. We could naively generalize our approach to parallel bandits by applying truncated product factorization to write an expression for each P {Y | a} in terms of observational quantities."}, {"heading": "5 Experiments", "text": "We compare algorithms 1 and 2 with the successive elimination of the parallel bandit problem under a variety of conditions, including the fact that the importance of the weighted estimator used by algorithm 2 is not 10 20 30 40 50 m (q) 0,00,10.20.3Re gr et Algorithm 1 Algorithm 2 Successive regret (a) Simple regret vs m (q) for the fixed horizon T = 400 and the number of variables N = 50 truncated, which is justified in this setting by Remark 5. Overall, we use a model in which Y depends only on a single variable X1 (this is unknown to the algorithms). Note: Bernoulli (12 + \u03b5) if the causal structure of Bernoulli (12 \u2212 \u03b5 \u2032) is otherwise, in which we use a model in which Y depends only on a single variable X1 (1 \u2212 q1). This results in an expected reward of 12 + processes for doing."}, {"heading": "6 Discussion & Future Work", "text": "The question that has arisen in recent years is whether such a development can take place at all. (...) The question is how this could take place. (...) The question is whether such a development can take place at all. (...) The question is whether such a development can take place at all. (...) The question is whether such a development can take place at all. (...) The question is whether such a development can take place at all. (...) The question is whether such a development can take place at all. (...) The question is only how such a development can take place. (...) The question is only whether such a development can take place at all. (...) The question is only how such a development can take place. (...)"}, {"heading": "7 Proof of Theorem 1", "text": "The assumption is not restrictive, as all variables are independent and the permutations of the variables can be pushed to the reward function. Proof of Theorem 1 then requires some lemas."}, {"heading": "8 Proof of Theorem 2", "text": "Let us assume without loss of universality that q1 \u2264 q2 \u2264.. \u2264 qN \u2264 1 / 2. For each I define reward function ri byr0 (X) = 12 ri (X) = {1 2 + \u03b5 if Xi = 1 1 1 2 otherwise, where 1 / 4 \u2265 \u03b5 > 0 is a constant to be chosen later. We abbreviate RT, i is the expected simple regret that arises when interacting with the environment determined by q and ri. Let Pi be the corresponding measure for all observations over all T rounds and Ei the expectation with respect to Pi. From Lemma 2.6 by Tsybakov (2008)"}, {"heading": "P0 {a\u0302\u2217T = a\u2217}+ Pi {a\u0302\u2217T 6= a\u2217} \u2265 exp (\u2212KL(P0,Pi)) ,", "text": "Where KL (P0, Pi) is the KL divergence between measures P0 and Pi, the KL divergence between measures P0 and Pi is. Then we have for i \u2264 m Qi \u2264 1 / m and the KL divergence between P0 and Pi can be limited by the telescope property (chain rule) and by the limitation of the local KL divergence by the KL divergence according to Auer et al. (1995). This leads to KL (P0, Pi) \u2264 6\u03b52E0 [T = 1 {Xt, i = 1} \u2264 6\u03b52 (E0Ti (T) + qiT) \u2264 6\u03b52 (E0Ti (T) + qiT (T) + T m)."}, {"heading": "9 Proof of Theorem 3", "text": "The first proof is that Xt, Yt are queried from Q. We define Za (Xt) = Q = = Q (Xt) = Q (YtRa) (Xt) 1 {Ra (Xt) \u2264 Ba} and abbreviate Zat = Za (Xt), Rat = Ra (Xt) and P [a] = Pa {PaY (X) = Pa {} (X). By definition we have (Zat) {Ba and VarQ [Zat] \u2264 EQ [Z2at] \u2264 EQ [R2at] = Ea [Rat] = Ea [Pa] {PaY (X) (X)} Q {PaY (X)} (X)} m (VarQ) \u2264 P [Zat] \u2264 EQ [Zat] \u2264 EQ [R2at] = Ea [Ea [Ea] = EaY \u2212 Ea [Y 1 Rat > Ba} = EaY \u2212 Ea Rat > Ba \u00b2 (X) > Ba \u00b2 (X = s \u00b2)."}], "references": [{"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["A. Agarwal", "D. Hsu", "S. Kale", "J. Langford", "L. Li", "R.E. Schapire"], "venue": null, "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Online learning with feedback graphs: Beyond bandits", "author": ["N. Alon", "N. Cesa-Bianchi", "O. Dekel", "T. Koren"], "venue": "In COLT,", "citeRegEx": "Alon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2015}, {"title": "Best arm identification in multi-armed bandits. In COLT, pages 13\u2013p", "author": ["Audibert", "J.-Y", "S. Bubeck"], "venue": null, "citeRegEx": "Audibert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2010}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "Proceedings of IEEE 36th Annual Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "Decoupling exploration and exploitation in multi-armed bandits", "author": ["O. Avner", "S. Mannor", "O. Shamir"], "venue": "In ICML,", "citeRegEx": "Avner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Avner et al\\.", "year": 2012}, {"title": "Bandits with unobserved confounders: A causal approach", "author": ["E. Bareinboim", "A. Forney", "J. Pearl"], "venue": "In NIPS,", "citeRegEx": "Bareinboim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bareinboim et al\\.", "year": 2015}, {"title": "Partial monitoring-classification, regret bounds, and algorithms", "author": ["G. Bart\u00f3k", "D.P. Foster", "D. P\u00e1l", "A. Rakhlin", "C. Szepesv\u00e1ri"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2014}, {"title": "Counterfactual reasoning and learning systems: The example of computational advertising", "author": ["L. Bottou", "J. Peters", "J. Quinonero-Candela", "D.X. Charles", "D.M. Chickering", "E. Portugaly", "D. Ray", "P. Simard", "E. Snelson"], "venue": null, "citeRegEx": "Bottou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2013}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In ALT,", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "Sequential design of experiments", "author": ["H. Chernoff"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Chernoff,? \\Q1959\\E", "shortCiteRegEx": "Chernoff", "year": 1959}, {"title": "Causal Discovery as a Game", "author": ["F. Eberhardt"], "venue": "In NIPS Causality: Objectives and Assessment,", "citeRegEx": "Eberhardt,? \\Q2010\\E", "shortCiteRegEx": "Eberhardt", "year": 2010}, {"title": "On the number of experiments sufficient and in the worst case necessary to identify all causal relations among n variables", "author": ["F. Eberhardt", "C. Glymour", "R. Scheines"], "venue": "In UAI", "citeRegEx": "Eberhardt et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Eberhardt et al\\.", "year": 2005}, {"title": "Pac bounds for multi-armed bandit and markov decision processes", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "In Computational Learning Theory,", "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric"], "venue": "In NIPS,", "citeRegEx": "Gabillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2012}, {"title": "A guided tour of chernoff bounds", "author": ["T. Hagerup", "C. R\u00fcb"], "venue": "Information processing letters,", "citeRegEx": "Hagerup and R\u00fcb,? \\Q1990\\E", "shortCiteRegEx": "Hagerup and R\u00fcb", "year": 1990}, {"title": "Two optimal strategies for active learning of causal models from interventional data", "author": ["A. Hauser", "P. B\u00fchlmann"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "Hauser and B\u00fchlmann,? \\Q2014\\E", "shortCiteRegEx": "Hauser and B\u00fchlmann", "year": 2014}, {"title": "Randomized experimental design for causal graph discovery", "author": ["H. Hu", "Z. Li", "A.R. Vetta"], "venue": "In NIPS,", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "lil\u2019UCB: An optimal exploration algorithm for multi-armed bandits", "author": ["K. Jamieson", "M. Malloy", "R. Nowak", "S. Bubeck"], "venue": "In COLT,", "citeRegEx": "Jamieson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2014}, {"title": "Efficient learning by implicit exploration in bandit problems with side observations", "author": ["T. Koc\u00e1k", "G. Neu", "M. Valko", "R. Munos"], "venue": "In NIPS,", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2014}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["J. Langford", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "Langford and Zhang,? \\Q2008\\E", "shortCiteRegEx": "Langford and Zhang", "year": 2008}, {"title": "On minimax optimal offline policy evaluation", "author": ["L. Li", "R. Munos", "C. Szepesvari"], "venue": "arXiv preprint arXiv:1409.3653", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Generalized thompson sampling for sequential decision-making and causal inference", "author": ["P.A. Ortega", "D.A. Braun"], "venue": "Complex Adaptive Systems Modeling,", "citeRegEx": "Ortega and Braun,? \\Q2014\\E", "shortCiteRegEx": "Ortega and Braun", "year": 2014}, {"title": "Causality: models, reasoning and inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q2000\\E", "shortCiteRegEx": "Pearl", "year": 2000}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Robbins,? \\Q1952\\E", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "Introduction to nonparametric estimation", "author": ["A.B. Tsybakov"], "venue": null, "citeRegEx": "Tsybakov,? \\Q2008\\E", "shortCiteRegEx": "Tsybakov", "year": 2008}, {"title": "Online Learning with Gaussian Payoffs and Side Observations", "author": ["Y. Wu", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Piecewise-stationary bandit problems with side observations", "author": ["J.Y. Yu", "S. Mannor"], "venue": "In ICML,", "citeRegEx": "Yu and Mannor,? \\Q2009\\E", "shortCiteRegEx": "Yu and Mannor", "year": 2009}, {"title": "1{at = do(Xi = 1)} be the total number of times the learner intervenes on variable i by setting it to 1. Then for i \u2264 m we have qi \u2264 1/m and the KL divergence between P0 and Pi may be bounded using the telescoping property (chain rule) and by bounding the local KL divergence by the \u03c7-squared distance", "author": ["Auer"], "venue": null, "citeRegEx": "Auer,? \\Q1995\\E", "shortCiteRegEx": "Auer", "year": 1995}], "referenceMentions": [{"referenceID": 24, "context": "Medical drug testing, policy setting, and other scientific processes are commonly framed and analysed in the language of sequential experimental design and, in special cases, as bandit problems (Robbins, 1952; Chernoff, 1959).", "startOffset": 194, "endOffset": 225}, {"referenceID": 9, "context": "Medical drug testing, policy setting, and other scientific processes are commonly framed and analysed in the language of sequential experimental design and, in special cases, as bandit problems (Robbins, 1952; Chernoff, 1959).", "startOffset": 194, "endOffset": 225}, {"referenceID": 23, "context": "We propose a generalisation of the standard model by assuming that, in addition to the reward signal, the learner observes the values of a number of covariates drawn from a probabilistic causal model (Pearl, 2000).", "startOffset": 200, "endOffset": 213}, {"referenceID": 17, "context": "Related Work As alluded to above, causal bandit problems can be treated as classical multi-armed bandit problems by simply ignoring the causal model and extra observations and applying an existing best-arm identification algorithm with well understood simple regret guarantees (Jamieson et al., 2014).", "startOffset": 277, "endOffset": 300}, {"referenceID": 6, "context": "(2014) have considered very general models related to partial monitoring games (Bart\u00f3k et al., 2014) where rewards on unplayed actions are revealed according to a feedback graph.", "startOffset": 79, "endOffset": 100}, {"referenceID": 8, "context": "They also focus on cumulative regret, which cannot be used to guarantee low simple regret (Bubeck et al., 2009).", "startOffset": 90, "endOffset": 111}, {"referenceID": 11, "context": "Related Work As alluded to above, causal bandit problems can be treated as classical multi-armed bandit problems by simply ignoring the causal model and extra observations and applying an existing best-arm identification algorithm with well understood simple regret guarantees (Jamieson et al., 2014). However, as we show in \u00a73, ignoring the extra information available in the non-intervened variables yields sub-optimal performance. A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al.", "startOffset": 278, "endOffset": 547}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention.", "startOffset": 114, "endOffset": 136}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention. However, a crucial difference is that in our model the extra observations are only revealed after selecting an intervention and hence cannot be used as context. There have been several proposals for bandit problems where extra feedback is received after an action is taken. Most recently, Alon et al. (2015), Koc\u00e1k et al.", "startOffset": 114, "endOffset": 635}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention. However, a crucial difference is that in our model the extra observations are only revealed after selecting an intervention and hence cannot be used as context. There have been several proposals for bandit problems where extra feedback is received after an action is taken. Most recently, Alon et al. (2015), Koc\u00e1k et al. (2014) have considered very general models related to partial monitoring games (Bart\u00f3k et al.", "startOffset": 114, "endOffset": 656}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention. However, a crucial difference is that in our model the extra observations are only revealed after selecting an intervention and hence cannot be used as context. There have been several proposals for bandit problems where extra feedback is received after an action is taken. Most recently, Alon et al. (2015), Koc\u00e1k et al. (2014) have considered very general models related to partial monitoring games (Bart\u00f3k et al., 2014) where rewards on unplayed actions are revealed according to a feedback graph. As we discuss in \u00a76, the parallel bandit problem can be captured in this framework, however the regret bounds are not optimal in our setting. They also focus on cumulative regret, which cannot be used to guarantee low simple regret (Bubeck et al., 2009). The partial monitoring approach taken by Wu et al. (2015) could be applied (up to modifications for the simple regret) to the parallel bandit, but the resulting strategy would need to know the likelihood of each factor in advance, while our strategy learns this online.", "startOffset": 114, "endOffset": 1141}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention. However, a crucial difference is that in our model the extra observations are only revealed after selecting an intervention and hence cannot be used as context. There have been several proposals for bandit problems where extra feedback is received after an action is taken. Most recently, Alon et al. (2015), Koc\u00e1k et al. (2014) have considered very general models related to partial monitoring games (Bart\u00f3k et al., 2014) where rewards on unplayed actions are revealed according to a feedback graph. As we discuss in \u00a76, the parallel bandit problem can be captured in this framework, however the regret bounds are not optimal in our setting. They also focus on cumulative regret, which cannot be used to guarantee low simple regret (Bubeck et al., 2009). The partial monitoring approach taken by Wu et al. (2015) could be applied (up to modifications for the simple regret) to the parallel bandit, but the resulting strategy would need to know the likelihood of each factor in advance, while our strategy learns this online. Yu and Mannor (2009) utilize extra observations to detect changes in the reward distribution, whereas we assume fixed reward distributions and use extra observations to improve arm selection.", "startOffset": 114, "endOffset": 1374}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention. However, a crucial difference is that in our model the extra observations are only revealed after selecting an intervention and hence cannot be used as context. There have been several proposals for bandit problems where extra feedback is received after an action is taken. Most recently, Alon et al. (2015), Koc\u00e1k et al. (2014) have considered very general models related to partial monitoring games (Bart\u00f3k et al., 2014) where rewards on unplayed actions are revealed according to a feedback graph. As we discuss in \u00a76, the parallel bandit problem can be captured in this framework, however the regret bounds are not optimal in our setting. They also focus on cumulative regret, which cannot be used to guarantee low simple regret (Bubeck et al., 2009). The partial monitoring approach taken by Wu et al. (2015) could be applied (up to modifications for the simple regret) to the parallel bandit, but the resulting strategy would need to know the likelihood of each factor in advance, while our strategy learns this online. Yu and Mannor (2009) utilize extra observations to detect changes in the reward distribution, whereas we assume fixed reward distributions and use extra observations to improve arm selection. Avner et al. (2012) analyse bandit problems where the choice of arm to pull and arm to receive feedback on are decoupled.", "startOffset": 114, "endOffset": 1565}, {"referenceID": 0, "context": "A well-studied class of bandit problems with side information are \u201ccontextual bandits\u201d Langford and Zhang (2008); Agarwal et al. (2014). Our framework bears a superficial similarity to contextual bandit problems since the extra observations on non-intervened variables might be viewed as context for selecting an intervention. However, a crucial difference is that in our model the extra observations are only revealed after selecting an intervention and hence cannot be used as context. There have been several proposals for bandit problems where extra feedback is received after an action is taken. Most recently, Alon et al. (2015), Koc\u00e1k et al. (2014) have considered very general models related to partial monitoring games (Bart\u00f3k et al., 2014) where rewards on unplayed actions are revealed according to a feedback graph. As we discuss in \u00a76, the parallel bandit problem can be captured in this framework, however the regret bounds are not optimal in our setting. They also focus on cumulative regret, which cannot be used to guarantee low simple regret (Bubeck et al., 2009). The partial monitoring approach taken by Wu et al. (2015) could be applied (up to modifications for the simple regret) to the parallel bandit, but the resulting strategy would need to know the likelihood of each factor in advance, while our strategy learns this online. Yu and Mannor (2009) utilize extra observations to detect changes in the reward distribution, whereas we assume fixed reward distributions and use extra observations to improve arm selection. Avner et al. (2012) analyse bandit problems where the choice of arm to pull and arm to receive feedback on are decoupled. The main difference from our present work is our focus on simple regret and the more complex information linking rewards for different arms via causal graphs. To the best of our knowledge, our paper is the first to analyse simple regret in bandit problems with extra post-action feedback. Two pieces of recent work also consider applying ideas from causal inference to bandit problems. Bareinboim et al. (2015) demonstrate that in the presence of confounding variables the value that a variable would have taken had it not been intervened on can provide important contextual information.", "startOffset": 114, "endOffset": 2078}, {"referenceID": 7, "context": "The truncated importance weighted estimators used in \u00a74 have been studied before in a causal framework by Bottou et al. (2013), where the focus is on learning from observational data, but not controlling the sampling process.", "startOffset": 106, "endOffset": 127}, {"referenceID": 8, "context": "This is sometimes refered to as a \u201cpure exploration\u201d (Bubeck et al., 2009) or \u201cbest-arm identification\u201d problem (Gabillon et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 13, "context": ", 2009) or \u201cbest-arm identification\u201d problem (Gabillon et al., 2012) and is most appropriate when, as in drug and policy testing, the learner has a fixed experimental budget after which its policy will be fixed indefinitely.", "startOffset": 45, "endOffset": 68}, {"referenceID": 0, "context": "If the non-intervened variables are observed before an intervention is selected our framework reduces to stochastic contextual bandits, which are already reasonably well understood (Agarwal et al., 2014).", "startOffset": 181, "endOffset": 203}, {"referenceID": 19, "context": "In these problems, rewards are given for repeated interventions on a fixed causal model Pearl (2000). Following the terminology and notation in Koller and Friedman (2009), a causal model is given by a directed acyclic graph G over a set of random variables X = {X1, .", "startOffset": 88, "endOffset": 101}, {"referenceID": 16, "context": "Following the terminology and notation in Koller and Friedman (2009), a causal model is given by a directed acyclic graph G over a set of random variables X = {X1, .", "startOffset": 42, "endOffset": 69}, {"referenceID": 16, "context": "Following the terminology and notation in Koller and Friedman (2009), a causal model is given by a directed acyclic graph G over a set of random variables X = {X1, . . . , XN} and a joint distribution P over X that factorises over G. We will assume each variable only takes on a finite number of distinct values. An edge from variable Xi to Xj is interpreted to mean that a change in the value of Xi may directly cause a change to the value of Xj . The parents of a variable Xi, denoted PaXi , is the set of all variables Xj such that there is an edge from Xj to Xi in G. An intervention or action (of size n), denoted do(X = x), assigns the values x = {x1, . . . , xn} to the corresponding variables X = {X1, . . . , Xn} \u2282 X with the empty intervention (where no variable is set) denoted do(). The intervention also \u201cmutilates\u201d the graph G by removing all edges from Pai to Xi for each Xi \u2208X . The resulting graph defines a probability distribution P {X|do(X = x)} over X := X \u2212X . Details can be found in Chapter 21 of Koller and Friedman (2009). A learner for a casual bandit problem is given the casual model\u2019s graph G and a set of allowed actions A.", "startOffset": 42, "endOffset": 1048}, {"referenceID": 23, "context": "XN |do(Xi = j)} can be expressed in terms of observational distributions via the truncated factorization formula Pearl (2000).", "startOffset": 113, "endOffset": 126}, {"referenceID": 7, "context": "For another discussion see the article by Bottou et al. (2013) who also use importance weighted estimators to learn from observational data.", "startOffset": 42, "endOffset": 63}, {"referenceID": 21, "context": "This is an interesting phenomenon that has been noted before in off-policy evaluation where the regression (and not the importance weighted) estimator is known to be minimax optimal asymptotically (Li et al., 2014).", "startOffset": 197, "endOffset": 214}, {"referenceID": 12, "context": "Making Better Use of the Reward Signal Existing algorithms for best arm identification are based on \u201csuccessive rejection\u201d (SR) of arms based on UCB-like bounds on their rewards (Even-Dar et al., 2002).", "startOffset": 178, "endOffset": 201}, {"referenceID": 26, "context": "In the case of the parallel bandit problem we can slightly modify the analysis from (Wu et al., 2015) on bandits with side information to get near-optimal cumulative regret guarantees.", "startOffset": 84, "endOffset": 101}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen.", "startOffset": 103, "endOffset": 142}, {"referenceID": 18, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen.", "startOffset": 103, "endOffset": 142}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al.", "startOffset": 104, "endOffset": 578}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al. (2014) give adversarial algorithms, which when applied to the parallel bandit problem obtain the standard bandit regret.", "startOffset": 104, "endOffset": 599}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al. (2014) give adversarial algorithms, which when applied to the parallel bandit problem obtain the standard bandit regret. A malicious adversary can select the same graph each time, such that the rewards for half the arms are never revealed by the informative action. This is equivalent to a nominally stochastic selection of feedback graph where q = 0. Causal Models with Non-Observable Variables If we assume knowledge of the conditional interventional distributions P {PaY |a} our analysis applies unchanged to the case of causal models with non-observable variables. Some of the interventional distributions may be non-identifiable meaning we can not obtain prior estimates for P {PaY |a} from even an infinite amount of observational data. Even if all variables are observable and the graph is known, if the conditional distributions are unknown, then Algorithm 2 cannot be used. Estimating these quantities while simultaneously minimising the simple regret is an interesting and challenging open problem. Partially or Completely Unknown Causal Graph A much more difficult generalisation would be to consider causal bandit problems where the causal graph is completely unknown or known to be a member of class of models. The latter case arises naturally if we assume free access to a large observational dataset, from which the Markov equivalence class can be found via causal discovery techniques. Work on the problem of selecting experiments to discover the correct causal graph from within a Markov equivalence class Eberhardt et al. (2005); Eberhardt (2010); Hauser and B\u00fchlmann (2014); Hu et al.", "startOffset": 104, "endOffset": 2139}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al. (2014) give adversarial algorithms, which when applied to the parallel bandit problem obtain the standard bandit regret. A malicious adversary can select the same graph each time, such that the rewards for half the arms are never revealed by the informative action. This is equivalent to a nominally stochastic selection of feedback graph where q = 0. Causal Models with Non-Observable Variables If we assume knowledge of the conditional interventional distributions P {PaY |a} our analysis applies unchanged to the case of causal models with non-observable variables. Some of the interventional distributions may be non-identifiable meaning we can not obtain prior estimates for P {PaY |a} from even an infinite amount of observational data. Even if all variables are observable and the graph is known, if the conditional distributions are unknown, then Algorithm 2 cannot be used. Estimating these quantities while simultaneously minimising the simple regret is an interesting and challenging open problem. Partially or Completely Unknown Causal Graph A much more difficult generalisation would be to consider causal bandit problems where the causal graph is completely unknown or known to be a member of class of models. The latter case arises naturally if we assume free access to a large observational dataset, from which the Markov equivalence class can be found via causal discovery techniques. Work on the problem of selecting experiments to discover the correct causal graph from within a Markov equivalence class Eberhardt et al. (2005); Eberhardt (2010); Hauser and B\u00fchlmann (2014); Hu et al.", "startOffset": 104, "endOffset": 2157}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al. (2014) give adversarial algorithms, which when applied to the parallel bandit problem obtain the standard bandit regret. A malicious adversary can select the same graph each time, such that the rewards for half the arms are never revealed by the informative action. This is equivalent to a nominally stochastic selection of feedback graph where q = 0. Causal Models with Non-Observable Variables If we assume knowledge of the conditional interventional distributions P {PaY |a} our analysis applies unchanged to the case of causal models with non-observable variables. Some of the interventional distributions may be non-identifiable meaning we can not obtain prior estimates for P {PaY |a} from even an infinite amount of observational data. Even if all variables are observable and the graph is known, if the conditional distributions are unknown, then Algorithm 2 cannot be used. Estimating these quantities while simultaneously minimising the simple regret is an interesting and challenging open problem. Partially or Completely Unknown Causal Graph A much more difficult generalisation would be to consider causal bandit problems where the causal graph is completely unknown or known to be a member of class of models. The latter case arises naturally if we assume free access to a large observational dataset, from which the Markov equivalence class can be found via causal discovery techniques. Work on the problem of selecting experiments to discover the correct causal graph from within a Markov equivalence class Eberhardt et al. (2005); Eberhardt (2010); Hauser and B\u00fchlmann (2014); Hu et al.", "startOffset": 104, "endOffset": 2185}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al. (2014) give adversarial algorithms, which when applied to the parallel bandit problem obtain the standard bandit regret. A malicious adversary can select the same graph each time, such that the rewards for half the arms are never revealed by the informative action. This is equivalent to a nominally stochastic selection of feedback graph where q = 0. Causal Models with Non-Observable Variables If we assume knowledge of the conditional interventional distributions P {PaY |a} our analysis applies unchanged to the case of causal models with non-observable variables. Some of the interventional distributions may be non-identifiable meaning we can not obtain prior estimates for P {PaY |a} from even an infinite amount of observational data. Even if all variables are observable and the graph is known, if the conditional distributions are unknown, then Algorithm 2 cannot be used. Estimating these quantities while simultaneously minimising the simple regret is an interesting and challenging open problem. Partially or Completely Unknown Causal Graph A much more difficult generalisation would be to consider causal bandit problems where the causal graph is completely unknown or known to be a member of class of models. The latter case arises naturally if we assume free access to a large observational dataset, from which the Markov equivalence class can be found via causal discovery techniques. Work on the problem of selecting experiments to discover the correct causal graph from within a Markov equivalence class Eberhardt et al. (2005); Eberhardt (2010); Hauser and B\u00fchlmann (2014); Hu et al. (2014) could potentially be incorporated into a causal bandit algorithm.", "startOffset": 104, "endOffset": 2203}, {"referenceID": 1, "context": "The parallel bandit problem can also be viewed as an instance of a time varying graph feedback problem (Alon et al., 2015; Koc\u00e1k et al., 2014), where at each timestep the feedback graph Gt is selected stochastically, dependent on q, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link A \u2192 B in Gt indicates that selecting the action A reveals the reward for action B. For this parallel bandit problem, Gt will always be a star graph with the action do() connected to half the remaining actions. However, Alon et al. (2015); Koc\u00e1k et al. (2014) give adversarial algorithms, which when applied to the parallel bandit problem obtain the standard bandit regret. A malicious adversary can select the same graph each time, such that the rewards for half the arms are never revealed by the informative action. This is equivalent to a nominally stochastic selection of feedback graph where q = 0. Causal Models with Non-Observable Variables If we assume knowledge of the conditional interventional distributions P {PaY |a} our analysis applies unchanged to the case of causal models with non-observable variables. Some of the interventional distributions may be non-identifiable meaning we can not obtain prior estimates for P {PaY |a} from even an infinite amount of observational data. Even if all variables are observable and the graph is known, if the conditional distributions are unknown, then Algorithm 2 cannot be used. Estimating these quantities while simultaneously minimising the simple regret is an interesting and challenging open problem. Partially or Completely Unknown Causal Graph A much more difficult generalisation would be to consider causal bandit problems where the causal graph is completely unknown or known to be a member of class of models. The latter case arises naturally if we assume free access to a large observational dataset, from which the Markov equivalence class can be found via causal discovery techniques. Work on the problem of selecting experiments to discover the correct causal graph from within a Markov equivalence class Eberhardt et al. (2005); Eberhardt (2010); Hauser and B\u00fchlmann (2014); Hu et al. (2014) could potentially be incorporated into a causal bandit algorithm. In particular, Hu et al. (2014) show that only O (log log n) multi-variable interventions are required on average to recover a causal graph over n variables once purely observational data is used to recover the \u201cessential", "startOffset": 104, "endOffset": 2301}], "year": 2016, "abstractText": "We study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment. Our formalism combines multi-arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information.", "creator": "LaTeX with hyperref package"}}}