{"id": "1606.03352", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Conditional Generation and Snapshot Learning in Neural Dialogue Systems", "abstract": "Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.", "histories": [["v1", "Fri, 10 Jun 2016 14:56:19 GMT  (1169kb,D)", "http://arxiv.org/abs/1606.03352v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.NE stat.ML", "authors": ["tsung-hsien wen", "milica gasic", "nikola mrksic", "lina maria rojas-barahona", "pei-hao su", "stefan ultes", "david vandyke", "steve j young"], "accepted": true, "id": "1606.03352"}, "pdf": {"name": "1606.03352.pdf", "metadata": {"source": "CRF", "title": "Conditional Generation and Snapshot Learning in Neural Dialogue Systems", "authors": ["Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina M. Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "emails": ["thw28@cam.ac.uk", "mg436@cam.ac.uk", "nm480@cam.ac.uk", "lmr46@cam.ac.uk", "phs26@cam.ac.uk", "su259@cam.ac.uk", "djv27@cam.ac.uk", "sjy11@cam.ac.uk"], "sections": [{"heading": null, "text": "In this paper, we examine different model architectures and different ways to present and aggregate source information in a system of endless neural dialog. Furthermore, a method called snapshot learning is proposed to facilitate learning from monitored sequential signals by applying an accompanying cross-entropy objective function to the conditioning vector. First, the experimental and analytical results show that there is competition between the conditioning vector and the LM and that the different architectures produce different trade-offs between the two. Second, the discriminatory power and transparency of the conditioning vector is key to providing both interpretability and better performance. Third, snapshot learning leads to consistent performance improvements regardless of which architecture is used."}, {"heading": "1 Introduction", "text": "In recent years, it has become clear that most of them are not self-service, but self-service, capable of unfolding."}, {"heading": "2 Related Work", "text": "This motivates the use of neural networks to model dialog systems from end to end as a conditional generation.Interest in the natural recognition of nanolanguage can be problematized through the use of nanolanguage as a partially observable Markov Decision Process (POMDP) from 2016 (Young et al., 2013) with the aim of enhancing learning (Young et al., 2010) and understanding (Henderson et al., 2014; Mrks talking to real users (Wen et al., 2015b; Wen et al., 2016b) Modules that are available or being trained to improve the underlying semantic representation (Dream, 1999), the conversation is far from natural and comprehensible. This motivates the use of neural networks to model dialogues from end to end as a conditional generation of problematic."}, {"heading": "3 Neural Dialogue System", "text": "The testbed for this paper is a task-oriented neural network dialog system proposed by Wen et al. (2016a). The model presents the dialog as the source for the problem of sequence transduction (modelled by a sequence-to-sequence architecture (Sutskever et al., 2014)), supplemented by the dialog history (modelled by a faith tracker (Henderson et al., 2014)) and the current database search result (modelled by a database operator). The model consists of both encoder and decoder modules. Details of each module are listed below."}, {"heading": "3.1 Encoder Module", "text": "In fact, it is as if it were an unforeseen situation."}, {"heading": "3.2 Decoder Module", "text": "Subject to the system action vector mt provided by the encoder module, the decoder module uses a conditional LSTM LM to generate the required system output token as a token in skeleton form1. The final system response can then be as follows: 2Informable slots are slots that the user can use to narrow the search, e.g. food type or price range; Requestable slots are slots for which the user can request a value, e.g. a phone number. This information is specified in the ontology.by substitution of the actual values of database entries in the skeleton record structure."}, {"heading": "3.2.1 Conditional Generation Network", "text": "In this thesis we examine and analyze three different variants of the LSTM-based conditional generation architectures: Language Model Type (Language Model Type 1j) The easiest way to condition the LSTM network for additional source information is to use the conditioning vector mt together with the input word embedding wj and the previous hidden layer hj \u2212 1, ij fj oj c = sigmoid sigmoid sigmoid tanhW4n, 3n mtwjhj \u2212 1 cj = fj cj \u2212 1 + ij c = oj tanh (cj), where the index j oj is the generation step, n is the hidden layer size, ij, fj, oj, oj, oj, oj, oj, oj [0, 1] n are input, forgotten, and output gates or c'j and cj are suggested cell values and true cell values at step j, and W4n, 3n are the model parameters."}, {"heading": "3.2.2 Attention and Belief Representation", "text": "Attention An attention-based mechanism provides an effective approach to aggregating multiple sources of information for predictive tasks. Like Wen et al. (2016a), we are investigating the use of an attention mechanism to combine the faith states of the tracker, in which the political network is modified in Equation 1 asmjt = tanh (Wzmzt + Wxmxt + HTMT), in which the attention weights \u03b1js = softmax (Wr \u00b7 (Wr \u00b7 (vt-pst-Wtj-htj \u2212 1))) are calculated, where vt = zt + xt and matrix Wr and vector r r are parameters to be learned. Belief Representation The effect of different faith state representations on final performance is also investigated. For informative slots, the full faith state pst is the original state that contains all categorical values; the summary faith state contains only three components of apparent belief, because it does not give the total value of all the probabilities."}, {"heading": "3.3 Snapshot Learning", "text": "To mitigate this difficulty, we introduce a novel method called snapshot learning to create a vector of binary names. Each element of the snapshot vector is an indicator function of a particular event that will happen in the future, which can be derived either from the system response or the dialog context in the training time. An accompanying cross-entropy error is then calculated to force a subset of the conditioning vector m. Each element of the snapshot vector is an indicator function of a particular event that will happen in the future."}, {"heading": "4 Experiments", "text": "The datasets used in this work were collected as part of the Wizard of Oz Online Data Collection by Wen et al. (2016a), in which the task of the system is to assist users to find a restaurant in Cambridge, UK. There are three informative slots (Food, Pricerange, Area) that users can use to restrict the search and the six available slots (addresses). There are 676 dialogs in the datasets (including finished and unfinished dialogs) and approximately 2750 phrases in the specific application used in this study. The database contains 99 unique restaurants. The training process has been divided into two stages."}, {"heading": "5 Model Analysis", "text": "We analyzed the hybrid models because their activation ratio from read-gate to output gate (rj / oj) has a clear target conflict between the LM and the conditioning vector components. As you can see in Table 2, the average activation of gate (fj) and the ratio of read-gate to output gate (rj / oj) seem to have strong correlations to performance: better performance (line 3 > line 2 > line 1) seems to come from models that can learn a longer word dependence (higher activation of gate and output gate) and a better conditioning vector (previous reading to output gate rj / oj). Learned attention We have visualized the learned thermal image map of models trained with and without snapshot learning in Figure 3. Attention is on both the informative slot trackers (first three columns) and the requestable lotters (which generate attention with the rab-4b-4b-4b-4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4b 4"}, {"heading": "6 Conclusion and Future Work", "text": "The results showed three key results: firstly, although the hybrid model did not rank first on all measures, it is still preferred because it achieved the highest task success and also provided better interpretable results; secondly, because it led to improvements on virtually all measures regardless of the architecture used; the analysis suggested that the benefit of snapshot learning derives mainly from the more differentiated and robust subspace representation learned from the heuristically designated accompanying signals, which in turn facilitates the optimization of the final target; and finally, the results suggested that improving the interpretability of a complex system at various levels not only helps our understanding, but also leads to the highest success rates. Nevertheless, much work remains to be done. This work focused on conditional generation architectures and snapshot learning in the scenario of generating dialogue responses. It would be very helpful if the same comparison could be performed in other approaches to image labeling to override the effectiveness of this effect."}, {"heading": "Acknowledgments", "text": "Tsung-Hsien Wen and David Vandyke are supported by Toshiba Research Europe Ltd., Cambridge Research Laboratory."}, {"heading": "Appendix: More snapshot neuron visualisation", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston."], "venue": "ICML.", "citeRegEx": "Bengio et al\\.,? 2009", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Mark Craven", "Johan Kumlien."], "venue": "ISMB.", "citeRegEx": "Craven and Kumlien.,? 1999", "shortCiteRegEx": "Craven and Kumlien.", "year": 1999}, {"title": "Evaluating prerequisite qualities for learning end-to-end dialog systems", "author": ["Jesse Dodge", "Andreea Gane", "Xiang Zhang", "Antoine Bordes", "Sumit Chopra", "Alexander Miller", "Arthur Szlam", "Jason Weston."], "venue": "ICLR.", "citeRegEx": "Dodge et al\\.,? 2016", "shortCiteRegEx": "Dodge et al\\.", "year": 2016}, {"title": "On-line policy optimisation of bayesian spoken dialogue systems via human interaction", "author": ["Milica Ga\u0161i\u0107", "Catherine Breslin", "Matthew Henderson", "Dongho Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Steve Young."], "venue": "ICASSP.", "citeRegEx": "Ga\u0161i\u0107 et al\\.,? 2013", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2013}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint:1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young."], "venue": "SIGdial.", "citeRegEx": "Henderson et al\\.,? 2014", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "NIPS.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "ICLR.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Knowledgebased weak supervision for information extraction of overlapping relations", "author": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S. Weld."], "venue": "ACL.", "citeRegEx": "Hoffmann et al\\.,? 2011", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei."], "venue": "CVPR.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "An iterative design methodology for user-friendly natural language office information applications", "author": ["John F. Kelley."], "venue": "ACM Transaction on Information Systems.", "citeRegEx": "Kelley.,? 1984", "shortCiteRegEx": "Kelley.", "year": 1984}, {"title": "Deeply-supervised nets", "author": ["Chen-Yu Lee", "Saining Xie", "Patrick Gallagher", "Zhengyou Zhang", "Zhuowen Tu."], "venue": "AISTATS.", "citeRegEx": "Lee et al\\.,? 2015", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "NAACL-HLT.", "citeRegEx": "Li et al\\.,? 2016a", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv perprint:1603.06155.", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Latent predictor networks for code generation", "author": ["Wang Ling", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Andrew Senior", "Fumin Wang", "Phil Blunsom."], "venue": "arXiv preprint:1603.06744.", "citeRegEx": "Ling et al\\.,? 2016", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."], "venue": "SIGdial.", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "What to talk about and how? selective generation using lstms with coarse-to-fine alignment", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."], "venue": "NAACL.", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafit", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "InterSpeech.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan H. \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "ICASSP.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "ACL.", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "author": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "TsungHsien Wen", "Steve Young."], "venue": "ACL.", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? 2015", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "ACL.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A survey of available corpora for building data-driven dialogue systems", "author": ["Iulian Vlad Serban", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint:1512.05742.", "citeRegEx": "Serban et al\\.,? 2015a", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau."], "venue": "AAAI.", "citeRegEx": "Serban et al\\.,? 2015b", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "ACL.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["Rion Snow", "Daniel Jurafsky", "Andrew Y. Ng."], "venue": "NIPS.", "citeRegEx": "Snow et al\\.,? 2004", "shortCiteRegEx": "Snow et al\\.", "year": 2004}, {"title": "Evaluating evaluation methods for generation in the presence of variation", "author": ["Amanda Stent", "Matthew Marge", "Mohit Singhai."], "venue": "CICLing 2005.", "citeRegEx": "Stent et al\\.,? 2005", "shortCiteRegEx": "Stent et al\\.", "year": 2005}, {"title": "Learning from real users: Rating dialogue success with neural networks for reinforcement learning in spoken dialogue systems", "author": ["Pei-Hao Su", "David Vandyke", "Milica Gasic", "Dongho Kim", "Nikola Mrksic", "Tsung-Hsien Wen", "Steve J. Young."], "venue": "Interspeech.", "citeRegEx": "Su et al\\.,? 2015", "shortCiteRegEx": "Su et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."], "venue": "NIPS.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E. Hinton."], "venue": "ICML.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Foundations of Rational Agency, chapter Speech Acts for Dialogue", "author": ["David R. Traum"], "venue": null, "citeRegEx": "Traum,? \\Q1999\\E", "shortCiteRegEx": "Traum", "year": 1999}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc V. Le."], "venue": "ICML Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking", "author": ["Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Dongho Kim", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young."], "venue": "SIGdial.", "citeRegEx": "Wen et al\\.,? 2015a", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young."], "venue": "EMNLP.", "citeRegEx": "Wen et al\\.,? 2015b", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "A network-based end-to-end trainable taskoriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young."], "venue": "arXiv preprint:1604.04562.", "citeRegEx": "Wen et al\\.,? 2016a", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Multidomain neural network language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young."], "venue": "NAACL-HLT.", "citeRegEx": "Wen et al\\.,? 2016b", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio."], "venue": "ICML.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Neural enquirer: Learning to query tables", "author": ["Pengcheng Yin", "Zhengdong Lu", "Hang Li", "Ben Kao."], "venue": "IJCAI.", "citeRegEx": "Yin et al\\.,? 2016", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "The hidden information state model: A practical framework for pomdp-based spoken dialogue management", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Simon Keizer", "Fran\u00e7ois Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu."], "venue": "Computer, Speech and Language.", "citeRegEx": "Young et al\\.,? 2010", "shortCiteRegEx": "Young et al\\.", "year": 2010}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Jason D. Williams."], "venue": "Proceedings of IEEE.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "Recurrent Neural Network (RNN)-based conditional language models (LM) have been shown to be very effective in solving a number of real world problems, such as machine translation (MT) (Cho et al., 2014) and image caption generation (Karpathy and Fei-Fei, 2015).", "startOffset": 184, "endOffset": 202}, {"referenceID": 12, "context": ", 2014) and image caption generation (Karpathy and Fei-Fei, 2015).", "startOffset": 37, "endOffset": 65}, {"referenceID": 36, "context": "Recently, RNNs were applied to task of generating sentences from an explicit semantic representation (Wen et al., 2015a).", "startOffset": 101, "endOffset": 120}, {"referenceID": 19, "context": "Attention-based methods (Mei et al., 2016) and Long Short-term Memory (LSTM)-like (Hochreiter and Schmidhuber, 1997) gating mechanisms (Wen et al.", "startOffset": 24, "endOffset": 42}, {"referenceID": 10, "context": ", 2016) and Long Short-term Memory (LSTM)-like (Hochreiter and Schmidhuber, 1997) gating mechanisms (Wen et al.", "startOffset": 47, "endOffset": 81}, {"referenceID": 37, "context": ", 2016) and Long Short-term Memory (LSTM)-like (Hochreiter and Schmidhuber, 1997) gating mechanisms (Wen et al., 2015b) have both been studied to improve generation quality.", "startOffset": 100, "endOffset": 119}, {"referenceID": 35, "context": "Neural conversational agents (Vinyals and Le, 2015; Shang et al., 2015) are direct extensions of the sequence-to-sequence model (Sutskever et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 27, "context": "Neural conversational agents (Vinyals and Le, 2015; Shang et al., 2015) are direct extensions of the sequence-to-sequence model (Sutskever et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 33, "context": ", 2015) are direct extensions of the sequence-to-sequence model (Sutskever et al., 2014) in which a conversation is cast as a source to target transduction problem.", "startOffset": 64, "endOffset": 88}, {"referenceID": 31, "context": "However, these models are still far from real world applications because they lack any capability for supporting domain specific tasks, for example, being able to interact with databases (Sukhbaatar et al., 2015; Yin et al., 2016) and aggregate useful information into their responses.", "startOffset": 187, "endOffset": 230}, {"referenceID": 41, "context": "However, these models are still far from real world applications because they lack any capability for supporting domain specific tasks, for example, being able to interact with databases (Sukhbaatar et al., 2015; Yin et al., 2016) and aggregate useful information into their responses.", "startOffset": 187, "endOffset": 230}, {"referenceID": 27, "context": "Neural conversational agents (Vinyals and Le, 2015; Shang et al., 2015) are direct extensions of the sequence-to-sequence model (Sutskever et al., 2014) in which a conversation is cast as a source to target transduction problem. However, these models are still far from real world applications because they lack any capability for supporting domain specific tasks, for example, being able to interact with databases (Sukhbaatar et al., 2015; Yin et al., 2016) and aggregate useful information into their responses. Recent work by Wen et al. (2016a), however, proposed an end-to-end trainable neural dialogue system that can assist users to complete specific tasks.", "startOffset": 52, "endOffset": 549}, {"referenceID": 36, "context": "In Wen et al. (2016a), the objective function for learning the dialogue policy and language generator depends solely on the likelihood of the output sentences.", "startOffset": 3, "endOffset": 22}, {"referenceID": 14, "context": "This idea is similar to deeply supervised nets (Lee et al., 2015) in which the final cost from the output layer is optimised together with the companion signals generated from each intermediary layer.", "startOffset": 47, "endOffset": 65}, {"referenceID": 43, "context": "Machine learning approaches to task-oriented dialogue system design have cast the problem as a partially observable Markov Decision Process (POMDP) (Young et al., 2013) with the aim of", "startOffset": 148, "endOffset": 168}, {"referenceID": 5, "context": "using reinforcement learning (RL) to train dialogue policies online through interactions with real users (Ga\u0161i\u0107 et al., 2013).", "startOffset": 105, "endOffset": 125}, {"referenceID": 42, "context": "In order to make RL tractable, the state and action space must be carefully designed (Young et al., 2010) and the understanding (Henderson et al.", "startOffset": 85, "endOffset": 105}, {"referenceID": 7, "context": ", 2010) and the understanding (Henderson et al., 2014; Mrk\u0161i\u0107 et al., 2015) and generation (Wen et al.", "startOffset": 30, "endOffset": 75}, {"referenceID": 23, "context": ", 2010) and the understanding (Henderson et al., 2014; Mrk\u0161i\u0107 et al., 2015) and generation (Wen et al.", "startOffset": 30, "endOffset": 75}, {"referenceID": 37, "context": ", 2015) and generation (Wen et al., 2015b; Wen et al., 2016b) modules were assumed available or trained standalone on supervised corpora.", "startOffset": 23, "endOffset": 61}, {"referenceID": 39, "context": ", 2015) and generation (Wen et al., 2015b; Wen et al., 2016b) modules were assumed available or trained standalone on supervised corpora.", "startOffset": 23, "endOffset": 61}, {"referenceID": 34, "context": "Due to the underlying hand-coded semantic representation (Traum, 1999), the conversation is far from natural and the comprehension capability is limited.", "startOffset": 57, "endOffset": 70}, {"referenceID": 20, "context": "Interest in generating natural language using NNs can be attributed to the success of RNN LMs for large vocabulary speech recognition (Mikolov et al., 2010; Mikolov et al., 2011).", "startOffset": 134, "endOffset": 178}, {"referenceID": 21, "context": "Interest in generating natural language using NNs can be attributed to the success of RNN LMs for large vocabulary speech recognition (Mikolov et al., 2010; Mikolov et al., 2011).", "startOffset": 134, "endOffset": 178}, {"referenceID": 12, "context": "Later on, this conditional generation idea has been tried in several research fields, for example, generating image captions by conditioning an RNN on a convolutional neural network (CNN) output (Karpathy and Fei-Fei, 2015; Xu et al., 2015); translating a source to a target language by conditioning a decoder LSTM on top of an encoder LSTM (Cho et al.", "startOffset": 195, "endOffset": 240}, {"referenceID": 40, "context": "Later on, this conditional generation idea has been tried in several research fields, for example, generating image captions by conditioning an RNN on a convolutional neural network (CNN) output (Karpathy and Fei-Fei, 2015; Xu et al., 2015); translating a source to a target language by conditioning a decoder LSTM on top of an encoder LSTM (Cho et al.", "startOffset": 195, "endOffset": 240}, {"referenceID": 2, "context": ", 2015); translating a source to a target language by conditioning a decoder LSTM on top of an encoder LSTM (Cho et al., 2014; Bahdanau et al., 2015); or generating natural language by conditioning on a symbolic semantic representation (Wen et al.", "startOffset": 108, "endOffset": 149}, {"referenceID": 0, "context": ", 2015); translating a source to a target language by conditioning a decoder LSTM on top of an encoder LSTM (Cho et al., 2014; Bahdanau et al., 2015); or generating natural language by conditioning on a symbolic semantic representation (Wen et al.", "startOffset": 108, "endOffset": 149}, {"referenceID": 37, "context": ", 2015); or generating natural language by conditioning on a symbolic semantic representation (Wen et al., 2015b; Mei et al., 2016).", "startOffset": 94, "endOffset": 131}, {"referenceID": 19, "context": ", 2015); or generating natural language by conditioning on a symbolic semantic representation (Wen et al., 2015b; Mei et al., 2016).", "startOffset": 94, "endOffset": 131}, {"referenceID": 15, "context": "Interest in generating natural language using NNs can be attributed to the success of RNN LMs for large vocabulary speech recognition (Mikolov et al., 2010; Mikolov et al., 2011). Sutskever et al. (2011) showed that plausible sentences can be obtained by sampling characters one by one from the output layer of an RNN.", "startOffset": 135, "endOffset": 204}, {"referenceID": 4, "context": "By conditioning an LSTM on a sequence of characters, Graves (2013) showed that machines can synthesise handwriting indistinguishable from that of a human.", "startOffset": 53, "endOffset": 67}, {"referenceID": 0, "context": "ods, attention-based mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Ling et al., 2016) have been shown to be very effective improving performance using a dynamic source aggregation strategy.", "startOffset": 32, "endOffset": 96}, {"referenceID": 8, "context": "ods, attention-based mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Ling et al., 2016) have been shown to be very effective improving performance using a dynamic source aggregation strategy.", "startOffset": 32, "endOffset": 96}, {"referenceID": 17, "context": "ods, attention-based mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Ling et al., 2016) have been shown to be very effective improving performance using a dynamic source aggregation strategy.", "startOffset": 32, "endOffset": 96}, {"referenceID": 33, "context": "Vinyals and Le (2015) trained the same model on several conversation datasets and showed that the model can generate plausible conversations.", "startOffset": 0, "endOffset": 22}, {"referenceID": 25, "context": "However, Serban et al. (2015b) discovered that the majority of the gener-", "startOffset": 9, "endOffset": 31}, {"referenceID": 15, "context": "ated responses are generic due to the maximum likelihood criterion, which was latter addressed by Li et al. (2016a) using a maximum mutual information decoding strategy.", "startOffset": 98, "endOffset": 116}, {"referenceID": 18, "context": "Many works (Lowe et al., 2015; Serban et al., 2015a; Dodge et al., 2016) have investigated conversation datasets for developing chat bot or QA-like general purpose conversation agents.", "startOffset": 11, "endOffset": 72}, {"referenceID": 25, "context": "Many works (Lowe et al., 2015; Serban et al., 2015a; Dodge et al., 2016) have investigated conversation datasets for developing chat bot or QA-like general purpose conversation agents.", "startOffset": 11, "endOffset": 72}, {"referenceID": 4, "context": "Many works (Lowe et al., 2015; Serban et al., 2015a; Dodge et al., 2016) have investigated conversation datasets for developing chat bot or QA-like general purpose conversation agents.", "startOffset": 11, "endOffset": 72}, {"referenceID": 13, "context": "(2016a), this problem was addressed by designing an online, parallel version of Wizard-of-Oz data collection (Kelley, 1984) which allows large scale and cheap in-domain conversation data to be collected using Amazon Mechanical Turk.", "startOffset": 109, "endOffset": 123}, {"referenceID": 4, "context": ", 2015a; Dodge et al., 2016) have investigated conversation datasets for developing chat bot or QA-like general purpose conversation agents. However, collecting data to develop goal oriented dialogue systems that can help users to complete a task in a specific domain remains difficult. In a recent work by Wen et al. (2016a), this problem was addressed by designing an online, parallel version of Wizard-of-Oz data collection (Kelley, 1984) which allows large scale and cheap in-domain conversation data to be collected using Amazon Mechanical Turk.", "startOffset": 9, "endOffset": 326}, {"referenceID": 3, "context": "Snapshot learning can be viewed as a special form of weak supervision (also known as distant- or self supervision) (Craven and Kumlien, 1999; Snow et al., 2004), in which supervision signals are heuristically labelled by matching unlabelled corpora with entities or attributes in a structured database.", "startOffset": 115, "endOffset": 160}, {"referenceID": 28, "context": "Snapshot learning can be viewed as a special form of weak supervision (also known as distant- or self supervision) (Craven and Kumlien, 1999; Snow et al., 2004), in which supervision signals are heuristically labelled by matching unlabelled corpora with entities or attributes in a structured database.", "startOffset": 115, "endOffset": 160}, {"referenceID": 22, "context": "It has been widely applied to relation extraction (Mintz et al., 2009) and information extraction (Hoffmann et al.", "startOffset": 50, "endOffset": 70}, {"referenceID": 11, "context": ", 2009) and information extraction (Hoffmann et al., 2011) in which facts from a knowledge base (e.", "startOffset": 35, "endOffset": 58}, {"referenceID": 9, "context": "Recently, self supervision was also used in memory networks (Hill et al., 2016) to improve the discriminative power of memory attention.", "startOffset": 60, "endOffset": 79}, {"referenceID": 1, "context": "ing (Bengio et al., 2009).", "startOffset": 4, "endOffset": 25}, {"referenceID": 14, "context": "In practice, snapshot learning is similar to deeply supervised nets (Lee et al., 2015) in which companion ob-", "startOffset": 68, "endOffset": 86}, {"referenceID": 33, "context": "The model casts dialogue as a source to target sequence transduction problem (modelled by a sequence-to-sequence architecture (Sutskever et al., 2014)) augmented with the dialogue history (modelled by a belief tracker (Henderson et", "startOffset": 126, "endOffset": 150}, {"referenceID": 34, "context": "(2016a), this representation can be viewed as a distributed version of the speech act (Traum, 1999) used in traditional systems.", "startOffset": 86, "endOffset": 99}, {"referenceID": 7, "context": "Belief Trackers In addition to the intent network, the neural dialogue system uses a set of slot-based belief trackers (Henderson et al., 2014; Mrk\u0161i\u0107 et al., 2015) to track user requests.", "startOffset": 119, "endOffset": 164}, {"referenceID": 23, "context": "Belief Trackers In addition to the intent network, the neural dialogue system uses a set of slot-based belief trackers (Henderson et al., 2014; Mrk\u0161i\u0107 et al., 2015) to track user requests.", "startOffset": 119, "endOffset": 164}, {"referenceID": 33, "context": "As mentioned in Wen et al. (2016a), this representation can be viewed as a distributed version of the speech act (Traum, 1999) used in traditional systems.", "startOffset": 16, "endOffset": 35}, {"referenceID": 36, "context": "Memory Type The memory type (mem) conditional generation network was introduced by Wen et al. (2015b), shown in Figure 1b, in which the conditioning vector mt is governed by a standalone reading gate rj .", "startOffset": 83, "endOffset": 102}, {"referenceID": 36, "context": "Dataset The dataset used in this work was collected in the Wizard-of-Oz online data collection described by Wen et al. (2016a), in which the task of the system is to assist users to find a restaurant in Cambridge, UK area.", "startOffset": 108, "endOffset": 127}, {"referenceID": 24, "context": "Three evaluation metrics were used: BLEU score (on top-1 and top5 candidates) (Papineni et al., 2002), slot matching rate and objective task success rate (Su et al.", "startOffset": 78, "endOffset": 101}, {"referenceID": 30, "context": ", 2002), slot matching rate and objective task success rate (Su et al., 2015).", "startOffset": 60, "endOffset": 77}, {"referenceID": 29, "context": "tions of using n-gram based metrics like BLEU to evaluate the generation quality (Stent et al., 2005).", "startOffset": 81, "endOffset": 101}], "year": 2016, "abstractText": "Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an endto-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.", "creator": "TeX"}}}