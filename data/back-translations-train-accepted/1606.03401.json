{"id": "1606.03401", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Memory-Efficient Backpropagation Through Time", "abstract": "We propose a novel approach to reduce memory consumption of the backpropagation through time (BPTT) algorithm when training recurrent neural networks (RNNs). Our approach uses dynamic programming to balance a trade-off between caching of intermediate results and recomputation. The algorithm is capable of tightly fitting within almost any user-set memory budget while finding an optimal execution policy minimizing the computational cost. Computational devices have limited memory capacity and maximizing a computational performance given a fixed memory budget is a practical use-case. We provide asymptotic computational upper bounds for various regimes. The algorithm is particularly effective for long sequences. For sequences of length 1000, our algorithm saves 95\\% of memory usage while using only one third more time per iteration than the standard BPTT.", "histories": [["v1", "Fri, 10 Jun 2016 17:20:39 GMT  (277kb,D)", "http://arxiv.org/abs/1606.03401v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["audrunas gruslys", "r\u00e9mi munos", "ivo danihelka", "marc lanctot", "alex graves"], "accepted": true, "id": "1606.03401"}, "pdf": {"name": "1606.03401.pdf", "metadata": {"source": "CRF", "title": "Memory-Efficient Backpropagation Through Time", "authors": ["Audr\u016bnas Gruslys", "Remi Munos", "Ivo Danihelka"], "emails": ["audrunas@google.com", "munos@google.com", "danihelka@google.com", "lanctot@google.com", "gravesa@google.com"], "sections": [{"heading": null, "text": "We propose a novel approach to reduce the memory consumption of the Backpropagation through Time (BPTT) algorithm when training recursive neural networks (RNNNs). Our approach uses dynamic programming to balance cache results and recalculation; the algorithm is able to adapt closely to almost any user-defined storage budget, while finding an optimal execution policy that minimizes computing costs. CPUs have limited storage capacity and maximizing computing power at a fixed storage budget is a practical use case; we offer asymptotic computing limits for different regimes; the algorithm is particularly effective for long sequences; for sequences of 1000 length, our algorithm saves 95% of memory consumption at just one-third more time per iteration than the standard BPTT."}, {"heading": "1 Introduction", "text": "Recursive neural networks (RNNs) are artificial neural networks in which connections between units can form cycles. They are often used for sequence mapping problems because they can propagate hidden state information from early parts of the sequence back to later points. LSTM in particular (Hochreiter and Schmidhuber [1997]) is an RNN architecture that excels in sequence generation (Eck and Schmidhuber [2002], Sutskever et al. [2011], Graves et al. [2013]) and in amplification learning (Sorokin et al. [2015], Mnih et al. [2016]). Other successful RNN architectures include Neural Turing Machines (NTM) (Graves et al. [2014]), DRAW Network (Gregor et al. [2015]), Neural Transducers (Grefenstette et al."}, {"heading": "2 Background and related work", "text": "In this section we describe the key terms and the relevant previous work on memory storage in RNNs. Definition 1. An RNN nucleus is an upstream neural network that is repeatedly cloned (unfolded over time), with each clone representing a specific point in time in the repetition.Definition 2. The hidden state of the recursive network is the part of the output of the RNN nucleus that is passed as input to the next RNN nucleus. In addition to the initial hidden state, there is a single hidden state per time step as soon as the network is unfolded. Definition 3. The internal state of the RNN nucleus for a given time is all the information required to propagate gradients in this step backwards as soon as an input vector, an internal input vector and a hidden state can be evaluated in relation to a single state."}, {"heading": "2.1 Backpropagation through Time", "text": "Backpropagation through Time (BPTT) (Rumelhart et al. [1985], Werbos [1990]) is one of the most common techniques for forming recurrent networks. BPTT \"unfolds\" the neural network over time by generating multiple copies of the relapsing units, which can then be treated like a (deep) feed network with bound weights. Once this is done, a standard feed method can be used to evaluate network suitability across the entire input sequence, while a standard backpropagation algorithm can be used to evaluate partial derivatives of the loss criteria with respect to all network parameters. While this approach is computationally efficient, it is also relatively intensive in memory usage. This is because the standard version of the algorithm effectively requires the storage of internal states of the unfolded network core at each step to evaluate correct partial derivatives."}, {"heading": "2.2 Trading memory for computation time", "text": "The general idea of trading in computing time and memory consumption in general calculation graphs has been studied in the Automatic Differentiation Community (Dauvergne and Hasco\u00ebt [2006]). Lately, the rise of deep architectures and recursive networks has increased interest in a less general case where the graph of forward computation is a chain and gradients need to be concatenated in reverse order. This simplification leads to relatively simple memory saving strategies and heuristics. In the context of BPTT, some of the intermediate results, rather than storing hidden network states, can be recalculated if necessary by executing an additional forward process. Chen et. proposed to divide the sequence of size t into equal parts and memorize only hidden states between the sub-sequences and all internal states within each segment (Chen et al. [2016]). Here, O (\u221a t) memory is propagated at the cost of producing an additional forward logging error, since the logging rights are used by the side."}, {"heading": "3 Memory-efficient backpropagation through time", "text": "We will first discuss two simple examples: when memory is very scarce and when it is somewhat limited. If memory is very scarce, it is easy to design a simple but mathematically inefficient algorithm to reverse propagate errors on RNNs, which uses only a constant amount of memory. Each time the state of the network needs to be restored at a given time t, the algorithm would simply re-evaluate the state by propagating inputs forward, starting from the beginning to the time. Since the reverse propagation occurs in the reverse temporal order, the results from the previous forward steps cannot be reused (since there is no memory to store them).This would require a repetition of the forward steps before the gradient backwards are propagated one step forward (remember only inputs and the initial state).This would create an algorithm that would require t (t + 1) / 2 forward steps to push forward the error forward (O) over an internal step (O)."}, {"heading": "3.1 Backpropagation though time with selective hidden state memorization (BPTT-HSM)", "text": "The idea behind the proposed algorithm is to compromise between two previous extremes. Let's say that we want to propagate a sequence of lengths t forward and backward, but we are only able to store m hidden states in memory at a given time. We can reuse the same memory slots to store different hidden states during backward propagation. Let's also say that we have a single RNN core available for the purposes of interim calculations, capable of storing a single internal state. Let's define C (t, m) as the calculation cost of backward propagation measured in terms of how many forward operations one needs to perform in total during forward and backward propagation steps, we combine an optimal storage usage policy that minimizes the calculation costs. It's easy to define the limit conditions: C (t, 1) = 12 t (t + 1) is the minimum price for the storage set to be made during the forward and backward expansion process."}, {"heading": "3.2 Backpropagation though time with selective internal state memorization (BPTT-ISM)", "text": "Storing internal RNN core states instead of hidden RNN states would allow us to store a single forward operation during backward propagation in each divide-and-conquer step, but at higher storage costs.Suppose we have a storage capacity capable of storing exactly m internal RNN states. First, we need to change the limit conditions: C (t, 1) = 12 t (t + 1) is a cost factor that reflects the minimum storage approach, while C (t, m) = t for all m \u2265 t when memory is sufficient (corresponds to standard BPTT). As before, C (t, m) is measured as the calculation cost for combined forward and backward propagations over a sequence of length t \u2212 m with memory addition m, while an optimal storage utilization policy is pursued. As before, the costs in terms of total forward execution are measured because the number of backward steps is constant."}, {"heading": "3.3 Backpropagation though time with mixed state memorization (BPTT-MSM)", "text": "It is possible to derive an even more general model by combining both approaches as described in Sections 3,1 and 3,2. Suppose we have a total storage capacity m measured in terms of how much a single hidden state can be stored. Let us also assume that storing an internal RNN core state requires alpha times more memory, where \u03b1 \u2265 2 is an integer. We choose between storing a single hidden state when using a single storage unit and storing an internal RNN core state by being able to store an internal RNN core state, being able to store a single forward operation during backward propagation. Let us define C (t, m) as a computational cost in terms of a total amount of forward operations when executing an optimal strategy. We use the following limit conditions: C (t, 1) = 12 t (+ 1) as a minimum storage cost, mirroring the C (while) m = the optimal storage location."}, {"heading": "3.4 Removing double hidden-state memorization", "text": "The definition 3 of the internal RNN core state would typically require a hidden input state to be included for each storage, which can lead to the multiplication of information. For example, if an optimal strategy is to memorize some internal RNN core states one after the other, a hidden input state of one of the stored hidden input states would correspond to the other (see definition 3). Whenever we want to push an internal RNN core state onto the stack and a previous internal state already exists, we can refrain from pushing the hidden input state. Remember that if an internal RNN core state is otherwise unknown, an internal input state is \u03b1 times greater than a hidden state. If we define \u03b2 \u2264 \u03b1 as the space required to store the internal core state (if a hidden input state is known), an internal RNN core state (if a hidden input state is not known otherwise, \u03b1 is greater than a hidden state. If we define \u03b2 \u2264 \u03b1 as the space needed to store the internal core state, if a hidden input state is known. A relationship between \u03b1 and \u03b2 is application-specific, \u03b1 = 1, but in many cases we must modify 1\u03b2 = 1m - C, 1y = 1m."}, {"heading": "3.5 Analytical upper bound for BPTT-HSM", "text": "We have set a theoretical upper limit for the BPTT HSM algorithm as C (t, m) \u2264 mt1 + 1m. Since the limit for short sequences is not tight, it has also been numerically confirmed that C (t, m) < 4t1 + 1 m for t < 105 and m < 103, or less than 3t1 + 1 m if initial forward speed is excluded. Furthermore, we have set another limit in the regime where t < m mm!. For each integer value a and for all t < m aa! the calculation costs are limited by C (t, m) \u2264 (a + 1) t."}, {"heading": "3.6 Comparison of the three different strategies", "text": "The calculation costs for each previously described strategy and results are shown in Figure 6. BPTT-MSM outperforms both BPTT-ISM and BPTT-HSM. This is not surprising, since the search space in this case represents a superset of both strategy spaces, while the algorithm within this space finds an optimal strategy. Furthermore, the strategy that stores only hidden states outperforms a strategy that stores internal RNN core states for long sequences, while the latter outperforms the former for relatively short sequences."}, {"heading": "4 Discussion", "text": "We used an LSTM mapping from 256 inputs to 256 inputs with a lot size of 64 and a measured execution time for a single downward step (forward and backward combined) depending on the sequence length (Figure 2 (b)). Please note that the measured computation time also includes the time required by backward operations in each time step, which does not take dynamic programming equations into account. A single backward operation is usually twice as expensive as a backward operation, as it evaluates both the input data and internal parameters, but as the number of backward operations is constant, it does not affect the optimal strategy."}, {"heading": "4.1 Optimality", "text": "The dynamic program finds the optimal calculation strategy by construction, is subject to memory constraints and a fairly general model \u03b2 \u03b2 \u03b2 that we impose. Since both strategies proposed by Chen et al. [2016] agree with all the assumptions we have made in Section 3.4 when applied to RNNNs, BPTT-MSM is guaranteed to perform at least as well under any memory budget and length. This is because strategies expressed by Chen et al. [2016] can be done by implementing a (potentially suboptimal) policy Di (t, m), H (t, m) under the same equations for Qi (t, m). 4.2 The numerical comparison with Chen's t algorithm requires remembering hidden states and internal RNN states (without entering hidden states), while the recursive approach requires remembering at least 2 logged states."}, {"heading": "5 Conclusion", "text": "We have shown that the most general of the algorithms is at least as good as many other common heuristics. The main advantage of our approach is the ability to adapt to almost all user-specific memory constraints while maximizing computing power."}, {"heading": "A Pseudocode for BPTT-HSM", "text": "It is a pseudo code of an algorithm that evaluates an optimal policy for BPTT-HSM. - This algorithm has a complexity of O (t2 \u00b7 m), but it is possible to optimize to O (t \u00b7 m) by using the maximum sequence length in t. - This is a one-time calculation that does not need to be repeated while evaluating an RNA algorithm 1: BPTT-HSM strategy. - Input: tmax - maximum sequence length; mmax - maximum storage capacity1 EVALUATESTRATEGY (tmax, mmax) 2 Let C and D each have a 2D array of size tmax 3 for r., tmax} do 4 C [t] [t [t]."}, {"heading": "B Upper bound of the computational costs for BPTT-SHM", "text": "B.1 General upper limit Consider the following dynamic program C (t, m) = min1 \u2264 y < t (y + C (t \u2212 y, m \u2212 1) + C (y, m) (11) with boundary conditions: C (t, 1) = 12 t (t + 2) and C (t, m) (t, m) = 2t \u2212 1 for all m + 1 Proposition 1. We have C (t, m) \u2264 mt1 + 1 / m for all t, m + 1 Production. It is easy to verify that the boundary is met. Let us now define the Boolean functions A (t, m): = {C (t, m) \u2264 mt1 + 1 / 1 / m} and A (m): = {t, m)."}, {"heading": "C Generalizing to deep feed-forward networks", "text": "The main difference is that different layers in deep architectures are not homogeneous (i.e., we have different computing costs and memory requirements), while cores within the same RNN are homogeneous. This difference can be addressed by modifying the dynamic programming equations, if we memorize the next state and derive a strategy from it, if only hidden states are stored. We can remember that an optimal policy of the BPTT HSM algorithms could be found by solving given dynamic programming equations. Costs, if we memorize the next state at position y and therenaxis, we have an optimal policy: Q (t, m, y) = y + C (t \u2212 y, m), m \u2212 1) + C (y, m) Optimal position of the next memorization: D (t, m) = argmin y y y y y y y y y < we, x, x, x, the cost (m \u2212 1), we, we \u2212 n, the cost (m \u2212, \u2212, we \u2212, we \u2212, we \u2212 n, we \u2212, \u2212 n, we \u2212 n, \u2212 n, n, n, n n, n n), n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}], "references": [{"title": "Training deep nets with sublinear memory cost", "author": ["Tianqi Chen", "Bing Xu", "Zhiyuan Zhang", "Carlos Guestrin"], "venue": "arXiv preprint arXiv:1604.06174,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "The data-flow equations of checkpointing in reverse automatic differentiation", "author": ["Benjamin Dauvergne", "Laurent Hasco\u00ebt"], "venue": "In Computational Science\u2013ICCS", "citeRegEx": "Dauvergne and Hasco\u00ebt.,? \\Q2006\\E", "shortCiteRegEx": "Dauvergne and Hasco\u00ebt.", "year": 2006}, {"title": "A first look at music composition using LSTM recurrent neural networks", "author": ["Douglas Eck", "Juergen Schmidhuber"], "venue": "Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale,", "citeRegEx": "Eck and Schmidhuber.,? \\Q2002\\E", "shortCiteRegEx": "Eck and Schmidhuber.", "year": 2002}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks", "author": ["Alex Graves"], "venue": "Studies in Computational Intelligence. Springer,", "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Learning internal representations by error propagation", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Rumelhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1985}, {"title": "Deep attention recurrent Q-network", "author": ["Ivan Sorokin", "Alexey Seleznev", "Mikhail Pavlov", "Aleksandr Fedorov", "Anastasiia Ignateva"], "venue": "arXiv preprint arXiv:1512.01693,", "citeRegEx": "Sorokin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sorokin et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}], "referenceMentions": [{"referenceID": 2, "context": "LSTM (Hochreiter and Schmidhuber [1997]) in particular is an RNN architecture that has excelled in sequence generation (Eck and Schmidhuber [2002], Sutskever et al.", "startOffset": 6, "endOffset": 40}, {"referenceID": 2, "context": "LSTM (Hochreiter and Schmidhuber [1997]) in particular is an RNN architecture that has excelled in sequence generation (Eck and Schmidhuber [2002], Sutskever et al.", "startOffset": 120, "endOffset": 147}, {"referenceID": 2, "context": "LSTM (Hochreiter and Schmidhuber [1997]) in particular is an RNN architecture that has excelled in sequence generation (Eck and Schmidhuber [2002], Sutskever et al. [2011], Graves [2012]), speech recognition (Graves et al.", "startOffset": 120, "endOffset": 172}, {"referenceID": 2, "context": "LSTM (Hochreiter and Schmidhuber [1997]) in particular is an RNN architecture that has excelled in sequence generation (Eck and Schmidhuber [2002], Sutskever et al. [2011], Graves [2012]), speech recognition (Graves et al.", "startOffset": 120, "endOffset": 187}, {"referenceID": 2, "context": "LSTM (Hochreiter and Schmidhuber [1997]) in particular is an RNN architecture that has excelled in sequence generation (Eck and Schmidhuber [2002], Sutskever et al. [2011], Graves [2012]), speech recognition (Graves et al. [2013]) and reinforcement learning (Sorokin et al.", "startOffset": 120, "endOffset": 230}, {"referenceID": 2, "context": "LSTM (Hochreiter and Schmidhuber [1997]) in particular is an RNN architecture that has excelled in sequence generation (Eck and Schmidhuber [2002], Sutskever et al. [2011], Graves [2012]), speech recognition (Graves et al. [2013]) and reinforcement learning (Sorokin et al. [2015], Mnih et al.", "startOffset": 120, "endOffset": 281}, {"referenceID": 2, "context": "LSTM (Hochreiter and Schmidhuber [1997]) in particular is an RNN architecture that has excelled in sequence generation (Eck and Schmidhuber [2002], Sutskever et al. [2011], Graves [2012]), speech recognition (Graves et al. [2013]) and reinforcement learning (Sorokin et al. [2015], Mnih et al. [2016]) settings.", "startOffset": 120, "endOffset": 301}, {"referenceID": 2, "context": "LSTM (Hochreiter and Schmidhuber [1997]) in particular is an RNN architecture that has excelled in sequence generation (Eck and Schmidhuber [2002], Sutskever et al. [2011], Graves [2012]), speech recognition (Graves et al. [2013]) and reinforcement learning (Sorokin et al. [2015], Mnih et al. [2016]) settings. Other successful RNN architectures include Neural Turing Machines (NTM) (Graves et al. [2014]), DRAW network (Gregor et al.", "startOffset": 120, "endOffset": 406}, {"referenceID": 2, "context": "LSTM (Hochreiter and Schmidhuber [1997]) in particular is an RNN architecture that has excelled in sequence generation (Eck and Schmidhuber [2002], Sutskever et al. [2011], Graves [2012]), speech recognition (Graves et al. [2013]) and reinforcement learning (Sorokin et al. [2015], Mnih et al. [2016]) settings. Other successful RNN architectures include Neural Turing Machines (NTM) (Graves et al. [2014]), DRAW network (Gregor et al. [2015]), Neural Transducers (Grefenstette et al.", "startOffset": 120, "endOffset": 443}, {"referenceID": 2, "context": "LSTM (Hochreiter and Schmidhuber [1997]) in particular is an RNN architecture that has excelled in sequence generation (Eck and Schmidhuber [2002], Sutskever et al. [2011], Graves [2012]), speech recognition (Graves et al. [2013]) and reinforcement learning (Sorokin et al. [2015], Mnih et al. [2016]) settings. Other successful RNN architectures include Neural Turing Machines (NTM) (Graves et al. [2014]), DRAW network (Gregor et al. [2015]), Neural Transducers (Grefenstette et al. [2015]).", "startOffset": 120, "endOffset": 492}, {"referenceID": 2, "context": "LSTM (Hochreiter and Schmidhuber [1997]) in particular is an RNN architecture that has excelled in sequence generation (Eck and Schmidhuber [2002], Sutskever et al. [2011], Graves [2012]), speech recognition (Graves et al. [2013]) and reinforcement learning (Sorokin et al. [2015], Mnih et al. [2016]) settings. Other successful RNN architectures include Neural Turing Machines (NTM) (Graves et al. [2014]), DRAW network (Gregor et al. [2015]), Neural Transducers (Grefenstette et al. [2015]). Backpropagation Through Time algorithm (BPTT) (Rumelhart et al. [1985], Werbos [1990]) is typically used to obtain gradients during training.", "startOffset": 120, "endOffset": 565}, {"referenceID": 2, "context": "LSTM (Hochreiter and Schmidhuber [1997]) in particular is an RNN architecture that has excelled in sequence generation (Eck and Schmidhuber [2002], Sutskever et al. [2011], Graves [2012]), speech recognition (Graves et al. [2013]) and reinforcement learning (Sorokin et al. [2015], Mnih et al. [2016]) settings. Other successful RNN architectures include Neural Turing Machines (NTM) (Graves et al. [2014]), DRAW network (Gregor et al. [2015]), Neural Transducers (Grefenstette et al. [2015]). Backpropagation Through Time algorithm (BPTT) (Rumelhart et al. [1985], Werbos [1990]) is typically used to obtain gradients during training.", "startOffset": 120, "endOffset": 580}, {"referenceID": 10, "context": "Backpropagation through Time (BPTT) (Rumelhart et al. [1985], Werbos [1990]) is one of the commonly used techniques to train recurrent networks.", "startOffset": 37, "endOffset": 61}, {"referenceID": 10, "context": "Backpropagation through Time (BPTT) (Rumelhart et al. [1985], Werbos [1990]) is one of the commonly used techniques to train recurrent networks.", "startOffset": 37, "endOffset": 76}, {"referenceID": 0, "context": "The general idea of trading computation time and memory consumption in general computation graphs has been investigated in the automatic differentiation community (Dauvergne and Hasco\u00ebt [2006]).", "startOffset": 164, "endOffset": 193}, {"referenceID": 0, "context": "proposed subdividing the sequence of size t into \u221a t equal parts and memorizing only hidden states between the subsequences and all internal states within each segment (Chen et al. [2016]).", "startOffset": 169, "endOffset": 188}, {"referenceID": 0, "context": "As both strategies proposed by Chen et al. [2016] are consistent with all the assumptions that we have made in section 3.", "startOffset": 31, "endOffset": 50}, {"referenceID": 0, "context": "As both strategies proposed by Chen et al. [2016] are consistent with all the assumptions that we have made in section 3.4 when applied to RNNs, BPTT-MSM is guaranteed to perform at least as well under any memory budget and any sequence length. This is because strategies proposed by Chen et al. [2016] can be expressed by providing a (potentially suboptimal) policy Di(t,m), H(t,m) subject to the same equations for Qi(t,m).", "startOffset": 31, "endOffset": 303}], "year": 2016, "abstractText": "We propose a novel approach to reduce memory consumption of the backpropagation through time (BPTT) algorithm when training recurrent neural networks (RNNs). Our approach uses dynamic programming to balance a trade-off between caching of intermediate results and recomputation. The algorithm is capable of tightly fitting within almost any user-set memory budget while finding an optimal execution policy minimizing the computational cost. Computational devices have limited memory capacity and maximizing a computational performance given a fixed memory budget is a practical use-case. We provide asymptotic computational upper bounds for various regimes. The algorithm is particularly effective for long sequences. For sequences of length 1000, our algorithm saves 95% of memory usage while using only one third more time per iteration than the standard BPTT.", "creator": "LaTeX with hyperref package"}}}