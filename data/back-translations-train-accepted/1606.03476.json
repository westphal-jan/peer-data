{"id": "1606.03476", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Generative Adversarial Imitation Learning", "abstract": "Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.", "histories": [["v1", "Fri, 10 Jun 2016 20:51:29 GMT  (179kb,D)", "http://arxiv.org/abs/1606.03476v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jonathan ho", "stefano ermon"], "accepted": true, "id": "1606.03476"}, "pdf": {"name": "1606.03476.pdf", "metadata": {"source": "CRF", "title": "Generative Adversarial Imitation Learning", "authors": ["Jonathan Ho", "Stefano Ermon"], "emails": ["hoj@cs.stanford.edu", "ermon@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "We are interested in a solution that is tailored to people's needs."}, {"heading": "2 Background", "text": "In Section 3, we will work with finite state and action zones S and A to avoid technical machines outside the scope of this work (concerning the compactness of certain function groups), but our algorithms and experiments later on in the work will run in high-dimensional, continuous environments. We will work in high-dimensional, infinite horizon setting, and we will apply a set of stationary stochastic strategies that take action in certain states in S. Successor states will be derived from the dynamic model P (s). We will work in the highly discounted, infinite horizon setting, and we will use an expectation regarding a policy that arises depending on the resulting strategies. \u2212 In Section 3, we will apply a policy that applies a policy depending on high cost. \u2212 In Section 3, we will apply a policy that applies a high cost. \u2212 In Section 3, we will apply a policy that depends on high cost. \u2212 In Section 4, we will apply a policy that we will apply a policy that we will apply. \u2212 In Section 4, we will apply a policy that we will apply a high cost."}, {"heading": "3 Characterizing the induced optimal policy", "text": "To begin our search for an imitation learning algorithm that defines both a function and a small function, which is in fact not sufficiently defined. (1): all functions RS \u00b7 A = [2]: all functions RS \u00b7 A = [3]: all functions RS \u00b7 A = [4]. We examine the best IRL functions in terms of expressiveness by examining their capabilities with C = RS \u00b7 A. Of course, IRL can correctly explain complex expert behavior without meticulous manual work. (1) We examine the best IRL functions in terms of expressiveness by examining their capabilities with C = RS \u00b7 A. We will therefore integrate a (closed, proper) convex cost function that incorporates RS \u00b7 A into our study. Note that convex is a not particularly restrictive requirement: the convex function must be defined as a function."}, {"heading": "4 Practical occupancy measure matching", "text": "We saw in Corollary 3.2.1 that when a constant occurs, the resulting primary problem (7) simply applies to the occupation measurements with experts in all states and actions. However, such an algorithm does not make sense in practice. In reality, the occupation measurement distribution is provided only as a finite set of samples, so in large environments we would like to use the functional approximation to learn a parameterized policy approach. The resulting optimization problem of finding the appropriate occupation measurements would have as many limitations as possible, leading to an unacceptably large problem and eliminating the very purpose of the functional approximation."}, {"heading": "5 Generative adversarial imitation learning", "text": "As discussed in Section 4, the constant regularizer leads to an imitated learning algorithm that exactly matches occupancy measures but is insoluble in large environments. On the other hand, the Regularizer indicator for the linear cost function classes (10) leads to algorithms that are not able to find exactly matching occupancy measures but are tractable in large environments. We propose the following new cost regularizers that combine the best of both worlds, as we will show in the coming sections: \"GA\" (c), \"E [g (s, a)), if c < 0 + \u00b2, if not, where g (x) =\" x \u2212 log. \"This regularizer imposes low penalties on cost functions c that allocate a negative amount to expert shareholder pairs; if c, however, allocates high costs (close to zero), which is the upper limit for costs."}, {"heading": "6 Experiments", "text": "We evaluated algorithm 1 against baselines on 9 physics-based control tasks 2. We examined three action packages, ranging from low-dimensional control tasks from the classical RL literature - the Cartpole [2], the Acrorobot [8], and the Mountaincar [17] - to difficult high-dimensional tasks such as 3D humanoid locomotion, which were recently solved by model-free enhancement of learning. All environments, except the classic control tasks, were simulated with MuJoJoCo [30]. See Appendix B for a complete description of all tasks. Each task has a true cost function, defined in the OpenAI Gym [5]. We first generated expert behavior for these tasks by applying TRPO [26] to these true cost functions to create expert policies."}, {"heading": "7 Discussion and outlook", "text": "As we have shown, our methodology is generally fairly sample-efficient in terms of expert data, but not particularly sample-efficient in terms of environmental interaction during training. We believe that we could significantly improve the learning speed of our algorithm by initializing policy parameters through behavioral cloning, which does not require any environmental interaction at all. Basically, our methodology is model-free, so it will generally require more environmental interaction than model-based methods. Guided Cost Learning [7], for example, builds on a guided policy search [13] and inherits sample efficiency, but also inherits requirement for the model to be well adapted through iteratively adapted time-varying linear dynamics. Interestingly, both our algorithm 1 and our guided cost learning alternate between policy optimization steps and cost adjustment (which we called discrimination adjustment), although the two algorithms have fully adapted linear dynamics that include both our interaction with IR31 and our interaction with IR31."}, {"heading": "Acknowledgments", "text": "We thank Jayesh K. Gupta and John Schulman for their support and advice. This work was supported by the SAIL-Toyota Center for AI Research and an NSF Graduate Research Fellowship (grant number DGE-114747)."}, {"heading": "A Proofs", "text": "This first shows the strict concavity (max.) and (p) the occupation dimensions (protocol, a) and (protocol, a) the protocol dimensions (protocol, a) and (p) the protocol dimensions (protocol, a) and (p) the protocol dimensions (protocol, a) and (p) the protocol dimensions (protocol, a) and (p) the protocol dimensions (protocol, a) and (p) the protocol dimensions (protocol, a) and (c) and (protocol, a) + (protocol, b) and (p). \u2212 The protocol sum (protocol, a) and (p) and (p) the protocol dimensions (protocol, a) and (c) and the protocol data (protocol) and (c) and the protocol data (protocol) and (c) and the protocol data (protocol) and (c) and the protocol data (protocol) and (c) and the protocol data. \u2212 The protocol data and (c) and the protocol data (c) and the protocol data and (c) and the protocol data and (c) and the protocol data and (c) and the protocol data and (c) and the protocol data and (c) and the protocol data and (and (c) and the protocol) and (c and the protocol) and the protocol data and (and (c) and the protocol) and (and the protocol) and the protocol data and (and (and (p) and (p) and (p) and (p) the protocol) and (and (p) and (p) and (p) the protocol and (and (and the protocol) and (and (p) and (and the protocol) and (and the protocol) and (and (and the protocol) and (and the protocol) the protocol) and (and (and the protocol) and (and (and) and (and ("}, {"heading": "B Environments and detailed results", "text": "The environments we used for our experiments come from the OpenAI Gym [5]. The names and version numbers of these environments are listed in Table 1, which also lists dimensions or cardinalities of their observation and action spaces (numbers marked as \"continuous\" indicate dimensions for a continuous space, and numbers marked as \"discrete\" indicate cardinality for a finite space).The amount of environmental interaction used for FEM, GTAL and our algorithm is in Table 2. To reduce the variance of the gradients for these three algorithms, we also adjust value functions using the same neural network architecture as the guidelines, and used generalized benefit estimates [27] (with g =.995 and \u03bb =.97).The exact experimental results are in Table 3. Means and standard deviations are calculated over 50 trajectories."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A.G. Barto", "R.S. Sutton", "C.W. Anderson"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on, (5):834\u2013846,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1983}, {"title": "Infinite time horizon maximum causal entropy inverse reinforcement learning", "author": ["M. Bloem", "N. Bambos"], "venue": "Decision and Control (CDC), 2014 IEEE 53rd Annual Conference on, pages 4911\u20134916. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge university press,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "OpenAI Gym", "author": ["G. Brockman", "V. Cheung", "L. Pettersson", "J. Schneider", "J. Schulman", "J. Tang", "W. Zaremba"], "venue": "arXiv preprint arXiv:1606.01540,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "John Wiley & Sons,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["C. Finn", "S. Levine", "P. Abbeel"], "venue": "Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Rlpy: A value-function-based reinforcement learning framework for education and research", "author": ["A. Geramifard", "C. Dann", "R.H. Klein", "W. Dabney", "J.P. How"], "venue": "JMLR,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS, pages 2672\u20132680,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex Analysis and Minimization Algorithms, volume 305", "author": ["J.-B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": "Springer,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Model-free imitation learning with policy optimization", "author": ["J. Ho", "J.K. Gupta", "S. Ermon"], "venue": "Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "Advances in Neural Information Processing Systems, pages 1071\u20131079,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Continuous inverse optimal control with locally optimal examples", "author": ["S. Levine", "V. Koltun"], "venue": "Proceedings of the 29th International Conference on Machine Learning, pages 41\u201348,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["S. Levine", "Z. Popovic", "V. Koltun"], "venue": "Advances in Neural Information Processing Systems, pages 19\u201327,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "The minimax principle in asymptotic statistical theory", "author": ["P.W. Millar"], "venue": "Ecole d\u2019Et\u00e9 de Probabilit\u00e9s de Saint-Flour XI\u20141981, pages 75\u2013265. Springer,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1983}, {"title": "Efficient memory-based learning for robot control", "author": ["A.W. Moore", "T. Hall"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1990}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A.Y. Ng", "S. Russell"], "venue": "ICML,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "On surrogate loss functions and f-divergences", "author": ["X. Nguyen", "M.J. Wainwright", "M.I. Jordan"], "venue": "The Annals of Statistics, pages 876\u2013904,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient training of artificial neural networks for autonomous navigation", "author": ["D.A. Pomerleau"], "venue": "Neural Computation, 3(1):88\u201397,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1991}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": "John Wiley & Sons,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["N.D. Ratliff", "D. Silver", "J.A. Bagnell"], "venue": "Autonomous Robots, 27(1):25\u201353,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient reductions for imitation learning", "author": ["S. Ross", "D. Bagnell"], "venue": "AISTATS, pages 661\u2013668,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G.J. Gordon", "D. Bagnell"], "venue": "AISTATS, pages 627\u2013635,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning agents for uncertain environments", "author": ["S. Russell"], "venue": "Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pages 101\u2013103. ACM,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M. Jordan", "P. Moritz"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, pages 1889\u20131897,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "A game-theoretic approach to apprenticeship learning", "author": ["U. Syed", "R.E. Schapire"], "venue": "Advances in Neural Information Processing Systems, pages 1449\u20131456,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Apprenticeship learning using linear programming", "author": ["U. Syed", "M. Bowling", "R.E. Schapire"], "venue": "Proceedings of the 25th International Conference on Machine Learning, pages 1032\u20131039,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Mujoco: A physics engine for model-based control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": "Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026\u20135033. IEEE,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B.D. Ziebart", "A. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "AAAI, AAAI\u201908,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling interaction via the principle of maximum causal entropy", "author": ["B.D. Ziebart", "J.A. Bagnell", "A.K. Dey"], "venue": "ICML, pages 1255\u20131262,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 19, "context": "There are two main approaches suitable for this setting: behavioral cloning [20], which learns a policy as a supervised learning problem over state-action pairs from expert trajectories; and inverse reinforcement learning [25, 18], which finds a cost function under which the expert is uniquely optimal.", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "There are two main approaches suitable for this setting: behavioral cloning [20], which learns a policy as a supervised learning problem over state-action pairs from expert trajectories; and inverse reinforcement learning [25, 18], which finds a cost function under which the expert is uniquely optimal.", "startOffset": 222, "endOffset": 230}, {"referenceID": 17, "context": "There are two main approaches suitable for this setting: behavioral cloning [20], which learns a policy as a supervised learning problem over state-action pairs from expert trajectories; and inverse reinforcement learning [25, 18], which finds a cost function under which the expert is uniquely optimal.", "startOffset": 222, "endOffset": 230}, {"referenceID": 22, "context": "Behavioral cloning, while appealingly simple, only tends to succeed with large amounts of data, due to compounding error caused by covariate shift [23, 24].", "startOffset": 147, "endOffset": 155}, {"referenceID": 23, "context": "Behavioral cloning, while appealingly simple, only tends to succeed with large amounts of data, due to compounding error caused by covariate shift [23, 24].", "startOffset": 147, "endOffset": 155}, {"referenceID": 30, "context": "Accordingly, IRL has succeeded in a wide range of problems, from predicting behaviors of taxi drivers [31] to planning footsteps for quadruped robots [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 21, "context": "Accordingly, IRL has succeeded in a wide range of problems, from predicting behaviors of taxi drivers [31] to planning footsteps for quadruped robots [22].", "startOffset": 150, "endOffset": 154}, {"referenceID": 6, "context": "Scaling IRL methods to large environments has thus been the focus of much recent work [7, 14].", "startOffset": 86, "endOffset": 93}, {"referenceID": 13, "context": "Scaling IRL methods to large environments has thus been the focus of much recent work [7, 14].", "startOffset": 86, "endOffset": 93}, {"referenceID": 30, "context": "To develop such an algorithm, we begin in Section 3, where we characterize the policy given by running reinforcement learning on a cost function learned by maximum causal entropy IRL [31, 32].", "startOffset": 183, "endOffset": 191}, {"referenceID": 31, "context": "To develop such an algorithm, we begin in Section 3, where we characterize the policy given by running reinforcement learning on a cost function learned by maximum causal entropy IRL [31, 32].", "startOffset": 183, "endOffset": 191}, {"referenceID": 8, "context": "networks [9], a technique from the deep learning community that has led to recent successes in modeling distributions of natural images: our algorithm harnesses generative adversarial training to fit distributions of states and actions defining expert behavior.", "startOffset": 9, "endOffset": 12}, {"referenceID": 30, "context": "For the remainder of this paper, we will adopt maximum causal entropy IRL [31, 32], which fits a cost function from a family of functions C with the optimization problem", "startOffset": 74, "endOffset": 82}, {"referenceID": 31, "context": "For the remainder of this paper, we will adopt maximum causal entropy IRL [31, 32], which fits a cost function from a family of functions C with the optimization problem", "startOffset": 74, "endOffset": 82}, {"referenceID": 2, "context": "where H(\u03c0) , E\u03c0[\u2212 log \u03c0(a|s)] is the \u03b3-discounted causal entropy [3] of the policy \u03c0.", "startOffset": 65, "endOffset": 68}, {"referenceID": 14, "context": "Using expressive cost function classes, like Gaussian processes [15] and neural networks [7], is crucial to properly explain complex expert behavior without meticulously hand-crafted features.", "startOffset": 64, "endOffset": 68}, {"referenceID": 6, "context": "Using expressive cost function classes, like Gaussian processes [15] and neural networks [7], is crucial to properly explain complex expert behavior without meticulously hand-crafted features.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "[7], effective for a range of robotic manipulation tasks, satisfy this requirement.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "A basic result [21] is that the set of valid occupancy measures D , {\u03c1\u03c0 : \u03c0 \u2208 \u03a0} can be written as a feasible set of affine constraints: if p0(s) is the distribution of starting states and P (s\u2032|s, a) is the dynamics model, then D = { \u03c1 : \u03c1 \u2265 0 and \u2211 a \u03c1(s, a) = p0(s) + \u03b3 \u2211 s\u2032,a P (s|s\u2032, a)\u03c1(s\u2032, a) \u2200 s \u2208 S } .", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "[29]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] that runs a variant of value iteration in an inner loop, can be interpreted as a form of dual ascent, in which one repeatedly solves the primal problem (reinforcement learning) with fixed dual values (costs).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "(8) takes on the form of regularized variants of existing apprenticeship learning algorithms, which indeed do scale to large environments with parameterized policies [11].", "startOffset": 166, "endOffset": 170}, {"referenceID": 0, "context": "Abbeel and Ng [1] and Syed et al.", "startOffset": 14, "endOffset": 17}, {"referenceID": 28, "context": "[29] use, respectively, Clinear = { \u2211 iwifi : \u2016w\u20162 \u2264 1} and Cconvex = { \u2211 iwifi : \u2211 iwi = 1, wi \u2265 0 \u2200i} .", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Clinear leads to feature expectation matching [1], which minimizes `2 distance between expected feature vectors: maxc\u2208Clinear E\u03c0[c(s, a)]\u2212E\u03c0E [c(s, a)] = \u2016E\u03c0[f(s, a)]\u2212E\u03c0E [f(s, a)]\u20162.", "startOffset": 46, "endOffset": 49}, {"referenceID": 27, "context": "Meanwhile, Cconvex leads to MWAL [28] and LPAL [29], which minimize worst-case excess cost among the individual basis functions, as maxc\u2208Cconvex E\u03c0[c(s, a)]\u2212E\u03c0E [c(s, a)] = maxi\u2208{1,.", "startOffset": 33, "endOffset": 37}, {"referenceID": 28, "context": "Meanwhile, Cconvex leads to MWAL [28] and LPAL [29], which minimize worst-case excess cost among the individual basis functions, as maxc\u2208Cconvex E\u03c0[c(s, a)]\u2212E\u03c0E [c(s, a)] = maxi\u2208{1,.", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "[11] rely on the following policy gradient formula for the apprenticeship objective (9) for a parameterized policy \u03c0\u03b8: \u2207\u03b8 max c\u2208C E\u03c0\u03b8 [c(s, a)]\u2212 E\u03c0E [c(s, a)] = \u2207\u03b8E\u03c0\u03b8 [c\u2217(s, a)] = E\u03c0\u03b8 [\u2207\u03b8 log \u03c0\u03b8(a|s)Qc\u2217(s, a)] where c\u2217= arg max c\u2208C E\u03c0\u03b8 [c(s, a)]\u2212 E\u03c0E [c(s, a)], Qc\u2217(s\u0304, \u0101) = E\u03c0\u03b8 [c\u2217(s\u0304, \u0101) | s0 = s\u0304, a0 = \u0101] (12)", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "For the cost classes Clinear and Cconvex (10), this cost fitting amounts to evaluating simple analytical expressions [11].", "startOffset": 117, "endOffset": 121}, {"referenceID": 25, "context": "(12) with ci and the sampled trajectories, and take a trust region policy optimization (TRPO) [26] step to produce \u03c0\u03b8i+1 .", "startOffset": 94, "endOffset": 98}, {"referenceID": 25, "context": "[26] for more details on TRPO.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "It turns out that this optimal loss is (up to a constant shift) the Jensen-Shannon divergence DJS(\u03c1\u03c0, \u03c1\u03c0E ) , DKL (\u03c1\u03c0\u2016(\u03c1\u03c0 + \u03c1E)/2) +DKL (\u03c1E\u2016(\u03c1\u03c0 + \u03c1E)/2), which is a squared metric between distributions [9, 19].", "startOffset": 202, "endOffset": 209}, {"referenceID": 18, "context": "It turns out that this optimal loss is (up to a constant shift) the Jensen-Shannon divergence DJS(\u03c1\u03c0, \u03c1\u03c0E ) , DKL (\u03c1\u03c0\u2016(\u03c1\u03c0 + \u03c1E)/2) +DKL (\u03c1E\u2016(\u03c1\u03c0 + \u03c1E)/2), which is a squared metric between distributions [9, 19].", "startOffset": 202, "endOffset": 209}, {"referenceID": 8, "context": "Algorithm Equation (15) draws a connection between imitation learning and generative adversarial networks [9], which train a generative model G by having it confuse a discriminative classifier D.", "startOffset": 106, "endOffset": 109}, {"referenceID": 11, "context": "Then, we alternate between an Adam [12] gradient step on w to increase Eq.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "[11]: it prevents the policy from changing too much due to noise in the policy gradient.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "We evaluated Algorithm 1 against baselines on 9 physics-based control tasks, ranging from lowdimensional control tasks from the classic RL literature\u2014the cartpole [2], acrobot [8], and mountain car [17]\u2014to difficult high-dimensional tasks such as a 3D humanoid locomotion, solved only recently by model-free reinforcement learning [27, 26].", "startOffset": 163, "endOffset": 166}, {"referenceID": 7, "context": "We evaluated Algorithm 1 against baselines on 9 physics-based control tasks, ranging from lowdimensional control tasks from the classic RL literature\u2014the cartpole [2], acrobot [8], and mountain car [17]\u2014to difficult high-dimensional tasks such as a 3D humanoid locomotion, solved only recently by model-free reinforcement learning [27, 26].", "startOffset": 176, "endOffset": 179}, {"referenceID": 16, "context": "We evaluated Algorithm 1 against baselines on 9 physics-based control tasks, ranging from lowdimensional control tasks from the classic RL literature\u2014the cartpole [2], acrobot [8], and mountain car [17]\u2014to difficult high-dimensional tasks such as a 3D humanoid locomotion, solved only recently by model-free reinforcement learning [27, 26].", "startOffset": 198, "endOffset": 202}, {"referenceID": 26, "context": "We evaluated Algorithm 1 against baselines on 9 physics-based control tasks, ranging from lowdimensional control tasks from the classic RL literature\u2014the cartpole [2], acrobot [8], and mountain car [17]\u2014to difficult high-dimensional tasks such as a 3D humanoid locomotion, solved only recently by model-free reinforcement learning [27, 26].", "startOffset": 331, "endOffset": 339}, {"referenceID": 25, "context": "We evaluated Algorithm 1 against baselines on 9 physics-based control tasks, ranging from lowdimensional control tasks from the classic RL literature\u2014the cartpole [2], acrobot [8], and mountain car [17]\u2014to difficult high-dimensional tasks such as a 3D humanoid locomotion, solved only recently by model-free reinforcement learning [27, 26].", "startOffset": 331, "endOffset": 339}, {"referenceID": 29, "context": "All environments, other than the classic control tasks, were simulated with MuJoCo [30].", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "Each task comes with a true cost function, defined in the OpenAI Gym [5].", "startOffset": 69, "endOffset": 72}, {"referenceID": 25, "context": "We first generated expert behavior for these tasks by running TRPO [26] on these true cost functions to create expert policies.", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "The policy is trained with supervised learning, using Adam [12] with minibatches of 128 examples, until validation error stops decreasing.", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "[11] using the cost function class Clinear (10) of Abbeel and Ng [1] 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[11] using the cost function class Clinear (10) of Abbeel and Ng [1] 3.", "startOffset": 65, "endOffset": 68}, {"referenceID": 10, "context": "[11] using the cost function class Cconvex (10) of Syed and Schapire [28]", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[11] using the cost function class Cconvex (10) of Syed and Schapire [28]", "startOffset": 69, "endOffset": 73}, {"referenceID": 6, "context": "Guided cost learning [7], for instance, builds upon guided policy search [13] and inherits its sample efficiency, but also inherits its requirement that the model is well-approximated by iteratively fitted time-varying linear dynamics.", "startOffset": 21, "endOffset": 24}, {"referenceID": 12, "context": "Guided cost learning [7], for instance, builds upon guided policy search [13] and inherits its sample efficiency, but also inherits its requirement that the model is well-approximated by iteratively fitted time-varying linear dynamics.", "startOffset": 73, "endOffset": 77}, {"referenceID": 30, "context": "Our approach builds upon a vast line of work on IRL [31, 1, 29, 28], and hence, just like IRL, our approach does not interact with the expert during training.", "startOffset": 52, "endOffset": 67}, {"referenceID": 0, "context": "Our approach builds upon a vast line of work on IRL [31, 1, 29, 28], and hence, just like IRL, our approach does not interact with the expert during training.", "startOffset": 52, "endOffset": 67}, {"referenceID": 28, "context": "Our approach builds upon a vast line of work on IRL [31, 1, 29, 28], and hence, just like IRL, our approach does not interact with the expert during training.", "startOffset": 52, "endOffset": 67}, {"referenceID": 27, "context": "Our approach builds upon a vast line of work on IRL [31, 1, 29, 28], and hence, just like IRL, our approach does not interact with the expert during training.", "startOffset": 52, "endOffset": 67}, {"referenceID": 23, "context": "Our method explores randomly to determine which actions bring a policy\u2019s occupancy measure closer to the expert\u2019s, whereas methods that do interact with the expert, like DAgger [24], can simply ask the expert for such actions.", "startOffset": 177, "endOffset": 181}], "year": 2016, "abstractText": "Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert\u2019s cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.", "creator": "LaTeX with hyperref package"}}}