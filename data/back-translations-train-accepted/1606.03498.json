{"id": "1606.03498", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Improved Techniques for Training GANs", "abstract": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.", "histories": [["v1", "Fri, 10 Jun 2016 22:53:35 GMT  (5720kb,D)", "http://arxiv.org/abs/1606.03498v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["tim salimans", "ian j goodfellow", "wojciech zaremba", "vicki cheung", "alec radford", "xi chen"], "accepted": true, "id": "1606.03498"}, "pdf": {"name": "1606.03498.pdf", "metadata": {"source": "CRF", "title": "Improved Techniques for Training GANs", "authors": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung"], "emails": ["tim@openai.com", "ian@openai.com", "woj@openai.com", "vicki@openai.com", "alec.radford@gmail.com", "peter@openai.com"], "sections": [{"heading": null, "text": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning and the generation of images that humans find visually realistic. Unlike most generative model work, our primary goal is not to train a model that assigns a high probability of test data, nor do we require that the model learn well without using labels. Using our new techniques, we obtain state-of-the-art results in semi-supervised classification of MNIST, CIFAR-10 and SVHN. The images generated are of high quality, as a visual turning test confirms: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods are capable of recognizable image classes."}, {"heading": "1 Introduction", "text": "Generative opposing networks [1] (GANs) are a class of methods for learning generative models based on game theory. The goal of GANs is to create a generator network G (z; \u03b8 (G), which generates samples from the data distribution, pdata (x), by transforming vectors of noise z as x = G (z; \u03b8 (G). Training signal for G is provided by a discriminator network D (x), which is trained to distinguish samples from the generator distribution model (x) from real data. Generator network G, in turn, is trained to deceive the discriminator into recognizing its results as real. Recent applications of GANs have shown that they can produce excellent samples [2, 3]. However, training GANs requires a Nash balance of a non-convective excessive game with continuous, high-dimensional parameters. GANs are typically trained on the basis of gradient ancestry function, which is a low level of training."}, {"heading": "2 Related work", "text": "Several recent papers have focused on improving the stability of training and the resulting perception quality of GAN samples [2, 3, 5, 6]. We are building on some of these techniques in this paper. For example, we are using some of the architectural innovations proposed in Radford et al. [3] \"DCGAN.\" One of our proposed techniques, the feature matching discussed in Section 3.1, is similar in spirit to approaches that use maximum mean discrepancy [7, 8, 9] for the formation of generator networks [10, 11]. Another of our proposed techniques, the minibatch features, is based in part on ideas used for batch normalization [12], while our proposed virtual batch normalization is a direct extension of batch normalization. One of the primary objectives of this work is to improve the effectiveness of generative adversarial networks for semi-supervised learning (improving the performance of a supervised task, in this case using uncharacterized learning)."}, {"heading": "3 Toward Convergent GAN Training", "text": "The training of GANs is to find a Nash balance for an uncooperative two-player game. Each player wishes to minimize his own cost function, J (D) (\u03b8 (D), \u03b8 (G))) for the discriminator and J (G) (\u03b8 (D), \u03b8 (G) for the generator. A Nash balance is a point (\u03b8 (D), \u03b8 (G)), so J (D) is at the minimum for the GAN game (D) and J (G) is at the minimum for the generator. Unfortunately, finding Nash balances is a very difficult problem. Algorithms exist for special cases, but we are not aware of any that are applicable to the GAN game, where the cost functions are not convex, the parameters are continuous, and the parameter space is extremely high."}, {"heading": "3.1 Feature matching", "text": "Instead of directly maximizing the output of the discriminator, the new goal requires that the generator generate data that matches the statistics of the real data, using the discriminator only to indicate the statistics that we believe match. Specifically, we train the generator to determine the expected value of the characteristics on an intermediate layer of the discriminator. This is a natural choice of statistics for the generator that match, since we ask it to find the characteristics that differ most from real data generated by the current model.Letting f (x) denotes activations on an intermediate layer of the discriminator, our new goal for the generator is defined as follows: | | Ex-pdataf (x) \u2212 Ez-pz (z) f (G (z)) | 22. The discriminator and thus f (x) are fixed, the regular results of the GAN training are exactly where the point at which the regular match of the GAN practice is guaranteed."}, {"heading": "3.2 Minibatch discrimination", "text": "One of the main failure modes for GAN is that the generator collapses to a parameter setting in which it always ejects the same point. If the collapse to a single mode is imminent, the gradient of the discriminator can similarly point to many similar points. Since the discriminator looks at each example independently, there is no coordination between its gradients, and therefore there is no mechanism to explain the results of the generator. Rather, all results in the direction of a single point in which the discriminator currently believes are very realistic. After the collapse, the discriminator learns that this single point comes from the generator, but the gradient descent is not able to separate the identical outputs. The graditions of the discriminator are then produced by the generator around the space, and the algorithm cannot produce a distribution with the correct amount of entropy. An obvious strategy to avoid this type of failure is that the discriminator looks at multiple examples in combination."}, {"heading": "3.3 Historical averaging", "text": "In applying this technique, we modify the cost of each player to include a term | | \u03b8 \u2212 1t \u2211 t i = 1 \u03b8 [i] | | 2, where \u03b8 [i] is the value of the parameters in the past i. The historical average of the parameters can be updated online so that this learning rule can be applied well to long time series. This approach is loosely inspired by the fictional game algorithm [16], which can find balances in other game types. We found that our approach was able to find balances of low-dimensional, continuous non-convex games, such as the Minimax game, where one player controls x, the other player controls y, and the value function (f (x) \u2212 1) (y \u2212 1), where f (x) = x for x < 0 and f (x) = x2 otherwise."}, {"heading": "3.4 One-sided label smoothing", "text": "Label smoothing, a technique from the 1980s that was recently independently rediscovered by Szegedy et. al [17], replaces targets 0 and 1 for a smoothed-out classifier such as.9 or.1 and has been shown to reduce the susceptibility of neural networks to hostile examples [18]. Replacing positive classification targets with \u03b1 and negative targets with \u03b2, the optimal discriminator becomes D (x) = \u03b1pdata (x) + \u03b2pmodel (x) pdata (x) + pmodel (x).The presence of pmodel in the counter is problematic because in areas where pdata is close to zero and pmodel is large, faulty samples from pmodel have no incentive to get closer to the data. We would therefore only smooth the positive labels to \u03b1, leaving negative labels set to 0."}, {"heading": "3.5 Virtual batch normalization", "text": "Batch normalization greatly improves the optimization of neural networks and has proven to be very effective for DCGANs [3]. To avoid this problem, we introduce virtual batch normalization (VBN), in which each example x is normalized based on statistics collected on a reference battery of examples selected and fixed once at the beginning of the training, and on x itself. The reference battery is normalized only using its own statistics. VBN is computationally complex as it requires forward propagation to two minibatches of data, so we only use it on the generator network."}, {"heading": "4 Assessment of image quality", "text": "Generative adversarial networks have no objective function, making it difficult to compare the performance of different models. An intuitive performance metric can be achieved by human commentators assessing the visual quality of the samples. [2] We automate this process using Amazon Mechanical Turk (MTurk), asking commentators to distinguish between generated data and real data, and the resulting quality ratings of our models are described in Section 6. As an alternative to human commentators, we propose an automatic method to evaluate the samples that we correlate well with human ratings: We apply the Inception Model1 [19] to each generated image to obtain the conditional label distribution p (y | x). Images that contain significant objects should have a conditional label distribution p (y | x) with low entropy, we expect the results (G) to be variable."}, {"heading": "5 Semi-supervised learning", "text": "Consider a standard model for classifying a data point x into one of the possible classes. (D) Such a model takes a K-dimensional vector of the logits in x as input and output. (D) Such a model can then be converted into class probabilities by using the model of predictive distribution pmodel (y = j | x) = exp (lj) x x-dimensional vector of learning (lk). (D) Such a model is then trained by applying the interdisciplinary data losses between the observed labels and the model of predictive distribution pmodel (y = x). We can do semi-supervised learning with any standard classifier by simply adding samples from the GAN generator G to our data set by labeling them with a new \"generated\" class y = K + 1, and increasing the dimensions of our classical outputs from K + 1 accordingly."}, {"heading": "5.1 Importance of labels for image quality", "text": "The reason for this seems to be that the human visual system is highly attuned to image statistics that can help us determine the class of object that represents an image, while it is probably less sensitive to local statistics that are less important for interpreting the image. This is supported by the high correlation we find between the quality reported by human commentators and the inception score that we have developed in Section 4, which is explicitly designed to measure the \"objectivity\" of a generated image. By classifying the object depicted in the image with discriminatorD, we tend to develop an internal representation that emphasizes the same characteristics that humans emphasize. This effect can be understood as a method of transferring learning and could potentially be applied much more broadly."}, {"heading": "6 Experiments", "text": "We performed semi-supervised experiments with MNIST, CIFAR-10 and SVHN, and sample generation experiments with MNIST, CIFAR-10, SVHN and ImageNet. We provide code to reproduce the majority of our experiments.6.1 MNIST data set contains 60,000 labeled images of digits. We perform semi-supervised training with a small randomly selected fraction of them, taking into account setups with 20, 50, 100 and 200 labeled examples. The results are averaged over 10 random subsets of labeled data, each of which was selected to have a balanced number of examples from each class.The remaining training images are provided without labels. Our networks each have 5 hidden layers. We use weight normalization [20] and add Gaussian noises to output each layer of the differentiator. Table 1 summarizes our results from the learning section. We use the weight normalization [20] and add Gaussian noises to output each layer of the differentiator, which would look like a mini-generator with the help of 3."}, {"heading": "6.2 CIFAR-10", "text": "CIFAR-10 is a small, well-studied dataset of 32 x 32 natural images. We use this dataset to study semi-supervised learning, as well as to examine the visual quality of samples that can be obtained. For the discriminator in our GAN, we use a 9-layer deep sinuous network with dropout and weight normalization. The generator is a 4-layer deep CNN with batch normalization. Table 2 summarizes our results on the semi-supervised learning assignment. If we are presented with 50% real and 50% fake data generated by our best CIFAR-10 model, the MTurk users correctly categorize 78.7% of the images. However, the users of MTurk may not be sufficiently familiar with the images of CIFAR-10 or sufficiently motivated; we ourselves were able to categorize images with > 95% accuracy. We have used the above-described Inception Score by observing that the accuracy of the tower decreases from 7k to only 1.4% when the accuracy of the M1 is suggested by the use."}, {"heading": "6.3 SVHN", "text": "For the SVHN dataset, we used the same architecture and experimental setup as for CIFAR-10."}, {"heading": "6.4 ImageNet", "text": "We tested our techniques in a dataset on an unprecedented scale: 128 x 128 images from the ILSVRC2012 dataset of 1,000 categories. To our knowledge, no prior publication has applied a generative model to a dataset with such a high resolution and such a large number of object classes. The large number of object classes represents for GANs, due to their tendency to underestimate entropy in distribution, a publicly available implementation of DCGANs2 that is extensively modified using TensorFlow [26] to achieve high performance, using multi-GPU implementation. DCGANs without modification learn some basic image statistics and generate contiguous shapes with some natural color and texture, but do not learn objects. With the techniques described in this paper, GANs learn to create objects that resemble animals but have an incorrect anatomy. The results are shown in Chttps / 6.2carth.com / Dgib / Dlow."}, {"heading": "7 Conclusion", "text": "Generative adversarial networks are a promising class of generative models that have been held back so far by unstable training and the lack of an adequate evaluation metric. This work provides partial solutions to both problems. We propose several training stabilization techniques that allow us to train models that were previously untraceable. In addition, our proposed evaluation metric (the Inception Score) provides us with a basis for comparing the quality of these models. We apply our techniques to the problem of semi-supervised learning and achieve state-of-the-art results on a range of different data sets in the field of computer vision. The contributions made in this work are practical; we hope to develop a more rigorous theoretical understanding in future work."}], "references": [{"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily Denton", "Soumith Chintala", "Arthur Szlam", "Rob Fergus"], "venue": "arXiv preprint arXiv:1506.05751,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "On distinguishability criteria for estimating generative models", "author": ["Ian J Goodfellow"], "venue": "arXiv preprint arXiv:1412.6515,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Generating images with recurrent adversarial networks", "author": ["Daniel Jiwoong Im", "Chris Dongjoo Kim", "Hui Jiang", "Roland Memisevic"], "venue": "arXiv preprint arXiv:1602.05110,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Pixel-level domain transfer", "author": ["Donggeun Yoo", "Namil Kim", "Sunggyun Park", "Anthony S Paek", "In So Kweon"], "venue": "arXiv preprint arXiv:1603.07442,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Measuring statistical dependence with hilbert-schmidt norms", "author": ["Arthur Gretton", "Olivier Bousquet", "Alex Smola", "Bernhard Sch\u00f6lkopf"], "venue": "In Algorithmic learning theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Kernel measures of conditional dependence", "author": ["Kenji Fukumizu", "Arthur Gretton", "Xiaohai Sun", "Bernhard Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "A hilbert space embedding for distributions", "author": ["Alex Smola", "Arthur Gretton", "Le Song", "Bernhard Sch\u00f6lkopf"], "venue": "In Algorithmic learning theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Generative moment matching", "author": ["Yujia Li", "Kevin Swersky", "Richard S. Zemel"], "venue": "networks. CoRR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Training generative neural networks via maximum mean discrepancy optimization", "author": ["Gintare Karolina Dziugaite", "Daniel M Roy", "Zoubin Ghahramani"], "venue": "arXiv preprint arXiv:1505.03906,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Towards principled unsupervised learning", "author": ["Ilya Sutskever", "Rafal Jozefowicz", "Karol Gregor"], "venue": "arXiv preprint arXiv:1511.06440,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Unsupervised and semi-supervised learning with categorical generative adversarial networks", "author": ["Jost Tobias Springenberg"], "venue": "arXiv preprint arXiv:1511.06390,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Iterative solution of games by fictitious play", "author": ["George W Brown"], "venue": "Activity analysis of production and allocation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1951}, {"title": "Rethinking the Inception Architecture for Computer Vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Adversarial perturbations of deep neural networks", "author": ["David Warde-Farley", "Ian Goodfellow"], "venue": "Perturbations, Optimization, and Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "arXiv preprint arXiv:1512.00567,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Tim Salimans", "Diederik P Kingma"], "venue": "arXiv preprint arXiv:1602.07868,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Semi-supervised learning with deep generative models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Distributional smoothing by virtual adversarial examples", "author": ["Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Ken Nakae", "Shin Ishii"], "venue": "arXiv preprint arXiv:1507.00677,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Auxiliary deep generative models", "author": ["Lars Maal\u00f8e", "Casper Kaae S\u00f8nderby", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "arXiv preprint arXiv:1602.05473,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Semi-supervised learning with ladder networks", "author": ["Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "TensorFlow: Large-scale machine learning", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham"], "venue": "on heterogeneous systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Generative adversarial networks [1] (GANs) are a class of methods for learning generative models based on game theory.", "startOffset": 32, "endOffset": 35}, {"referenceID": 1, "context": "Recent applications of GANs have shown that they can produce excellent samples [2, 3].", "startOffset": 79, "endOffset": 85}, {"referenceID": 2, "context": "Recent applications of GANs have shown that they can produce excellent samples [2, 3].", "startOffset": 79, "endOffset": 85}, {"referenceID": 3, "context": "When used to seek for a Nash equilibrium, these algorithms may fail to converge [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "Several recent papers focus on improving the stability of training and the resulting perceptual quality of GAN samples [2, 3, 5, 6].", "startOffset": 119, "endOffset": 131}, {"referenceID": 2, "context": "Several recent papers focus on improving the stability of training and the resulting perceptual quality of GAN samples [2, 3, 5, 6].", "startOffset": 119, "endOffset": 131}, {"referenceID": 4, "context": "Several recent papers focus on improving the stability of training and the resulting perceptual quality of GAN samples [2, 3, 5, 6].", "startOffset": 119, "endOffset": 131}, {"referenceID": 5, "context": "Several recent papers focus on improving the stability of training and the resulting perceptual quality of GAN samples [2, 3, 5, 6].", "startOffset": 119, "endOffset": 131}, {"referenceID": 2, "context": "[3], as discussed below.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "1, is similar in spirit to approaches that use maximum mean discrepancy [7, 8, 9] to train generator networks [10, 11].", "startOffset": 72, "endOffset": 81}, {"referenceID": 7, "context": "1, is similar in spirit to approaches that use maximum mean discrepancy [7, 8, 9] to train generator networks [10, 11].", "startOffset": 72, "endOffset": 81}, {"referenceID": 8, "context": "1, is similar in spirit to approaches that use maximum mean discrepancy [7, 8, 9] to train generator networks [10, 11].", "startOffset": 72, "endOffset": 81}, {"referenceID": 9, "context": "1, is similar in spirit to approaches that use maximum mean discrepancy [7, 8, 9] to train generator networks [10, 11].", "startOffset": 110, "endOffset": 118}, {"referenceID": 10, "context": "1, is similar in spirit to approaches that use maximum mean discrepancy [7, 8, 9] to train generator networks [10, 11].", "startOffset": 110, "endOffset": 118}, {"referenceID": 11, "context": "Another of our proposed techniques, minibatch features, is based in part on ideas used for batch normalization [12], while our proposed virtual batch normalization is a direct extension of batch normalization.", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "Like many deep generative models, GANs have previously been applied to semi-supervised learning [13, 14], and our work can be seen as a continuation and refinement of this effort.", "startOffset": 96, "endOffset": 104}, {"referenceID": 13, "context": "Like many deep generative models, GANs have previously been applied to semi-supervised learning [13, 14], and our work can be seen as a continuation and refinement of this effort.", "startOffset": 96, "endOffset": 104}, {"referenceID": 2, "context": "[3] is well explained from this perspective.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "This approach is loosely inspired by the fictitious play [16] algorithm that can find equilibria in other kinds of games.", "startOffset": 57, "endOffset": 61}, {"referenceID": 15, "context": "al [17], replaces the 0 and 1 targets for a classifier with smoothed values, like .", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "1, and was recently shown to reduce the vulnerability of neural networks to adversarial examples [18].", "startOffset": 97, "endOffset": 101}, {"referenceID": 2, "context": "Batch normalization greatly improves optimization of neural networks, and was shown to be highly effective for DCGANs [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "One intuitive metric of performance can be obtained by having human annotators judge the visual quality of samples [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 17, "context": "As an alternative to human annotators, we propose an automatic method to evaluate samples, which we find to correlate well with human evaluation: We apply the Inception model1 [19] to every generated image to get the conditional label distribution p(y|x).", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "Our Inception score is closely related to the objective used for training generative models in CatGAN [14]: Although we had less success using such an objective for training, we find it is a good metric for evaluation that correlates very", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "[13], and we can hope to better estimate this optimal solution from the data by minimizing these two loss functions jointly.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "We use weight normalization [20] and add Gaussian noise to the output of each layer of the discriminator.", "startOffset": 28, "endOffset": 32}, {"referenceID": 19, "context": "DGN [21] 333 \u00b1 14 Virtual Adversarial [22] 212 CatGAN [14] 191 \u00b1 10 Skip Deep Generative Model [23] 132 \u00b1 7 Ladder network [24] 106 \u00b1 37 Auxiliary Deep Generative Model [23] 96 \u00b1 2 Our model 1677 \u00b1 452 221 \u00b1 136 93 \u00b1 6.", "startOffset": 4, "endOffset": 8}, {"referenceID": 20, "context": "DGN [21] 333 \u00b1 14 Virtual Adversarial [22] 212 CatGAN [14] 191 \u00b1 10 Skip Deep Generative Model [23] 132 \u00b1 7 Ladder network [24] 106 \u00b1 37 Auxiliary Deep Generative Model [23] 96 \u00b1 2 Our model 1677 \u00b1 452 221 \u00b1 136 93 \u00b1 6.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "DGN [21] 333 \u00b1 14 Virtual Adversarial [22] 212 CatGAN [14] 191 \u00b1 10 Skip Deep Generative Model [23] 132 \u00b1 7 Ladder network [24] 106 \u00b1 37 Auxiliary Deep Generative Model [23] 96 \u00b1 2 Our model 1677 \u00b1 452 221 \u00b1 136 93 \u00b1 6.", "startOffset": 54, "endOffset": 58}, {"referenceID": 21, "context": "DGN [21] 333 \u00b1 14 Virtual Adversarial [22] 212 CatGAN [14] 191 \u00b1 10 Skip Deep Generative Model [23] 132 \u00b1 7 Ladder network [24] 106 \u00b1 37 Auxiliary Deep Generative Model [23] 96 \u00b1 2 Our model 1677 \u00b1 452 221 \u00b1 136 93 \u00b1 6.", "startOffset": 95, "endOffset": 99}, {"referenceID": 22, "context": "DGN [21] 333 \u00b1 14 Virtual Adversarial [22] 212 CatGAN [14] 191 \u00b1 10 Skip Deep Generative Model [23] 132 \u00b1 7 Ladder network [24] 106 \u00b1 37 Auxiliary Deep Generative Model [23] 96 \u00b1 2 Our model 1677 \u00b1 452 221 \u00b1 136 93 \u00b1 6.", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "DGN [21] 333 \u00b1 14 Virtual Adversarial [22] 212 CatGAN [14] 191 \u00b1 10 Skip Deep Generative Model [23] 132 \u00b1 7 Ladder network [24] 106 \u00b1 37 Auxiliary Deep Generative Model [23] 96 \u00b1 2 Our model 1677 \u00b1 452 221 \u00b1 136 93 \u00b1 6.", "startOffset": 169, "endOffset": 173}, {"referenceID": 22, "context": "Ladder network [24] 20.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "47 CatGAN [14] 19.", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "We caution that the Inception score should be used as a rough guide to evaluate models that were trained via some independent criterion; directly optimizing Inception score will lead to the generation of adversarial examples [25].", "startOffset": 225, "endOffset": 229}, {"referenceID": 19, "context": "DGN [21] 36.", "startOffset": 4, "endOffset": 8}, {"referenceID": 20, "context": "10 Virtual Adversarial [22] 24.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "63 Auxiliary Deep Generative Model [23] 22.", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "86 Skip Deep Generative Model [23] 16.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "We extensively modified a publicly available implementation of DCGANs2 using TensorFlow [26] to achieve high performance, using a multi-GPU implementation.", "startOffset": 88, "endOffset": 92}], "year": 2016, "abstractText": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.", "creator": "LaTeX with hyperref package"}}}