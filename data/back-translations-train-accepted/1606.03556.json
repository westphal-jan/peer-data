{"id": "1606.03556", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2016", "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?", "abstract": "We conduct large-scale studies on `human attention' in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). We find that depending on the implementation used, machine-generated attention maps are either \\emph{negatively correlated} with human attention or have positive correlation worse than task-independent saliency. Overall, our experiments paint a bleak picture for the current generation of attention models in VQA.", "histories": [["v1", "Sat, 11 Jun 2016 05:41:10 GMT  (8091kb,D)", "http://arxiv.org/abs/1606.03556v1", "9 pages, 6 figures, 3 tables; Under review at EMNLP 2016"], ["v2", "Fri, 17 Jun 2016 04:39:01 GMT  (9100kb,D)", "http://arxiv.org/abs/1606.03556v2", "9 pages, 6 figures, 3 tables; Under review at EMNLP 2016"]], "COMMENTS": "9 pages, 6 figures, 3 tables; Under review at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["abhishek das", "harsh agrawal", "c lawrence zitnick", "devi parikh", "dhruv batra"], "accepted": true, "id": "1606.03556"}, "pdf": {"name": "1606.03556.pdf", "metadata": {"source": "CRF", "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?", "authors": ["Abhishek Das", "Harsh Agrawal", "C. Lawrence Zitnick", "Devi Parikh", "Dhruv Batra"], "emails": ["dbatra}@vt.edu", "zitnick@fb.com"], "sections": [{"heading": "1 Introduction", "text": "People have the ability to quickly perceive a scene by selectively participating in parts of the image rather than processing the whole scene as a whole (Rensink, 2000). Inspired by human attention, a current trend in computer vision and deep learning is to build computer-aided attention models. Faced with an input signal, these models learn to use parts of it for further processing and have been successfully applied in machine translation (Bahdanau et al., 2015; Firat et al., 2016), object recognition (Ba et al., 2015; Mnih et al., 2014; wear denotes that are the same. Sermanet al., 2014), image processing (Xu et al., 2015; Cho et al.) and visual question answered (Yang et al., 2015; Lu et al., 2016; Xu and Saenko, 2015; Xiong et al., 2016)."}, {"heading": "2 Related Work", "text": "Our work refers to recent work in attention-based VQA and human studies in the Saliency Prediction. We work with the freeform and open VQA dataset published by (Antol et al., 2015). VQA models. Attention-based models for VQA generally use Convolutionary Neural Networks to highlight relevant regions of the image. Hierarchical Co-Attention Network (Lu et al., 2016) generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge1 as of the time of this submission."}, {"heading": "3 VQA-HAT (Human ATtention) Dataset", "text": "Our basic interface design consists of a \"defusing exercise\" to answer visual questions. Specifically, we present subjects with a blurred image and a question about the image and ask them to sharpen areas of the image that help them answer the question correctly, in a smooth, mouse-click and drag \"colorize\" motion with the mouse. Sharpening occurs step by step: successive scrubbing of the same region sharpens them step by step. Figure 3 shows intermediate steps in our user interface for attention comments, from a completely blurred image to a blurred attention map."}, {"heading": "3.1 Attention Annotation Interface", "text": "Our user interface begins with a blurred, low-resolution version of the image, which is designed to give subjects a partial, \"holistic\" understanding of the scene so they can intelligently choose which regions to focus on. Gradual sharpening with strokes should capture the initial exploration as they try to get a better feel for the scene, and eventually focus on sharpening to answer the question. Next, we describe the three variants of our attention commentary interface we experimented with."}, {"heading": "3.1.1 Blurred Image without Answer", "text": "In our first user interface, subjects were shown a blurred image and a question with no answer, and asked to hide regions and type in the answer. We found that this user interface sometimes led to \"exploratory attention,\" where the subject slightly sharpens large areas of an image to find distinctive regions that eventually lead them to the answer. However, subjects often ended up with \"incomplete\" attention maps because they did not see the high-resolution image and the answer, i.e. did not know when to stop blurring or exploring. For example, if an image with three players playing a sport, the question is \"How many players are visible in the image?,\" the subject might sharpen a region that the player appears to have, and another region of the image that had another one, completely miss the resulting attention map is incomplete in this case, as there are three players in the image. This effect of incomplete attention maps is considered to be incomplete, and... \"(As many of these questions have been asked).\""}, {"heading": "3.1.2 Blurred Image with Answer", "text": "In our second user interface, the subjects were shown the correct answer in addition to the question and the blurred image, and were asked to sharpen as few regions as possible so that someone could answer the question by looking at the blurred image with sharpened regions. Figure 4b illustrates this user interface. If the answer remedies the errors from the first user interface, i.e. counting and binary questions, since the subjects now knew the answer, they continued to investigate until they found the answer region in the image."}, {"heading": "3.1.3 Blurred and Original Image with Answer", "text": "To facilitate exploitation, the subjects were shown the question-answer pair and the original image in full resolution in our third user interface. In principle, viewing the original image (in full resolution), the question-and-answer gives the subjects most information, enabling them to provide the \"most accurate\" attention maps. However, this task turns out to be quite counterintuitive - the subjects are shown images in full resolution and the answer and asked to imagine a scenario in which someone else must answer the question without looking at the original image. Figure 4 shows screen shots of the three user interfaces for attention annotation."}, {"heading": "3.2 Dataset Evaluation", "text": "We conducted pilot studies on AMT to experiment with the three interfaces described above. To quantify the interfaces, we conducted a second human study in which (a second group of) subjects were shown the images generated from the individual attention interfaces of the first experiment and asked to answer the question. The intuition behind this experiment is that this second group of subjects would incorrectly answer the question if the attention card revealed too little information. Table 1 shows the accuracy of the responses of human subjects under these three interfaces. We can see that the interface \"Blurred Image with Answer\" (Section 3.1.2) provides the highest accuracy in human evaluation. Since the payment structure encourages AMT to complete tasks as quickly as possible, this implicitly encourages the subjects to blur as few regions as possible, and our human study shows that humans can still answer questions, i.e. we can achieve an overall balance of many or too few."}, {"heading": "4 Human Attention Maps vs Unsupervised Attention Models", "text": "In fact, it is the case that most of them are in a position to move into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live."}, {"heading": "5 Conclusion & Discussion", "text": "We introduce the VQA-HAT dataset and publish it. This dataset can be used to evaluate attention maps generated unattended by attention-based VQA models, or to train models explicitly with attention monitoring for VQA. We quantify whether current attention-based VQA models \"look\" at the same regions of the image as humans do to provide an answer. Necessary vs. Sufficient Maps. Are human attention maps \"necessary\" and / or \"sufficient\"? If regions highlighted by the human attention maps are sufficient to precisely answer the question, then any region that is a superset is also sufficient. For example, if the attention mass focuses on a \"cat\" because \"which animal is present in the image?,\" then an attention map that assigns any large region weights to the cat, the \"region that includes the\" attention mass, \"is sufficient to answer the\" small question, and the \"region that is sufficient.\""}, {"heading": "6 Acknowledgements", "text": "We thank Jiasen Lu and Ramakrishna Vedantam for their helpful suggestions and discussions. This work was partially supported by: National Science Foundation CAREER Awards to DB and DP, Army Research Office YIP Awards to DB and DP, ICTAS Junior Faculty Awards to DB and DP, Army Research Lab Grant W911NF-15-2-0080 to DP and DB, Office of Naval Research Grant N00014-14-10679 to DB, Paul G. Allen Family Foundation Allen Distinguished Investigator Award to DP, Google Faculty Research Award to DP and DB, AWS in Education Research Grant to DB and NVIDIA GPU donation to DB."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "CoRR, abs/1601.01705", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Multiple Object Recognition With Visual Attention", "author": ["Ba et al.2015] Jimmy Lei Ba", "Volodymyr Mnih", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Describing multimedia content using attention-based encoder-decoder networks. CoRR, abs/1507.01053", "author": ["Cho et al.2015] KyungHyun Cho", "Aaron C. Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Exploring Nearest Neighbor Approaches for Image Captioning", "author": ["Devlin et al.2015] Jacob Devlin", "Saurabh Gupta", "Ross Girshick", "Margaret Mitchell", "C. Lawrence Zitnick"], "venue": "arXiv preprint", "citeRegEx": "Devlin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "What do we perceive in a glance of a real-world scene", "author": ["Fei-Fei et al.2007] Li Fei-Fei", "Asha Iyer", "Christof Koch", "Pietro Perona"], "venue": "Journal of Vision,", "citeRegEx": "Fei.Fei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2007}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Firat et al.2016] Orhan Firat", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1601.01073", "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Salicon: Saliency in context", "author": ["Jiang et al.2015] Ming Jiang", "Shengsheng Huang", "Juanyong Duan", "Qi Zhao"], "venue": "In CVPR, June", "citeRegEx": "Jiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2015}, {"title": "Learning to predict where humans look", "author": ["Judd et al.2009] Tilke Judd", "Krista Ehinger", "Fr\u00e9do Durand", "Antonio Torralba"], "venue": "In ICCV. 2,", "citeRegEx": "Judd et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Judd et al\\.", "year": 2009}, {"title": "Hierarchical Co-Attention for Visual Question Answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "ArXiv e-prints, May. 1,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Recurrent Models of Visual Attention", "author": ["Mnih et al.2014] Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "venue": "arXiv preprint", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "The dynamic representation of scenes", "author": ["Ronald A. Rensink"], "venue": "Visual Cognition,", "citeRegEx": "Rensink.,? \\Q2000\\E", "shortCiteRegEx": "Rensink.", "year": 2000}, {"title": "Attention for fine-grained categorization", "author": ["Andrea Frome", "Esteban Real"], "venue": "CoRR, abs/1412.7054", "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "The central fixation bias in scene viewing: Selecting an optimal viewing position independently of motor biases and image feature distributions", "author": ["Benjamin W. Tatler"], "venue": "Journal of Vision,", "citeRegEx": "Tatler.,? \\Q2007\\E", "shortCiteRegEx": "Tatler.", "year": 2007}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Xiong et al.2016] Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "CoRR, abs/1603.01417", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Ask, attend and answer: Exploring questionguided spatial attention for visual question answering", "author": ["Xu", "Saenko2015] Huijuan Xu", "Kate Saenko"], "venue": "CoRR, abs/1511.05234", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "CoRR, abs/1502.03044", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Yang et al.2015] Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alexander J. Smola"], "venue": "CoRR, abs/1511.02274", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Humans have the ability to quickly perceive a scene by selectively attending to parts of the image instead of processing the whole scene in its entirety (Rensink, 2000).", "startOffset": 153, "endOffset": 168}, {"referenceID": 2, "context": "Given an input signal, these models learn to attend to parts of it for further processing and have been successfully applied in machine translation (Bahdanau et al., 2014; Firat et al., 2016), object recognition (Ba et al.", "startOffset": 148, "endOffset": 191}, {"referenceID": 6, "context": "Given an input signal, these models learn to attend to parts of it for further processing and have been successfully applied in machine translation (Bahdanau et al., 2014; Firat et al., 2016), object recognition (Ba et al.", "startOffset": 148, "endOffset": 191}, {"referenceID": 15, "context": ", 2014), image captioning (Xu et al., 2015; Cho et al., 2015) and visual question answering (Yang et al.", "startOffset": 26, "endOffset": 61}, {"referenceID": 3, "context": ", 2014), image captioning (Xu et al., 2015; Cho et al., 2015) and visual question answering (Yang et al.", "startOffset": 26, "endOffset": 61}, {"referenceID": 17, "context": ", 2015) and visual question answering (Yang et al., 2015; Lu et al., 2016; Xu and Saenko, 2015; Xiong et al., 2016).", "startOffset": 38, "endOffset": 115}, {"referenceID": 9, "context": ", 2015) and visual question answering (Yang et al., 2015; Lu et al., 2016; Xu and Saenko, 2015; Xiong et al., 2016).", "startOffset": 38, "endOffset": 115}, {"referenceID": 14, "context": ", 2015) and visual question answering (Yang et al., 2015; Lu et al., 2016; Xu and Saenko, 2015; Xiong et al., 2016).", "startOffset": 38, "endOffset": 115}, {"referenceID": 4, "context": "Unlike image captioning, where a coarse understanding of an image is often sufficient for producing generic descriptions (Devlin et al., 2015), visual questions selectively target different areas of an image including background details and underlying context.", "startOffset": 121, "endOffset": 142}, {"referenceID": 17, "context": "parison of the maps generated by state-of-the-art attention-based VQA models (Yang et al., 2015; Lu et al., 2016) and a task-independent saliency baseline (Judd et al.", "startOffset": 77, "endOffset": 113}, {"referenceID": 9, "context": "parison of the maps generated by state-of-the-art attention-based VQA models (Yang et al., 2015; Lu et al., 2016) and a task-independent saliency baseline (Judd et al.", "startOffset": 77, "endOffset": 113}, {"referenceID": 8, "context": ", 2016) and a task-independent saliency baseline (Judd et al., 2009) against our human attention maps through visualizations and rank-order correlation.", "startOffset": 49, "endOffset": 68}, {"referenceID": 13, "context": "It is well understood that task-independent saliency maps have a \u2018center bias\u2019 (Tatler, 2007; Judd et al., 2009).", "startOffset": 79, "endOffset": 112}, {"referenceID": 8, "context": "It is well understood that task-independent saliency maps have a \u2018center bias\u2019 (Tatler, 2007; Judd et al., 2009).", "startOffset": 79, "endOffset": 112}, {"referenceID": 17, "context": "Stacked Attention Networks (SAN) proposed in (Yang et al., 2015) use LSTM encodings of ques-", "startOffset": 45, "endOffset": 64}, {"referenceID": 9, "context": "Hierarchical Co-Attention Network (Lu et al., 2016) generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge1 as of the time of this submission.", "startOffset": 34, "endOffset": 51}, {"referenceID": 0, "context": "Another interesting approach uses question parsing to compose the neural network from modules, attention being one of the sub-tasks addressed by these modules (Andreas et al., 2016).", "startOffset": 159, "endOffset": 181}, {"referenceID": 8, "context": "There\u2019s a rich history of work in collecting eye tracking data from human subjects to gain an understanding of image saliency and visual perception (Jiang et al., 2014; Judd et al., 2009; Fei-Fei et al., 2007; Yarbus, 1967).", "startOffset": 148, "endOffset": 223}, {"referenceID": 5, "context": "There\u2019s a rich history of work in collecting eye tracking data from human subjects to gain an understanding of image saliency and visual perception (Jiang et al., 2014; Judd et al., 2009; Fei-Fei et al., 2007; Yarbus, 1967).", "startOffset": 148, "endOffset": 223}, {"referenceID": 8, "context": "data to study natural visual exploration (Jiang et al., 2014; Judd et al., 2009) is useful but difficult and expensive to collect on a large scale.", "startOffset": 41, "endOffset": 80}, {"referenceID": 7, "context": "(Jiang et al., 2015) established mouse tracking as an accurate ap-", "startOffset": 0, "endOffset": 20}, {"referenceID": 7, "context": "While (Jiang et al., 2015) studies natural exploration and collects task-independent human annotations by", "startOffset": 6, "endOffset": 26}, {"referenceID": 17, "context": "We evaluate maps generated by the following unsupervised models: \u2022 Stacked Attention Network (SAN) (Yang et al., 2015) with a single attention layer (SAN-1)2.", "startOffset": 99, "endOffset": 118}, {"referenceID": 9, "context": "\u2022 Hierarchical Co-Attention Network (HieCoAtt) (Lu et al., 2016) with word-level (HieCoAtt-W), phrase-level (HieCoAtt-P) and question-level (HieCoAtt-Q) attention maps; we evaluate all three maps3.", "startOffset": 47, "endOffset": 64}, {"referenceID": 17, "context": "SAN-1 (Yang et al., 2015) -0.", "startOffset": 6, "endOffset": 25}, {"referenceID": 9, "context": "HieCoAtt-W (Lu et al., 2016) 0.", "startOffset": 11, "endOffset": 28}, {"referenceID": 9, "context": "004 HieCoAtt-P (Lu et al., 2016) 0.", "startOffset": 15, "endOffset": 32}, {"referenceID": 9, "context": "004 HieCoAtt-Q (Lu et al., 2016) 0.", "startOffset": 15, "endOffset": 32}, {"referenceID": 8, "context": "(Judd et al., 2009) 0.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "jiasenlu/HieCoAttenVQA where subjects are asked to freely view an image for 3 seconds (Judd et al., 2009).", "startOffset": 86, "endOffset": 105}, {"referenceID": 17, "context": "SAN-1 (Yang et al., 2015) -0.", "startOffset": 6, "endOffset": 25}, {"referenceID": 9, "context": "HieCoAtt-W (Lu et al., 2016) 0.", "startOffset": 11, "endOffset": 28}, {"referenceID": 9, "context": "012 HieCoAtt-P (Lu et al., 2016) 0.", "startOffset": 15, "endOffset": 32}, {"referenceID": 9, "context": "010 HieCoAtt-Q (Lu et al., 2016) 0.", "startOffset": 15, "endOffset": 32}, {"referenceID": 8, "context": "(Judd et al., 2009) -0.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "We conduct large-scale studies on \u2018human attention\u2019 in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by stateof-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). We find that depending on the implementation used, machine-generated attention maps are either negatively correlated with human attention or have positive correlation worse than task-independent saliency. Overall, our experiments paint a bleak picture for the current generation of attention models in VQA.", "creator": "LaTeX with hyperref package"}}}