{"id": "1606.03622", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2016", "title": "Data Recombination for Neural Semantic Parsing", "abstract": "Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a high-precision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision.", "histories": [["v1", "Sat, 11 Jun 2016 20:34:09 GMT  (84kb,D)", "http://arxiv.org/abs/1606.03622v1", "ACL 2016"]], "COMMENTS": "ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["robin jia", "percy liang"], "accepted": true, "id": "1606.03622"}, "pdf": {"name": "1606.03622.pdf", "metadata": {"source": "CRF", "title": "Data Recombination for Neural Semantic Parsing", "authors": ["Robin Jia", "Percy Liang"], "emails": ["robinjia@stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Semantic parsing - the precise translation of natural language utterances into logical forms - has many applications, including question answer (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Zettlemoyer and Barzilay, 2007), instruction following (Artzi and Zettlemoyer, 2013b), and regular expression generation (Kushman and Barzilay, 2013). Modern semantic parsers (Artzi and Zettlemoyer, 2013a; Berant et al., are complex pieces of software, requires handcrafted features, lexicons, and grammars.In the meantime, reurrent neural networks (RNNs) have swift inhics in many structured prediction tasks in NLP, including machine translation (Sutskever et al., 2014) and synthetic parsing."}, {"heading": "2 Problem statement", "text": "We throw semantic parsing as a sequence-to-equation task. The input expression x is a sequence of words x1,.., xm, V (in), the input vocabulary; similarly, the output elogic form y is a sequence of tokens y1,.., yn, V (out), the output vocabulary. A linear sequence of tokens may seem to lose the hierarchical structure of a logical form, but there are precedents for this choice: Vinyals et al. (2015b) showed that an RNN can reliably predict tree-structured outputs in linear mode. We evaluate our system using three existing semantic parsing datasets. Figure 2 shows sample input and output pairs from each of these datasets. \u2022 GeoQuery (GEO) contains natural language questions about US geography paired with corresponding prolog database queries."}, {"heading": "3 Sequence-to-sequence RNN Model", "text": "Our sequence-to-sequence RNN model is based on existing attention-based models of neuronal machine translation (Bahdanau et al., 2014; Luong et al., 2015a), but also includes a novel attention-based copy mechanism. Similar copy mechanisms have been studied in parallel by Gu et al. (2016) and Gulcehre et al. (2016)."}, {"heading": "3.1 Basic Model", "text": "The encoder transforms the input sequence x1,. >., xm into a sequence of context-sensitive embedding b1,.., bm using a bidirectional RNN (Bahdanau et al., 2014). First, a word begins with an embedding function \u03c6 (in) and maps each word xi into a fixed-dimensional vector. These vectors are fed as input into two RNNNs: a forward-directed RNN and a backward-directed RNN. The forward-directed RNN begins with an initial hidden state hF0 and generates a sequence of hidden states hF1,..., h F m by repeated application of the recurrencehFi = LSTM (in), h F i \u2212 1). (1) The repetition takes the form of an LSTM (Hochreiter und Schmidhuber, 1997). The backward-directed RNN generates hidden states Bhm,."}, {"heading": "3.2 Attention-based Copying", "text": "In the base model of the previous section, the next output word yj is chosen via a simple softmax over all words in the output vocabulary. However, this model has difficulty generalizing to the long tail of entity names commonly found in semantic parsing datasets. Conveniently, entity names in the input often correspond directly with tokens in the output (e.g. \"iowa\" becomes \"iowa\" in Figure 2). 1To grasp this intuition, we introduce a new attention-based copying mechanism. At each step j, the decoder generates one of two types of actions. As before, it can copy any word in the output vocabulary. \u2212 In addition, it can copy any word xi directly into the output, with the probability that we copy xi being determined by the attention evaluation on xi. Formally, we define a latent action aj, which can either write [w] a function for any kind of copy [w = i] or an out."}, {"heading": "4 Data Recombination", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Motivation", "text": "The most important contribution of this paper is a novel data recombination system that brings important prior knowledge to our forgotten sequence sequence RNN. In this context, we induce a high-precision generative model from the training data to generate new training examples from it. The process of generating this generative model can use any existing prior knowledge that is transferred to the RNN model by the generated examples. A major advantage of our two-step approach is that it allows us to explain desired characteristics of the task that might be difficult to capture in the model architecture. [1] On GEO and ATIS we specify a point that does not rely on the orthography of non-entities such as \"state,\" since it uses information that is not available to earlier models (Zettlemoyer and Collins, 2005) and is much less language independent."}, {"heading": "4.2 General Setting", "text": "In the general context of data recombination, we start with a training set D of (x, y) pairs that defines the empirical distribution p (x, y), then adapt a generative model p (x, y) to p that generalizes beyond the support of p, e.g. by gluing together fragments of different examples. We point to examples supporting p (x, y) as recombinant examples. Finally, to train our actual model p\u03b8 (y | x), we maximize the expected value of log p\u03b8 (y | x), drawing (x, y) from p (x, y)."}, {"heading": "4.3 SCFGs for Semantic Parsing", "text": "For semantic parsing, we induce a synchronous context-free grammar (SCFG) that serves as the backbone of our generative model p \u00fc. An SCFG consists of a series of production rules X \u2192 < \u03b1, \u03b2 >, where X is a category (non-terminal) and \u03b1 and \u03b2 are sequences of terminal and non-terminal symbols. In our case, we use an SCFG to represent common derivatives of expressions x and logical forms y (which for us is only a sequence of tokens). After deriving an SCFG G G from D, the corresponding generative model p (x, y) is used to represent a common grammar x and logical forms y (which for us is only a sequence of tokens). After deriving an SCFG G from D, the corresponding generative model p (x, y) is the distribution over pairs (x, y) defined by sampling G, where we select production rules to compare the actual data with each other."}, {"heading": "4.3.1 Abstracting Entities", "text": "Our first grammar induction strategy, ABSENTITIES, simply abstracts entities by their types. We assume that each entity e (e.g. Texas) has a corresponding type, i.e. (e.g. State), which we deduce to the existence of certain predicates in the logical form (e.g. Stateid). For each grammar rule X \u2192 < \u03b1, \u03b2 > in gin, where \u03b1 contains a symbol (e.g. \"Texas\") that corresponds to an entity (e.g. Texas) in \u03b2, we add two rules to Gout: (i) a rule where both occurrences are replaced by the type of the entity (e.g. State), and (ii) a new rule that assigns the type to the entity (e.g. STATEID \u2192 < \"Texas,\" Texas >; we reserve the category name STATE for the next section)."}, {"heading": "4.3.2 Abstracting Whole Phrases", "text": "Our second grammar rule, ABSWHOLEPHRASES, abstracts entities as well as whole phrases with their types. For each grammatical rule X \u2192 < \u03b1, \u03b2 > in gin, we add two rules to Gout. First, if \u03b1 contains symbols that match an entity in \u03b2, we replace both occurrences with the type of entity, similar to rule (i) in ABSENTITIES. Second, if we can conclude that the entire expression \u03b2 is associated with a group of a certain type (e.g. state), we create a rule that matches the type to < \u03b1, \u03b2 >. In practice, we also use some simple rules to remove question marks from \u03b1, so that the resulting examples are more natural. Let's refer again to Figure 3 for a concrete example. This strategy works on the basis of a more general conditional property of independence: The meaning of each semantically related phrase is conditional on the rest of the cornerstone, which is the most of the phenomena."}, {"heading": "4.3.3 Concatenation", "text": "For each k \u2265 2, we define the CONCAT-k strategy, which creates two types of rules. First, we create a single rule in which ROOT leads to a sequence of k SENTs. Then, for each root rule, we add ROOT \u2192 < \u03b1, \u03b2 > in gin to the rule SENT \u2192 < \u03b1, \u03b2 > to Gout. See Figure 3 for an example. Unlike ABSENTITIES and ABSWHOLEPHRASES, however, the concatenation is very general and can be applied to any sequence transduction problem. Of course, it also does not introduce additional information about compositionality or independence properties present in semantic parsing. Unlike ABSWHOLEPHRASES, however, it generates harder examples of attention-based RNA, as the model now needs to learn how to correct the more difficult parts in 2013."}, {"heading": "4.3.4 Composition", "text": "We find that grammar induction strategies can be composed, resulting in more complex grammars. If we base two grammar induction strategies f1 and f2, the composition f1 \u0445 f2 is the grammar induction strategy that Gin picks up and returns f1 (f2 (Gin). For the strategies we have defined, we can symbolically perform this operation using the grammar rules without having to select the intermediate grammar f2 (Gin)."}, {"heading": "5 Experiments", "text": "We evaluate our system in three areas: GEO, ATIS and OVERNIGHT. For ATIS, we report on the accuracy of the logical form. For GEO and OVERNIGHT, we determine the correctness based on the conformity of the names, as in Liang et al. (2011) and Wang et al. (2015)."}, {"heading": "5.1 Choice of Grammar Induction Strategy", "text": "We find that not all grammar induction strategies make sense for all areas. In particular, we apply ABSWHOLEPHRASES only to GEO and OVERNIGHT. We do not apply ABSWHOLEPHRASES to ATIS, because the data set has little nesting structure."}, {"heading": "5.2 Implementation Details", "text": "We tokenize logical forms in a domain-specific manner based on the syntax of the formal language used. On GEO and ATIS, we prohibit copying predicate names to ensure fair comparison with previous work, since string matching between input words and predicate names is not widely used. We prevent copying examples to dictate tokens; see Figure 2 for examples. On ATIS alone, when doing attention-based copying and data recombination, we use an external lexicon that links natural language phrases (e.g., \"Kennedy Airport\") to entities (e.g., MbP: ap). When we copy a word contained in the lexicon, we write the entity associated with that lexicon entry. When we recombine data, we identify the entity based on matching phrases and entities."}, {"heading": "5.3 Impact of the Copying Mechanism", "text": "First, we measure the contribution of the attention-based copy mechanism to the overall performance of the model. In each task, we train and evaluate two models: one with a copy mechanism and one without. Training takes place without data recombination. Results are shown in Table 1. For GEO and ATIS, the copy mechanism helps considerably: it improves test accuracy by 10.4 percentage points for GEO and 6.4 points for ATIS. Adding the copy mechanism actually makes our model a bit worse. This result is somewhat to be expected, as the OVERNIGHT dataset contains a very small number of different units. It is also noteworthy that both systems exceed the previous best system for OVERNIGHT by many times. We choose the copy mechanism in all subsequent experiments because it has a great advantage in realistic environments where there are many different units in the world."}, {"heading": "5.4 Main Results", "text": "2The method used by Liang et al. (2011) is not comparable to our main results, we train our model with a variety of data recombination strategies on all three sets of data. These results are summarized in Tables 2 and 3. We compare our system with the baseline not to use data recombination, as well as with state-of-the-art systems on all three sets of data. We find that data recombination consistently improves accuracy in the three areas we evaluated, and that the strongest results come from the composition of several strategies. Combining ABSWHOLEPASES, ABSENTITIES and CONCAT-2 yields a 4.3% improvement in data recombination without data recombination to 1.7%."}, {"heading": "5.5 Effect of Longer Examples", "text": "Interestingly, strategies such as ABSWHOLEPHRASES and CONCAT-2 help the model, although the resulting recombinant examples tend not to support test distribution. Specifically, these recombinant examples are, on average, longer than the actual ones, making them more difficult for the attention-based model. In fact, our best precision numbers were generally less effective. We conducted additional experiments with artificial data to investigate the importance of adding longer, harder examples."}, {"heading": "6 Discussion", "text": "In this paper we have a strong similarity to two models developed independently of each other. (2016) we use an RNN model to perform a similar task of data acquisition. (2016) we use a similar task of data acquisition. (2016) we use a semantic RNN model for semantic parsing, although they did not use data recombination. (2014) we propose a non-recursive neural model for semantic parsing, although they did not conduct experiments. (2016) we use an RNN model to perform a similar task of data acquisition. (Our proposed copying mechanisms bear a strong similarity to two models developed independently of each other. (2016) we use an RNN model to perform a similar task of appropriation of data."}], "references": [{"title": "UW SPF: The University of Washington semantic parsing framework", "author": ["Artzi", "Zettlemoyer2013a] Y. Artzi", "L. Zettlemoyer"], "venue": "arXiv preprint arXiv:1311.3011", "citeRegEx": "Artzi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2013}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics (TACL), 1:49\u201362", "author": ["Artzi", "Zettlemoyer2013b] Y. Artzi", "L. Zettlemoyer"], "venue": null, "citeRegEx": "Artzi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau et al.2014] D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Semantic parsing on Freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP)", "author": ["Berant et al.2013] J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra et al.2010] J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In Python for Scientific Computing", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["Clarke et al.2010] J. Clarke", "D. Goldwasser", "M. Chang", "D. Roth"], "venue": "In Computational Natural Language Learning (CoNLL),", "citeRegEx": "Clarke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Clarke et al\\.", "year": 2010}, {"title": "Language to logical form with neural attention. In Association for Computational Linguistics (ACL)", "author": ["Dong", "Lapata2016] L. Dong", "M. Lapata"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2016}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Dyer et al.2015] C. Dyer", "M. Ballesteros", "W. Ling", "A. Matthews", "N.A. Smith"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "A deep architecture for semantic parsing", "author": ["P. Blunsom", "N. de Freitas", "K.M. Hermann"], "venue": "In ACL Workshop on Semantic Parsing,", "citeRegEx": "Grefenstette et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2014}, {"title": "Incorporating copying mechanism in sequence-tosequence learning. In Association for Computational Linguistics (ACL)", "author": ["Gu et al.2016] J. Gu", "Z. Lu", "H. Li", "V.O. Li"], "venue": null, "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words. In Association for Computational Linguistics (ACL)", "author": ["Gulcehre et al.2016] C. Gulcehre", "S. Ahn", "R. Nallapati", "B. Zhou", "Y. Bengio"], "venue": null, "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Traversing knowledge graphs in vector space. In Empirical Methods in Natural Language Processing (EMNLP)", "author": ["Guu et al.2015] K. Guu", "J. Miller", "P. Liang"], "venue": null, "citeRegEx": "Guu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Hinton et al.2012] G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Vocal tract length perturbation (vtlp) improves speech recognition", "author": ["Jaitly", "Hinton2013] N. Jaitly", "G.E. Hinton"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "Jaitly et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jaitly et al\\.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Using semantic unification to generate regular expressions from natural language", "author": ["Kushman", "Barzilay2013] N. Kushman", "R. Barzilay"], "venue": "In Human Language Technology and North American Association", "citeRegEx": "Kushman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kushman et al\\.", "year": 2013}, {"title": "Inducing probabilistic CCG grammars from logical form with higher-order unification", "author": ["L. Zettlemoyer", "S. Goldwater", "M. Steedman"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2010}, {"title": "Lexical generalization in CCG grammar induction for semantic parsing", "author": ["L. Zettlemoyer", "S. Goldwater", "M. Steedman"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2011}, {"title": "Learning dependency-based compositional semantics", "author": ["Liang et al.2011] P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Generative oversampling for mining imbalanced datasets", "author": ["A. Liu", "J. Ghosh", "C. Martin"], "venue": "In International Conference on Data Mining (DMIN)", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015a] M. Luong", "H. Pham", "C.D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong et al.2015b] M. Luong", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Listen, attend, and walk: Neural mapping", "author": ["Mei et al.2016] H. Mei", "M. Bansal", "M.R. Walter"], "venue": null, "citeRegEx": "Mei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Uptraining for accurate deterministic question parsing", "author": ["S. Petrov", "P. Chang", "M. Ringgaard", "H. Alshawi"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Petrov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2010}, {"title": "Grounded unsupervised semantic parsing. In Association for Computational Linguistics (ACL)", "author": ["H. Poon"], "venue": null, "citeRegEx": "Poon.,? \\Q2013\\E", "shortCiteRegEx": "Poon.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["O. Vinyals", "Q.V. Le"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "2015b. Grammar as a foreign language", "author": ["Vinyals et al.2015b] O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Dropout training as adaptive regularization", "author": ["Wager et al.2013] S. Wager", "S.I. Wang", "P. Liang"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "That\u2019s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets", "author": ["Wang", "Yang2015] W.Y. Wang", "D. Yang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Building a semantic parser overnight. In Association for Computational Linguistics (ACL)", "author": ["Y. Wang", "J. Berant", "P. Liang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Learning for semantic parsing with statistical machine translation", "author": ["Wong", "Mooney2006] Y.W. Wong", "R.J. Mooney"], "venue": "In North American Association for Computational Linguistics (NAACL),", "citeRegEx": "Wong et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2006}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Wong", "Mooney2007] Y.W. Wong", "R.J. Mooney"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Wong et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2007}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["Zelle", "Mooney1996] M. Zelle", "R.J. Mooney"], "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),", "citeRegEx": "Zelle et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Zelle et al\\.", "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Zettlemoyer", "Collins2005] L.S. Zettlemoyer", "M. Collins"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Zettlemoyer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer et al\\.", "year": 2005}, {"title": "Online learning of relaxed CCG grammars for parsing to logical form", "author": ["Zettlemoyer", "Collins2007] L.S. Zettlemoyer", "M. Collins"], "venue": "In Empirical Methods in Natural Language Processing and Computational Natural Language Learning", "citeRegEx": "Zettlemoyer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zettlemoyer et al\\.", "year": 2007}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Type-driven incremental semantic parsing with polymorphism. In North American Association for Computational Linguistics (NAACL)", "author": ["Zhao", "Huang2015] K. Zhao", "L. Huang"], "venue": null, "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Semantic parsing\u2014the precise translation of natural language utterances into logical forms\u2014has many applications, including question answering (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Liang et al., 2011; Berant et al., 2013), instruction following (Artzi and Zettlemoyer, 2013b), and regular expression generation (Kushman and Barzilay, 2013).", "startOffset": 143, "endOffset": 270}, {"referenceID": 3, "context": "Semantic parsing\u2014the precise translation of natural language utterances into logical forms\u2014has many applications, including question answering (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Liang et al., 2011; Berant et al., 2013), instruction following (Artzi and Zettlemoyer, 2013b), and regular expression generation (Kushman and Barzilay, 2013).", "startOffset": 143, "endOffset": 270}, {"referenceID": 3, "context": "Modern semantic parsers (Artzi and Zettlemoyer, 2013a; Berant et al., 2013) are complex pieces of software, requiring handcrafted features, lexicons, and grammars.", "startOffset": 24, "endOffset": 75}, {"referenceID": 26, "context": "have made swift inroads into many structured prediction tasks in NLP, including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014) and syntactic parsing (Vinyals et al.", "startOffset": 100, "endOffset": 147}, {"referenceID": 2, "context": "have made swift inroads into many structured prediction tasks in NLP, including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014) and syntactic parsing (Vinyals et al.", "startOffset": 100, "endOffset": 147}, {"referenceID": 7, "context": ", 2014) and syntactic parsing (Vinyals et al., 2015b; Dyer et al., 2015).", "startOffset": 30, "endOffset": 72}, {"referenceID": 27, "context": "A linear sequence of tokens might appear to lose the hierarchical structure of a logical form, but there is precedent for this choice: Vinyals et al. (2015b) showed that an RNN can reliably predict tree-structured outputs in a linear fashion.", "startOffset": 135, "endOffset": 158}, {"referenceID": 29, "context": "We evaluate on the same train/test splits as Wang et al. (2015).", "startOffset": 45, "endOffset": 64}, {"referenceID": 5, "context": "emergence of semantic parsers learned from denotations (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Artzi and Zettlemoyer, 2013b).", "startOffset": 55, "endOffset": 147}, {"referenceID": 19, "context": "emergence of semantic parsers learned from denotations (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Artzi and Zettlemoyer, 2013b).", "startOffset": 55, "endOffset": 147}, {"referenceID": 3, "context": "emergence of semantic parsers learned from denotations (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Artzi and Zettlemoyer, 2013b).", "startOffset": 55, "endOffset": 147}, {"referenceID": 2, "context": "Our sequence-to-sequence RNN model is based on existing attention-based neural machine translation models (Bahdanau et al., 2014; Luong et al., 2015a), but also includes a novel attention-based copying mechanism.", "startOffset": 106, "endOffset": 150}, {"referenceID": 2, "context": "Our sequence-to-sequence RNN model is based on existing attention-based neural machine translation models (Bahdanau et al., 2014; Luong et al., 2015a), but also includes a novel attention-based copying mechanism. Similar copying mechanisms have been explored in parallel by Gu et al. (2016) and Gulcehre et al.", "startOffset": 107, "endOffset": 291}, {"referenceID": 2, "context": "Our sequence-to-sequence RNN model is based on existing attention-based neural machine translation models (Bahdanau et al., 2014; Luong et al., 2015a), but also includes a novel attention-based copying mechanism. Similar copying mechanisms have been explored in parallel by Gu et al. (2016) and Gulcehre et al. (2016).", "startOffset": 107, "endOffset": 318}, {"referenceID": 2, "context": ", bm using a bidirectional RNN (Bahdanau et al., 2014).", "startOffset": 31, "endOffset": 54}, {"referenceID": 2, "context": "The decoder is an attention-based model (Bahdanau et al., 2014; Luong et al., 2015a)", "startOffset": 40, "endOffset": 84}, {"referenceID": 2, "context": "Attention-based copying can be seen as a combination of a standard softmax output layer of an attention-based model (Bahdanau et al., 2014) and a Pointer Network (Vinyals et al.", "startOffset": 116, "endOffset": 139}, {"referenceID": 15, "context": "These techniques have proven effective in areas like computer vision (Krizhevsky et al., 2012) and speech recognition (Jaitly and Hinton, 2013).", "startOffset": 69, "endOffset": 94}, {"referenceID": 12, "context": "Related work has shown that training a model on more difficult examples can improve generalization, the most canonical case being dropout (Hinton et al., 2012; Wager et al., 2013).", "startOffset": 138, "endOffset": 179}, {"referenceID": 28, "context": "Related work has shown that training a model on more difficult examples can improve generalization, the most canonical case being dropout (Hinton et al., 2012; Wager et al., 2013).", "startOffset": 138, "endOffset": 179}, {"referenceID": 19, "context": "For GEO and OVERNIGHT, we determine correctness based on denotation match, as in Liang et al. (2011) and Wang et al.", "startOffset": 81, "endOffset": 101}, {"referenceID": 19, "context": "For GEO and OVERNIGHT, we determine correctness based on denotation match, as in Liang et al. (2011) and Wang et al. (2015), respectively.", "startOffset": 81, "endOffset": 124}, {"referenceID": 4, "context": "Our model is implemented in Theano (Bergstra et al., 2010).", "startOffset": 35, "endOffset": 58}, {"referenceID": 17, "context": "6 Kwiatkowski et al. (2010) 88.", "startOffset": 2, "endOffset": 28}, {"referenceID": 17, "context": "6 Kwiatkowski et al. (2010) 88.9 Liang et al. (2011) 91.", "startOffset": 2, "endOffset": 53}, {"referenceID": 17, "context": "6 Kwiatkowski et al. (2010) 88.9 Liang et al. (2011) 91.1 Kwiatkowski et al. (2011) 88.", "startOffset": 2, "endOffset": 84}, {"referenceID": 17, "context": "6 Kwiatkowski et al. (2010) 88.9 Liang et al. (2011) 91.1 Kwiatkowski et al. (2011) 88.6 82.8 Poon (2013) 83.", "startOffset": 2, "endOffset": 106}, {"referenceID": 17, "context": "6 Kwiatkowski et al. (2010) 88.9 Liang et al. (2011) 91.1 Kwiatkowski et al. (2011) 88.6 82.8 Poon (2013) 83.5 Zhao and Huang (2015) 88.", "startOffset": 2, "endOffset": 133}, {"referenceID": 9, "context": "The concurrent work of Gu et al. (2016) and Gulcehre et al.", "startOffset": 23, "endOffset": 40}, {"referenceID": 9, "context": "The concurrent work of Gu et al. (2016) and Gulcehre et al. (2016), both of whom propose similar copying mechanisms, provides additional evidence for the utility of copying on a wide range of NLP tasks.", "startOffset": 23, "endOffset": 67}, {"referenceID": 19, "context": "The method of Liang et al. (2011) is not comparable to For our main results, we train our model with a variety of data recombination strategies on all three datasets.", "startOffset": 14, "endOffset": 34}, {"referenceID": 19, "context": "the previous state-of-the-art, excluding Liang et al. (2011), which used a seed lexicon for predicates.", "startOffset": 41, "endOffset": 61}, {"referenceID": 29, "context": "Previous Work Wang et al. (2015) 46.", "startOffset": 14, "endOffset": 33}, {"referenceID": 8, "context": "Grefenstette et al. (2014) proposed a non-recurrent neural model for semantic parsing, though they did not run experiments.", "startOffset": 0, "endOffset": 27}, {"referenceID": 9, "context": "Gu et al. (2016) apply a very similar copying mechanism to text summarization and singleturn dialogue generation.", "startOffset": 0, "endOffset": 17}, {"referenceID": 9, "context": "Gu et al. (2016) apply a very similar copying mechanism to text summarization and singleturn dialogue generation. Gulcehre et al. (2016) propose a model that decides at each step whether to write from a \u201cshortlist\u201d vocabulary or copy from the input, and report improvements on machine translation and text summarization.", "startOffset": 0, "endOffset": 137}, {"referenceID": 9, "context": "Gu et al. (2016) apply a very similar copying mechanism to text summarization and singleturn dialogue generation. Gulcehre et al. (2016) propose a model that decides at each step whether to write from a \u201cshortlist\u201d vocabulary or copy from the input, and report improvements on machine translation and text summarization. Another piece of related work is Luong et al. (2015b), who train a neural machine translation system to copy rare words, relying on an external system to generate alignments.", "startOffset": 0, "endOffset": 375}, {"referenceID": 36, "context": "Zhang et al. (2015) augment their data by swapping out words for synonyms from WordNet.", "startOffset": 0, "endOffset": 20}, {"referenceID": 36, "context": "Zhang et al. (2015) augment their data by swapping out words for synonyms from WordNet. Wang and Yang (2015) use a similar strategy, but identify similar words and phrases based on cosine distance between vector space embeddings.", "startOffset": 0, "endOffset": 109}, {"referenceID": 20, "context": "Generative oversampling (Liu et al., 2007) learns a generative model in a multiclass classification setting, then uses it to generate additional examples from rare classes in order to combat label imbalance.", "startOffset": 24, "endOffset": 42}, {"referenceID": 24, "context": "Uptraining (Petrov et al., 2010) uses data labeled by an ac-", "startOffset": 11, "endOffset": 32}, {"referenceID": 27, "context": "Vinyals et al. (2015b) generate a large dataset of constituency parse trees by taking sentences that multiple existing systems parse in the same way, and train a neural model on", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "Dropout training has been shown to be a form of adaptive regularization (Hinton et al., 2012; Wager et al., 2013).", "startOffset": 72, "endOffset": 113}, {"referenceID": 28, "context": "Dropout training has been shown to be a form of adaptive regularization (Hinton et al., 2012; Wager et al., 2013).", "startOffset": 72, "endOffset": 113}, {"referenceID": 11, "context": "Guu et al. (2015) showed that encouraging a knowledge base completion model to handle longer path queries", "startOffset": 0, "endOffset": 18}], "year": 2016, "abstractText": "Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a highprecision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision.", "creator": "LaTeX with hyperref package"}}}