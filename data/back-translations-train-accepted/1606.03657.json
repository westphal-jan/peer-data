{"id": "1606.03657", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2016", "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets", "abstract": "This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.", "histories": [["v1", "Sun, 12 Jun 2016 02:14:31 GMT  (3507kb,D)", "http://arxiv.org/abs/1606.03657v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["xi chen", "xi chen", "yan duan", "rein houthooft", "john schulman", "ilya sutskever", "pieter abbeel"], "accepted": true, "id": "1606.03657"}, "pdf": {"name": "1606.03657.pdf", "metadata": {"source": "META", "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets", "authors": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "emails": [], "sections": [{"heading": null, "text": "This paper describes InfoGAN, an information theoretical extension of the Generative Adversarial Network, which is capable of learning unsupervised unbundled representations. InfoGAN is a generative adversarial network that also maximizes mutual information between a small subset of latent variables and observation. We derive a lower boundary of the mutual information target that can be efficiently optimized. In particular, InfoGAN successfully unravels writing styles of number shapes on the MNIST dataset, poses from the lighting of 3D rendered images, and background numbers from the central digit on the SVHN dataset. It also discovers visual concepts that include hairstyles, the presence / absence of glasses, and emotions on the CelebA facial dataset. Experiments show that InfoGAN learns interpretable representations that compete with representations learned through existing monitored methods."}, {"heading": "1 Introduction", "text": "It is as if it were a purely mental game, in which it was a matter of giving people the opportunity to understand the world, to understand the world and to understand it."}, {"heading": "2 Related Work", "text": "There are a large number of people who are able to move to another world in which they are able to find themselves."}, {"heading": "3 Background: Generative Adversarial Networks", "text": "Goodfellow et al. [4] introduced the Generative Adversarial Networks (GAN), a framework for training deep generative models by means of a Minimax game. The goal is to learn a generator distributionPG (x), which corresponds to the real data distribution Pdata (x). Instead of trying to explicitly assign probability to each x in the data distribution, GAN learns a generator network G, which generates samples from the generator distribution PG by transforming a noise variable z \u00b2 Pnoise (z) into a sample G (z). This generator is trained by playing against an opposing discriminator network D, which aims to distinguish between samples from the real data distribution Pdata and the generator distribution PG. Thus, for a given generator, the optimal discriminator is D (x) = Pdata (x) / (Pdata (x) + PG (x)). Formally, the Minimax game is given by the following expression: min G max (max + logV (PD-data) (PD-Ex (x))."}, {"heading": "4 Mutual Information for Inducing Latent Codes", "text": "The GAN formulation uses a simple factor input vector of z, while it does not impose any limitations on the way in which the generator can use that noise. As a result, it is possible that the noise is used by the generator in a highly intricate manner, causing the individual dimensions of z that do not correspond to the semantic characteristics of the data. However, many domains are naturally broken down into a number of semantically significant factors of variation, for example, if the generation of images from the MNIST dataset would be ideal if the model automatically chose a discrete random variable to represent the numerical identity of the digital (0-9), and it has chosen to have two more continuous variables representing the angle and thickness of the digital. It is the case that these attributes are both independent and significant, and it would be useful if we represent the numerical identity of the digital."}, {"heading": "5 Variational Mutual Information Maximization", "text": "In practice, the mutual information concept I (c; G (z, c)) is difficult to maximize because it requires access to the posterior P (c | x). Fortunately, we can reach a lower limit of this by defining an auxiliary distribution Q (c | x), around P (c; G (z, c) = H (c) \u2212 H (c) \u2212 G (z, c)) = Exxiliary distribution Q (z, c) [Ec \u00b2 distributions P (c | x) [logP (c \u2032 | x)] + H (z, c) [DKL (P (z, c) \u2022 Q (z, c)) = Exiliary distributions G (z, c) [Ec \u00b2 distributions P (c | x) [logP P (c \u00b2 x)] [logP (c \u00b2 x)] [logP (c \u00b2 x)] [logP (c \u00b2 x, c \u00b2 x)] [logP (c, c \u00b2 x)], c \u00b2 (c \u00b2 iliz | x), c (c \u00b2, c \u00b2, c, c \u00b2 c, c, c, c \u00b2, c, c \u00b2, c, c \u00b2 (c), c (c \u00b2), c \u00b2, c (c, c \u00b2, c), Ec (c \u00b2, c (c), Ec (c \u00b2), Ex (c), P (c (c), Ec (c \u00b2)"}, {"heading": "6 Implementation", "text": "In practice, we parameterize the auxiliary distribution Q as a neural network. In most experiments, Q and D share all the convolutionary layers and there is a last fully connected layer with output parameters for the conditional distribution Q (c | x), which means that InfoGAN adds negligible computational costs to GAN. We also observed that LI (G, Q) converges faster and faster than normal GAN targets, and therefore InfoGAN comes essentially free with GAN. In our experiments, we found that the natural choice of softmax nonlinearity is sufficient to represent Q (ci | x). For the continuous latent code cj, there are more options depending on what the true posterior P (cj | x) is. In our experiments, we found that simply treating Q (cj | x) as factorized Gaussian code is sufficient. Even if InfoGAN introduces an additional hyperparameter, it is easy to do."}, {"heading": "7 Experiments", "text": "The first goal of our experiments is to investigate whether mutual information can be efficiently maximized; the second goal is to assess whether InfoGAN can learn unbundled and interpretable representations by using the generator to vary only one latent factor at a time, to assess whether a variation of this factor leads to only one kind of semantic variation in generated images; DC-IGN [7] also uses this method to efficiently evaluate their learned representations on 3D image datasets, to which we also apply InfoGAN for direct comparison. 7.1 Mutual Information Maximization To assess whether mutual information between latent codes c and generated images G (z, c) can be efficiently maximized with proposed methods, we train InfoGAN on MNIST datasets with a uniform categorical distribution on latent codes c-Cat (K = 10, p = 0.1)."}, {"heading": "7.2 Disentangled Representation", "text": "In the second series of Figure 2a we can observe a digit 7 as a 9.Continuous codes c2, c3 capture continuous variations in the style of MNIST."}, {"heading": "8 Conclusion", "text": "Unlike previous approaches that required supervision, InfoGAN is completely unattended and learns interpretable and unbundled representations on challenging datasets. In addition, InfoGAN adds negligible computing costs to GAN and is easy to train. The basic idea of using mutual information to induce representation can be applied to other methods such as UAE [3], which is a promising area of future work. Other possible extensions of this work include: learning hierarchical latent representations, improving semi-supervised learning with better codes [34], and using InfoGAN as a high-dimensional tool for data discovery."}, {"heading": "A Proof of Lemma 5.1", "text": "Lemma A.1 For random variables X, Y and function f (x, y) under suitable regularity conditions: Ex \u0445 X, y \u0445 Y | x [f (x, y)] = Ex \u0445 X, y \u0445 Y | x, x \u0445 X | y [f (x \u2032, y)]."}, {"heading": "Proof", "text": "The principle of \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"Q-Phase,\" the \"X-Phase,\" the \"X-Phase,\" the \"X-Phase,\" the \"Y-Phase,\" the \"the\" X-Phase, \"the\" the \"Y-Phase,\" the \"the\" X-Phase, \"the\" the \"X-Phase,\" the \"the\" X-Phase, \"the\" the \"Y-Phase,\" the \"the\" X-Phase, \"the\" the \"Y-Phase,\" the \"the\" Y-Phase, \"the\" the \"the\" Y-Phase, \"the\" the \"the\" Y-Phase, \"the\" the Y-Phase, \"the Y-Phase,\" the Y-Phase, \"the Y-Phase,\" the \"the Y-Phase, the Y-Phase,\" the Y-Phase, the Y-Phase, the \"the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-Phase, the Y-phase, the Y-phase, the Y-phase, the Y-phase, the Y-phase, the Y-phase, the Y-phase, the Y-phase, the Y-phase, the Y-phase, the"}, {"heading": "C Experiment Setup", "text": "For all experiments, we use Adam [3] for online optimization and apply batch normalization [4] after most layers, the details of which are specified for each experiment. We use an up-convolutional architecture for the generator networks [5]. We use leaky rectified linear units (lRELU) [6] with leak rate 0.1 as nonlinearity applied to hidden layers of the disclaimer networks, and normal rectified linear units (RELU) for the generator networks. Unless otherwise noted, the learning rate is 2e-4 for D and 1e-3 for G; \u03bb is to 1. For discrete latent codes, we use a softmax nonlinearity over the corresponding units in the detection network output. For continuous latent parameterization, we parameterise the approximate posterior value through a diagonal Gaussian distribution, and the detection network gives its standard and mean deviation from the standard deviation, with the transformations shown below."}, {"heading": "C.1 MNIST", "text": "The network architectures are shown in Table 1. The discriminator D and the detection network Q share the largest part of the network. For this task we use 1 ten-dimensional categorical code, 2 consecutive latent codes and 62 disturbances, resulting in a concatenated dimension of 74."}, {"heading": "C.2 SVHN", "text": "The network architectures are shown in Table 2. The discriminator D and the detection network Q share the largest part of the network. For this task we use 4 ten-dimensional categorical codes, 4 consecutive latent codes and 124 disturbances, resulting in a concatenated dimension of 168."}, {"heading": "C.3 CelebA", "text": "The network architectures are shown in Table 3. The discriminator D and the detection network Q share the largest part of the network. For this task we use 10 ten-dimensional categorical codes and 128 disturbances, resulting in a concatenated dimension of 228."}, {"heading": "C.4 Faces", "text": "The network architectures are shown in Table 4. The discriminator D and the detection network Q share the same network and only have separate output units on the last level. For this task we use 5 consecutive latent codes and 128 disturbances, so that the input to the generator has the dimension 133."}, {"heading": "C.5 Chairs", "text": "The network architectures are in Table 6. The discriminator D and the detection network Q share the same network and have only separate output units on the last level. For this task, we use 1 continuous latent code, 3 discrete latent codes (each with dimension 20), and 128 noise variables, so that the input to the generator has dimension 189. We used separate configurations for each learned variation, as in Table 7. For this task, we found it necessary to use different regulation coefficients for the continuous and discrete latent codes."}], "references": [{"title": "The helmholtz machine", "author": ["P. Dayan", "G.E. Hinton", "R.M. Neal", "R.S. Zemel"], "venue": "Neural computation, vol. 7, no. 5, pp. 889\u2013904, 1995.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "The\" wake-sleep\" algorithm for unsupervised neural networks", "author": ["G.E. Hinton", "P. Dayan", "B.J. Frey", "R.M. Neal"], "venue": "Science, vol. 268, no. 5214, pp. 1158\u20131161, 1995.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ArXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ArXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["A. Dosovitskiy", "J. Tobias Springenberg", "T. Brox"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1538\u20131546.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "Proc. ICML, vol. 30, 2013, p. 1. 13 Table 7: The hyperparameters for Chairs dataset. Learning rate for D / Q Learning rate for G \u03bbcont \u03bbdisc Rotation 2e-4  1e-3 10.0 1.0 Width 2e-4  1e-3 0.05 2.0 14", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "A popular framework for unsupervised learning is that of representation learning [1, 2], whose goal is to use unlabelled data to learn a representation that exposes important semantic features as easily decodable factors.", "startOffset": 81, "endOffset": 87}, {"referenceID": 1, "context": "A popular framework for unsupervised learning is that of representation learning [1, 2], whose goal is to use unlabelled data to learn a representation that exposes important semantic features as easily decodable factors.", "startOffset": 81, "endOffset": 87}, {"referenceID": 1, "context": "A method that can learn such representations is likely to exist [2], and to be useful for many downstream tasks which include classification, regression, visualization, and policy learning in reinforcement learning.", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "The most prominent generative models are the variational autoencoder (VAE) [3] and the generative adversarial network (GAN) [4].", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "The most prominent generative models are the variational autoencoder (VAE) [3] and the generative adversarial network (GAN) [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "The quality of our unsupervised disentangled representation matches previous works that made use of supervised label information [5\u20139].", "startOffset": 129, "endOffset": 134}, {"referenceID": 5, "context": "The quality of our unsupervised disentangled representation matches previous works that made use of supervised label information [5\u20139].", "startOffset": 129, "endOffset": 134}, {"referenceID": 3, "context": "GANs [4] have been used by Radford et al.", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "Similarly, VAEs [5] and Adversarial Autoencoders [9] were shown to learn representations in which class label is separated from other variations.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "[4] introduced the Generative Adversarial Networks (GAN), a framework for training deep generative models using a minimax game.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "The core idea of using mutual information to induce representation can be applied to other methods like VAE [3], which is a promising area of future work.", "startOffset": 108, "endOffset": 111}], "year": 2016, "abstractText": "This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods.", "creator": "LaTeX with hyperref package"}}}