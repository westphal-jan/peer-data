{"id": "1606.03667", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2016", "title": "Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads", "abstract": "We introduce an online popularity prediction and tracking task as a benchmark task for reinforcement learning with a combinatorial, natural language action space. A specified number of discussion threads predicted to be popular are recommended, chosen from a fixed window of recent comments to track. Novel deep reinforcement learning architectures are studied for effective modeling of the value function associated with actions comprised of interdependent sub-actions. The proposed model, which represents dependence between sub-actions through a bi-directional LSTM, gives the best performance across different experimental configurations and domains, and it also generalizes well with varying numbers of recommendation requests.", "histories": [["v1", "Sun, 12 Jun 2016 05:38:20 GMT  (5418kb,D)", "http://arxiv.org/abs/1606.03667v1", null], ["v2", "Thu, 16 Jun 2016 22:31:36 GMT  (5730kb,D)", "http://arxiv.org/abs/1606.03667v2", null], ["v3", "Thu, 8 Sep 2016 06:38:20 GMT  (1281kb,D)", "http://arxiv.org/abs/1606.03667v3", "To be published in EMNLP 2016, 11 pages"], ["v4", "Sat, 17 Sep 2016 00:52:43 GMT  (1281kb,D)", "http://arxiv.org/abs/1606.03667v4", "To be published in EMNLP 2016, 11 pages"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["ji he", "mari ostendorf", "xiaodong he", "jianshu chen", "jianfeng gao", "lihong li", "li deng"], "accepted": true, "id": "1606.03667"}, "pdf": {"name": "1606.03667.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning with a Combinatorial Action Space for Predicting and Tracking Popular Discussion Threads", "authors": ["Ji He", "Mari Ostendorf", "Xiaodong He", "Jianshu Chen", "Jianfeng Gao", "Lihong Li", "Li Deng"], "emails": ["jvking@uw.edu", "ostendor@uw.edu", "xiaohe@microsoft.com", "jianshuc@microsoft.com", "jfgao@microsoft.com", "lihongli@microsoft.com", "deng@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Popularity Prediction and Tracking", "text": "Experiments are based on Reddit2, one of the largest public forums in the world. On Reddit, registered users automatically initiate a post and people respond with comments, either on the original post or one of the comments associated with it. Together, the comments and the original post form a discussion tree. Over time, more comments will appear and the tree will continue to grow. Reddit discussions are grouped into different domains, referred to as sub-credits, depending on different topics or topics. Depending on the popularity of the subredit, a post can actually receive hundreds of comments. Comments (and posts) are associated with positive and negative voices (i.e., likes and dislikes) from registered users, which are combined to obtain a karma score that can be used as a measure of popularity. An example of the top of a Reddit discussion tree is given in Figure 1. Results in red fields highlight the current karma (popularity) of each comment, and it is quite common that a lower karma comment system (e.g., the discussion system \"looks like this one)."}, {"heading": "3 Related Work", "text": "Among the most interested are deep reinforcement methods that neural networks use due to their success in dealing with large discrete governmental spaces of action. Early work such as TD-gammon used a neural network to approximate state value function (Tesauro, 1995). Recent advances in the field of deep learning (LeCun et al., 2015; Deng and Yu, 2014; Hinton et al., 2016; Krizhevskyet al., 2012; Sordoni et al., 2015) have inspired significant advances in linking deep learning with enhanced learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., processing natural language, enhancing learning has been successfully applied to dialogue systems that generate natural language and conversation with a human user (Scheffler and Young, 2002; Duan et al., 2016)."}, {"heading": "4 Characterizing a combinatorial action space", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Notation", "text": "In this sequential decision problem, the agent receives a text sequence that describes the state of st-S (i.e., \"state text\") and selects a text sequence that describes the action in \"A\" (i.e. \"action text\"), with \"S\" and \"A\" each denoting the state and scope of action. Here, we assume that \"A\" is selected from a series of given candidates. In our case, both \"S\" and \"A\" are described by natural language, and the agent receives a reward rt + 1 for that particular transition. We can select the best action to maximize his long-term reward. Then, the ambient state of \"st\" + 1 = s \"is updated according to a probability p (s\" | s, a), and the agent receives a reward rt + 1 for that particular transition. We can define action value function (i.e. Q function) Q (s, a) as the expected return of \"s\" and the execution of the action a."}, {"heading": "4.2 Q-function alternatives", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets and Experimental Configurations", "text": "Our data consists of 5 subreddits (askscience, askmen, todayilearned, worldnews, nfl) with different topics and genres. In our experiments, we filter out discussion trees with fewer than 100 comments to have long enough discussion threads. For each subreddit, we randomly distribute 90% of the data for online training and 10% of the data for testing (deployment). The basic subreddit statistics are presented in Table 1. In all of our experiments, we set N = 10. Explicit representation of all N-choose-K actions requires a lot of memory and does not scale. We therefore use 3Upper bounds by carefully scanning each discussion tree to determine that K comments are not scanned once in real time."}, {"heading": "K Random Upper bound", "text": "A variant of Q-Learning: When we take the maximum over possible next actions, we instead randomly examine m \u2032 actions and take the maximum over them. We use m \u2032 = 10 during our experiments. This heuristic technique works well in our experiments. For text preprocessing, we remove punctuation and uppercase letters in lowercase letters. With respect to the Q-Learning agent, we use fully networked neural networks for text embedding. The network has L = 2 hidden layers, each with 20 nodes, and model parameters are initialized with small random numbers. -Greedily is used for exploration exploitation, and we hold = 0.1 throughout the online training and testing. We select the discontraction factor rate = 0.9. During the online training, we use explicit experiential comments (Lin 1993) and the memory DRDR1 are compared with multiple episodes DR00st-1 each."}, {"heading": "5.2 Experimental Results", "text": "In Figure 3, we provide learning curves of different models on the askscience subreddit during online learning. In this experiment, we set N = 10, K = 3. Each curve is achieved by averaging 3 independent runs, and the error bars are also shown. All models start with random performance and converge after about 15 repetitions of experience. The DRRN sum converges as fast as base models, with better converged performance. DRRNBiLSTM converts slower than other methods, but with the best converged performance. After we have trained all models on the training set, we set the model parameters and apply (implement) to the test set, where the models predict what action to take, but no reward is shown until evaluation. Test performance is averaged over 1000 episodes, and we report mean and standard deviations over 5 independent runs."}, {"heading": "K DRRN-Sum DRRN-BiLSTM", "text": "On askscience, we try several settings with N = 10, K = 2, 3, 4, 5 and the results are indeed shown in Table 4. Both DRRN-Sum and DRRN-BiLSTM consistently outperform baseline methods. The DRRNBiLSTM performs better with larger K, probably due to the greater chance of redundancy in combining more sub-actions.We also conduct online training and tests on different sub-runs. With N = 10, K = 3, the test success gains are shown across the linear baseline in Figure 4. Again, the test performance is averaged over 1000 episodes, and we report mean and standard deviations over 5 independent runs. The results are consistent with those for askscience. Since different sub-rates have very different karma distributions and language styles, this suggests that the algorithms are applicable to different text genres."}, {"heading": "6 Conclusion", "text": "In this post, we present a new learning task associated with predicting and tracking popular threads on Reddit. States and actions are all described in natural language, making the task useful for language studies. Subsequently, we develop novel deep-Q learning architectures to better model the value function of the state with a combinatorial action space. The proposed DRRN BiLSTM method not only performs better across different experimental configurations and domains, but also generalizes well for scenarios where the user can request changes in the number tracked. This work represents a first step toward solving the popularity prediction and tracking problem. Although the system's performance exceeds several baselines, it still lags far behind the oracle result. Previous work has shown that timing is an important factor in predicting popularity (Lamp and Resech, 2004; Jaeger, 2015 and any other reactions suggested in this post)."}], "references": [{"title": "Topic detection and tracking: event-based information organization, volume 12", "author": ["J. Allan"], "venue": null, "citeRegEx": "Allan.,? \\Q2012\\E", "shortCiteRegEx": "Allan.", "year": 2012}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["H. Chen", "L. Zettlemoyer", "R. Barzilay"], "venue": "In Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th IJCNLP,", "citeRegEx": "Branavan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2009}, {"title": "Learning to win by reading manuals in a monte-carlo framework", "author": ["D. Silver", "R. Barzilay"], "venue": "In Proc. of the Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Branavan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2011}, {"title": "Deep learning: Methods and applications", "author": ["Deng", "Yu2014] L. Deng", "D. Yu"], "venue": "Foundations and Trends in Signal Processing,", "citeRegEx": "Deng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2014}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan et al.2016] Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML)", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Deep reinforcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679", "author": ["R. Evans", "H. van Hasselt", "P. Sunehag", "T. Lillicrap", "J. Hunt"], "venue": null, "citeRegEx": "Dulac.Arnold et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dulac.Arnold et al\\.", "year": 2016}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Schmidhuber2005] A. Graves", "J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Multiagent planning with factored mdps", "author": ["Guestrin et al.2001] C. Guestrin", "D. Koller", "R. Parr"], "venue": "In NIPS,", "citeRegEx": "Guestrin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2001}, {"title": "Deep reinforcement learning in parameterized action space", "author": ["Hausknecht", "Stone2016] M. Hausknecht", "P. Stone"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Hausknecht et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2016}, {"title": "Deep reinforcement learning with an action space defined by natural language. arXiv preprint arXiv:1511.04636", "author": ["He et al.2015] J. He", "J. Chen", "X. He", "J. Gao", "L. Li", "L. Deng", "M. Ostendorf"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research", "author": ["Hinton et al.2012] G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Talking to the crowd: What do people react to in online discussions", "author": ["Jaech et al.2015] A. Jaech", "V. Zayats", "H. Fang", "M. Ostendorf", "H. Hajishirzi"], "venue": null, "citeRegEx": "Jaech et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaech et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["I. Sutskever", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Slash(dot) and burn: distributed moderation in a large online conversation space", "author": ["Lampe", "Resnick2004] C. Lampe", "P. Resnick"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,", "citeRegEx": "Lampe et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lampe et al\\.", "year": 2004}, {"title": "Continuous control with deep reinforcement learning", "author": ["J. J Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Reinforcement learning for robots using neural networks", "author": ["L-J. Lin"], "venue": "Technical report, DTIC Document", "citeRegEx": "Lin.,? \\Q1993\\E", "shortCiteRegEx": "Lin.", "year": 1993}, {"title": "Twittermonitor: trend detection over the twitter stream", "author": ["Mathioudakis", "Koudas2010] M. Mathioudakis", "N. Koudas"], "venue": "In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,", "citeRegEx": "Mathioudakis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mathioudakis et al\\.", "year": 2010}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih et al.2015] V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. A Rusu", "J. Veness", "M. G Bellemare", "A. Graves", "M. Riedmiller", "A. K Fidjeland", "G. Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Language understanding for textbased games using deep reinforcement learning", "author": ["T. Kulkarni", "R. Barzilay"], "venue": "In Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Improving information extraction by acquiring external evidence with reinforcement learning", "author": ["A. Yala", "R. Barzilay"], "venue": "arXiv preprint arXiv:1603.07954", "citeRegEx": "Narasimhan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2016}, {"title": "Webnav: A new large-scale task for natural language based sequential decision making", "author": ["Nogueira", "Cho2016] R. Nogueira", "K. Cho"], "venue": "arXiv preprint arXiv:1602.02261", "citeRegEx": "Nogueira et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nogueira et al\\.", "year": 2016}, {"title": "Reinforcement learning with factored states and actions", "author": ["Sallans", "Hinton2004] B. Sallans", "G. E Hinton"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Sallans et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sallans et al\\.", "year": 2004}, {"title": "Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning", "author": ["Scheffler", "Young2002] K. Scheffler", "S. Young"], "venue": null, "citeRegEx": "Scheffler et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Scheffler et al\\.", "year": 2002}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["Silver et al.2016] D. Silver", "A. Huang", "C. J Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Reinforcement learning for spoken dialogue systems", "author": ["Singh et al.1999] S. P Singh", "M. J Kearns", "D. J Litman", "M. A Walker"], "venue": "In NIPS,", "citeRegEx": "Singh et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1999}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Sordoni et al.2015] A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J.-Y. Nie", "J. Gao", "B. Dolan"], "venue": "NAACL-HLT", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Temporal difference learning and td-gammon", "author": ["G. Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro.", "year": 1995}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Wen et al.2016] T.-H. Wen", "M. Gasic", "N. Mrksic", "L. M Rojas-Barahona", "P.-H. Su", "S. Ultes", "D. Vandyke", "S. Young"], "venue": "arXiv preprint arXiv:1604.04562", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "An exploration of discussion threads in social news sites: A case study of the reddit community", "author": ["Weninger et al.2013] T. Weninger", "X.A. Zhu", "J. Han"], "venue": "In Advances in Social Networks Analysis and Mining (ASONAM),", "citeRegEx": "Weninger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weninger et al\\.", "year": 2013}, {"title": "Linear submodular bandits and their application to diversified retrieval", "author": ["Yue", "Guestrin2011] Y. Yue", "C. Guestrin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yue et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 26, "context": "Early work such as TD-gammon used a neural network to approximate the state value function (Tesauro, 1995).", "startOffset": 91, "endOffset": 106}, {"referenceID": 10, "context": "Recent advances in deep learning (LeCun et al., 2015; Deng and Yu, 2014; Hinton et al., 2012; Krizhevsky et al., 2012; Sordoni et al., 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al.", "startOffset": 33, "endOffset": 140}, {"referenceID": 12, "context": "Recent advances in deep learning (LeCun et al., 2015; Deng and Yu, 2014; Hinton et al., 2012; Krizhevsky et al., 2012; Sordoni et al., 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al.", "startOffset": 33, "endOffset": 140}, {"referenceID": 25, "context": "Recent advances in deep learning (LeCun et al., 2015; Deng and Yu, 2014; Hinton et al., 2012; Krizhevsky et al., 2012; Sordoni et al., 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al.", "startOffset": 33, "endOffset": 140}, {"referenceID": 17, "context": ", 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016).", "startOffset": 93, "endOffset": 176}, {"referenceID": 23, "context": ", 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016).", "startOffset": 93, "endOffset": 176}, {"referenceID": 14, "context": ", 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016).", "startOffset": 93, "endOffset": 176}, {"referenceID": 4, "context": ", 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016).", "startOffset": 93, "endOffset": 176}, {"referenceID": 24, "context": "In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016).", "startOffset": 167, "endOffset": 232}, {"referenceID": 27, "context": "In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016).", "startOffset": 167, "endOffset": 232}, {"referenceID": 1, "context": "There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011).", "startOffset": 162, "endOffset": 208}, {"referenceID": 2, "context": "There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011).", "startOffset": 162, "endOffset": 208}, {"referenceID": 17, "context": "Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework.", "startOffset": 10, "endOffset": 35}, {"referenceID": 9, "context": "He et al. (2015) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interac-", "startOffset": 0, "endOffset": 17}, {"referenceID": 18, "context": "Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve in-", "startOffset": 0, "endOffset": 25}, {"referenceID": 7, "context": "valued parameters; factored Markov Decision Process (MDP) (Guestrin et al., 2001; Sallans and Hinton, 2004) assume certain independence between a next-state component and a sub-action.", "startOffset": 58, "endOffset": 107}, {"referenceID": 7, "context": "valued parameters; factored Markov Decision Process (MDP) (Guestrin et al., 2001; Sallans and Hinton, 2004) assume certain independence between a next-state component and a sub-action. As for bandits setting, Yue and Guestrin (2011) considered diversification of multi-item recommendation, but their methodology is limited to using linear approximation with hand-crafted features.", "startOffset": 59, "endOffset": 233}, {"referenceID": 9, "context": "two separate deep neural networks for modeling state embedding and action embedding, performs better than per-action DQN (PA-DQN) in Figure 2(a), as well as other DQN variants for dealing with large action spaces(He et al., 2015).", "startOffset": 212, "endOffset": 229}, {"referenceID": 16, "context": "To handle a large state space, Mnih et al. (2015) proposed a Deep Q-Network (DQN).", "startOffset": 31, "endOffset": 50}, {"referenceID": 15, "context": "During online training, we use experience replay (Lin, 1993) and the memory size is set to 10000 tuples of (st, at, rt+1, st+1).", "startOffset": 49, "endOffset": 60}, {"referenceID": 11, "context": "Prior work has shown that timing is an important factor in predicting popularity (Lampe and Resnick, 2004; Jaech et al., 2015), and all the proposed models would benefit from incorporating this information.", "startOffset": 81, "endOffset": 126}], "year": 2017, "abstractText": "We introduce an online popularity prediction and tracking task as a benchmark task for reinforcement learning with a combinatorial, natural language action space. A specified number of discussion threads predicted to be popular are recommended, chosen from a fixed window of recent comments to track. Novel deep reinforcement learning architectures are studied for effective modeling of the value function associated with actions comprised of interdependent sub-actions. The proposed model, which represents dependence between sub-actions through a bi-directional LSTM, gives the best performance across different experimental configurations and domains, and it also generalizes well with varying numbers of recommendation requests.", "creator": "LaTeX with hyperref package"}}}