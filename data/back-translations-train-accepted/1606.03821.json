{"id": "1606.03821", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Learning to Generate Compositional Color Descriptions", "abstract": "The production of color language is essential for grounded language generation. Color descriptions have many challenging properties: they can be vague, compositionally complex, and denotationally rich. We present an effective approach to generating color descriptions using recurrent neural networks and a Fourier-transformed color representation. Our model outperforms previous work on a conditional language modeling task over a large corpus of naturalistic color descriptions. In addition, probing the model's output reveals that it can accurately produce not only basic color terms but also descriptors with non-convex denotations (\"greenish\"), bare modifiers (\"bright\", \"dull\"), and compositional phrases (\"faded teal\") not seen in training.", "histories": [["v1", "Mon, 13 Jun 2016 06:17:32 GMT  (625kb,D)", "https://arxiv.org/abs/1606.03821v1", "5 pages, 4 figures, 3 tables. Under review at EMNLP 2016"], ["v2", "Tue, 18 Oct 2016 18:28:12 GMT  (626kb,D)", "http://arxiv.org/abs/1606.03821v2", "6 pages, 4 figures, 3 tables. EMNLP 2016"]], "COMMENTS": "5 pages, 4 figures, 3 tables. Under review at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["will monroe", "noah d goodman", "christopher potts"], "accepted": true, "id": "1606.03821"}, "pdf": {"name": "1606.03821.pdf", "metadata": {"source": "CRF", "title": "Learning to Generate Compositional Color Descriptions", "authors": ["Will Monroe", "Noah D. Goodman", "Christopher Potts"], "emails": ["wmonroe4@cs.stanford.edu,", "cgpotts}@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Color descriptions represent a microcosm of grounded language semantics. Basic color terms like \"red\" and \"blue\" provide a set of semantic building blocks in a continuous space of meaning; people also use compositional color descriptions to express meanings that are not covered by basic terms, such as \"greenish blue\" or \"the color of the rust on my aunt's old Chevrolet\" (Berlin and Kay, 1991). We consider color descriptions to be a basic language modeling problem. We present an effective new model for this task that uses long-term short-term memory (LSTM) and captions (Kulkarni et al., 2011; Mitchell et al., 2012), among other problems of grounded language generation."}, {"heading": "2 Model formulation", "text": "Formally, a model of the color description generation is a probability distribution S (d | c) over sequences ofar Xiv: 160 6.03 821v 2 [cs.C L] 18 October 2tokens d conditioned by a color c, where c is represented as a three-dimensional real vector in HSV space.1Architecture Our main model is a recurring neural network sequence decoder (Fig. 1, left panel).An input color c = (h, s, v) is mapped to a representation f (see Color features, below).In each time step, the model takes a concatenation of f and an embedding for the previous output token di, starting with the start token d0 =. This concatenated vector is guided through an LSTM layer, using the formulation of Graves (2013)."}, {"heading": "3 Experiments", "text": "We demonstrate the effectiveness of our model using the same data and statistical modeling metrics as McMahan and Stone (2015).Data The data set used to train and evaluate our model consists of pairs of colors and descriptions collected in an open online survey (Munroe, 2010).Participants were asked to write a free-form description of the color in a text field. McMahan and Stone filtered the answers to normalize spelling differences and exclude spam responses and descriptions that very seldom occur.The resulting data sets contain 2,176,417 pairs divided in training (1,523,108), development (108,545) and test (544,764) sets.Metrics We quantify the effectiveness of the model using the following evaluation metrics metrics: \u2022 The perplexity of the geometric mean of the reciprocessional probability assigned by the model."}, {"heading": "4 Analysis", "text": "In fact, in a country where most people are able to feed themselves, you feel able to find a new home."}, {"heading": "5 Conclusion and future work", "text": "We have presented a model for generating compositional color descriptions that is capable of creating new descriptions that are not seen in training and clearly surpass previous work in conditional language modeling. [3] A natural extension is the use of sequence models at the drawing level to capture complex morphology (e.g. \"-ish\" in \"greenish\"). Kawakami et al. (2016) build character-level models to predict colors in addition to describing colors. Their model uses a labspace color representation and uses the color to initialize the LSTM rather than feed it into each time step; they also focus on visualizing point predictions of their description-to-color model while we investigate the full distributions implied by our color tooling model. Another extension that we want to examine is context modeling to capture how colors differ to describe them in order to contrast them with other colors: http.us / dnubl.com /"}, {"heading": "Acknowledgments", "text": "We thank Jiwei Li, Jian Zhang, Anusha Balakrishnan and Daniel Ritchie for valuable advice and discussion. This research was partially supported by the Stanford Data Science Initiative, NSF BCS 1456077 and NSF IIS 1159679."}], "references": [{"title": "A new look at the statistical model identification", "author": ["Hirotugu Akaike."], "venue": "IEEE Transactions on Automatic Control, 19(6):716\u2013723.", "citeRegEx": "Akaike.,? 1974", "shortCiteRegEx": "Akaike.", "year": 1974}, {"title": "Theano: A Python framework for fast computation of mathematical expressions. arXiv preprint arXiv:1605.02688", "author": ["Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi", "Christof Angermueller", "Dzmitry Bahdanau", "Nicolas Ballas"], "venue": null, "citeRegEx": "Al.Rfou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2016}, {"title": "Basic color terms: Their universality and evolution", "author": ["Brent Berlin", "Paul Kay."], "venue": "University of California Press.", "citeRegEx": "Berlin and Kay.,? 1991", "shortCiteRegEx": "Berlin and Kay.", "year": 1991}, {"title": "Managing ambiguities across utterances in dialogue", "author": ["David DeVault", "Matthew Stone."], "venue": "Ron Artstein and Laure Vieu, editors, Proceedings of DECALOG 2007: Workshop on the Semantics and Pragmatics of Dialogue.", "citeRegEx": "DeVault and Stone.,? 2007", "shortCiteRegEx": "DeVault and Stone.", "year": 2007}, {"title": "Lasagne: First release", "author": ["Sander Dieleman", "Jan Schl\u00fcter", "Colin Raffel", "Eben Olson", "S\u00f8ren Kaae S\u00f8nderby", "Daniel Nouri"], "venue": null, "citeRegEx": "Dieleman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "AISTATS.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "A game-theoretic approach to generating spatial descriptions", "author": ["Dave Golland", "Percy Liang", "Dan Klein."], "venue": "EMNLP.", "citeRegEx": "Golland et al\\.,? 2010", "shortCiteRegEx": "Golland et al\\.", "year": 2010}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov."], "venue": "arXiv preprint arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Character sequence models for colorful words", "author": ["Kazuya Kawakami", "Chris Dyer", "Bryan Routledge", "Noah A. Smith."], "venue": "EMNLP.", "citeRegEx": "Kawakami et al\\.,? 2016", "shortCiteRegEx": "Kawakami et al\\.", "year": 2016}, {"title": "Computational generation of referring expressions: A survey", "author": ["Emiel Krahmer", "Kees Van Deemter."], "venue": "Computational Linguistics, 38(1):173\u2013218.", "citeRegEx": "Krahmer and Deemter.,? 2012", "shortCiteRegEx": "Krahmer and Deemter.", "year": 2012}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C. Berg"], "venue": null, "citeRegEx": "Kulkarni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "A Bayesian model of grounded color semantics", "author": ["Brian McMahan", "Matthew Stone."], "venue": "Transactions of the Association for Computational Linguistics, 3:103\u2013 115.", "citeRegEx": "McMahan and Stone.,? 2015", "shortCiteRegEx": "McMahan and Stone.", "year": 2015}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["Margaret Mitchell", "Xufeng Han", "Jesse Dodge", "Alyssa Mensch", "Amit Goyal", "Alex Berg"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "Learning in the Rational Speech Acts model", "author": ["Will Monroe", "Christopher Potts."], "venue": "Proceedings of the 20th Amsterdam Colloquium.", "citeRegEx": "Monroe and Potts.,? 2015", "shortCiteRegEx": "Monroe and Potts.", "year": 2015}, {"title": "Color survey results", "author": ["Randall Munroe."], "venue": "Online at http://blog.xkcd.com/2010/05/03/color-surveyresults.", "citeRegEx": "Munroe.,? 2010", "shortCiteRegEx": "Munroe.", "year": 2010}, {"title": "User\u2019s guide to sigf: Significance testing by approximate randomisation. http://www.nlpado.de/~sebastian/ software/sigf.shtml", "author": ["Sebastian Pad\u00f3"], "venue": null, "citeRegEx": "Pad\u00f3,? \\Q2006\\E", "shortCiteRegEx": "Pad\u00f3", "year": 2006}, {"title": "Shape-based image retrieval using generic Fourier descriptor", "author": ["Dengsheng Zhang", "Guojun Lu."], "venue": "Signal Processing: Image Communication, 17(10):825\u2013 848.", "citeRegEx": "Zhang and Lu.,? 2002", "shortCiteRegEx": "Zhang and Lu.", "year": 2002}], "referenceMentions": [{"referenceID": 2, "context": "Basic color terms like \u201cred\u201d and \u201cblue\u201d provide a rich set of semantic building blocks in a continuous meaning space; in addition, people employ compositional color descriptions to express meanings not covered by basic terms, such as \u201cgreenish blue\u201d or \u201cthe color of the rust on my aunt\u2019s old Chevrolet\u201d (Berlin and Kay, 1991).", "startOffset": 304, "endOffset": 326}, {"referenceID": 13, "context": "The production of color language is essential for referring expression generation (Krahmer and Van Deemter, 2012) and image captioning (Kulkarni et al., 2011; Mitchell et al., 2012), among other grounded language generation problems.", "startOffset": 135, "endOffset": 181}, {"referenceID": 15, "context": "The production of color language is essential for referring expression generation (Krahmer and Van Deemter, 2012) and image captioning (Kulkarni et al., 2011; Mitchell et al., 2012), among other grounded language generation problems.", "startOffset": 135, "endOffset": 181}, {"referenceID": 10, "context": "an effective new model for this task that uses a long short-term memory (LSTM) recurrent neural network (Hochreiter and Schmidhuber, 1997; Graves, 2013) and a Fourier-basis color representation inspired by feature representations in computer vision.", "startOffset": 104, "endOffset": 152}, {"referenceID": 8, "context": "an effective new model for this task that uses a long short-term memory (LSTM) recurrent neural network (Hochreiter and Schmidhuber, 1997; Graves, 2013) and a Fourier-basis color representation inspired by feature representations in computer vision.", "startOffset": 104, "endOffset": 152}, {"referenceID": 14, "context": "We compare our model with LUX (McMahan and Stone, 2015), a Bayesian generative model of color semantics.", "startOffset": 30, "endOffset": 55}, {"referenceID": 8, "context": "This concatenated vector is passed through an LSTM layer, using the formulation of Graves (2013). The output of the LSTM at each step is passed through a fully-connected layer, and a softmax nonlinearity is applied to produce a probability distribution for the following token.", "startOffset": 83, "endOffset": 97}, {"referenceID": 4, "context": "Our implementation uses Lasagne (Dieleman et al., 2015), a neural network library based on Theano (Al-Rfou et al.", "startOffset": 32, "endOffset": 55}, {"referenceID": 1, "context": ", 2015), a neural network library based on Theano (Al-Rfou et al., 2016).", "startOffset": 50, "endOffset": 72}, {"referenceID": 19, "context": "The Fourier representation is inspired by the use of Fourier feature descriptors in computer vision applications (Zhang and Lu, 2002).", "startOffset": 113, "endOffset": 133}, {"referenceID": 5, "context": "Training We train using Adagrad (Duchi et al., 2011) with initial learning rate \u03b7 = 0.", "startOffset": 32, "endOffset": 52}, {"referenceID": 9, "context": "1, hidden layer size and cell size 20, and dropout (Hinton et al., 2012) with a rate of 0.", "startOffset": 51, "endOffset": 72}, {"referenceID": 6, "context": "Dense weights use normalized uniform initialization (Glorot and Bengio, 2010).", "startOffset": 52, "endOffset": 77}, {"referenceID": 14, "context": "We demonstrate the effectiveness of our model using the same data and statistical modeling metrics as McMahan and Stone (2015).", "startOffset": 102, "endOffset": 127}, {"referenceID": 17, "context": "Data The dataset used to train and evaluate our model consists of pairs of colors and descriptions collected in an open online survey (Munroe, 2010).", "startOffset": 134, "endOffset": 148}, {"referenceID": 14, "context": "HM and LUX are from McMahan and Stone (2015). We reimplemented HM and re-ran LUX from publicly available code, confirming all results to the reported precision except perplexity of LUX, for which we obtained a figure of 13.", "startOffset": 20, "endOffset": 45}, {"referenceID": 14, "context": "We follow McMahan and Stone (2015) in reporting perplexity per-description, not per-token as in the language modeling literature.", "startOffset": 10, "endOffset": 35}, {"referenceID": 0, "context": "\u2022 AIC: The Akaike information criterion (Akaike, 1974) is given by AIC = 2` + 2k, where ` is log likelihood and k is the total number of real-valued parameters of the model (e.", "startOffset": 40, "endOffset": 54}, {"referenceID": 18, "context": "Improvements are highly significant on all metrics (p < 0.001, approximate permutation test, R = 10,000 samples; Pad\u00f3, 2006).", "startOffset": 51, "endOffset": 124}, {"referenceID": 14, "context": "Our best model outperforms both the histogram baseline (HM) and the improved LUX model of McMahan and Stone (2015), obtaining state-of-the-art results on this task.", "startOffset": 90, "endOffset": 115}, {"referenceID": 14, "context": "In particular, our model addresses the shortcoming identified by McMahan and Stone (2015) that their model cannot capture non-convex denotations.", "startOffset": 65, "endOffset": 90}, {"referenceID": 11, "context": "Kawakami et al. (2016) build character-level models for predicting colors given descriptions in addition to describing colors.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "pragmatic reasoning (DeVault and Stone, 2007; Golland et al., 2010; Monroe and Potts, 2015).", "startOffset": 20, "endOffset": 91}, {"referenceID": 7, "context": "pragmatic reasoning (DeVault and Stone, 2007; Golland et al., 2010; Monroe and Potts, 2015).", "startOffset": 20, "endOffset": 91}, {"referenceID": 16, "context": "pragmatic reasoning (DeVault and Stone, 2007; Golland et al., 2010; Monroe and Potts, 2015).", "startOffset": 20, "endOffset": 91}], "year": 2016, "abstractText": "The production of color language is essential for grounded language generation. Color descriptions have many challenging properties: they can be vague, compositionally complex, and denotationally rich. We present an effective approach to generating color descriptions using recurrent neural networks and a Fouriertransformed color representation. Our model outperforms previous work on a conditional language modeling task over a large corpus of naturalistic color descriptions. In addition, probing the model\u2019s output reveals that it can accurately produce not only basic color terms but also descriptors with non-convex denotations (\u201cgreenish\u201d), bare modifiers (\u201cbright\u201d, \u201cdull\u201d), and compositional phrases (\u201cfaded teal\u201d) not seen in training.", "creator": "LaTeX with hyperref package"}}}