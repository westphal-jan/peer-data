{"id": "1606.03841", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Efficient Learning with a Family of Nonconvex Regularizers by Redistributing Nonconvexity", "abstract": "The use of convex regularizers allow for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, for a large class of nonconvex regularizers, we propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex regularizer, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the standard proximal algorithm and Frank-Wolfe algorithm). Moreover, it can be shown that critical points of the transformed problem are also critical points of the original problem. Extensive experiments on a number of nonconvex regularization problems show that the proposed procedure is much faster than the state-of-the-art nonconvex solvers.", "histories": [["v1", "Mon, 13 Jun 2016 07:21:31 GMT  (482kb,D)", "https://arxiv.org/abs/1606.03841v1", "Full version of ICML 2016 paper with same title"], ["v2", "Thu, 20 Oct 2016 05:57:58 GMT  (446kb,D)", "http://arxiv.org/abs/1606.03841v2", null], ["v3", "Mon, 13 Feb 2017 01:27:29 GMT  (2689kb,D)", "http://arxiv.org/abs/1606.03841v3", "Journal version of previous conference paper appeared at ICML-2016 with same title"]], "COMMENTS": "Full version of ICML 2016 paper with same title", "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["quanming yao", "james t kwok"], "accepted": true, "id": "1606.03841"}, "pdf": {"name": "1606.03841.pdf", "metadata": {"source": "CRF", "title": "Efficient Learning with a Family of Nonconvex Regularizers by Redistributing Nonconvexity", "authors": ["Quanming Yao", "James T. Kwok"], "emails": ["QYAOAA@CSE.UST.HK", "JAMESK@CSE.UST.HK"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "Notation", "text": "We denote vectors and matrices with lowercase and uppercase letters. For a vector x-Rd, the diag (x) results in a diagonal matrix x x-d with Xii = xi. for a matrix X-Rm \u00b7 n (where m \u2264 n without loss of generality) the diagonal matrix X-Rd \u00b7 d with Xii = xi. for a square matrix X (X) s are the singular values of X, and its Frobenius norm is a positive semidefinitive. For two matrices X-R is < X-R = 1X 2 ij, and for a square matrix X, j | Xij. For a square matrix X-S + indicates that it is a positive semidefinitive. For two matrices X and Y < X, > i-J, Yj, for a smooth function (f) it is (f)."}, {"heading": "2. Related Works", "text": "In this section we will look at some popular solution algorithms (1), where f is considered smooth."}, {"heading": "2.1 Convex-Concave Procedure (CCCP)", "text": "The convex-concave method (CCCP) (Yuille and Rangarajan, 2002; Lu, 2012) is a popular and general solver for (1).It assumes that F can be decomposed as the difference of the convex (DC) functions (Hiriart-Urruty, 1985), i.e., F (x) = F (x) + F (x), where F (x) is convex and F (st) is concave. In any CCCP iteration, F (xt) is linearized, and xt + 1 is asxed + 1 = arg min x F (x) + F (xt) - (xt) \u2212 x \u2212 st, (st) > st: [\u2212 F (xt)]] is a subgradient. Note that since the last two terms are linear (2), the convex form of the convex is and may be simpler than the original F."}, {"heading": "2.2 Proximal Algorithm", "text": "The proximal algorithm (Parikh and Boyd, 2013) was popularly used for optimization problems of the form in (1). Let f be convex and L-tip smooth, and g is convex. However, the proximal algorithm produces iterate (xt) asxt + 1 = arg min x f (xt) + (x \u2212 xt) > f (xt) = proximal algorithms converge at a rate of O (1 / T). This can be further accelerated to O (1 / T 2) by modifying the generation of {xt} as (Beck, 2009; Nesterov, 2013): The proximal algorithms converge at a rate of O (1 / T)."}, {"heading": "2.3 Frank-Wolfe (FW) Algorithm", "text": "The FW algorithms are used to solve these problems. (1) The FW algorithms are used to solve these problems. (2) The FW algorithms are used to solve these problems. (3) The FW algorithms are used to solve problems. (3) The FW algorithms are used to solve problems. (4) The FW algorithms are used to solve problems. (5) The FW algorithms are used to solve problems. (6) The FW algorithms are used to find solutions. (4) The FW algorithms are used to solve problems. (4) The FW algorithms are used to solve problems. (5) The FW algorithms are often simply solved. (6) The FW algorithms are sought for finding solutions. (4) The FW algorithms are used to solve problems. (4) The FW algorithms are used to solve problems. (6) The FW algorithms are often simply solved."}, {"heading": "2.4 Alternating Direction Method of Multipliers (ADMM)", "text": "ADMM is a simple but powerful algorithm, first introduced in the 1970s (Glowinski and Marroco, 1975), which can be used to solve optimization problems of the form x, y f (x) + g (y): Ax + By = c, (13), where f, g are convex functions, and A, B (or c) are constant matrices (or vector) of appropriate size. Consider the augmented Lagrangian L (x, y, u) = f (x) + g (y) + u > (Ax) + 0 converted functions, and A, B (or c) are converted matrices (or vector) of appropriate size. Consider the augmented Lagrangian L (x, y, u) = f (x) ariable (y) + u > (Ax)."}, {"heading": "3. Shifting Nonconvexity from Regularizer to Loss", "text": "In recent years, a number of non-konvexen regulators has been proposed, examples of which are: (1), (2), (2), (2), (2), (3), (3), (3), (3), (3), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5, (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, ("}, {"heading": "4. Example Use Cases", "text": "In this section, we show concrete examples of how the proposed convexification scheme can be used with various optimisation algorithms. An overview is summarised in Table 2."}, {"heading": "4.1 Proximal Algorithms", "text": "The proximal algorithm has often been used for learning with convex regulators (Parikh and Boyd, 2013); with a non-convex regulator, the underlying proximal step becomes much more difficult. Gong et al. (2013); Li and Lin (2015) and Bot et al. (2016) extended the proximal algorithm to simple non-convex g, but cannot handle more complicated non-convex regulators such as the tree-structured lasso regulator (Liu and Ye, 2010; Schmidt et al., 2011), but cannot handle more complicated non-convex regulators such as the tree-structured lasso regulator (Nikolova, 2004); with the proximal average (Bauschke et al, 2008), Zhong and Rangwok (2014), non-convex regulators of the form of the convex law (Jacob et al, 2009) and total variation (Nikolova, 2004)."}, {"heading": "4.1.1 NONCONVEX SPARSE GROUP LASSO", "text": "Let's say the group Gj contains dimensions in x, which contains group j. Leave [xGj] i = xi, if i-Gj, and 0 otherwise. Given learning samples {(a1, y1),.., (aN, yN)}, (convex) sparse group lasso is formulated as (Jacob et al., 2009): min x N, i = 1 '(yi, a > i x) + 1 + K, 1 + K, x, xGj, 2, (22), where \"a smooth loss is x, and K is the number of (non-overlapping) groups. For the non-convex expansion, the regulator can becomesg (x) = 1, x, x x x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x"}, {"heading": "4.1.2 NONCONVEX TREE-STRUCTURED GROUP LASSO", "text": "In the (convex) tree-structured group (Lilasso and Yu, 2010; Jenatton et al., 2011) the dimensions are organized in nodes in one tree, and each group corresponds to a partial tree. The regularizer is of the form \u2211 K = 1 \u03bbj and Lin, 2015). 1: Initialization z1 = x1 = x0, \u03b10 = 0, \u03b11 = 1, [0, 1), c1 = F (x1), q1 = 1, and step size of the tree is 1 = 1, and step size of the tree is 1 = 1 = 1, and step size of the tree is 1 = 1."}, {"heading": "4.1.3 NONCONVEX TOTAL VARIATION (TV) REGULARIZER", "text": "In a picture, nearby pixels are usually strongly correlated. The TV regulator captures such behavior by assuming that changes between adjacent pixels are small. (In view of an image X-Rm \u00b7 n, the TV regulator becomes TV (X) = 1-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z"}, {"heading": "4.2 Frank-Wolfe Algorithm", "text": "In this section, we use the Frank Wolfe algorithm to learn a low matrix. (1) Recently, however, there has been a growing interest in replacing this with non-convex regulators (Lu et al., 2014; Yao et al., 2015; Gui et al., 2016). Note that the FW algorithm is not directly to (8), as its linear subproblem in (10) then becomes minS. (1) If the identity function (26) is reduced to (8), we cannot be used directly to (8), as its linear subproblem in (10)."}, {"heading": "4.3 Alternating Direction Method of Multipliers (ADMM)", "text": "In this section, we will consider the use of ADMM for the consensus optimization problem (16). If all fi's and g's are convex, ADMM has a convergence rate of O (1 / T) (He and Yuan, 2012). Lately, ADMM has been extended to problems where g's are convex, but fi's are nonconvex (Hong et al., 2016). If g's is non-convex, e.g. when a nonconvex regulator is used in regularized risk minimization, ADMM convergence is still an open review problem (Hong et al, 2016)."}, {"heading": "4.4 Stochastic Variance Reduced Gradient", "text": "To accelerate the often slow convergence of stochastic gradients, variance reduction methods (SVRG) (Johnson and Zhang, 2013) and their proximal extension Prox-SVRG (Xiao and Zhang, 2014) are commonly used for the following optimization problems: \"is a smooth convex loss function i = 1\" (yi, a > i) + g (x), (38) where {(a1, y1),.. (aN, yN)} are training samples, \"is a smooth convex loss function, and g is a convex convex regulator. Recently, Prox-SVRG has been extended to non-convex targets as well. Reddi et al (2016a) and Zhu and Hazan (2016) are used as smooth convergence (convex), but without this being further extended to the convex and hazan."}, {"heading": "4.5 With OWL-QN", "text": "In this section we look at OWL-QN (Andrew and Gao, 2007) and its variant mOWL-QN (Gong and Ye, 2015b), which are efficient algorithms for the \"1 regulation problem.\" (39) Recently, however, Gong and Ye (2015a) have proposed a non-convex generalization for (39), in which the standard \"1 regulator is replaced by the non-convex g (x). (39): min x f (x) + \u00b5 verifying information (x). (40) Gong and Ye (2015a) have proposed a complex algorithm (HONOR), which includes a combination of quasi-newton and gradient descending steps (xi). Although the algorithm is similar to OWL-QN and mOWL-QN."}, {"heading": "4.6 Nonsmooth and Nonconvex Loss", "text": "The optimization problem becomes even more difficult, and many existing algorithms cannot be used. In particular, the proximal algorithm f in (1) requires to be smooth (possibly not convex) (Gong et al., 2013; Li and Lin, 2015; Bot et al., 2016). The FW algorithm requires f in (4) to be smooth and convex (Jaggi, 2013). For the ADMM, it allows f to be smooth in the consensus problem, but g must be convex (Hong et al., 2016). In case of problems with the form minx, z f (y) + g (y): y = Ax, ADMM it requires a complete sequence (Li et al., 2016). As will be shown, it is not satisfactory for problems of the form minx, z f (y) + g (y)."}, {"heading": "4.6.1 TOTAL VARIATION IMAGE DENOISING", "text": "Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-"}, {"heading": "4.6.2 ROBUST SPARSE CODING", "text": "The second application is the robust sparse encoding used in Volksmund for facial recognition (Yang et al., 2011), image analysis (Lu et al., 2013) and background modeling (Zhao et al., 2011). In view of an observed signal y-Rm, the goal is to search for a robust sparse representation x-Rd by y based on the dictionary D-Rm \u00b7 d (which is adopted here as a fix). Mathematically, it is formulated as the following optimization problem: min x-y-Dx-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "5. Experiments", "text": "In this section, we conduct experiments to apply the proposed method with (i) proximal algorithms (Section 5.1 and 5.2); (ii) Frank Wolfe algorithm (Section 5.3); (iii) comparison with HONOR (Section 5.4) and (vi) image denoising (Section 5.5). Experiments are performed on a PC with Intel i7 CPU and 32 GB of memory. All algorithms are implemented in Matlab."}, {"heading": "5.1 Nonconvex Sparse Group Lasso", "text": "In this section we perform experiments in which we set 75% of the groups to zero. In this experiment we use the LSP regulators in Table 1 (with 3%) as a result (with 0.5%).The synthetic data set is generated as follows: Let d = 10000. Let d = 10000. The truth is divided into 100 non-overlapping groups: {1,.,.,, 100}, {101,.,.,., 200},.,.,.,.,.,.,.,.,.,.,.,.,.,....,.....,....,........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "5.2 Nonconvex Tree-Structured Group Lasso", "text": "In this section, we will conduct experiments on the non-convex tree-structured group lasso model in Section 4.1.2. In addition, we will use the facial data set JAFFE1, which contains 213 images with seven facial expressions: anger, disgust, fear, happy, neutral, sadness and surprise. According to (Liu and Ye, 2010), we will resize each image from 256 x 256 to 64 x 64. Their tree structure, which is based on pixel environments, is also used here. The total number of groups K is 85. Since our goal is only the usefulness of the proposed conversion scheme, we will focus on the binary classification problem \"anger against non-anger\" (with 30 anger images and 183 non-anger images). Logistic loss is used, which is more suitable for classification. Given training samples {(a1, y1), (aN, yN)}, which is optimization problem."}, {"heading": "5.3 Nonconvex Low-Rank Matrix Completion", "text": "In this section we conduct experiments on non-convex low-level matrix trades (Section 4.2), with square loss in (26). LSP regulators are used to evaluate the matrix trades (Hsieh and Olsen, 2014; Yao et al., 2015). They contain valuations {1, 2} submitted by different users on movies.The proposed Frank Wolfe method (Algorithm 4), the N2C-FW, is compared with the following algorithms: 1. FaNCL et al, 2015): This is a newer non-convex matrix algorithm."}, {"heading": "5.4 Comparison with HONOR", "text": "In this section, we compare the proposed method experimentally with HONOR (Section 4.5) on the model in (40), using the logistical loss and the LSP regulator. Following (Gong and Ye, 2015a), we fix \u00b5 = 1 in (40) and \u03b8 in the LSP regulator to 0.01 \u00b5. Experiments are performed on three large data sets, kdd2010a, kdd2010b and url 3 (Table 7). Both kdd2010a and kdd2010b are educational data sets, and the task is to predict the successful attempts of students to predict concepts related to algebra.3. https: / www.csie.ntu.edu.tw / \u02dc cjlin / libsvmtools / datasets / binary.htmlThe Url data set contains a collection of websites, and the task is to predict whether a particular website is malicious."}, {"heading": "5.5 Image Denoising", "text": "In this area, we have the opportunity to see how they are able to hide themselves, and how they are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "6. Conclusion", "text": "In this paper, we proposed a novel approach to learning with non-convex regularizers. By shifting the non-convexity associated with the non-convex regularizer to loss, the non-convex regularizer is convexed to become a familiar convex regularizer, while the increased loss is still smooth. This allows to reuse efficient algorithms that were originally designed for convex regularization of the transformed problem. To illustrate the benefits of the proposed transformation, we put them into many popular optimization algorithms. First, we look at the proximal algorithm and show that the proximal step, if it does not have a closed solution, is expensive for the original problem, but will be much easier for the transformed problem. Second, we combine the proposed convex-empifying algorithm with the Frank Wolearic-based algorithm that will be low-cost for the proposed problem."}, {"heading": "Appendix A. Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proposition 1", "text": "\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "A.2 Corollary 2", "text": "Proof From sentence 1 and the definition of g-i we can see that it is concave. Then g-i is for each x, y, q-h (Aix) -h (Aiy) -22 \u2264 4\u03c12-Aix-Aiy-22 \u2264 4\u03c12-Ai-2F-x-y-22. Thus g-i is 2\u043c-Ai-F-Lipschitz smooth."}, {"heading": "A.3 Corollary 3", "text": "Proof It is easy to see that g-i (x) = \u03ba0-Ki = 1 \u00b5i-i-Aix-2 is convex but not smooth. In episode 2, g-i is also concave and Lipschitz-smooth, since each g-i is concave and Lipschitz-smooth."}, {"heading": "A.4 Proposition 5", "text": "Proof First we perform a few lemmas.Definition 19 (Bertsekas, 1999) A function f: Rm \u2192 R is absolutely symmetrical if f ([x1;.; xm]) = f ([| x\u03c0 (1) |...; x\u03c0 (m) |] for any permutation. Lemma 20 (Lewis and Sendov, 2005) Let f (X) = [\u03c31 (X);.; \u03c3m (X)] be the vector, the singular value of X. For an absolute symmetrical function f: Rm \u2192 R, \u03c6 (X): f (Lewis (X)) is concave to X if and only if f is concave. From the definition of g: in (21), g: (X), g: (X), g: (m \u00b2), i = symmetrical function f: Rm \u00b2."}, {"heading": "A.5 Proposition 6", "text": "Evidence First, we introduce the following Lemma.Lemma 22 (Boyd and Vandenberghe, 2004) \u03c6 (x) = \u03c0 (x) is convex if \u03c0 convex, not decreasing and q convex is.Lie q (x) = x (2) and \u03c0 (\u03b1) = \u03c0 (\u03b1) + \u03c12\u03b1 2 if \u03b1 (\u03b2) = 0. Thus, \u03c6 (x) = \u03c0 (q (x)) = \u03b1 (2) + \u03c12 x 2. Obviously, q is convex. For \u03b2 = 0, 0 \u2264 (\u03b2) = 0. Thus, \"Lipschitz is smooth\" (\u03b2) \u2212 ice \"(\u03b1). This results in \u03c0\" (v) - con vex = (\u03b2). + \u0421\u0430 \"Convex\" (\u00df) = 0, 0 \u2264 (\u03b2). Thus, \"Lipschitz is smooth.\""}, {"heading": "A.6 Theorem 7", "text": "The proof that we are not able to pay our debts has not yet been provided. \u2212 Should you not be able to repay your debts? \u2212 Should you not be able to repay your debts? \u2212 Should you not be able to repay your debts? \u2212 Should you not be able to repay your debts? \u2212 Should you not be able to repay your money? \u2212 Should you be unable? \u2212 Should you not be able? \u2212 Should you not be able? \u2212 Should you not be able? \u2212 Should you not be able? \u2212 Should you not be able. \u2212 Should you not be able to repay your debts? \u2212 Should you not be able? \u2212 Should you not be able. \u2212 Should you not be able? \u2212 Should you not be able. \u2212 Should you not be able? \u2212 Should you not be able. \u2212 Should you not be able. \u2212 Should you not be able. \u2212 Should you not be able. \u2212 Should you not be able. \u2212 Should you not be able. \u2212 Should you not be able. \u2212 Should you not be able? \u2212 should not be able. \u2212 Should you not be able? \u2212 should not be able. \u2212 Should you not be able. \u2212 Should you not be able? \u2212 should not be able. \u2212 should not be able."}, {"heading": "A.7 Proposition 8", "text": "Credits From (67) x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t1 x t2 x t2 x t2 x t2 x t2 x t2 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x 2 x t2 x 2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t1 x t1 x t1 x t1 x t1 x t1 x x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t2 x t1 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t2 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x t1 x"}, {"heading": "A.8 Proposition 9", "text": "Proof note of (28) that \"f\" (S) = \"f\" (S) + \"g\" (S). Application of the matrix chain rule, since \"S\" = \"f\" (S) = \"f\" (S) = \"f\" (S), \"S\" = \"f\" (S), \"S\" = \"f\" (S) >. Likewise since \"S\" = \"f\" (S) > \"t\" (S) = \"f\" (S), \"S\" = \"f\" (S), \"f\" (S) > t, \"f\" (S) = \"f\" (S) >. As \"g\" (S) = \"mi\" (S) = \"f\" (S) \u2212 \"0\" (S), using \"Lemma 21\" f \"(X) =\" f \"(S) +\" USDiag (w) V > S \"and\" wi \"=\" f \"(S) \u2212\""}, {"heading": "A.9 Corollary 10", "text": "Note that the SVD of X (UUB) is Diag ([\u03c31 (B),..., \u03c3k (B)] (V VB) >. Use Lemma 21, \u0435f (X) = \u0435f (X) + \u0394g (X) = \u0435f (X) + \u00b5 (UUB) Diag (w) (V VB) >.where w-Rk is wi = \u0445 (\u03c3i (B) \u2212 \u03ba0."}, {"heading": "A.10 Proposition 11", "text": "The proof that g-Diag (X) is defined on singular values of the input matrix X, only UBV > and B need have exactly the same singular values. Suppose SVD of B = UBDiag (B)) V > B, where \u03c3 (B) = [\u03c31 (B),..., \u03c3m (B)]. Since U and V are orthogonal, it is easy to view Diag (\u03c3 (B)) (VBV) > as SVD of X."}, {"heading": "A.11 Theorem 12", "text": "Proposition 24 (Mishra et al., 2013) For a square matrix X, let sym (X) = 12 (X + X >) The optimal first order conditions for (33) are for (X) V B-U sym (U > f-sym (X) V B) + \u00b5 f-sym (U > f-sym (X) V B) = 0, sym (U > f-sym (X) V) + \u00b5 I = 0.Proposition 25 (27) If (27) has a critical point with rank-r, choose matrix size of U-Rm \u00b7 r and B's Sr \u00b7 r +, then any critical points of (33) is also critical point of (27).Proof subdifferential of the nuclear norm can be received as (Watson, 1992)."}, {"heading": "Appendix B. Details in Section 5.5", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 CCCP", "text": "Proposal 6 allows us to apply the following procedures: the decomposition is convex and the decomposition is concave; we can apply the above decomposition to (43) and obtain a DCdecomposition: asF (X) = m = 1 n = 1 n = 1 x ([Y \u2212 X] ij) + m \u2212 1 m = 1 m = 1 x ([DvX] ij) + m = 1 m = 1 x ([XDh] ij), F (X) = 1 n = 1 x ([Y \u2212 X] ij) + m = 1 x ([Y \u2212 X] ij) + m \u2212 1 x (DvX] ij)."}, {"heading": "B.2 Smoothing", "text": "Since the LSP function is used as a model, one can obtain a smoothed version of it by defining it as follows: \u03ba \u03bb (x) = \u03b2 log (1 + h\u03bb (x) \u03b8), where h\u03bb (x) = [Y \u2212 X] ij) + \u00b5 m \u2212 1 \u03bb ([DvX] ij) + \u00b5 n \u2211 (X) = m \u2211 i = 1 \u0445 ([XDh] ij). Then the gradient departure is used for optimization (Chen, 2012). Specifically, we must create a sequence of partial problems {F-1 (X), F-2 (X),.} with the aim of minimizing a sequence of F-2 (X) (F-2), and with the aim of generating a sequence of F-2 (F-2)."}], "references": [{"title": "Scalable training of `1-regularized log-linear models", "author": ["G. Andrew", "J. Gao"], "venue": "In Proceedings of the 24th International Conference on Machine learning,", "citeRegEx": "Andrew and Gao.,? \\Q2007\\E", "shortCiteRegEx": "Andrew and Gao.", "year": 2007}, {"title": "The proximal average: Basic theory", "author": ["H. Bauschke", "R. Goebel", "Y. Lucet", "X. Wang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Bauschke et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bauschke et al\\.", "year": 2008}, {"title": "Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems", "author": ["A. Beck", "M. Teboulle"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Beck and Teboulle.,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2009}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A.M. Beck", "Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Beck and Teboulle.,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2009}, {"title": "Nonlinear Programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas.", "year": 1999}, {"title": "Parallel and Distributed Computation", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Numerical Methods. Prentice-Hall,", "citeRegEx": "Bertsekas and Tsitsiklis.,? \\Q1989\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis.", "year": 1989}, {"title": "An inertial forward-backward algorithm for the minimization of the sum of two nonconvex functions", "author": ["R.I. Bot", "E. Robert Csetnek", "S.C. L\u00e1szl\u00f3"], "venue": "EURO Journal on Computational Optimization,", "citeRegEx": "Bot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bot et al\\.", "year": 2016}, {"title": "Online learning and stochastic approximations", "author": ["L. Bottou"], "venue": "On-line Learning in Neural Networks,", "citeRegEx": "Bottou.,? \\Q1998\\E", "shortCiteRegEx": "Bottou.", "year": 1998}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "A generalized conditional gradient method and its connection to an iterative shrinkage method", "author": ["K. Bredies", "D.A. Lorenz", "P. Maass"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Bredies et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bredies et al\\.", "year": 2009}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["E.J. Cand\u00e8s", "M.B. Wakin", "S. Boyd"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2008}, {"title": "Robust principal component analysis", "author": ["E.J. Cand\u00e8s", "X. Li", "Yi Ma", "J. Wright"], "venue": "Journal of the ACM,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Smoothing methods for nonsmooth, nonconvex minimization", "author": ["X. Chen"], "venue": "Mathematical Programming,", "citeRegEx": "Chen.,? \\Q2012\\E", "shortCiteRegEx": "Chen.", "year": 2012}, {"title": "Image denoising by sparse 3-D transformdomain collaborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Dabov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dabov et al\\.", "year": 2007}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Donoho.,? \\Q2006\\E", "shortCiteRegEx": "Donoho.", "year": 2006}, {"title": "Applied Mathematics: Body and Soul: Volume 1: Derivatives and Geometry in IR3", "author": ["K. Eriksson", "F. Estep", "C. Johnson"], "venue": null, "citeRegEx": "Eriksson et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Eriksson et al\\.", "year": 2004}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["J. Fan", "R. Li"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Fan and Li.,? \\Q2001\\E", "shortCiteRegEx": "Fan and Li.", "year": 2001}, {"title": "An algorithm for quadratic programming", "author": ["M. Frank", "P. Wolfe"], "venue": "Naval Research Logistics,", "citeRegEx": "Frank and Wolfe.,? \\Q1956\\E", "shortCiteRegEx": "Frank and Wolfe.", "year": 1956}, {"title": "Nonlinear image recovery with half-quadratic regularization", "author": ["D. Geman", "C. Yang"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Geman and Yang.,? \\Q1995\\E", "shortCiteRegEx": "Geman and Yang.", "year": 1995}, {"title": "Accelerated gradient methods for nonconvex nonlinear and stochastic programming", "author": ["S. Ghadimi", "G. Lan"], "venue": "Mathematical Programming,", "citeRegEx": "Ghadimi and Lan.,? \\Q2016\\E", "shortCiteRegEx": "Ghadimi and Lan.", "year": 2016}, {"title": "Sur l\u2019approximation, par \u00e9l\u00e9ments finis d\u2019ordre un, et la r\u00e9solution, par p\u00e9nalisation-dualit\u00e9 d\u2019une classe de probl\u00e8mes de dirichlet non lin\u00e9aires. Revue fran\u00e7aise d\u2019automatique, informatique, recherche op\u00e9rationnelle", "author": ["R. Glowinski", "A. Marroco"], "venue": "Analyse nume\u0301rique,", "citeRegEx": "Glowinski and Marroco.,? \\Q1975\\E", "shortCiteRegEx": "Glowinski and Marroco.", "year": 1975}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "Golub and Loan.,? \\Q2012\\E", "shortCiteRegEx": "Golub and Loan.", "year": 2012}, {"title": "HONOR: Hybrid Optimization for NOn-convex Regularized problems", "author": ["P. Gong", "J. Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gong and Ye.,? \\Q2015\\E", "shortCiteRegEx": "Gong and Ye.", "year": 2015}, {"title": "A modified orthant-wise limited memory quasi-Newton method with convergence analysis", "author": ["P. Gong", "J. Ye"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Gong and Ye.,? \\Q2015\\E", "shortCiteRegEx": "Gong and Ye.", "year": 2015}, {"title": "A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems", "author": ["P. Gong", "C. Zhang", "Z. Lu", "J. Huang", "J. Ye"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Towards faster rates and oracle property for low-rank matrix estimation", "author": ["H. Gui", "J. Han", "Q. Gu"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Gui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gui et al\\.", "year": 2016}, {"title": "On the o(1/n) convergence rate of the douglas-rachford alternating direction method", "author": ["B. He", "X. Yuan"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "He and Yuan.,? \\Q2012\\E", "shortCiteRegEx": "He and Yuan.", "year": 2012}, {"title": "Generalized differentiability, duality and optimization for problems dealing with differences of convex functions", "author": ["J.B. Hiriart-Urruty"], "venue": "Convexity and Duality in Optimization,", "citeRegEx": "Hiriart.Urruty.,? \\Q1985\\E", "shortCiteRegEx": "Hiriart.Urruty.", "year": 1985}, {"title": "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems", "author": ["M. Hong", "Z.-Q. Luo", "M. Razaviyayn"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Hong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hong et al\\.", "year": 2016}, {"title": "Nuclear norm minimization via active subspace selection", "author": ["C.-J. Hsieh", "P. Olsen"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Hsieh and Olsen.,? \\Q2014\\E", "shortCiteRegEx": "Hsieh and Olsen.", "year": 2014}, {"title": "Group lasso with overlap and graph lasso", "author": ["L. Jacob", "G. Obozinski", "J.-P. Vert"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Jacob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2009}, {"title": "Revisiting Frank-Wolfe: Projection-free sparse convex optimization", "author": ["M. Jaggi"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Jaggi.,? \\Q2013\\E", "shortCiteRegEx": "Jaggi.", "year": 2013}, {"title": "Proximal methods for hierarchical sparse coding", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jenatton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2011}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "A hybrid algorithm for convex semidefinite optimization", "author": ["S. Laue"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Laue.,? \\Q2012\\E", "shortCiteRegEx": "Laue.", "year": 2012}, {"title": "Nonsmooth analysis of singular values", "author": ["A.S. Lewis", "H.S. Sendov"], "venue": "Part ii: Applications. SetValued Analysis,", "citeRegEx": "Lewis and Sendov.,? \\Q2005\\E", "shortCiteRegEx": "Lewis and Sendov.", "year": 2005}, {"title": "Global convergence of splitting methods for nonconvex composite optimization", "author": ["G. Li", "T.K. Pong"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Li and Pong.,? \\Q2015\\E", "shortCiteRegEx": "Li and Pong.", "year": 2015}, {"title": "Accelerated proximal gradient methods for nonconvex programming", "author": ["H. Li", "Z. Lin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Li and Lin.,? \\Q2015\\E", "shortCiteRegEx": "Li and Lin.", "year": 2015}, {"title": "Moreau-Yosida regularization for grouped tree structure learning", "author": ["J. Liu", "J. Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Liu and Ye.,? \\Q2010\\E", "shortCiteRegEx": "Liu and Ye.", "year": 2010}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Online robust dictionary learning", "author": ["C. Lu", "J. Shi", "J. Jia"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Lu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2013}, {"title": "Generalized nonconvex nonsmooth low-rank minimization", "author": ["C. Lu", "J. Tang", "S. Yan", "Z. Lin"], "venue": "In Proceedings of the International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Lu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2014}, {"title": "Generalized singular value thresholding", "author": ["C. Lu", "C. Zhu", "C. Xu", "S. Yan", "Z. Lin"], "venue": "In Proceedings of the 29th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Sequential convex programming methods for a class of structured nonlinear programming", "author": ["Z. Lu"], "venue": "Preprint arXiv:1210.3039,", "citeRegEx": "Lu.,? \\Q2012\\E", "shortCiteRegEx": "Lu.", "year": 2012}, {"title": "Online dictionary learning for sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["R. Mazumder", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "Low-rank optimization with trace norm penalty", "author": ["B. Mishra", "G. Meyer", "F. Bach", "R. Sepulchre"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Mishra et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mishra et al\\.", "year": 2013}, {"title": "Gradient methods for minimizing composite functions", "author": ["Y. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov.,? \\Q2013\\E", "shortCiteRegEx": "Nesterov.", "year": 2013}, {"title": "Scaled gradients on Grassmann manifolds for matrix completion", "author": ["T. Ngo", "Y. Saad"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ngo and Saad.,? \\Q2012\\E", "shortCiteRegEx": "Ngo and Saad.", "year": 2012}, {"title": "A variational approach to remove outliers and impulse noise", "author": ["M. Nikolova"], "venue": "Journal of Mathematical Imaging and Vision,", "citeRegEx": "Nikolova.,? \\Q2004\\E", "shortCiteRegEx": "Nikolova.", "year": 2004}, {"title": "iPiano: Inertial proximal algorithm for nonconvex optimization", "author": ["P. Ochs", "Y. Chen", "T. Brox", "T. Pock"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Ochs et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ochs et al\\.", "year": 2014}, {"title": "Stochastic variance reduction for nonconvex optimization", "author": ["S.J. Reddi", "A. Hefny", "S. Sra", "B. P\u00f3czos", "A.J. Smola"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Reddi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2016}, {"title": "Fast stochastic methods for nonsmooth nonconvex optimization", "author": ["S.J. Reddi", "S. Sra", "B. Poczos", "A. Smola"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Reddi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2016}, {"title": "Convergence rates of inexact proximal-gradient methods for convex optimization", "author": ["M. Schmidt", "N.L. Roux", "F. Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Schmidt et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2011}, {"title": "Scalable nonconvex inexact proximal splitting", "author": ["S. Sra"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sra.,? \\Q2012\\E", "shortCiteRegEx": "Sra.", "year": 2012}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T.S. Jaakkola"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Srebro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2004}, {"title": "Robust principal component analysis via capped norms", "author": ["Q. Sun", "S. Xiang", "J. Ye"], "venue": "In Proceedings of the 19th International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Sun et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2013}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B,", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Sparsity and smoothness via the fused lasso", "author": ["R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "Tibshirani et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2005}, {"title": "Highly undersampled magnetic resonance image reconstruction via homotopic-minimization", "author": ["J. Trzasko", "A. Manduca"], "venue": "IEEE Transactions on Medical Imaging,", "citeRegEx": "Trzasko and Manduca.,? \\Q2009\\E", "shortCiteRegEx": "Trzasko and Manduca.", "year": 2009}, {"title": "Characterization of the subdifferential of some matrix norms", "author": ["G.A. Watson"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Watson.,? \\Q1992\\E", "shortCiteRegEx": "Watson.", "year": 1992}, {"title": "Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm", "author": ["Z. Wen", "W. Yin", "Y. Zhang"], "venue": "Mathematical Programming Computation,", "citeRegEx": "Wen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2012}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["L. Xiao", "T. Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Xiao and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Xiao and Zhang.", "year": 2014}, {"title": "Restoration of images corrupted by impulse noise and mixed gaussian impulse noise using blind inpainting", "author": ["M. Yan"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Yan.,? \\Q2013\\E", "shortCiteRegEx": "Yan.", "year": 2013}, {"title": "Robust sparse coding for face recognition", "author": ["M. Yang", "L. Zhang", "J. Yang", "D. Zhang"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Yang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "Efficient learning with a family of nonconvex regularizers by redistributing nonconvexity", "author": ["Q Yao", "J.T. Kwok"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Yao and Kwok.,? \\Q2016\\E", "shortCiteRegEx": "Yao and Kwok.", "year": 2016}, {"title": "Fast low-rank matrix learning with nonconvex regularization", "author": ["Q. Yao", "J.T. Kwok", "W. Zhong"], "venue": "In Proceedings of IEEE International Conference on Data Mining,", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Efficient methods for overlapping group lasso", "author": ["L. Yuan", "J. Liu", "J. Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yuan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2011}, {"title": "The concave-convex procedure (CCCP)", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yuille and Rangarajan.,? \\Q2002\\E", "shortCiteRegEx": "Yuille and Rangarajan.", "year": 2002}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "author": ["C.H. Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "Zhang.,? \\Q2010\\E", "shortCiteRegEx": "Zhang.", "year": 2010}, {"title": "Analysis of multi-stage convex relaxation for sparse regularization", "author": ["T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang.,? \\Q2010\\E", "shortCiteRegEx": "Zhang.", "year": 2010}, {"title": "Accelerated training for matrix-norm regularization: A boosting approach", "author": ["X. Zhang", "D. Schuurmans", "Y.-L. Yu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Background subtraction via robust dictionary learning", "author": ["C. Zhao", "X. Wang", "W.-K. Cham"], "venue": "EURASIP Journal on Image and Video Processing,", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}, {"title": "Gradient descent with proximal average for nonconvex and composite regularization", "author": ["W. Zhong", "J.T. Kwok"], "venue": "In Proceedings of the 28th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Zhong and Kwok.,? \\Q2014\\E", "shortCiteRegEx": "Zhong and Kwok.", "year": 2014}, {"title": "Variance reduction for faster non-convex optimization", "author": ["Z.A. Zhu", "E. Hazan"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Zhu and Hazan.,? \\Q2016\\E", "shortCiteRegEx": "Zhu and Hazan.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Popular examples include the sparsity-inducing regularizers, which have been commonly used in image processing (Beck and Teboulle, 2009; Mairal et al., 2009; Jenatton et al., 2011) and highdimensional feature selection (Tibshirani et al.", "startOffset": 111, "endOffset": 180}, {"referenceID": 46, "context": "Popular examples include the sparsity-inducing regularizers, which have been commonly used in image processing (Beck and Teboulle, 2009; Mairal et al., 2009; Jenatton et al., 2011) and highdimensional feature selection (Tibshirani et al.", "startOffset": 111, "endOffset": 180}, {"referenceID": 34, "context": "Popular examples include the sparsity-inducing regularizers, which have been commonly used in image processing (Beck and Teboulle, 2009; Mairal et al., 2009; Jenatton et al., 2011) and highdimensional feature selection (Tibshirani et al.", "startOffset": 111, "endOffset": 180}, {"referenceID": 60, "context": ", 2011) and highdimensional feature selection (Tibshirani et al., 2005; Jacob et al., 2009; Liu and Ye, 2010); and the low-rank regularizer in matrix and tensor learning, with good empirical performance on tasks such as recommender systems (Cand\u00e8s and Recht, 2009; Mazumder et al.", "startOffset": 46, "endOffset": 109}, {"referenceID": 32, "context": ", 2011) and highdimensional feature selection (Tibshirani et al., 2005; Jacob et al., 2009; Liu and Ye, 2010); and the low-rank regularizer in matrix and tensor learning, with good empirical performance on tasks such as recommender systems (Cand\u00e8s and Recht, 2009; Mazumder et al.", "startOffset": 46, "endOffset": 109}, {"referenceID": 40, "context": ", 2011) and highdimensional feature selection (Tibshirani et al., 2005; Jacob et al., 2009; Liu and Ye, 2010); and the low-rank regularizer in matrix and tensor learning, with good empirical performance on tasks such as recommender systems (Cand\u00e8s and Recht, 2009; Mazumder et al.", "startOffset": 46, "endOffset": 109}, {"referenceID": 11, "context": ", 2009; Liu and Ye, 2010); and the low-rank regularizer in matrix and tensor learning, with good empirical performance on tasks such as recommender systems (Cand\u00e8s and Recht, 2009; Mazumder et al., 2010) and visual data analysis (Liu et al.", "startOffset": 156, "endOffset": 203}, {"referenceID": 47, "context": ", 2009; Liu and Ye, 2010); and the low-rank regularizer in matrix and tensor learning, with good empirical performance on tasks such as recommender systems (Cand\u00e8s and Recht, 2009; Mazumder et al., 2010) and visual data analysis (Liu et al.", "startOffset": 156, "endOffset": 203}, {"referenceID": 41, "context": ", 2010) and visual data analysis (Liu et al., 2013; Lu et al., 2014).", "startOffset": 33, "endOffset": 68}, {"referenceID": 43, "context": ", 2010) and visual data analysis (Liu et al., 2013; Lu et al., 2014).", "startOffset": 33, "endOffset": 68}, {"referenceID": 16, "context": "Well-known examples include the `1-regularizer for sparse coding (Donoho, 2006), and the nuclear norm regularizer in low-rank matrix learning", "startOffset": 65, "endOffset": 79}, {"referenceID": 20, "context": "\u03ba(\u03b1) \u03ba\u2032(\u03b1) \u03ba0 \u03c1 GP (Geman and Yang, 1995) \u03b2\u03b1 \u03b8+\u03b1 \u03b2\u03b8 (\u03b8+\u03b1)2 \u03b2 \u03b8 2\u03b2 \u03b82 LSP (Cand\u00e8s et al.", "startOffset": 19, "endOffset": 41}, {"referenceID": 12, "context": "\u03ba(\u03b1) \u03ba\u2032(\u03b1) \u03ba0 \u03c1 GP (Geman and Yang, 1995) \u03b2\u03b1 \u03b8+\u03b1 \u03b2\u03b8 (\u03b8+\u03b1)2 \u03b2 \u03b8 2\u03b2 \u03b82 LSP (Cand\u00e8s et al., 2008) \u03b2 log(1 + \u03b1\u03b8 ) \u03b2 \u03b8+\u03b1 \u03b2 \u03b8 \u03b2 \u03b82", "startOffset": 73, "endOffset": 94}, {"referenceID": 61, "context": "MCP (Zhang, 2010a) { \u03b2\u03b1\u2212 \u03b12 2\u03b8 \u03b1 \u2264 \u03b2\u03b8 1 2\u03b8\u03b2 2 \u03b1 > \u03b2\u03b8 { \u03b2 \u2212 \u03b1\u03b8 \u03b1 \u2264 \u03b2\u03b8 0 \u03b1 > \u03b2\u03b8 \u03b2 1\u03b8 Laplace (Trzasko and Manduca, 2009) \u03b2(1\u2212 exp(\u2212\u03b1\u03b8 )) \u03b2 \u03b8 exp ( \u2212\u03b1\u03b8 ) \u03b2 \u03b8 \u03b2 \u03b82", "startOffset": 91, "endOffset": 118}, {"referenceID": 18, "context": "SCAD (Fan and Li, 2001) \uf8f4\uf8f2\uf8f4\uf8f3 \u03b2\u03b1 \u03b1 \u2264 \u03b2 \u2212\u03b12+2\u03b8\u03b2\u03b1\u2212\u03b22 2(\u03b8\u22121) \u03b2 < \u03b1 \u2264 \u03b8\u03b2 \u03b22(1+\u03b8) 2 \u03b1 > \u03b8\u03b2 \uf8f4\uf8f2\uf8f4\uf8f3 \u03b2 \u03b1 \u2264 \u03b2 \u2212\u03b1+\u03b8\u03b2 \u03b8\u22121 \u03b2 < \u03b1 \u2264 \u03b8\u03b2 0 \u03b1 > \u03b8\u03b2 \u03b2 1 \u03b8\u22121", "startOffset": 5, "endOffset": 23}, {"referenceID": 11, "context": "(Cand\u00e8s and Recht, 2009).", "startOffset": 0, "endOffset": 24}, {"referenceID": 33, "context": "Popular optimization algorithms in machine learning include the proximal algorithm (Parikh and Boyd, 2013), Frank-Wolfe (FW) algorithm (Jaggi, 2013), the alternating direction method of multipliers (ADMM) (Boyd et al.", "startOffset": 135, "endOffset": 148}, {"referenceID": 9, "context": "Popular optimization algorithms in machine learning include the proximal algorithm (Parikh and Boyd, 2013), Frank-Wolfe (FW) algorithm (Jaggi, 2013), the alternating direction method of multipliers (ADMM) (Boyd et al., 2011), stochastic gradient descent and its variants (Bottou, 1998; Xiao and Zhang, 2014).", "startOffset": 205, "endOffset": 224}, {"referenceID": 7, "context": ", 2011), stochastic gradient descent and its variants (Bottou, 1998; Xiao and Zhang, 2014).", "startOffset": 54, "endOffset": 90}, {"referenceID": 64, "context": ", 2011), stochastic gradient descent and its variants (Bottou, 1998; Xiao and Zhang, 2014).", "startOffset": 54, "endOffset": 90}, {"referenceID": 47, "context": "In lowrank matrix learning, the estimated rank obtained with the nuclear norm regularizer is often much higher (Mazumder et al., 2010).", "startOffset": 111, "endOffset": 134}, {"referenceID": 20, "context": "To alleviate this problem, a number of nonconvex regularizers have been recently proposed (Geman and Yang, 1995; Fan and Li, 2001; Cand\u00e8s et al., 2008; Zhang, 2010a; Trzasko and Manduca, 2009).", "startOffset": 90, "endOffset": 192}, {"referenceID": 18, "context": "To alleviate this problem, a number of nonconvex regularizers have been recently proposed (Geman and Yang, 1995; Fan and Li, 2001; Cand\u00e8s et al., 2008; Zhang, 2010a; Trzasko and Manduca, 2009).", "startOffset": 90, "endOffset": 192}, {"referenceID": 12, "context": "To alleviate this problem, a number of nonconvex regularizers have been recently proposed (Geman and Yang, 1995; Fan and Li, 2001; Cand\u00e8s et al., 2008; Zhang, 2010a; Trzasko and Manduca, 2009).", "startOffset": 90, "endOffset": 192}, {"referenceID": 61, "context": "To alleviate this problem, a number of nonconvex regularizers have been recently proposed (Geman and Yang, 1995; Fan and Li, 2001; Cand\u00e8s et al., 2008; Zhang, 2010a; Trzasko and Manduca, 2009).", "startOffset": 90, "endOffset": 192}, {"referenceID": 70, "context": "One can use general-purpose nonconvex optimization solvers such as the concave-convex procedure (Yuille and Rangarajan, 2002).", "startOffset": 96, "endOffset": 125}, {"referenceID": 26, "context": "However, the subproblem in each iteration can be as expensive as the original problem, and the concave-convex procedure is thus often slow in practice (Gong et al., 2013; Zhong and Kwok, 2014).", "startOffset": 151, "endOffset": 192}, {"referenceID": 75, "context": "However, the subproblem in each iteration can be as expensive as the original problem, and the concave-convex procedure is thus often slow in practice (Gong et al., 2013; Zhong and Kwok, 2014).", "startOffset": 151, "endOffset": 192}, {"referenceID": 56, "context": "Examples include the NIPS (Sra, 2012), IPiano (Ochs et al.", "startOffset": 26, "endOffset": 37}, {"referenceID": 52, "context": "Examples include the NIPS (Sra, 2012), IPiano (Ochs et al., 2014), UAG (Ghadimi and Lan, 2016), GIST (Gong et al.", "startOffset": 46, "endOffset": 65}, {"referenceID": 21, "context": ", 2014), UAG (Ghadimi and Lan, 2016), GIST (Gong et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 26, "context": ", 2014), UAG (Ghadimi and Lan, 2016), GIST (Gong et al., 2013), IFB (Bot et al.", "startOffset": 43, "endOffset": 62}, {"referenceID": 6, "context": ", 2013), IFB (Bot et al., 2016), and nmAPG (Li and Lin, 2015).", "startOffset": 13, "endOffset": 31}, {"referenceID": 39, "context": ", 2016), and nmAPG (Li and Lin, 2015).", "startOffset": 19, "endOffset": 37}, {"referenceID": 75, "context": "When the regularizer is complicated, such as the nonconvex versions of the fused lasso and overlapping group lasso regularizers (Zhong and Kwok, 2014),", "startOffset": 128, "endOffset": 150}, {"referenceID": 75, "context": "Another approach is by using the proximal average (Zhong and Kwok, 2014), which computes and averages the proximal step of each underlying regularizer.", "startOffset": 50, "endOffset": 72}, {"referenceID": 39, "context": "However, because the proximal step is only approximate, convergence is usually slower than typical applications of the proximal algorithm (Li and Lin, 2015).", "startOffset": 138, "endOffset": 156}, {"referenceID": 30, "context": "For the global consensus problem, standard ADMM converges only when g is convex (Hong et al., 2016).", "startOffset": 80, "endOffset": 99}, {"referenceID": 38, "context": "When g is nonconvex, convergence of ADMM is only established for problems of the form minx,y f(x) + g(y) : y = Ax, where matrix A has full row rank (Li and Pong, 2015).", "startOffset": 148, "endOffset": 167}, {"referenceID": 35, "context": "More recently, the stochastic variance reduced gradient (SVRG) algorithm (Johnson and Zhang, 2013), which is a variant of the popular stochastic gradient descent with reduced variance in the gradient estimates, has also been extended for problems with nonconvex f .", "startOffset": 73, "endOffset": 98}, {"referenceID": 76, "context": "However, the regularizer g is still required to be convex (Reddi et al., 2016a; Zhu and Hazan, 2016).", "startOffset": 58, "endOffset": 100}, {"referenceID": 65, "context": "For example, the absolute loss is more robust to outliers than the square loss, and has been popularly used in applications such as image denoising (Yan, 2013), robust dictionary learning (Zhao et al.", "startOffset": 148, "endOffset": 159}, {"referenceID": 74, "context": "For example, the absolute loss is more robust to outliers than the square loss, and has been popularly used in applications such as image denoising (Yan, 2013), robust dictionary learning (Zhao et al., 2011) and robust PCA (Cand\u00e8s et al.", "startOffset": 188, "endOffset": 207}, {"referenceID": 13, "context": ", 2011) and robust PCA (Cand\u00e8s et al., 2011).", "startOffset": 23, "endOffset": 44}, {"referenceID": 28, "context": "When both f and g are convex, ADMM is often the main optimization tool for problem (1) (He and Yuan, 2012).", "startOffset": 87, "endOffset": 106}, {"referenceID": 65, "context": "Besides a nonconvex g, we may also want to use a nonconvex loss f , such as `0-norm (Yan, 2013) and capped-`1 norm (Sun et al.", "startOffset": 84, "endOffset": 95}, {"referenceID": 58, "context": "Besides a nonconvex g, we may also want to use a nonconvex loss f , such as `0-norm (Yan, 2013) and capped-`1 norm (Sun et al., 2013), as they are more robust to outliers and can obtain better performance.", "startOffset": 115, "endOffset": 133}, {"referenceID": 70, "context": "As a last resort, one can use more general nonconvex optimization approaches such as convex concave programming (CCCP) (Yuille and Rangarajan, 2002).", "startOffset": 119, "endOffset": 148}, {"referenceID": 67, "context": "Note that this paper extends a shorter version published in the proceedings of the International Conference of Machine Learning (Yao and Kwok, 2016).", "startOffset": 128, "endOffset": 148}, {"referenceID": 70, "context": "1 Convex-Concave Procedure (CCCP) The convex-concave procedure (CCCP) (Yuille and Rangarajan, 2002; Lu, 2012) is a popular and general solver for (1).", "startOffset": 70, "endOffset": 109}, {"referenceID": 45, "context": "1 Convex-Concave Procedure (CCCP) The convex-concave procedure (CCCP) (Yuille and Rangarajan, 2002; Lu, 2012) is a popular and general solver for (1).", "startOffset": 70, "endOffset": 109}, {"referenceID": 29, "context": "It assumes thatF can be decomposed as a difference of convex (DC) functions (Hiriart-Urruty, 1985), i.", "startOffset": 76, "endOffset": 98}, {"referenceID": 45, "context": "Sequential convex programming (SCP) (Lu, 2012) improves its efficiency when F is in form of (1).", "startOffset": 36, "endOffset": 46}, {"referenceID": 26, "context": "However, its convergence is still slow in general (Gong et al., 2013; Zhong and Kwok, 2014; Li and Lin, 2015).", "startOffset": 50, "endOffset": 109}, {"referenceID": 75, "context": "However, its convergence is still slow in general (Gong et al., 2013; Zhong and Kwok, 2014; Li and Lin, 2015).", "startOffset": 50, "endOffset": 109}, {"referenceID": 39, "context": "However, its convergence is still slow in general (Gong et al., 2013; Zhong and Kwok, 2014; Li and Lin, 2015).", "startOffset": 50, "endOffset": 109}, {"referenceID": 49, "context": "This can be further accelerated to O(1/T 2) by modifying the generation of {xt} as (Beck, 2009; Nesterov, 2013): yt = xt + \u03b1t\u22121 \u2212 1 \u03b1t (xt \u2212 xt\u22121), xt+1 = prox 1 L g ( yt \u2212 1 L \u2207f(yt) ) ,", "startOffset": 83, "endOffset": 111}, {"referenceID": 56, "context": "In particular, NIPS (Sra, 2012), IPiano (Ochs et al.", "startOffset": 20, "endOffset": 31}, {"referenceID": 52, "context": "In particular, NIPS (Sra, 2012), IPiano (Ochs et al., 2014) and UAG (Ghadimi and Lan, 2016) allow f to be nonconvex, while g is still required to be convex.", "startOffset": 40, "endOffset": 59}, {"referenceID": 21, "context": ", 2014) and UAG (Ghadimi and Lan, 2016) allow f to be nonconvex, while g is still required to be convex.", "startOffset": 16, "endOffset": 39}, {"referenceID": 26, "context": "GIST (Gong et al., 2013), IFB (Bot et al.", "startOffset": 5, "endOffset": 24}, {"referenceID": 6, "context": ", 2013), IFB (Bot et al., 2016) and nmAPG (Li and Lin, 2015) further remove this restriction and allow g to be nonconvex.", "startOffset": 13, "endOffset": 31}, {"referenceID": 39, "context": ", 2016) and nmAPG (Li and Lin, 2015) further remove this restriction and allow g to be nonconvex.", "startOffset": 18, "endOffset": 36}, {"referenceID": 59, "context": "This is true for many convex regularizers such as the lasso regularier (Tibshirani, 1996), tree-structured lasso regularizer (Liu and Ye, 2010; Jenatton et al.", "startOffset": 71, "endOffset": 89}, {"referenceID": 40, "context": "This is true for many convex regularizers such as the lasso regularier (Tibshirani, 1996), tree-structured lasso regularizer (Liu and Ye, 2010; Jenatton et al., 2011) and sparse group lasso regularizer (Jacob et al.", "startOffset": 125, "endOffset": 166}, {"referenceID": 34, "context": "This is true for many convex regularizers such as the lasso regularier (Tibshirani, 1996), tree-structured lasso regularizer (Liu and Ye, 2010; Jenatton et al., 2011) and sparse group lasso regularizer (Jacob et al.", "startOffset": 125, "endOffset": 166}, {"referenceID": 32, "context": ", 2011) and sparse group lasso regularizer (Jacob et al., 2009).", "startOffset": 43, "endOffset": 63}, {"referenceID": 26, "context": ", nonconvex lasso regularizer (Gong et al., 2013), and usually do not exist for more general cases, e.", "startOffset": 30, "endOffset": 49}, {"referenceID": 75, "context": ", nonconvex tree-structured lasso regularizer (Zhong and Kwok, 2014).", "startOffset": 46, "endOffset": 68}, {"referenceID": 1, "context": "On the other hand, Zhong and Kwok (2014) used proximal average (Bauschke et al., 2008) to handle complicate g which is in the form g(x) = \u2211K i=1 \u03bcigi(x), where each gi has a simple proximal step.", "startOffset": 63, "endOffset": 86}, {"referenceID": 5, "context": ", 2013), IFB (Bot et al., 2016) and nmAPG (Li and Lin, 2015) further remove this restriction and allow g to be nonconvex. It is desirable that the proximal step has a closed-form solution. This is true for many convex regularizers such as the lasso regularier (Tibshirani, 1996), tree-structured lasso regularizer (Liu and Ye, 2010; Jenatton et al., 2011) and sparse group lasso regularizer (Jacob et al., 2009). However, when g is nonconvex, such solution only exists for some simple g, e.g., nonconvex lasso regularizer (Gong et al., 2013), and usually do not exist for more general cases, e.g., nonconvex tree-structured lasso regularizer (Zhong and Kwok, 2014). On the other hand, Zhong and Kwok (2014) used proximal average (Bauschke et al.", "startOffset": 14, "endOffset": 707}, {"referenceID": 19, "context": "3 Frank-Wolfe (FW) Algorithm The FW algorithm (Frank and Wolfe, 1956) is used for solving optimization problems of the form", "startOffset": 46, "endOffset": 69}, {"referenceID": 33, "context": "Recently, it has been popularly used in machine learning (Jaggi, 2013).", "startOffset": 57, "endOffset": 70}, {"referenceID": 33, "context": "The FW algorithm has a convergence rate of O(1/T ) (Jaggi, 2013).", "startOffset": 51, "endOffset": 64}, {"referenceID": 11, "context": "The nuclear norm of X , \u2016X\u2016\u2217 = \u2211m i=1 \u03c3i(X), is the tightest convex envelope of rank(X), and is often used as a low-rank regularizer (Cand\u00e8s and Recht, 2009).", "startOffset": 133, "endOffset": 157}, {"referenceID": 11, "context": "For example, in matrix completion (Cand\u00e8s and Recht, 2009),", "startOffset": 34, "endOffset": 58}, {"referenceID": 73, "context": "The FW algorithm for this nuclear norm regularized problem is shown in Algorithm 1 (Zhang et al., 2012).", "startOffset": 83, "endOffset": 103}, {"referenceID": 33, "context": "As in (5), the following linear subproblem has to be solved (Jaggi, 2013): min S:\u2016S\u2016\u2217\u22641 \u3008S,\u2207f(Xt)\u3009.", "startOffset": 60, "endOffset": 73}, {"referenceID": 33, "context": "The FW algorithm has a convergence rate of O(1/T ) (Jaggi, 2013).", "startOffset": 51, "endOffset": 64}, {"referenceID": 36, "context": "To make it empirically faster, Algorithm 1 also performs optimization at step 6 (Laue, 2012; Zhang et al., 2012).", "startOffset": 80, "endOffset": 112}, {"referenceID": 73, "context": "To make it empirically faster, Algorithm 1 also performs optimization at step 6 (Laue, 2012; Zhang et al., 2012).", "startOffset": 80, "endOffset": 112}, {"referenceID": 57, "context": "Substituting \u2016X\u2016\u2217 = minX=UV > 1 2 ( \u2016U\u2016F + \u2016V \u2016F ) (Srebro et al., 2004) into (8), we have the following local optimization problem:", "startOffset": 51, "endOffset": 72}, {"referenceID": 73, "context": "Algorithm 1 Frank-Wolfe algorithm for problem (8) with f convex (Zhang et al., 2012).", "startOffset": 64, "endOffset": 84}, {"referenceID": 22, "context": "4 Alternating Direction Method of Multipliers (ADMM) ADMM is a simple but powerful algorithm first introduced in the 1970s (Glowinski and Marroco, 1975).", "startOffset": 123, "endOffset": 152}, {"referenceID": 9, "context": "Recently, it has been popularly used in diverse fields such as machine learning, data mining and image processing (Boyd et al., 2011).", "startOffset": 114, "endOffset": 133}, {"referenceID": 5, "context": "processing and wireless communication (Bertsekas and Tsitsiklis, 1989; Boyd et al., 2011).", "startOffset": 38, "endOffset": 89}, {"referenceID": 9, "context": "processing and wireless communication (Bertsekas and Tsitsiklis, 1989; Boyd et al., 2011).", "startOffset": 38, "endOffset": 89}, {"referenceID": 30, "context": "When fi is smooth and g is convex, ADMM converges to a critical point of (16) (Hong et al., 2016).", "startOffset": 78, "endOffset": 97}, {"referenceID": 20, "context": "Examples include the Geman penalty (GP) (Geman and Yang, 1995), log-sum penalty (LSP) (Cand\u00e8s et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 12, "context": "Examples include the Geman penalty (GP) (Geman and Yang, 1995), log-sum penalty (LSP) (Cand\u00e8s et al., 2008) and Laplace penalty (Trzasko and Manduca, 2009).", "startOffset": 86, "endOffset": 107}, {"referenceID": 61, "context": ", 2008) and Laplace penalty (Trzasko and Manduca, 2009).", "startOffset": 28, "endOffset": 55}, {"referenceID": 32, "context": "By using different Ai\u2019s, g becomes various structured sparsity regularizers such as the group lasso (Jacob et al., 2009), fused lasso (Tibshirani et al.", "startOffset": 100, "endOffset": 120}, {"referenceID": 60, "context": ", 2009), fused lasso (Tibshirani et al., 2005), and graphical lasso (Jacob et al.", "startOffset": 21, "endOffset": 46}, {"referenceID": 32, "context": ", 2005), and graphical lasso (Jacob et al., 2009).", "startOffset": 29, "endOffset": 49}, {"referenceID": 29, "context": "Since \u1e21 is concave and \u011f is convex, the nonconvex regularizer g = \u011f\u2212 (\u2212\u1e21) can be viewed as a difference of convex functions (DC) (Hiriart-Urruty, 1985).", "startOffset": 129, "endOffset": 151}, {"referenceID": 28, "context": "Since \u1e21 is concave and \u011f is convex, the nonconvex regularizer g = \u011f\u2212 (\u2212\u1e21) can be viewed as a difference of convex functions (DC) (Hiriart-Urruty, 1985). Lu (2012); Gong et al.", "startOffset": 130, "endOffset": 163}, {"referenceID": 26, "context": "Lu (2012); Gong et al. (2013); Zhong and Kwok (2014) also relied on DC decompositions of the nonconvex regularizer.", "startOffset": 11, "endOffset": 30}, {"referenceID": 26, "context": "Lu (2012); Gong et al. (2013); Zhong and Kwok (2014) also relied on DC decompositions of the nonconvex regularizer.", "startOffset": 11, "endOffset": 53}, {"referenceID": 40, "context": "(2016) extended proximal algorithm to simple nonconvex g, but cannot handle more complicated nonconvex regularizers such as the tree-structured lasso regularizer (Liu and Ye, 2010; Schmidt et al., 2011), sparse group lasso regularizer (Jacob et al.", "startOffset": 162, "endOffset": 202}, {"referenceID": 55, "context": "(2016) extended proximal algorithm to simple nonconvex g, but cannot handle more complicated nonconvex regularizers such as the tree-structured lasso regularizer (Liu and Ye, 2010; Schmidt et al., 2011), sparse group lasso regularizer (Jacob et al.", "startOffset": 162, "endOffset": 202}, {"referenceID": 32, "context": ", 2011), sparse group lasso regularizer (Jacob et al., 2009) and total variation regularizer (Nikolova, 2004).", "startOffset": 40, "endOffset": 60}, {"referenceID": 51, "context": ", 2009) and total variation regularizer (Nikolova, 2004).", "startOffset": 40, "endOffset": 56}, {"referenceID": 1, "context": "Using the proximal average (Bauschke et al., 2008), Zhong and Kwok (2014) can handle nonconvex regularizers of the form g = \u2211K i=1 \u03bcigi, where each gi is simple.", "startOffset": 27, "endOffset": 50}, {"referenceID": 70, "context": "General nonconvex optimization techniques such as the concave-convex procedure (CCCP) (Yuille and Rangarajan, 2002) or its variant sequential convex programming (SCP) (Lu, 2012) can also be used, though they are slow in general (Gong et al.", "startOffset": 86, "endOffset": 115}, {"referenceID": 45, "context": "General nonconvex optimization techniques such as the concave-convex procedure (CCCP) (Yuille and Rangarajan, 2002) or its variant sequential convex programming (SCP) (Lu, 2012) can also be used, though they are slow in general (Gong et al.", "startOffset": 167, "endOffset": 177}, {"referenceID": 26, "context": "General nonconvex optimization techniques such as the concave-convex procedure (CCCP) (Yuille and Rangarajan, 2002) or its variant sequential convex programming (SCP) (Lu, 2012) can also be used, though they are slow in general (Gong et al., 2013; Zhong and Kwok, 2014).", "startOffset": 228, "endOffset": 269}, {"referenceID": 75, "context": "General nonconvex optimization techniques such as the concave-convex procedure (CCCP) (Yuille and Rangarajan, 2002) or its variant sequential convex programming (SCP) (Lu, 2012) can also be used, though they are slow in general (Gong et al., 2013; Zhong and Kwok, 2014).", "startOffset": 228, "endOffset": 269}, {"referenceID": 24, "context": "Gong et al. (2013); Li and Lin (2015) and Bot et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 24, "context": "Gong et al. (2013); Li and Lin (2015) and Bot et al.", "startOffset": 0, "endOffset": 38}, {"referenceID": 5, "context": "(2013); Li and Lin (2015) and Bot et al. (2016) extended proximal algorithm to simple nonconvex g, but cannot handle more complicated nonconvex regularizers such as the tree-structured lasso regularizer (Liu and Ye, 2010; Schmidt et al.", "startOffset": 30, "endOffset": 48}, {"referenceID": 1, "context": "Using the proximal average (Bauschke et al., 2008), Zhong and Kwok (2014) can handle nonconvex regularizers of the form g = \u2211K i=1 \u03bcigi, where each gi is simple.", "startOffset": 28, "endOffset": 74}, {"referenceID": 26, "context": "convergence guarantee for convex and nonconvex f (Gong et al., 2013; Li and Lin, 2015), solving the transformed problem can be much faster.", "startOffset": 49, "endOffset": 86}, {"referenceID": 39, "context": "convergence guarantee for convex and nonconvex f (Gong et al., 2013; Li and Lin, 2015), solving the transformed problem can be much faster.", "startOffset": 49, "endOffset": 86}, {"referenceID": 32, "context": ", (aN , yN )}, (convex) sparse group lasso is formulated as (Jacob et al., 2009):", "startOffset": 60, "endOffset": 80}, {"referenceID": 69, "context": "Its proximal step can be easily computed by the algorithm in (Yuan et al., 2011).", "startOffset": 61, "endOffset": 80}, {"referenceID": 39, "context": "In particular, we will adopt the state-of-the-art nonmontonic APG (nmAPG) algorithm (Li and Lin, 2015) (shown in Algorithm 2).", "startOffset": 84, "endOffset": 102}, {"referenceID": 40, "context": "2 NONCONVEX TREE-STRUCTURED GROUP LASSO In (convex) tree-structured group lasso (Liu and Ye, 2010; Jenatton et al., 2011), the dimensions in x are organized as nodes in a tree, and each group corresponds to a subtree.", "startOffset": 80, "endOffset": 121}, {"referenceID": 34, "context": "2 NONCONVEX TREE-STRUCTURED GROUP LASSO In (convex) tree-structured group lasso (Liu and Ye, 2010; Jenatton et al., 2011), the dimensions in x are organized as nodes in a tree, and each group corresponds to a subtree.", "startOffset": 80, "endOffset": 121}, {"referenceID": 40, "context": "Interested readers are referred to (Liu and Ye, 2010) for details.", "startOffset": 35, "endOffset": 53}, {"referenceID": 39, "context": "Algorithm 2 Nonmonotonic APG (nmAPG) (Li and Lin, 2015).", "startOffset": 37, "endOffset": 55}, {"referenceID": 40, "context": "As shown in (Liu and Ye, 2010), its proximal step can be computed efficiently by processing all the groups once in some appropriate order.", "startOffset": 12, "endOffset": 30}, {"referenceID": 51, "context": "Given an image X \u2208 Rm\u00d7n, the TV regularizer is defined as TV(X) = \u2016DvX\u20161 + \u2016XDh\u20161 (Nikolova, 2004), Dv = \uf8ef\uf8f0\u22121 1 .", "startOffset": 82, "endOffset": 98}, {"referenceID": 51, "context": "Thus, it is popular on image processing problems, such as image denoising and deconvolution (Nikolova, 2004; Beck and Teboulle, 2009).", "startOffset": 92, "endOffset": 133}, {"referenceID": 2, "context": "Thus, it is popular on image processing problems, such as image denoising and deconvolution (Nikolova, 2004; Beck and Teboulle, 2009).", "startOffset": 92, "endOffset": 133}, {"referenceID": 39, "context": "However, Lemma 2 of (Li and Lin, 2015), which is key to the convergence of nmAPG, no longer holds dues to inexact proximal step.", "startOffset": 20, "endOffset": 38}, {"referenceID": 54, "context": "In this case, Schmidt et al. (2011) showed that using inexact proximal steps can make proximal algorithms faster.", "startOffset": 14, "endOffset": 36}, {"referenceID": 26, "context": "If the proximal step is exact, \u2016Vt \u2212 prox 1 \u03c4 \u011f(Vt \u2212 1 \u03c4\u2207f\u0304(Vt))\u2016 2 F can be used to measure how far Vt is from a critical point (Gong et al., 2013; Ghadimi and Lan, 2016).", "startOffset": 129, "endOffset": 171}, {"referenceID": 21, "context": "If the proximal step is exact, \u2016Vt \u2212 prox 1 \u03c4 \u011f(Vt \u2212 1 \u03c4\u2207f\u0304(Vt))\u2016 2 F can be used to measure how far Vt is from a critical point (Gong et al., 2013; Ghadimi and Lan, 2016).", "startOffset": 129, "endOffset": 171}, {"referenceID": 68, "context": "Recently, there is growing interest to replace this with nonconvex regularizers (Lu et al., 2014, 2015; Yao et al., 2015; Gui et al., 2016).", "startOffset": 80, "endOffset": 139}, {"referenceID": 27, "context": "Recently, there is growing interest to replace this with nonconvex regularizers (Lu et al., 2014, 2015; Yao et al., 2015; Gui et al., 2016).", "startOffset": 80, "endOffset": 139}, {"referenceID": 10, "context": "A FW variant allowing nonconvex f\u0304 is proposed in (Bredies et al., 2009).", "startOffset": 50, "endOffset": 72}, {"referenceID": 10, "context": "However, condition 1 in (Bredies et al., 2009) requires g to satisfy lim\u2016X\u2016F\u2192\u221e g(X) \u2016X\u2016F =\u221e.", "startOffset": 24, "endOffset": 46}, {"referenceID": 50, "context": "(34) This can be efficiently solved using matrix optimization techniques on the Grassmann manifold (Ngo and Saad, 2012).", "startOffset": 99, "endOffset": 119}, {"referenceID": 28, "context": "When all the fi\u2019s and g are convex, ADMM has a convergence rate of O(1/T ) (He and Yuan, 2012).", "startOffset": 75, "endOffset": 94}, {"referenceID": 30, "context": "Recently, ADMM has been extended to problems where g is convex but fi\u2019s are nonconvex (Hong et al., 2016).", "startOffset": 86, "endOffset": 105}, {"referenceID": 30, "context": "4 of (Hong et al., 2016) can now be applied.", "startOffset": 5, "endOffset": 24}, {"referenceID": 35, "context": "Examples are stochastic variance reduced gradient (SVRG) (Johnson and Zhang, 2013) and its proximal extension Prox-SVRG (Xiao and Zhang, 2014).", "startOffset": 57, "endOffset": 82}, {"referenceID": 64, "context": "Examples are stochastic variance reduced gradient (SVRG) (Johnson and Zhang, 2013) and its proximal extension Prox-SVRG (Xiao and Zhang, 2014).", "startOffset": 120, "endOffset": 142}, {"referenceID": 53, "context": "Reddi et al. (2016a) and Zhu and Hazan (2016) considered smooth nonconvex ` but without g.", "startOffset": 0, "endOffset": 21}, {"referenceID": 53, "context": "Reddi et al. (2016a) and Zhu and Hazan (2016) considered smooth nonconvex ` but without g.", "startOffset": 0, "endOffset": 46}, {"referenceID": 0, "context": "5 With OWL-QN In this section, we consider OWL-QN (Andrew and Gao, 2007) and its variant mOWL-QN (Gong and Ye, 2015b), which are efficient algorithms for the `1-regularization problem min x f(x) + \u03bc\u2016x\u20161.", "startOffset": 50, "endOffset": 72}, {"referenceID": 0, "context": "5 With OWL-QN In this section, we consider OWL-QN (Andrew and Gao, 2007) and its variant mOWL-QN (Gong and Ye, 2015b), which are efficient algorithms for the `1-regularization problem min x f(x) + \u03bc\u2016x\u20161. (39) Recently, Gong and Ye (2015a) proposed a nonconvex generalization for (39), in which the standard `1 regularizer is replaced by the nonconvex g(x) = \u03bc \u2211d i=1 \u03ba(|xi|):", "startOffset": 51, "endOffset": 239}, {"referenceID": 0, "context": "On the other hand, the Hessian in (41) depends only on f\u0304 , as the Hessian due to \u2016x\u20161 is zero (Andrew and Gao, 2007), and mOWL-QN now extracts Hessian from f\u0304 .", "startOffset": 95, "endOffset": 117}, {"referenceID": 26, "context": "In particular, the proximal algorithm requires f in (1) to be smooth (possibly nonconvex) (Gong et al., 2013; Li and Lin, 2015; Bot et al., 2016).", "startOffset": 90, "endOffset": 145}, {"referenceID": 39, "context": "In particular, the proximal algorithm requires f in (1) to be smooth (possibly nonconvex) (Gong et al., 2013; Li and Lin, 2015; Bot et al., 2016).", "startOffset": 90, "endOffset": 145}, {"referenceID": 6, "context": "In particular, the proximal algorithm requires f in (1) to be smooth (possibly nonconvex) (Gong et al., 2013; Li and Lin, 2015; Bot et al., 2016).", "startOffset": 90, "endOffset": 145}, {"referenceID": 33, "context": "The FW algorithm requires f in (4) to be smooth and convex (Jaggi, 2013).", "startOffset": 59, "endOffset": 72}, {"referenceID": 30, "context": "For the ADMM, it allows f in the consensus problem to be smooth, but g has to be convex (Hong et al., 2016).", "startOffset": 88, "endOffset": 107}, {"referenceID": 38, "context": "For problems of the form minx,z f(y) + g(y) : y = Ax, ADMM requires A to have full row-rank (Li and Pong, 2015).", "startOffset": 92, "endOffset": 111}, {"referenceID": 70, "context": "CCCP (Yuille and Rangarajan, 2002) and smoothing (Chen, 2012) are more general and can still be used, but are usually very slow.", "startOffset": 5, "endOffset": 34}, {"referenceID": 14, "context": "CCCP (Yuille and Rangarajan, 2002) and smoothing (Chen, 2012) are more general and can still be used, but are usually very slow.", "startOffset": 49, "endOffset": 61}, {"referenceID": 65, "context": "The use of nonconvex loss and regularizer often produce better performance (Yan, 2013).", "startOffset": 75, "endOffset": 86}, {"referenceID": 30, "context": "As (44) is not a consensus problem, the method in (Hong et al., 2016) cannot be used.", "startOffset": 50, "endOffset": 69}, {"referenceID": 38, "context": "To use the ADMM algorithm in (Li and Pong, 2015), extra variables and constraints Zv = DvX and Zh = XDh have to be imposed.", "startOffset": 29, "endOffset": 48}, {"referenceID": 38, "context": "However, the full row-rank condition in (Li and Pong, 2015) does not hold.", "startOffset": 40, "endOffset": 59}, {"referenceID": 49, "context": "As (46) is a smooth and convex problem, both accelerated gradient descent (Nesterov, 2013) and L-BFGS (Nocedal and Wright, 2006) can be applied.", "startOffset": 74, "endOffset": 90}, {"referenceID": 66, "context": "2 ROBUST SPARSE CODING The second application is robust sparse coding, which has been popularly used in face recognition (Yang et al., 2011), image analysis (Lu et al.", "startOffset": 121, "endOffset": 140}, {"referenceID": 42, "context": ", 2011), image analysis (Lu et al., 2013) and background modeling (Zhao et al.", "startOffset": 24, "endOffset": 41}, {"referenceID": 74, "context": ", 2013) and background modeling (Zhao et al., 2011).", "startOffset": 32, "endOffset": 51}, {"referenceID": 69, "context": "The proximal step of the convexified regularizer \u011f(x) = \u03ba0(\u03bb\u2016x\u20161 + \u2211K j=1 \u03bcj\u2016xGj\u20162) is obtained using the algorithm in (Yuan et al., 2011).", "startOffset": 119, "endOffset": 138}, {"referenceID": 39, "context": "The nmAPG algorithm (Algorithm 2) in (Li and Lin, 2015) is used for optimization.", "startOffset": 37, "endOffset": 55}, {"referenceID": 45, "context": "SCP: Sequential convex programming (Lu, 2012), in which the LSP regularizer is decomposed following (24).", "startOffset": 35, "endOffset": 45}, {"referenceID": 26, "context": "GIST (Gong et al., 2013): Since the nonconvex regularizer is not separable, the associated proximal operator has no closed-form solution.", "startOffset": 5, "endOffset": 24}, {"referenceID": 75, "context": "GD-PAN (Zhong and Kwok, 2014): It performs gradient descent with proximal average (Bauschke et al.", "startOffset": 7, "endOffset": 29}, {"referenceID": 1, "context": "GD-PAN (Zhong and Kwok, 2014): It performs gradient descent with proximal average (Bauschke et al., 2008) of the nonconvex regularizers.", "startOffset": 82, "endOffset": 105}, {"referenceID": 70, "context": "We do not compare with the concave-convex procedure (Yuille and Rangarajan, 2002), which has been shown to be slow (Gong et al.", "startOffset": 52, "endOffset": 81}, {"referenceID": 26, "context": "We do not compare with the concave-convex procedure (Yuille and Rangarajan, 2002), which has been shown to be slow (Gong et al., 2013; Zhong and Kwok, 2014).", "startOffset": 115, "endOffset": 156}, {"referenceID": 75, "context": "We do not compare with the concave-convex procedure (Yuille and Rangarajan, 2002), which has been shown to be slow (Gong et al., 2013; Zhong and Kwok, 2014).", "startOffset": 115, "endOffset": 156}, {"referenceID": 40, "context": "Following (Liu and Ye, 2010), we resize each image from 256 \u00d7 256 to 64 \u00d7 64.", "startOffset": 10, "endOffset": 28}, {"referenceID": 40, "context": "5),, wi\u2019s are weights (set to be the reciprocal of the size of sample i\u2019s class) used to alleviate class imbalance, and \u03bbi = 1/ \u221a \u2016Gi\u20161 as in (Liu and Ye, 2010).", "startOffset": 142, "endOffset": 160}, {"referenceID": 40, "context": "For the proposed N2C algorithm, the proximal step of the convexified regularizer is obtained as in (Liu and Ye, 2010).", "startOffset": 99, "endOffset": 117}, {"referenceID": 68, "context": "The LSP regularizer is used, with \u03b8 = \u221a \u03bc as in (Yao et al., 2015).", "startOffset": 48, "endOffset": 66}, {"referenceID": 31, "context": "We use the MovieLens data sets2 (Table 5), which have been commonly used for evaluating matrix completion (Hsieh and Olsen, 2014; Yao et al., 2015).", "startOffset": 106, "endOffset": 147}, {"referenceID": 68, "context": "We use the MovieLens data sets2 (Table 5), which have been commonly used for evaluating matrix completion (Hsieh and Olsen, 2014; Yao et al., 2015).", "startOffset": 106, "endOffset": 147}, {"referenceID": 68, "context": "FaNCL (Yao et al., 2015): This is a recent nonconvex matrix regularization algorithm.", "startOffset": 6, "endOffset": 24}, {"referenceID": 63, "context": "LMaFit (Wen et al., 2012): It factorizes X as a product of low-rank matrices U \u2208 Rm\u00d7k and V \u2208 Rn\u00d7k.", "startOffset": 7, "endOffset": 25}, {"referenceID": 31, "context": "Active subspace selection (denoted \u201cactive\u201d) (Hsieh and Olsen, 2014): This solves the (convex) nuclear norm regularized problem (with \u03ba being the identity function in (8)) by using the active row/column subspaces to reduce the optimization problem size.", "startOffset": 45, "endOffset": 68}, {"referenceID": 43, "context": "We do not compare with IRNN (Lu et al., 2014) and GPG (Lu et al.", "startOffset": 28, "endOffset": 45}, {"referenceID": 44, "context": ", 2014) and GPG (Lu et al., 2015), which have been shown to be much slower than FaNCL (Yao et al.", "startOffset": 16, "endOffset": 33}, {"referenceID": 68, "context": ", 2015), which have been shown to be much slower than FaNCL (Yao et al., 2015).", "startOffset": 60, "endOffset": 78}, {"referenceID": 68, "context": "Following (Yao et al., 2015), we use 50% of the ratings for training, 25% for validation and the rest for testing.", "startOffset": 10, "endOffset": 28}, {"referenceID": 47, "context": "Moreover, the convex model needs a much higher rank than the nonconvex models, which agrees with the previous observations in (Mazumder et al., 2010; Yao et al., 2015).", "startOffset": 126, "endOffset": 167}, {"referenceID": 68, "context": "Moreover, the convex model needs a much higher rank than the nonconvex models, which agrees with the previous observations in (Mazumder et al., 2010; Yao et al., 2015).", "startOffset": 126, "endOffset": 167}, {"referenceID": 15, "context": "Eight popular images4 from (Dabov et al., 2007) are used (Figure 7).", "startOffset": 27, "endOffset": 47}, {"referenceID": 70, "context": "CCCP (Yuille and Rangarajan, 2002): Proposition 6 is used to construct DC decomposition for \u03ba (Details are at Appendix B.", "startOffset": 5, "endOffset": 34}, {"referenceID": 14, "context": "Smoothing (Chen, 2012): The nonsmooth \u03ba is smoothed, and then gradient descent is used (Details are at Appendix B.", "startOffset": 10, "endOffset": 22}, {"referenceID": 39, "context": "nmAPG (Li and Lin, 2015): This optimizes (47) with Algorithm 2, and the exact proximal step is solved numerically using CCCP; 4.", "startOffset": 6, "endOffset": 24}, {"referenceID": 9, "context": "As a baseline, we also compare with ADMM (Boyd et al., 2011) with the convex formulation.", "startOffset": 41, "endOffset": 60}, {"referenceID": 17, "context": "Lemma 15 (Eriksson et al., 2004) Let f : R \u2192 R be a differentiable function.", "startOffset": 9, "endOffset": 32}, {"referenceID": 17, "context": "Lemma 16 (Eriksson et al., 2004) If a continuous function f : R\u2192 R isL1-Lipschitz continuous in [a, b] and L2-Lipschitz continuous in [b, c] (where \u2212\u221e \u2264 a < b < c \u2264 \u221e), then it is max(L1, L2)Lipschitz continuous in [a, c].", "startOffset": 9, "endOffset": 32}, {"referenceID": 8, "context": "Lemma 18 (Boyd and Vandenberghe, 2004) \u03c6(x) = \u03c0(q(x)) is concave if \u03c0 is concave, nonincreasing and q is convex.", "startOffset": 9, "endOffset": 38}, {"referenceID": 4, "context": "Definition 19 (Bertsekas, 1999) A function f : Rm \u2192 R is absolute symmetric if f ([x1; .", "startOffset": 14, "endOffset": 31}, {"referenceID": 37, "context": "Lemma 20 (Lewis and Sendov, 2005) Let \u03c3(X) = [\u03c31(X); .", "startOffset": 9, "endOffset": 33}, {"referenceID": 37, "context": "Lemma 21 (Lewis and Sendov, 2005) Let the SVD of X be UDiag(\u03c3(X))V >, where \u03c3(X) = [\u03c31(X); .", "startOffset": 9, "endOffset": 33}, {"referenceID": 8, "context": "Lemma 22 (Boyd and Vandenberghe, 2004) \u03c6(x) = \u03c0(q(x)) is convex if \u03c0 is convex, nondecreasing and q is convex.", "startOffset": 9, "endOffset": 38}, {"referenceID": 48, "context": "Proposition 24 (Mishra et al., 2013) For a square matrix X , let sym(X) = 12(X + X >).", "startOffset": 15, "endOffset": 36}, {"referenceID": 62, "context": "Proof Subdifferential of the nuclear norm can be obtained as (Watson, 1992) \u2202\u2016X\u2016\u2217 = {UV > +W : U>W = 0,WV = 0, \u2016W\u2016\u221e \u2264 1}, (70) where X = UBV >.", "startOffset": 61, "endOffset": 75}], "year": 2017, "abstractText": "The use of convex regularizers allows for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, for a large class of nonconvex regularizers, we propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex regularizer, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the proximal algorithm, FrankWolfe algorithm, alternating direction method of multipliers and stochastic gradient descent). Extensions are made when the convexified regularizer does not have closed-form proximal step, and when the loss function is nonconvex, nonsmooth. Extensive experiments on a variety of machine learning application scenarios show that optimizing the transformed problem is much faster than running the state-of-the-art on the original problem.", "creator": "LaTeX with hyperref package"}}}