{"id": "1606.03976", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Estimating individual treatment effect: generalization bounds and algorithms", "abstract": "There is intense interest in applying machine learning methods to problems of causal inference which arise in applications such as healthcare, economic policy, and education. In this paper we use the counterfactual inference approach to causal inference, and propose new theoretical results and new algorithms for performing counterfactual inference. Building on an idea recently proposed by Johansson et al., our results and methods rely on learning so-called \"balanced\" representations: representations that are similar between the factual and counterfactual distributions. We give a novel, simple and intuitive bound, showing that the expected counterfactual error of a representation is bounded by a sum of the factual error of that representation and the distance between the factual and counterfactual distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, and focus on two special cases: the Wasserstein distance and the Maximum Mean Discrepancy (MMD) distance. Our bound leads directly to new algorithms, which are simpler and easier to employ compared to those suggested in Johansson et al.. Experiments on real and simulated data show the new algorithms match or outperform state-of-the-art methods.", "histories": [["v1", "Mon, 13 Jun 2016 14:40:57 GMT  (205kb,D)", "http://arxiv.org/abs/1606.03976v1", null], ["v2", "Fri, 24 Jun 2016 13:13:05 GMT  (205kb,D)", "http://arxiv.org/abs/1606.03976v2", "added missing definition of PEHE_nn"], ["v3", "Fri, 28 Oct 2016 18:17:38 GMT  (861kb,D)", "http://arxiv.org/abs/1606.03976v3", "Re-focus the theoretical section of the paper on individual level treatment effect prediction, using counterfactual error as the key component. Added new experiments"], ["v4", "Wed, 1 Mar 2017 15:44:15 GMT  (1191kb,D)", "http://arxiv.org/abs/1606.03976v4", "Added out-of-sample experiments, better hyperparameter selection, new algorithm architecture, bound is IPM based instead of previous UIPM, new policy-curve figures"], ["v5", "Tue, 16 May 2017 15:11:15 GMT  (1191kb,D)", "http://arxiv.org/abs/1606.03976v5", "Added name \"TARNet\" to refer to version with alpha = 0. Removed supp"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["uri shalit", "fredrik d johansson", "david sontag"], "accepted": true, "id": "1606.03976"}, "pdf": {"name": "1606.03976.pdf", "metadata": {"source": "CRF", "title": "Bounding and Minimizing Counterfactual Error", "authors": ["Uri Shalit", "Fredrik D. Johansson"], "emails": ["shalit@cs.nyu.edu", "frejohk@chalmers.se", "dsontag@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of us are able to surpass ourselves by putting ourselves at the center. (...) In fact, it is the case that most of us are able to surpass ourselves. (...) It is the case that they are able to trump themselves. (...) It is the case that they are able to trump themselves. (...) It is the case that they are able to trump themselves. (...) It is the case that they are able to trump themselves. (...) It is the case that they are able to trump themselves. (...) It is the case that they are able to trump themselves. (...) It is the case that they are able to trump themselves. (...) It is the case. (...) It is the case. (...) It is the case. (...) It is. (...) It is. (...) It is the case. (...) It is. (...) It is the case. (...) It is the case. (... It is. (...) It is the case. (... It is.) It is the case. (... It is. (...) It is. It is the case. (... It is. It is the case. (... It is.) It is the case. (... It is."}, {"heading": "2 Counterfactual learning bound", "text": "In this section, we will consider several limitations related to the learning of representations, factual and counterfactual losses and errors in estimating the treatment effect for unit X. The most important quantity bound by us is \"CF: the expected loss of a model, putting the expectation above the counterfactual distribution defined below.\" The limitations are expressed as \"F: the expected loss of the same model over the factual distribution together with a distance measure between the distribution of treated and control units.\" The term \"F\" is the classic generalization error of machine learning and can in turn be limited upwards by the empirical error over the observations and model complexity concepts applying the known theory of machine learning. [28] All the evidence is included in the supplement."}, {"heading": "2.1 Problem setup", "text": "We will apply the following assumptions and notations: The most important notations are in the notation box in addition. The space of the covariates is a limited subset X \u00b7 Rd. The result of the treatment and control is the factual distribution depending on the treatment: pF (x): pF (x) = 1), and pt = 0 (x): pF (x) = 0 (x). The counterfactual distribution is the factual distribution with the treatment: pCF (x, t): pF (x, 1 \u2212 t) = 0 (x)."}, {"heading": "2.2 Bounds", "text": "We assume that it is an \"unnormalized\" integral probability, which F. Lass Y1, Y1, Y2, Y2, Y2, Y2, Y2, Y2, Y2, Y2, Y2, Y2, Y2, Y2, Y2, Y3, Y3, Y3, Y3, Y3, Y3, Y3, Y4, Y3, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, 7, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, 7, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, 7, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, Y5, 7, Y5, Y5, 7, Y5, Y5, Y5, 7, Y5, Y5, 7, Y5, Y5, 7, Y5, 7, 7, Y5, Y5, 7, 7, Y5, Y5, 7, Y5, Y5,"}, {"heading": "3 Our approach", "text": "We propose a general framework for counterfactual estimates based on the theoretical results of Section 2. Our algorithm is a single regulated minimization method that simultaneously corresponds to both a balanced representation of the data and a hypothesis for the result, in contrast to [20], which has proposed a two-step procedure that corresponds to its theoretical results at the discrepancy distance. [7] We note that our framework is also more flexible in practice, as our theory supports several equilibrium quantities that can be efficiently minimized; this is rarely true of variants of the discrepancy distance. We minimize the following objective gradations."}, {"heading": "4 Experiments", "text": "We evaluate our framework in counterfactual regression and classification tasks by using various functions to measure imbalances, including Waterstone Distance and MMD, and compare them with established methods. We report on the absolute error or bias in estimating average and individual treatment effects, ATE and ITE, as well as the precision in estimating heterogeneous effects (PEHE) [19], PEHE = 1 n, the n i = 1 [(Y1 (xi) \u2212 Y0 (xi) \u2212 g (xi, 0) \u2212 g (xi, 0)] 2, where g (x, t) is the reproduced result for individual x under treatment. We also report on the RMSE of the predicted actual result (RMSEfact) or the binary classification error (Errfact). Standard methods for hyperparameter parameter selection such as cross validation are not applicable because there are no contrafactual samples."}, {"heading": "4.1 Simulated outcome - IHDP", "text": "The covariates are from a randomized experiment that examines the effects of childcare and home visits on (future) cognitive test scores. Imbalance in the covariates was artificially introduced by removing a biased subset of the treatment population. The dataset includes 747 observations (139 treated, 608 controlled) and 25 covariates that measure aspects of children and their mothers, see Hill (2011) [19]. We use the log-linear outcome model implemented as setting \"B\" in the NPCI package.4 We also evaluate our method on another semi-synthetic dataset called News [20], the results of which are included in the supplementation.The results of the IHDP experiments are presented in Table 1. We see that in general non-linear estimators such as BART, CFR, CN and the square metrics from Forest.com."}, {"heading": "4.2 Real-world outcome - Jobs", "text": "LaLonde [22] conducted a well-known observational study based on the National Supported Work (NSW) program. It combines a randomized study based on NSW with observational data to form a larger observational data set [29]. We refer to this data set as jobs. The initial result of the forecast is 1978 income and the 8 original covariates include age, education, ethnicity, and income in 1974 and 1975. To evaluate our framework for classification, we construct an alternative binary task for predicting unemployment in the jobs study, i.e. the event that is y = 0. We use the expanded functionality of Dehejia & Wahba [10]. In the note by Smith et al. [29] we use the experimental sample of LaLonde (297 treated, 425 controlled) and the PSID comparison group (2490 controlled). We can calculate the average treatment effect for the treated subjects (compared jobs = better ATT 886 results)."}, {"heading": "5 Conclusion", "text": "Our boundary refers to counterfactual conclusions to the classic problem of machine learning from finite samples, along with methods for measuring the distribution distances of finite samples. This boundary is naturally suitable for creating learning algorithms; we focus on the use of neural networks as representations and hypotheses. We apply our theory-led approach, which leads to both synthetic and real tasks, and show that our method is always consistent with or exceeds the latest methods proposed for these tasks. Important open questions are theoretical considerations in selecting the imbalance \u03b1 and how best to derive confidence intervals for predicting our model, as well as the link to adaptation problems in the domain."}, {"heading": "Acknowledgments", "text": "We thank Esteban Tabak and Marco Cuturi for fruitful discussions and Stefan Wager for his help in creating the code for Causal Forests. DS and US were supported by the NSF CAREER award # 1350965."}, {"heading": "A Proofs", "text": "\"We assume that the distribution relations between the individual countries are different.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" \"We.\" \"\" We. \"\" We. \"\" We. \"\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\""}, {"heading": "B Treatment effect estimation loss", "text": "We now show that minimizing counterfactual loss (x) is closely related to minimizing the error in estimating the customized treatment effect (x) = Y1 (x) \u2212 Y0 (x) \u00b7 Y0 (x) \u00b7 Y0 (x) \u00b7 Y0 (x) \u00b7 Y0 (x) \u00b7 Y0 (x) \u00b7 Y0 (x) \u00b7 Y0 (x) \u00b7 Y0 (x) \u00b7 Y0 (x) \u00b7 Y0 (x) \u00b7 Y0 (x) \u00b7 Y0 (x) \u00b7 Y0 (x) \u00b7 Y1 (x) \u00b7 Y0 (x) \u00b7 Y1 (x) \u00b7 Y1 (x) \u00b7 Y1 (x) \u00b7 \u00b7 \u00b7 (x) \u00b7 \u00b7 X) \u00b7 (x) \u00b7 (x) \u00b7 (x) \u00b7 (x) \u00b7 (x), x (x), x (x), x (x), x (x), x (x), x (x)."}, {"heading": "C Algorithmic details", "text": "We give information about the algorithms used in our framework. First, we restate algorithm 1.algorithm 3 (expensive), we calculate the stochastic gradient of Waterstone distance1: Input: Factual (x1, t1, yF1),., (xim, tn, yFn), representation network \u03a6W with current weights of W 2: Random Sample a mini-batch with m \u00b2 control units (xim, 0, y F i1),., (xim, y F im), (xim, y F im),.,., (xi2m, y F i2m) 3: Calculate the m, pairwise distance matrix between all treatments and control pairs M (ell W): Mkl (sp.) =, (xim) --W (xim + l),."}, {"heading": "D Empirical results", "text": "D.1 General observationAlthough not visible in the tables due to averaging, for some of the 1000 findings of the IHDP dataset the imbalance penalty is not effective (neither positive nor negative).We believe that this is due to the fact that these sentences are already fairly balanced. We also point out that in some cases methods with linear hypotheses perform best in treatment by removing any influence of covariates on the model. Indeed, this is what L + R and BLR do on IHDP.D.2 IHDP and NewsJohansson et al. [20] introduced another semi-simulated dataset called News, based on the topic modeling of a collection of news articles. Covariates represent the number of words in a pre-specified vocabulary that are relevant to at least one of the topics.The result and treatment models are based on the topic distribution of documents. The theorem includes 5000 observations (articles) with 3477 covariates (words) of the news, both the IDP outcomes and the HDP outcomes."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "There is intense interest in applying machine learning methods to problems of<lb>causal inference which arise in applications such as healthcare, economic policy,<lb>and education. In this paper we use the counterfactual inference approach to causal<lb>inference, and propose new theoretical results and new algorithms for performing<lb>counterfactual inference. Building on an idea recently proposed by Johansson et al.<lb>[20], our results and methods rely on learning so-called \u201cbalanced\u201d representations:<lb>representations that are similar between the factual and counterfactual distributions.<lb>We give a novel, simple and intuitive bound, showing that the expected counter-<lb>factual error of a representation is bounded by a sum of the factual error of that<lb>representation and the distance between the factual and counterfactual distributions<lb>induced by the representation. We use Integral Probability Metrics to measure<lb>distances between distributions, and focus on two special cases: the Wasserstein<lb>distance and the Maximum Mean Discrepancy (MMD) distance. Our bound leads<lb>directly to new algorithms, which are simpler and easier to employ compared to<lb>those suggested in [20]. Experiments on real and simulated data show the new<lb>algorithms match or outperform state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}