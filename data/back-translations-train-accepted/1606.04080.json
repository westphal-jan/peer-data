{"id": "1606.04080", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Matching Networks for One Shot Learning", "abstract": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.", "histories": [["v1", "Mon, 13 Jun 2016 19:34:22 GMT  (2522kb,D)", "http://arxiv.org/abs/1606.04080v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["oriol vinyals", "charles blundell", "tim lillicrap", "koray kavukcuoglu", "daan wierstra"], "accepted": true, "id": "1606.04080"}, "pdf": {"name": "1606.04080.pdf", "metadata": {"source": "CRF", "title": "Matching Networks for One Shot Learning", "authors": ["Oriol Vinyals", "Charles Blundell"], "emails": ["vinyals@google.com", "cblundell@google.com", "countzero@google.com", "korayk@google.com", "wierstra@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "2 Model", "text": "Our non-parametric approach to solving one-shot learning is based on two components, which we describe in the following sections: First, our model architecture follows recent advances in neural networks supplemented by memory (as described in Section 3); second, with a (small) support set S, our model defines a function cS (or classifier) for each S, i.e. a figure S \u2192 cS (.); and second, we apply a training strategy tailored to one-shot learning from the support set S."}, {"heading": "2.1 Model Architecture", "text": "In recent years, many groups have found ways to supplement neural network architectures with external memories and other components that make them more \"computer-like.\" We draw inspiration from models such as sequence to sequence (seq2seq) with attention (2), memory networks [29], and pointer networks [27]. In all of these models, a neural attention mechanism is defined that is often completely differentiated to find (or read) access to a memory matrix that stores useful information to solve the task. Typical applications of this kind include machine translation, speech recognition, or questions that are answered. More generally, these architecture models P (B), in which A and / or B can be a sequence (as in seq2seq models), are more interesting for us, a sentence [26].Our contribution is to throw the problem of one-sided learning within the set framework [26]."}, {"heading": "2.1.1 The Attention Kernel", "text": "Equation 1 is based on the choice of a (.,.), the attention mechanism, which fully specifies the classifier. The simplest form this takes (and which has very close relations to common attention models and core functions) is the use of the Softmax over the cosinal distance c, i.e., a (x, xi) = e (f (x), g (xi) / k = 1 ec (f (x), g (xj)), with functions f and g being suitable neural networks (potentially with f = g) for embedding x and xi. In our experiments we will see examples where f and g are parameterized differently as deep revolutionary networks for image tasks (as in VGG [22] or Inception [24]) or a simple form for language tasks (see section 4)."}, {"heading": "2.1.2 Full Context Embeddings", "text": "The most important innovation of our model is the reinterpretation of a well-studied framework (neural networks with external memory) to enable one-off learning. Closely related to metric learning, the embedding functions f and g act as an elevator to mark room X to achieve maximum accuracy through the classification function described in the equation. Despite the fact that the classification strategy is entirely dependent on the total support by P (. | x, S), the embedding we \"care\" for the cosine similarity, \"point\" or simply calculating the nearest neighbor is shortsighted in the sense that each element xi is embedded by g (xi) independently of other elements in the support group S. In addition, S should be able to change how we embed the test image x by f. We suggest embedding the elements of the group by a function that will include the full set S as input, i.e., xi (S)."}, {"heading": "2.2 Training Strategy", "text": "In the previous subsection, we have described matching networks that map a support group to a classification function, S \u2192 c (x). We achieve this by modifying the set-to-set paradigm, which is extended by attention, whereby the resulting mapping corresponds to the form P\u03b8 (. | x, S), whereby we point out that \u03b8 are the parameters of the model (i.e. the previously described embedding functions f and g). The training procedure must be carefully selected in order to draw conclusions about the examination time. Our model must perform well with support groups S that contain classes that were never seen during the training. Specifically, let us define a task T as distribution over possible label sets Sample Sample Samplement Samplement Samplement Samplement Samplement Sampling Sample Sample Sample. Typically, we consider T to uniformly weight all data sets up to a few unique classes (e.g. 5), with a few examples per class (e.g. up to 5)."}, {"heading": "3 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Memory Augmented Neural Networks", "text": "A current wave of models that go beyond the \"static\" classification of solid vectors into their classes has reshaped current research and industrial applications alike, most notably with the massive introduction of LSTMs [8] in a variety of tasks such as language [7], translation [23, 2], or tutorials [4, 27]. A key component that enabled more meaningful models was the introduction of \"content-based\" attention into [2] and \"computer-like\" architectures such as the Neural Turing Machine [4] or storage networks [29]. Our work is oriented toward the metal-mining paradigm of [21], where an LSTM learned to quickly learn from sequentially presented data, but we treat the data as a group. The unique learning task we defined at Penn Treebank [15] relates to evaluation techniques and models presented in [6], and we discuss this in Section 4."}, {"heading": "3.2 Metric Learning", "text": "As discussed in Section 2, there are many linkages between content-based attention, kernel-based neighbor learning, and metric learning [1]. The most relevant work is Neighborhood Component Analysis (NCA) [18] and the subsequent nonlinear version [20]. The loss is very similar to ours, except that we use the entire S support set instead of pairwise comparisons that are more suitable for one-time learning. Successor work in the form of profound Constitutional Siamese [11] networks included much more powerful nonlinear mappings. Other losses that include the notion of a set (but use less powerful metrics) were proposed in [28]. Finally, the one-time learning work in [14] was inspiring and also provided us with the invaluable Omniglot dataset - known as the \"transpose\" of MNIST. Other work used zero-shot learning on ImageNet, for example [17]."}, {"heading": "4 Experiments", "text": "In this section, we describe the results of many experiments by comparing our Matching Networks model with strong baselines. All of our experiments revolve around the same basic task: a N-Way k-shot learning task. Each method yields a series of k-marked examples from each of the N classes that have not been trained before. Then, the task is to classify an unrelated group of unmarked examples into one of these n-classes. Therefore, the random performance in this task is 1 / N. We have compared a number of alternative models as baselines with Matching Networks. Let's introduce some notations. L \u00b2 refers to the sustained subset of labels that we use only for one recording. So, unless otherwise specified, the training is always set to 6 = L \u00b2 and the test in one-shot mode to L \u00b2. We performed one-time experiments on three sets of data: two image classification sets (Omniglot [14] and Imageesse [19, SVILC], and one language)."}, {"heading": "4.1 Image Classification Results", "text": "For visual problems, we looked at four types of baselines: raw pixel matching, discriminatory feature matching from a state-of-the-art classifier (Baseline Classifier), MAN [21], and our re-implementation of the Convolutional Siamese Net [11]. The basic classifier was trained to classify an image into one of the original classes included in the training dataset, but to exclude the N classes so as not to give it an unfair advantage (i.e. trained to classify classes in 6 = L). We then took this network and used the features from the last layer (before Softmax) for the next neighbor matching, a strategy commonly used in computer vision [3] and achieved excellent results in many tasks. Subsequently [11], the Convolutionary Siamese networks were trained on one or other task of the original training dataset, and then the last layer was used for the next neighbor matching."}, {"heading": "4.1.1 Omniglot", "text": "Omniglot [14] consists of 1623 characters from 50 different alphabets, each of which has been hand-drawn by 20 different people. The large number of classes (characters) with relatively little data per class (63), makes this an ideal dataset for testing small-scale one-shot classification. The N-path Omniglot task is as follows: Select N unseen character classes, regardless of the alphabet, such as L. Enter the model with a drawing of each of the N characters as S \u0445 L and a batch B. Following [21], we expand the dataset with random rotations by a multiple of 90 degrees and use 1200 characters for training, and the remaining character classes for evaluation. We used a simple but powerful CNN as an embedding function - consisting of a stack of modules, each of which is a 3 \u00d7 3 conversion with 64 filters, followed by batch normalization [10], a relativity-2 \u00d7 2 and pooling."}, {"heading": "4.1.2 ImageNet", "text": "This year it is so far that it will only be once before it goes to the next round."}, {"heading": "4.1.3 One-Shot Language Modeling", "text": "The task is as follows: In view of a query sentence with one missing word in this article and a series of sentences, each with a missing word and a corresponding caption, we can only select the following sentences: There is only one example of the words on the right not being available and the labels on the left are referred to as 1-hot-of-5 vectors. 1. An experimental vaccine can alter the immune response of people infected with the virus. < < < < < < < < < < # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160;"}, {"heading": "5 Conclusion", "text": "In this paper, we introduced Matching Networks, a new neural architecture that, through its corresponding training regime, is able to perform a variety of state-of-the-art one-shot classification tasks. There are a few key insights in this work. First, one-shot learning is much easier if you train the network to do one-shot learning. Second, non-parametric structures in a neural network make it easier for networks to remember and adapt to new training sets in the same tasks. Combining these observations together, Matching Networks result. In addition, we have defined new one-shot tasks on ImageNet, a reduced version of ImageNet (for quick experimentation) and a language modeling task. An obvious drawback of our model is the fact that the calculation for each gradient update becomes more expensive as the S support set increases in size. Although there are sparse and sampling-based methods to focus on this as a major part of our efforts to streamline up."}, {"heading": "Acknowledgements", "text": "We would like to thank Nal Kalchbrenner for brainstorming around the design of the g function and Sander Dieleman and Sergio Guadarrama for their help in building ImageNet. We would also like to thank Simon Osindero for useful discussions around the tasks discussed in this post, as well as Theophane Weber and Remi Munos for tracking some previous developments. Karen Simonyan and David Silver helped with the manuscript, as well as many at Google DeepMind. Thanks also to Geoff Hinton and Alex Toshev for discussing our results."}, {"heading": "A Model Description", "text": "In this section, we specify the models that embed functions f and g on the entire support group. \"hi = 02q.\" Much previous work has described similar mechanisms, so we leave the exact details for this appendix.A.1 The fully conditional embedding of fAs described in Section 2.1.2, the embedding function for an example x in batch B is as follows: f (x, S) = attLSTM (f), g (S), K), where f \"is a neural network (e.g. VGG or conception, as described in the main text).We define K as the number of\" processing steps \"after the work of [26], g blocks\" process. \"g (S) represents the embedding function g that is applied to each element."}, {"heading": "C PTB Class Splits", "text": "In fact, it is not that we did not agree on a common denominator, but that we could have agreed on a common denominator, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}], "references": [{"title": "Locally weighted learning", "author": ["C Atkeson", "A Moore", "S Schaal"], "venue": "Artificial Intelligence Review", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D Bahdanau", "K Cho", "Y Bengio"], "venue": "ICLR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J Donahue", "Y Jia", "O Vinyals", "J Hoffman", "N Zhang", "E Tzeng", "T Darrell"], "venue": "ICML", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural turing machines", "author": ["A Graves", "G Wayne", "I Danihelka"], "venue": "arXiv preprint arXiv:1410.5401", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["K Hermann", "T Kocisky", "E Grefenstette", "L Espeholt", "W Kay", "M Suleyman", "P Blunsom"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["F Hill", "A Bordes", "S Chopra", "J Weston"], "venue": "arXiv preprint arXiv:1511.02301", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G Hinton"], "venue": "Signal Processing Magazine,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Long short-term memory", "author": ["S Hochreiter", "J Schmidhuber"], "venue": "Neural computation", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep metric learning using triplet network", "author": ["E Hoffer", "N Ailon"], "venue": "Similarity-Based Pattern Recognition", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S Ioffe", "C Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Siamese neural networks for one-shot image recognition", "author": ["G Koch", "R Zemel", "R Salakhutdinov"], "venue": "ICML Deep Learning workshop", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional deep belief networks on cifar-10", "author": ["A Krizhevsky", "G Hinton"], "venue": "Unpublished", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A Krizhevsky", "I Sutskever", "G Hinton"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "One shot learning of simple visual concepts", "author": ["BM Lake", "R Salakhutdinov", "J Gross", "J Tenenbaum"], "venue": "CogSci", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["MP Marcus", "MA Marcinkiewicz", "B Santorini"], "venue": "Computational linguistics", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["T Mikolov", "M Karafi\u00e1t", "L Burget", "J Cernock\u1ef3", "S Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["M Norouzi", "T Mikolov", "S Bengio", "Y Singer", "J Shlens", "A Frome", "G Corrado", "J Dean"], "venue": "arXiv preprint arXiv:1312.5650", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Neighbourhood component analysis", "author": ["S Roweis", "G Hinton", "R Salakhutdinov"], "venue": "NIPS", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O Russakovsky", "J Deng", "H Su", "J Krause", "S Satheesh", "S Ma", "Z Huang", "A Karpathy", "A Khosla", "M Bernstein", "A Berg", "L Fei-Fei"], "venue": "IJCV", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a nonlinear embedding by preserving class neighbourhood structure", "author": ["R Salakhutdinov", "G Hinton"], "venue": "AISTATS", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Meta-learning with memory-augmented neural networks", "author": ["A Santoro", "S Bartunov", "M Botvinick", "D Wierstra", "T Lillicrap"], "venue": "ICML", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K Simonyan", "A Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I Sutskever", "O Vinyals", "QV Le"], "venue": "NIPS", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C Szegedy", "W Liu", "Y Jia", "P Sermanet", "S Reed", "D Anguelov", "D Erhan", "V Vanhoucke", "A Rabinovich"], "venue": "CVPR", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C Szegedy", "V Vanhoucke", "S Ioffe", "J Shlens", "Z Wojna"], "venue": "arXiv preprint arXiv:1512.00567", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Order matters: Sequence to sequence for sets", "author": ["O Vinyals", "S Bengio", "M Kudlur"], "venue": "arXiv preprint arXiv:1511.06391", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Pointer networks", "author": ["O Vinyals", "M Fortunato", "N Jaitly"], "venue": "NIPS", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K Weinberger", "L Saul"], "venue": "JMLR", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Memory networks", "author": ["J Weston", "S Chopra", "A Bordes"], "venue": "ICLR", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["W Zaremba", "I Sutskever", "O Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "Deep learning has made major advances in areas such as speech [7], vision [13] and language [16], but is notorious for requiring large datasets.", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": "Deep learning has made major advances in areas such as speech [7], vision [13] and language [16], but is notorious for requiring large datasets.", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "Deep learning has made major advances in areas such as speech [7], vision [13] and language [16], but is notorious for requiring large datasets.", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": ", nearest neighbors) do not require any training but performance depends on the chosen metric [1].", "startOffset": 94, "endOffset": 97}, {"referenceID": 17, "context": "Previous work on metric learning in non-parametric setups [18] has been influential on our model, and we aim to incorporate the best characteristics from both parametric and non-parametric models \u2013 namely, rapid acquisition of new examples while providing excellent generalisation from common examples.", "startOffset": 58, "endOffset": 62}, {"referenceID": 1, "context": "We draw inspiration from models such as sequence to sequence (seq2seq) with attention [2], memory networks [29] and pointer networks [27].", "startOffset": 86, "endOffset": 89}, {"referenceID": 28, "context": "We draw inspiration from models such as sequence to sequence (seq2seq) with attention [2], memory networks [29] and pointer networks [27].", "startOffset": 107, "endOffset": 111}, {"referenceID": 26, "context": "We draw inspiration from models such as sequence to sequence (seq2seq) with attention [2], memory networks [29] and pointer networks [27].", "startOffset": 133, "endOffset": 137}, {"referenceID": 25, "context": "More generally, these architectures model P (B|A) where A and/or B can be a sequence (like in seq2seq models), or, more interestingly for us, a set [26].", "startOffset": 148, "endOffset": 152}, {"referenceID": 25, "context": "Our contribution is to cast the problem of one-shot learning within the set-to-set framework [26].", "startOffset": 93, "endOffset": 97}, {"referenceID": 1, "context": "However, unlike other attentional memory mechanisms [2], (1) is non-parametric in nature: as the support set size grows, so does the memory used.", "startOffset": 52, "endOffset": 55}, {"referenceID": 21, "context": "In our experiments we shall see examples where f and g are parameterised variously as deep convolutional networks for image tasks (as in VGG[22] or Inception[24]) or a simple form word embedding for language tasks (see Section 4).", "startOffset": 140, "endOffset": 144}, {"referenceID": 23, "context": "In our experiments we shall see examples where f and g are parameterised variously as deep convolutional networks for image tasks (as in VGG[22] or Inception[24]) or a simple form word embedding for language tasks (see Section 4).", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "This kind of loss is also related to methods such as Neighborhood Component Analysis (NCA) [18], triplet loss [9] or large margin nearest neighbor [28].", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "This kind of loss is also related to methods such as Neighborhood Component Analysis (NCA) [18], triplet loss [9] or large margin nearest neighbor [28].", "startOffset": 110, "endOffset": 113}, {"referenceID": 27, "context": "This kind of loss is also related to methods such as Neighborhood Component Analysis (NCA) [18], triplet loss [9] or large margin nearest neighbor [28].", "startOffset": 147, "endOffset": 151}, {"referenceID": 7, "context": "We use a bidirectional Long-Short Term Memory (LSTM) [8] to encode xi in the context of the support set S, considered as a sequence (see appendix for a more precise definition).", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "This is most notable in the massive adoption of LSTMs [8] in a variety of tasks such as speech [7], translation [23, 2] or learning programs [4, 27].", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "This is most notable in the massive adoption of LSTMs [8] in a variety of tasks such as speech [7], translation [23, 2] or learning programs [4, 27].", "startOffset": 95, "endOffset": 98}, {"referenceID": 22, "context": "This is most notable in the massive adoption of LSTMs [8] in a variety of tasks such as speech [7], translation [23, 2] or learning programs [4, 27].", "startOffset": 112, "endOffset": 119}, {"referenceID": 1, "context": "This is most notable in the massive adoption of LSTMs [8] in a variety of tasks such as speech [7], translation [23, 2] or learning programs [4, 27].", "startOffset": 112, "endOffset": 119}, {"referenceID": 3, "context": "This is most notable in the massive adoption of LSTMs [8] in a variety of tasks such as speech [7], translation [23, 2] or learning programs [4, 27].", "startOffset": 141, "endOffset": 148}, {"referenceID": 26, "context": "This is most notable in the massive adoption of LSTMs [8] in a variety of tasks such as speech [7], translation [23, 2] or learning programs [4, 27].", "startOffset": 141, "endOffset": 148}, {"referenceID": 1, "context": "A key component which allowed for more expressive models was the introduction of \u201ccontent\u201d based attention in [2], and \u201ccomputer-like\u201d architectures such as the Neural Turing Machine [4] or Memory Networks [29].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "A key component which allowed for more expressive models was the introduction of \u201ccontent\u201d based attention in [2], and \u201ccomputer-like\u201d architectures such as the Neural Turing Machine [4] or Memory Networks [29].", "startOffset": 183, "endOffset": 186}, {"referenceID": 28, "context": "A key component which allowed for more expressive models was the introduction of \u201ccontent\u201d based attention in [2], and \u201ccomputer-like\u201d architectures such as the Neural Turing Machine [4] or Memory Networks [29].", "startOffset": 206, "endOffset": 210}, {"referenceID": 20, "context": "Our work takes the metalearning paradigm of [21], where an LSTM learnt to learn quickly from data presented sequentially, but we treat the data as a set.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "The one-shot learning task we defined on the Penn Treebank [15] relates to evaluation techniques and models presented in [6], and we discuss this in Section 4.", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "The one-shot learning task we defined on the Penn Treebank [15] relates to evaluation techniques and models presented in [6], and we discuss this in Section 4.", "startOffset": 121, "endOffset": 124}, {"referenceID": 0, "context": "As discussed in Section 2, there are many links between content based attention, kernel based nearest neighbor and metric learning [1].", "startOffset": 131, "endOffset": 134}, {"referenceID": 17, "context": "(NCA) [18], and the follow up non-linear version [20].", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "(NCA) [18], and the follow up non-linear version [20].", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "Follow-up work in the form of deep convolutional siamese [11] networks included much more powerful non-linear mappings.", "startOffset": 57, "endOffset": 61}, {"referenceID": 27, "context": "Other losses which include the notion of a set (but use less powerful metrics) were proposed in [28].", "startOffset": 96, "endOffset": 100}, {"referenceID": 13, "context": "Lastly, the work in one-shot learning in [14] was inspirational and also provided us with the invaluable Omniglot dataset \u2013 referred to as the \u201ctranspose\u201d of MNIST.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We ran one-shot experiments on three data sets: two image classification sets (Omniglot [14] and ImageNet [19, ILSVRC-2012]) and one language modeling (Penn Treebank).", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "For vision problems, we considered four kinds of baselines: matching on raw pixels, matching on discriminative features from a state-of-the-art classifier (Baseline Classifier), MANN [21], and our reimplementation of the Convolutional Siamese Net [11].", "startOffset": 183, "endOffset": 187}, {"referenceID": 10, "context": "For vision problems, we considered four kinds of baselines: matching on raw pixels, matching on discriminative features from a state-of-the-art classifier (Baseline Classifier), MANN [21], and our reimplementation of the Convolutional Siamese Net [11].", "startOffset": 247, "endOffset": 251}, {"referenceID": 2, "context": "We then took this network and used the features from the last layer (before the softmax) for nearest neighbour matching, a strategy commonly used in computer vision [3] which has achieved excellent results across many tasks.", "startOffset": 165, "endOffset": 168}, {"referenceID": 10, "context": "Following [11], the convolutional siamese nets were trained on a same-or-different task of the original training data set and then the last layer was used for nearest neighbour matching.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "MANN (NO CONV) [21] Cosine N 82.", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "9% \u2013 \u2013 CONVOLUTIONAL SIAMESE NET [11] Cosine N 96.", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "5% CONVOLUTIONAL SIAMESE NET [11] Cosine Y 97.", "startOffset": 29, "endOffset": 33}, {"referenceID": 13, "context": "Omniglot [14] consists of 1623 characters from 50 different alphabets.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "Following [21], we augmented the data set with random rotations by multiples of 90 degrees and used 1200 characters for training, and the remaining character classes for evaluation.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "We used a simple yet powerful CNN as the embedding function \u2013 consisting of a stack of modules, each of which is a 3 \u00d7 3 convolution with 64 filters followed by batch normalization [10], a Relu non-linearity and 2\u00d7 2 max-pooling.", "startOffset": 181, "endOffset": 185}, {"referenceID": 10, "context": "Like the authors in [11], we also test our method trained on Omniglot on a completely disjoint task \u2013 one-shot, 10 way MNIST classification.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "This dataset is more complex than CIFAR10 [12], but fits in memory on modern machines, making it very convenient for rapid prototyping and experimentation.", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "Our baseline classifier for this data set was Inception [25] trained to classify on all classes except those in the test set of classes (for randImageNet) or those concerning dogs (for dogsImageNet).", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "Sentences were taken from the Penn Treebank dataset [15].", "startOffset": 52, "endOffset": 56}, {"referenceID": 29, "context": "We compared our one-shot matching model to an oracle LSTM language model (LSTM-LM) [30] trained on all the words.", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "Two related tasks are the CNN QA test of entity prediction from news articles [5], and the Children\u2019s Book Test (CBT) [6].", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "Two related tasks are the CNN QA test of entity prediction from news articles [5], and the Children\u2019s Book Test (CBT) [6].", "startOffset": 118, "endOffset": 121}], "year": 2016, "abstractText": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.", "creator": "LaTeX with hyperref package"}}}