{"id": "1606.04164", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Zero-Resource Translation with Multi-Lingual Neural Machine Translation", "abstract": "In this paper, we propose a novel finetuning algorithm for the recently introduced multi-way, mulitlingual neural machine translate that enables zero-resource machine translation. When used together with novel many-to-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivot-based translation strategy, while keeping only one additional copy of attention-related parameters.", "histories": [["v1", "Mon, 13 Jun 2016 22:40:33 GMT  (25kb)", "http://arxiv.org/abs/1606.04164v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["orhan firat", "baskaran sankaran", "yaser al-onaizan", "fatos t yarman-vural", "kyunghyun cho"], "accepted": true, "id": "1606.04164"}, "pdf": {"name": "1606.04164.pdf", "metadata": {"source": "CRF", "title": "Zero-Resource Translation with Multi-Lingual Neural Machine Translation", "authors": ["Orhan Firat", "Baskaran Sankaran", "Fatos T. Yarman"], "emails": ["orhan.firat@ceng.metu.edu.tr"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.04 164v 1 [cs.C L] 13 Ju"}, {"heading": "1 Introduction", "text": "A recently introduced neural machine translation (Forcada and N-eco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has proven to be a platform for new possibilities in machine translation research. Instead of word-based translation with language-specific preprocessing, neural machine translation has been shown to work well with statistically segmented subword sequences and character sequences (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015b; Ling et al., 2015). Recent work also shows that neural machine translation provides a seamless way to integrate several modalities other than natural strategies while the author was at IBM Research. Language text in translation (Luong et al al al al al., 2015a; Caglayan et al et al., 2016) offers multilingual models of multilingual translation in language."}, {"heading": "2 Multi-Way, Multilingual", "text": "Recently, Firat et al. (2016) proposed an extension of attention-based neural machine translation (Bahdanau et al., 2015), which can handle multiple, multilingual translations with a common attention mechanism. This model is designed to handle multiple source and target languages. In this section, we give a brief overview of this multiple, multilingual model. For more details, please refer to the reader (Firat et al., 2016)."}, {"heading": "2.1 Model Description", "text": "The goal of the multi-track, multilingual model is to build a neural translation model that can translate a source sentence given in one of the N languages into one of the M target languages. In order to handle these N source and M target languages, the model consists of N encoders and M decoders. In contrast to these language-specific encoders and decoders, only a single attention mechanism is shared in all M-N language pairs. Encoder An encoder for the n th source language reads a source sentence X = (x1,., xTx) as a sequence of linguistic symbols and returns a series of context vectors."}, {"heading": "2.2 Learning", "text": "The formation of this multi-page, multi-lingual model does not require multi-path parallel corpora, but only one set of bilingual corpora. For each bilingual pair, the conditional log probability of a ground truth translation of a source sentence is maximized by adjusting the relevant parameters according to the slope of the log probability."}, {"heading": "3 Translation Strategies", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 One-to-One Translation", "text": "In the original paper by Firat et al. (2016), only one translation strategy was evaluated, namely one-to-one translation. This one-to-one strategy works on a source sentence given in a language by taking the encoder of that source language, the decoder of a target language, and the mechanism of joint attention. These three components are glued together as if they form a single pair of neural translation models and translate the source sentence into a target language.However, we note that this is not the only translation strategy available with the multi-page, multilingual model. Since we end up with multiple encoders, multiple decoders, and a common attention mechanism, this model naturally allows us to take advantage of a source sentence given in multiple languages, which leads to a multilingual translation strategy recently proposed by Zoph and Knight (2016) in the context of neural machine translation. Unlike (Zoph and Knight), this model becomes multilingual in a parallel way, but not multilingual and multilingual."}, {"heading": "3.2 Many-to-One Translation", "text": "In this section, we will consider a case where a source sentence in two languages, X1 and X2, is given. However, each of the approaches described below trivially applies to more than two source languages. In this multilingual model, multiple translation can be regarded as the mean of two separate translation paths. In the case of Es + Fr to En, for example, we want to combine Es \u2192 En and Fr \u2192 En to get a better translation into English. We note that in the multiple multilingual model, there are two points where this averaging can happen. Early averages The first candidate is two translation paths in calculating the time-phased context vector (see Eq. (1). At each point in the decoder, we calculate a time-dependent context vector for each source language, c1t and c 2 t for the two source languages."}, {"heading": "4 Experiments: Translation Strategies and Multi-Source Translation", "text": "Before proceeding with machine translation without resources, we first evaluate the translation strategies described in the previous section on multisource translation, as these translation strategies form a fundamental basis on which we expand the multiple, multilingual model of machine translation without resources."}, {"heading": "4.1 Settings", "text": "When evaluating multi-source translation strategies, we use English, Spanish and French and focus on a scenario where only En-Es and En-Fr parallel corpora are available."}, {"heading": "4.1.1 Corpora", "text": "En-Es We combine the following corpora to 34.71 m parallel Es-En sets: UN (8.8 m), Europarl-v7 (1.8 m), News Comment v7 (150k), LDC2011T07-T12 (2.9 m) and Internal Technical Domain Data (21.7 m). En-Fr We combine the following corpora to 65.77 m parallel En-Fr sets: UN (9.7 m), Europarl-v7 (1.9 m), News Comment v7 (1.2 m), LDC2011T07-T10 (1.6 m), ReutersUN (4.5 m), Internal Technical Domain Data (23.5 m) and Gigaword R2 (20.66 m). Rating sets We use WMT's Newstest-2012 and Newstest-2013 as development and test sets. Monolingual Corpora We do not use additional tools."}, {"heading": "4.2 Models and Training", "text": "Starting from the publicly available code (Firat et al., 2016), we made two changes to the original code. First, we replaced the decoder with the conditioned recurring network with the attention mechanism as outline in (Firat and Cho, 2016). Second, we fed a binary indicator vector whose encoder (s) the output set of each decoder's output layer (gmw in Eq. (4) was processed. Each dimension of the indicator vector corresponds to a source language, and in the case of multisource translation, there may be more than one dimension, which is 1. We construct the following models: four single-pair models (Es-En and Fr-En) and one multi-way model, multilingual model (Es, Fr, En-Es, En). As proposed by Firat et al. (2016), we distribute an attention mechanism for the latter case."}, {"heading": "4.3 One-to-One Translation", "text": "First, we confirm that the multiple, multilingual translation model actually works as well as single-pair models on the translation pathways taken into account during the training (Firat et al., 2016). In Table 2, we present the re-https: / / github.com / nyu-dl / dl4mt-multi T-B score, which is defined as TER \u2212 BLEU2, which we considered more stable than TER or BLEU solely for the purpose of early stop (Zhao and Chen, 2009).sulks in four language pairs (Es \u2194 En and Fr \u2194 En).It is clear that the multilingual model actually performs comparably well in all four cases with fewer parameters (due to the mechanism of shared attention.) As previously observed in (Firat et al., 2016), we also see that the multilingual model performs better when a target language is English."}, {"heading": "4.4 Many-to-One Translation", "text": "We look at the translation of a pair of source sentences into Spanish (Eb) and French (Fr) into English (En). It is important to note that the multilingual model was not trained with a multilingual parallel corpus. Nevertheless, we note that the early mean strategy has implicitly encouraged translation quality (measured in BLEU) in the case of the test sentence by 3 points (compare Table 2 (a-b) and Table 3 (a). We suspect that this, as training of the multilingual model, has encouraged finding a common context vector space across multiple source languages. However, the late mean strategy exceeds the early mean formation in both cases of the multilingual model and a pair of monolingual models (see Table 3 (b)), although marginal. The best quality was observed when the early and late mean formation of strategies were combined at the starting level, bringing us up to EU + 3.5 BLc (Table 2) and 3."}, {"heading": "5 Zero-Resource Translation Strategies", "text": "The network architecture of the multipage, multilingual model indicates the potential to translate between two languages without a direct parallel corpus being available. In the environment considered in this paper (see paragraph 4.1,), these translation paths correspond to Es \u2194 Fr, since only parallel corpus was used for training. However, the most naive approach to translating on a zero resource path is to treat them simply like any other path taken in the context of training, which is in line with the one-to-one strategy in paragraph 3.1. However, our experiments have shown that this naive approach does not work at all, as can be seen in Table 4 (a). In this section we will examine this potential of zero resource translation with the multipage, multilingual model in depth. Specifically, we propose a number of approaches that enable zero resource translation without the need for additional bilingual corpus."}, {"heading": "5.1 Pivot-based Translation", "text": "The first set of approaches takes advantage of the fact that the target zero resource translation path can be specifically decomposed into a sequence of high resource translation paths (Wu and Wang, 2007; Utiyama and Isahara, 2007). In our case, for example, It \u2192 Fr can be decomposed into a sequence of Es \u2192 En and En \u2192 Fr. In other words, we translate each source sentence (It) independently into a rotary language (En) and then translate the English translation into a target language (Fr). One-to-One translation The most basic approach is to perform each translation path in the decomposed sequence. This one-to-one approach introduces only a minimal complexity of the translation (the multiplicative factor of two). We can further improve this one-to-one pivot-based translation path by adding a set of k-best translations from the first stage of the overall practice, but only by increasing the complexity of the En."}, {"heading": "5.2 Finetuning with Pseudo Parallel Corpus", "text": "The failure of naive zero-resource translation earlier (see Table 4 (a)) suggests that the context vectors returned by the encoder are not compatible with the decoder if the combination was not included during the training. However, the good translation qualities of the translation paths included in the training imply that the representations learned by the encoders and decoders are good. Based on these two observations, we suspect that all that is required for a zero-resource translation path is a simple customization that makes the context vectors of the encoder compatible with the target decoder. Therefore, we propose to translate this zero-resource translation path without additional parallel corpus.First, we create a small set of pseudo-bilingual sentence pairs for the zero-resource language pair (Es \u2192 Fr) in the interest."}, {"heading": "6 Experiments: Zero-Resource Translation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Without Finetuning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1.1 Settings", "text": "To evaluate zero-resource translation strategies, we use the same multi-page, multilingual model that was already trained in Section 4.2. We emphasize here that this model has only been practiced using bilingual parallel corpora Es-En and Fr-En without parallel corpus Es-Fr. We evaluate the proposed approaches to zero-resource translation using the same multi-page, multilingual model from Section 4.1. We selectively choose the path from Spanish to French (Es \u2192 Fr) as the destination path to zero-resource translation."}, {"heading": "6.1.2 Result and Analysis", "text": "As already mentioned, we observed that the multiple, multilingual model could not translate directly between two languages if the translation path between these two languages was not included in the training (Table 4 (a). On the other hand, the model was able to translate decently with the pivoting one-to-one translation strategy, as can be seen in Table 4 (b). Unsurprisingly, all the many-to-one strategies resulted in poorer translation quality due to the inclusion of the useless translation path (direct path between the zero-resource pair, Es-Fr.)."}, {"heading": "6.2 Finetuning with a Pseudo Parallel Corpus", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1 Settings", "text": "The proposed finetuning strategy raises a number of questions. First, it is unclear how many pseudo-sentence pairs are needed to achieve decent translation quality. As the purpose of this finetuning level is simply to adjust the common attention mechanism so that it can build a proper bridge from source-side encoder to target-side decoder, we expect it to work with only a small number of pseudo-sentence pairs. We verify this by creating pseudo-corpus of different sizes - 1k, 10k, 100k and 1m.by combining the subsets of UN (7.8m), Europral-v7 (1.8m), OpenSubtitles-2013 (1m), Message Comments-v7 (174k), LDC2011T07 (335k, 100k) and Finn-3 (1m)."}, {"heading": "6.2.2 Result and Analysis", "text": "Table 5 summarizes all the results; the most important observation is that the proposed finetuning strategy with pseudo-parallel sentence pairs is sufficient only if it has pairs with pseudo-parallel sentence pairs that exceed the pivot-based approach (using the early mean strategy of Sec. 4.4), even if we have used only 1,000 such pairs (compare (b) and (d). Unlike us, we are increasing the size of the pseudo-parallel corpus, we are observing a marked improvement. Furthermore, these models exhibit comparable or better than the single-pair model with 1M true parallel sentence pairs, even though they have never seen a single true bilingual sentence pair of Spanish and French (compare) and (d). Even if we trained a uniform model with 11m real parallel pairs, the model might not match or better than the multilingual model that we achieve with 1m true parallel pairs, by showing that the EU translation quality of 24.26 is only a true translation pair.26 is an interesting finding for the multilingual pair.26"}, {"heading": "7 Conclusion: Implications and Limitations", "text": "Implications There are two main findings in this paper. First, we showed that the multiple, multilingual neural translation model of Firat et al. (2016) is able to exploit common, underlying structures in many languages in order to be able to better translate when a set of sources is given in multiple languages. This confirms the usefulness of positive language transfer, which is considered an important factor for learning the human language (Odlin, 1989; Ringbom, 2007), in machine translation. Furthermore, our result extends the applicability of multisource translation (Zoph and Knight, 2016), as it does not assume the availability of multi-way parallel corporations for training. Second, the experiments with zero-resource translation have shown that it is not necessary to have a direct parallel corpus, or deep linguistic knowledge, between two languages, in order to build a machine translation system."}, {"heading": "Acknowledgments", "text": "OF thanks Georgiana Dinu and Iulian Vlad Serban for insightful conversations. KC thanks the support of Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Does multimodality help human and machine for translation and image caption", "author": ["Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u0131\u0301aMart\u0131\u0301nez", "Fethi Bougares", "Lo\u0131\u0308c Barrault", "Joost van de Weijer"], "venue": null, "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Chung et al.2016] Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Multi-task learning for multiple language translation", "author": ["Dong et al.2015] Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "DL4MT-Tutorial: Conditional gated recurrent unit with attention mechanism", "author": ["Firat", "Cho2016] Orhan Firat", "Kyunghyun Cho"], "venue": null, "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Firat et al.2016] Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Recursive hetero-associative memories for translation", "author": ["Forcada", "\u00d1eco1997] Mikel L Forcada", "Ram\u00f3n P \u00d1eco"], "venue": "In Biological and Artificial Computation: From Neuroscience to Technology,", "citeRegEx": "Forcada et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Forcada et al\\.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "The International Conference on Learning Representations (ICLR)", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Character-based neural machine translation", "author": ["Ling et al.2015] Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Luong", "Manning2016] Minh-Thang Luong", "Christopher D Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": "arXiv preprint arXiv:1511.06114", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "On the difficulty of training recurrent neural networks. arXiv preprint arXiv:1211.5063", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1511.06709", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A comparison of pivot methods for phrase-based statistical machine translation", "author": ["Utiyama", "Isahara2007] Masao Utiyama", "Hitoshi Isahara"], "venue": "In HLTNAACL,", "citeRegEx": "Utiyama et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Utiyama et al\\.", "year": 2007}, {"title": "Pivot language approach for phrase-based statistical machine translation", "author": ["Wu", "Wang2007] Hua Wu", "Haifeng Wang"], "venue": "Machine Translation,", "citeRegEx": "Wu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2007}, {"title": "A simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters", "author": ["Zhao", "Chen2009] Bing Zhao", "Shengyuan Chen"], "venue": "In HLT-NAACL,", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}, {"title": "Multi-source neural translation", "author": ["Zoph", "Knight2016] Barret Zoph", "Kevin Knight"], "venue": "In NAACL", "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "A recently introduced neural machine translation (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has proven to be a platform for new opportunities in machine translation research.", "startOffset": 49, "endOffset": 147}, {"referenceID": 2, "context": "A recently introduced neural machine translation (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has proven to be a platform for new opportunities in machine translation research.", "startOffset": 49, "endOffset": 147}, {"referenceID": 3, "context": "Rather than word-level translation with language-specific preprocessing, neural machine translation has found to work well with statistically segmented subword sequences as well as sequences of characters (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015b; Ling et al., 2015).", "startOffset": 205, "endOffset": 293}, {"referenceID": 11, "context": "Rather than word-level translation with language-specific preprocessing, neural machine translation has found to work well with statistically segmented subword sequences as well as sequences of characters (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015b; Ling et al., 2015).", "startOffset": 205, "endOffset": 293}, {"referenceID": 1, "context": "language text in translation (Luong et al., 2015a; Caglayan et al., 2016).", "startOffset": 29, "endOffset": 73}, {"referenceID": 4, "context": "Furthermore, neural machine translation has been found to translate between multiple languages, achieving better translation quality by exploiting positive language transfer (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016).", "startOffset": 174, "endOffset": 236}, {"referenceID": 5, "context": "Furthermore, neural machine translation has been found to translate between multiple languages, achieving better translation quality by exploiting positive language transfer (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016).", "startOffset": 174, "endOffset": 236}, {"referenceID": 5, "context": "In this paper, we conduct in-depth investigation into the recently proposed multi-way, multilingual neural machine translation (Firat et al., 2016).", "startOffset": 127, "endOffset": 147}, {"referenceID": 5, "context": "This result re-confirms the potential of the multi-way, multilingual model for low/zero-resource language translation, which was earlier argued by Firat et al. (2016).", "startOffset": 147, "endOffset": 167}, {"referenceID": 0, "context": "(2016) proposed an extension of attention-based neural machine translation (Bahdanau et al., 2015) that can handle multiway, multilingual translation with a shared attention mechanism.", "startOffset": 75, "endOffset": 98}, {"referenceID": 5, "context": "For more detailed exposition, we refer the reader to (Firat et al., 2016).", "startOffset": 53, "endOffset": 73}, {"referenceID": 4, "context": "Recently Firat et al. (2016) proposed an extension of attention-based neural machine translation (Bahdanau et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 15, "context": "Decoder and Attention Mechanism A decoder for the m-th target language is a conditional recurrent language model (Mikolov et al., 2010).", "startOffset": 113, "endOffset": 135}, {"referenceID": 2, "context": "\u03c6 m is a gated recurrent unit (GRU, (Cho et al., 2014)).", "startOffset": 36, "endOffset": 54}, {"referenceID": 5, "context": "In the original paper by Firat et al. (2016), only one translation strategy was evaluated, that is, one-toone translation.", "startOffset": 25, "endOffset": 45}, {"referenceID": 19, "context": "In fact, if X1 and X2 are same and the two translation paths are simply two different models trained on the same language pair\u2013 direction, this is equivalent to constructing an ensemble, which was found to greatly improve translation quality (Sutskever et al., 2014)", "startOffset": 242, "endOffset": 266}, {"referenceID": 10, "context": "Preprocessing All the sentences are tokenized using the tokenizer script from Moses (Koehn et al., 2007).", "startOffset": 84, "endOffset": 104}, {"referenceID": 5, "context": "We start from the code made publicly available as a part of (Firat et al., 2016).", "startOffset": 60, "endOffset": 80}, {"referenceID": 5, "context": "As proposed by Firat et al. (2016), we share one attention mechanism for the latter case.", "startOffset": 15, "endOffset": 35}, {"referenceID": 5, "context": "Training We closely follow the setup from (Firat et al., 2016).", "startOffset": 42, "endOffset": 62}, {"referenceID": 2, "context": "Any recurrent layer, be it in the encoder or decoder, consists of 1000 gated recurrent units (GRU, (Cho et al., 2014)), and the attention mechanism has a hidden layer of 1200 tanh units (fscore in Eq.", "startOffset": 99, "endOffset": 117}, {"referenceID": 16, "context": "The gradient is clipped to have the norm of at most 1 (Pascanu et al., 2012).", "startOffset": 54, "endOffset": 76}, {"referenceID": 5, "context": "We first confirm that the multi-way, multilingual translation model indeed works as well as singlepair models on the translation paths that were considered during training, which was the major claim in (Firat et al., 2016).", "startOffset": 202, "endOffset": 222}, {"referenceID": 5, "context": ") As observed earlier in (Firat et al., 2016), we also see that the multilingual model performs better when a target language is English.", "startOffset": 25, "endOffset": 45}, {"referenceID": 5, "context": "First, we showed that the multi-way, multilingual neural translation model by Firat et al. (2016) is able to exploit common, underlying structures across many languages in order to better translate when a source sentence is given in multiple languages.", "startOffset": 78, "endOffset": 98}, {"referenceID": 5, "context": "More investigation with a diverse set of languages needs to be done in order to make a more solid conclusion, such as was done in (Firat et al., 2016; Chung et al., 2016).", "startOffset": 130, "endOffset": 170}, {"referenceID": 3, "context": "More investigation with a diverse set of languages needs to be done in order to make a more solid conclusion, such as was done in (Firat et al., 2016; Chung et al., 2016).", "startOffset": 130, "endOffset": 170}], "year": 2016, "abstractText": "In this paper, we propose a novel finetuning algorithm for the recently introduced multiway, mulitlingual neural machine translate that enables zero-resource machine translation. When used together with novel manyto-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivotbased translation strategy, while keeping only one additional copy of attention-related parameters.", "creator": "LaTeX with hyperref package"}}}