{"id": "1606.04199", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "abstract": "Neural machine translation (NMT) aims at solving machine translation (MT) problems with purely neural networks and exhibits promising results in recent years. However, most of the existing NMT models are of shallow topology and there is still a performance gap between the single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) network, together with the interleaved bi-directional way for stacking them. Fast-forward connections play an essential role to propagate the gradients in building the deep topology of depth 16. On WMT'14 English- to-French task, we achieved BLEU=37.7 with single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. It is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. Even without considering attention mechanism, we can still achieve BLEU=36.3. After the special handling for unknown words and the model ensemble, we obtained the best score on this task with BLEU=40.4. Our models are also verified on the more difficult WMT'14 English-to-German task.", "histories": [["v1", "Tue, 14 Jun 2016 03:53:00 GMT  (153kb,D)", "http://arxiv.org/abs/1606.04199v1", "To be published on TACL 2016"], ["v2", "Wed, 15 Jun 2016 04:21:03 GMT  (153kb,D)", "http://arxiv.org/abs/1606.04199v2", "To be published on TACL 2016"], ["v3", "Sat, 23 Jul 2016 13:14:17 GMT  (153kb,D)", "http://arxiv.org/abs/1606.04199v3", "TACL 2016"]], "COMMENTS": "To be published on TACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["jie zhou", "ying cao", "xuguang wang", "peng li", "wei xu"], "accepted": true, "id": "1606.04199"}, "pdf": {"name": "1606.04199.pdf", "metadata": {"source": "CRF", "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "authors": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu"], "emails": ["zhoujie01@baidu.com", "caoying03@baidu.com", "wangxuguang@baidu.com", "lipeng17@baidu.com", "wei.xu@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "In recent years, it has become clear that it is not just a question of whether and to what extent it is actually a problem, but also of whether and to what extent it is a problem or not. (...) Furthermore, it is not that it is only a matter of time. (...) It is not as if it is a problem. (...) It is not as if it is a problem. (...) It is as if it is a problem. (...) It is not as if it is a problem. (...) \"(...) It is not as if it is a problem.\" (...) It is not as if it is a problem. (...) It is not as if it is a problem. (...) It is not as if it is. (...) It is not as if it is. (...) It is not as if it is. (...) It is not as if it is. (...)"}, {"heading": "2 Neural Machine Translation", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live."}, {"heading": "3 Deep Topology", "text": "We build the deep LSTM network with the new linear connections. The shortest paths through the proposed connections do not involve non-linear transformations and are not based on recurring calculations. We call these connections quick connections. Within the deep topology we also introduce the interleaved bi-directional way to stack the LSTM layers. And the way to extract the source sequence representation is consistently modified."}, {"heading": "3.1 Network", "text": "We have two instances of this topology called Deep-ED and Deep-Att, which relate to the extension of the encoder decoder network and the attention network. Our major innovation is made for the adjacent stacked recurrent layers, and we will start with the basic RNN model to ensure clarity. Recurrent layer: When an input layer {x1, xm} is given to a recurrent layer, the output layer can be called at any time (see Fig. 1 (a) ht = RNN (Wfxt) = RNN."}, {"heading": "3.2 Training technique", "text": "We take the parallel data as the only input without using any monolingual data for neither word representation prior to training nor speech modeling. Due to the deep bidirectional structure, we do not need to reverse the sequence sequence sequence as (Sutskever et al., 2014).The deep topology brings difficulties into model training, especially when using first order methods such as stochastic gradient descent (SGD) (LeCun et al., 1998).The parameters should be properly initialized and the convergence process would be slow. We have tried several first order optimization techniques, such as AdaDelta (Zeiler, 2012), RMSProp (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014).We have found that they are all able to significantly accelerate the process compared to simple SGD, while no significant difference in performance is observed between them. In this work, we choose Adam for the model training and its drop-out will also be used for this large scale interface."}, {"heading": "3.3 Generation", "text": "When generating sequences, we use the usual left-to-right beam search method. At any time, the word yt can be predicted as follows: y-t = arg max y P (y-y-0: t \u2212 1, x; \u03b8) (11), where y-t is the predicted target word. y-0: t \u2212 1 is the generated sequence from time step 0 to t \u2212 1. We consider nb the best candidates according to Equation 11 in each time step until the end marker is generated. The hypothesis is classified according to the overall expectancy probability of the generated sequence, although in some papers a normalized probability is taken into account (Jean et al., 2015)."}, {"heading": "4 Experiment", "text": "We evaluate our method mainly on the basis of the widely used translation task WMT '14. In order to verify our model of more difficult language pairs, we also give the results of the translation task WMT' 14 from English to German."}, {"heading": "4.1 Data sets", "text": "Detailed data sets are listed below: \u2022 English-French: Europarl v7, Common Crawl, UN, News Commentary, Gigaword \u2022 English-German: Europarl v7, Common Crawl, News CommentaryOverall, the Anglo-French corpus comprises 36 million sentence pairs and the Anglo-German corpus 4.5 million sentence pairs. To ensure fair comparison, the 2012 Newstest and News-Test-2013 are linked as development sets, and the 2014 News-Test is the test set. Our data set matches previous work on NMT (Luong et al., 2015; Jean et al., 2015). For the source language, we select the most common 200K words as input vocabulary. For the target language, we select the most common 80K words and & 0K words as source vocabulary."}, {"heading": "4.2 Model settings", "text": "We have two models described above called Deep-ED and Deep-Att. Both models have exactly the same configuration and layer size with the exception of the interface part P-I. We use 256-dimensional word embedding for source and target languages. All LSTM layers, including 2 \u00d7 ne layers in encoder and nd layers in the decoder, have 512 memory cells. The output layer size corresponds to the size of the target vocabulary. The dimension of ct is 5120 and 1280 for Deep-ED and Deep-Att respectively. For each LSTM layer, the activation functions for gates, inputs and outputs are tanh and tanh in relation to our network. Our network is narrow on word embedding and LSTM layers. Note that in previous work (Sutskever et al., 2014; Bahdanau et al., 2014) 1000-dimensional word embedding and larger LSTM models have not been tried out as compared to larger ones in 2015."}, {"heading": "4.3 Optimization", "text": "Note that each LSTM layer contains two parts, as described in Eq.3, a feed calculation and a recurring calculation. Since there are non-linear activations in the recurring calculation, a higher learning rate lr = 5 x 10 \u2212 4 is used, while a smaller learning rate lf = 4 x 10 \u2212 5 is used for the feed calculation. Word embedding and Softmax layer also use this learning rate lf. We refer to all parameters that are not used for recurring calculation as a non-recurring part of the model. Due to the large model size, we use a strong L2 regulation to restrict the parameter matrix v in the following way: v \u2212 l \u00b7 (g + r \u00b7 v) (12) Here r is the regulation strength, l is the corresponding learning rate, g is the gradations of two layers."}, {"heading": "4.4 Results", "text": "Most previous NMT work (Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015) uses the same evaluation rules. All reported BLEU results are calculated using the multi-bleu.perl1 script, which is also used in the aforementioned work."}, {"heading": "4.4.1 Single models", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4.4.2 Post processing", "text": "First, three Deep-Att models are developed for ensemble results, which are initialized with different parameters, and the training corpus for these models is mixed with different random seeds. We sum up the distribution of the predicted words and normalize the final distribution to generate the next word. In Tab. 8 it is shown that the model ensemble can further improve performance to 38.9. In (Luong et al., 2015) and (Jean et al., 2015) there are eight models for the best results, but we only use three models and make no further profit from further models. Second, we gain the unknown words in the generated sequences using the Positional Unknown (PosUnk) model introduced in (Luong et al., 2015). The full parallel corpus is used to obtain the word mappings (Liang et al., 2006)."}, {"heading": "4.5 Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.5.1 Length", "text": "In the Anglo-French task, we analyze the influence of source sequence length on our models, as shown in Fig. 3. Here, we show five curves of our Deep-Att model, our Deep-Att ensemble model, our Deep-ED model, the previous Enc-Dec model with four layers (Sutskever et al., 2014) and the SMT model (Durrani et al., 2014). We find that our Deep-Att model works better on almost all length scales than the previous two models (Enc-Dec and SMT). It also shows that the performance of our Deep-Att model does not deteriorate on very long sequences with a length of over 70 words, compared to another NMT model Enc-December. Our Deep-ED model also performs much better than the flat Enc-Dec model on almost all length scales, although it begins to decrease and fall behind Aeptt at long sequences."}, {"heading": "4.5.2 Unknown words", "text": "We select the subset with no unknown words on the target sentence from the original test sentence. There are 1705 sequences (56.8%). We calculate the BLEU values on this subset and the results will be in Tab. 9. We also list the results from the SMT model (Durrani et al., 2014) for comparison. We find the BLEU value from Deep-Att to subset rises to 40.3. Looking at the value of the full test set of 37.7, we have a gap of 2.4. On this subset, the SMT model yields 37.5, and it should be noted that its value on full set is 37.0. This indicates that the difficulty on this subset is not much different from the complete sentence. Therefore, we put the larger gap for Deepatt back to the presence of unknown words. We also calculate the BLEU value on the subset with the ensemble model and get 41.4. In relation to the human level (the UM Scale 2014 was rated best) the BLEU 1000."}, {"heading": "4.5.3 Over-fitting", "text": "Deep models have more parameters and are therefore better able to adjust the large dataset. In addition, we will show that deep models are less likely to exceed the dataset. In Fig. 4, we show three results of models with different depth in the Anglo-French task. These three models are evaluated using a token error rate defined as the ratio of incorrectly predicted words throughout the target sequence with correct historical input. The curve with square markers corresponds to Deep-Att with ne = 9 and nd = 7. The curve with circle markers corresponds to ne = 5 and nd = 3. The curve with triangular markers corresponds to ne = 1 and nd = 1. We find that deep models perform better on the test set if the token error rate is the same as that of flat models on the traction set. It turns out that with a reduced token error rate, a deeper model is more advantageous in avoiding the overfitting phenomenon."}, {"heading": "4.5.4 Examples", "text": "For the task between English and French, we choose two sample sequences shown in Tab. 10. For each example, there are four lines corresponding to the source language, the reference (ref), our ensemble model with PosUnk or the SMT model (Durrani et al., 2014)."}, {"heading": "5 Conclusion", "text": "With the introduction of fast-forward connections to the deep LSTM network, we are building a fast path without non-linear transformations or recurring calculations to propagate the gradients from top to bottom. On this path, gradients decompose much more slowly compared to the standard LSTM network, allowing us to build the deep topology of NMT models. We trained NMT models with a depth of 16, including 25 LSTM layers, and rated them mainly on the WMT '14 translation techniques from English to French. This is the deepest topology studied in the NMT field for this task. We showed that our Deep-Att models have 6.2 BLEU points improvement over the previous best single model and achieve 37.7 BLEU score. This single end-to-end NMT model surpasses the conventional best SMT models (Durrani et et al., 2014) and achieves the most powerful models."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "IEEE Transactions on Neural Networks, 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Yoshua Bengio."], "venue": "arXiv:1206.5533.", "citeRegEx": "Bengio.,? 2012", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "N-gram counts and language models from the common crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas van Ooyen."], "venue": "Proceedings of the Language Resources and Evaluation Conference.", "citeRegEx": "Buck et al\\.,? 2014", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the Empiricial Methods in Nat-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Edinburghs phrase-based machine translation systems for WMT-14", "author": ["Nadir Durrani", "Barry Haddow", "Philipp Koehn", "Kenneth Heafield."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97\u2013104, June.", "citeRegEx": "Durrani et al\\.,? 2014", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fernandez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Graves et al\\.,? 2009", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv:1512.03385.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "arXiv:1506.03340.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700\u20131709, October.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves."], "venue": "arXiv:1507.01526.", "citeRegEx": "Kalchbrenner et al\\.,? 2015", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba."], "venue": "arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, Volume 1, NAACL \u201903, pages", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner."], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324, Nov.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Alignment by agreement", "author": ["Percy Liang", "Ben Taskar", "Dan Klein."], "venue": "Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL \u201906,", "citeRegEx": "Liang et al\\.,? 2006", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan L. Yuille."], "venue": "arXiv:1412.6632.", "citeRegEx": "Mao et al\\.,? 2014", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "http://www-lium.univlemans.fr/\u223cschwenk/cslm joint paper [online; accessed 03-september-2014]. University Le Mans", "author": ["Holger Schwenk"], "venue": null, "citeRegEx": "Schwenk.,? \\Q2014\\E", "shortCiteRegEx": "Schwenk.", "year": 2014}, {"title": "Highway networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "arXiv:1505.00387.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc Le."], "venue": "Advances in Neural Information Processing Systems 27 (NIPS 2014), pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich."], "venue": "Proceedings of the 2015 IEEE Conference on Computer Vision", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv:1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Depth-gated LSTM", "author": ["Kaisheng Yao", "Trevor Cohn", "Katerina Vylomova", "Kevin Duh", "Chris Dyer."], "venue": "arXiv:1508.03790.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Empirical study on deep learning models for QA", "author": ["Yang Yu", "Wei Zhang", "Chung-Wei Hang", "Bing Xiang", "Bowen Zhou."], "venue": "arXiv:1510.07526.", "citeRegEx": "Yu et al\\.,? 2015", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "End-to-end learning of semantic role labeling using recurrent neural networks", "author": ["Jie Zhou", "Wei Xu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language", "citeRegEx": "Zhou and Xu.,? 2015", "shortCiteRegEx": "Zhou and Xu.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Neural machine translation (NMT) has attracted a lot of interests in solving the machine translation (MT) problems in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 131, "endOffset": 210}, {"referenceID": 22, "context": "Neural machine translation (NMT) has attracted a lot of interests in solving the machine translation (MT) problems in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 131, "endOffset": 210}, {"referenceID": 0, "context": "Neural machine translation (NMT) has attracted a lot of interests in solving the machine translation (MT) problems in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 131, "endOffset": 210}, {"referenceID": 15, "context": "Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end form.", "startOffset": 66, "endOffset": 108}, {"referenceID": 5, "context": "Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end form.", "startOffset": 66, "endOffset": 108}, {"referenceID": 24, "context": "Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al.", "startOffset": 86, "endOffset": 108}, {"referenceID": 26, "context": "Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al., 2015) and image caption generation (Mao et al.", "startOffset": 137, "endOffset": 154}, {"referenceID": 19, "context": ", 2015) and image caption generation (Mao et al., 2014).", "startOffset": 37, "endOffset": 55}, {"referenceID": 22, "context": "Generally, there are two types of NMT topologies named encoder-decoder network (Sutskever et al., 2014) and attention network (Bahdanau et al.", "startOffset": 79, "endOffset": 103}, {"referenceID": 0, "context": ", 2014) and attention network (Bahdanau et al., 2014).", "startOffset": 30, "endOffset": 53}, {"referenceID": 18, "context": "Recent results show that the systems based on these models can achieve similar performance with conventional SMT systems (Luong et al., 2015; Jean et al., 2015).", "startOffset": 121, "endOffset": 160}, {"referenceID": 11, "context": "Recent results show that the systems based on these models can achieve similar performance with conventional SMT systems (Luong et al., 2015; Jean et al., 2015).", "startOffset": 121, "endOffset": 160}, {"referenceID": 5, "context": "However, single neural models of either of the above types have not been competitive with the best conventional system (Durrani et al., 2014) when evaluated on WMT\u201914 English-to-French task.", "startOffset": 119, "endOffset": 141}, {"referenceID": 18, "context": "5 (Luong et al., 2015) while the conventional method of (Durrani et al.", "startOffset": 2, "endOffset": 22}, {"referenceID": 5, "context": ", 2015) while the conventional method of (Durrani et al., 2014) gives 37.", "startOffset": 41, "endOffset": 63}, {"referenceID": 23, "context": "In the past two years the top positions of the ImageNet contest are always occupied by the systems with more than tens or even hundreds of layers (Szegedy et al., 2015; He et al., 2015).", "startOffset": 146, "endOffset": 185}, {"referenceID": 7, "context": "In the past two years the top positions of the ImageNet contest are always occupied by the systems with more than tens or even hundreds of layers (Szegedy et al., 2015; He et al., 2015).", "startOffset": 146, "endOffset": 185}, {"referenceID": 18, "context": "But in NMT, the largest depth used successfully is only six (Luong et al., 2015).", "startOffset": 60, "endOffset": 80}, {"referenceID": 10, "context": "We attribute this problem to the properties of Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) which is widely used in NMT.", "startOffset": 77, "endOffset": 111}, {"referenceID": 13, "context": "There are also many efforts to increase the depth of LSTM such as (Kalchbrenner et al., 2015), where the shortcuts do not avoid the nonlinear and recurrent computation.", "startOffset": 66, "endOffset": 93}, {"referenceID": 18, "context": "7 and outperforms the shallow model which has six layers (Luong et al., 2015) by 6.", "startOffset": 57, "endOffset": 77}, {"referenceID": 5, "context": "This is also the first time on this task that a single NMT model achieves state-ofthe-art performance and outperforms the best conventional SMT system (Durrani et al., 2014) with the improvement of 0.", "startOffset": 151, "endOffset": 173}, {"referenceID": 22, "context": "As a reference, previous work showed that oracle re-scoring the SMT generated 1000-best sequences exhibits the BLEU score of about 45 (Sutskever et al., 2014).", "startOffset": 134, "endOffset": 158}, {"referenceID": 22, "context": "In the encoder-decoder model (Sutskever et al., 2014), a single vector extracted from e is used as the representation.", "startOffset": 29, "endOffset": 53}, {"referenceID": 0, "context": "In the attention model (Bahdanau et al., 2014), c is dynamically obtained according to the relationship between target sequence and source sequence.", "startOffset": 23, "endOffset": 46}, {"referenceID": 18, "context": "In encoding-decoding network, people used at most six LSTM layers (Luong et al., 2015).", "startOffset": 66, "endOffset": 86}, {"referenceID": 21, "context": "with more than tens of convolution layers outperform the shallow ones on a series of image tasks in recent years (Srivastava et al., 2015; He et al., 2015; Szegedy et al., 2015).", "startOffset": 113, "endOffset": 177}, {"referenceID": 7, "context": "with more than tens of convolution layers outperform the shallow ones on a series of image tasks in recent years (Srivastava et al., 2015; He et al., 2015; Szegedy et al., 2015).", "startOffset": 113, "endOffset": 177}, {"referenceID": 23, "context": "with more than tens of convolution layers outperform the shallow ones on a series of image tasks in recent years (Srivastava et al., 2015; He et al., 2015; Szegedy et al., 2015).", "startOffset": 113, "endOffset": 177}, {"referenceID": 8, "context": "The investigations were also made on question-answering to encode the questions, where at most two LSTM layers are stacked (Hermann et al., 2015).", "startOffset": 123, "endOffset": 145}, {"referenceID": 7, "context": ", 2015; He et al., 2015; Szegedy et al., 2015). Different kinds of shortcut connections are proposed to decrease the length of the gradient propagation path. Training networks based on LSTM layers which is widely used in language problems is a much more challenging task. Because of the existence of large amount of nonlinear activations and the recurrent computation, gradient values are not stable and generally are smaller. Following the same spirit for convolutional network, a lot of efforts have also been spent into training deep LSTM topologies. Yao et al. (2015) introduced depth-gated shortcuts connecting LSTM cells at adjacent layers to provide a fast way to propagate the gradients.", "startOffset": 8, "endOffset": 572}, {"referenceID": 7, "context": ", 2015; He et al., 2015; Szegedy et al., 2015). Different kinds of shortcut connections are proposed to decrease the length of the gradient propagation path. Training networks based on LSTM layers which is widely used in language problems is a much more challenging task. Because of the existence of large amount of nonlinear activations and the recurrent computation, gradient values are not stable and generally are smaller. Following the same spirit for convolutional network, a lot of efforts have also been spent into training deep LSTM topologies. Yao et al. (2015) introduced depth-gated shortcuts connecting LSTM cells at adjacent layers to provide a fast way to propagate the gradients. They verified the modification of these shortcuts on MT task and language modeling task, but the best score are from models with three layers. Similarly, Kalchbrenner et al. (2015) extended this topology to be multi-dimensional.", "startOffset": 8, "endOffset": 877}, {"referenceID": 10, "context": "LSTM layer: Actually, a specific type recurrent layer called LSTM (Hochreiter and Schmidhuber, 1997; Graves et al., 2009) is used in our work.", "startOffset": 66, "endOffset": 121}, {"referenceID": 6, "context": "LSTM layer: Actually, a specific type recurrent layer called LSTM (Hochreiter and Schmidhuber, 1997; Graves et al., 2009) is used in our work.", "startOffset": 66, "endOffset": 121}, {"referenceID": 9, "context": "Half(f) means the first half elements of h, and Dr(h) is the dropout operation (Hinton et al., 2012) which randomly set an element of h to zero with a certain probability.", "startOffset": 79, "endOffset": 100}, {"referenceID": 7, "context": "was also used in (He et al., 2015; Zhou and Xu, 2015).", "startOffset": 17, "endOffset": 53}, {"referenceID": 28, "context": "was also used in (He et al., 2015; Zhou and Xu, 2015).", "startOffset": 17, "endOffset": 53}, {"referenceID": 0, "context": "We only concatenate the 4 output vectors at each time steps to obtain et, and a soft attention mechanism (Bahdanau et al., 2014) is used to calculate the final representation ct from et.", "startOffset": 105, "endOffset": 128}, {"referenceID": 0, "context": "a(\u00b7) is an alignment model described in (Bahdanau et al., 2014).", "startOffset": 40, "endOffset": 63}, {"referenceID": 22, "context": "Because of the deep bi-directional structure, we do not need to reverse the sequence order as (Sutskever et al., 2014).", "startOffset": 94, "endOffset": 118}, {"referenceID": 16, "context": "The deep topology brings difficulties into the model training, especially when the first order methods such as stochastic gradient descent (SGD) (LeCun et al., 1998) is used.", "startOffset": 145, "endOffset": 165}, {"referenceID": 27, "context": "We tried several high order optimization techniques such as AdaDelta (Zeiler, 2012), RMSProp (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014).", "startOffset": 69, "endOffset": 83}, {"referenceID": 14, "context": "We tried several high order optimization techniques such as AdaDelta (Zeiler, 2012), RMSProp (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014).", "startOffset": 130, "endOffset": 151}, {"referenceID": 9, "context": "Dropout (Hinton et al., 2012) is also used to avoid over-fitting.", "startOffset": 8, "endOffset": 29}, {"referenceID": 11, "context": "The hypothesis are ranked by the total likelihood of the generated sequence, despite in some works normalized likelihood is considered (Jean et al., 2015).", "startOffset": 135, "endOffset": 154}, {"referenceID": 18, "context": "Our data set is consistent with the previous works on NMT (Luong et al., 2015; Jean et al., 2015) to ensure fair comparison.", "startOffset": 58, "endOffset": 97}, {"referenceID": 11, "context": "Our data set is consistent with the previous works on NMT (Luong et al., 2015; Jean et al., 2015) to ensure fair comparison.", "startOffset": 58, "endOffset": 97}, {"referenceID": 11, "context": "The full vocabulary of German corpus is larger (Jean et al., 2015), so we select more German words to build the target vocabulary.", "startOffset": 47, "endOffset": 66}, {"referenceID": 20, "context": "and 30K output words on the sub train set with selected 12M parallel sequences (Schwenk, 2014; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 79, "endOffset": 136}, {"referenceID": 22, "context": "and 30K output words on the sub train set with selected 12M parallel sequences (Schwenk, 2014; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 79, "endOffset": 136}, {"referenceID": 4, "context": "and 30K output words on the sub train set with selected 12M parallel sequences (Schwenk, 2014; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 79, "endOffset": 136}, {"referenceID": 22, "context": "Note that in previous work (Sutskever et al., 2014; Bahdanau et al., 2014), 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used.", "startOffset": 27, "endOffset": 74}, {"referenceID": 0, "context": "Note that in previous work (Sutskever et al., 2014; Bahdanau et al., 2014), 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used.", "startOffset": 27, "endOffset": 74}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014), 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used. We also tried larger scale models but did not obtain further improvements. Consider comparing the computation complexity with Luong et. al. (2015), ours might be much lower than theirs.", "startOffset": 8, "endOffset": 255}, {"referenceID": 2, "context": "A detailed guide for setting hyper-parameters can be found in (Bengio, 2012).", "startOffset": 62, "endOffset": 76}, {"referenceID": 22, "context": "We keep the same evaluation rule with most previous NMT works (Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015).", "startOffset": 62, "endOffset": 125}, {"referenceID": 18, "context": "We keep the same evaluation rule with most previous NMT works (Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015).", "startOffset": 62, "endOffset": 125}, {"referenceID": 11, "context": "We keep the same evaluation rule with most previous NMT works (Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015).", "startOffset": 62, "endOffset": 125}, {"referenceID": 22, "context": "5 (Sutskever et al., 2014).", "startOffset": 2, "endOffset": 26}, {"referenceID": 18, "context": "6 (Luong et al., 2015).", "startOffset": 2, "endOffset": 22}, {"referenceID": 5, "context": "We also list the previous state-of-the-art performance from conventional SMT system (Durrani et al., 2014) with BLEU", "startOffset": 84, "endOffset": 106}, {"referenceID": 0, "context": "Two attention models RNNsearch (Bahdanau et al., 2014) and RNNsearch-LV (Jean et al.", "startOffset": 31, "endOffset": 54}, {"referenceID": 11, "context": ", 2014) and RNNsearch-LV (Jean et al., 2015) give the BLEU score of 28.", "startOffset": 25, "endOffset": 44}, {"referenceID": 20, "context": "The SMT result from (Schwenk, 2014) is also listed and falls behind our model by 2.", "startOffset": 20, "endOffset": 35}, {"referenceID": 11, "context": "1, where the beam size is 12 (Jean et al., 2015; Sutskever et al., 2014).", "startOffset": 29, "endOffset": 72}, {"referenceID": 22, "context": "1, where the beam size is 12 (Jean et al., 2015; Sutskever et al., 2014).", "startOffset": 29, "endOffset": 72}, {"referenceID": 0, "context": "Note that the model without F-F is exactly the standard attention model (Bahdanau et al., 2014).", "startOffset": 72, "endOffset": 95}, {"referenceID": 1, "context": "When the model is trained with larger depth without F-F connections, we find that the parameter exploding problem (Bengio et al., 1994) happens too frequently that we could not finish training.", "startOffset": 114, "endOffset": 135}, {"referenceID": 3, "context": "7 (Buck et al., 2014).", "startOffset": 2, "endOffset": 21}, {"referenceID": 18, "context": "In (Luong et al., 2015) and (Jean et al.", "startOffset": 3, "endOffset": 23}, {"referenceID": 11, "context": ", 2015) and (Jean et al., 2015) there are eight models for the best scores, but we only use three models and we do not obtain further gain from", "startOffset": 12, "endOffset": 31}, {"referenceID": 18, "context": "Second, we recover the unknown words in the generated sequences with Positional Unknown (PosUnk) model introduced in (Luong et al., 2015).", "startOffset": 117, "endOffset": 137}, {"referenceID": 17, "context": "The full parallel corpus is used to obtain the word mappings (Liang et al., 2006).", "startOffset": 61, "endOffset": 81}, {"referenceID": 18, "context": "5 BLEU points, consistent with the conclusion in (Luong et al., 2015).", "startOffset": 49, "endOffset": 69}, {"referenceID": 5, "context": "At the last two lines, we list the conventional SMT model (Durrani et al., 2014) and previous best neural models based system Encoding-Decoding (Luong et al.", "startOffset": 58, "endOffset": 80}, {"referenceID": 18, "context": ", 2014) and previous best neural models based system Encoding-Decoding (Luong et al., 2015) for comparison.", "startOffset": 71, "endOffset": 91}, {"referenceID": 22, "context": "Here we show five curves of our Deep-Att single model, our Deep-Att ensemble model, our Deep-ED model, previous Enc-Dec model with four layers (Sutskever et al., 2014) and SMT model (Durrani et al.", "startOffset": 143, "endOffset": 167}, {"referenceID": 5, "context": ", 2014) and SMT model (Durrani et al., 2014).", "startOffset": 22, "endOffset": 44}, {"referenceID": 5, "context": "We also list the results from SMT model (Durrani et al., 2014) as comparison .", "startOffset": 40, "endOffset": 62}, {"referenceID": 22, "context": "As a reference related to human level, in (Sutskever et al., 2014), it has been tested that the BLEU score of oracle re-scoring the LIUM 1000-best results (Schwenk, 2014) is 45.", "startOffset": 42, "endOffset": 66}, {"referenceID": 20, "context": ", 2014), it has been tested that the BLEU score of oracle re-scoring the LIUM 1000-best results (Schwenk, 2014) is 45.", "startOffset": 96, "endOffset": 111}, {"referenceID": 5, "context": "For each example, there are four lines corresponding to source language, reference(ref), our ensemble model result with PosUnk and SMT model (Durrani) (Durrani et al., 2014) respectively.", "startOffset": 151, "endOffset": 173}, {"referenceID": 5, "context": "This single end-to-end NMT model outperforms the conventional best SMT system (Durrani et al., 2014) and achieves the state-ofthe-art performance.", "startOffset": 78, "endOffset": 100}, {"referenceID": 22, "context": "As a reference number, the work (Sutskever et al., 2014) show that oracle re-scoring the SMT generated 1000-best lists exhibits the BLEU score of about 45.", "startOffset": 32, "endOffset": 56}, {"referenceID": 11, "context": "The best results from both single model and model ensemble are obtained with beam size of 3, much smaller than previous NMT systems where beam size is about 12 (Jean et al., 2015; Sutskever et al., 2014).", "startOffset": 160, "endOffset": 203}, {"referenceID": 22, "context": "The best results from both single model and model ensemble are obtained with beam size of 3, much smaller than previous NMT systems where beam size is about 12 (Jean et al., 2015; Sutskever et al., 2014).", "startOffset": 160, "endOffset": 203}], "year": 2016, "abstractText": "Neural machine translation (NMT) aims at solving machine translation (MT) problems with purely neural networks and exhibits promising results in recent years. However, most of the existing NMT models are of shallow topology and there is still a performance gap between the single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) network, together with the interleaved bidirectional way for stacking them. Fastforward connections play an essential role to propagate the gradients in building the deep topology of depth 16. On WMT\u201914 Englishto-French task, we achieved BLEU=37.7 with single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. It is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. Even without considering attention mechanism, we can still achieve BLEU=36.3. After the special handling for unknown words and the model ensembling, we obtained the best score on this task with BLEU=40.4. Our models are also verified on the more difficult WMT\u201914 English-to-German task.", "creator": "LaTeX with hyperref package"}}}