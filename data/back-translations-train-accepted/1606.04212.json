{"id": "1606.04212", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Active Discriminative Text Representation Learning", "abstract": "We propose a new active learning (AL) method for text classification based on convolutional neural networks (CNNs). In AL, one selects the instances to be manually labeled with the aim of maximizing model performance with minimal effort. Neural models capitalize on word embeddings as features, tuning these to the task at hand. We argue that AL strategies for neural text classification should focus on selecting instances that most affect the embedding space (i.e., induce discriminative word representations). This is in contrast to traditional AL approaches (e.g.,uncertainty sampling), which specify higher level objectives. We propose a simple approach that selects instances containing words whose embeddings are likely to be updated with the greatest magnitude, thereby rapidly learning discriminative, task-specific embeddings. Empirical results show that our method outperforms baseline AL approaches.", "histories": [["v1", "Tue, 14 Jun 2016 06:49:40 GMT  (75kb,D)", "http://arxiv.org/abs/1606.04212v1", null], ["v2", "Mon, 21 Nov 2016 19:31:07 GMT  (140kb,D)", "http://arxiv.org/abs/1606.04212v2", "This paper got accepted by AAAI 2017"], ["v3", "Sun, 27 Nov 2016 05:58:25 GMT  (124kb,D)", "http://arxiv.org/abs/1606.04212v3", "This paper got accepted by AAAI 2017"], ["v4", "Thu, 1 Dec 2016 18:53:32 GMT  (125kb,D)", "http://arxiv.org/abs/1606.04212v4", "This paper got accepted by AAAI 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ye zhang", "matthew lease", "byron c wallace"], "accepted": true, "id": "1606.04212"}, "pdf": {"name": "1606.04212.pdf", "metadata": {"source": "CRF", "title": "Active Discriminative Word Embedding Learning", "authors": ["Ye Zhang", "Byron Wallace"], "emails": ["yezhang@utexas.edu", "byron.wallace@utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "The idea is that through smart selection of training data, better models can be learned at a lower cost (Settles, 2010), and for text classification in particular (Tong and Koller, 2002; McCallumzyand Nigamy, 1998; Wallace et al., 2010). There has been a wealth of work on AL approaches to traditional methods of machine learning in general (Settles, 2010), and for text classification in particular (Tong and Koller, 2002; McCallumzyand Nigamy, 1998; Wallace et al., 2010)."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 CNNs for Text Classification", "text": "Here we offer a brief overview of CNNs for text classification, using the model proposed by Kim (2014). We designate the word embedding matrix by E, RV, and d, where V is the word size and d is the dimension of the embedding layer. A specific instance (piece of text to be categorized) is then represented by stacking the vectors corresponding to the words it contains (stored in E), preserving the word order, resulting in an instance matrix A, RS, and d, where S is the text length. We then apply folding operations to this matrix, using multiple linear filters. Each filter matrix, Wi, Rh, and d, performs a folding operation on A, using a characteristic map, RS \u2212 h + 1. We then apply 1-max pooing to each ci to obtain a characteristic value oi for this filter. (We note that we use multiple filters, height and red elevations)"}, {"heading": "2.2 Active Learning", "text": "We look at the pool-based AL scenario, which assumes that there is a small set of labeled data L and a large pool of available unlabeled data U. The task for the learner is to selectively draw examples from that pool to be labeled. These selections or queries are typically made in a greedy manner; an informativeness measurement is used to evaluate all instances in the pool, and the instance maximizes that measurement. The key to developing AL strategies is to develop a good informativeness measurement. Let x be the most informative instance according to a query strategy (xi; \u03b8), which is used to evaluate each instance xi in the unlabeled pool U, conditioned on the current set of parameter estimates. Thenx \u0445 = argmaxxi, that this uncertainty (xi; TB) (1) In the case of CNNs, includes the embedding of parameters E, constellation C, and constellation of lightness parameters."}, {"heading": "3 AL with Embeddings", "text": "This is based on the EGL method described above. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p > p\" p > p > p > p > p \"p > p > p > p\" p > p > p \"p > p\" p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"."}, {"heading": "4 Experimental Setup and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "CR customer reviews of products classified as \"positive\" or \"negative\" (Hu and Liu, 2004).1MR film reviews classified as \"positive\" or \"negative\" (Pang and Lee, 2005).Subj Sentences classified as \"subjective\" or \"objective\" (Pang and Lee, 2004).21http: / / www.cs.uic.edu / liub / FBS / sentiment-analysis.html 2Both the MR and Subj data sets are available at:"}, {"heading": "4.2 Model Configuration", "text": "For CNNs, we used pre-trained word2vec-induced vectors 3 to initialize E. We used three filter heights (3, 4, 5) and 50 filters each. We did not tune these hyperparameters. We performed 20 rounds of batch-active learning. All learners started with the same 25 instances. In the following rounds, each learner selected 25 instances from U according to their respective query strategies. These examples were added to L, and the models were retrained. Performance was evaluated by calculating accuracy on a pre-set test set after each round. We repeated the entire AL process ten times over a 10-fold CV and replicated the experiment five times per fold to account for the variation. Report results are averages of these averages. We used Adadelta (Zeiler, 2012) for parameter estimation and we tapped E during reverse propagation, generating discriminatory embedding."}, {"heading": "4.3 Results and Discussion", "text": "We show learning curves in Figure 2. Our proposed EGL Word Active Learning Method outperforms all baselines and performs particularly well in sentiment analysis tasks (MR and CR). Wehttp: / / www.cs.cornell.edu / people / pabo / movie-review-data /.3https: / / code.google.com / archive / p / word2vec / believes this is due to the fact that our model quickly learns more differentiated representations of words with opposing polarities. To further clarify this point, we also provide diagrams that show the euclidean distance between selected pairs of words induced by different AL strategies (Figure 2). In the case of customer rating data, for example, we see that the EGL word quickly separates the embeddings that correspond to \"good\" and \"bad\" from each other. \""}, {"heading": "5 Conclusions", "text": "We have proposed a new active learning strategy for CNNs specifically designed to quickly introduce discriminatory word embeddings and thus improve text classification. We have shown that this approach outperforms basic AL strategies on three text classification datasets, and that it quickly finds discriminatory word embeddings."}], "references": [{"title": "Mining and summarizing customer reviews", "author": ["Hu", "Liu2004] Minqing Hu", "Bing Liu"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Effective use of word order for text categorization with convolutional neural networks. arXiv preprint arXiv:1412.1058", "author": ["Johnson", "Zhang2014] Rie Johnson", "Tong Zhang"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "A sequential algorithm for training text classifiers", "author": ["Lewis", "Gale1994] David D Lewis", "William A Gale"], "venue": "In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Lewis et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 1994}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Lee2004] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Lee2005] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Learning representations by back-propagating errors", "author": ["Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "An analysis of active learning strategies for sequence labeling tasks", "author": ["Settles", "Craven2008] Burr Settles", "Mark Craven"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Settles et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Settles et al\\.", "year": 2008}, {"title": "Active learning literature survey", "author": ["Burr Settles"], "venue": "University of Wisconsin,", "citeRegEx": "Settles.,? \\Q2010\\E", "shortCiteRegEx": "Settles.", "year": 2010}, {"title": "A mathematical theory of communication", "author": ["Claude Elwood Shannon"], "venue": "ACM SIGMOBILE Mobile Computing and Communications", "citeRegEx": "Shannon.,? \\Q2001\\E", "shortCiteRegEx": "Shannon.", "year": 2001}, {"title": "Support vector machine active learning with applications to text classification", "author": ["Tong", "Koller2002] Simon Tong", "Daphne Koller"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Tong et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tong et al\\.", "year": 2002}, {"title": "Active learning for biomedical citation screening", "author": ["Kevin Small", "Carla E Brodley", "Thomas A Trikalinos"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Wallace et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wallace et al\\.", "year": 2010}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Zhang", "Wallace2015] Ye Zhang", "Byron Wallace"], "venue": "arXiv preprint arXiv:1510.03820", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Mgnc-cnn: A simple approach to exploiting multiple word embeddings for sentence classification", "author": ["Zhang et al.2016] Ye Zhang", "Stephen Roller", "Byron Wallace"], "venue": "arXiv preprint arXiv:1603.00968", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Active learning with sampling by uncertainty and density for word sense disambiguation and text classification", "author": ["Zhu et al.2008] Jingbo Zhu", "Huizhen Wang", "Tianshun Yao", "Benjamin K Tsou"], "venue": "In Proceedings of the 22nd International Conference", "citeRegEx": "Zhu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 8, "context": "In active learning (AL), the machine learning algorithm being trained is allowed to select the examples to be manually annotated (and in turn learned from) (Settles, 2010).", "startOffset": 156, "endOffset": 171}, {"referenceID": 8, "context": "There has been a wealth of work on AL approaches for traditional machine learning methods generally (Settles, 2010), and for text classification in particular (Tong and Koller, 2002; McCallumzy and Nigamy, 1998; Wallace et al.", "startOffset": 100, "endOffset": 115}, {"referenceID": 11, "context": "There has been a wealth of work on AL approaches for traditional machine learning methods generally (Settles, 2010), and for text classification in particular (Tong and Koller, 2002; McCallumzy and Nigamy, 1998; Wallace et al., 2010).", "startOffset": 159, "endOffset": 233}, {"referenceID": 2, "context": "Here we propose a method for AL using convolutional neural networks (CNNs), which have recently achieved strong performance across a diverse set of text classification tasks (Kim, 2014; Zhang and Wallace, 2015; Johnson and Zhang, 2014; Zhang et al., 2016).", "startOffset": 174, "endOffset": 255}, {"referenceID": 14, "context": "Here we propose a method for AL using convolutional neural networks (CNNs), which have recently achieved strong performance across a diverse set of text classification tasks (Kim, 2014; Zhang and Wallace, 2015; Johnson and Zhang, 2014; Zhang et al., 2016).", "startOffset": 174, "endOffset": 255}, {"referenceID": 2, "context": "Here we provide a brief review of CNNs for text classification, using the model proposed by Kim (2014). We will denote the word embedding matrix by E \u2208 RV\u00d7d, where V is the vocabulary size, and d is the dimension of the embedding layer.", "startOffset": 92, "endOffset": 103}, {"referenceID": 6, "context": "Typically this model is trained by minimizing the cross-entropy loss via back-propagation (Rumelhart et al., 1988).", "startOffset": 90, "endOffset": 114}, {"referenceID": 2, "context": "For more details, we refer the reader to (Kim, 2014; Zhang and Wallace, 2015).", "startOffset": 41, "endOffset": 77}, {"referenceID": 8, "context": "Many querying strategies have been proposed in the literature (Settles, 2010).", "startOffset": 62, "endOffset": 77}, {"referenceID": 15, "context": "One of the simplest and perhaps the most commonly used query framework is uncertainty sampling (Lewis and Gale, 1994; Tong and Koller, 2002; Zhu et al., 2008).", "startOffset": 95, "endOffset": 158}, {"referenceID": 9, "context": "A general uncertainty sampling variant uses entropy (Shannon, 2001) as an uncertainty measure, defining \u03c6(xi;\u03b8) as:", "startOffset": 52, "endOffset": 67}, {"referenceID": 8, "context": "Entropy-based uncertainty sampling has been observed to achieve strong empirical performance across many tasks (Settles, 2010).", "startOffset": 111, "endOffset": 126}, {"referenceID": 12, "context": "We used Adadelta (Zeiler, 2012) for parameter estimation, and we tuned E during back-propagation, thus inducing discriminative embeddings.", "startOffset": 17, "endOffset": 31}], "year": 2017, "abstractText": "We propose a new active learning (AL) method for text classification based on convolutional neural networks (CNNs). In AL, one selects the instances to be manually labeled with the aim of maximizing model performance with minimal effort. Neural models capitalize on word embeddings as features, tuning these to the task at hand. We argue that AL strategies for neural text classification should focus on selecting instances that most affect the embedding space (i.e., induce discriminative word representations). This is in contrast to traditional AL approaches (e.g., uncertainty sampling), which specify higher level objectives. We propose a simple approach that selects instances containing words whose embeddings are likely to be updated with the greatest magnitude, thereby rapidly learning discriminative, task-specific embeddings. Empirical results show that our method outperforms baseline AL approaches.", "creator": "LaTeX with hyperref package"}}}