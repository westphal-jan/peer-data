{"id": "1606.04218", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Conditional Generative Moment-Matching Networks", "abstract": "Maximum mean discrepancy (MMD) has been successfully applied to learn deep generative models for characterizing a joint distribution of variables via kernel mean embedding. In this paper, we present conditional generative moment- matching networks (CGMMN), which learn a conditional distribution given some input variables based on a conditional maximum mean discrepancy (CMMD) criterion. The learning is performed by stochastic gradient descent with the gradient calculated by back-propagation. We evaluate CGMMN on a wide range of tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge, which distills knowledge from a Bayesian model by learning a relatively small CGMMN student network. Our results demonstrate competitive performance in all the tasks.", "histories": [["v1", "Tue, 14 Jun 2016 07:11:29 GMT  (1491kb,D)", "http://arxiv.org/abs/1606.04218v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yong ren", "jun zhu", "jialian li", "yucen luo"], "accepted": true, "id": "1606.04218"}, "pdf": {"name": "1606.04218.pdf", "metadata": {"source": "CRF", "title": "Conditional Generative Moment-Matching Networks", "authors": ["Yong Ren", "Jialian Li", "Yucen Luo", "Jun Zhu"], "emails": ["{renyong15@mails,", "luoyc15@mails,", "jl12@mails,", "dcszj@mail}.tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where the only question is to what extent it is a country, where it is a country, where it is a country, where it is a country."}, {"heading": "2 Preliminary", "text": "In this section, we briefly review some preliminary findings, including the maximum mean discrepancy (FMD) and the core embedding of conditional distributions."}, {"heading": "2.1 Hilbert Space Embedding", "text": "An RKHS F on X with kernel k is a Hilbert space of functions f: X \u2192 R. Its internal product < \u00b7, \u00b7 > F fulfills the reproducing property: < f (\u00b7), k (x, \u00b7) > F = f (x). Core functions are not limited to Rd. They can also be defined on charts, time series and structured objects [11]. Normally, we consider \u03c6 (x): = k (x, \u00b7) as a (normally infinite dimension) characteristic map of x. The most interesting part is that we can embed a distribution by taking the expectation into its characteristic map: \u00b5X: = EX [\u03c6 (X)] = (X) dP (X)."}, {"heading": "2.2 Maximum Mean Discrepancy", "text": "Leave X = {xi} Ni = 1 and Y = {yi} Mj = 1 the sets of samples from the distributions PX and PY, respectively. Maximum Mean Discrepancy (MMD), also known as the Kernel 2 sample test [7], is a frequentistic estimator to answer the question whether PX = PY based on the samples observed. The basic idea behind MMD is that if the generating distributions are identical, all statistics are the same. [7] Formally, MMD defines the following difference magnitude: MMD [K, PX, PY]: = sup f-K (EX) \u2212 EY [f (Y) \u2212 Y [f (Y)]], where K is a class of functions. [7] found that the class of functions in a universal RKHS F is rich enough to express any two distributions and MD difference as their mean values."}, {"heading": "2.3 Kernel Embedding of Conditional Distributions", "text": "The kernel embedding of a conditional distribution P (Y | X) is defined as: \u00b5Y | x: = EY | x (Y) | x [\u03c6 (Y)] = \u0445 \u03c6 (y) dP (y | x). In contrast to embedding a single distribution, the embedding of a conditional distribution is not a single element in the RKHS, but removes a family of points in the RKHS, each of which is represented by a fixed value of x. Formally, the embedding of a conditional distribution is represented as an operator CY | X, which fulfils the following properties: 1. \u00b5Y | x = CY | X\u03c6 (x); 2. EY | x [g (Y) | x] = < g, \u00b5Y | x > G, (1) where G is the RKHS element corresponding to Y. [29] Such an operator exists under some assumptions, using the technique of the cross-covance operator CXX \u2212 CY."}, {"heading": "3 Conditional Generative Moment-Matching Networks", "text": "We now present CGMMN, including a conditional criterion of maximum mean discrepancy as a training target, a deep generative architecture, and a learning algorithm."}, {"heading": "3.1 Conditional Maximum Mean Discrepancy", "text": "Given the conditional distribution PY - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X X - X - X X - X - X - X - X - X X - X - X - X - X - X - X - X X - X - X - X - X - X - X - X X - X - X - X X - X - X - X - X - X - X - X X - X - X - X X X - X - X X - X X X X X - X X X X - X - X X X - X - X - X - X - X X - X X - X"}, {"heading": "3.2 CGMMN Nets", "text": "A desirable characteristic of the DGM is that we can simply take samples from it to estimate the CMMD target. In the following, we present such a network that takes both the given conditional variables and an additional series of random variables as input factors and then passes through a deep neural network with nonlinear transformations to produce the samples of the target variants. In particular, our network is built on the fact that for each distribution P to sample space K and each subsequent distribution Q to L, which is regular enough, there is a function G: L \u2192 K such that G (x).P, in the x series Q [12]. This fact has recently been explored to define a deep generative model and estimate the parameters of the MMMMD. For a conditional model, we would make the function G dependent on the given values of the input variables."}, {"heading": "4 Experiments", "text": "We now present a wide range of applications to evaluate our model, including predictive models, contextual generation, and an interesting case of Bayesian dark knowledge [15]. Our results show that CGMMN is competitive in all tasks."}, {"heading": "4.1 Predictive Performance", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Results on MNIST dataset", "text": "We present first the prediction performance on the widely used MINIST data set, which consists of pictures in 10 classes. Each picture is of the size 32 x 28 and the grayscale are normalized to be in the range [0, 1]. The entire data set is divided into 3 parts with 50 000 training examples, 10 000 validation examples and 10 000 test samples. For the prediction task, the conditional variables are the pictures x, 1, 2, 3, 4, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 10, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 11, 10, 10, 10, 10, 10, 10, 10, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,"}, {"heading": "4.2 Generative Performance", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Results on MNIST dataset", "text": "For the creation of tasks, the conditional variables are the captions. As the architecture has a finite number of values, as mentioned in Sec. 2.3, we estimate CY X and C \u2212 1XX directly and combine them as the estimate of CY | X (see Appendix A.2.2 for practical details).The architecture is the same as before, but they replace the positions of x and y. For the input layer, the label information is as conditional variables (represented by a single hot spot vector of dimension 10), we take a sample of dimension 20 that is sufficiently large. Overall, the network is a 5-layer MLP with the input size 30 and the middle layer hidden unit number (64, 256, 256, 512)."}, {"heading": "4.2.2 Results on Yale Face dataset", "text": "We show the generating results on the Extended Yale Facedataset [9], the 2, 414 grayscale images for 38 individuals of the dimension 32 x 32. There are about 64 images per subject, one per different facial expression or configuration. A smaller version of the dataset consists of 165 images of 15 individuals and the generating result can be found in Appendix A.4.2. We adopt the same architecture as the first generating experiment for MNIST, which is a 5-layer MLP with an input dimension of 50 (12 hidden variables and 38 dimensions for conditional variables, i.e., labels) and the middle layer hidden unit number (64, 256, 512). The other settings are the same as in the MNIST experiment. The general generating results are shown in Fig. 5, where we really generate different images for different individuals."}, {"heading": "5 Conclusions and Discussions", "text": "We present Conditional Generative Moment Matching Networks (CGMMN), which provide a flexible framework for displaying conditional distributions. CGMMN extends the capability of previous DTMs largely on the basis of maximum mean discrepancy (MMD) while simplifying the training process through backpropagation. Experimental results on various tasks, including predictive modelling, data generation and Bayesian obscurity, demonstrate competitive performance. Conditional modelling has been practiced as a natural step to improve the discriminatory capability of a statistical model and / or to loosen unnecessary assumptions of conditional variables. For deep learning models, Summary Product-based Networks (SPN) [24] provide accurate conclusions about DGMs and their conditional expansion [4] improves discriminatory capability; and recent work [21] represents a conditional version of generative adversarial networks (GAN) with expandability."}, {"heading": "Acknowledgments", "text": "The work was supported by the National Basic Research Programme (Nos. 2013CB329403, 2012CB316301), the National NSF of China (Nos. 61322308, 61332007), the Tsinghua TNList Lab Big Data Initiative and the Scientific Research Programme of the Tsinghua Initiative (Nos. 20121088071, 20141080934)."}, {"heading": "A Appendix", "text": "\"We have the possibility that we will see ourselves in a position to reduce the costs.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"\" \"\" \"\" We. \"\" \"\" \"\" \"\" We. \"\" \"\" \"\" We. \"\" \"\" \"\" We. \"\" \"\" \"We.\". \"\" \"\" We. \".\". \"\" We. \".\". \"\" We. \".\". \"\" We.. \"...\" \"\" We........ \"\" \"\" We...... \"\" \"\" We... \"\" \"\" \"We.\" \"\" \"\" \"\" We.. \"\" \"\" \"\" We..... \"\" \"\" \"\" \"\" We....... \"\" \"\" \"\" We....... \"\" \"\" \"\" \"\" \"We..........\" \"\" \"\" \"\" \"\" \"\" \"\" We.................. \"\" \"\" \"\" \"\" \"We...................\" \"\" \"\" \"\" \"\" \"\" \"We...............\" \"\" \"\" \"\" \"\" \"\" We......... \"\" \"\" \"\" \"\" \"\" \"\"............ \"\" \"\" \"\" \"\" \"\" \"\" \"We...................\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" We............ \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"............."}], "references": [{"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E. Denton", "S. Chintala", "A. Szlam", "R. Fergus"], "venue": "NIPS", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to generate chairs", "author": ["A. Dosovitskiy", "J. Springenberg", "M. Tatarchenko", "T. Brox"], "venue": "tables and cars with convolutional networks. arXiv:1411.5928", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Training generative neural networks via maximum mean discrepancy optimization", "author": ["G. Dziugaite", "D. Roy", "Z. Ghahramani"], "venue": "UAI", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Discriminative learning of sum-product networks", "author": ["R. Gens", "P. Domingos"], "venue": "NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Generative adverisarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout networks", "author": ["I. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A kernel two-sample test", "author": ["A. Gretton", "K. Borgwardt", "M. Rasch", "B. Scholkopf", "A. Smola"], "venue": "JMLR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Conditional mean embedding as regressors", "author": ["S. Grunewalder", "G. Lever", "L. Baldassarre", "S. Patterson", "A. Gretton", "M. Pontil"], "venue": "ICML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Face recognition using laplacianfaces", "author": ["X. He", "S. Yan", "Y. Hu", "P. Niyogi", "H. Zhang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence, 27(3):328\u2013340", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Probabilistic backpropagation for scalable learning of bayesian neural networks", "author": ["J. Hernandez-Lobato", "R. Adams"], "venue": "ICML", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Kernel methods in machine learning", "author": ["T. Hofmann", "B. Scholkopf", "A. Smola"], "venue": "The Annals of Statistics, 36(3):1171\u20131220", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Foundations of modern probability", "author": ["O. Kallenbery"], "venue": "New York: Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["D. Kingma", "M. Welling"], "venue": "ICLR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian dark knowledge", "author": ["A. Korattikara", "V. Rathod", "K. Murphy", "M. Welling"], "venue": "NIPS", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "ICML", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Deeply-supervised nets", "author": ["C. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "AISTATS", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Max-margin deep generative models", "author": ["C. Li", "J. Zhu", "T. Shi", "B. Zhang"], "venue": "NIPS", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative moment matching networks", "author": ["Y. Li", "K. Swersky", "R. Zemel"], "venue": "ICML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "ICLR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "ArXiv:1411.1784v1", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G. Hinton"], "venue": "ICML", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "On discriminative vs", "author": ["A. Ng", "M.I. Jordan"], "venue": "generative classifiers: a comparison of logistic regression and naive bayes. NIPS", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Sum-product networks: A new deep architecture", "author": ["H. Poon", "P. Domingos"], "venue": "UAI", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["P. Sermanet", "S. Chintala", "Y. Lecun"], "venue": "ICPR", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathmetical Programming, Series B", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "A hilbert space embedding for distributions", "author": ["A. Smola", "A. Gretton", "L. Song", "B. Scholkopf"], "venue": "International Conference on Algorithmic Learning Theory", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["K. Sohn", "X. Yan", "H. Lee"], "venue": "NIPS", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems", "author": ["L. Song", "J. Huang", "A. Smola", "K. Fukumizu"], "venue": "ICML", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "NIPS", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv:1411.4555v2", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["X. Yan", "J. Yang", "K. Sohn", "H. Lee"], "venue": "arXiv:1512.00570", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M. Zeiler", "R. Fergus"], "venue": "ICLR", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Among various deep learning methods, DGMs are natural choice for those tasks that require probabilistic reasoning and uncertainty estimation, such as image generation [1], multimodal learning [30], and missing data imputation.", "startOffset": 167, "endOffset": 170}, {"referenceID": 29, "context": "Among various deep learning methods, DGMs are natural choice for those tasks that require probabilistic reasoning and uncertainty estimation, such as image generation [1], multimodal learning [30], and missing data imputation.", "startOffset": 192, "endOffset": 196}, {"referenceID": 17, "context": ", deep convolutional networks), has also been significantly improved by employing the discriminative max-margin learning [18].", "startOffset": 121, "endOffset": 125}, {"referenceID": 4, "context": "For the arguably more challenging unsupervised learning, [5] presents a generative adversarial network (GAN), which adopts a game-theoretical min-max optimization formalism.", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "GAN has been extended with success in various tasks [1, 21].", "startOffset": 52, "endOffset": 59}, {"referenceID": 20, "context": "GAN has been extended with success in various tasks [1, 21].", "startOffset": 52, "endOffset": 59}, {"referenceID": 2, "context": "The recent work [3, 19] presents generative moment matching networks (GMMN), which has a simpler objective function than GAN while retaining the advantages of deep learning.", "startOffset": 16, "endOffset": 23}, {"referenceID": 18, "context": "The recent work [3, 19] presents generative moment matching networks (GMMN), which has a simpler objective function than GAN while retaining the advantages of deep learning.", "startOffset": 16, "endOffset": 23}, {"referenceID": 6, "context": "To learn the parameters, GMMN adopts maximum mean discrepancy (MMD) [7], a moment matching criterion where kernel mean embedding techniques are used to avoid unnecessary assumptions of the distributions.", "startOffset": 68, "endOffset": 71}, {"referenceID": 15, "context": "However, we are more interested in a conditional distribution in many cases, including (1) predictive modeling: compared to a generative model that defines the joint distribution p(x,y) of input data x and response variable y, a conditional model p(y|x) is often more direct without unnecessary assumptions on modeling x, and leads to better performance with fewer training examples [16, 23]; (2) contextual generation: in some cases, we are interested in generating samples based on some context, such as class labels [21], visual attributes [32] or the input information in cross-modal generation (e.", "startOffset": 383, "endOffset": 391}, {"referenceID": 22, "context": "However, we are more interested in a conditional distribution in many cases, including (1) predictive modeling: compared to a generative model that defines the joint distribution p(x,y) of input data x and response variable y, a conditional model p(y|x) is often more direct without unnecessary assumptions on modeling x, and leads to better performance with fewer training examples [16, 23]; (2) contextual generation: in some cases, we are interested in generating samples based on some context, such as class labels [21], visual attributes [32] or the input information in cross-modal generation (e.", "startOffset": 383, "endOffset": 391}, {"referenceID": 20, "context": "However, we are more interested in a conditional distribution in many cases, including (1) predictive modeling: compared to a generative model that defines the joint distribution p(x,y) of input data x and response variable y, a conditional model p(y|x) is often more direct without unnecessary assumptions on modeling x, and leads to better performance with fewer training examples [16, 23]; (2) contextual generation: in some cases, we are interested in generating samples based on some context, such as class labels [21], visual attributes [32] or the input information in cross-modal generation (e.", "startOffset": 519, "endOffset": 523}, {"referenceID": 31, "context": "However, we are more interested in a conditional distribution in many cases, including (1) predictive modeling: compared to a generative model that defines the joint distribution p(x,y) of input data x and response variable y, a conditional model p(y|x) is often more direct without unnecessary assumptions on modeling x, and leads to better performance with fewer training examples [16, 23]; (2) contextual generation: in some cases, we are interested in generating samples based on some context, such as class labels [21], visual attributes [32] or the input information in cross-modal generation (e.", "startOffset": 543, "endOffset": 547}, {"referenceID": 30, "context": ", from image to text [31] or vice versa [2]); and (3) building large networks: conditional distributions are essential building blocks of a large generative probabilistic model.", "startOffset": 21, "endOffset": 25}, {"referenceID": 1, "context": ", from image to text [31] or vice versa [2]); and (3) building large networks: conditional distributions are essential building blocks of a large generative probabilistic model.", "startOffset": 40, "endOffset": 43}, {"referenceID": 0, "context": "One recent relevant work [1] provides a good example of stacking multiple conditional GAN networks [21] in a Laplacian pyramid structure to generate natural images.", "startOffset": 25, "endOffset": 28}, {"referenceID": 20, "context": "One recent relevant work [1] provides a good example of stacking multiple conditional GAN networks [21] in a Laplacian pyramid structure to generate natural images.", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "We evaluate CGMMN in a wide range of tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge [15], an interesting case of distilling dark knowledge from Bayesian models.", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "They can also be defined on graphs, time series and structured objects [11].", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "This technique has been widely applied in many tasks, including feature extractor, density estimation and two-sample test [7, 27].", "startOffset": 122, "endOffset": 129}, {"referenceID": 26, "context": "This technique has been widely applied in many tasks, including feature extractor, density estimation and two-sample test [7, 27].", "startOffset": 122, "endOffset": 129}, {"referenceID": 6, "context": "Maximum Mean Discrepancy (MMD), also known as kernel two sample test [7], is a frequentist estimator to answer the query whether PX = PY based on the observed samples.", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "[7] found that the class of functions in an universal RKHS F is rich enough to distinguish any two distributions and MMD can be expressed as the difference of their mean embeddings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Theorem 1 [7] Let K be a unit ball in a universal RKHS F , defined on the compact metric space X , with an associated continuous kernel k(\u00b7, \u00b7).", "startOffset": 10, "endOffset": 13}, {"referenceID": 28, "context": "[29] found that such an operator exists under some assumptions, using the technique of crosscovariance operator CXY : G \u2192 F :", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Theorem 2 [29] Assuming that EY |X [g(Y )|X] \u2208 F , the embedding of conditional distributions CY |X defined as CY |X := CY XC XX satisfies properties 1 and 2.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "It is worth mentioning that we have assumed that the conditional mean embedding operator CY |X \u2208 F \u2297 G to have the CMMD objective well-defined, which needs some smoothness assumptions such that C XX CXY is Hilbert-Schmidt [8].", "startOffset": 222, "endOffset": 225}, {"referenceID": 28, "context": ", the Hilbert-Schmidt norm exists) for practical use [29].", "startOffset": 53, "endOffset": 57}, {"referenceID": 28, "context": "Similar observations have been shown in [29] for the conditional mean operator, where the estimated conditional embedding \u03bcY |x is a non-uniform weighted combination of \u03c6(xi).", "startOffset": 40, "endOffset": 44}, {"referenceID": 11, "context": "Specifically, our network is built on the fact that for any distribution P on sample space K and any continuous distribution Q on L that are regular enough, there is a function G : L \u2192 K such that G(x) \u223c P , where x \u223c Q [12].", "startOffset": 220, "endOffset": 224}, {"referenceID": 2, "context": "This fact has been recently explored by [3, 19] to define a deep generative model and estimate the parameters by the MMD criterion.", "startOffset": 40, "endOffset": 47}, {"referenceID": 18, "context": "This fact has been recently explored by [3, 19] to define a deep generative model and estimate the parameters by the MMD criterion.", "startOffset": 40, "endOffset": 47}, {"referenceID": 0, "context": "U(h) = I(0\u2264h\u22641) is a uniform distribution on [0, 1] and I(\u00b7) is the indicator function that equals to 1 if the predicate holds and 0 otherwise.", "startOffset": 45, "endOffset": 51}, {"referenceID": 21, "context": "For example, a simple network can consist of multiple layer perceptrons (MLP) activated by some non-linear functions such as the rectified linear unit (ReLu) [22].", "startOffset": 158, "endOffset": 162}, {"referenceID": 24, "context": "We also use convolutional neural networks (CNN) as hidden layers [25] in our experiments.", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "4 Experiments We now present a diverse range of applications to evaluate our model, including predictive modeling, contextual generation and an interesting case of Bayesian dark knowledge [15].", "startOffset": 188, "endOffset": 192}, {"referenceID": 0, "context": "Each image is of size 28\u00d7 28 and the gray-scale is normalized to be in range [0, 1].", "startOffset": 77, "endOffset": 83}, {"referenceID": 0, "context": "For prediction task, the conditional variables are the images x \u2208 [0, 1]28\u00d728, and the generated sample is a class label, which is represented as a vector y \u2208 R + and each yi denotes the confidence that x is in class i.", "startOffset": 66, "endOffset": 72}, {"referenceID": 17, "context": "Table 1: Error rates (%) on MNIST dataset Model Error Rate VA+Pegasos [18] 1.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "04 MMVA [18] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "97 CVA + Pegasos [18] 1.", "startOffset": 17, "endOffset": 21}, {"referenceID": 32, "context": "47 Stochastic Pooling [33] 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "47 Network in Network [20] 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "47 Maxout Network [6] 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 17, "context": "45 CMMVA [18] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "45 DSN [17] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "39 We compare our model, denoted as CGMMN in the MLP case and CGMMN-CNN in the CNN case, with Varitional Auto-encoder (VA) [14], which is an unsupervised DGM learnt by stochastic variational methods.", "startOffset": 123, "endOffset": 127}, {"referenceID": 25, "context": "To use VA for classification, a subsequent classifier is built \u2014 We first learn feature representations by VA and then learn a linear SVM on these features using Pegasos algorithm [26].", "startOffset": 180, "endOffset": 184}, {"referenceID": 17, "context": "We also compare with maxmargin DGMs (denoted as MMVA with MLP as hidden layers and CMMVA in the CNN case) [18], which is a state-of-the-art DGM for prediction, and several other strong baselines, including Stochastic Pooling [33], Network in Network [20], Maxout Network [6] and Deeply-supervised nets (DSN) [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 32, "context": "We also compare with maxmargin DGMs (denoted as MMVA with MLP as hidden layers and CMMVA in the CNN case) [18], which is a state-of-the-art DGM for prediction, and several other strong baselines, including Stochastic Pooling [33], Network in Network [20], Maxout Network [6] and Deeply-supervised nets (DSN) [17].", "startOffset": 225, "endOffset": 229}, {"referenceID": 19, "context": "We also compare with maxmargin DGMs (denoted as MMVA with MLP as hidden layers and CMMVA in the CNN case) [18], which is a state-of-the-art DGM for prediction, and several other strong baselines, including Stochastic Pooling [33], Network in Network [20], Maxout Network [6] and Deeply-supervised nets (DSN) [17].", "startOffset": 250, "endOffset": 254}, {"referenceID": 5, "context": "We also compare with maxmargin DGMs (denoted as MMVA with MLP as hidden layers and CMMVA in the CNN case) [18], which is a state-of-the-art DGM for prediction, and several other strong baselines, including Stochastic Pooling [33], Network in Network [20], Maxout Network [6] and Deeply-supervised nets (DSN) [17].", "startOffset": 271, "endOffset": 274}, {"referenceID": 16, "context": "We also compare with maxmargin DGMs (denoted as MMVA with MLP as hidden layers and CMMVA in the CNN case) [18], which is a state-of-the-art DGM for prediction, and several other strong baselines, including Stochastic Pooling [33], Network in Network [20], Maxout Network [6] and Deeply-supervised nets (DSN) [17].", "startOffset": 308, "endOffset": 312}, {"referenceID": 17, "context": "In the CNN case, we use the same architecture as [18], where there are 32 feature maps in the first two convolutional layers and 64 feature maps in the last three hidden layers.", "startOffset": 49, "endOffset": 53}, {"referenceID": 5, "context": "The total number of parameters in the network is comparable with the competitors [6, 17, 18, 20].", "startOffset": 81, "endOffset": 96}, {"referenceID": 16, "context": "The total number of parameters in the network is comparable with the competitors [6, 17, 18, 20].", "startOffset": 81, "endOffset": 96}, {"referenceID": 17, "context": "The total number of parameters in the network is comparable with the competitors [6, 17, 18, 20].", "startOffset": 81, "endOffset": 96}, {"referenceID": 19, "context": "The total number of parameters in the network is comparable with the competitors [6, 17, 18, 20].", "startOffset": 81, "endOffset": 96}, {"referenceID": 12, "context": "In both settings, we use AdaM [13] to optimize parameters.", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "2 Results on SVHN dataset Table 2: Error rates (%) on SVHN dataset Model Error Rate CVA+Pegasos [18] 25.", "startOffset": 96, "endOffset": 100}, {"referenceID": 24, "context": "13 CNN [25] 4.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "9 CMMVA [18] 3.", "startOffset": 8, "endOffset": 12}, {"referenceID": 32, "context": "09 Stochastic Pooling [33] 2.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "80 Network in Network [20] 2.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "47 Maxout Network [6] 2.", "startOffset": 18, "endOffset": 21}, {"referenceID": 16, "context": "35 DSN [17] 1.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "Following [18, 25], we preprocess the data by Local Contrast Normalization (LCN).", "startOffset": 10, "endOffset": 18}, {"referenceID": 24, "context": "Following [18, 25], we preprocess the data by Local Contrast Normalization (LCN).", "startOffset": 10, "endOffset": 18}, {"referenceID": 18, "context": "As in [19], we investigate whether the models learn to merely copy the data.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "We visualize the nearest neighbors in the MNIST dataset of several samples generated by CGMMN in terms of Euclidean pixelwise distance [5] in Fig.", "startOffset": 135, "endOffset": 138}, {"referenceID": 18, "context": "The architecture follows from [19].", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "As also discussed in [19], real-world data can be complicated and high-dimensional and autoencoder can be good at representing data in a code space that captures enough statistical information to reliably reconstruct the data.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "2 Results on Yale Face dataset We now show the generating results on the Extended Yale Face dataset [9], which contains 2, 414 grayscale images for 38 individuals of dimension 32\u00d7 32.", "startOffset": 100, "endOffset": 103}, {"referenceID": 14, "context": "[15] show that we can learn a relatively simple student network to distill knowledge from the teacher network (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "For high-dimensional data, adopting the same strategy as [15], we sample \u201cnear\" the training data to generate the former dataset (i.", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "Due to the space limitation, we test our model on a regression problem on the Boston housing dataset, which was also used in [10, 15], while deferring the other results on a synthetic dataset to Appendix A.", "startOffset": 125, "endOffset": 133}, {"referenceID": 14, "context": "Due to the space limitation, we test our model on a regression problem on the Boston housing dataset, which was also used in [10, 15], while deferring the other results on a synthetic dataset to Appendix A.", "startOffset": 125, "endOffset": 133}, {"referenceID": 9, "context": "We first train a PBP model [10], which is a scalable method for posterior inference in Bayesian neural networks, as the teacher and then distill it using our CGMMN model.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "093 We distill the PBP model [10] using an MLP network with three hidden layers and (100, 50, 50) hidden units for middle layers.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "For deep learning models, sum product networks (SPN) [24] provide exact inference on DGMs and its conditional extension [4] improves the discriminative ability; and the recent work [21] presents a conditional version of the generative adversarial networks (GAN) [5] with wider applicability.", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "For deep learning models, sum product networks (SPN) [24] provide exact inference on DGMs and its conditional extension [4] improves the discriminative ability; and the recent work [21] presents a conditional version of the generative adversarial networks (GAN) [5] with wider applicability.", "startOffset": 120, "endOffset": 123}, {"referenceID": 20, "context": "For deep learning models, sum product networks (SPN) [24] provide exact inference on DGMs and its conditional extension [4] improves the discriminative ability; and the recent work [21] presents a conditional version of the generative adversarial networks (GAN) [5] with wider applicability.", "startOffset": 181, "endOffset": 185}, {"referenceID": 4, "context": "For deep learning models, sum product networks (SPN) [24] provide exact inference on DGMs and its conditional extension [4] improves the discriminative ability; and the recent work [21] presents a conditional version of the generative adversarial networks (GAN) [5] with wider applicability.", "startOffset": 262, "endOffset": 265}, {"referenceID": 27, "context": "Besides, the recent proposed conditional variational autoencoder [28] also works well on structured prediction.", "startOffset": 65, "endOffset": 69}], "year": 2016, "abstractText": "Maximum mean discrepancy (MMD) has been successfully applied to learn deep generative models for characterizing a joint distribution of variables via kernel mean embedding. In this paper, we present conditional generative moment-matching networks (CGMMN), which learn a conditional distribution given some input variables based on a conditional maximum mean discrepancy (CMMD) criterion. The learning is performed by stochastic gradient descent with the gradient calculated by back-propagation. We evaluate CGMMN on a wide range of tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge, which distills knowledge from a Bayesian model by learning a relatively small CGMMN student network. Our results demonstrate competitive performance in all the tasks.", "creator": "LaTeX with hyperref package"}}}