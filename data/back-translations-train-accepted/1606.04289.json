{"id": "1606.04289", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Automatic Text Scoring Using Neural Networks", "abstract": "Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the predictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which specific words contribute to the text's score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent advances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative.", "histories": [["v1", "Tue, 14 Jun 2016 10:17:27 GMT  (33kb)", "https://arxiv.org/abs/1606.04289v1", "11 pages, 3 figures, 2 tables, ACL-2016"], ["v2", "Thu, 16 Jun 2016 16:30:33 GMT  (33kb,D)", "http://arxiv.org/abs/1606.04289v2", "11 pages, 3 figures, 2 tables, ACL-2016"]], "COMMENTS": "11 pages, 3 figures, 2 tables, ACL-2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["dimitrios alikaniotis", "helen yannakoudakis", "marek rei"], "accepted": true, "id": "1606.04289"}, "pdf": {"name": "1606.04289.pdf", "metadata": {"source": "CRF", "title": "Automatic Text Scoring Using Neural Networks", "authors": ["Dimitrios Alikaniotis", "Helen Yannakoudakis"], "emails": ["da352@cam.ac.uk", "hy260@cl.cam.ac.uk", "mr472@cl.cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is a purely mental game in which it is a matter of finding a path to tread, of finding a path to tread, of finding a path to tread, of finding a path to tread, of exploring it, of walking it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it, of exploring it."}, {"heading": "2 Related Work", "text": "In this area, we are able to take a number of measures aimed at achieving objectives, such as the creation of jobs, the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs and the creation of jobs, the creation of jobs, the creation of jobs and the creation of jobs, the creation of the creation of jobs, the creation of jobs and the creation of the creation of jobs, the creation of the creation of jobs, the creation of the creation of jobs, the creation of jobs and the creation of the creation of jobs, the creation of the creation of jobs, the creation of jobs, the creation of the creation of jobs and the creation of jobs, the creation of jobs, the creation of jobs, the creation of the creation of jobs, the creation of the creation of jobs, the creation of jobs, the creation of jobs and the creation of jobs, the creation of the creation of jobs, the creation of the creation of jobs, the creation of jobs, the creation of the creation of jobs, the creation of jobs, the creation of jobs, the creation of the creation of jobs, the creation of the creation of jobs, the creation of jobs, the creation of jobs, the creation of the creation of jobs, the creation of jobs, the creation of the creation of jobs, the creation of the creation of the creation of the"}, {"heading": "3 Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 C&W Embeddings", "text": "In this way, each word is more predictive of its local context than any other random word in the world. < c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c."}, {"heading": "3.2 Augmented C&W model", "text": "Following Tang (2015), we expand the previous model to capture not only the local linguistic environment of each word (1), but also how each word contributes to the overall result of the essay. The goal is to construct representations that, together with the linguistic information provided by the linear order of words in each sentence, are able to capture usage information. Words such as \"is,\" to which each essay score appears, are considered too little informative in the sense that they activate both high and low scoring essays. Informative words, on the other hand, are those that have an impact on the essay score (e.g. misspelling). To capture these point-specific word embeddings (SSWEs), we expand (4) by adding another linear unit in the output layer that performs linear regression and predicts the essay score."}, {"heading": "3.3 Long-Short Term Memory Network", "text": "We use the SSWEs derived from our models to derive continuous representations for each essay. We treat each essay as a sequence of tokens and explore the use of unidirectional and bi-directional (Graves, 2012) LSTMs (Hochreiter and Schmidhuber, 1997) to embed these sequences in a fixed size vector. Output and bi-directional LSTMs are a kind of recurrent neural network (RNN) in which the output is conditioned at a time in which both the input s and the bi-directional LSTMs are effectively used for embedding long sequences (Hermann et al., 2015). LSTMs are a kind of recurrent neural (RNN) architecture in which the output s is both in time and in time t \u2212 and in time t \u2212 yt = yt = yt = yt, 2015). LSTMs are a type of recurrent neural (RNN) architecture in which the output s is in both time t \u2212 and time \u2212 yt = yt = yt = yt =, 2015). LSTMs are the recurrent neural (RNN) architecture in which the output s is effectively used for embedding long sequences (Hermann et al., 2015)."}, {"heading": "3.4 Other Baselines", "text": "We train a support vector regression model (see Section 4), which is one of the most widely used approaches in text recovery. We analyze the data using the RASP parser (Briscoe et al., 2006) and extract a number of different characteristics to assess the quality of the essays. Specifically, we use characters and parts of speech unigrams, bigrams, and trigrams; word unigrams and trigrams in which we replace open-class words with their POS; and the distribution of common nouns, prepositions, and coordinators. In addition, we extract and use the rules from the phrase structure tree based on the top parse for each sentence, as well as an estimate of the error rate based on manually derived error rules. Ngrams are weighted with tf-idf, while the rest counts and scales so that all characteristics are approximately of the same magnitude."}, {"heading": "4 Dataset", "text": "The Kaggle dataset contains 12,976 essays between 150 and 550 words, each marked by two evaluators (Cohen value = 0.86).5 For our experiments, we use the resolved combined score between the two evaluators, which is calculated as the average between the evaluators \"scores (if the scores are close to each other) or determined by a third expert (if the scores are far apart).5 At present, the state of the art for this dataset is a Cohen value = 0.81 (using square weights).However, the test set was published without the gold score annotations, making all comparisons meaningless, and we are therefore limited in sharing the given training set to create a new test.The sentences broken down as follows: 80% of the entire dataset was reserved for validation / validation, 20% for actual validation and 20% for actual validation (during the 64% validation)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Results", "text": "The hyperparameters for our model were as follows: the sizes of the layers H, D, the learning rate, the window size n, the number of noisy sequences E and the weight factor \u03b1. Even though the hyperparameters of the LSTM were the size of the LSTM layer DLSTM as well as the failure rate r.5Five statements employed a holistic scoring rubric, one was evaluated with a two-trait rubric, and two were evaluated with a multi-trait rubric, but reported as a holistic score (Shermis and Hammer, 2012).6The code, by-model hyperparameter configurations and the IDs of the test set are available."}, {"heading": "5.2 Discussion", "text": "Our SSWE + LSTM approach, which has no prior knowledge of the grammar of the language or the domain of the text, is able to evaluate the essays in a very human way and outperform other state-of-the-art systems. Furthermore, while tuning the hyperparameters of the models to a separate validation set, we have not performed any further pre-processing of the text as simple tokenization. In the essay evaluation literature, the text length tends to be a strong predictor of the overall value. In order to investigate possible effects of the essay length, we also calculate the correlation between the gold values and the length of the essays. We note that the correlations on the test set are relatively small (r = 0.3, \u03c1 = 0.44) and therefore conclude that there are no such strong effects. As described above, we used the Bayesian optimization of S1 to find optimal hyperparameter configurations in fewer steps than in regular web searches."}, {"heading": "6 Visualizing the black box", "text": "In this section, inspired by recent advances in (de-) revolutionary neural networks in computer vision (Simonyan et al., 2013) and textual summaries (Denil et al., 2014), we would introduce a novel method of generating false interpretations of the performance of the network. In the current context, this is particularly important as an advantage of the manual methods discussed in \u00a7 2 to know on which basis the model made its decisions and which characteristics are most discriminatory. In the beginning, our goal is to assess the \"quality of our word vectors.\" By \"quality,\" we mean the level at which a word appears in a particular context would prove problematic for predicting the network. In order to identify \"high\" and \"low quality\" vectors, we perform a single pass of a single pass from left to right and let the LSTM make its score predictions. Normally, we would provide the gold scores and network weights based on the errors."}, {"heading": "7 Conclusion", "text": "In this paper, we presented an in-depth neural network model that is capable of mapping both local context and usage information as summarized in essay scoring, which provides score-specific word embedding that is later used by a recurrent neural network to form essay representations. We demonstrated that this type of architecture is capable of surpassing similar state-of-the-art systems, as well as systems based on manual feature engineering that have achieved results close to the upper limit in previous work. We also introduced a novel method to investigate the basis of the network's internal scoring criteria, and demonstrated that such models are interpretable and can be further exploited to provide useful feedback to the author."}, {"heading": "Acknowledgments", "text": "The first author is supported by the Onassis Foundation. We would like to thank the three anonymous reviewers for their valuable feedback. 10We point out that the same visualization technique can be used to show the \"goodness\" of phrases / sentences. Within the phrase setting, the LSTM layer contains the phrase embedding after feeding the last word of the phrase into the network. Then, we can assess the \"goodness\" of this embedding by evaluating the error grades after predicting the highest / lowest score."}], "references": [{"title": "Automated essay scoring with e-Rater v.2.0", "author": ["Attali", "Burstein2006] Yigal Attali", "Jill Burstein"], "venue": "Journal of Technology, Learning, and Assessment,", "citeRegEx": "Attali et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Attali et al\\.", "year": 2006}, {"title": "The second release of the RASP system", "author": ["Briscoe et al.2006] Ted Briscoe", "John Carroll", "Rebecca Watson"], "venue": "In Proceedings of the COLING/ACL,", "citeRegEx": "Briscoe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Briscoe et al\\.", "year": 2006}, {"title": "Automated assessment of ESOL free text examinations", "author": ["Briscoe et al.2010] Ted Briscoe", "Ben Medlock", "\u00d8istein E. Andersen"], "venue": "Technical Report UCAM-CL-TR-790,", "citeRegEx": "Briscoe et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Briscoe et al\\.", "year": 2010}, {"title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "author": ["Tom\u00e1\u0161 Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "In arXiv preprint", "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "An Unsupervised Automated Essay Scoring System", "author": ["Chen et al.2010] YY Chen", "CL Liu", "TH Chang", "CH Lee"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "Proceedings of the TwentyFifth international conference on Machine Learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing", "author": ["Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Pssst... textual features... there is more to automatic essay scoring than just you", "author": ["Laura K Allen", "Erica L Snow", "Danielle S McNamara"], "venue": "In Proceedings of the Fifth International Conference on Learning Analyt-", "citeRegEx": "Crossley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Crossley et al\\.", "year": 2015}, {"title": "Equilibrated adaptive learning rates for non-convex optimization", "author": ["Harm de Vries", "Yoshua Bengio"], "venue": null, "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "Modelling, visualising and summarising documents with a single convolutional neural network. Jun", "author": ["Denil et al.2014] Misha Denil", "Alban Demiraj", "Nal Kalchbrenner", "Phil Blunsom", "Nando de Freitas"], "venue": null, "citeRegEx": "Denil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2014}, {"title": "IntellimetricTM: From here to validity", "author": ["S. Elliot"], "venue": "Automated Essay Scoring: A CrossDisciplinary Perspective,", "citeRegEx": "Elliot.,? \\Q2003\\E", "shortCiteRegEx": "Elliot.", "year": 2003}, {"title": "Scoring persuasive essays using opinions and their targets", "author": ["Farra et al.2015] Noura Farra", "Swapna Somasundaran", "Jill Burstein"], "venue": "In Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Farra et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Farra et al\\.", "year": 2015}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Gutmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2012}, {"title": "Teaching machines to read and comprehend", "author": ["Tom Koisk", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": null, "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S Hochreiter", "J Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Word association profiles and their use for automated scoring of essays", "author": ["Klebanov", "Flor2013] Beata Beigman Klebanov", "Michael Flor"], "venue": "In Proceedings of the 51st Annual Meeting of the Association", "citeRegEx": "Klebanov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Klebanov et al\\.", "year": 2013}, {"title": "Automated scoring and annotation of essays with the Intelligent Essay Assessor", "author": ["Darrell Laham", "Peter W. Foltz"], "venue": null, "citeRegEx": "Landauer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 2003}, {"title": "Distributed representations of sentences and documents. May", "author": ["Le", "Mikolov2014] Quoc V. Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Lee et al.2009] Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y. Ng"], "venue": "Proceedings of the", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Automated rating of ESL essays", "author": ["Lonsdale", "D. Strong-Krause"], "venue": "In Proceedings of the HLT-NAACL", "citeRegEx": "Lonsdale et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lonsdale et al\\.", "year": 2003}, {"title": "A hierarchical classification approach to automated essay scoring", "author": ["Scott A Crossley", "Rod D Roscoe", "Laura K Allen", "Jianmin Dai"], "venue": "Assessing Writing,", "citeRegEx": "McNamara et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McNamara et al\\.", "year": 2015}, {"title": "RNNLM-Recurrent neural network language modeling toolkit", "author": ["Stefan Kombrink", "Anoop Deoras", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd"], "venue": "In ASRU 2011 Demo Session", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["I Sutskever", "K Chen", "G S Corrado", "Jeffrey Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Grading essays by computer: progress report", "author": ["Ellis B. Page"], "venue": "In Proceedings of the Invitational Conference on Testing Problems,", "citeRegEx": "Page.,? \\Q1967\\E", "shortCiteRegEx": "Page.", "year": 1967}, {"title": "The use of the computer in analyzing student essays", "author": ["Ellis B. Page"], "venue": "International Review of Education,", "citeRegEx": "Page.,? \\Q1968\\E", "shortCiteRegEx": "Page.", "year": 1968}, {"title": "Project essay grade: PEG", "author": ["E.B. Page"], "venue": "Automated essay scoring: A cross-disciplinary perspective,", "citeRegEx": "Page.,? \\Q2003\\E", "shortCiteRegEx": "Page.", "year": 2003}, {"title": "Automated essay scoring using Bayes", "author": ["Rudner", "Liang2002] L.M. Rudner", "Tahung Liang"], "venue": "theorem. The Journal of Technology, Learning and Assessment,", "citeRegEx": "Rudner et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Rudner et al\\.", "year": 2002}, {"title": "Effective feature integration for automated short answer scoring", "author": ["Michael Heilman", "Nitin Madnani"], "venue": "In Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications", "citeRegEx": "Sakaguchi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sakaguchi et al\\.", "year": 2015}, {"title": "Contrasting state-of-the-art automated scoring of essays: analysis. Technical report, The University of Akron and Kaggle", "author": ["Shermis", "Hammer2012] M Shermis", "B Hammer"], "venue": null, "citeRegEx": "Shermis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shermis et al\\.", "year": 2012}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Andrea Vedaldi", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Practical bayesian optimization of machine learning", "author": ["Snoek et al.2012] Jasper Snoek", "Hugo Larochelle", "Ryan P. Adams"], "venue": null, "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Lexical chaining for measuring discourse coherence quality in test-taker essays", "author": ["Jill Burstein", "Martin Chodorow"], "venue": "In COLING,", "citeRegEx": "Somasundaran et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Somasundaran et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Sentiment-specific representation learning for document-level sentiment analysis", "author": ["Duyu Tang"], "venue": "In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining - WSDM '15", "citeRegEx": "Tang.,? \\Q2015\\E", "shortCiteRegEx": "Tang.", "year": 2015}, {"title": "A framework for implementing automated scoring", "author": ["D.M. Williamson"], "venue": "Technical report, Educational Testing Service", "citeRegEx": "Williamson.,? \\Q2009\\E", "shortCiteRegEx": "Williamson.", "year": 2009}, {"title": "Evaluating the performance of automated text scoring systems", "author": ["Yannakoudakis", "Cummins2015] Helen Yannakoudakis", "Ronan Cummins"], "venue": "In Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications", "citeRegEx": "Yannakoudakis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yannakoudakis et al\\.", "year": 2015}, {"title": "A new dataset and method for automatically grading ESOL texts. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "author": ["Ted Briscoe", "Ben Medlock"], "venue": null, "citeRegEx": "Yannakoudakis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yannakoudakis et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 24, "context": "The advantages of ATS systems have been established since Project Essay Grade (PEG) (Page, 1967; Page, 1968), one of the earliest systems whose development was largely motivated by the prospect of reducing labour-intensive marking activities.", "startOffset": 84, "endOffset": 108}, {"referenceID": 25, "context": "The advantages of ATS systems have been established since Project Essay Grade (PEG) (Page, 1967; Page, 1968), one of the earliest systems whose development was largely motivated by the prospect of reducing labour-intensive marking activities.", "startOffset": 84, "endOffset": 108}, {"referenceID": 36, "context": ", 2015, among others), overviews of which can be found in various studies (Williamson, 2009; Dikli, 2006; Shermis and Hammer, 2012).", "startOffset": 74, "endOffset": 131}, {"referenceID": 19, "context": "Multi-layer neural networks are known for automatically learning useful features from data, with lower layers learning basic feature detectors and upper levels learning more high-level abstract features (Lee et al., 2009).", "startOffset": 203, "endOffset": 221}, {"referenceID": 22, "context": "Additionally, recurrent neural networks are well-suited for modeling the compositionality of language and have been shown to perform very well on the task of language modeling (Mikolov et al., 2011; Chelba et al., 2013).", "startOffset": 176, "endOffset": 219}, {"referenceID": 3, "context": "Additionally, recurrent neural networks are well-suited for modeling the compositionality of language and have been shown to perform very well on the task of language modeling (Mikolov et al., 2011; Chelba et al., 2013).", "startOffset": 176, "endOffset": 219}, {"referenceID": 24, "context": "Project Essay Grade (Page, 1967; Page, 1968; Page, 2003) is one of the earliest automated scoring systems, predicting a score using linear regression over vectors of textual features considered to be proxies of writing quality.", "startOffset": 20, "endOffset": 56}, {"referenceID": 25, "context": "Project Essay Grade (Page, 1967; Page, 1968; Page, 2003) is one of the earliest automated scoring systems, predicting a score using linear regression over vectors of textual features considered to be proxies of writing quality.", "startOffset": 20, "endOffset": 56}, {"referenceID": 26, "context": "Project Essay Grade (Page, 1967; Page, 1968; Page, 2003) is one of the earliest automated scoring systems, predicting a score using linear regression over vectors of textual features considered to be proxies of writing quality.", "startOffset": 20, "endOffset": 56}, {"referenceID": 17, "context": "Intelligent Essay Assessor (Landauer et al., 2003) uses Latent Semantic Analysis to compute the semantic similarity between texts at specific grade points and a test text, which is assigned a score based on the ones in the training set to which it is most similar.", "startOffset": 27, "endOffset": 50}, {"referenceID": 17, "context": "Intelligent Essay Assessor (Landauer et al., 2003) uses Latent Semantic Analysis to compute the semantic similarity between texts at specific grade points and a test text, which is assigned a score based on the ones in the training set to which it is most similar. Lonsdale and Strong-Krause (2003) use the Link Grammar parser (Sleator and Templerley, 1995) to analyse and score texts based on the average sentence-level", "startOffset": 28, "endOffset": 299}, {"referenceID": 20, "context": "Recently, McNamara et al. (2015) used a hierachical classification approach to scoring, utilizing linguistic, semantic and rhetorical features, among others.", "startOffset": 10, "endOffset": 33}, {"referenceID": 11, "context": "Farra et al. (2015) utilize variants of logistic and linear regression and develop models that score persuasive essays based on features extracted from opinion expressions and topical elements.", "startOffset": 0, "endOffset": 20}, {"referenceID": 31, "context": "Somasundaran et al. (2014) exploit lexical chains and their interaction with discourse elements for evaluating the quality of persuasive essays with respect to discourse coherence.", "startOffset": 0, "endOffset": 27}, {"referenceID": 7, "context": "Crossley et al. (2015) identify student attributes, such as standardized test scores, as predictive of writing success and use them in conjunction with textual features to develop essay scoring models.", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "Collobert and Weston (2008) and Collobert et al. (2011) introduce a neural network architecture (Fig.", "startOffset": 32, "endOffset": 56}, {"referenceID": 23, "context": "where E is another hyperparameter controlling the number of \u2018noisy\u2019 sequences we give along with the correct sequence (Mikolov et al., 2013; Gutmann and Hyv\u00e4rinen, 2012).", "startOffset": 118, "endOffset": 169}, {"referenceID": 35, "context": "Following Tang (2015), we extend the previous model to capture not only the local linguistic environment of each word, but also how each word contributes to the overall score of the essay.", "startOffset": 10, "endOffset": 22}, {"referenceID": 12, "context": "We treat each essay as a sequence of tokens and explore the use of uni- and bi-directional (Graves, 2012) LongShort Term Memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997) in order to embed these sequences in a vector of fixed size.", "startOffset": 91, "endOffset": 105}, {"referenceID": 14, "context": "Both uniand bi-directional LSTMs have been effectively used for embedding long sequences (Hermann et al., 2015).", "startOffset": 89, "endOffset": 111}, {"referenceID": 8, "context": "We use the mean square error between the predicted and the gold score as our loss function, and optimize with RMSprop (Dauphin et al., 2015), propagating the errors back to the word embeddings.", "startOffset": 118, "endOffset": 140}, {"referenceID": 1, "context": "We parse the data using the RASP parser (Briscoe et al., 2006) and extract a number of different features for assessing the quality of the essays.", "startOffset": 40, "endOffset": 62}, {"referenceID": 23, "context": "\u2022 word2vec embeddings (Mikolov et al., 2013) trained on our training set (see Section 4).", "startOffset": 22, "endOffset": 44}, {"referenceID": 23, "context": "\u2022 Publicly available word2vec embeddings (Mikolov et al., 2013) pre-trained on the Google News corpus (ca.", "startOffset": 41, "endOffset": 63}, {"referenceID": 31, "context": "search, the best hyperparameters were determined using Bayesian Optimization (Snoek et al., 2012).", "startOffset": 77, "endOffset": 97}, {"referenceID": 38, "context": "As described above, the SVM model has rich linguistic knowledge and consists of hand-picked features which have achieved excellent performance in similar tasks (Yannakoudakis et al., 2011).", "startOffset": 160, "endOffset": 188}, {"referenceID": 22, "context": "The number of \u2018noisy\u2019 sequences was set to 200, which was the highest possible setting we considered, although this might be related more to the size of the corpus (see Mikolov et al. (2013) for a similar discussion) rather than to our approach.", "startOffset": 169, "endOffset": 191}, {"referenceID": 30, "context": "In this section, inspired by recent advances in (de-) convolutional neural networks in computer vision (Simonyan et al., 2013) and text summarization (Denil et al.", "startOffset": 103, "endOffset": 126}, {"referenceID": 9, "context": ", 2013) and text summarization (Denil et al., 2014), we introduce a novel method of generating interpretable visualizations of the network\u2019s performance.", "startOffset": 31, "endOffset": 51}], "year": 2016, "abstractText": "Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the predictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which specific words contribute to the text\u2019s score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent advances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative.", "creator": "LaTeX with hyperref package"}}}