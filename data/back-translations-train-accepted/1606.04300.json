{"id": "1606.04300", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Neural Word Segmentation Learning for Chinese", "abstract": "Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task so that only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured. In this paper, we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history. Our model employs a gated combination neural network over characters to produce distributed representations of word candidates, which are then given to a long short-term memory (LSTM) language scoring model. Experiments on the benchmark datasets show that without the help of feature engineering as most existing approaches, our models achieve competitive or better performances with previous state-of-the-art methods.", "histories": [["v1", "Tue, 14 Jun 2016 10:52:21 GMT  (703kb,D)", "http://arxiv.org/abs/1606.04300v1", "ACL2016"], ["v2", "Fri, 2 Dec 2016 08:06:10 GMT  (897kb,D)", "http://arxiv.org/abs/1606.04300v2", "ACL2016"]], "COMMENTS": "ACL2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["deng cai", "hai zhao"], "accepted": true, "id": "1606.04300"}, "pdf": {"name": "1606.04300.pdf", "metadata": {"source": "CRF", "title": "Neural Word Segmentation Learning for Chinese", "authors": ["Deng Cai", "Hai Zhao"], "emails": ["thisisjcykcd@gmail.com,", "zhaohai@cs.sjtu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Most of them are able to survive themselves, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own."}, {"heading": "2 Overview", "text": "We formulate the CWS problem in such a way that we find an assignment from an input string x to a word string y, and the output set y * suffices: y * = arg max y \u0433GEN (x) (n \u2211 i = 1 score (yi | y1, \u00b7 \u00b7, yi \u2212 1)), where n is the number of word candidates in y, and GEN (x) denotes the set of possible segmentations for an input sequence x. Unlike all previous work, our scoring function is sensitive to the complete content of the partially segmented sentence. As shown in Figure 1, a neural network scoring model is developed to solve CWS in this way to evaluate the probability of a segmented sentence. Based on the proposed model, a decoder is developed to find the segmented sentence with the highest score. Meanwhile, a maximum margin method is used to perform the training by comparing the structured segmentation and the gilded output."}, {"heading": "3 Neural Network Scoring Model", "text": "The score for a segmented sentence is calculated by first mapping it into a sequence of word candidate vectors, then the scoring model takes the vector sequence as input and evaluates each word candidate from two perspectives: (1) how likely the word candidate itself can be recognized as a legal word; (2) how useful the linkage is for the word candidate to immediately follow the previous segmentation history. Then, the word candidate is appended to the segmentation history and updates the state of the scoring system for subsequent evaluations. Figure 2 illustrates the entire neural scoring network."}, {"heading": "3.1 Word Score", "text": "While the results are decided at the word level, word embedding is integrated into a complex network (Bengio et al., 2003; Wang et al., 2016) will immediately lead to a notable problem that is rare words and vocabulary terms (Kim et al., 2015). Furthermore, the character level in an n-gram can be helpful to judge whether it is a real word. (D) It is a real vector (character embedding) c, where the dimensionality of the vector space is used as the lower word. (D) Any character c that is used as a real vector (character embedding) c, where d is the dimensionality of the vector space. The character embedding is then stacked into an embedding matrix M."}, {"heading": "3.2 Link Score", "text": "Inspired by the Recurrent Neural Network Language Model (RNN-LM) (Mikolov et al., 2010; Sundermeyer et al., 2012), we use an LSTM system to capture coherence in a segmented sentence. Long Short-Term Memory Networks. The LSTM Neural Network (Hochreiter and Schmidhuber, 1997) is an extension of the Recurrent Neural Network (RNN), which is an effective tool for sequence modeling tasks that use their hidden states to preserve historical information. At any time, an RNN takes the input xt and updates its recurrent hidden state ht byht = g (Uht \u2212 1 + Wxt + b) where g is a non-linear function. Although RNN is able to process arbitrary lengths of sequences, it can be difficult to learn an RNN to learn long-range dependencies due to winding increases."}, {"heading": "3.3 Sentence score", "text": "The sentence value for a segmented sentence y with n word candidates is calculated by adding word values (2) and link values (3) as follows: s (y [1: n], \u03b8) = n \u2211 t = 1 (u \u00b7 yt + pt \u00b7 yt) (4), \u03b8 being the parameter set used in our model."}, {"heading": "4 Decoding", "text": "The total number of possible segmented sentences grows exponentially with the length of the string, which makes it impossible to calculate the results of all possible segmentations (Markov assumption). However, since our model aims to capture the complete history of the segmentation decisions, such dynamic programming algorithms cannot be adopted in this situation. Algorithm 1 Beam Search. Input: Parameter, beam size k, maximum word length and input string c [1: n] Output: Approx. k Best segmentation algorithms cannot be used in this situation. Algorithm 1 Beam Search. Input: Parameter \u03b8, beam size k, maximum word length and input string c [1: n] Output: Approx. k best segments 1: Prox. [0] Procedure: (Score = 0, h = h0, c = c0)."}, {"heading": "5 Training", "text": "As reported in Kummerfeld et al., 2015, margin methods generally exceed both probability and perception methods. For a given string x (i), we designate the correct segmented sentence for x (i) as y (i). We define a structured margin loss where the length of the sequence x (i) and \u00b5 is the discount parameter. Calculation of margin loss could be considered a count of the number of incorrectly segmented characters and then a multiplication with a fixed discount parameter for smoothing. Therefore, the loss is proportional to the number of incorrectly segmented words."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Datasets", "text": "To evaluate the proposed segmentator, we use two popular datasets, PKU and MSR, from the second International Chinese Word Segmentation Bakeoff (Emerson, 2005). These datasets are commonly used by earlier state-of-the-art models and neural network models. Both datasets are pre-processed by replacing the consecutive English characters and digits with a unique token. All experiments are performed using the standard Bakeoff Scoring Program 1, which calculates precision, retrieval and F1 score."}, {"heading": "6.2 Hyper-parameters", "text": "In order to determine a set of suitable hyperparameters, we divide the training data into two sets, the first 90% sets as a training set and the remaining 10% sets as a development set. We choose the hyperparameters as shown in Table 2.We found that the embedding size of the character has a limited effect on performance as long as it is large enough. Size 50 is chosen as a good compromise between speed and performance. The number of hidden units is set to correspond to the embedding of the character. The maximum word length determines the number of parameters in the GCNN part and the time-consuming effort of bar searching, as the words with the length l > 4 are relatively rare, 0.29% in the PKU training data and 1.25% in the MSR training data, we set the maximum word length to 4 in our experiments."}, {"heading": "6.3 Model Analysis", "text": "Figure 5 shows that a beam size 4 segmentator is sufficient to achieve the best performance, which allows our model to strike a good balance between accuracy and efficiency.GCNN. We then examined the role of GCNN in our model. To demonstrate the effects of GCNN, we implemented a simplified version of our model that replaces the GCNN part with a single nonlinear layer, as shown in Equation (1).The results are listed in Table 3, which show that performance is significantly increased by using the GCNN architecture (94.0% to 95.5% for F1score), while the best performance the simplified version can achieve is by using a much larger character.Link Score & Word Score. We conducted several experiments to examine the individual effect of linkscore and word score, as these two types of results are designed to estimate the probability from two different perspectives."}, {"heading": "6.4 Results", "text": "We are comparing our model with the latest neural methods listed in Table 4. (Chen et al., 2015b) We are using an additional method of filtering Chinese expressions by an external dictionary. (3) The list of Chinese expressions included in the dictionaries is long. (4) This method means that we are not moving in the same direction."}, {"heading": "7 Related Work", "text": "Most modern CWS methods followed (Xue, 2003), but treated CWS as sequence labeling problems (Zhao et al., 2006b). Recently, researchers have tended to explore neural network-based approaches (Collobert et al., 2011) in order to reduce efforts in the field of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b). To make comparisons fair, we have reprinted their code (https: / github.com / dalstonChen) without their unspecified Chinese dictatorships dictatoriary.6http: / / code.google.com / p / word2vec / 7Our code is released at https: / / github.com / jcyk / CWS.A tagged problem."}, {"heading": "8 Conclusion", "text": "This paper presents a novel neural framework for the task of Chinese word segmentation that contains three main components: (1) a factory for producing word representations when they are accompanied by their regulated characters; (2) a system for assessing the probability at sentence level for segmented sentences; (3) an efficient and effective algorithm to find the best segmentation; and the proposed framework is a recent attempt to formalize word segmentation as a direct structured learning process in light of the most recent distributed representation framework. Although our system delivers results that are better than the most recent segmented neural networks, but comparable to all previous state-of-the-art systems, the framework remains a great potential that can be further studied and improved in the future."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A maximum entropy approach to natural language processing", "author": ["Adam L Berger", "Vincent J Della Pietra", "Stephen A Della Pietra."], "venue": "Computational linguistics, 22(1):39\u201371.", "citeRegEx": "Berger et al\\.,? 1996", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Gated recursive neural network for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Chen et al\\.,? 2015a", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Chen et al\\.,? 2015b", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "The second international chinese word segmentation bakeoff", "author": ["Thomas Emerson."], "venue": "Proceedings of the fourth SIGHAN workshop on Chinese language Processing, volume 133.", "citeRegEx": "Emerson.,? 2005", "shortCiteRegEx": "Emerson.", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Which is essential for chinese word segmentation: Character versus word", "author": ["Chang-Ning Huang", "Hai Zhao."], "venue": "The 20th Pacific Asia Conference on Language, Information and Computation, pages 1\u201312.", "citeRegEx": "Huang and Zhao.,? 2006", "shortCiteRegEx": "Huang and Zhao.", "year": 2006}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "An empirical analysis of optimization for max-margin nlp", "author": ["Jonathan K. Kummerfeld", "Taylor Berg-Kirkpatrick", "Dan Klein."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 273\u2013279.", "citeRegEx": "Kummerfeld et al\\.,? 2015", "shortCiteRegEx": "Kummerfeld et al\\.", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando CN Pereira."], "venue": "Proceedings of the Eighteenth Interntional Conference on Machine Learning.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Unified dependency parsing of chinese morphological and syntactic structures", "author": ["Zhongguo Li", "Guodong Zhou."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Li and Zhou.,? 2012", "shortCiteRegEx": "Li and Zhou.", "year": 2012}, {"title": "Exploring segment representations for neural segmentation models", "author": ["Yijia Liu", "Wanxiang Che", "Jiang Guo", "Bing Qin", "Ting Liu."], "venue": "arXiv preprint arXiv:1604.05499.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "A maximum entropy approach to chinese word segmentation", "author": ["Jin Kiat Low", "Hwee Tou Ng", "Wenyuan Guo."], "venue": "Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 1612164, pages 448\u2013455.", "citeRegEx": "Low et al\\.,? 2005", "shortCiteRegEx": "Low et al\\.", "year": 2005}, {"title": "Accurate linear-time chinese word segmentation via embedding matching", "author": ["Jianqiang Ma", "Erhard Hinrichs."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Ma and Hinrichs.,? 2015", "shortCiteRegEx": "Ma and Hinrichs.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "11th Annual Conference of the International Speech Communication Association, pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Maxmargin tensor neural network for chinese word segmentation", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 293\u2013303.", "citeRegEx": "Pei et al\\.,? 2014", "shortCiteRegEx": "Pei et al\\.", "year": 2014}, {"title": "Chinese segmentation and new word detection using conditional random fields", "author": ["Fuchun Peng", "Fangfang Feng", "Andrew McCallum."], "venue": "Proceedings of the 20th international conference on Computational Linguistics, page 562.", "citeRegEx": "Peng et al\\.,? 2004", "shortCiteRegEx": "Peng et al\\.", "year": 2004}, {"title": "Deep learning for character-based information extraction", "author": ["Yanjun Qi", "Sujatha G Das", "Ronan Collobert", "Jason Weston."], "venue": "Advances in Information Retrieval, pages 668\u2013674.", "citeRegEx": "Qi et al\\.,? 2014", "shortCiteRegEx": "Qi et al\\.", "year": 2014}, {"title": "Joint chinese word segmentation, pos tagging and parsing", "author": ["Xian Qian", "Yang Liu."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 501\u2013", "citeRegEx": "Qian and Liu.,? 2012", "shortCiteRegEx": "Qian and Liu.", "year": 2012}, {"title": "approximate) subgradient methods for structured prediction", "author": ["Nathan D Ratliff", "J Andrew Bagnell", "Martin Zinkevich."], "venue": "International Conference on Artificial Intelligence and Statistics, pages 380\u2013 387.", "citeRegEx": "Ratliff et al\\.,? 2007", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Enhancing chinese word segmentation using unlabeled data", "author": ["Weiwei Sun", "Jia Xu."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 970\u2013979.", "citeRegEx": "Sun and Xu.,? 2011", "shortCiteRegEx": "Sun and Xu.", "year": 2011}, {"title": "A discriminative latent variable chinese segmenter with hybrid word/character information", "author": ["Xu Sun", "Yaozhong Zhang", "Takuya Matsuzaki", "Yoshimasa Tsuruoka", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of Human Language Technologies:", "citeRegEx": "Sun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection", "author": ["Xu Sun", "Houfeng Wang", "Wenjie Li."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Sun et al\\.,? 2012", "shortCiteRegEx": "Sun et al\\.", "year": 2012}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "13th Annual Conference of the International Speech Communication Association.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin."], "venue": "Proceedings of the 22nd international conference on Machine learning, pages 896\u2013903.", "citeRegEx": "Taskar et al\\.,? 2005", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "A conditional random field word segmenter for sighan bakeoff 2005", "author": ["Huihsin Tseng", "Pichuan Chang", "Galen Andrew", "Daniel Jurafsky", "Christopher Manning."], "venue": "Proceedings of the fourth SIGHAN workshop on Chinese language Processing, volume", "citeRegEx": "Tseng et al\\.,? 2005", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "Learning distributed word representations for bidirectional lstm recurrent neural network", "author": ["Peilu Wang", "Yao Qian", "Hai Zhao", "Frank K. Soong", "Lei He", "Ke Wu."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Chinese word segmentation as character tagging", "author": ["Nianwen Xue."], "venue": "Computational Linguistics and Chinese Language Processing, 8(1):29\u201348.", "citeRegEx": "Xue.,? 2003", "shortCiteRegEx": "Xue.", "year": 2003}, {"title": "Graph-based semi-supervised model for joint chinese word segmentation and partof-speech tagging", "author": ["Xiaodong Zeng", "Derek F. Wong", "Lidia S. Chao", "Isabel Trancoso."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational", "citeRegEx": "Zeng et al\\.,? 2013", "shortCiteRegEx": "Zeng et al\\.", "year": 2013}, {"title": "Chinese segmentation with a word-based perceptron algorithm", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840\u2013 847.", "citeRegEx": "Zhang and Clark.,? 2007", "shortCiteRegEx": "Zhang and Clark.", "year": 2007}, {"title": "Word segmentation on chinese mirco-blog data with a linear-time incremental model", "author": ["Kaixu Zhang", "Maosong Sun", "Changle Zhou."], "venue": "Second CIPSSIGHAN Joint Conference on Chinese Language Processing, pages 41\u201346.", "citeRegEx": "Zhang et al\\.,? 2012", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Exploring representations from unlabeled data with co-training for Chinese word segmentation", "author": ["Longkai Zhang", "Houfeng Wang", "Xu Sun", "Mairgup Mansur."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Zhang et al\\.,? 2013", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Transition-based neural word segmentation", "author": ["Meishan Zhang", "Yue Zhang", "Guohong Fu."], "venue": "Proceedings of the 54nd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Zhang et al\\.,? 2016a", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Probabilistic graph-based dependency parsing with convolutional neural network", "author": ["Zhiong Zhang", "Hai Zhao", "Lianhui Qin."], "venue": "Proceedings of the 54nd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Zhang et al\\.,? 2016b", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Incorporating global information into supervised learning for chinese word segmentation", "author": ["Hai Zhao", "Chunyu Kit."], "venue": "Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics, pages 66\u201374.", "citeRegEx": "Zhao and Kit.,? 2007", "shortCiteRegEx": "Zhao and Kit.", "year": 2007}, {"title": "Exploiting unlabeled text with different unsupervised segmentation criteria for chinese word segmentation", "author": ["Hai Zhao", "Chunyu Kit."], "venue": "Research in Computing Science, 33:93\u2013104.", "citeRegEx": "Zhao and Kit.,? 2008a", "shortCiteRegEx": "Zhao and Kit.", "year": 2008}, {"title": "Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition", "author": ["Hai Zhao", "Chunyu Kit."], "venue": "Proceedings of the Third International Joint Conference on Natural Language Pro-", "citeRegEx": "Zhao and Kit.,? 2008b", "shortCiteRegEx": "Zhao and Kit.", "year": 2008}, {"title": "Integrating unsupervised and supervised word segmentation: The role of goodness measures", "author": ["Hai Zhao", "Chunyu Kit."], "venue": "Information Sciences, 181(1):163\u2013183.", "citeRegEx": "Zhao and Kit.,? 2011", "shortCiteRegEx": "Zhao and Kit.", "year": 2011}, {"title": "An improved chinese word segmentation system with conditional random field", "author": ["Hai Zhao", "Chang-Ning Huang", "Mu Li."], "venue": "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, volume 1082117.", "citeRegEx": "Zhao et al\\.,? 2006a", "shortCiteRegEx": "Zhao et al\\.", "year": 2006}, {"title": "Effective tag set selection in chinese word segmentation via conditional random field modeling", "author": ["Hai Zhao", "Chang-Ning Huang", "Mu Li", "Bao-Liang Lu."], "venue": "Proceedings of the 9th Pacific Association for Computational Linguistics, volume 20,", "citeRegEx": "Zhao et al\\.,? 2006b", "shortCiteRegEx": "Zhao et al\\.", "year": 2006}, {"title": "A unified character-based tagging framework for chinese word segmentation", "author": ["Hai Zhao", "Chang-Ning Huang", "Mu Li", "Bao-Liang Lu."], "venue": "ACM Transactions on Asian Language Information Processing, 9(2):5.", "citeRegEx": "Zhao et al\\.,? 2010", "shortCiteRegEx": "Zhao et al\\.", "year": 2010}, {"title": "Deep learning for Chinese word segmentation and POS tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 647\u2013657.", "citeRegEx": "Zheng et al\\.,? 2013", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 34, "context": "Since Xue (2003), most methods formalize the Chinese word segmentation (CWS) as a sequence labeling problem with character position tags, which can be handled with su-", "startOffset": 6, "endOffset": 17}, {"referenceID": 1, "context": "pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al.", "startOffset": 50, "endOffset": 89}, {"referenceID": 16, "context": "pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al.", "startOffset": 50, "endOffset": 89}, {"referenceID": 13, "context": ", 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a).", "startOffset": 38, "endOffset": 100}, {"referenceID": 21, "context": ", 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a).", "startOffset": 38, "endOffset": 100}, {"referenceID": 45, "context": ", 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a).", "startOffset": 38, "endOffset": 100}, {"referenceID": 6, "context": "(2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al., 2011), and used character embeddings as input to a two-layer network.", "startOffset": 89, "endOffset": 113}, {"referenceID": 48, "context": "(2014) improved upon (Zheng et al., 2013) by explicitly modeling the interactions between local context and previous tag.", "startOffset": 21, "endOffset": 41}, {"referenceID": 10, "context": "Moreover, word-level information, which is being the greater granularity unit as suggested in (Huang and Zhao, 2006), remains ar X iv :1 60 6.", "startOffset": 94, "endOffset": 116}, {"referenceID": 1, "context": "pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a). However, those methods heavily depend on the choice of handcrafted features. Recently, neural models have been widely used for NLP tasks for their ability to minimize the effort in feature engineering. For the task of CWS, Zheng et al. (2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al.", "startOffset": 51, "endOffset": 427}, {"referenceID": 1, "context": "pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a). However, those methods heavily depend on the choice of handcrafted features. Recently, neural models have been widely used for NLP tasks for their ability to minimize the effort in feature engineering. For the task of CWS, Zheng et al. (2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al., 2011), and used character embeddings as input to a two-layer network. Pei et al. (2014) improved upon (Zheng et al.", "startOffset": 51, "endOffset": 616}, {"referenceID": 1, "context": "pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a). However, those methods heavily depend on the choice of handcrafted features. Recently, neural models have been widely used for NLP tasks for their ability to minimize the effort in feature engineering. For the task of CWS, Zheng et al. (2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al., 2011), and used character embeddings as input to a two-layer network. Pei et al. (2014) improved upon (Zheng et al., 2013) by explicitly modeling the interactions between local context and previous tag. Chen et al. (2015a) proposed a gated recursive neural network to model the feature combinations of context characters.", "startOffset": 51, "endOffset": 751}, {"referenceID": 1, "context": "pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a). However, those methods heavily depend on the choice of handcrafted features. Recently, neural models have been widely used for NLP tasks for their ability to minimize the effort in feature engineering. For the task of CWS, Zheng et al. (2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al., 2011), and used character embeddings as input to a two-layer network. Pei et al. (2014) improved upon (Zheng et al., 2013) by explicitly modeling the interactions between local context and previous tag. Chen et al. (2015a) proposed a gated recursive neural network to model the feature combinations of context characters. Chen et al. (2015b) used an LSTM architecture to capture potential long-distance dependencies, which alleviates the limitation of the size of context window but introduced another window for hidden states.", "startOffset": 51, "endOffset": 870}, {"referenceID": 48, "context": "character based (Zheng et al., 2013), .", "startOffset": 16, "endOffset": 36}, {"referenceID": 3, "context": "ci\u22122, ci\u22121, ci, ci+1, ci+2 ti\u22121ti (Chen et al., 2015b) c0, c1, .", "startOffset": 34, "endOffset": 54}, {"referenceID": 36, "context": "word based (Zhang and Clark, 2007), .", "startOffset": 11, "endOffset": 34}, {"referenceID": 0, "context": "To feature a segmented sentence, a series of distributed vector representations (Bengio et al., 2003) are generated to characterize the corresponding word candidates.", "startOffset": 80, "endOffset": 101}, {"referenceID": 9, "context": "Third, to evaluate how a segmented sentence makes sense through word interacting, an LSTM (Hochreiter and Schmidhuber, 1997) is used to chain together word candidates incrementally and construct the representation of partially segmented sentence at each decoding step, so that the coherence between next word candidate and previous segmentation history can be depicted.", "startOffset": 90, "endOffset": 124}, {"referenceID": 0, "context": "While the scores are decided at the word-level, using word embedding (Bengio et al., 2003; Wang et al., 2016) immediately will lead to a remarkable issue that rare words and out-of-vocabulary words will be poorly estimated (Kim et al.", "startOffset": 69, "endOffset": 109}, {"referenceID": 33, "context": "While the scores are decided at the word-level, using word embedding (Bengio et al., 2003; Wang et al., 2016) immediately will lead to a remarkable issue that rare words and out-of-vocabulary words will be poorly estimated (Kim et al.", "startOffset": 69, "endOffset": 109}, {"referenceID": 11, "context": ", 2016) immediately will lead to a remarkable issue that rare words and out-of-vocabulary words will be poorly estimated (Kim et al., 2015).", "startOffset": 121, "endOffset": 139}, {"referenceID": 2, "context": "Gated structure in neural network can be useful for hybrid feature extraction according to (Chen et al., 2015a; Chung et al., 2014; Cho et al., 2014), we therefore propose a gated combination neural network (GCNN) especially for character com-", "startOffset": 91, "endOffset": 149}, {"referenceID": 5, "context": "Gated structure in neural network can be useful for hybrid feature extraction according to (Chen et al., 2015a; Chung et al., 2014; Cho et al., 2014), we therefore propose a gated combination neural network (GCNN) especially for character com-", "startOffset": 91, "endOffset": 149}, {"referenceID": 4, "context": "Gated structure in neural network can be useful for hybrid feature extraction according to (Chen et al., 2015a; Chung et al., 2014; Cho et al., 2014), we therefore propose a gated combination neural network (GCNN) especially for character com-", "startOffset": 91, "endOffset": 149}, {"referenceID": 18, "context": "Inspired by the recurrent neural network language model (RNN-LM) (Mikolov et al., 2010; Sundermeyer et al., 2012), we utilize an LSTM system to capture the coherence in a segmented sentence.", "startOffset": 65, "endOffset": 113}, {"referenceID": 30, "context": "Inspired by the recurrent neural network language model (RNN-LM) (Mikolov et al., 2010; Sundermeyer et al., 2012), we utilize an LSTM system to capture the coherence in a segmented sentence.", "startOffset": 65, "endOffset": 113}, {"referenceID": 9, "context": "The LSTM neural network (Hochreiter and Schmidhuber, 1997) is an extension of the recurrent neural network (RNN), which is an effective tool for sequence modeling tasks using its hidden states for history information preservation.", "startOffset": 24, "endOffset": 58}, {"referenceID": 30, "context": "LSTMs have been shown to outperform RNNs on many NLP tasks, notably language modeling (Sundermeyer et al., 2012).", "startOffset": 86, "endOffset": 112}, {"referenceID": 31, "context": "We use the max-margin criterion (Taskar et al., 2005) to train our model.", "startOffset": 32, "endOffset": 53}, {"referenceID": 12, "context": "As reported in (Kummerfeld et al., 2015), the margin methods generally outperform both likelihood and perception methods.", "startOffset": 15, "endOffset": 40}, {"referenceID": 24, "context": "Due to the hinge loss, the objective function is not differentiable, we use a subgradient method (Ratliff et al., 2007) which computes a gradientlike direction.", "startOffset": 97, "endOffset": 119}, {"referenceID": 25, "context": "Following (Socher et al., 2013), we use the diagonal variant of AdaGrad (Duchi et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 7, "context": ", 2013), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective.", "startOffset": 48, "endOffset": 68}, {"referenceID": 8, "context": "International Chinese Word Segmentation Bakeoff (Emerson, 2005).", "startOffset": 48, "endOffset": 63}, {"referenceID": 26, "context": "Dropout is a popular technique for improving the performance of neural networks by reducing overfitting (Srivastava et al., 2014).", "startOffset": 104, "endOffset": 129}, {"referenceID": 2, "context": "PKU MSR +Dictionary ours theirs ours theirs (Chen et al., 2015a) 94.", "startOffset": 44, "endOffset": 64}, {"referenceID": 3, "context": "2 (Chen et al., 2015b) 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 2, "context": "However, (Chen et al., 2015a; Chen et al., 2015b) used an extra preprocess to filter out Chinese idioms according to an external dictionary.", "startOffset": 9, "endOffset": 49}, {"referenceID": 3, "context": "However, (Chen et al., 2015a; Chen et al., 2015b) used an extra preprocess to filter out Chinese idioms according to an external dictionary.", "startOffset": 9, "endOffset": 49}, {"referenceID": 2, "context": "The dictionary used in (Chen et al., 2015a; Chen et al., 2015b) is neither publicly released nor specified the exact source until now.", "startOffset": 23, "endOffset": 63}, {"referenceID": 3, "context": "The dictionary used in (Chen et al., 2015a; Chen et al., 2015b) is neither publicly released nor specified the exact source until now.", "startOffset": 23, "endOffset": 63}, {"referenceID": 48, "context": "Models PKU MSR P R F P R F (Zheng et al., 2013) 92.", "startOffset": 27, "endOffset": 47}, {"referenceID": 20, "context": "3 (Pei et al., 2014) 93.", "startOffset": 2, "endOffset": 20}, {"referenceID": 2, "context": "4 (Chen et al., 2015a)* 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "1 (Chen et al., 2015b) * 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 48, "context": "+Pre-trained character embedding (Zheng et al., 2013) 93.", "startOffset": 33, "endOffset": 53}, {"referenceID": 20, "context": "9 (Pei et al., 2014) 94.", "startOffset": 2, "endOffset": 20}, {"referenceID": 2, "context": "9 (Chen et al., 2015a)* 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "4 (Chen et al., 2015b)* 95.", "startOffset": 2, "endOffset": 22}, {"referenceID": 32, "context": "Models PKU MSR PKU MSR (Tseng et al., 2005) 95.", "startOffset": 23, "endOffset": 43}, {"referenceID": 36, "context": "4 - (Zhang and Clark, 2007) 94.", "startOffset": 4, "endOffset": 27}, {"referenceID": 43, "context": "2 - (Zhao and Kit, 2008b) 95.", "startOffset": 4, "endOffset": 25}, {"referenceID": 28, "context": "6 - (Sun et al., 2009) 95.", "startOffset": 4, "endOffset": 22}, {"referenceID": 29, "context": "3 - (Sun et al., 2012) 95.", "startOffset": 4, "endOffset": 22}, {"referenceID": 38, "context": "4 - (Zhang et al., 2013) - - 96.", "startOffset": 4, "endOffset": 24}, {"referenceID": 2, "context": "4* (Chen et al., 2015a) 94.", "startOffset": 3, "endOffset": 23}, {"referenceID": 3, "context": "6* (Chen et al., 2015b) 94.", "startOffset": 3, "endOffset": 23}, {"referenceID": 19, "context": "Therefore, we use word2vec (Mikolov et al., 2013) toolkit6 to pre-train the character embeddings on the Chinese Wikipedia corpus and use them for initialization.", "startOffset": 27, "endOffset": 49}, {"referenceID": 38, "context": "Recent systems such as (Zhang et al., 2013), (Chen et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 3, "context": ", 2013), (Chen et al., 2015b) and (Chen et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 2, "context": ", 2015b) and (Chen et al., 2015a) rely on both extensive feature engineering and external corpora to boost performance.", "startOffset": 13, "endOffset": 33}, {"referenceID": 47, "context": "In the closed-set setting, our models can achieve state-of-the-art performance on PKU dataset but a competitive result on MSR dataset, which can attribute to too strict maximum word length setting for consistence as it is well known that MSR corpus has a much longer average word length (Zhao et al., 2010).", "startOffset": 287, "endOffset": 306}, {"referenceID": 34, "context": "Most modern CWS methods followed (Xue, 2003) treated CWS as a sequence labeling problems (Zhao et al.", "startOffset": 33, "endOffset": 44}, {"referenceID": 46, "context": "Most modern CWS methods followed (Xue, 2003) treated CWS as a sequence labeling problems (Zhao et al., 2006b).", "startOffset": 89, "endOffset": 109}, {"referenceID": 6, "context": "Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al.", "startOffset": 77, "endOffset": 101}, {"referenceID": 48, "context": ", 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b).", "startOffset": 49, "endOffset": 126}, {"referenceID": 22, "context": ", 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b).", "startOffset": 49, "endOffset": 126}, {"referenceID": 2, "context": ", 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b).", "startOffset": 49, "endOffset": 126}, {"referenceID": 3, "context": ", 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b).", "startOffset": 49, "endOffset": 126}, {"referenceID": 20, "context": "Pei et al. (2014) introduced the tag embedding as input to capture the combinations of context and tag history.", "startOffset": 0, "endOffset": 18}, {"referenceID": 17, "context": "Another notable exception is (Ma and Hinrichs, 2015), which is also an embedding-based model, but models CWS as configuration-action matching.", "startOffset": 29, "endOffset": 52}, {"referenceID": 35, "context": "Besides sequence labeling schemes, Zhang and Clark (2007) proposed a word-based perceptron method.", "startOffset": 35, "endOffset": 58}, {"referenceID": 35, "context": "Besides sequence labeling schemes, Zhang and Clark (2007) proposed a word-based perceptron method. Zhang et al. (2012) used a linear-time incremental model which can also benefits from various kinds of features including word-based features.", "startOffset": 35, "endOffset": 119}, {"referenceID": 39, "context": "At the same time of this work, some other neural models (Zhang et al., 2016a; Liu et al., 2016) have been proposed, which can also leverage word-level information but are quite different from ours in the basic framework.", "startOffset": 56, "endOffset": 95}, {"referenceID": 15, "context": "At the same time of this work, some other neural models (Zhang et al., 2016a; Liu et al., 2016) have been proposed, which can also leverage word-level information but are quite different from ours in the basic framework.", "startOffset": 56, "endOffset": 95}, {"referenceID": 43, "context": "The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al.", "startOffset": 134, "endOffset": 253}, {"referenceID": 42, "context": "The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al.", "startOffset": 134, "endOffset": 253}, {"referenceID": 27, "context": "The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al.", "startOffset": 134, "endOffset": 253}, {"referenceID": 44, "context": "The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al.", "startOffset": 134, "endOffset": 253}, {"referenceID": 35, "context": "The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al.", "startOffset": 134, "endOffset": 253}, {"referenceID": 38, "context": "The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al.", "startOffset": 134, "endOffset": 253}, {"referenceID": 41, "context": ", 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al., 2016b), and joint models (Qian and Liu, 2012; Li and Zhou, 2012).", "startOffset": 42, "endOffset": 83}, {"referenceID": 40, "context": ", 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al., 2016b), and joint models (Qian and Liu, 2012; Li and Zhou, 2012).", "startOffset": 42, "endOffset": 83}, {"referenceID": 23, "context": ", 2016b), and joint models (Qian and Liu, 2012; Li and Zhou, 2012).", "startOffset": 27, "endOffset": 66}, {"referenceID": 14, "context": ", 2016b), and joint models (Qian and Liu, 2012; Li and Zhou, 2012).", "startOffset": 27, "endOffset": 66}], "year": 2017, "abstractText": "Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task so that only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured. In this paper, we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history. Our model employs a gated combination neural network over characters to produce distributed representations of word candidates, which are then given to a long shortterm memory (LSTM) language scoring model. Experiments on the benchmark datasets show that without the help of feature engineering as most existing approaches, our models achieve competitive or better performances with previous stateof-the-art methods.", "creator": "TeX"}}}