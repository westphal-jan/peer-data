{"id": "1606.04442", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "DeepMath - Deep Sequence Models for Premise Selection", "abstract": "We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the hand-engineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving.", "histories": [["v1", "Tue, 14 Jun 2016 16:27:41 GMT  (904kb,D)", "http://arxiv.org/abs/1606.04442v1", null], ["v2", "Thu, 26 Jan 2017 19:35:16 GMT  (1067kb,D)", "http://arxiv.org/abs/1606.04442v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.LO", "authors": ["geoffrey irving", "christian szegedy", "alexander a alemi", "niklas e\u00e9n", "fran\u00e7ois chollet", "josef urban"], "accepted": true, "id": "1606.04442"}, "pdf": {"name": "1606.04442.pdf", "metadata": {"source": "CRF", "title": "DeepMath - Deep Sequence Models for Premise Selection", "authors": ["Alexander A. Alemi", "Fran\u00e7ois Chollet", "Geoffrey Irving"], "emails": ["alemi@google.com", "fchollet@google.com", "geoffreyi@google.com", "szegedy@google.com", "josef.urban@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Mathematics underpins all scientific disciplines. Machine learning itself is based on measurement and probability theory, linear algebra, functional analysis, and information theory. Complex mathematics is subject to computer chips, transit systems, and financial infrastructure - that is, the correctness of many of these systems - can be reduced to mathematical proofs."}, {"heading": "2 Formalization and Theorem Proving", "text": "In the last two decades, large corporations of complex mathematical knowledge have been formalized so that computers can fully understand the semantics of complex mathematical objects. The process of writing such formal and verifiable theorems, definitions, proofs and theories is known as interactive theorems (ITP).The ITP field dates back to the 1960s and the automatism system of N.G. de Bruijn [10].ITP systems include HOL (41), Mizar [14], Coq [8], and ACL2 (25).The development of ITP systems has been interwoven with the development of automated theorems."}, {"heading": "3 Premise Selection, Experimental Setting and Previous Results", "text": "This year, it has reached the point where it will be able to take the lead in creating a wide range of opportunities to travel the world."}, {"heading": "4 Motivation for the use of Deep Learning", "text": "In natural language processing, deep neural networks have been found useful for speech modeling [30], text classification [9], sentence pairing [4], dependency analysis [2], conversation modeling [40], and simple questions to answer [36]. These results have shown that deep networks extract useful representations from sequential inputs without hand-held feature engineering. Neural networks can also mimic some overarching arguments for simple algorithmic tasks [15, 42, 20]. Here, we have learned representations of mathematical statements to help with premise selection and testing. 100 101 102 104 105100 102 104 104 104 104 104 (a) Length in charts."}, {"heading": "5 Overview of our approach", "text": "We simplify the selection of the subset in pairs of relevance by predicting the likelihood that a particular axiom will be useful to prove a given assumption. This approach is based on a relatively sparse dependency graph: we ignore the problem of pruning redundant axiom propositions. Our general architecture is illustrated in Figure 3 (left): the conjecture and axiom sequences are separately embedded in real fixed-length vectors, then linked together and passed on to a third network with some fully connected layers and logistical losses. During the training period, the two embedded networks and the associated predictor path are treated as a single large neural network and trained together. As discussed in Section 3, we train our models based on premise selection data generated by a combination of different methods, including k-nearest neighbor searches based on technical similarity."}, {"heading": "5.1 Stage 1: Character-level models", "text": "First of all, we avoid special engineering by treating formulas at the character level. We use an 80-dimensional one-hot encoding of the string, including 79 unique characters that occur in the translated Mizar corpus, and a special character to mark the beginning and end of each instruction. These sequences are passed on to a network with weight distribution for variable length input. To embed, we examined the following architectures: 1. Purely recursive LSTM [19] and GRU [7] networks; 2. A pure multi-layer Convolutionary network with varying numbers of Constitutional layers (with steps) followed by a global Max Pooling reduction in time (see Figure 3 (right); 3. A recursive-Convolutionary network that uses Convolutionary layers to produce a shorter sequence processed by an LSTM. The exact architectures used are specified in the Experimental Section."}, {"heading": "5.2 Stage 2: Word-level models", "text": "Since Mizar is based on first-order set theory, definitions of names can be either explicit or implicit. To avoid the structure of implicit definitions, we embed the entire definition that defines an identifier x, and then use a property of the defined object, such as defining a function f (x) by defining f (x) = g (x). To manually encode the structure of implicit definitions, we embed the entire definition of an identifier x, and then use that definition as word-level embedding. Ideally, we would train a single network that includes statements by recursively extending and embedding the definitions of words."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Experimental Setup", "text": "For training and evaluation, we used a subset of 32,524 out of 57,917 theorems that are known to be provable by an ATP based on the correct assumptions. We separated 10% of these theorems (3,124 statements) for holdout tests and validations. This can lead to learning from future evidence: Proof for Theorem Tj written according to Theorem Ti can guide the premise selection for Ti. However, earlier K-NN experiments show a similar performance between complete 10-fold cross-validation and incremental evaluation as long as chronologically preceding formulas participate in evidence for later theorems. We provided an additional 400 statements from the 3,124 for monitoring training progress as well as for model and checkpoint selection. The final evaluation was made on the remaining 2,724 assumptions, using only guesses as axioms."}, {"heading": "6.2 Metrics", "text": "For each assumption, our models give a ranking of the possible assumptions. Our primary measurement is the number of assumptions proven by the top k assumptions, where k = 16, 32,.., 1024. This measurement can accommodate alternative proofs, but is mathematically expensive. Therefore, we additionally measure the ranking quality based on the average maximum relative rank of the test premise. Formally, the average maximum rank is isaMRR = the mean C max P-Ptest (C) rank (P, Pavail (C)) | where C is above assumptions, Pavail (C) is the set of assumptions available to C, Ptest (C) is the set of assumptions for the assumption C from the test set, and rank (P, Pavail (C) is the rank of premise P below the premises set, Pavail (C) is the premise density (C) according to the model."}, {"heading": "6.3 Network Architectures", "text": "All of our neural network models use the general architecture of Fig. 3: a classifier over the concatenated embedding of an axiom and a conjecture. The same classification architecture was used for all models: a fully connected neural network with a hidden layer of size 1024. For each model, the axiom and the conjecture embedding networks have the same architecture but not common weights. The details of the embedding networks are shown in Fig. 4. a) Training accuracy for different character level models. Recurrent models appear to be below average, while pure revolutionary models provide the best results. For each architecture, we trained three models with different random initialization seeds. Only the best runs are shown in this graph; we did not see a large discrepancy between runs on the same architecture. b) Let's test the average maximum relative order for different models. The best is a CNN at word level using definition layer 2 CNN."}, {"heading": "6.4 Network Training", "text": "The neural networks were trained using asynchronous, distributed stochastic gradient lineage using the Adam Optimizer [26] with up to 20 parallel NVIDIA K-80 GPU workers per model. We used the TensorFlow framework [1] and the Keras library [6]. We used [11] to initialize tortuous and fully connected layers and polyak averages with 0.9999 decay to produce the final weights [32]. We experimented with gradient clipping at various thresholds, which helped stabilize the training but gave worse results than untruncated models. Character plane models were trained with a maximum sequence length of 2048 characters, with word level (and definition embedding) based models trained with a maximum sequence length of 500 words."}, {"heading": "6.5 Experimental Results", "text": "Our best selection pipeline uses a level 1 convolutionary neural network model to achieve word-level embedding for the second stage. We use distance-weighted kNN [21, 23] with handmade semantic characteristics [24]. For all assumptions in our holdout set, we consider any previous statement (Lemma, definition axiom) in chronological order as a premise candidate. In the case of DeepMath, premises were sorted according to their logistical assessments. CNN testers applied to the top K of premise candidates for each of the cutoffs. (16, 32,., 1024) until evidence is found or k = 1024 fails. Table 1 reports that the number of theorems with a cutoff value of no more than k in the left column has been proven. For e-testers, we use a soft time limit of 90 seconds, a hard time limit of 1650 seconds, a limit of memory of 4 GB and even a limit of 500,000 GB."}, {"heading": "7 Conclusions", "text": "In this paper, we provide evidence that even simple neural models can compete with handcrafted features in premise selection, helping to find many new proofs, leading to real benefits in automatic theory testing. Despite these encouraging results, our models are relatively flat networks with inherent limitations on representativeness, and unable to grasp properties of mathematical statements at a high level. We believe that the theory proof is a challenging and important domain for deep learning methods, and that more complex optimization techniques and training methods will prove more useful than in less structured areas."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Globally normalized transition-based neural networks", "author": ["D. Andor", "C. Alberti", "D. Weiss", "A. Severyn", "A. Presta", "K. Ganchev", "S. Petrov", "M. Collins"], "venue": "arXiv preprint arXiv:1603.06042,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "A Compendium of Continuous Lattices in MIZAR", "author": ["G. Bancerek", "P. Rudnicki"], "venue": "J. Autom. Reasoning, 29(3-4):189\u2013224,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Sentence pair scoring: Towards unified framework for text comprehension", "author": ["P. Baudi\u0161", "J. Pichl", "T. Vysko\u010dil", "J. \u0160ediv\u00fd"], "venue": "arXiv preprint arXiv:1603.06127,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Hammering towards QED", "author": ["J.C. Blanchette", "C. Kaliszyk", "L.C. Paulson", "J. Urban"], "venue": "J. Formalized Reasoning, 9(1):101\u2013148,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Gated feedback recurrent neural networks", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1502.02367,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised sequence learning", "author": ["A.M. Dai", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, pages 3061\u20133069,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "The mathematical language AUTOMATH, its usage, and some of its extensions", "author": ["N. de Bruijn"], "venue": "Proceedings of the Symposium on Automatic Demonstration,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1968}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, pages 249\u2013256,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "The four colour theorem: Engineering of a formal proof", "author": ["G. Gonthier"], "venue": "D. Kapur, editor, Computer Mathematics, 8th Asian Symposium, ASCM 2007, Singapore, December 15-17, 2007. Revised and Invited Papers, volume 5081 of Lecture Notes in Computer Science, page 333. Springer,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "A machine-checked proof of the Odd Order Theorem", "author": ["G. Gonthier", "A. Asperti", "J. Avigad", "Y. Bertot", "C. Cohen", "F. Garillot", "S.L. Roux", "A. Mahboubi", "R. O\u2019Connor", "S.O. Biha", "I. Pasca", "L. Rideau", "A. Solovyev", "E. Tassi", "L. Th\u00e9ry"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Mizar in a nutshell", "author": ["A. Grabowski", "A. Korni\u0142owicz", "A. Naumowicz"], "venue": "J. Formalized Reasoning, 3(2):153\u2013245,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural Turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A formal proof of the Kepler conjecture", "author": ["T.C. Hales", "M. Adams", "G. Bauer", "D.T. Dang", "J. Harrison", "T.L. Hoang", "C. Kaliszyk", "V. Magron", "S. McLaughlin", "T.T. Nguyen", "T.Q. Nguyen", "T. Nipkow", "S. Obua", "J. Pleso", "J. Rute", "A. Solovyev", "A.H.T. Ta", "T.N. Tran", "D.T. Trieu", "J. Urban", "K.K. Vu", "R. Zumkeller"], "venue": "CoRR, abs/1501.02155,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "HOL Light: A tutorial introduction", "author": ["J. Harrison"], "venue": "M. K. Srivas and A. J. Camilleri, editors, FMCAD, volume 1166 of LNCS, pages 265\u2013269. Springer,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "History of interactive theorem proving", "author": ["J. Harrison", "J. Urban", "F. Wiedijk"], "venue": "J. H. Siekmann, editor, Computational Logic, volume 9 of Handbook of the History of Logic, pages 135 \u2013 214. North-Holland,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Neural gpus learn algorithms", "author": ["\u0141. Kaiser", "I. Sutskever"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Stronger automation for Flyspeck by feature weighting and strategy evolution", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. C. Blanchette and J. Urban, editors, PxTP 2013, volume 14 of EPiC Series, pages 87\u201395. EasyChair,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning-assisted automated reasoning with Flyspeck", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning, 53(2):173\u2013213,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "MizAR 40 for Mizar 40", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning, 55(3):245\u2013256,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient semantic features for automated reasoning over large theories", "author": ["C. Kaliszyk", "J. Urban", "J. Vyskocil"], "venue": "Q. Yang and M. Wooldridge, editors, Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 3084\u20133090. AAAI Press,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "seL4: formal verification of an operatingsystem kernel", "author": ["G. Klein", "J. Andronick", "K. Elphinstone", "G. Heiser", "D. Cock", "P. Derrin", "D. Elkaduwe", "K. Engelhardt", "R. Kolanski", "M. Norrish", "T. Sewell", "H. Tuch", "S. Winwood"], "venue": "Commun. ACM, 53(6):107\u2013115,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning from multiple proofs: First experiments", "author": ["D. Kuehlwein", "J. Urban"], "venue": "P. Fontaine, R. A. Schmidt, and S. Schulz, editors, PAAR-2012, volume 21 of EPiC Series, pages 82\u201394. EasyChair,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Formal verification of a realistic compiler", "author": ["X. Leroy"], "venue": "Commun. ACM, 52(7):107\u2013115,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH, volume 2, page 3,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization, 30(4):838\u2013855,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1992}, {"title": "Handbook of Automated Reasoning (in 2 volumes)", "author": ["J.A. Robinson", "A. Voronkov", "editors"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "E - A Brainiac Theorem Prover", "author": ["S. Schulz"], "venue": "AI Commun., 15(2-3):111\u2013126,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151\u2013161. Association for Computational Linguistics,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "J. Weston", "R. Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "The TPTP world - infrastructure for automated reasoning", "author": ["G. Sutcliffe"], "venue": "E. M. Clarke and A. Voronkov, editors, LPAR (Dakar), volume 6355 of LNCS, pages 1\u201312. Springer,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "MPTP 0.2: Design, implementation, and initial experiments", "author": ["J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "Theorem proving in large formal mathematics as an emerging AI field", "author": ["J. Urban", "J. Vysko\u010dil"], "venue": "M. P. Bonacina and M. E. Stickel, editors, Automated Reasoning and Mathematics: Essays in Memory of William McCune, volume 7788 of LNAI, pages 240\u2013257. Springer,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q. Le"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Such guidance is crucial: exhaustive deductive reasoning tools such as today\u2019s resolution/superposition automated theorem provers (ATPs) quickly hit combinatorial explosion, and are unusable when reasoning with a very large number of facts without careful selection [5].", "startOffset": 266, "endOffset": 269}, {"referenceID": 16, "context": "The ITP field dates back to 1960s [18] and the Automath system by N.", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "de Bruijn [10].", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "ITP systems include HOL (Light) [17], Isabelle [41], Mizar [14], Coq [8], and ACL2 [25].", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "ITP systems include HOL (Light) [17], Isabelle [41], Mizar [14], Coq [8], and ACL2 [25].", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "The development of ITP has been intertwined with the development of its cousin field of Automated Theorem Proving (ATP) [33], where proofs of conjectures are attempted fully automatically.", "startOffset": 120, "endOffset": 124}, {"referenceID": 14, "context": "Examples in mathematics include the HOL Light proof of the Kepler conjecture (Flyspeck project) [16], the Coq proofs of the Feit-Thompson theorem [13] and Four Color theorem [12], and the verification of most of the Compendium of Continuous Lattices in Mizar [3].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Examples in mathematics include the HOL Light proof of the Kepler conjecture (Flyspeck project) [16], the Coq proofs of the Feit-Thompson theorem [13] and Four Color theorem [12], and the verification of most of the Compendium of Continuous Lattices in Mizar [3].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "Examples in mathematics include the HOL Light proof of the Kepler conjecture (Flyspeck project) [16], the Coq proofs of the Feit-Thompson theorem [13] and Four Color theorem [12], and the verification of most of the Compendium of Continuous Lattices in Mizar [3].", "startOffset": 174, "endOffset": 178}, {"referenceID": 2, "context": "Examples in mathematics include the HOL Light proof of the Kepler conjecture (Flyspeck project) [16], the Coq proofs of the Feit-Thompson theorem [13] and Four Color theorem [12], and the verification of most of the Compendium of Continuous Lattices in Mizar [3].", "startOffset": 259, "endOffset": 262}, {"referenceID": 24, "context": "ITP verifications of the seL4 kernel [27] and CompCert compiler [29] show comparable progress in large scale software verification.", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "ITP verifications of the seL4 kernel [27] and CompCert compiler [29] show comparable progress in large scale software verification.", "startOffset": 64, "endOffset": 68}, {"referenceID": 35, "context": "Recently the field of Automated Reasoning in Large Theories (ARLT) [39] has developed, including AI/ATP/ITP (AITP) systems called hammers that assist ITP formalization [5].", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "Recently the field of Automated Reasoning in Large Theories (ARLT) [39] has developed, including AI/ATP/ITP (AITP) systems called hammers that assist ITP formalization [5].", "startOffset": 168, "endOffset": 171}, {"referenceID": 20, "context": "Recent evaluations have proved 40% of all Mizar and Flyspeck theorems fully automatically [22, 23].", "startOffset": 90, "endOffset": 98}, {"referenceID": 21, "context": "Recent evaluations have proved 40% of all Mizar and Flyspeck theorems fully automatically [22, 23].", "startOffset": 90, "endOffset": 98}, {"referenceID": 4, "context": "However, there is significant room for improvement: with perfect premise selection (a perfect choice of library facts) ATPs can prove at least 56% of Mizar and Flyspeck instead of today\u2019s 40% [5].", "startOffset": 192, "endOffset": 195}, {"referenceID": 30, "context": "11472 as the formal corpus and E prover [34] version 1.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "This version of MML was used for the latest AITP evaluation reported in [23].", "startOffset": 72, "endOffset": 76}, {"referenceID": 33, "context": "The formulas have been translated into the TPTP format [37] used by first-order ATPs by the MPTP system [38] (see Figure 1).", "startOffset": 55, "endOffset": 59}, {"referenceID": 34, "context": "The formulas have been translated into the TPTP format [37] used by first-order ATPs by the MPTP system [38] (see Figure 1).", "startOffset": 104, "endOffset": 108}, {"referenceID": 25, "context": "We can learn from both human proofs and ATP proofs, but previous experiments [28, 22] show that learning only from the ATP proofs is preferable to including human proofs if the set of ATP proofs is sufficiently large.", "startOffset": 77, "endOffset": 85}, {"referenceID": 20, "context": "We can learn from both human proofs and ATP proofs, but previous experiments [28, 22] show that learning only from the ATP proofs is preferable to including human proofs if the set of ATP proofs is sufficiently large.", "startOffset": 77, "endOffset": 85}, {"referenceID": 21, "context": "2%) of the 57,917 theorems an ATP proof was previously found by a combination of manual and learning-based premise selection [23], we use only these ATP proofs for training.", "startOffset": 125, "endOffset": 129}, {"referenceID": 21, "context": "The 40% success rate from [23] used a portfolio of 14 AITP methods using different learners, ATPs, and numbers of premises.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 146, "endOffset": 149}, {"referenceID": 1, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 170, "endOffset": 173}, {"referenceID": 31, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 194, "endOffset": 198}, {"referenceID": 36, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 222, "endOffset": 226}, {"referenceID": 32, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 258, "endOffset": 262}, {"referenceID": 13, "context": "Neural networks can also mimic some higher-level reasoning on simple algorithmic tasks [15, 42, 20].", "startOffset": 87, "endOffset": 99}, {"referenceID": 37, "context": "Neural networks can also mimic some higher-level reasoning on simple algorithmic tasks [15, 42, 20].", "startOffset": 87, "endOffset": 99}, {"referenceID": 18, "context": "Neural networks can also mimic some higher-level reasoning on simple algorithmic tasks [15, 42, 20].", "startOffset": 87, "endOffset": 99}, {"referenceID": 17, "context": "Pure recurrent LSTM [19] and GRU [7] networks.", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "Pure recurrent LSTM [19] and GRU [7] networks.", "startOffset": 33, "endOffset": 36}, {"referenceID": 23, "context": "The neural networks were trained using asynchronous distributed stochastic gradient descent using the Adam optimizer [26] with up to 20 parallel NVIDIA K-80 GPU workers per model.", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "We used the TensorFlow framework [1] and the Keras library [6].", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "We used the TensorFlow framework [1] and the Keras library [6].", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "We used [11] to initialize convolutional and fully connected layers and Polyak averaging with 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "9999 decay to produce the final weights [32].", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "The baseline uses distance-weighted kNN [21, 23] with handcrafted semantic features [24].", "startOffset": 40, "endOffset": 48}, {"referenceID": 21, "context": "The baseline uses distance-weighted kNN [21, 23] with handcrafted semantic features [24].", "startOffset": 40, "endOffset": 48}, {"referenceID": 22, "context": "The baseline uses distance-weighted kNN [21, 23] with handcrafted semantic features [24].", "startOffset": 84, "endOffset": 88}], "year": 2016, "abstractText": "We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the handengineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving.", "creator": "LaTeX with hyperref package"}}}