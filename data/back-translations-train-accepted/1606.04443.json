{"id": "1606.04443", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification", "abstract": "We present a general framework for classification of sparse and irregularly-sampled time series. The properties of such time series can result in substantial uncertainty about the values of the underlying temporal processes, while making the data difficult to deal with using standard classification methods that assume fixed-dimensional feature spaces. To address these challenges, we propose an uncertainty-aware classification framework based on a special computational layer we refer to as the Gaussian process adapter that can connect irregularly sampled time series data to to any black-box classifier learnable using gradient descent. We show how to scale up the required computations based on combining the structured kernel interpolation framework and the Lanczos approximation method, and how to discriminatively train the Gaussian process adapter in combination with a number of classifiers end-to-end using backpropagation.", "histories": [["v1", "Tue, 14 Jun 2016 16:31:14 GMT  (84kb,D)", "https://arxiv.org/abs/1606.04443v1", null], ["v2", "Fri, 28 Oct 2016 20:05:02 GMT  (58kb,D)", "http://arxiv.org/abs/1606.04443v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["steven cheng-xian li", "benjamin m marlin"], "accepted": true, "id": "1606.04443"}, "pdf": {"name": "1606.04443.pdf", "metadata": {"source": "CRF", "title": "A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification", "authors": ["Steven Cheng-Xian", "Li Benjamin Marlin"], "emails": ["cxl@cs.umass.edu", "marlin@cs.umass.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, we will be able to put ourselves at the top, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "2 Gaussian processes for sparse and irregularly-sampled time series", "text": "In this paper, we focus on time series classification in the presence of sparse and irregular scans. In this problem, the data D N contains independent tuples consisting of a time series Si and a label yi. Therefore, D = {(S1, y1),.., (SN, yN)}. Each time series Si is presented as a list of time points ti = [ti1,...) in which different time series are not necessarily observed at the same time (i.e. ti 6 = tj in general)., vi | Si |] >. We assume that each time series x is observed over a common time interval [0, T]. However, different time series are not necessarily observed at the same time (i.e. ti 6 = tj in general). This implies that the number of observations in different time series x is not necessary (i.e. Si | 6 = | Sj in general)."}, {"heading": "3 The GP adapter and uncertainty-aware time series classification", "text": "In this section, we describe our framework for classifying time series in sparse and irregular sampling. Our framework allows us to apply all black box classifiers that can be learned through gradient-based methods to the problem of classifying sparse and irregularly sampled time series."}, {"heading": "3.1 Classification frameworks and the Gaussian process adapter", "text": "In Section 2, we described how to represent a time series by the marginal posterior problem. (f) It induces under a Gaussian process regression model at any set of reference points x. By specifying a common set of reference points x for all time series in a data set, any time series can be converted into a common representation in the form of a multivariate Gaussian N (z | \u00b5, 270). If the z values were observed, we could simply apply a black box classifier. A classifier can generally be parameterized by a mapping function f (z; w) with w associated with a loss function. \"(f; w), associated with a loss function,\" y), where y is a label value from the production space Y."}, {"heading": "3.2 Learning with the GP adapter", "text": "In the previous section, we have shown that the UAC framework can be tested with the help of (3). In this work, we use stochastic gradients to optimize the model scalably (3) by updating the model using a single time series at a time, although it can be slightly modified for batch or mini-batch updates. From now on, we will focus on the optimization problem minw. For many classifiers, the expected loss Ez-N (\u00b5, \u03a3; \u03b8) ['(f (z; w), y)] where the results of the GP adapter will be due to a time series S = (t, v) and its label w. For many classifiers, the expected loss Ez-N (p; p (z; w), y)] cannot be analytically calculated. In such cases, we use the Monte Carlo average to approximate the expected loss."}, {"heading": "4 Fast sampling from posterior Gaussian processes", "text": "The calculation required by the GP adapter is dominated by the time it takes to take samples from the posterior marginal GP using z = \u00b5 + \u04411 / 2\u044b. In Section 2, we found that the time complexity of the exact calculation of the rear mean \u00b5 and the covariance \u03a3 O (n3 + nd) or O (n3 + n2d + nd2) is. Once we have both \u00b5 and \u03a3, we still need to calculate the square root of \u03a3, which requires additional O (d3) time to calculate accurately. In this section, we will show how to efficiently generate samples of z."}, {"heading": "4.1 Structured kernel interpolation for approximating GP posterior means", "text": "The main idea of the structured kernel interpolation (SKI) recently proposed by Wilson and Nickisch is to approach a stationary kernel matrix Ka, b defined by the approximate kernel K-a, b) below which u = [u1,.., um] > is a collection of evenly delimited inductive points where each row contains only a small number of non-zero entries. We use local cubic convolution interpolation (cubic interpolation for short periods of time) [10], as proposed in Wilson and Nickisch [25]. Each set of interpolation matrices Wa, Wb has a maximum of four non-zero entries. Wilson and Nickisch [25] showed that the kernel is locally smooth (under the safe resolution of the interpolation matrices Wa, Wb)."}, {"heading": "4.2 The Lanczos method for covariance square root-vector products", "text": "If we calculate accurately, we take O (n3 + n2d + nd2) time to accelerate the calculation of computing power, and O (d3) time to take the square root. Lanczos method has the advantage that neither the result nor the results of the Lanczos method need to be calculated explicitly. Like the conjugated gradient method, another example of the Krylov method, it only requires the calculation of the matrix vector products using the Matrix method. Lanczos method is the approximate method of the Krylov subspace method."}, {"heading": "5 End-to-end learning with the GP adapter", "text": "The most common way to train GP parameters is by maximizing the limit probability [17] log p (v | t, \u03b8) = \u2212 1 2 v > (Kt, t + \u03c3 2I) \u2212 1 v \u2212 1 2 log that the limit probability is maximized. (11) Following this criterion, the formation of the UAC frame becomes a two-step process: First, we learn GP parameters by maximizing the limit probability. We then calculate \u00b5 and \u03a3 in each time series S and the learned GP parameters. Both \u00b5 and \u03a3 are then fixed and used to train the classifier by means of (6). In this section, instead, we describe the end-to-end discriminatory training of GP parameters by means of backpropagation. As mentioned in Section 3, we train the UAC framework by jointly optimizing GP parameters and classifier parameters."}, {"heading": "6 Related work", "text": "The recently proposed mixtures of expected Gaussian cores (MEG) [13] for the classification of irregular time series are probably the closest work to us. Random representation of the characteristics of the MEG kernel takes the form of \u221a 2 / m Ez \u0445 N (\u00b5, \u03a3) [cos (w > i z + bi)], to which the algorithm described in Section 4 can be applied directly. However, by exploiting the spectral property of the Gaussian cores, the expected random property of the MEG kernel proves to be analytically calculable by \u221a 2 / m exp (\u2212 w > i-wi / 2) cos (w > i-m + bi).Using the SKI techniques, we can efficiently calculate both w > i-wi and w > i-m kernel in the same time and space complexity as the GP adapters. Furthermore, the random characteristics of the MEG kernel can be considered as random layers in the classification network without being analytically traceable."}, {"heading": "7 Experiments", "text": "In this section, we present experiments and results that examine various facets of the GP adapter framework, including the quality of the approximations and the classification performance of the framework in combination with various basic classifiers."}, {"heading": "7.1 Quality of GP sampling approximations", "text": "The key to scalable learning with the GP adapter is based on both fast and accurate approximation for drawing samples from the rear GP. To assess the approximation quality, we first create a synthetic sparse and irregularly sampled time series S by selecting random samples from an approximate Lancaster error at random points in time. However, we then compare the quadratic exponential kernel (ti, tj) = an exact exact mapping (-b (ti \u2212 tj) 2) with randomly selected hyperparameters. We then infer an exact x given reference point. Let us specify exactly our approximation of z = + 2 / 4 / 4. In this experiment, we set the output size z, i.e. d = n. We evaluate the approximation quality by evaluating the error quality by calculating the z \u2212 proximan, which shows a random vector in 1."}, {"heading": "7.2 Classification with GP adapter", "text": "In this section, we evaluate the performance of the classification of sparse and irregularly sampled time series using the UAC framework. We test the framework using the uWave data set, 4 of a collection of gesture samples categorized into eight gesture patterns. [14] The data set was divided into 3582 training instances and 896 test instances. Each time series contains 945 fully observed samples. Following the data processing process in the MEG core work [13], 10% of the observations from each time series were randomly sampled to simulate the sparse and irregular sampling scenario.In this experiment, we use the square exponential covariance function (tj) = an extrospective (TCP) for a time series. Together with the independent noise parameter 2 > 0, the GP parameters that we use with {a, b, a TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / UAC / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP / TP"}, {"heading": "8 Conclusions and future work", "text": "We have presented a general framework for classifying sparse and irregularly sampled time series and demonstrated how to extend the required calculations with a new approach to generating approximate samples. We have validated the approximation quality, computational acceleration, and utility of the proposed approach compared to existing baselines. There are many promising directions for future work, including the study of more complicated covariance functions such as spectral mixing core [24], various classifiers including the LSTM [23] encoder, and the extension of the framework to multi-dimensional time series and GPs with multi-dimensional index sets (e.g. for spatial data). Finally, the GP adapter can also be applied to other problems such as reducing dimensionality by combining it with an autoencoder."}, {"heading": "Acknowledgements", "text": "This work was supported by the National Science Foundation under grant number 1350522.4. The UWaveGestureLibraryAll dataset is available at http: / / timeseriesclassification.com."}, {"heading": "A Gradients for GP approximation", "text": "(WtKu, uW) \u2212 WtKu, uW > d) = Wx Ku, u > W > W > W > W > W > W > W (WxKu, uW > WtKu, uW > WtKu, uW > WtKu, uW > t (WxKu, uW), uW > t (WtKu, uW), uW > t (WtKu, uW), WtKu, uW > WtKu, uW (WxKu, uW), uW > t (WxKu, uW), uW > WtKu, uW (WtKu), uW > WtKu, uW (WtKu), uW (WtKu), uW, uW (WtKu)."}, {"heading": "B Cubic interpolation in backpropagation", "text": "The choice of the cubic folding interpolation proposed by Keys [10] is preferable to other interpolation methods, such as the spline interpolation in the training of GP parameters. If the spline interpolation is used to construct the SKI core K-a, b, the interpolation matrix Wa depends not only on a and u, but also on the kernel Ku, u, which depends on the GP parameters \u03b8. Consequently, the gradient \u2202 Wa / zipation must be calculated and thus leads to an enormous overhead in the reverse propagation. On the other hand, the interpolation matrix based on the cubic folding interpolation only depends on a and u, which are fixed as soon as the data are available. Therefore, both Wa and Wb are constant matrices during the entire training process in the cubic folding interpolation."}, {"heading": "C Architectures used in the experiment", "text": "The architecture of each classifier compared in Section 7.2 is described as follows: The fully connected network consists of two fully connected layers, each containing 256 units; the domed network comprises a total of five layers: the first and third layers are both one-dimensional corrugated layers with four filters of size 5; the second and fourth layers are one-dimensional max-pooling layers of size 2; the last layer is a fully connected layer of 256 units; we apply reflected linear activation to all corrugated and fully connected layers; each classifier assumes d = 254 input features generated by the GP adapter."}], "references": [{"title": "Solution of the matrix equation AX +XB = C", "author": ["Richard H. Bartels", "GW Stewart"], "venue": "Communications of the ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1972}, {"title": "A Schur method for the square root of a matrix", "author": ["\u00c5ke Bj\u00f6rck", "Sven Hammarling"], "venue": "Linear algebra and its applications,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1983}, {"title": "Preconditioned krylov subspace methods for sampling multivariate gaussian distributions", "author": ["Edmond Chow", "Yousef Saad"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Population time series: process variability, observation errors, missing values, lags, and hidden states", "author": ["J.S. Clark", "O.N. Bj\u00f8rnstad"], "venue": "Ecology, 85(11):3140\u20133150,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Retooling the method of block conjugate gradients", "author": ["Augustin A Dubrulle"], "venue": "Electronic Transactions on Numerical Analysis,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "A block conjugate gradient method applied to linear systems with multiple right-hand sides", "author": ["YT Feng", "DRJ Owen", "D Peri\u0107"], "venue": "Computer methods in applied mechanics and engineering,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "The block Lanczos method for computing eigenvalues", "author": ["Gene Howard Golub", "Richard Underwood"], "venue": "Mathematical software,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1977}, {"title": "A restarted Lanczos approximation to functions of a symmetric matrix", "author": ["M Ili\u0107", "Ian W Turner", "Daniel P Simpson"], "venue": "IMA journal of numerical analysis, page drp003,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Cubic convolution interpolation for digital image processing", "author": ["Robert G Keys"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1981}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "Proceedings of the 2nd International Conference on Learning Representations (ICLR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Yann LeCun", "Fu Jie Huang", "Leon Bottou"], "venue": "In Proceedings of Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Classification of sparse and irregularly sampled time series with mixtures of expected Gaussian kernels and random features", "author": ["Steven Cheng-Xian Li", "Benjmain M. Marlin"], "venue": "In 31st Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "uwave: Accelerometer-based personalized gesture recognition and its applications", "author": ["Jiayang Liu", "Lin Zhong", "Jehan Wickramasuriya", "Venu Vasudevan"], "venue": "Pervasive and Mobile Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Unsupervised pattern discovery in electronic health care data using probabilistic clustering models", "author": ["Benjamin M. Marlin", "David C. Kale", "Robinder G. Khemani", "Randall C. Wetzel"], "venue": "In Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "The symmetric eigenvalue problem, volume", "author": ["Beresford N Parlett"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1980}, {"title": "Gaussian processes for machine learning", "author": ["Carl Edward Rasmussen"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "The lomb-scargle periodogram in biological rhythm research: analysis of incomplete and unequally spaced time-series", "author": ["T. Ruf"], "venue": "Biological Rhythm Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "On the rates of convergence of the Lanczos and the block-Lanczos methods", "author": ["Yousef Saad"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1980}, {"title": "Iterative methods for sparse linear systems", "author": ["Yousef Saad"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Studies in astronomical time series analysis. ii-statistical aspects of spectral analysis of unevenly spaced data", "author": ["Jeffrey D Scargle"], "venue": "The Astrophysical Journal,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1982}, {"title": "Spectrum: Spectral analysis of unevenly spaced paleoclimatic time series", "author": ["M. Schulz", "K. Stattegger"], "venue": "Computers & Geosciences,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Gaussian process kernels for pattern discovery and extrapolation", "author": ["Andrew Gordon Wilson", "Ryan Prescott Adams"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}], "referenceMentions": [{"referenceID": 20, "context": "Such time series data are studied in various areas including climate science [22], ecology [4], biology [18], medicine [15] and astronomy [21].", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "Such time series data are studied in various areas including climate science [22], ecology [4], biology [18], medicine [15] and astronomy [21].", "startOffset": 91, "endOffset": 94}, {"referenceID": 16, "context": "Such time series data are studied in various areas including climate science [22], ecology [4], biology [18], medicine [15] and astronomy [21].", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "Such time series data are studied in various areas including climate science [22], ecology [4], biology [18], medicine [15] and astronomy [21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "Such time series data are studied in various areas including climate science [22], ecology [4], biology [18], medicine [15] and astronomy [21].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "Recently, Li and Marlin [13] introduced the mixture of expected Gaussian kernels (MEG) framework, an uncertainty-aware kernel for classifying sparse and irregularly sampled time series.", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "On the other hand, various deep learning models including convolutional neural networks [12] have been successfully applied to fields such as computer vision and natural language processing, and have been shown to achieve state-of-the-art results on various tasks.", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "To address this problem, we show how to speed up the key computation of sampling from a GP posterior based on combining the structured kernel interpolation (SKI) framework that was recently proposed by Wilson and Nickisch [25] with Lanczos methods for approximating matrix functions [3].", "startOffset": 283, "endOffset": 286}, {"referenceID": 11, "context": "To address these challenges, we build on ideas from the MEG kernel [13] by using GP regression [17] to provide an uncertainty-aware representation of sparse and irregularly sampled time series.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "To address these challenges, we build on ideas from the MEG kernel [13] by using GP regression [17] to provide an uncertainty-aware representation of sparse and irregularly sampled time series.", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "To achieve this, we reparameterize the Gaussian random variable using the identity z = \u03bc + R\u03be where \u03be \u223c N (0, I) and R satisfies \u03a3 = RR> [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 8, "context": "We use local cubic convolution interpolation (cubic interpolation for short) [10] as suggested in Wilson and Nickisch [25].", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "In fact, we can show that K\u0303a,b asymptotically converges to Ka,b as m increases by following the derivation in Keys [10].", "startOffset": 116, "endOffset": 120}, {"referenceID": 18, "context": "One can show that the tridiagonal matrix H defined in Algorithm 1 satisfies D>\u03a3D = H [20].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "Also, we have D>\u03a31D \u2248 (D>\u03a3D)/2 since the eigenvalues of H approximate the extremal eigenvalues of \u03a3 [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 7, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Alternatively one can show that the Lanczos approximation converges superlinearly [16].", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "As H is now a k\u00d7 k matrix, we can use any standard method to compute its square root in O(k) time [2], which is considered O(1) when k is chosen to be a small constant.", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "Since all of the samples are associated with the same covariance matrix \u03a3, we can use the block Lanczos process [8], an extension to the single-vector Lanczos method presented in Algorithm 1, to simultaneously approximate \u03a31\u039e for all S random", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "Similarly, during the block Lanczos process, we use the block conjugate gradient method [6, 5] to simultaneously solve the linear equation (WtKu,uW t + \u03c3 2I)\u22121\u03b1 for multiple \u03b1.", "startOffset": 88, "endOffset": 94}, {"referenceID": 4, "context": "Similarly, during the block Lanczos process, we use the block conjugate gradient method [6, 5] to simultaneously solve the linear equation (WtKu,uW t + \u03c3 2I)\u22121\u03b1 for multiple \u03b1.", "startOffset": 88, "endOffset": 94}, {"referenceID": 15, "context": "The most common way to train GP parameters is through maximizing the marginal likelihood [17] log p(v|t,\u03b8) = \u2212 2 v> ( Kt,t + \u03c3 I )\u22121 v \u2212 1 2 log \u2223\u2223Kt,t + \u03c32I\u2223\u2223\u2212 n 2 log 2\u03c0.", "startOffset": 89, "endOffset": 93}, {"referenceID": 0, "context": "We can compute the gradient \u2202H/2 by solving the Sylvester equation using the Bartels-Stewart algorithm [1] in O(k) time for a k \u00d7 k matrix H, which is considered O(1) for a small constant k.", "startOffset": 103, "endOffset": 106}, {"referenceID": 11, "context": "The recently proposed mixtures of expected Gaussian kernels (MEG) [13] for classification of irregular time series is probably the closest work to ours.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "We test the framework on the uWave data set,4 a collection of gesture samples categorized into eight gesture patterns [14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "Following the data preparation procedure in the MEG kernel work [13], we randomly sample 10% of the observations from each time series to simulate the sparse and irregular sampling scenario.", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "There are many promising directions for future work including investigating more complicated covariance functions like the spectral mixture kernel [24], different classifiers including the encoder LSTM [23], and extending the framework to multi-dimensional time series and GPs with multidimensional index sets (e.", "startOffset": 147, "endOffset": 151}, {"referenceID": 21, "context": "There are many promising directions for future work including investigating more complicated covariance functions like the spectral mixture kernel [24], different classifiers including the encoder LSTM [23], and extending the framework to multi-dimensional time series and GPs with multidimensional index sets (e.", "startOffset": 202, "endOffset": 206}], "year": 2016, "abstractText": "We present a general framework for classification of sparse and irregularly-sampled time series. The properties of such time series can result in substantial uncertainty about the values of the underlying temporal processes, while making the data difficult to deal with using standard classification methods that assume fixeddimensional feature spaces. To address these challenges, we propose an uncertaintyaware classification framework based on a special computational layer we refer to as the Gaussian process adapter that can connect irregularly sampled time series data to any black-box classifier learnable using gradient descent. We show how to scale up the required computations based on combining the structured kernel interpolation framework and the Lanczos approximation method, and how to discriminatively train the Gaussian process adapter in combination with a number of classifiers end-to-end using backpropagation.", "creator": "LaTeX with hyperref package"}}}