{"id": "1606.04474", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Learning to learn by gradient descent by gradient descent", "abstract": "The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.", "histories": [["v1", "Tue, 14 Jun 2016 17:49:32 GMT  (3040kb,D)", "http://arxiv.org/abs/1606.04474v1", null], ["v2", "Wed, 30 Nov 2016 16:45:45 GMT  (3270kb,D)", "http://arxiv.org/abs/1606.04474v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["marcin andrychowicz", "misha denil", "sergio gomez colmenarejo", "matthew w hoffman", "david pfau", "tom schaul", "nando de freitas"], "accepted": true, "id": "1606.04474"}, "pdf": {"name": "1606.04474.pdf", "metadata": {"source": "CRF", "title": "Learning to learn by gradient descent by gradient descent", "authors": ["Marcin Andrychowicz", "Misha Denil", "Sergio Gomez"], "emails": ["marcin.andrychowicz@gmail.com", "mdenil@google.com", "sergomez@google.com", "mwhoffman@google.com", "pfau@google.com", "schaul@google.com", "nandodefreitas@google.com"], "sections": [{"heading": "1 Introduction", "text": "The target in this case is to find the minimizer method that leads to a sequence of update / update / / update / / update / / update / / update / / / update / / / update / / update / / / update / / update / / / update / / update / / / update / / / update / / / update / / / / update / / / update / / / update / / / update / / / / update / / / update / / / / update / / / / update / / / update / / / update / / / update / / / update / / update / / update / / / update / / / update / / update / / / update / / / update / / / update / / / update / / / / update / / / / update / / / / update / / / / update / / / / / update / / / / update / / / / / update / / / / update / / / / update / / / / update / / / / update / / / / / update / / / / update / / / / update / / / / update / / / / update / / / / update / / / / update / / / update / / / / update / / / / update / / / / update / / / update / / / / update / / / / update / / / / update / / / update / update / / / / update / / / / / / update / update / / / / update / / / / / / / update / / update / / / / update / / / / / update / / / / update / / / update / / / / update / / / / / / update / / / / / / / update / update / / / update / / / / update / / / / / / update / / / update / / / / update / / / / / update / / / / / / update / / / update / / / / / / update / / / / update / / / / / update / / / / / / / / / / update / / update / / / update / / update / / / update / / / / / / / / / / update / / / / / / update / / / / / / / update / / / / / update / / / / update / / / / / / update / / / / update / / / / / / update / / /"}, {"heading": "1.1 Transfer learning and generalization", "text": "The aim of this work is to develop a method of constructing a learning algorithm that behaves well in relation to a particular class of optimization problems. If we consider algorithm design as a learning problem, we can specify the class of problems we are interested in using example problems. This is in contrast to the conventional approach of analytically characterizing properties of interesting problems and using these analytical findings to design learning algorithms.It is interesting to consider the importance of generalization in this framework.In ordinary statistical learning, we have a special function of interest, whose behavior is limited by a data set of sample function assessments.When selecting a model, we determine a number of inductive distortions about how the function of interest in points should behave behave that we have not observed, and the generalization corresponds to the ability to make predictions about the behavior of the target function at novel points.In our setting, the examples themselves are problem cases, which means that the perspective we commonly use between problems with the ability to translate is a common thing."}, {"heading": "1.2 Related work and a brief history", "text": "The idea of using learning for learning or meta-learning to acquire knowledge or inductive biases has a long history [Thrun and Pratt, 1998]. More recently, Lake et al. [2016] argued vigorously for its importance as a building block in artificial intelligence. Generally, however, these ideas include learning that takes place on two different time scales: fast learning within tasks and more gradual, global learning across many different tasks. In some of the earliest work on meta-learning, Naik and Mammone [1992] used the results of previous training runs to modify the downward direction of backward propagation; their updating strategy, however, is somewhat more ad-hoc and not directly learnable. Santoro et al al's work [2016] adopts a similar approach to our multi-task learning as a generalization, but they form a base learner directly, rather than educate a higher-value educational algorithm. [Enabling 2001 is a lineage related to the 1999 problem with Cotton and later Cotton]."}, {"heading": "2 Learning to learn with recurrent neural networks", "text": "In this thesis, we look directly at the parameterization of the optimizer. As a result, we can then ask the question: What does it mean for an optimizer to be good? In view of a distribution of functions f, we write the expected loss asL (\u03c6, f) = Ef [f (f, \u03c6)))]. (2) As already mentioned, we will take the update steps gt to be the output of a recursive neural network m, parameterized by \u03c6, whose state we explicitly call ht. (2) While the objective function in (2) depends only on the final parameter value, the optimization is gt to be the output of a recursive neural network m, parameterized by \u03c6, whose state we do not explicitly call ht. (4) The objective function in (2) depends only on the final parameter value, for the training of the optimizer it will be convenient to have a target that depends on the entire orbit of the optimization (T = some horizon)."}, {"heading": "2.1 Coordinatewise LSTM optimizer", "text": "One challenge in applying RNNs in our environment is that we want to be able to optimize at least tens of thousands of parameters. Optimizing on this scale with a fully connected RNN is not feasible, as it would require a huge hidden state and an enormous number of parameters. To avoid this difficulty, we will use an optimizer based on the parameters of the objective function, similar to other common update rules such as RMSprop and ADAM. This coordinated network architecture allows us to use a very small network that only looks at a single coordinate to define the optimizer and distribute optimizer parameters across different optimization parameters."}, {"heading": "2.2 Information sharing between coordinates", "text": "In the previous section, we looked at a coordinated architecture analogous to a learned version of RMSprop or ADAM. Although diagonal methods are very effective in practice, we can also consider learning more sophisticated optimizers that translate the correlations between the coordinates into action. To this end, we are introducing a mechanism that allows different LSTMs to communicate with each other. Global average cells The simplest solution is to designate a subset of cells in each LSTM layer for communication. These cells function like normal LSTM cells, but their outgoing activations are averaged across all coordinates at each step. These global average cells (GACs) are sufficient to allow networks to implement L2 gradient clipping [Bengio et al, 2013] provided that each LSTM can calculate the square of the gradient. This architecture is called the LSTM + GAC optimizer."}, {"heading": "3 Experiments", "text": "In all experiments, the trained optimizers use two-layer LSTMs with 20 hidden units in each shift. Each optimizer is trained by minimizing Equation 3 using the shortened BPTT described in Section 2. Minimizing is done with ADAM at a random learning rate. In training the optimizer, we use an early stop to avoid overadjustment of the optimizer. After each epoch (a specified number of learning steps), we freeze the optimization parameters and evaluate its performance. We select the best optimizer (according to the final validation loss) and report on its average performance based on a number of freshly sampled test problems. We compare our trained optimizers with standard optimizers used in deep learning: SGD, RMSprop, ADAM, Adadelta, Adagrad, and Rprop. For each of these optimizers and each problem, we try the following learning rates: 10 \u00b7 6 \u2212 6, 22 \u2212 229 \u2212 6."}, {"heading": "3.1 Quadratic functions", "text": "In this experiment, we look at the formation of an optimizer based on a simple class of synthetic 10-dimensional square functions. In particular, we look at the minimization of the functions of Formf (\u03b8) = \u0442 W\u03b8 \u2212 y \u0432 22 for different 10x10 matrices W and 10-dimensional vectors y whose elements are drawn from an IID-Gauss distribution. Optimizers were trained by optimizing random functions from this family and tested on newly tested functions from the same distribution. Each function was optimized for 100 steps and the trained optimizers were unrolled for 20 steps. We did not use pre-processing, nor post-processing. For LSTM + GAC and NTM-BFGS models, we designate 5 of the 20 units in each layer as global average cells. NTM-BFGS uses a read head and 3 writing heads. Learning curves for various optimizers averaged over many functions are shown in the left graph of Figure 4. Each curve corresponds to the average performance of an optimization algorithm on many test functions."}, {"heading": "3.2 Training a small neural network on MNIST", "text": "In this experiment, we will test whether trainable optimizers can learn to optimize a small neural network based on MNIST, and also explore how the trained optimizers generalize to functions on which they were trained. To this end, we will train the optimizer to optimize a base network, and explore a number of changes to the network architecture and training process in the test phase. In this setting, the target function f (include) is the cross-entropy of a small MLP with parameters. The values of f and the gradients are estimated with random minibatches of 128 examples. The base network is an MLP with a hidden layer of 20 units that exhibits variability between different runs."}, {"heading": "3.3 Training a convolutional network on CIFAR-10", "text": "Next, we test the performance of the trained neural optimizers to optimize the classification performance for the CIFAR 10 dataset [Krizhevsky, 2009]. In these experiments, we used a model with both revolutionary and advanced layers. Specifically, the model used in these experiments comprises three revolutionary layers with maximum pooling followed by a fully connected layer with 32 hidden units; all nonlinearities were ReLU activations with batch normalization. The coordinative network decomposition introduced in Section 2.1 - and used in the previous experiment - uses a single LSTM architecture with common weights but separate hidden states for each optimizer parameter. We found that this decomposition is not sufficient for the model architecture introduced in this section due to the differences between fully connected and revolutionary layers. Instead, we modify the optimizer by introducing two LSTMs: one proposes updating parameters for the other and completely updating the layer updates."}, {"heading": "3.4 Neural Art", "text": "The recent work on artistic style transfer using Convolutionary Networks or Neural Art [Gatys et al., 2015] provides a natural test bed for our method, as each pair of content and style images leads to a different optimization problem. Each problem of Neural Art begins with a content image, c and a style image, s, and is aligned by f (\u03b8) = \u03b1Lcontent (c, \u03b8) + \u03b2Lstyle (s, \u03b8) + \u03b3Lreg (\u03b8) The minimizer of f is the style image. The first two terms attempt to reconcile the content and style of the styled image with that of their first argument, and the third term is a regulator that promotes the smoothness of the styled image. Details can be found in [Gatys et al., 2015]. We train optimizers by using only 1 style and 1800 content optimizers taken from ImageNet [Deng et al., 2009]."}, {"heading": "4 Visualizations", "text": "In this section, we will try to take a look at the decisions of the LSTM optimizer trained in the neural art task. Update history We select a single optimization parameter (a color channel of one pixel in the stylized image) and track the updates proposed by the LSTM optimizer to this coordinate over a single optimization path. We will also record the updates that would have been proposed by both SGD and ADAM if they had followed the same iteration path. Figure 9 shows the course of updates for two different optimization parameters. The diagrams show that the trained optimizer performs larger updates than SGD and ADAM. It is also apparent that it uses some kind of dynamics, but the updates are louder than those proposed by ADAM, which can be interpreted as a shorter time span."}, {"heading": "5 Conclusion", "text": "We have demonstrated how to view the design of optimization algorithms as a learning problem that allows us to train optimizers who specialize in specific classes of functions. Our experiments have confirmed that learned neural optimizers are favorable compared to modern optimization methods used in deep learning. We experienced a remarkable degree of transfer, for example, the LSTM optimizer was trained on 12,288 neural parameter tasks, was able to generalize tasks with 49,152 parameters, different styles, and different content images at the same time. Similar impressive results were observed when transferring to different architectures in the MNIST task. Results of the CIFAR image caption task show that the LSTM optimizers outperform hand-made optimizers when transferring to datasets originating from the same data distribution. In future work, we plan to further study the design of the NTM-FGS optimizer."}, {"heading": "A Gradient preprocessing", "text": "One potential challenge in training optimizers is that different input coordinates (i.e. the gradients with different optimization parameters) can have very different orders of magnitude. Indeed, this is the case if the optimizer is a neural network and different parameters correspond to the weights at different levels. This can make the formation of an optimizer more difficult, since neural networks naturally ignore small deviations in the input signals and focus on larger input values.To this end, we suggest pre-processing the optimizer inputs. A solution would be to use the optimizer (log) as input, with the gradient being in the current time span. This has a problem that the protocol (log) for the inputs of the optimizer differs from each other. Therefore, we use the following preprocessing formula k \u2192 (log), sgn (log) p \u2212 p (\u2212 1 ep, \u00b2)."}, {"heading": "B NTM-BFGS optimizer", "text": "In this section we describe the construction of NFS optimization in detail."}, {"heading": "C Gradient Visualizations", "text": "The illustrations are shown in Section 4.Step 1 \u2212 10010Step 2 Step 3 Step 4Step 5 \u2212 10010Step 6 Step 7 Step 8Step 9 \u2212 10010Step 10 Step 11 Step 12Step 13 \u2212 10010Step 14 Step 15 Step 16Step 17 \u2212 10010Step 18 Step 19 Step 20Step 21 \u2212 10010Step 22 Step 23 Step 24Step 25 \u2212 10010Step 26 1024 1024 10Step 1024 1024 10Step 1024 1024 1024 10Step 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1024 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1024 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1024 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1024 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1024 1026 1026 1026 1026 1026 1026 1024 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1026 1024 1026 1026 1026 1026"}], "references": [{"title": "Optimization with sparsity-inducing penalties", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bach et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2012}, {"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "In International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "URL https://www.flickr.com/photos/fbobolas/ 3822222947", "author": ["F. Bobolas"], "venue": "brain-neurons,", "citeRegEx": "Bobolas.,? \\Q2009\\E", "shortCiteRegEx": "Bobolas.", "year": 2009}, {"title": "Fixed-weight networks can learn", "author": ["N.E. Cotter", "P.R. Conwell"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Cotter and Conwell.,? \\Q1990\\E", "shortCiteRegEx": "Cotter and Conwell.", "year": 1990}, {"title": "Learning step size controllers for robust neural network training", "author": ["C. Daniel", "J. Taylor", "S. Nowozin"], "venue": "In Association for the Advancement of Artificial Intelligence,", "citeRegEx": "Daniel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A signal processing framework based on dynamic neural networks with application to problems in adaptation, filtering, and classification", "author": ["L.A. Feldkamp", "G.V. Puskorius"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Feldkamp and Puskorius.,? \\Q1998\\E", "shortCiteRegEx": "Feldkamp and Puskorius.", "year": 1998}, {"title": "A neural algorithm of artistic style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "arXiv Report 1508.06576,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning to learn using gradient descent", "author": ["S. Hochreiter", "A.S. Younger", "P.R. Conwell"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Building machines that learn and think like people", "author": ["B.M. Lake", "T.D. Ullman", "J.B. Tenenbaum", "S.J. Gershman"], "venue": "arXiv Report", "citeRegEx": "Lake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "URL https://www.flickr.com/photos/taylortotz101/ 6280077898", "author": ["T. Maley"], "venue": "Creative Commons Attribution 2.0 Generic", "citeRegEx": "Maley.,? \\Q2011\\E", "shortCiteRegEx": "Maley.", "year": 2011}, {"title": "Optimizing neural networks with Kronecker-factored approximate curvature", "author": ["J. Martens", "R. Grosse"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Martens and Grosse.,? \\Q2015\\E", "shortCiteRegEx": "Martens and Grosse.", "year": 2015}, {"title": "Meta-neural networks that learn by learning", "author": ["D.K. Naik", "R. Mammone"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Naik and Mammone.,? \\Q1992\\E", "shortCiteRegEx": "Naik and Mammone.", "year": 1992}, {"title": "Integer and combinatorial optimization", "author": ["G.L. Nemhauser", "L.A. Wolsey"], "venue": null, "citeRegEx": "Nemhauser and Wolsey.,? \\Q1988\\E", "shortCiteRegEx": "Nemhauser and Wolsey.", "year": 1988}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Y. Nesterov"], "venue": "In Soviet Mathematics Doklady,", "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Adaptive behavior with fixed weights in rnn: an overview", "author": ["D.V. Prokhorov", "L.A. Feldkamp", "I.Y. Tyukin"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Prokhorov et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Prokhorov et al\\.", "year": 2002}, {"title": "A direct adaptive method for faster backpropagation learning: The RPROP algorithm", "author": ["M. Riedmiller", "H. Braun"], "venue": "In International Conference on Neural Networks,", "citeRegEx": "Riedmiller and Braun.,? \\Q1993\\E", "shortCiteRegEx": "Riedmiller and Braun.", "year": 1993}, {"title": "Evolution and design of distributed learning rules", "author": ["T.P. Runarsson", "M.T. Jonsson"], "venue": "In IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks,", "citeRegEx": "Runarsson and Jonsson.,? \\Q2000\\E", "shortCiteRegEx": "Runarsson and Jonsson.", "year": 2000}, {"title": "Meta-learning with memoryaugmented neural networks", "author": ["A. Santoro", "S. Bartunov", "M. Botvinick", "D. Wierstra", "T. Lillicrap"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks", "author": ["J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1992}, {"title": "A neural network that embeds its own meta-levels", "author": ["J. Schmidhuber"], "venue": "In International Conference on Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q1993\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1993}, {"title": "Local gain adaptation in stochastic gradient descent", "author": ["N.N. Schraudolph"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Schraudolph.,? \\Q1999\\E", "shortCiteRegEx": "Schraudolph.", "year": 1999}, {"title": "Adapting bias by gradient descent: An incremental version of delta-bar-delta", "author": ["R.S. Sutton"], "venue": "In Association for the Advancement of Artificial Intelligence,", "citeRegEx": "Sutton.,? \\Q1992\\E", "shortCiteRegEx": "Sutton.", "year": 1992}, {"title": "Learning to learn", "author": ["S. Thrun", "L. Pratt"], "venue": "Springer Science & Business Media,", "citeRegEx": "Thrun and Pratt.,? \\Q1998\\E", "shortCiteRegEx": "Thrun and Pratt.", "year": 1998}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "An incremental gradient (-projection) method with momentum term and adaptive stepsize rule", "author": ["P. Tseng"], "venue": "Journal on Optimization,", "citeRegEx": "Tseng.,? \\Q1998\\E", "shortCiteRegEx": "Tseng.", "year": 1998}, {"title": "No free lunch theorems for optimization", "author": ["D.H. Wolpert", "W.G. Macready"], "venue": "Transactions on Evolutionary Computation,", "citeRegEx": "Wolpert and Macready.,? \\Q1997\\E", "shortCiteRegEx": "Wolpert and Macready.", "year": 1997}, {"title": "Fixed-weight on-line learning", "author": ["A.S. Younger", "P.R. Conwell", "N.E. Cotter"], "venue": "Transactions on Neural Networks,", "citeRegEx": "Younger et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Younger et al\\.", "year": 1999}, {"title": "Meta-learning with backpropagation", "author": ["A.S. Younger", "S. Hochreiter", "P.R. Conwell"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Younger et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Younger et al\\.", "year": 2001}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv Report 1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 20, "context": "These include momentum [Nesterov, 1983, Tseng, 1998], Rprop [Riedmiller and Braun, 1993], Adagrad [Duchi et al.", "startOffset": 60, "endOffset": 88}, {"referenceID": 6, "context": "These include momentum [Nesterov, 1983, Tseng, 1998], Rprop [Riedmiller and Braun, 1993], Adagrad [Duchi et al., 2011], Adadelta [Zeiler, 2012], RMSprop [Tieleman and Hinton, 2012], and ADAM [Kingma and Ba, 2015].", "startOffset": 98, "endOffset": 118}, {"referenceID": 33, "context": ", 2011], Adadelta [Zeiler, 2012], RMSprop [Tieleman and Hinton, 2012], and ADAM [Kingma and Ba, 2015].", "startOffset": 18, "endOffset": 32}, {"referenceID": 28, "context": ", 2011], Adadelta [Zeiler, 2012], RMSprop [Tieleman and Hinton, 2012], and ADAM [Kingma and Ba, 2015].", "startOffset": 42, "endOffset": 69}, {"referenceID": 11, "context": ", 2011], Adadelta [Zeiler, 2012], RMSprop [Tieleman and Hinton, 2012], and ADAM [Kingma and Ba, 2015].", "startOffset": 80, "endOffset": 101}, {"referenceID": 15, "context": "problem is known [Martens and Grosse, 2015].", "startOffset": 17, "endOffset": 43}, {"referenceID": 17, "context": "This is even more the case for combinatorial optimization for which relaxations are often the norm [Nemhauser and Wolsey, 1988].", "startOffset": 99, "endOffset": 127}, {"referenceID": 30, "context": "Moreover the No Free Lunch Theorems for Optimization [Wolpert and Macready, 1997] show that in the setting of combinatorial optimization, no algorithm is able to do better than a random strategy in expectation.", "startOffset": 53, "endOffset": 81}, {"referenceID": 27, "context": "2 Related work and a brief history The idea of using learning to learn or meta-learning to acquire knowledge or inductive biases has a long history [Thrun and Pratt, 1998].", "startOffset": 148, "endOffset": 171}, {"referenceID": 13, "context": "More recently, Lake et al. [2016] have argued forcefully for its importance as a building block in artificial intelligence.", "startOffset": 15, "endOffset": 34}, {"referenceID": 12, "context": "In some of the earliest work on meta-learning, Naik and Mammone [1992] use the results from previous training runs to modify the descent direction of backpropagation; however, their update strategy is somewhat more ad-hoc and not directly learned.", "startOffset": 47, "endOffset": 71}, {"referenceID": 12, "context": "In some of the earliest work on meta-learning, Naik and Mammone [1992] use the results from previous training runs to modify the descent direction of backpropagation; however, their update strategy is somewhat more ad-hoc and not directly learned. The work of Santoro et al. [2016] takes an approach similar to ours in that multi-task learning is cast as generalization, however they directly train a base learner rather than a higher-level training algorithm.", "startOffset": 47, "endOffset": 282}, {"referenceID": 3, "context": "More closely related is the line of work that began with Cotter and Conwell [1990] and later Younger et al.", "startOffset": 57, "endOffset": 83}, {"referenceID": 3, "context": "More closely related is the line of work that began with Cotter and Conwell [1990] and later Younger et al. [1999] who showed that due to their hidden state, fixed-weight recurrent neural networks can exhibit dynamic behavior without need to modify their network weights.", "startOffset": 57, "endOffset": 115}, {"referenceID": 3, "context": "More closely related is the line of work that began with Cotter and Conwell [1990] and later Younger et al. [1999] who showed that due to their hidden state, fixed-weight recurrent neural networks can exhibit dynamic behavior without need to modify their network weights. This work was built on by [Younger et al., 2001, Hochreiter et al., 2001] wherein a higher-level network act as a gradient descent procedure, with both levels trained during learning. Earlier work of Runarsson and Jonsson [2000] trains similar feed-forward meta-learning rules using evolutionary strategies.", "startOffset": 57, "endOffset": 501}, {"referenceID": 3, "context": "More closely related is the line of work that began with Cotter and Conwell [1990] and later Younger et al. [1999] who showed that due to their hidden state, fixed-weight recurrent neural networks can exhibit dynamic behavior without need to modify their network weights. This work was built on by [Younger et al., 2001, Hochreiter et al., 2001] wherein a higher-level network act as a gradient descent procedure, with both levels trained during learning. Earlier work of Runarsson and Jonsson [2000] trains similar feed-forward meta-learning rules using evolutionary strategies. Alternatively Schmidhuber [1992, 1993] considers networks that are able to modify their own behavior and act as an alternative to recurrent networks in meta-learning. Note, however that these earlier works do not directly address the transfer of a learned training procedure to novel problem instances and instead focus on adaptivity in the online setting. Similar work has also been attacked in a filtering context [Feldkamp and Puskorius, 1998, Prokhorov et al., 2002], a line of work that is directly related to simple multi-timescale optimizers [Sutton, 1992, Schraudolph, 1999]. Finally, Daniel et al. [2016] considers using reinforcement learning to train a controller for selecting step-sizes, however this work is much more constrained than ours and still requires hand-tuned features.", "startOffset": 57, "endOffset": 1194}, {"referenceID": 9, "context": "We implement the update rule for each coordinate using a two-layer Long Short Term Memory (LSTM) network [Hochreiter and Schmidhuber, 1997].", "startOffset": 105, "endOffset": 139}, {"referenceID": 1, "context": "These global averaging cells (GACs) are sufficient to allow the networks to implement L2 gradient clipping [Bengio et al., 2013] assuming that each LSTM can compute the square of the gradient.", "startOffset": 107, "endOffset": 128}, {"referenceID": 12, "context": "3 Training a convolutional network on CIFAR-10 Next we test the performance of the trained neural optimizers on optimizing classification performance for the CIFAR-10 dataset [Krizhevsky, 2009].", "startOffset": 175, "endOffset": 193}, {"referenceID": 8, "context": "4 Neural Art The recent work on artistic style transfer using convolutional networks, or Neural Art [Gatys et al., 2015], gives a natural testbed for our method, since each content and style image pair gives rise to a different optimization problem.", "startOffset": 100, "endOffset": 120}, {"referenceID": 8, "context": "Details can be found in [Gatys et al., 2015].", "startOffset": 24, "endOffset": 44}, {"referenceID": 5, "context": "We train optimizers using only 1 style and 1800 content images taken from ImageNet [Deng et al., 2009].", "startOffset": 83, "endOffset": 102}], "year": 2016, "abstractText": "The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.", "creator": "LaTeX with hyperref package"}}}