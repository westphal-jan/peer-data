{"id": "1606.04582", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Query-Reduction Networks for Question Answering", "abstract": "We present Query-Regression Network (QRN), a variant of Recurrent Neural Network (RNN) that is suitable for end-to-end machine comprehension. While previous work largely relied on external memory and attention mechanism, QRN is a single recurrent unit with internal memory. Unlike most RNN-based models, QRN is able to effectively handle long-term dependencies and is highly parallelizable. In our experiments we show that QRN outperforms the state of the art in end-to-end bAbI QA tasks.", "histories": [["v1", "Tue, 14 Jun 2016 21:54:46 GMT  (111kb,D)", "http://arxiv.org/abs/1606.04582v1", "9 pages"], ["v2", "Wed, 6 Jul 2016 21:54:45 GMT  (106kb,D)", "http://arxiv.org/abs/1606.04582v2", "9 pages"], ["v3", "Wed, 16 Nov 2016 10:07:22 GMT  (514kb,D)", "http://arxiv.org/abs/1606.04582v3", "Title of the paper has changed from \"Query-Regression Networks for Machine Comprehension\""], ["v4", "Fri, 9 Dec 2016 00:05:06 GMT  (514kb,D)", "http://arxiv.org/abs/1606.04582v4", "Title of the paper has changed from \"Query-Regression Networks for Machine Comprehension\""], ["v5", "Tue, 7 Feb 2017 22:04:54 GMT  (517kb,D)", "http://arxiv.org/abs/1606.04582v5", "Published as a conference paper at ICLR 2017. Title of the paper has changed from \"Query-Regression Networks for Machine Comprehension\""], ["v6", "Fri, 24 Feb 2017 19:59:01 GMT  (517kb,D)", "http://arxiv.org/abs/1606.04582v6", "Published as a conference paper at ICLR 2017. Title of the paper has changed from \"Query-Regression Networks for Machine Comprehension\""]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["minjoon seo", "sewon min", "ali farhadi", "hannaneh hajishirzi"], "accepted": true, "id": "1606.04582"}, "pdf": {"name": "1606.04582.pdf", "metadata": {"source": "CRF", "title": "Query-Regression Networks for Machine Comprehension", "authors": ["Minjoon Seo", "Hannaneh Hajishirzi", "Ali Farhadi"], "emails": ["hannaneh}@washington.edu,", "ali@cs.washington.edu"], "sections": [{"heading": "1 Introduction", "text": "We are particularly interested in the task of end-to-end MC, where (1) external language resources, such as lexicon or dependency parser, are not provided, and (2) the only monitoring during training is the answer to the question. Note that both of these limitations only make the task harder. [19] have shown that RNN-based models work poorly on end-to-end MC, in large part because RNN internal memory [8] and gated recurrent unit [4], are popular choices for sequential data modeling, Weston et al. [19] have shown that RNN-based models work poorly on end-to-end MC, in large part because RNN internal memory is inherently unstable."}, {"heading": "2 Model", "text": "We use lowercase letters, obesity letters to denote column vectors (e.g. x), and uppercase letters, obesity letters (e.g. W) to denote matrices. Without obesity letters denote scalar values (e.g. t, T, d), and obesity letters denote non-numeric objects such as a sentence (e.g. x). > is used to denote vectors or matrix transpose. Scalar and vector functions are denoted by non-obesity and obesity lowercase letters. [;] is the concatenation of vectors in a row, and [,] is the concatenation of vectors in a column. < > denotes a sequence denoting a sequence. A subscript denotes a sequence (t) in history (t), and a superscript denotes the layer index (k) of networks."}, {"heading": "2.1 QRN Unit", "text": "As an RNN-based model, QRN is a single recursive unit that updates its hidden state through time and layers. A QRN unit accepts two inputs (local query vector qt, sentence vector xt, sentence vector xt, sentence vector q, sentence vector xt from input without modification).The local query vector is not necessarily identical to the original query (question) vector q.To compress the outputs, we use the update gate function \u03b1: Rd \u00b7 Rd \u2192 [0, 1] andregress function \u03b3: Rd \u00b7 Rd \u2192 Rd. Intuitively, the update gate function measures the relevance between the set and the local query, and the regress function transforms the local query input to a new regresse (simpler) query that has given the set. Now, the outputs can be reached with the following equations:"}, {"heading": "2.2 Variations", "text": "Here we present a few variations of QRN, and later in our experiments we test the performance of QRN with and without any of these variations.Reset gate. Inspired by the Gated Recurrent Unit (GRU) [4], we found that it is useful to allow the QRN unit to undo (cancel) the candidate if necessary using the undone Tor function: Rd \u00b7 Rd \u2192 [0, 1], which can be defined similarly to the Update Tor function: rt = \u03c1 (xt, qt) = \u03c3 (W (r) (xt) qt) + b (r)))) (4), where W (r), R1 \u00b7 d is a weight matrix, and b (r)."}, {"heading": "3 Parallelization", "text": "An important advantage of QRN is that the recursive updates in Equations 3 and 4 = > RT can be computed in parallel over time, in contrast to most RNN-based models, which cannot be parallelized, where calculating the output to time t requires a calculation to all previous time steps i < t. Here, we primarily show that the query update in Eq.3 can be parallelized by paralleling the equation with matrix operations. \u2212 The extension to Eq.4 is simple. Proof this will also give an intuition on how QRN can also be parallelized with vector gates. The recursive definition of Eq.3 can be explicitly written. \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b b b b = 1 \u2212 zj = 1 \u2212 zj zih i =."}, {"heading": "4 Related Work", "text": "In fact, it is that we are able to hide, and that we are able to hide, \"he said.\" We have to be able to hide, \"he said.\" We have to be able to hide. We have to be able to hide, \"he said.\" We have to be able to hide, \"he said."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data", "text": "Following earlier models of neural architecture in end-to-end machine understanding [16, 20, 13], we test our model using the bAbI questionnaire dataset [19]. The bAbi dataset consists of 20 different tasks, each of which has a 1000 (1k) synthetically generated story question pair. A story can be as short as two sentences and longer than 200 + sentences. A system is tested for the correct answer to the questions, and the authors of the dataset define that a system exists if its error rate does not exceed 5%. Most answers are single words, and some of them are lists (e.g. \"football, apple\"). Answering questions in each task requires selecting a set of relevant sentences and applying a different kind of logical reasoning about them. See Figure 1 for the full list of the 20 tasks."}, {"heading": "5.2 Model Details", "text": "In the input module we are given sentences xt and a question q, and we want to get their vector representations, xt, q, Rd. We use a practicable embedding matrix A, Rd, V to get the most unified vector of each word xtj in each sentence xt into a d-dimensional vector xtj, Rd. Then the sentence representation xt is obtained from position encoder [18], where the number of words in the vector lj, xtj, where the k-th element of the vector lj, Rd is (1 \u2212 j / J) \u2212 (1 \u2212 2j / J) the exploitation of 1-based indexing), where J is the number of words in the text. The order of the words concerns xt. The same encoder with the same embedding matrix is also used to get the question vector q from q.Output module we will get the vector representation of q.A"}, {"heading": "5.3 Results", "text": "We report on the results of our model (QRN) and the previous work on bAbI QA. Mainly in 1k data, the average accuracy of QRN's \"2rb\" (2 levels + reset gate + bi-direction) model outperforms that of MemN2N (PE + LW + joint model) by 5.3%, and the task-oriented accuracy of the QRN model is equal to or higher than that of MemNN in 19 tasks, only losing in task 16. The QRN \"3rb\" model also has 5 more tasks than MemN2N.In 10k dataset, the average accuracy of the QRN model is equal to or higher than that of MemNN. \"(2 levels + reset gate + vector gates + bi-direction) model outperforms that of MemN2N (PE + LS + LW * model) by 1.0%. It is comparable to that of DMN +, although our model is 0.3% faster."}, {"heading": "6 Conclusion", "text": "Query Regression Network (QRN) answers questions of machine understanding by storing local queries and regressing them, while QRN observes state-changing sentences. QRN is an RNN-based model, but effective for encoding long-term dependencies between sentences. Furthermore, it is easier than previous work [16, 20] and highly parallelizable, which makes it a suitable choice for answering big questions when the size of stories grows. Finally, QRN is able to answer questions after each time step, and its internal representation in the form of updated queries is interpretable. We have shown experimentally that QRN is able to make different kinds of arguments about several facts in the bAbI QA dataset."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang"], "venue": "In EMNLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Large-scale semantic parsing via schema matching and lexicon extension", "author": ["Qingqing Cai", "Alexander Yates"], "venue": "In ACL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In EMNLP,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Anthony Fader", "Luke S. Zettlemoyer", "Oren Etzioni"], "venue": "In ACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In JMLR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Learning to solve arithmetic word problems with verb categorization", "author": ["Mohammad Javad Hosseini", "Hannaneh Hajishirzi", "Oren Etzioni", "Nate Kushman"], "venue": "In EMNLP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Reasoning in vector space: An exploratory study of question answering", "author": ["Moontae Lee", "Xiaodong He", "Wen tau Yih", "Jianfeng Gao", "Li Deng", "Paul Smolensky"], "venue": "In ICLR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning", "author": ["Arindam Mitra", "Chitta Baral"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Towards neural network-based reasoning", "author": ["Baolin Peng", "Zhengdong Lu", "Hang Li", "Kam-Fai Wong"], "venue": "arXiv preprint arXiv:1508.05508,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Knowledge in Action", "author": ["Raymond Reiter"], "venue": "MIT Press, 1st edition,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw"], "venue": "In EMNLP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Wen tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao"], "venue": "In ACL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "In ICLR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "Machine comprehension (MC) is the task of obtaining the answer to a natural language question given a sequence of natural language sentences (story) [15].", "startOffset": 149, "endOffset": 153}, {"referenceID": 7, "context": "While Recurrent Neural Network (RNN) and its variants, such as Long Short-Term Memory [8] and Gated Recurrent Unit [4], are popular choices for modeling sequential data, Weston et al.", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "While Recurrent Neural Network (RNN) and its variants, such as Long Short-Term Memory [8] and Gated Recurrent Unit [4], are popular choices for modeling sequential data, Weston et al.", "startOffset": 115, "endOffset": 118}, {"referenceID": 17, "context": "[19] have shown that RNN-based models perform poorly on end-to-end MC, largely because RNN\u2019s internal memory is inherently unstable over a long term.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Hence, recent approaches in this literature have mainly relied on global attention mechanism with shared external memory [18, 16, 13, 10, 20].", "startOffset": 121, "endOffset": 141}, {"referenceID": 12, "context": "Hence, recent approaches in this literature have mainly relied on global attention mechanism with shared external memory [18, 16, 13, 10, 20].", "startOffset": 121, "endOffset": 141}, {"referenceID": 9, "context": "Hence, recent approaches in this literature have mainly relied on global attention mechanism with shared external memory [18, 16, 13, 10, 20].", "startOffset": 121, "endOffset": 141}, {"referenceID": 18, "context": "Hence, recent approaches in this literature have mainly relied on global attention mechanism with shared external memory [18, 16, 13, 10, 20].", "startOffset": 121, "endOffset": 141}, {"referenceID": 15, "context": "End-to-end Memory Network [16] attempts to resolve this problem by adding time-dependent variable to the sentence representation at each time step (address) of the memory.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "Dynamic Memory Network [10, 20] combines RNN and attention mechanism together to incorporate time dependency into the model.", "startOffset": 23, "endOffset": 31}, {"referenceID": 18, "context": "Dynamic Memory Network [10, 20] combines RNN and attention mechanism together to incorporate time dependency into the model.", "startOffset": 23, "endOffset": 31}, {"referenceID": 13, "context": "This mechanism is akin to logic regression in situation calculus [14].", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "While previous work [18, 16, 13, 10, 20] can be also viewed as performing query regression through multiple layers, QRN is distinct from their approaches in that the query regression in our model is performed locally (timewise), so it can better encode locality information.", "startOffset": 20, "endOffset": 40}, {"referenceID": 12, "context": "While previous work [18, 16, 13, 10, 20] can be also viewed as performing query regression through multiple layers, QRN is distinct from their approaches in that the query regression in our model is performed locally (timewise), so it can better encode locality information.", "startOffset": 20, "endOffset": 40}, {"referenceID": 9, "context": "While previous work [18, 16, 13, 10, 20] can be also viewed as performing query regression through multiple layers, QRN is distinct from their approaches in that the query regression in our model is performed locally (timewise), so it can better encode locality information.", "startOffset": 20, "endOffset": 40}, {"referenceID": 18, "context": "While previous work [18, 16, 13, 10, 20] can be also viewed as performing query regression through multiple layers, QRN is distinct from their approaches in that the query regression in our model is performed locally (timewise), so it can better encode locality information.", "startOffset": 20, "endOffset": 40}, {"referenceID": 0, "context": "In order to compute the outputs, we use update gate function \u03b1 : R \u00d7 R \u2192 [0, 1] and", "startOffset": 73, "endOffset": 79}, {"referenceID": 3, "context": "Inspired by Gated Recurrent Unit (GRU) [4], we found that it is useful to allow the QRN unit to reset (nullify) the candidate regressed query (i.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "For this we use reset gate function \u03c1 : R \u00d7R \u2192 [0, 1], which can be defined similarly to the update gate function: rt = \u03c1(xt,qt) = \u03c3(W (xt \u25e6 qt) + b) (4) where W \u2208 R1\u00d7d is a weight matrix, and b \u2208 R is a bias term.", "startOffset": 47, "endOffset": 53}, {"referenceID": 7, "context": "As in LSTM [8] or GRU [4], update and reset gates can be vectors instead of scalar values for fine-controlled gating.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "As in LSTM [8] or GRU [4], update and reset gates can be vectors instead of scalar values for fine-controlled gating.", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "Previous solutions to question answering use open information extraction or textual semantic parsing to query or create a knowledge base [3, 17, 9, 2, 6].", "startOffset": 137, "endOffset": 153}, {"referenceID": 16, "context": "Previous solutions to question answering use open information extraction or textual semantic parsing to query or create a knowledge base [3, 17, 9, 2, 6].", "startOffset": 137, "endOffset": 153}, {"referenceID": 8, "context": "Previous solutions to question answering use open information extraction or textual semantic parsing to query or create a knowledge base [3, 17, 9, 2, 6].", "startOffset": 137, "endOffset": 153}, {"referenceID": 1, "context": "Previous solutions to question answering use open information extraction or textual semantic parsing to query or create a knowledge base [3, 17, 9, 2, 6].", "startOffset": 137, "endOffset": 153}, {"referenceID": 5, "context": "Previous solutions to question answering use open information extraction or textual semantic parsing to query or create a knowledge base [3, 17, 9, 2, 6].", "startOffset": 137, "endOffset": 153}, {"referenceID": 7, "context": "QRN is inspired by RNN-based models with gating mechanism, such as LSTM [8] and GRU [4].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "QRN is inspired by RNN-based models with gating mechanism, such as LSTM [8] and GRU [4].", "startOffset": 84, "endOffset": 87}, {"referenceID": 15, "context": "End-to-end Memory Network (MemN2N) [16] (and Neural Reasoner [13]) uses external memory with multi-layer attention mechanism to focus on sentences that are relevant to the question.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "End-to-end Memory Network (MemN2N) [16] (and Neural Reasoner [13]) uses external memory with multi-layer attention mechanism to focus on sentences that are relevant to the question.", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "(b) MemN2N [16] x\" x# x$ q = h( AGRU AGRU AGRU GRU \u22ef x\" x# x$ AGRU AGRU AGRU GRU \u22ef", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": "(c) DMN+ [20]", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "Figure 2: The schematics of QRN and the two state-of-the-art models, End-to-End Memory Networks (MemN2N [16]) and Improved Dynamic Memory Networks (DMN+ [20]), simplified to emphasize the differences among the models.", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "Figure 2: The schematics of QRN and the two state-of-the-art models, End-to-End Memory Networks (MemN2N [16]) and Improved Dynamic Memory Networks (DMN+ [20]), simplified to emphasize the differences among the models.", "startOffset": 153, "endOffset": 157}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Improved Dynamic Memory Network (DMN+) [20] uses the hybrid of the attention mechanism and the RNN architecture to model the sequence of sentences.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "Memory Networks [18] and Dynamic Memory Networks [10] are earlier models that inspired MemN2N and DMN+, respectively.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "While Mitra and Baral [12] and Lee et al.", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "[11] have shown near-perfect accuracies in bAbI QA dataset, they are largely rule-based (requiring human efforts), they use different models for different tasks, and/or they use external language resources such as dependency parser.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Following previous neural architecture models in end-to-end machine comprehension [16, 20, 13], we test our model on bAbI question answering (QA) dataset [19].", "startOffset": 82, "endOffset": 94}, {"referenceID": 18, "context": "Following previous neural architecture models in end-to-end machine comprehension [16, 20, 13], we test our model on bAbI question answering (QA) dataset [19].", "startOffset": 82, "endOffset": 94}, {"referenceID": 12, "context": "Following previous neural architecture models in end-to-end machine comprehension [16, 20, 13], we test our model on bAbI question answering (QA) dataset [19].", "startOffset": 82, "endOffset": 94}, {"referenceID": 17, "context": "Following previous neural architecture models in end-to-end machine comprehension [16, 20, 13], we test our model on bAbI question answering (QA) dataset [19].", "startOffset": 154, "endOffset": 158}, {"referenceID": 18, "context": "However, since DMN+ [20] only reports on the 10k dataset, we also report our result on it for a fair comparison.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "Following MemN2N [16] and DMN+, QRN is only supervised by question answers.", "startOffset": 17, "endOffset": 21}, {"referenceID": 9, "context": "Hence we do not compare QRN against Memory Networks [18] and DMN [10], which require strong supervision of supporting facts during training.", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "Although not explored, an alternative way will be to use an RNN decoder [4] to output each word of the answer at each time step of the decoder.", "startOffset": 72, "endOffset": 75}, {"referenceID": 6, "context": "Weights in the QRN unit were initialized using techniques by Glorot and Bengio [7], and were tied across the layers.", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "The loss is minimized by stochastic gradient descent for 150 epochs, and the learning rate was controlled by AdaGrad [5] with the initial learning rate of 0.", "startOffset": 117, "endOffset": 120}, {"referenceID": 17, "context": "9 Table 1: bAbI QA dataset [19] error rates (%) of QRN and previous work: LSTM [19], End-to-end Memory Networks (N2N [16]), and Dynamic Memory Networks (DMN+ [20]).", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "9 Table 1: bAbI QA dataset [19] error rates (%) of QRN and previous work: LSTM [19], End-to-end Memory Networks (N2N [16]), and Dynamic Memory Networks (DMN+ [20]).", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "9 Table 1: bAbI QA dataset [19] error rates (%) of QRN and previous work: LSTM [19], End-to-end Memory Networks (N2N [16]), and Dynamic Memory Networks (DMN+ [20]).", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "9 Table 1: bAbI QA dataset [19] error rates (%) of QRN and previous work: LSTM [19], End-to-end Memory Networks (N2N [16]), and Dynamic Memory Networks (DMN+ [20]).", "startOffset": 158, "endOffset": 162}, {"referenceID": 0, "context": "We implemented QRN with and without parallelization in TensorFlow [1] on a single Titan X GPU to qunaitify the computational gain of the parallelization.", "startOffset": 66, "endOffset": 69}, {"referenceID": 12, "context": "[13]).", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We present Query-Regression Network (QRN), a variant of Recurrent Neural Network (RNN) that is suitable for end-to-end machine comprehension. While previous work largely relied on external memory and attention mechanism, QRN is a single recurrent unit with internal memory. Unlike most RNN-based models, QRN is able to effectively handle long-term dependencies and is highly parallelizable. In our experiments we show that QRN outperforms the state of the art in end-to-end bAbI QA tasks.", "creator": "LaTeX with hyperref package"}}}