{"id": "1606.04596", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Semi-Supervised Learning for Neural Machine Translation", "abstract": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi-supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the source-to-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese-English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.", "histories": [["v1", "Wed, 15 Jun 2016 00:22:27 GMT  (223kb,D)", "https://arxiv.org/abs/1606.04596v1", "Accepted for publication in the Proceedings of ACL 2016"], ["v2", "Wed, 10 Aug 2016 19:08:20 GMT  (223kb,D)", "http://arxiv.org/abs/1606.04596v2", "Accepted for publication in the Proceedings of ACL 2016. A typo in the references was corrected"], ["v3", "Sat, 10 Dec 2016 20:02:52 GMT  (223kb,D)", "http://arxiv.org/abs/1606.04596v3", "Corrected a typo"]], "COMMENTS": "Accepted for publication in the Proceedings of ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yong cheng", "wei xu 0005", "zhongjun he", "wei he", "hua wu", "maosong sun", "yang liu 0005"], "accepted": true, "id": "1606.04596"}, "pdf": {"name": "1606.04596.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Learning for Neural Machine Translation", "authors": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "emails": ["chengyong3001@gmail.com", "weixu@tsinghua.edu.cn", "hua}@baidu.com", "sms@tsinghua.edu.cn", "liuyang2011@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to outdo themselves and that they are able to outdo themselves. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2 Semi-Supervised Learning for Neural Machine Translation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Supervised Learning", "text": "Given a parallel corpus D = {< x (n), y (n) >} Nn = 1, the standard training goal in the NMT is to maximize the probability of training data: L (\u03b8) = N \u2211 n = 1logP (y (n) | x (n); \u03b8)), (1) where P (y | x; \u03b8) is a neural translation model and \u03b8 is a set of model parameters. D can be labeled data for predicting a target y with a source set x.Since P (y | x; \u03b8) is modeled through a single, large neural network, there is no separate target language model P (y; \u03b8) in the NMT. Therefore, parallel corpora is the only resource for parameter estimation in most existing NMT systems. Unfortunately, even for a handful of resource-rich languages, the available areas are unbalanced and limited to government documents and news."}, {"heading": "2.2 Autoencoders on Monolingual Corpora", "text": "It is attractive to explore the more readily available, abundant, monolingual corpora in order to improve the translation model in China. First, consider an unattended setting: how to train NMT models on a monolingual corpus. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 Sentence in a latent source set using a target-to-source translation model and (2) Decoding the source set to reconstruct the observed target set (Vincent et al., 2010; Socher et al., 2011): \u2212 Sentence in a latent source set using a target-to-source translation model and (2) Decoding the source set to reconstruct the observed target set (sentence in a source text)."}, {"heading": "2.3 Semi-Supervised Learning", "text": "Since autoencoders include both source-to-destination and target-to-source models, it is natural to combine parallel corpus and monolingual corpus to learn birectional NMT translation models in a semi-monitored environment.Formally speaking, with a parallel corpus D = {< x (s), y (n) >} Nn = 1, a target language monolingual corpus T = {y (t)} Tt = 1, and a source language monolingual corpus S = {x (s)} Ss = 1, we present our new semi-monitored training goal as follows: J (\u2212 \u2192 Prognos, Prognos) T = N = 1 logP \u2212 n = 1 logP (n), target (n) is the source."}, {"heading": "2.4 Training", "text": "We use mini-batch stochastics gradient descent to form our common model. For each q iteration, in addition to the mini-batch from the parallel corpus, two further mini-batches are formed by randomly selecting sentences from the source and target corpus. Afterwards, gradients from these mini-batches are collected to update model parameters. The partial derivative of J (\u2212 \u2192 \u03b8, \u2212 \u03b8) is given taking into account the source and target models of X (t)."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Setup", "text": "As shown in Table 1, we use both a parallel corpus and two monolingual corpus as a training set. The parallel corpus of LDC consists of 2.56M pairs of sentences with 67.53M Chinese words and 74.81M English words. The vocabulary sizes of Chinese and English are 0.21M and 0.16M, respectively. We use the Chinese and English parts of the Xinhua part of the GIGAWORD corpus as a monolingual corpus. The Chinese monolingual corpus contains 18.75M sentences with 451.94M words. The English corpus contains 22.32M sentences with 399.83M words. The word sizes of Chinese and English are 0.97M and 1.34M, respectively. For Chinese-English translations, we use the NIST system 2006 Chinese-English records as a validation set for hyperparameter optimization and English selection."}, {"heading": "3.2 Effect of Sample Size k", "text": "Since the conclusion of our approach is unworkable, we propose approximating the entire search space with the top-k list of candidate translations to improve efficiency (see Equation (9)). Figure 2 shows the BLEU values of different settings of k over time. Only the English monolingual corpus is included with the training data. We observe that increasing the approximate search space generally leads to an improvement in BLEU values. There are significant gaps between k = 1 and k = 5. However, maintaining the increasing k does not lead to significant improvements and a reduction in training efficiency. We note that k = 10 achieves a balance between training efficiency and translation quality."}, {"heading": "3.3 Effect of OOV Ratio", "text": "To answer this question, we examine the effects of the OOV ratio on the translation quality defined asratio = \u2211 y-yJy / \u0102VDtK | y, (10) where y is a target-language sentence in the monolingual corpus T, y is the vocabulary of the target page of the parallel corpus D. Intuitively, the OOV ratio shows how a sentence in the monolingual corpus resembles the parallel corpus. If the ratio is 0, all words in the monolingual sentence also appear in the parallel corpus. Figure 4 shows the effect of the OOV ratio on the Chinese-English validation ratio. Only the monolingual corpus is appended to the parallel corpus."}, {"heading": "3.4 Comparison with SMT", "text": "Table 2 shows the comparison between MOSES and our work. MOSES used the monolingual corpora as shown in Table 1: 18.75M Chinese sentences and 22.32M English sentences. We note that using monolingual corpora dramatically improves translation performance in both Chinese-English and English-Chinese. If one relies only on parallel corpus, RNSEARCH outperforms MOSES, which is also trained only in the parallel corpus. However, the ability to use abundant monolingual corpora allows MOSES to achieve much higher BLEU values than RNSEARCH by using only parallel corpus.Instead of using all sentences in the monolingual corpus, we constructed smaller monolingual corpora with zero OOV ratio: 2.56M Chinese sentences with 47.51M words and 2.56M English sentences with 37.47M words."}, {"heading": "3.5 Comparison with Previous Work", "text": "We used the method of Sennrich et al. (2015) on the parallel corpus D = {< x (n), y (n) >} Nn = 1. The trained target-to-source model of neural translation is used to pair a target-to-source corpus T = {y (t)} Tt = 1 into a source corpus S = {x (t)} Tt = 1.3. The target-to-source model is paired with its translations to form a pseudo-parallel corpus T = {y (t)} Tt = 1 into a source corpus S = {x)} Tt = 1.3. The target-to-source corpus is paired with its translations to form a pseudo-parallel corpus, which is then attached to the original parallel corpus to obtain a larger parallel corpus: D < S < S, T >.4."}, {"heading": "4 Related Work", "text": "Our work is inspired by two areas of research: (1) the use of monolingual corpora for machine translation and (2) autoencoders for unsupervised and semi-supervised learning."}, {"heading": "4.1 Exploiting Monolingual Corpora for Machine Translation", "text": "Several authors have introduced transductive learning to fully exploit monolingual corpora (Ueffing et al., 2007; Bertoldi and Federico, 2009), using an existing translation model to translate invisible source code that can be paired with its translations to form a pseudo-parallel corpus, a process that repeats itself to convergence. Klementiev et al. (2012) suggest an approach to estimate phrase translation probabilities from monolingual corpora. Zhang and Zong (2013) directly extract parallel phrases from monolingual corpora using retrievable techniques. Another important line of research is to address the translation of monolingual corpora as a deciphering problem (Ravi and Knight, 2011; Dou et al., 2014). Closely related to Gulccehre et al. (2015) and Sennal. (2015), our 2015 approach is also focused on a corporal approach, both autocooperative and autodidactic."}, {"heading": "4.2 Autoencoders in Unsupervised and Semi-Supervised Learning", "text": "The approach of Socher et al. (2011) is similar to our approach to the introduction of semi-monitored recursive autoencoders for mood analysis. The difference is that we are interested in making better use of parallel and monolingual corpora while focusing on giving partial supervision to conventional, unsupervised autoencoders. Dai and Le (2015) introduce a sequence autoencoder to reconstruct an observed sequence via RNNNs. Our approach differs from sequence autoencoders in that we use bidirectional translation models as encoders and decoders to enable them to interact within the autoencoders."}, {"heading": "5 Conclusion", "text": "We have presented a semi-supervised approach to the formation of bidirectional neural machine translation models, the central idea being to introduce autoencoders on monolingual corpora with source-totargett and target-to-source translation models as encoders and decoders. Experiments with Chinese NIST data sets show that our approach leads to significant improvements. As our method is sensitive to the OOVs present in monolingual corpora, we plan to integrate Jean et al. (2015) \"s technique for using very large vocabularies into our approach. It is also necessary to further validate the effectiveness of our approach to more language pairs and NMT architectures, and another interesting direction is to improve the link between source-to-target and target casting models (e.g. by having the two models use the same word embeddings) to help them benefit from each other better."}, {"heading": "Acknowledgements", "text": "This work was carried out during a visit of Yong Cheng to Baidu. This research is supported by the 973 Program (2014CB340501, 2014CB340505), the National Natural Science Foundation of China (No. 61522204, 61331013, 61361136003), 1000 Talent Plan Grant, Tsinghua Initiative Research Program No. 61522204, 61361136003 and a Google Faculty Research Award. We sincerely thank the audience for their valuable suggestions."}], "references": [{"title": "Conditional random field autoencoders for unsupervised structred prediction", "author": ["Waleed Ammar", "Chris Dyer", "Noah Smith."], "venue": "Proceedings of NIPS 2014.", "citeRegEx": "Ammar et al\\.,? 2014", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyungHyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Domain adaptation for statistical machine translation with monolingual resources", "author": ["Nicola Bertoldi", "Marcello Federico."], "venue": "Proceedings of WMT.", "citeRegEx": "Bertoldi and Federico.,? 2009", "shortCiteRegEx": "Bertoldi and Federico.", "year": 2009}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer."], "venue": "Computational Linguisitics.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang."], "venue": "Proceedings of ACL.", "citeRegEx": "Chiang.,? 2005", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunhyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of SSST-8.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Semisupervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le."], "venue": "Proceedings of NIPS.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Beyond parallel data: Joint word alignment and decipherment improves machine translation", "author": ["Qing Dou", "Ashish Vaswani", "Kevin Knight."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Dou et al\\.,? 2014", "shortCiteRegEx": "Dou et al\\.", "year": 2014}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Caglar Gulccehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Gulccehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gulccehre et al\\.", "year": 2015}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Computational Linguisitics.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1993", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1993}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Sebastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of ACL.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Toward statistical machine translation without paralel corpora", "author": ["Alexandre Klementiev", "Ann Irvine", "Chris CallisonBurch", "David Yarowsky."], "venue": "Proceedings of EACL.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz J. Och", "Daniel Marcu."], "venue": "Proceedings of NAACL.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of ACL.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Och."], "venue": "Proceedings of ACL.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a methof for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of ACL.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Deciphering foreign language", "author": ["Sujith Ravi", "Kevin Knight."], "venue": "Proceedings of ACL.", "citeRegEx": "Ravi and Knight.,? 2011", "shortCiteRegEx": "Ravi and Knight.", "year": 2011}, {"title": "Improving nerual machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv:1511.06709 [cs.CL].", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric Huang", "Andrew Ng", "Christopher Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Srilm - am extensible language modeling toolkit", "author": ["Andreas Stolcke."], "venue": "Proceedings of ICSLP.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Trasductive learning for statistical machine translation", "author": ["Nicola Ueffing", "Gholamreza Haffari", "Anoop Sarkar."], "venue": "Proceedings of ACL.", "citeRegEx": "Ueffing et al\\.,? 2007", "shortCiteRegEx": "Ueffing et al\\.", "year": 2007}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Autoine Manzagol."], "venue": "Journal of Machine Learning", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Learning a phrase-based translation model from monolingual data with application to domain adaptation", "author": ["Jiajun Zhang", "Chengqing Zong."], "venue": "Proceedings of ACL.", "citeRegEx": "Zhang and Zong.,? 2013", "shortCiteRegEx": "Zhang and Zong.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 233, "endOffset": 312}, {"referenceID": 21, "context": "End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 233, "endOffset": 312}, {"referenceID": 1, "context": "End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 233, "endOffset": 312}, {"referenceID": 3, "context": "Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model-", "startOffset": 128, "endOffset": 182}, {"referenceID": 13, "context": "Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model-", "startOffset": 128, "endOffset": 182}, {"referenceID": 4, "context": "Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model-", "startOffset": 128, "endOffset": 182}, {"referenceID": 9, "context": "ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al.", "startOffset": 93, "endOffset": 169}, {"referenceID": 5, "context": "ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al.", "startOffset": 93, "endOffset": 169}, {"referenceID": 21, "context": "ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al.", "startOffset": 93, "endOffset": 169}, {"referenceID": 1, "context": ", 2014) and attention mechanisms (Bahdanau et al., 2015).", "startOffset": 33, "endOffset": 56}, {"referenceID": 11, "context": "This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 168, "endOffset": 247}, {"referenceID": 21, "context": "This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 168, "endOffset": 247}, {"referenceID": 1, "context": "This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 168, "endOffset": 247}, {"referenceID": 1, "context": ", 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unfortunately, parallel corpora are usually only available for a handful of resourcerich languages and restricted to limited domains such as government documents and news reports. In contrast, SMT is capable of exploiting abundant target-side monolingual corpora to boost fluency of translations. Therefore, the unavailability of large-scale, high-quality, and wide-coverage parallel corpora hinders the applicability of NMT. As a result, several authors have tried to use abundant monolingual corpora to improve NMT. Gulccehre et al. (2015) propose two methods, which are referred to as shallow fusion and deep fusion, to integrate a language model into NMT.", "startOffset": 34, "endOffset": 988}, {"referenceID": 1, "context": ", 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unfortunately, parallel corpora are usually only available for a handful of resourcerich languages and restricted to limited domains such as government documents and news reports. In contrast, SMT is capable of exploiting abundant target-side monolingual corpora to boost fluency of translations. Therefore, the unavailability of large-scale, high-quality, and wide-coverage parallel corpora hinders the applicability of NMT. As a result, several authors have tried to use abundant monolingual corpora to improve NMT. Gulccehre et al. (2015) propose two methods, which are referred to as shallow fusion and deep fusion, to integrate a language model into NMT. The basic idea is to use the language model to score the candidate words proposed by the translation model at each time step or concatenating the hidden states of the language model and the decoder. Although their approach leads to significant improvements, one possible downside is that the network architecture has to be modified to integrate the language model. Alternatively, Sennrich et al. (2015) propose two approaches to exploiting monolingual corpora that is transparent to network architectures.", "startOffset": 34, "endOffset": 1509}, {"referenceID": 22, "context": "Similar ideas have also been suggested in conventional SMT (Ueffing et al., 2007; Bertoldi and Federico, 2009).", "startOffset": 59, "endOffset": 110}, {"referenceID": 2, "context": "Similar ideas have also been suggested in conventional SMT (Ueffing et al., 2007; Bertoldi and Federico, 2009).", "startOffset": 59, "endOffset": 110}, {"referenceID": 2, "context": ", 2007; Bertoldi and Federico, 2009). Sennrich et al. (2015) report that their approach significantly improves translation quality across a variety of language pairs.", "startOffset": 8, "endOffset": 61}, {"referenceID": 23, "context": "Let us first consider an unsupervised setting: how to train NMT models on a monolingual corpus T = {y}t=1? Our idea is to leverage autoencoders (Vincent et al., 2010; Socher et al., 2011): (1) encoding an observed target sentence into a latent source sentence using a target-to-source translation model and (2) decoding the source sentence to reconstruct the observed target sentence using a source-to-target model.", "startOffset": 144, "endOffset": 187}, {"referenceID": 19, "context": "Let us first consider an unsupervised setting: how to train NMT models on a monolingual corpus T = {y}t=1? Our idea is to leverage autoencoders (Vincent et al., 2010; Socher et al., 2011): (1) encoding an observed target sentence into a latent source sentence using a target-to-source translation model and (2) decoding the source sentence to reconstruct the observed target sentence using a source-to-target model.", "startOffset": 144, "endOffset": 187}, {"referenceID": 23, "context": "Note that our autoencoders inherit the same spirit from conventional autoencoders (Vincent et al., 2010; Socher et al., 2011) except that the hidden layer is denoted by a latent sentence instead of real-valued vectors.", "startOffset": 82, "endOffset": 125}, {"referenceID": 19, "context": "Note that our autoencoders inherit the same spirit from conventional autoencoders (Vincent et al., 2010; Socher et al., 2011) except that the hidden layer is denoted by a latent sentence instead of real-valued vectors.", "startOffset": 82, "endOffset": 125}, {"referenceID": 0, "context": "Our definition of auotoencoders is inspired by Ammar et al. (2014). Note that our autoencoders inherit the same spirit from conventional autoencoders (Vincent et al.", "startOffset": 47, "endOffset": 67}, {"referenceID": 16, "context": "The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.", "startOffset": 47, "endOffset": 70}, {"referenceID": 1, "context": "RNNSEARCH (Bahdanau et al., 2015): an attention-based NMT system.", "startOffset": 10, "endOffset": 33}, {"referenceID": 15, "context": "For MOSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003).", "startOffset": 196, "endOffset": 207}, {"referenceID": 20, "context": "We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models.", "startOffset": 25, "endOffset": 40}, {"referenceID": 14, "context": "We follow Luong et al. (2015) to address rare words.", "startOffset": 10, "endOffset": 30}, {"referenceID": 1, "context": "RNNSEARCH is an attention-based neural machine translation system (Bahdanau et al., 2015).", "startOffset": 66, "endOffset": 89}, {"referenceID": 18, "context": "Table 3: Comparison with Sennrich et al. (2015). Both Sennrich et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 18, "context": "Table 3: Comparison with Sennrich et al. (2015). Both Sennrich et al. (2015) and our approach build on top of RNNSEARCH to exploit monolingual corpora.", "startOffset": 25, "endOffset": 77}, {"referenceID": 18, "context": "Table 3: Comparison with Sennrich et al. (2015). Both Sennrich et al. (2015) and our approach build on top of RNNSEARCH to exploit monolingual corpora. The BLEU scores are case-insensitive. \u201c*\u201d: significantly better than Sennrich et al. (2015) (p < 0.", "startOffset": 25, "endOffset": 244}, {"referenceID": 18, "context": "Table 3: Comparison with Sennrich et al. (2015). Both Sennrich et al. (2015) and our approach build on top of RNNSEARCH to exploit monolingual corpora. The BLEU scores are case-insensitive. \u201c*\u201d: significantly better than Sennrich et al. (2015) (p < 0.05); \u201c**\u201d: significantly better than Sennrich et al. (2015) (p < 0.", "startOffset": 25, "endOffset": 311}, {"referenceID": 18, "context": "We re-implemented Sennrich et al. (2015)\u2019s method on top of RNNSEARCH as follows:", "startOffset": 18, "endOffset": 41}, {"referenceID": 18, "context": "Our approach achieves significant improvements over Sennrich et al. (2015) in both Chinese-to-English and English-to-Chinese directions (up to +1.", "startOffset": 52, "endOffset": 75}, {"referenceID": 18, "context": "Our approach achieves significant improvements over Sennrich et al. (2015) in both Chinese-to-English and English-to-Chinese directions (up to +1.8 and +1.0 BLEU points). One possible reason is that Sennrich et al. (2015) only use the pesudo parallel corpus for parameter estimation for once (see Step 4 above) while our approach enables source-to-target and targetto-source models to interact with each other iteratively on both parallel and monolingual corpora.", "startOffset": 52, "endOffset": 222}, {"referenceID": 18, "context": "To some extent, our approach can be seen as an iterative extension of Sennrich et al. (2015)\u2019s approach: after estimating model parameters on the pseudo parallel corpus, the learned model parameters are used to produce a better pseudo parallel corpus.", "startOffset": 70, "endOffset": 93}, {"referenceID": 22, "context": "Several authors have introduced transductive learning to make full use of monolingual corpora (Ueffing et al., 2007; Bertoldi and Federico, 2009).", "startOffset": 94, "endOffset": 145}, {"referenceID": 2, "context": "Several authors have introduced transductive learning to make full use of monolingual corpora (Ueffing et al., 2007; Bertoldi and Federico, 2009).", "startOffset": 94, "endOffset": 145}, {"referenceID": 17, "context": "Another important line of research is to treat translation on monolingual corpora as a decipherment problem (Ravi and Knight, 2011; Dou et al., 2014).", "startOffset": 108, "endOffset": 149}, {"referenceID": 7, "context": "Another important line of research is to treat translation on monolingual corpora as a decipherment problem (Ravi and Knight, 2011; Dou et al., 2014).", "startOffset": 108, "endOffset": 149}, {"referenceID": 2, "context": ", 2007; Bertoldi and Federico, 2009). They use an existing translation model to translate unseen source text, which can be paired with its translations to form a pseudo parallel corpus. This process iterates until convergence. While Klementiev et al. (2012) propose an approach to estimating phrase translation probabilities from monolingual corpora, Zhang and Zong (2013) directly extract parallel phrases from monolingual corpora using retrieval techniques.", "startOffset": 8, "endOffset": 258}, {"referenceID": 2, "context": ", 2007; Bertoldi and Federico, 2009). They use an existing translation model to translate unseen source text, which can be paired with its translations to form a pseudo parallel corpus. This process iterates until convergence. While Klementiev et al. (2012) propose an approach to estimating phrase translation probabilities from monolingual corpora, Zhang and Zong (2013) directly extract parallel phrases from monolingual corpora using retrieval techniques.", "startOffset": 8, "endOffset": 373}, {"referenceID": 8, "context": "Closely related to Gulccehre et al. (2015) and Sennrich et al.", "startOffset": 19, "endOffset": 43}, {"referenceID": 8, "context": "Closely related to Gulccehre et al. (2015) and Sennrich et al. (2015), our approach focuses on learning birectional NMT models via autoencoders on monolingual corpora.", "startOffset": 19, "endOffset": 70}, {"referenceID": 23, "context": "Autoencoders and their variants have been widely used in unsupervised deep learning ((Vincent et al., 2010; Socher et al., 2011; Ammar et al., 2014), just to name a few).", "startOffset": 85, "endOffset": 148}, {"referenceID": 19, "context": "Autoencoders and their variants have been widely used in unsupervised deep learning ((Vincent et al., 2010; Socher et al., 2011; Ammar et al., 2014), just to name a few).", "startOffset": 85, "endOffset": 148}, {"referenceID": 0, "context": "Autoencoders and their variants have been widely used in unsupervised deep learning ((Vincent et al., 2010; Socher et al., 2011; Ammar et al., 2014), just to name a few).", "startOffset": 85, "endOffset": 148}, {"referenceID": 0, "context": ", 2011; Ammar et al., 2014), just to name a few). Among them, Socher et al. (2011)\u2019s approach bears close resemblance to our approach as they introduce semi-supervised recursive autoencoders for sentiment analysis.", "startOffset": 8, "endOffset": 83}, {"referenceID": 0, "context": ", 2011; Ammar et al., 2014), just to name a few). Among them, Socher et al. (2011)\u2019s approach bears close resemblance to our approach as they introduce semi-supervised recursive autoencoders for sentiment analysis. The difference is that we are interested in making a better use of parallel and monolingual corpora while they concentrate on injecting partial supervision to conventional unsupervised autoencoders. Dai and Le (2015) introduce a sequence autoencoder to reconstruct an observed sequence via RNNs.", "startOffset": 8, "endOffset": 432}, {"referenceID": 10, "context": "As our method is sensitive to the OOVs present in monolingual corpora, we plan to integrate Jean et al. (2015)\u2019s technique on using very large vocabulary into our approach.", "startOffset": 92, "endOffset": 111}], "year": 2016, "abstractText": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semisupervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the sourceto-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the ChineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.", "creator": "TeX"}}}