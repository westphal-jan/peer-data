{"id": "1606.04640", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations", "abstract": "We present the Siamese Continuous Bag of Words (Siamese CBOW) model, a neural network for efficient estimation of high-quality sentence embeddings. Averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way of obtaining sentence embeddings. However, word embeddings trained with the methods currently available are not optimized for the task of sentence representation, and, thus, likely to be suboptimal. Siamese CBOW handles this problem by training word embeddings directly for the purpose of being averaged. The underlying neural network learns word embeddings by predicting, from a sentence representation, its surrounding sentences. We show the robustness of the Siamese CBOW model by evaluating it on 20 datasets stemming from a wide variety of sources.", "histories": [["v1", "Wed, 15 Jun 2016 04:47:43 GMT  (955kb,D)", "http://arxiv.org/abs/1606.04640v1", "Accepted as full paper at ACL 2016, Berlin. 11 pages"]], "COMMENTS": "Accepted as full paper at ACL 2016, Berlin. 11 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tom kenter", "alexey borisov", "maarten de rijke"], "accepted": true, "id": "1606.04640"}, "pdf": {"name": "1606.04640.pdf", "metadata": {"source": "CRF", "title": "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations", "authors": ["Tom Kenter", "Alexey Borisov", "Maarten de Rijke"], "emails": ["tom.kenter@uva.nl", "alborisov@yandex-team.ru", "derijke@uva.nl"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we present a model that is specifically tailored to the needs of people who are able to realize their needs. (...) We have it in our hands, who are able to realize their needs. (...) We have it in our hands to realize them. (...) We have not been able to realize them. (...) We have not been able to realize them. (...) We have not been able to realize them. (...) We have not been able to realize them. (...) We have not been able to realize them. (...) We have been able to realize them. (...) We have been able to realize them. (...) We have not been able to realize them. \"(...) We have been able to realize them.\""}, {"heading": "2 Siamese CBOW", "text": "We introduce the Siamese Continuous Bag of Words (CBOW) model, a neural network for the efficient estimation of high-quality sentence embeddings. Quality should manifest itself in the embedding of semantically close sentences that are similar to each other, and in the embedding of semantically different sentences. An efficient and surprisingly successful method of sentence embeddings is the average embedding of their constituent words. Recent work uses pre-trained word embeddings (such as word2vec and GloVe) that are not optimized for sentence representation. Following these approaches, we calculate sentence embeddings by the average word embeddings, but optimize word embeddings directly in order to be averaged."}, {"heading": "2.1 Training objective", "text": "We construct a supervised training criterion by having our network predict sentences that occur side by side in the training data. Specifically, we define for a pair of sentences (si, sj) a probability p (si, sj) that reflects how likely it is that the sentences in the training data are next to each other. We calculate the probability p (si, sj) with a Softmax function: p\u03b8 (si, sj) = ecos (s\u03b8 i, s \u03b8 j). The sum in the denominator of equation 1 should extend over all possible sentences S, which is not feasible in practice. (1) Where s\u03b8i, s\u03b8 denotes the embedding of the sentence sx, based on the model parameters. Theoretically, the sum in the denominator of equation 1 should extend over all possible sentences S, which is not feasible in practice. Therefore, we replace the sentence S by the union of the sentences that occur next to the sentence si in the training data."}, {"heading": "2.2 Network architecture", "text": "Figure 1 shows the architecture of the proposed Siamese CBOW network. Input is a projection layer that selects embedding from a word-embedding matrix W (which is used across input) for a given input set. Word embedding is averaged to the next level, resulting in a sentence representation with the same dimensions as the input word embedding (the fields labeled average i in Figure 1). Cosmic similarities between the sentence representation for broadcast time and the other sentences are calculated at the penultimate level, and a softmax is applied at the last level to generate the final probability distribution."}, {"heading": "2.3 Training", "text": "The weights in the Word embedding matrix are the only traceable parameters in the Siamese CBOW network. They are updated using stochastic gradient pedigree. Initial learning rate is reduced monotonously in proportion to the number of training sessions."}, {"heading": "3 Experimental Setup", "text": "To test the effectiveness of our Siamese network for creating sentence embeddings, we use multiple test sets. We use Siamese CBOW to learn word embeddings from an unlabeled corpus. For each pair of sentences in the test sets, we calculate two sentence representations by averaging the word embeddings of each sentence. Words that are missing from the vocabulary and therefore have no word embeddings are omitted. Cosmic similarity between the two sentence vectors is produced as the last semantic similarity scale. As we want a clean way to directly evaluate the embeddings on multiple sets, we train our model and the models we compare with on exactly the same training data. We do not calculate any additional features, perform additional pre-processing steps or incorporate the embeddings into supervised training programs. Further steps such as these will most likely improve the evaluation results, but they would disguise our main purpose in this work, to test the embeddings directly."}, {"heading": "3.1 Data", "text": "We use the Toronto Book Corpus1 to train word embedding. This corpus contains a total of 74,004,228 pre-processed sentences consisting of 1,057,070,918 tokens drawn from 7,087 unique books. In our experiments, we look at tokens occurring five times or more, resulting in a vocabulary of 315,643 words."}, {"heading": "3.2 Baselines", "text": "We use two baselines to create sentence embeddings in our experiments. We obtain similarity values between sentence pairs from baselines in the same way as those produced by the Siamese CBOW, i.e. we calculate the cosine similarity between the sentence embeddings they produce. 1The corpus can be accessed from http: / / www. cs.toronto.edu / \u02dc mbweb /; cf. (Zhu et al., 2015).Word2vec We use average word embeddings associated with word2vec.2 We use both architectures, Skipgram and CBOW, and apply default settings: minimum word frequency 5, word embeddings size 300, context window 5, sample threshold 10-5, no hierarchical softmax, 5 negative examples."}, {"heading": "3.3 Evaluation", "text": "We use 20 SemEval datasets from the SemEval task of semantic textual similarity in 2012, 2013, 2014 and 2015 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), which consist of sentence pairs from a variety of sources (e.g. message wire, tweets, video descriptions) commented manually by several human assessors on a 5-point scale (1: semantically independent, 5: semantically similar). In fact, the final similarity value for each sentence pair is is2The code is available at https: / / cod. google.com / archive / p / word2vec /.3The code and the trained models can be downloaded from https: / / github.com / ryankiros / skip-thoughts /.the meaning of the annotator judgments."}, {"heading": "3.4 Network", "text": "In accordance with other research results (Mikolov et al., 2013b; Kusner et al., 2015), we are correcting the embedding size to 300 and only taking into account words that occur 5 times or more in the training corpus. We are using 2 negative examples (see \u00a7 4.2.2 for an analysis of different settings).The embedding is randomly initialized by drawing from a normal distribution of \u00b5 = 0.0 and \u03c3 = 0.01. Batch size is 100. The initial learning rate \u03b1 is 0.0001, which we obtain by observing the loss on the training data.The training consists of an epoch.We are using Theano (Theano Development Team, 2016) to implement our network.4 We have conducted our experiments on GPUs in the DAS5 cluster (Bal et al., 2016).4The code for the Siamese CBOW is available under an open source license at https: / / etbitbuese / tomcenter.org / Ksibow."}, {"heading": "4 Results", "text": "In this section we present the results of our experiments and analyze the stability of the Siamese CBOW in terms of its (hyper-) parameters."}, {"heading": "4.1 Main experiments", "text": "In Table 1, the results of the Siamese CBOW are presented on 20 SemEval data sets, together with the results of the base systems. As we can see from the table, the Siamese CBOW performs better than the baselines in most cases (14 out of 20). The very low values of fasting thinking on MSRpar appear to be a glitch that we will ignore. It is interesting to see that for the group with the highest average sentence length (2013 SMT, with an average of 24.7 words per sentence), the Siamese CBOW is very close to skip thinking, the most powerful baseline. As for lexical term overlaps, it is not surprising that all methods have problems with the sets with low overlap (2013 FNWN, 2015 response forums, both with 7% lexical overlap). However, it is interesting to see that for the next two SSSets (2015 Faith and 2012 MSRpar, 11% and 14% respectively, the two CamesiBOW overlaps) the two overlaps show the same."}, {"heading": "4.2 Analysis", "text": "Next, we examine the stability of the Siamese CBOW in terms of its hyperparameters. The comparison should be interpreted with caution, as it is not clear which vocabulary was used for the experiments (Hill et al., 2016); therefore, the differences observed here may simply be due to differences in vocabulary coverage. In particular, we consider stability across iterations, the different number of negative examples, and the dimensionality of the embedding. Other parameter settings are set, as reported in \u00a7 3.4."}, {"heading": "4.2.1 Performance across iterations", "text": "Ideally, the optimization criterion of a learning algorithm extends over the entire range of its loss function. As discussed in paragraph 2, our loss function considers only a sample, so convergence is not guaranteed. Regardless, an ideal learning system should not fluctuate in terms of performance relative to the amount of observed training data, provided that this amount is significant: the further the training progresses, the more stable the performance should be. To see whether the performance of the Siamese CBOW fluctuates during training, we observe it during 5 epochs; at every 10,000,000 example and at the end of each epoch. Fig. 2 shows the results for all 20 data sets. We observe that the performance of the Siamese CBOW during training has very little fluctuation. There are three exceptions: the performance of the 2014 Deft News dataset is steadily decreasing, while the performance of the 2013 OnWN is steadily increasing, although both data sets seem to stabilize at the end of epoch 5."}, {"heading": "4.2.2 Number of negative examples", "text": "Figure 3 presents the results of the Siamese CBOW with respect to Pearson's r for a different number of negative examples. We observe that the number of negative examples in most groups has a limited impact on the performance of the Siamese CBOW. Choosing a higher number, e.g. 10, occasionally leads to a slightly better performance, e.g. the FNWN set of 2013. However, a small number such as 1 or 2 is typically sufficient and is sometimes significantly better, e.g. in the case of the 2015 beliefs. From a high number of negative examples with significant computational costs, we conclude from the results presented here that although the Siamese CBOW is robust against different settings of this parameter, setting the number of negative examples to 1 or 2 should by default be the right choice."}, {"heading": "4.2.3 Number of dimensions", "text": "Figure 4 shows the results of the Siamese CBOW for different numbers of vector dimensions. It is clear from the figure that for some groups (in particular the Skill Forum of 2014, the Answer Forums of 2015 and the Faith of 2015), the increase in the number of embedding dimensions consistently leads to higher performance. Insufficient dimensionality (50 or 100) inevitably leads to worse results. As similar to a higher number of negative examples, a higher embedding dimension leads to higher computing costs, we conclude from these results that a moderate number of dimensions (200 or 300) is preferable."}, {"heading": "4.3 Time complexity", "text": "The complexity of all the algorithms we are looking at is O (n), i.e., linear in the number of input terms. As in practice the number of arithmetic operations is the decisive factor in determining the computational time, we will now focus on the computational time. both word2vec and the Siamese CBOW calculate embedding of a text T = t1,., t | T by the number of arithmetic operations is the decisive factor in determining the computational time. the number of computational operations is in practice the number of arithmetic operations is the decisive factor in determining the computational time that we concentrate on the computational time. the two word2vec and the Siamese CBOW calculate the embedding of a text T = t1,., t | T by averaging the term bed embedding x. This requires | T | \u2212 1 vector additions and 1 multiplication by an alscary value."}, {"heading": "4.4 Qualitative analysis", "text": "In fact, on the other side of the spectrum, we find many personal pronouns: if they, we, I, he, she, you, she, I, which is natural, since the corpus on which we train is fiction that typically contains dialogues, it is interesting to see what differences in related words exist between Siamese CBOW and word2vec when trained on the same corpus. For example, for a cosmic similarity > 0,6, the words associated with it in the word2vec space are typically dialogues. It is interesting to see what differences in related words exist between Siamese CBOW and word2vec when trained on the same corpus."}, {"heading": "5 Related Work", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "6 Conclusion", "text": "We introduced the Siamese CBOW, a neural network architecture that efficiently learns word embedding optimized to generate sentence representations; the model is trained only on blank text data; it predicts the preceding and subsequent sentence based on an input sentence representation; we evaluated the model on 20 test sentences and show that in most cases, 14 out of 20, the Siamese CBOW outperforms a word2vec baseline and a baseline based on the recently proposed architecture of skip thinking; and since further analysis of various parameters shows that the method is stable across settings, we conclude that the Siamese CBOW provides a robust way to generate high-quality sentence representations. Word and sentence embeddings are ubiquitous and many different ways to use them in monitored tasks have been suggested."}, {"heading": "Acknowledgments", "text": "The authors would like to express their gratitude for the valuable advice and relevant hints provided by the anonymous reviewers. Many thanks to Christophe Van Gysel for the help in implementing this research, which was supported by Ahold, Amsterdam Data Science, the Bloomberg Research Grant Programme, the Dutch national programme COMMIT, Elsevier, the Seventh Framework Programme of the European Community (FP7 / 2007-2013) under funding agreement no. 312827 (VOX-Pol), the ESF Research Network Program ELIAS, the Royal Dutch Academy of Sciences (KNAW) under the Elite Network Shifts project, the Microsoft Research Ph.D. Programme, the Netherlands eScience Center under project no. 027.012.105, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scientific Research (NWO) under project numbers 727.011.005, 612.001.116, HOR-11-10, 640.066.930, CI-1425, SH-22-15, 65.02.0051 or 6.0051 the employers respectively."}], "references": [{"title": "Semeval2012 task 6: A pilot on semantic textual similarity", "author": ["Agirre et al.2012] Eneko Agirre", "Mona Diab", "Daniel Cer", "Aitor Gonzalez-Agirre"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume", "citeRegEx": "Agirre et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity", "author": ["Agirre et al.2013] Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo"], "venue": "In Second Joint Conference on Lexical and Computational", "citeRegEx": "Agirre et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2013}, {"title": "A mediumscale distributed system for computer science research: Infrastructure for the long term", "author": ["Bal et al.2016] Henri Bal", "Dick Epema", "Cees de Laat", "Rob van Nieuwpoort", "John Romein", "Frank Seinstra", "Cees Snoek", "Harry Wijshoff"], "venue": null, "citeRegEx": "Bal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bal et al\\.", "year": 2016}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Phrase similarity in humans and machines", "author": ["Gershman", "Joshua B. Tenenbaum"], "venue": "In Proceedings of the 37th Annual Conference of the Cognitive Science Society,", "citeRegEx": "Gershman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gershman et al\\.", "year": 2015}, {"title": "Multilingual distributed representations without word alignment", "author": ["Hermann", "Phil Blunsom"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Hermann", "Phil Blunsom"], "venue": "In Proceeedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Hill et al.2016] Felix Hill", "Kyunghyun Cho", "Anna Korhonen"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "In Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Short text similarity with word embeddings", "author": ["Kenter", "de Rijke2015] Tom Kenter", "Maarten de Rijke"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM", "citeRegEx": "Kenter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kenter et al\\.", "year": 2015}, {"title": "Ad hoc monitoring of vocabulary shifts over time", "author": ["Kenter et al.2015] Tom Kenter", "Melvin Wevers", "Pim Huijnen", "Maarten de Rijke"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM", "citeRegEx": "Kenter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kenter et al\\.", "year": 2015}, {"title": "Temporal analysis of language through neural language models", "author": ["Kim et al.2014] Yoon Kim", "I Yi-Chiu", "Kentaro Hanaki", "Darshan Hegde", "Slav Petrov"], "venue": "Proceeedings of the 52nd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "From word embeddings to document distances", "author": ["Kusner et al.2015] Matt Kusner", "Yu Sun", "Nicholas Kolkin", "Kilian Q Weinberger"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML", "citeRegEx": "Kusner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kusner et al\\.", "year": 2015}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C Raykar", "Amrita Saha"], "venue": "In Advances in Neural Information Processing Sys-", "citeRegEx": "Lauly et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lauly et al\\.", "year": 2014}, {"title": "Ntnu: Measuring semantic similarity with sublexical feature representations and soft cardinality", "author": ["Lynum et al.2014] Andr\u00e9 Lynum", "Partha Pakray", "Bj\u00f6rn Gamb\u00e4ck", "Sergio Jimenez"], "venue": "In Proceedings of the 8th International Workshop on Semantic Eval-", "citeRegEx": "Lynum et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lynum et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg S. Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems (NIPS 2013),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Mining, ranking and recommending entity aspects", "author": ["Edgar Meij", "Maarten de Rijke"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "Reinanda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reinanda et al\\.", "year": 2015}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["Severyn", "Moschitti2015] Aliaksei Severyn", "Alessandro Moschitti"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development", "citeRegEx": "Severyn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2015}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Learning to explain entity relationships in knowledge graphs", "author": ["Edgar Meij", "Manos Tsagkias", "Maarten de Rijke", "Wouter Weerkamp"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Voskarides et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Voskarides et al\\.", "year": 2015}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["Wieting et al.2016] John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "Proceedings of the International Conference on Learning Representations", "citeRegEx": "Wieting et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}, {"title": "Learning discriminative projections for text similarity measures", "author": ["Yih et al.2011] Wentau Yih", "Kristina Toutanova", "John C. Platt", "Christopher Meek"], "venue": "In Proceedings of the Fifteenth Conference on Computational Natural Language", "citeRegEx": "Yih et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2011}, {"title": "Convolutional neural network for paraphrase identification", "author": ["Yin", "Sch\u00fctze2015] Wenpeng Yin", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL 2015),", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Deep learning for answer sentence selection", "author": ["Yu et al.2014] Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman"], "venue": "In NIPS 2014 Deep Learning and Representation Learning Workshop", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Zhu et al.2015] Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y. Zou", "Richard Socher", "Daniel M. Cer", "Christopher D. Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 28, "context": "Word embeddings have proven to be beneficial in a variety of tasks in NLP such as machine translation (Zou et al., 2013), parsing (Chen and Manning, 2014), semantic search (Reinanda et al.", "startOffset": 102, "endOffset": 120}, {"referenceID": 19, "context": ", 2013), parsing (Chen and Manning, 2014), semantic search (Reinanda et al., 2015; Voskarides et al., 2015), and tracking the meaning of words and concepts over time (Kim et al.", "startOffset": 59, "endOffset": 107}, {"referenceID": 22, "context": ", 2013), parsing (Chen and Manning, 2014), semantic search (Reinanda et al., 2015; Voskarides et al., 2015), and tracking the meaning of words and concepts over time (Kim et al.", "startOffset": 59, "endOffset": 107}, {"referenceID": 13, "context": ", 2015), and tracking the meaning of words and concepts over time (Kim et al., 2014; Kenter et al., 2015).", "startOffset": 66, "endOffset": 105}, {"referenceID": 11, "context": ", 2015), and tracking the meaning of words and concepts over time (Kim et al., 2014; Kenter et al., 2015).", "startOffset": 66, "endOffset": 105}, {"referenceID": 5, "context": "Surprisingly, simply averaging word embeddings of all words in a text has proven to be a strong baseline or feature across a multitude of tasks (Faruqui et al., 2014; Yu et al., 2014; Gershman and Tenenbaum, 2015; Kenter and de Rijke, 2015).", "startOffset": 144, "endOffset": 240}, {"referenceID": 26, "context": "Surprisingly, simply averaging word embeddings of all words in a text has proven to be a strong baseline or feature across a multitude of tasks (Faruqui et al., 2014; Yu et al., 2014; Gershman and Tenenbaum, 2015; Kenter and de Rijke, 2015).", "startOffset": 144, "endOffset": 240}, {"referenceID": 21, "context": ", 2013b) are employed across different tasks (Socher et al., 2012; Kenter and de Rijke, 2015; Hu et al., 2014).", "startOffset": 45, "endOffset": 110}, {"referenceID": 10, "context": ", 2013b) are employed across different tasks (Socher et al., 2012; Kenter and de Rijke, 2015; Hu et al., 2014).", "startOffset": 45, "endOffset": 110}, {"referenceID": 9, "context": "We apply this strategy at the sentence level, where we aim to predict a sentence from its adjacent sentences (Kiros et al., 2015; Hill et al., 2016).", "startOffset": 109, "endOffset": 148}, {"referenceID": 5, "context": "Surprisingly, simply averaging word embeddings of all words in a text has proven to be a strong baseline or feature across a multitude of tasks (Faruqui et al., 2014; Yu et al., 2014; Gershman and Tenenbaum, 2015; Kenter and de Rijke, 2015). Word embeddings, however, are not optimized specifically for representing sentences. In this paper we present a model for obtaining word embeddings that are tailored specifically for the task of averaging them. We do this by directly including a comparison of sentence embeddings\u2014the averaged embeddings of the words they contain\u2014in the cost function of our network. Word embeddings are typically trained in a fast and scalable way from unlabeled training data. As the training data is unlabeled, word embeddings are usually not task-specific. Rather, word embeddings trained on a large training corpus, like the ones from (Collobert and Weston, 2008; Mikolov et al., 2013b) are employed across different tasks (Socher et al., 2012; Kenter and de Rijke, 2015; Hu et al., 2014). These two qualities\u2014(i) being trainable from large quantities of unlabeled data in a reasonable amount of time, and (ii) robust performance across different tasks\u2014are highly desirable and allow word embeddings to be used in many large-scale applications. In this work we aim to optimize word embeddings for sentence representations in the same manner. We want to produce general purpose sentence embeddings that should score robustly across multiple test sets, and we want to leverage large amounts of unlabeled training material. In the word2vec algorithm, Mikolov et al. (2013a) construe a supervised training criterion for obtaining word embeddings from unsupervised data, by predicting, for every word, its surrounding words.", "startOffset": 145, "endOffset": 1601}, {"referenceID": 27, "context": "(Zhu et al., 2015).", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "We use 20 SemEval datasets from the SemEval semantic textual similarity task in 2012, 2013, 2014 and 2015 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), which consist of sentence pairs from a wide array of sources (e.", "startOffset": 106, "endOffset": 190}, {"referenceID": 1, "context": "We use 20 SemEval datasets from the SemEval semantic textual similarity task in 2012, 2013, 2014 and 2015 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), which consist of sentence pairs from a wide array of sources (e.", "startOffset": 106, "endOffset": 190}, {"referenceID": 14, "context": "To comply with results reported in other research (Mikolov et al., 2013b; Kusner et al., 2015) we fix the embedding size to 300 and only consider words appearing 5 times or more in the training corpus.", "startOffset": 50, "endOffset": 94}, {"referenceID": 2, "context": "4 We ran our experiments on GPUs in the DAS5 cluster (Bal et al., 2016).", "startOffset": 53, "endOffset": 71}, {"referenceID": 16, "context": "This is remarkable as Siamese CBOW is completely unsupervised, while the NTNU system which scored best on this set (Lynum et al., 2014) was optimized using multiple training sets.", "startOffset": 115, "endOffset": 135}, {"referenceID": 9, "context": "In recent work, Hill et al. (2016) present FastSent, a model similar to ours (see \u00a75 for a more elaborate discussion); results are not reported for all evaluation sets we use, and hence, we compare the results of FastSent and Siamese CBOW separately, in Table 2.", "startOffset": 16, "endOffset": 35}, {"referenceID": 9, "context": "The comparison is to be interpreted with caution as it is not evident what vocabulary was used for the experiments in (Hill et al., 2016); hence, the differences observed here might simply be due to differences in vocabulary coverage.", "startOffset": 118, "endOffset": 137}, {"referenceID": 9, "context": "FastSent results are reprinted from (Hill et al., 2016) where they are reported in two-digit precision.", "startOffset": 36, "endOffset": 55}, {"referenceID": 10, "context": "In the first setting, word vectors are typically used as features or network initialisations (Kenter and de Rijke, 2015; Hu et al., 2014; Severyn and Moschitti, 2015; Yin and Sch\u00fctze, 2015).", "startOffset": 93, "endOffset": 189}, {"referenceID": 15, "context": "Many models related to the one we present here are used in a multilingual setting (Hermann and Blunsom, 2014b; Hermann and Blunsom, 2014a; Lauly et al., 2014).", "startOffset": 82, "endOffset": 158}, {"referenceID": 10, "context": "In the first setting, word vectors are typically used as features or network initialisations (Kenter and de Rijke, 2015; Hu et al., 2014; Severyn and Moschitti, 2015; Yin and Sch\u00fctze, 2015). Our work can be classified in the latter category of unsupervised approaches. Many models related to the one we present here are used in a multilingual setting (Hermann and Blunsom, 2014b; Hermann and Blunsom, 2014a; Lauly et al., 2014). The key difference between this work and ours is that in a multilingual setting the goal is to predict, from a distributed representation of an input sentence, the same sentence in a different language, whereas our goals is to predict surrounding sentences. Wieting et al. (2016) apply a model similar to ours in a related but different setting where explicit semantic knowledge is leveraged.", "startOffset": 121, "endOffset": 709}, {"referenceID": 9, "context": "Most importantly, however, the cost function used in (Hill et al., 2016) is crucially different from ours.", "startOffset": 53, "endOffset": 72}, {"referenceID": 9, "context": "Finally, independently from our work, Hill et al. (2016) also present a log-linear model.", "startOffset": 38, "endOffset": 57}, {"referenceID": 9, "context": "Finally, independently from our work, Hill et al. (2016) also present a log-linear model. Rather than comparing sentence representations to each other, as we propose, words in one sentence are compared to the representation of another sentence. As both input and output vectors are learnt, while we tie the parameters across the entire model, Hill et al. (2016)\u2019s model has twice as many parameters as ours.", "startOffset": 38, "endOffset": 362}], "year": 2016, "abstractText": "We present the Siamese Continuous Bag of Words (Siamese CBOW) model, a neural network for efficient estimation of highquality sentence embeddings. Averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way of obtaining sentence embeddings. However, word embeddings trained with the methods currently available are not optimized for the task of sentence representation, and, thus, likely to be suboptimal. Siamese CBOW handles this problem by training word embeddings directly for the purpose of being averaged. The underlying neural network learns word embeddings by predicting, from a sentence representation, its surrounding sentences. We show the robustness of the Siamese CBOW model by evaluating it on 20 datasets stemming from a wide variety of sources.", "creator": "LaTeX with hyperref package"}}}