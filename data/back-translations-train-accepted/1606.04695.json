{"id": "1606.04695", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Strategic Attentive Writer for Learning Macro-Actions", "abstract": "We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.", "histories": [["v1", "Wed, 15 Jun 2016 09:28:52 GMT  (1653kb,D)", "http://arxiv.org/abs/1606.04695v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["alexander vezhnevets", "volodymyr mnih", "simon osindero", "alex graves", "oriol vinyals", "john agapiou", "koray kavukcuoglu"], "accepted": true, "id": "1606.04695"}, "pdf": {"name": "1606.04695.pdf", "metadata": {"source": "CRF", "title": "Strategic Attentive Writer for Learning Macro-Actions", "authors": ["Alexander (Sasha", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu"], "emails": ["vezhnick@google.com", "vmnih@google.com", "jagapiou@google.com", "osindero@google.com", "gravesa@google.com", "vinyals@google.com", "korayk@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, we are able to assert ourselves, we are able to assert ourselves, we are able to assert ourselves in the world."}, {"heading": "2 Related Work", "text": "Learning temporally extended actions and temporal abstraction in generic terms are longstanding problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28]. The framework of options [21, 28] provides a general formulation. One option is a sub-policy with a termination condition that includes environmental observations and issues actions until the termination condition is met. An actor selects an option using his policy-over-options and then follows it until the termination, at which the policy-over-options is re-examined and the process continues. It is noteworthy that macro-actions are a special, simpler instance of options, in which the action sequence (or the distribution over them) is decided at the point at which the macro-over-action is initiated. Options are typically learned using subtargets and \"pseudo-rewards\" that are explicitly provided in politics [7, 8, 28] being considered as a problem-over-options, by learning about options-over-practice."}, {"heading": "3 The model", "text": "The first module translates the environmental observations into an action plan that includes a number of steps; the second module maintains an action framework that determines the step in which the network terminates a macro action and updates the action plan; the action plan is a matrix in which one dimension corresponds to time and the other to the amount of possible discrete actions; the elements of this matrix are proportional to the likelihood of the corresponding action in the corresponding time; and the action plan represents the probabilities of the completion of a macro action in the respective step. To update both plans, we use an attention-oriented writing technique [10] that allows us to focus on parts of a plan in which the current observation is informative."}, {"heading": "3.1 Structured exploration with macro-actions", "text": "The architecture defined above is capable of generating macro actions. This section describes how macro actions are used for structured exploration. We introduce STRAW Explorer (STRAWe), a version of STRAW with a loud communication channel between the feature extractor (e.g. a Convolutionary Neural Network) and the STRAW scheduling modules. Let's not consider activating the last layer of the feature extractor (STRAW) as activation. We use STRAW to undo the parameters of a Gaussian distribution Q (zt-t) = N (\u00b5-t) that zt scans, which is a vector and a scalar. Injection of noise at this level of the network generates randomness at the level of plan updates covering several time steps. This effect is amplified by commitment that forces the STRAWe to execute the plan and experience the result, rather than having the update scroll back to the next level of play as we do in the ac5 and actz."}, {"heading": "4 Learning", "text": "The loss of formation of the model is defined as: L = T \u2211 t (Lout (At) + 1gt \u00b7 \u03b1KL (Q (xt)) | P (zt)) + \u03bbct [t]))), (3) where Lout is a domain-specific, differentiable loss function defined by the output of the network. For monitored problems, such as the next prediction in the text, Lout can be defined as a negative logging probability of the correct output. We will discuss the case of amplification later in this section. The two additional terms are regulators. The first is a communication cost through the noisy channel, which is defined as a KL divergence between latent distributions Q (zt) and some preceding P (zt). Since the latent distribution is a Gaussian (sec. 3.1), a natural choice for the previous one is a Gaussian with a mean and standard deviation from one."}, {"heading": "5 Experiments", "text": "The aim of our experiments was to show that STRAW learns meaningful and useful macro actions. We use three areas of increasing complexity: supervised next character prediction in the text [9], 2D labyrinth navigation and ATARI games [2]."}, {"heading": "5.1 Experimental setup", "text": "The time horizon T = 500. For STRAWe (sec. 3.1) the Gaussian distribution for structured exploration is 128-dimensional. Ablative analysis for some of these decisions is provided in Section 5.5. Characteristic representation of state space is of particular importance for each domain. For 2D labyrinths and ATARI it is a Convolutionary Neural Network (CNN) and it is an LSTM [11] for text. We provide more details in the corresponding sections. The experiments use two bases - a simple feed network (FF) and a recursive LSTM network (CNN) and it is an LSTM [11] for text. We provide more details in the corresponding sections. The experiments use two bases - a simple feed network (FF) and a recursive LSTM network. FF directly reduces the probabilities of action and value estimates from the representation of features. The LSTM [11] architecture is a widely used feed network (FF) and a recursive LSTM network (CNN). FF directly reduces the value estimates from the representation of features."}, {"heading": "5.2 Text", "text": "STRAW is a general sequence prediction architecture. To show that STRAW is able to learn output patterns with complex structure, we present a qualitative experiment to the next character prediction using the Penn Treebank dataset [16]. Actions in this case correspond to the emission of characters and macro actions to their sequences. For this experiment, we use an LSTM that receives a one-time hot encoding of 50 characters as input. A STRAW module is connected at the top. We omit the noisy Gaussian channel as this task is fully monitored and does not require research. the actions now correspond to the emission of characters. The network is trained with stochastic gradient decrease using a monitored negative log probability loss (Figure 4). At each step, we feed a character into the model that updates the LSTM representation but only updates the STRAW plans according to the connection plan."}, {"heading": "5.3 2D mazes", "text": "In fact, it is so that most of them are able to move, and that they are able to move themselves, are able to achieve their goals. Are able to achieve their goals. Are not able to achieve their goals. Are not able to achieve their goals. Are not able to achieve their goals. Are not able to achieve their goals. Are not able to achieve their goals. Are not able to achieve their goals. Are not able to achieve their goals. Are able to achieve their goals. Are able to achieve their goals. Are able to achieve their goals. Are able to achieve their goals. Are not able to achieve their goals. Are not able to, and are able to achieve their goals. Are able to achieve their goals. Are able to achieve their goals."}, {"heading": "5.4 ATARI", "text": "In fact, it is such that the greater number of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "5.5 Ablative analysis", "text": "Figure 6 shows the performance curves of the different STRAW versions of Mrs. Pacman's 100-epoch-trained game. From left to right, the first figure shows the STRAW performance at different resolution of the action site. The higher it is, the more complex the update that STRAW can generate for the action plan. The second figure shows the influence of the dimensionality of the Gaussian channel used for structured exploration (see 3.1). Results show that a higher dimensionality is generally advantageous, but these benefits are quickly saturated. In the third figure, we examine various possible options for the rescheduling mechanism: We compare STRAW with two simple modifications, one rescheduling each step, the other committing to the plan for a random number of steps between 0 and 4. Rescheduling each step is not only less elegant, but also more costly and less efficient from a computational point of view. Results show that learning when the plan commits to adhere, when and when it agrees."}, {"heading": "6 Conclusion", "text": "We introduced the STRategic Attentive Writer architecture (STRAW) and demonstrated its ability to learn implicitly useful time-abstracted macro actions in an end-to-end manner. In addition, STRAW advances the state of the art in several challenging Atari areas that require time-extended planning and exploration strategies, and also has the ability to learn time abstractions in general sequence prediction, opening up a fruitful new direction in addressing an important problem area for sequential decision-making and AI more broadly."}, {"heading": "7 Acknowledgements", "text": "We thank David Silver, Joseph Modayil and Nicolas Heess for many helpful discussions, suggestions and comments on the essay."}], "references": [{"title": "The option-critic architecture", "author": ["Pierre-Luc Bacon", "Doina Precup"], "venue": "In NIPS Deep RL Workshop,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Yoshua Bengio", "Nicholas L\u00e9onard", "Aaron Courville"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective", "author": ["Matthew M Botvinick", "Yael Niv", "Andrew C Barto"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Prioritized goal decomposition of markov decision processes: Toward a synthesis of classical and decision theoretic planning", "author": ["Craig Boutilier", "Ronen I Brafman", "Christopher Geib"], "venue": "In IJCAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Improving generalization for temporal difference learning: The successor representation", "author": ["Peter Dayan"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1993}, {"title": "Feudal reinforcement learning", "author": ["Peter Dayan", "Geoffrey E Hinton"], "venue": "In NIPS. Morgan Kaufmann Publishers,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["Thomas G Dietterich"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Rezende", "Daan Wierstra"], "venue": "In ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Hierarchical learning in stochastic domains: Preliminary results", "author": ["Leslie Pack Kaelbling"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "ICLR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D. Kulkarni", "Karthik R. Narasimhan", "Ardavan Saeedi", "Joshua B. Tenenbaum"], "venue": "arXiv preprint arXiv:1604.06057,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "A focused back-propagation algorithm for temporal pattern recognition", "author": ["Michael C Mozer"], "venue": "Complex systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1989}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["Ronald Parr", "Stuart Russell"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Temporal abstraction in reinforcement learning", "author": ["Doina Precup"], "venue": "PhD thesis, University of Massachusetts,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Planning with closed-loop macro actions", "author": ["Doina Precup", "Richard S Sutton", "Satinder P Singh"], "venue": "Technical report,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Theoretical results on reinforcement learning with temporally abstract options", "author": ["Doina Precup", "Richard S Sutton", "Satinder Singh"], "venue": "In European Conference on Machine Learning (ECML). Springer,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Neural sequence chunkers", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Technical report,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1991}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Td models: Modeling the world at a mixture of time scales", "author": ["Richard S Sutton"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1995}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["Chen Tessler", "Shahar Givony", "Tom Zahavy", "Daniel J Mankowitz", "Shie Mannor"], "venue": "arXiv preprint arXiv:1604.07255,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "Using reinforcement learning to train neural network controllers has recently led to rapid progress on a number of challenging control tasks [15, 17, 26].", "startOffset": 141, "endOffset": 153}, {"referenceID": 16, "context": "Using reinforcement learning to train neural network controllers has recently led to rapid progress on a number of challenging control tasks [15, 17, 26].", "startOffset": 141, "endOffset": 153}, {"referenceID": 25, "context": "Using reinforcement learning to train neural network controllers has recently led to rapid progress on a number of challenging control tasks [15, 17, 26].", "startOffset": 141, "endOffset": 153}, {"referenceID": 14, "context": "Unlike the vast majority of reinforcement learning approaches [15, 17, 26], which output a single action after each observation, STRAW maintains a multi-step action plan.", "startOffset": 62, "endOffset": 74}, {"referenceID": 16, "context": "Unlike the vast majority of reinforcement learning approaches [15, 17, 26], which output a single action after each observation, STRAW maintains a multi-step action plan.", "startOffset": 62, "endOffset": 74}, {"referenceID": 25, "context": "Unlike the vast majority of reinforcement learning approaches [15, 17, 26], which output a single action after each observation, STRAW maintains a multi-step action plan.", "startOffset": 62, "endOffset": 74}, {"referenceID": 9, "context": "convolutional neural network) and the planning modules, taking inspiration from recent developments in variational auto-encoders [10, 13, 24].", "startOffset": 129, "endOffset": 141}, {"referenceID": 12, "context": "convolutional neural network) and the planning modules, taking inspiration from recent developments in variational auto-encoders [10, 13, 24].", "startOffset": 129, "endOffset": 141}, {"referenceID": 23, "context": "convolutional neural network) and the planning modules, taking inspiration from recent developments in variational auto-encoders [10, 13, 24].", "startOffset": 129, "endOffset": 141}, {"referenceID": 3, "context": "First and foremost, it facilitates structured exploration in reinforcement learning \u2013 as the network learns meaningful action patterns it can use them to make longer exploratory steps in the state space [4].", "startOffset": 203, "endOffset": 206}, {"referenceID": 4, "context": "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].", "startOffset": 125, "endOffset": 169}, {"referenceID": 5, "context": "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].", "startOffset": 125, "endOffset": 169}, {"referenceID": 6, "context": "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].", "startOffset": 125, "endOffset": 169}, {"referenceID": 7, "context": "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].", "startOffset": 125, "endOffset": 169}, {"referenceID": 11, "context": "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].", "startOffset": 125, "endOffset": 169}, {"referenceID": 19, "context": "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].", "startOffset": 125, "endOffset": 169}, {"referenceID": 20, "context": "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].", "startOffset": 125, "endOffset": 169}, {"referenceID": 21, "context": "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].", "startOffset": 125, "endOffset": 169}, {"referenceID": 22, "context": "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].", "startOffset": 125, "endOffset": 169}, {"referenceID": 24, "context": "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].", "startOffset": 125, "endOffset": 169}, {"referenceID": 26, "context": "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].", "startOffset": 125, "endOffset": 169}, {"referenceID": 27, "context": "Learning temporally extended actions and temporal abstraction in genral are long standing problems in reinforcement learning [5, 6, 7, 8, 12, 20, 21, 22, 23, 25, 27, 28].", "startOffset": 125, "endOffset": 169}, {"referenceID": 20, "context": "The options framework [21, 28] provides a general formulation.", "startOffset": 22, "endOffset": 30}, {"referenceID": 27, "context": "The options framework [21, 28] provides a general formulation.", "startOffset": 22, "endOffset": 30}, {"referenceID": 6, "context": "Options are typically learned using subgoals and \u2019pseudo-rewards\u2019 that are provided explicitly [7, 8, 28].", "startOffset": 95, "endOffset": 105}, {"referenceID": 7, "context": "Options are typically learned using subgoals and \u2019pseudo-rewards\u2019 that are provided explicitly [7, 8, 28].", "startOffset": 95, "endOffset": 105}, {"referenceID": 27, "context": "Options are typically learned using subgoals and \u2019pseudo-rewards\u2019 that are provided explicitly [7, 8, 28].", "startOffset": 95, "endOffset": 105}, {"referenceID": 13, "context": "Recently [14, 29] have demonstrated that combining deep learning with pre-defined subgoals delivers promising results in challenging environments like Minecraft and Atari, however, subgoal discovery remains an unsolved problem.", "startOffset": 9, "endOffset": 17}, {"referenceID": 28, "context": "Recently [14, 29] have demonstrated that combining deep learning with pre-defined subgoals delivers promising results in challenging environments like Minecraft and Atari, however, subgoal discovery remains an unsolved problem.", "startOffset": 9, "endOffset": 17}, {"referenceID": 0, "context": "Another recent work by [1] shows a theoretical possibility of learning options jointly with a policy-over-options by extending the policy gradient theorem to options, but the approach was only tested on a toy problem.", "startOffset": 23, "endOffset": 26}, {"referenceID": 9, "context": "For updating both plans we use attentive writing technique [10], which allows the network to focus on parts of a plan", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "To achieve this, we apply differentiable attentive reading and writing operations [10], where attention is defined over the temporal dimension.", "startOffset": 82, "endOffset": 86}, {"referenceID": 9, "context": "The differentiability of the attention model [10] makes it possible to train with standard backpropagation.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "We direct readers to [10] for details.", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "We use a recently proposed Asynchronous Advantage Actor-Critic (A3C) method [18], which directly optimizes the policy of an agent.", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "For zt we employ the re-parametrization trick [13, 24].", "startOffset": 46, "endOffset": 54}, {"referenceID": 23, "context": "For zt we employ the re-parametrization trick [13, 24].", "startOffset": 46, "endOffset": 54}, {"referenceID": 2, "context": "For gt, we set\u2207ct\u22121 1 \u2261 \u2207gt as proposed in [3].", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "We use three domains of increasing complexity: supervised next character prediction in text [9], 2D maze navigation and ATARI games [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "We use three domains of increasing complexity: supervised next character prediction in text [9], 2D maze navigation and ATARI games [2].", "startOffset": 132, "endOffset": 135}, {"referenceID": 10, "context": "For 2D mazes and ATARI it is a convolutional neural net (CNN) and it is an LSTM [11] for text.", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "The LSTM [11] architecture is a widely used recurrent network and it was demonstrated to perform well on a suite of reinforcement learning problems [18].", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "The LSTM [11] architecture is a widely used recurrent network and it was demonstrated to perform well on a suite of reinforcement learning problems [18].", "startOffset": 148, "endOffset": 152}, {"referenceID": 17, "context": "We use the A3C method [18] for all reinforcement learning experiments.", "startOffset": 22, "endOffset": 26}, {"referenceID": 17, "context": "It was shown to achieve state-of-the-art results on several challenging benchmarks [18].", "startOffset": 83, "endOffset": 87}, {"referenceID": 18, "context": "We cut the trajectory and run backpropagation through time [19] after 40 forward passes of a network or if a terminal signal is received.", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "To demonstrate that it is capable of learning output patterns with complex structure we present a qualitative experiment on next character prediction using Penn Treebank dataset [16].", "startOffset": 178, "endOffset": 182}, {"referenceID": 16, "context": "This is the same architecture as in [17, 18], the only difference is that in pre-processing stage we keep colour channels.", "startOffset": 36, "endOffset": 44}, {"referenceID": 17, "context": "This is the same architecture as in [17, 18], the only difference is that in pre-processing stage we keep colour channels.", "startOffset": 36, "endOffset": 44}], "year": 2016, "abstractText": "We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous subsequences by learning for how long the plan can be committed to \u2013 i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macroactions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macroactions), demonstrating the generality of the approach.", "creator": "LaTeX with hyperref package"}}}