{"id": "1606.04753", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Safe Exploration in Finite Markov Decision Processes with Gaussian Processes", "abstract": "In classical reinforcement learning, when exploring an environment, agents accept arbitrary short term loss for long term gain. This is infeasible for safety critical applications, such as robotics, where even a single unsafe action may cause system failure. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an, a priori unknown, safety constraint that depends on states and actions. We aim to explore the MDP under this constraint, assuming that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm for this task and prove that it is able to completely explore the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.", "histories": [["v1", "Wed, 15 Jun 2016 13:18:30 GMT  (505kb,D)", "http://arxiv.org/abs/1606.04753v1", "9 pages, extended version with proofs"], ["v2", "Tue, 15 Nov 2016 14:00:11 GMT  (506kb,D)", "http://arxiv.org/abs/1606.04753v2", "15 pages, extended version with proofs"]], "COMMENTS": "9 pages, extended version with proofs", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO stat.ML", "authors": ["matteo turchetta", "felix berkenkamp", "andreas krause 0001"], "accepted": true, "id": "1606.04753"}, "pdf": {"name": "1606.04753.pdf", "metadata": {"source": "CRF", "title": "Safe Exploration in Finite Markov Decision Processes with Gaussian Processes", "authors": ["Matteo Turchetta", "Felix Berkenkamp"], "emails": ["matteotu@ethz.ch", "befelix@ethz.ch", "krausea@ethz.ch"], "sections": [{"heading": null, "text": "This is not feasible for safety-critical applications, such as robotics, where even a single unsafe act can lead to system failures. In this paper, we address the problem of safe exploration of Markov finite decision-making processes (MDP). We define security in the sense of a previously unknown security constraint that depends on states and actions. We intend to investigate the MDP under this condition, provided that the unknown function fulfills regularity conditions previously expressed through a Gaussian process. We are developing a novel algorithm for this task and prove that it is capable of fully exploring the safely accessible part of the MDP without violating safety constraints. To achieve this, the algorithm carefully examines safe states and actions to gain statistical confidence in the safety of unattended governmental pairs of shareholders collected while navigating the environment. Furthermore, the algorithm explicitly takes into account the accessibility of the exploration of the known rain in order to ensure that our MDP does not enter an unsafe state."}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is in which there is in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is in which there is in which there is a process in which there is a process in which there is in which there is in which there is in which there is a process in which there is a process in which there is in which there is in which there is a process in which there is in which there is in which there is in which there is a process in which there is in which there is in which there is in which there is in which there is a process in which there is"}, {"heading": "2 Problem Statement", "text": "In this section, we define our problem and our assumptions. The unknown environment is modeled as finite Markov decision-making processes (MDP). A finite MDP is a tuple M = < S, A, f (s, a), r (s, a) > with finite states and spaces of action, S and A, each a known deterministic transition model, f (s, a), and reward function, r (s, a). However, in the typical amplification framework, reward is a known function that encodes desirable states. In this paper, we consider the problem of safe exploration of the MDP and therefore do not aim to maximize reward. Instead, we define reward as a primarily unknown safety feature. Although r (s, a) is unknown, we assume that the assumptions regarding the regularity of the problem are understandable."}, {"heading": "3 Algorithm", "text": "The algorithms rely on a GP model of r to make predictions about the security feature and to use the probable uncertainty to ensure safety, there must be two sets. The first group, St, contains all states that can be classified as safe, while the second group, S, additionally considers the ability to reach points in St and the ability to safely return to the previous safe environment, S, - 1. The algorithm ensures safety and ergodicity by selecting only visiting states in S to expand the safe region, the algorithm visits to Gt S, a number of candidate states that, if visited, could expand the safe facility. Specifically, the algorithm selects the most unsafe state in Gt, which represents the safest state that we can gain most information about this state."}, {"heading": "4 Theoretical Results", "text": "The security and exploration aspects of the algorithm that we have presented in the previous section are due to the correctness of the confidence intervals Ct (s). Specifically, they require that the true value of the security characteristics, r (s), within Ct (s), with high probability for all s (s) S and all iterations t > 0 (s).Furthermore, these confidence intervals must shrink sufficiently quickly over time. The probability that we will adopt values within the confidence intervals depends on the scaling factor \u03b2t. This scaling factor trades conservatism in exploration for the probability of uncertain states being visited. Appropriate selection of \u03b2t was justified by Srinivas et al. [21] in the configuration of our framework conditions is different, but its setting can be applied to our case. We choose, \u00df = 2B + 300g (t / g), where justified."}, {"heading": "5 Experiments", "text": "In this context, the question is how could such an outbreak have occurred, such a disaster, such a disaster, such a disaster, such a disaster, such a disaster, such a disaster, such a disaster, such a disaster, such a disaster, such a disaster, such a disaster, such a disaster, such a disaster, such a disaster, such a disaster, such a disinterest, such a disinterest, such a disinterest in culture, such a disinterest in culture, such a disinterest in culture, such a disinterest in culture, such a disinterest in culture, disinterest in culture, disinterest in culture."}, {"heading": "6 Conclusion", "text": "An important aspect of the algorithm is that it takes into account the transition dynamics of the MDP to ensure that there is a safe way back before visiting states. We have proven that the algorithm is capable of exploring the entire safely accessible region with a few measurements, and has proven its practicability and performance in experiments."}, {"heading": "A Preliminary lemmas", "text": "Lemma 3 (S, S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S (S).S (S).S (S).S (S (S).S (S).S (S (S).S (S).S (S (S).S (S).S (S (S).S (S).S (S (S).S (S (S).S (S).S (S (S).S (S (S).S (S).S (S (S).S (S (S).S (S (S).S (S).S (S (S).S (S (S).S (S).S (S).S (S (S).S (S).S (S (S).S (S).S (S (S).S (S).S).S (S (S).S (S).S (S (S).S (S).S (S).S).S (S (S).S (S (S).S (S).S)."}, {"heading": "B Safety", "text": "Lemma. (a1, a2,., ak) (s0, s1, s1, s1,.) (s0, s1, s1, s1, s1, s1, s1, sj, s1, s0, s0, s1, s1, s1, s1, s1, s1, s1, sj, s1, sj, s1, sl, sj, s1, sc, sc, sc, sc, sc, sc, sc, sc, s1, sj, s1, s1, s1, s1, s1, s1, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, s1, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc, sc,"}, {"heading": "C Completeness", "text": "Lemma (s) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s) n (n (n) n (n) s (n) n (n) n (n) n (n (n) n (s) n (n (n) n (n (s) n (n (n (n) n (n (n) n (n (n) n (n (n (n) n (n (n) n (n (n (n) n (n (n) n (n (n) n (n (n (n) n (n (n) n (n (n (n) n (n (n) n (n (n (n) n (n (n) n (n (n) n (n (n (n (n (n) n (n (n) n (n (n (n) n (n) n) n (n (n) n (n (n (n (n (n (n (n) n) n (n (n) n) n (n (n (n) n (n (n (n) n (n) n (n (n) n (n (n (n (n) n) n (n) n (n) n (n) n (n (n) n) n (n) n (n (n (n) n (n (n (n) n (n) n) n (n (n) n (n (n (n) n (n) n) n) n (n) n) n (n (n (n) n (n (n) n) n) n"}, {"heading": "D Main result", "text": "Theorem 1. Let us assume that r L-Lipschitz is continuous and that the assumptions of Lemma 1 are valid. Let us also assume that S0 6 = \u2205, r (s) \u2265 h for all s-S0, and that for any two states, s, s-S0, s-Rret (S0, {s}). Let us choose \u03b2t as in (8). Let us specify (s0, s1,., sk,..) a state course induced by algorithm 1 on an MDP with transition function f (s, a). Then with probability at least 1 \u2212 3, r (sk) \u2265 h, \u0441k > 0.Furthermore, let us not be the smallest integer, so that it is not vice versa."}], "references": [{"title": "Reachability-based safe learning with Gaussian processes", "author": ["Anayo K. Akametalu", "Shahab Kaynama", "Jaime F. Fisac", "Melanie N. Zeilinger", "Jeremy H. Gillula", "Claire J. Tomlin"], "venue": "In Proc. of the IEEE Conference on Decision and Control (CDC),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A survey of robot learning from demonstration", "author": ["Brenna D. Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Safe and robust learning control with Gaussian processes", "author": ["Felix Berkenkamp", "Angela P. Schoellig"], "venue": "In Proc. of the European Control Conference (ECC),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Safe controller optimization for quadrotors with Gaussian processes", "author": ["Felix Berkenkamp", "Angela P. Schoellig", "Andreas Krause"], "venue": "In Proc. of the IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Risk-sensitive and minimax control of discrete-time, finite-state Markov decision processes", "author": ["Stefano P. Coraluppi", "Steven I. Marcus"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Safe exploration of state and action spaces in reinforcement learning", "author": ["Javier Garcia", "Fernando Fern\u00e1ndez"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Risk-sensitive reinforcement learning applied to control under constraints", "author": ["Peter Geibel", "Fritz Wysotzki"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Posterior consistency of Gaussian process prior for nonparametric binary regression", "author": ["Subhashis Ghosal", "Anindya Roy"], "venue": "The Annals of Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Safe exploration for reinforcement learning", "author": ["Alexander Hans", "Daniel Schneega\u00df", "Anton Maximilian Sch\u00e4fer", "Steffen Udluft"], "venue": "In Proc. of the European Symposium on Artificial Neural Networks (ESANN),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Reinforcement learning in robotics: a survey", "author": ["Jens Kober", "J. Andrew Bagnell", "Jan Peters"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Introduction: Mars Science Laboratory: The Next Generation of Mars Landers", "author": ["Mary Kae Lockwood"], "venue": "Journal of Spacecraft and Rockets,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Mars Reconnaissance Orbiter\u2019s High Resolution Imaging Science Experiment (HiRISE)", "author": ["Alfred S. McEwen", "Eric M. Eliason", "James W. Bergstrom", "Nathan T. Bridges", "Candice J. Hansen", "W. Alan Delamere", "John A. Grant", "Virginia C. Gulick", "Kenneth E. Herkenhoff", "Laszlo Keszthelyi", "Randolph L. Kirk", "Michael T. Mellon", "Steven W. Squyres", "Nicolas Thomas", "Catherine M. Weitz"], "venue": "Journal of Geophysical Research: Planets,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Bayesian Approach to Global Optimization, volume 37 of Mathematics and Its Applications", "author": ["Jonas Mockus"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "Safe exploration in Markov decision processes", "author": ["Teodor Mihai Moldovan", "Pieter Abbeel"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Safe exploration techniques for reinforcement learning \u2013 an overview", "author": ["Martin Pecka", "Tomas Svoboda"], "venue": "In Modelling and Simulation for Autonomous Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Gaussian processes for machine learning. Adaptive computation and machine learning", "author": ["Carl Edward Rasmussen", "Christopher K.I. Williams"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Learning Control in Robotics", "author": ["Stefan Schaal", "Christopher Atkeson"], "venue": "IEEE Robotics & Automation Magazine,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J. Smola"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Safe exploration for active learning with Gaussian processes", "author": ["Jens Schreiter", "Duy Nguyen-Tuong", "Mona Eberts", "Bastian Bischoff", "Heiner Markert", "Marc Toussaint"], "venue": "In Proc. of the European Conference on Machine Learning (ECML),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Gaussian process optimization in the bandit setting: no regret and experimental design", "author": ["Niranjan Srinivas", "Andreas Krause", "Sham M. Kakade", "Matthias Seeger"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Safe exploration for optimization with Gaussian processes", "author": ["Yanan Sui", "Alkis Gotovos", "Joel Burdick", "Andreas Krause"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "As a consequence, robots need to be able to learn and adapt to unknown environments autonomously [10, 2].", "startOffset": 97, "endOffset": 104}, {"referenceID": 1, "context": "As a consequence, robots need to be able to learn and adapt to unknown environments autonomously [10, 2].", "startOffset": 97, "endOffset": 104}, {"referenceID": 16, "context": "While exploration algorithms are known, safety is still an open problem in the development of such systems [18].", "startOffset": 107, "endOffset": 111}, {"referenceID": 14, "context": "Safe exploration is an open problem in the reinforcement learning community and several definitions of safety have been proposed [16].", "startOffset": 129, "endOffset": 133}, {"referenceID": 4, "context": "In risk-sensitive reinforcement learning, the goal is to maximize the expected return for the worst case scenario [5].", "startOffset": 114, "endOffset": 117}, {"referenceID": 6, "context": "For example, Geibel and Wysotzki [7] define risk as the probability of driving the system to a previously known set of undesirable states.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "Garcia and Fern\u00e1ndez [6] propose to ensure safety by means of a backup policy; that is, a policy that is known to be safe in advance.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "[9], where safety is defined in terms of a minimum reward, which is learned from data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Moldovan and Abbeel [14] provide probabilistic safety guarantees at every time step by optimizing over ergodic policies; that is, policies that let the agent recover from any visited state.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "[1] use reachability analysis to ensure stability under the assumption of bounded disturbances.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "The work in [3] uses robust control techniques in order to ensure robust stability for model uncertainties, while the uncertain model is improved.", "startOffset": 12, "endOffset": 15}, {"referenceID": 12, "context": "Another field that has recently considered safety is Bayesian optimization [13].", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "There, in order to find the global optimum of an a priori unknown function [21], regularity assumptions in form of a Gaussian process (GP) [17] are made.", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "There, in order to find the global optimum of an a priori unknown function [21], regularity assumptions in form of a Gaussian process (GP) [17] are made.", "startOffset": 139, "endOffset": 143}, {"referenceID": 20, "context": "[22] and Schreiter et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20], where the goal is to find the safely reachable optimum without violating an a priori unknown safety constraint at any evaluation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "The method in [22] has the advantage of being sample efficient.", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "It was applied to the field of robotics to safely optimize the controller parameters of a quadrotor vehicle [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 20, "context": "The main contribution consists of extending the work on safe Bayesian optimization in [22] from the bandit setting to MDPs.", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "It enjoys similar safety guarantees in terms of ergodicity, the ability to return to the safe starting point, as presented in [14], but at a vastly reduced computational cost.", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "In the following, we assume that S is endowed with a positive definite kernel function, k(\u00b7, \u00b7), and that the function, r(\u00b7), has bounded norm in the associated Reproducing Kernel Hilbert Space (RKHS) [19].", "startOffset": 201, "endOffset": 205}, {"referenceID": 15, "context": "This assumption allows us to model r as a Gaussian Process (GP) [17], r(s) \u223c GP(\u03bc(s), k(s, s\u2032)).", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "For many commonly used kernels, the GP assumption implies that the reward function is Lipschitz continuous with high probability [21, 8].", "startOffset": 129, "endOffset": 136}, {"referenceID": 7, "context": "For many commonly used kernels, the GP assumption implies that the reward function is Lipschitz continuous with high probability [21, 8].", "startOffset": 129, "endOffset": 136}, {"referenceID": 19, "context": "[21] in the multi-armed bandit setting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "More details on these bounds on \u03b3t can be found in [21].", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "The choice of \u03b2t in (8) is justified by the following Lemma, which follows from Theorem 6 in [21]: Lemma 1.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "We consider the setting in [14], the exploration of the surface of Mars with a rover.", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "For the experiment, we consider the Mars Science Laboratory (MSL) [11], a rover deployed on Mars.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "In our experiments we use digital terrain models of the surface of Mars from High Resolution Imaging Science Experiment (HiRISE), which have a resolution of one meter [12].", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "As opposed to the experiments considered by Moldovan and Abbeel [14], we do not have to subsample or smoothen the data in order to achieve good exploration results.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "Therefore, every state in the MDP represents a d\u00d7 d square area with d = 1m, as opposed to d = 20m in [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 20, "context": "Following the recommendations in [22], in our experiments we use the GP confidence intervals Qt(s) directly to determine the safe set St.", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "The same assumption is used in [14].", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "In this setting the agent samples the most uncertain safe state transaction, which corresponds to the safe Bayesian optimization framework in [20].", "startOffset": 142, "endOffset": 146}], "year": 2016, "abstractText": "In classical reinforcement learning, when exploring an environment, agents accept arbitrary short term loss for long term gain. This is infeasible for safety critical applications, such as robotics, where even a single unsafe action may cause system failure. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an, a priori unknown, safety constraint that depends on states and actions. We aim to explore the MDP under this constraint, assuming that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm for this task and prove that it is able to completely explore the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.", "creator": "LaTeX with hyperref package"}}}