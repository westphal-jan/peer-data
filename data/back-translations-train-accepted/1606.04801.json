{"id": "1606.04801", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "A Powerful Generative Model Using Random Weights for the Deep Image Representation", "abstract": "To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained, random weight networks? To address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization.", "histories": [["v1", "Wed, 15 Jun 2016 14:56:42 GMT  (5212kb,D)", "https://arxiv.org/abs/1606.04801v1", "10 pages, 10 figures, submited to NIPS 2016 conference. Computer Vision and Pattern Recognition, Neurons and Cognition, Neural and Evolutionary Computing"], ["v2", "Thu, 16 Jun 2016 06:53:55 GMT  (5212kb,D)", "http://arxiv.org/abs/1606.04801v2", "10 pages, 10 figures, submited to NIPS 2016 conference. Computer Vision and Pattern Recognition, Neurons and Cognition, Neural and Evolutionary Computing"]], "COMMENTS": "10 pages, 10 figures, submited to NIPS 2016 conference. Computer Vision and Pattern Recognition, Neurons and Cognition, Neural and Evolutionary Computing", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["kun he", "yan wang", "john e hopcroft"], "accepted": true, "id": "1606.04801"}, "pdf": {"name": "1606.04801.pdf", "metadata": {"source": "CRF", "title": "A Powerful Generative Model Using Random Weights for the Deep Image Representation", "authors": ["Kun He", "Yan Wang"], "emails": ["brooklet60@hust.edu.cn,", "kh555@cs.cornell.edu", "yanwang@hust.edu.cn", "jeh@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "In recent years, we have shown very competitive results on object recognition and image classification [1, 2, 3, 4]. With advances in education, there is a growing trend toward understanding the inner workings of these deep networks. By training on a very large image dataset, DNNs develop a representation of images that make object information increasingly explicit at various levels of hierarchical architecture. Major visualization techniques have been developed to understand the deep image representations in trained networks [5, 6, 7, 8, 9, 10, 11]. The three authors contribute equally. \u2020 The corresponding author.ar Xiv: 160 6.04 801v 2 [csInversion techniques have been developed to create synthetic images with representations similar to the representations of an original image in one or more layers of the network."}, {"heading": "2 Convolutional neural network", "text": "VGG-19 [3] is a Convolutionary Neural Network trained on the 1.3 million image ILSVRC 2012 ImageNet dataset [1] using the Caffe framework [21].The VGG architecture has 16 Convolutionary and 5 Pooling layers, followed by 3 fully connected layers as shown in Figure 1.The filters are of the size 3 x 3 x Nl, where Nl is the number of Characteristic Maps (or Channels).The Pooling layers are applied between the Convolutionary layers with 2 x 2 spatial scans on the Characteristic Maps. A pre-processing step subtracts the mean RGB value of each pixel of the training set.Gatys et al. recreates the VGG-19 network using average pooling rather than maximum pooling layers, which they believe could improve the gradient flow and achieve slightly better results [8] for the texture and texture pooling layers."}, {"heading": "3 Methods", "text": "In order to better understand the deep representation in the CNN architecture, we focus on three tasks: reversing image noise, synthesizing texture, and creating image noise. (1) The main difference is that we use random weights instead of trained weights, and that we use the weight factors determined by a preliminary process to normalize the different parameters of the different image dimensions on the input layer. (1) Another change is that we use a \"stacked\" random approach to use the inversion techniques to stabilize visualization quality. (1) Given a representation function F l: RH \u00b7 W \u00b7 C \u2192 RNl \u00b7 Ml for the lth layer of a deep network, and F l (x0) for an input image x0, we want to reconstruct an image x that restores the loss of L2 under the representations 0 and x.x"}, {"heading": "4 Experiments", "text": "This chapter assesses the loss of Euclidean through the activation and use of the natural resources obtained through our model, in the way we have seen them over the last few years. [That's why we've behaved like this]. [That's why it's come so far]. [That's why it's come so far]. [That's why it's come so far.] [That's why it's come so far.] [That's why it's come so far.] [That's why it's come so far.] [It's why it's come so far.] [It's why it's come so far.] It's why it's come so far. [It's] why it's come so far. [It's] why it's come so far. \""}, {"heading": "5 Discussion", "text": "Our work offers a testable hypothesis about the representation of appearance based only on the structure of the network. However, the success of untrained random weight networks in depth visualization raises several fundamental questions in the field of deep learning. Researchers have developed many visualization techniques to understand the representation of well-trained deep networks. However, if we could do the same or similar visualization with an untrained network, then the understanding does not relate to training but to the network architecture. What is the difference between a trained network and a network with random weight distribution in the same architecture, and how could we explore the difference? What else could one do with the generative power of untrained random weight networks? Explore other computer vision visualization tasks developed on the well-trained network, such as image changes [22], would be a promising aspect."}, {"heading": "Acknowledgments", "text": "This research was supported by the US Army Research Bureau (W911NF-14-1-0477), the National Science Foundation of China (61472147) and the National Science Foundation of Hubei Province (2015CFB566)."}], "references": [{"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "University de Montreal Technical Report", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Understanding deep image representations by inverting them", "author": ["Aravindh Mahendran", "Andrea Vedaldi"], "venue": "In CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Texture synthesis using convolutional neural networks", "author": ["Leon A. Gatys", "Alexander S. Ecker", "Matthias Bethge"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Understanding neural networks through deep visualization", "author": ["Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson"], "venue": "In Deep Learning Workshop, International Conference on Machine Learning (ICML),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Inverting visual representations with convolutional networks", "author": ["Alexey Dosovitskiy", "Thomas Brox"], "venue": "arXiv preprint arXiv:1506.02753,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "arXiv preprint arXiv:1505.07376,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A neural algorithm of artistic style", "author": ["Leon A Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Exploring the neural algorithm of artistic style", "author": ["Yaroslav Nikulin", "Roman Novak"], "venue": "arXiv preprint arXiv:1602.07188,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Perceptual losses for real-time style transfer and superresolution", "author": ["Justin Johnson", "Alexandre Alahi", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1603.08155,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks", "author": ["Anh Mai Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "arXiv preprint arXiv:1602.03616,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Understanding intra-class knowledge inside CNN", "author": ["Donglai Wei", "Bolei Zhou", "Antonio Torralba", "William T. Freeman"], "venue": "arXiv preprint arXiv:1507.02379,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Generating images with perceptual similarity metrics based on deep networks", "author": ["Alexey Dosovitskiy", "Thomas Brox"], "venue": "arXiv preprint arXiv:1602.02644,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Texture networks: Feedforward synthesis of textures and stylized images", "author": ["Dmitry Ulyanov", "Vadim Lebedev", "Andrea Vedaldi", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1603.03417,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Deep manifold traversal: Changing labels with convolutional features", "author": ["Jacob R. Gardner", "Paul Upchurch", "Matt J. Kusner", "Yixuan Li", "Kilian Q. Weinberger", "John E. Hopcroft"], "venue": "arXiv preprint arXiv:1511.06421,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4].", "startOffset": 187, "endOffset": 199}, {"referenceID": 1, "context": "In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4].", "startOffset": 187, "endOffset": 199}, {"referenceID": 2, "context": "In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4].", "startOffset": 187, "endOffset": 199}, {"referenceID": 3, "context": "In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4].", "startOffset": 187, "endOffset": 199}, {"referenceID": 4, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 5, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 6, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 7, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 8, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 9, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 10, "context": "Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11].", "startOffset": 122, "endOffset": 145}, {"referenceID": 6, "context": "[7] use the pretrained CNN AlexNet [2] and define a squared Euclidean loss on the activations to capture the representation differences and reconstruct the image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[7] use the pretrained CNN AlexNet [2] and define a squared Euclidean loss on the activations to capture the representation differences and reconstruct the image.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "[8, 12] define a squared loss on the correlations between feature maps of some layers and synthesize natural textures of high perceptual quality using the pretrained CNN called VGG [3].", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "[8, 12] define a squared loss on the correlations between feature maps of some layers and synthesize natural textures of high perceptual quality using the pretrained CNN called VGG [3].", "startOffset": 0, "endOffset": 7}, {"referenceID": 2, "context": "[8, 12] define a squared loss on the correlations between feature maps of some layers and synthesize natural textures of high perceptual quality using the pretrained CNN called VGG [3].", "startOffset": 181, "endOffset": 184}, {"referenceID": 12, "context": "[13] then combine the loss on the correlations as a proxy to the style of a painting and the loss on the activations to represent the content of an image, and successfully create artistic images by converting the artistic style to the content image, inspiring several followups [14, 15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13] then combine the loss on the correlations as a proxy to the style of a painting and the loss on the activations to represent the content of an image, and successfully create artistic images by converting the artistic style to the content image, inspiring several followups [14, 15].", "startOffset": 278, "endOffset": 286}, {"referenceID": 14, "context": "[13] then combine the loss on the correlations as a proxy to the style of a painting and the loss on the activations to represent the content of an image, and successfully create artistic images by converting the artistic style to the content image, inspiring several followups [14, 15].", "startOffset": 278, "endOffset": 286}, {"referenceID": 4, "context": "Another stream of visualization aims to understand what each neuron has learned in a pretrained network and synthesize an image that maximally activates individual features [5, 9] or the class prediction scores [6].", "startOffset": 173, "endOffset": 179}, {"referenceID": 8, "context": "Another stream of visualization aims to understand what each neuron has learned in a pretrained network and synthesize an image that maximally activates individual features [5, 9] or the class prediction scores [6].", "startOffset": 173, "endOffset": 179}, {"referenceID": 5, "context": "Another stream of visualization aims to understand what each neuron has learned in a pretrained network and synthesize an image that maximally activates individual features [5, 9] or the class prediction scores [6].", "startOffset": 211, "endOffset": 214}, {"referenceID": 15, "context": "further try multifaceted visualization to separate and visualize different features that a neuron learns [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 5, "context": "In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including \u03b1\u2212norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc.", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including \u03b1\u2212norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc.", "startOffset": 148, "endOffset": 151}, {"referenceID": 6, "context": "In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including \u03b1\u2212norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc.", "startOffset": 159, "endOffset": 162}, {"referenceID": 8, "context": "In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including \u03b1\u2212norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc.", "startOffset": 178, "endOffset": 181}, {"referenceID": 16, "context": "In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including \u03b1\u2212norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc.", "startOffset": 208, "endOffset": 212}, {"referenceID": 9, "context": "The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction[10, 18], texture synthesis[19] or even style transfer[15].", "startOffset": 133, "endOffset": 141}, {"referenceID": 17, "context": "The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction[10, 18], texture synthesis[19] or even style transfer[15].", "startOffset": 133, "endOffset": 141}, {"referenceID": 18, "context": "The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction[10, 18], texture synthesis[19] or even style transfer[15].", "startOffset": 160, "endOffset": 164}, {"referenceID": 14, "context": "The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction[10, 18], texture synthesis[19] or even style transfer[15].", "startOffset": 187, "endOffset": 191}, {"referenceID": 19, "context": "Instead of designing a natural prior, some researchers incorporate adversarial training[20] to improve the realism of the generated images[18].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "Instead of designing a natural prior, some researchers incorporate adversarial training[20] to improve the realism of the generated images[18].", "startOffset": 138, "endOffset": 142}, {"referenceID": 7, "context": "demonstrated that the VGG architecture with random weights failed in generating textures and resulted in white noise images in an experiment indicating the trained filters might be crucial for texture generation [8], we conjecture the success of deep visualization mainly originates from the intrinsic nonlinearity and complexity of the deep network hierarchical structure rather than from the training, and that the architecture itself may cause the inversion invariant to the original image.", "startOffset": 212, "endOffset": 215}, {"referenceID": 7, "context": "[8] on well-trained VGG.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "VGG-19 [3] is a convolutional neural network trained on the 1.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "3 million-image ILSVRC 2012 ImageNet dataset [1] using the Caffe-framework [21].", "startOffset": 45, "endOffset": 48}, {"referenceID": 20, "context": "3 million-image ILSVRC 2012 ImageNet dataset [1] using the Caffe-framework [21].", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "re-train the VGG-19 network using average pooling instead of maximum pooling, which they suggest could improve the gradient flow and obtain slightly better results [8].", "startOffset": 164, "endOffset": 167}, {"referenceID": 6, "context": "Our methods are similar in spirit to existing methods [7, 8, 13].", "startOffset": 54, "endOffset": 64}, {"referenceID": 7, "context": "Our methods are similar in spirit to existing methods [7, 8, 13].", "startOffset": 54, "endOffset": 64}, {"referenceID": 12, "context": "Our methods are similar in spirit to existing methods [7, 8, 13].", "startOffset": 54, "endOffset": 64}, {"referenceID": 7, "context": "[8] and use the correlations between feature responses on each layer as the texture representation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "The derivative of E(x, x0, l) with respect to the activations F l in layer l is [8]: \u2202E(x, x0, l) \u2202F l i,k = 1 N2 l M 2 l {(F (x)) [G(F (x))\u2212G(F (x0))]}i,k (7)", "startOffset": 80, "endOffset": 83}, {"referenceID": 12, "context": "[13] from the feature responses of VGG trained on ImageNet, we use an untrained VGG and succeed in separating and recombining content and style of arbitrary images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "We apply a regularizer R(x), total variation(TV) [7] defined as the squared sum on the adjacent pixel\u2019s difference of x, to encourage the spatial smoothness in the output image.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "We select several source images from the ILSVRC 2012 challenge [1] validation data as examples for the inversion task, and choose a monkey image as the reference image to build the stacked ranVGG3.", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "[7], we only consider the Euclidean loss over the activations and ignore the regularizer they used to capture the natural image prior.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "use a reference network AlexNet [2] which contains 8 layers of trained weights (5 convolutional layers and 3 fully connected layers), plus 3 pooling layers.", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "Figure 3: Reconstructions from layers of ranVGG (top) and the pretrained VGG (middle) and [7] (bottom).", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "[8] on the trained VGG using four convolutional layers up to conv4 1 are as shown at the bottom row.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] on several well-known artworks for the style: Night Starry by Vincent van Gohn 1989, Der Schrei by Edward Munch 1893, Picasso by Pablo Picasso 1907, Woman with a Hat by Henri Matisse 1905, Meadow with Poplars by Claude Monet 1875.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Our results are comparable to their work [13] on the pretrained VGG (third row), and are in the same order of magnitude.", "startOffset": 41, "endOffset": 45}, {"referenceID": 21, "context": "What is the difference of a trained network and a random weight network with the same architecture, and how could we explore the difference? What else could one do using the generative power of untrained, random weight networks? Explore other visualization tasks in computer vision developed on the well-trained network, such as image morphing [22], would be a promising aspect.", "startOffset": 344, "endOffset": 348}, {"referenceID": 0, "context": "[1 3] on V G G Night Starry Der Schrei Photograph Picasso Woman with a Hat Meadow with Poplars", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[1 3] on V G G Night Starry Der Schrei Photograph Picasso Woman with a Hat Meadow with Poplars", "startOffset": 0, "endOffset": 5}, {"referenceID": 7, "context": "[8] on the pretrained VGG (bottom row).", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained, random weight networks? To address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization.", "creator": "LaTeX with hyperref package"}}}