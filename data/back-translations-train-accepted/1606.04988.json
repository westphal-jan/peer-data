{"id": "1606.04988", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Logarithmic Time One-Against-Some", "abstract": "We create a new online reduction of multiclass classification to binary classification for which training and prediction time scale logarithmically with the number of classes. Compared to previous approaches, we obtain substantially better statistical performance for two reasons: First, we prove a tighter and more complete boosting theorem, and second we translate the results more directly into an algorithm. We show that several simple techniques give rise to an algorithm that can compete with one-against-all in both space and predictive power while offering exponential improvements in speed when the number of classes is large.", "histories": [["v1", "Wed, 15 Jun 2016 21:27:43 GMT  (198kb,D)", "http://arxiv.org/abs/1606.04988v1", null], ["v2", "Thu, 1 Dec 2016 02:09:04 GMT  (202kb,D)", "http://arxiv.org/abs/1606.04988v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["hal daum\u00e9 iii", "nikos karampatziakis", "john langford", "paul mineiro"], "accepted": true, "id": "1606.04988"}, "pdf": {"name": "1606.04988.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Nikos Karampatziakis"], "emails": ["hal@cs.umd.edu", "nikosk@microsoft.com", "jcl@microsoft.com", "pmineiro@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that we are in a position to find a solution that will enable us to find a solution that is also capable of finding a solution."}, {"heading": "1.1 Prior Work", "text": "The logical consequence is that most people are able to identify themselves. (...) Most people who are able to identify themselves are not able. (...) Most people who are able to identify themselves will be able to identify themselves. (...) Most people who are able to identify themselves will be able to identify themselves. (...) Most others who are able to identify themselves will be able to identify themselves. (...) Most others who are able to identify themselves will be able to identify themselves. (...) Most others who are able to identify themselves will be able to identify themselves. (...) Most others who are able to identify themselves will be able to identify themselves. (...) Most of those who are able to identify themselves will be able to identify themselves. (...) Most of those who are able to identify themselves will be able to identify themselves. (...) Most of those who are able to identify themselves will be able to identify themselves. (...) Most of those who are able to identify themselves will be able to identify themselves. (...)"}, {"heading": "2 The Recall Tree Algorithm", "text": "iDe \"iDe\" n \"iSe\" i \"s\" i. \"() D\" i \"n\" iDe \"iSe\" nvo \"nvo,\" iSe \"i\", \"iwr\" i, \"iwr\" i, \"iwr\" i \"s,\" iwr \"iwr\" e. \"n\" D \"ie\" iDe \"iSe\" n, \"iwr\" iwr \"e,\" iwr \"iwr\" i, \"iwr\" iwr \"e,\" iwr \"iwr\" e \"s,\" iwr \"iwr\" e \"n,\" iwr \"e\" n, \"iwr\" e \"n,\" in, \"iwr\" n, \"in,\" wr \"wr\" n, we \"we\" n, in, we \"in,\" in, \"in\" in, \"iwr\" n \"wr\" n, \"we\" we \"we\" n, in \"in\" in, \"ie\" in, \"iwr\" in \"n,\" iwr \"n\" iwr \"n\" n \"n\""}, {"heading": "3 Theoretical Motivation", "text": "The online construction of an optimal logarithmic time predictor for multi-class classification seems profoundly insoluble in the face of arbitrary fixed representation on each node. A major difficulty is that decisions must be hard because we cannot afford to maintain a distribution across all class names. Selecting a classifier to minimize the error rate is not only NP-difficult, it is so difficult that it has been considered for cryptographic primitives [5]. Moreover, the common optimization of all predictors does not disintegrate nicely into independent problems. Solving the above problems requires an implausible breakthrough in complexity theory, which we are not achieving here. Instead, we use learning theory to support the design by analyzing various simplifications of the problem."}, {"heading": "3.1 Optimization Objective", "text": "Shannon Entropy of class names is optimized in the router of Figure 2b. Why? Because the Recall Tree is optimized jointly through many basic learning algorithms, the systemic properties of collective optimization are more important to consider. A theory of the Decision Tree as a boost [12] provides a way to understand these common properties in a population boundary (or equivalent on a training set that iterates to convergence). Essentially, the analysis shows that each level of the Decision Tree has the accuracy of the resulting tree with this conclusion for several common objectives. For multicultural classification, it is important to achieve a weak dependence on the number of class names. Shannon Entropy is particularly well suited for this goal because it only has a logarithmic dependence on the number of class names. Let's be the probability that the correct designation is i conditioned on the corresponding example that reaches a node."}, {"heading": "3.2 One-Against-Some Prediction and Recall", "text": "The most important observation is that nodes at the same level with a similar distribution via class markers can be grouped into a single node, implying that the number of nodes at one level is only \u03b8 (1 / \u03b3), where \u03b3 is the weak learning parameter, and not exponentially in depth. This approach generally fails in the multiclass environment, because covering the simplex of multiclass markup examples for tree markup examples requires that the number of nodes at one level (K \u2212 1) markup markup markup markup markup markup examples be K \u2212 1. A simple special case is that the distribution via class markup markup markup markup indicates that the majority class, the learning of a minimal entropy markup markup markup markup type is equivalent to the possible majority of the class, or it is only the prediction for this class."}, {"heading": "3.3 Path Features", "text": "The relative power of different solutions is an important consideration. Are OAA types of representations inherently stronger or less powerful than a tree-based representation? Figure 3 shows two learning problems that illustrate two extremes assuming a linear representation. On the left is a learning problem for which OAA is ideal, while a tree-based representation requires at least 3 nodes. Conversely, the right image shows a distribution that can be easily resolved by a decision tree, but not by OAA predictions. On this basis, we can add features that map the path through the tree, with the best solution depending on the problem. Since we are interested in starting with a tree-based approach and ending with an OAS classifier, there is a simple representation trick that offers the best of both worlds. To be precise, we can add features that map the path through the tree."}, {"heading": "4 Empirical Results", "text": "We examine several questions empirically. 1. What is the benefit of using one-on-one on a recall set? 2. What is the benefit of path functions? 3. Is the online nature of the recall tree useful for non-stationary problems? 4. How does the recall tree compare statistically and arithmetically with one-on-all? 5. How does the recall tree compare statistically and arithmetically with LOMTree? During this section, we conduct experiments using learning with a linear representation."}, {"heading": "4.1 Datasets", "text": "Table 1 provides an overview of the datasets used for experiments, including the largest datasets in which results published for LOMTree are available (Aloi, Imagenet, ODP) and an additional Language Modeling Data Set (LTCB). Implementations of learning algorithms and scripts for reproducing the datasets and experimental results can be found at (url redacted)."}, {"heading": "4.2 Comparison with other Algorithms", "text": "In our first set of experiments, we compare Recall Tree with a strong computational baseline and a strong statistical baseline. The computational baseline is LOMTree, the only other logarithmic time multiclass algorithm we know of. The statistical baseline is OAA, whose statistical performance we would like to exceed (or even exceed), and whose linear computational dependence on the number of classes we would like to avoid. Details regarding the experimental methodology are in Appendix C. The results are summarized in Figure 4.Comparison with LOMTree. Recall Tree typically provides superior statistical performance on LOMTree, although it uses a factor of 32 less state."}, {"heading": "4.3 Online Operation", "text": "In this experiment, we use the online nature of the algorithm to exploit the instantaneous nature of the data to improve outcomes. It is not easy with batch-oriented algorithms or with algorithms that post-process a trained predictor to speed up the conclusion. We are looking at two versions of the LTCB. In both versions, the task is to predict the next word using the previous 6 tokens. The difference is that in one version the Wikipedia dump is processed in the original order (\"in-order\"), while in the other version the training data is permuted before input into the learning algorithm. We evaluate the progressive validation loss [6] based on the order. We assume that an online algorithm can exploit the sequential correlations of the order variant for improved performance. The result in Figure 5a confirms this."}, {"heading": "4.4 Impact of Design Choices", "text": "Two differences between Recall Tree and LOMTree are the use of multiple predictors on each tree node and the extension of the example to include path characteristics. In this experiment, we investigate the impact of these design decisions using the ALOI dataset. Figure 5b shows the effect of these two aspects on statistical performance. The larger the number of candidates, the lower the test error, but with decreasing yield. Disabling path characteristics worsen performance, and the effect is more pronounced the larger the number of candidates becomes. This is not surprising (in retrospect), since a larger size of candidate sets reduces the difficulty of obtaining a good memory (i.e., a good tree), but increases the difficulty of obtaining good precision (i.e., good class predictors), and path characteristics only apply to the latter."}, {"heading": "5 Conclusion", "text": "In this work, we proposed the Recall Tree, a reduction of the multi-class classification to binary classification, which works online and scales logarithmically with the number of classes. Unlike LOMTree [7], we share classifiers under the nodes of the tree, which reduces the data sparsity at low levels and significantly reduces the required state. We also use a narrower analysis, which is followed more closely in the implementation. These features allow us to reduce the statistical gap with OAA while working many orders of magnitude faster for large K multi-class records. In the future, we plan to investigate multi-way splits in the tree, as O (logK) way splits could not affect our O (poly logK) runtime and they could reduce the dispute at the root and the nodes high up in the tree."}, {"heading": "A Proof of theorem 2", "text": "The proof. For the solid tree with a fixed separation function in the nodes, the weighted entropy of the class labels Wt = \u2211 {n \u00b2 Leaves} fnHn.Weak Learning implies entropy improvement according to Lemma 6 of [12], which states: Lemma 3. For all q = 0, 1], Hn \u2212 (fl fn Hl + fr fn Hr) \u2212 \u2212 2q (1 \u2212 q) With this approach, the weak learning assumption implies the class entropy Hl and Hr satisfy Hn \u00b2 (fl fn Hr). (fl fn Hr) + \u03b3Kp \u2212 2q \u2212 (1 \u2212 q). Hence, Wt \u2212 Wt \u2212 1 = fnHn \u2212 flHl \u2212 frHr."}, {"heading": "B Datasets", "text": "ALOI [9] is a color image collection of a thousand small objects recorded for scientific purposes [9]. We use the same traction test split and representation as Choromanska et. al. [7].Imagenet consists of features derived from intermediate layers of a Convolutionary Neural Network trained on the ILVSRC2012 dataset. This dataset was originally designed to study transfer learning in visual tasks [18]; more details are available at http: / / www.di.ens.fr / willow / research / cnn /. We use a linear predictor in this representation. LTCB is the Large Text Compression Benchmark, which consists of the first billion bytes of a particular Wikipedia dump [13]. Originally designed to study text compression, it is now commonly used as a language modeling benchmark, where the task is to predict the next word in the sequence, plus limiting the vocabulary to a single set of 80,000 words."}, {"heading": "C Experimental Methodology", "text": "For the larger datasets (Imagenet, ODP), we perform a single run over the training data; for the smaller dataset (ALOI), we perform multiple runs over the training data, monitoring a 10% share of the training data to determine when the optimization needs to be set; for the larger datasets (Imagenet, ODP), we use a random search over hyperparameters, obtaining the best result over 59 probes; for the smaller dataset (ALOI), we optimize the validation error on a 10% subset of training data; for the larger datasets (Imagenet, ODP), we optimize progressive validation losses on the initial 10% of training data; after determining the hyperparameters, we train with the entire training set and report on the resulting tests."}], "references": [{"title": "Label embedding trees for large multi-class tasks", "author": ["Samy Bengio", "Jason Weston", "David Grangier"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Refined experts: improving classification in large taxonomies", "author": ["Paul N Bennett", "Nam Nguyen"], "venue": "In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Error-correcting tournaments", "author": ["Alina Beygelzimer", "John Langford", "Pradeep Ravikumar"], "venue": "In Algorithmic Learning Theory, 20th International Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Sparse local embeddings for extreme multi-label classification", "author": ["Kush Bhatia", "Himanshu Jain", "Purushottam Kar", "Manik Varma", "Prateek Jain"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Cryptographic primitives based on hard learning problems", "author": ["Avrim Blum", "Merrick Furst", "Michael Kearns", "Richard J Lipton"], "venue": "In Advances in cryptologyCRYPTO93,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Beating the hold-out: Bounds for k-fold and progressive cross-validation", "author": ["Avrim Blum", "Adam Kalai", "John Langford"], "venue": "In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Logarithmic time online multiclass prediction", "author": ["Anna E Choromanska", "John Langford"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "An exploration of softmax alternatives belonging to the spherical loss family", "author": ["Alexandre de Br\u00e9bisson", "Pascal Vincent"], "venue": "arXiv preprint arXiv:1511.05042,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "The Amsterdam library of object images", "author": ["Jan-Mark Geusebroek", "Gertjan J Burghouts", "Arnold WM Smeulders"], "venue": "International Journal of Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "The population frequencies of species and the estimation of population", "author": ["I.J. Good"], "venue": "parameters. Biometrika,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1953}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "On the boosting ability of top-down decision tree learning algorithms", "author": ["Michael Kearns", "Yishay Mansour"], "venue": "In Proceedings of STOC,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Boosting using branching programs", "author": ["Yishay Mansour", "David McAllester"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Empirical bernstein bounds and sample variance penalization", "author": ["Andreas Maurer", "Massimiliano Pontil"], "venue": "arXiv preprint arXiv:0907.3740,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio"], "venue": "In Proceedings of the international workshop on artificial intelligence and statistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning", "author": ["Yashoteja Prabhu", "Manik Varma"], "venue": "In Proceedings of the 20th ACM SIGKDD,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "In defense of one-vs-all classification", "author": ["Ryan Rifkin", "Aldebaro Klautau"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)", "author": ["Anshumali Shrivastava", "Ping Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Efficient exact gradient update for training deep networks with very large sparse targets", "author": ["Pascal Vincent", "Alexandre de Br\u00e9bisson", "Xavier Bouthillier"], "venue": "In NIPS", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "WSABIE: scaling up to large vocabulary image annotation", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "IJCAI", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Can we effectively predict one of K classes in deeply sublinear time? This question gives rise to the area of extreme multiclass classification [1, 3, 4, 7, 17, 19, 25], in whichK is very large.", "startOffset": 144, "endOffset": 168}, {"referenceID": 2, "context": "Can we effectively predict one of K classes in deeply sublinear time? This question gives rise to the area of extreme multiclass classification [1, 3, 4, 7, 17, 19, 25], in whichK is very large.", "startOffset": 144, "endOffset": 168}, {"referenceID": 3, "context": "Can we effectively predict one of K classes in deeply sublinear time? This question gives rise to the area of extreme multiclass classification [1, 3, 4, 7, 17, 19, 25], in whichK is very large.", "startOffset": 144, "endOffset": 168}, {"referenceID": 6, "context": "Can we effectively predict one of K classes in deeply sublinear time? This question gives rise to the area of extreme multiclass classification [1, 3, 4, 7, 17, 19, 25], in whichK is very large.", "startOffset": 144, "endOffset": 168}, {"referenceID": 15, "context": "Can we effectively predict one of K classes in deeply sublinear time? This question gives rise to the area of extreme multiclass classification [1, 3, 4, 7, 17, 19, 25], in whichK is very large.", "startOffset": 144, "endOffset": 168}, {"referenceID": 17, "context": "Can we effectively predict one of K classes in deeply sublinear time? This question gives rise to the area of extreme multiclass classification [1, 3, 4, 7, 17, 19, 25], in whichK is very large.", "startOffset": 144, "endOffset": 168}, {"referenceID": 18, "context": "\u2022 High accuracy: The approach should provide accuracy competitive with OAA, a remarkably strong baseline[20] which is the standard \u201coutput layer\u201d of many learning systems such as recent winners of the imagenet contest [11, 22].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "\u2022 High accuracy: The approach should provide accuracy competitive with OAA, a remarkably strong baseline[20] which is the standard \u201coutput layer\u201d of many learning systems such as recent winners of the imagenet contest [11, 22].", "startOffset": 218, "endOffset": 226}, {"referenceID": 20, "context": "\u2022 High accuracy: The approach should provide accuracy competitive with OAA, a remarkably strong baseline[20] which is the standard \u201coutput layer\u201d of many learning systems such as recent winners of the imagenet contest [11, 22].", "startOffset": 218, "endOffset": 226}, {"referenceID": 6, "context": "\u2022 High speed at training time and test time: A multiclass classifier must spend at least \u03b8(logK) time [7]) so this is a natural benchmark to optimize against.", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "The Recall Tree achieves good accuracy, always improving on previous online approaches [7] and sometimes surpassing the OAA baseline.", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "The LOMTree[7] is the closest prior work, only missing on space requirements where up to a factor of 64 more space than OAA was used experimentally.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "Other approaches such as hierarchical softmax (HSM) and the the Filter Tree [3] use a fixed tree structure [17].", "startOffset": 76, "endOffset": 79}, {"referenceID": 15, "context": "Other approaches such as hierarchical softmax (HSM) and the the Filter Tree [3] use a fixed tree structure [17].", "startOffset": 107, "endOffset": 111}, {"referenceID": 0, "context": "In domains in which there is no prespecified tree hierarchy, using a random tree structure can lead to considerable underperformance as shown previously [1, 7].", "startOffset": 153, "endOffset": 159}, {"referenceID": 6, "context": "In domains in which there is no prespecified tree hierarchy, using a random tree structure can lead to considerable underperformance as shown previously [1, 7].", "startOffset": 153, "endOffset": 159}, {"referenceID": 14, "context": "Most other approaches in extreme classification either do not work online [16, 19] or only focus on speeding up either prediction time or training time but not both.", "startOffset": 74, "endOffset": 82}, {"referenceID": 17, "context": "Most other approaches in extreme classification either do not work online [16, 19] or only focus on speeding up either prediction time or training time but not both.", "startOffset": 74, "endOffset": 82}, {"referenceID": 14, "context": "In [16] the authors try to add tree structure learning to HSM via iteratively clustering the classes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Similar remarks apply to [1] where the authors propose to learn a tree by solving an eigenvalue problem after (OAA) training.", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "Among the approaches that speed up training time we distinguish exact ones [8, 23] that have only been proposed for particular loss functions and approximate ones such as negative sampling as used e.", "startOffset": 75, "endOffset": 82}, {"referenceID": 21, "context": "Among the approaches that speed up training time we distinguish exact ones [8, 23] that have only been proposed for particular loss functions and approximate ones such as negative sampling as used e.", "startOffset": 75, "endOffset": 82}, {"referenceID": 22, "context": "in [24].", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "Though these techniques do not address inference time, separate procedures for speeding up inference (given a trained model) have been proposed [21].", "startOffset": 144, "endOffset": 148}, {"referenceID": 4, "context": "Choosing a classifier so as to minimize error rate is not just NP-hard, it is so difficult it has been considered for cryptographic primitives [5].", "startOffset": 143, "endOffset": 146}, {"referenceID": 11, "context": "A theory of decision tree learning as boosting [12] provides a way to understand these joint properties in a population limit (or equivalently on a training set iterated until convergence).", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "The proof in appendix A reuses techniques from [7, 12].", "startOffset": 47, "endOffset": 54}, {"referenceID": 11, "context": "The proof in appendix A reuses techniques from [7, 12].", "startOffset": 47, "endOffset": 54}, {"referenceID": 6, "context": "These results are similar to previous results [7, 12] with two advantages.", "startOffset": 46, "endOffset": 53}, {"referenceID": 11, "context": "These results are similar to previous results [7, 12] with two advantages.", "startOffset": 46, "endOffset": 53}, {"referenceID": 11, "context": "We handle multiclass rather than binary classification [12] and we bound error rates instead of entropy [7].", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "We handle multiclass rather than binary classification [12] and we bound error rates instead of entropy [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "For binary classification, branching programs [14] result in exponentially more succinct representations than decision trees [12] by joining nodes into a directed acyclic graphs.", "startOffset": 46, "endOffset": 50}, {"referenceID": 11, "context": "For binary classification, branching programs [14] result in exponentially more succinct representations than decision trees [12] by joining nodes into a directed acyclic graphs.", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "Nevertheless, as data gets divided down the branches of the tree, empirical estimates for the \u201ctop F most frequent labels\u201d suffer from a substantial missing mass problem [10].", "startOffset": 170, "endOffset": 174}, {"referenceID": 13, "context": "Thus, instead of computing empirical recall to determine when to halt descent, we use an empirical Bernstein (lower) bound [15], which is summarized by the following proposition.", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "Dataset Task Classes Examples ALOI[9] Visual Object Recognition 1k 10 Imagenet[18] Visual Object Recognition \u2248 20k \u2248 10 LTCB[13] Language Modeling \u2248 80k \u2248 10 ODP[2] Document Classification \u2248 100k \u2248 10", "startOffset": 34, "endOffset": 37}, {"referenceID": 16, "context": "Dataset Task Classes Examples ALOI[9] Visual Object Recognition 1k 10 Imagenet[18] Visual Object Recognition \u2248 20k \u2248 10 LTCB[13] Language Modeling \u2248 80k \u2248 10 ODP[2] Document Classification \u2248 100k \u2248 10", "startOffset": 78, "endOffset": 82}, {"referenceID": 1, "context": "Dataset Task Classes Examples ALOI[9] Visual Object Recognition 1k 10 Imagenet[18] Visual Object Recognition \u2248 20k \u2248 10 LTCB[13] Language Modeling \u2248 80k \u2248 10 ODP[2] Document Classification \u2248 100k \u2248 10", "startOffset": 161, "endOffset": 164}, {"referenceID": 5, "context": "We assess progressive validation loss [6] on the sequence.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Unlike the LOMTree [7], we share classifiers among the nodes of the tree which alleviates data sparsity at deep levels while greatly reducing the required state.", "startOffset": 19, "endOffset": 22}], "year": 2016, "abstractText": "We create a new online reduction of multiclass classification to binary classification for which training and prediction time scale logarithmically with the number of classes. Compared to previous approaches, we obtain substantially better statistical performance for two reasons: First, we prove a tighter and more complete boosting theorem, and second we translate the results more directly into an algorithm. We show that several simple techniques give rise to an algorithm that can compete with one-against-all in both space and predictive power while offering exponential improvements in speed when the number of classes is large.", "creator": "LaTeX with hyperref package"}}}