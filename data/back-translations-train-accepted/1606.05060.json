{"id": "1606.05060", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Pruning Random Forests for Prediction on a Budget", "abstract": "We propose to prune a random forest (RF) for resource-constrained prediction. We first construct a RF and then prune it to optimize expected feature cost &amp; accuracy. We pose pruning RFs as a novel 0-1 integer program with linear constraints that encourages feature re-use. We establish total unimodularity of the constraint set to prove that the corresponding LP relaxation solves the original integer program. We then exploit connections to combinatorial optimization and develop an efficient primal-dual algorithm, scalable to large datasets. In contrast to our bottom-up approach, which benefits from good RF initialization, conventional methods are top-down acquiring features based on their utility value and is generally intractable, requiring heuristics. Empirically, our pruning algorithm outperforms existing state-of-the-art resource-constrained algorithms.", "histories": [["v1", "Thu, 16 Jun 2016 05:56:36 GMT  (168kb,D)", "http://arxiv.org/abs/1606.05060v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["feng nan", "joseph wang", "venkatesh saligrama"], "accepted": true, "id": "1606.05060"}, "pdf": {"name": "1606.05060.pdf", "metadata": {"source": "CRF", "title": "Pruning Random Forests for Prediction on a Budget", "authors": ["Feng Nan", "Joseph Wang", "Venkatesh Saligrama"], "emails": ["fnan@bu.edu", "joewang@bu.edu", "srv@bu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Learning with Resource Constraints", "text": "In this thesis, we look at solving the relaxed lenggric problem of learning under prediction-time resource constraints, also known as the error cost compromise problem: (1) where sample / label pairs (x, y) are extracted from a distribution P; (y, y, f (x) is the error function; (f, x) the cost of evaluating classifier f based on example x; (1) where sample / label pairs (x, y) are extracted from a distribution P; (y, y) is the error function; (f, x) the cost of evaluating classifier f based on example x; \u03bb is a compromise parameter. A larger \u03bb imposes a greater penalty on the cost, causing the classifier to incur a lower cost. By adapting this approach, we can obtain a classifier that meets the budget constraint. The family of classifiers F in our setting is the space of the RF, and each step of the second one is bleed by constructing an RF-1 decision semester."}, {"heading": "3 Pruning with Costs", "text": "In fact, it is a reactionary U-turn, capable of putting itself at the top of society."}, {"heading": "4 A Primal-Dual Algorithm", "text": "Although (IP) can solve a problem by relaxing the LP, the resulting LP problem can be too large in practice (P solution). (P solution) Specifically, the number of variables and constraints (P rule), where the number of trees is the maximum number of nodes in a tree; N is the number of examples; Kmax is the maximum number of attributes that an example uses in a tree. Thus, the runtime of the LP scales O (T-3) with the number of trees in a tree; N is the number of examples; Kmax is the maximum number of attributes that an example uses in a tree. The runtime of the LP scales O (T-3) with the number of trees in the ensemble, which limits the application to only small ensembles. In this section, we propose a primary-dual approach that effectively splits optimization into many sub-problems."}, {"heading": "5 Experiments", "text": "This year it is more than ever before."}, {"heading": "5.1 Discussion & Concluding Comments", "text": "We have empirically several resource-limited learning algorithms, including the BUDGETPRUNE and its variations on benchmarked data sets here and in the appendix. We highlight the main features of our approach. (I) STATE-DATE METHODS. Recent work has shown that GREEDYMISER and BUDGETRF are state-of-the-art methods that dominate a number of other methods. (I) GREEDYMISER requires the creation of class-specific groups and tends to perform poorly, and it is increasingly difficult to fit into multi-class constellations. RF is inherently capable of handling multi-class constellations."}, {"heading": "6 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 A Naive Pruning Formulation", "text": "The beautiful property of completely unimmodular constraints in Theorem 3.2 is due to our specific formulation. Here, we present an alternative integer program formulation and show its inadequacy. Recall we define the following nodes: 1 = 0.5, if the node h is a leaf in the pruned tree, 0 otherwise.and indicator variables of attribute usage: 1 + 1, if the characteristics of x (h) in each Tt, t = 1,., T 0 otherwise.First, if zh = 1 for some nodes h, then the examples that are applied to h must have used all the characteristics in the previous nodes p (h), except k p (h) to denote attribute k is used in every predecessor of h, except h."}, {"heading": "6.2 Transformation to Network Matrices and Shortest Path Problems", "text": "rE \"s rf\u00fc ide r\u00fc for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green.\" rE \"s rf\u00fc ide r\u00fc for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "6.3 Proof of Theorem 3.2", "text": "Any constraint matrix in J1 associated with a tree can be converted into a network matrix according to Lemma3.1. Stacking these matrices results in a larger network matrix. Name the w (t) k, i \u2264 wk, i constraints according to index set J2. Consider the constraints in J2. Each w (t) k, i \u2264 wk appears only once in J2, meaning that the column corresponding to w (t) k has only one element equal to 1 and the rest equal to 0. If we arrange the constraints in J2 such that for each given k, i w (t) k, i \u2264 wk, i to t [T], then the constraint matrix for J2 has an interval structure, so that the non-zeros appear in each column separately."}, {"heading": "6.4 Additional Details of Experiments", "text": "This year it is more than ever before."}], "references": [{"title": "Sequential prediction for budgeted learning : Application to trigger design", "author": ["Djalel Benbouzid"], "venue": "Theses, Universite\u0301 Paris Sud - Paris XI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1):5\u201332", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Classification and regression trees", "author": ["Leo Breiman", "Jerome Friedman", "Charles J Stone", "Richard A Olshen"], "venue": "CRC press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1984}, {"title": "Decision trees for entity identification: Approximation algorithms and hardness results", "author": ["Venkatesan T. Chakaravarthy", "Vinayaka Pandit", "Sambuddha Roy", "Pranjal Awasthi", "Mukesh K. Mohania"], "venue": "ACM Trans. Algorithms,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "editors", "author": ["O Chapelle", "Y Chang", "T Liu"], "venue": "Proceedings of the Yahoo! Learning to Rank Challenge, held at ICML 2010, Haifa, Israel, June 25, 2010", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["Jerome H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Active classification based on value of classifier", "author": ["T. Gao", "D. Koller"], "venue": "Advances in Neural Information Processing Systems ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Pruning of random forest classifiers: A survey and future directions", "author": ["V.Y. Kulkarni", "P.K. Sinha"], "venue": "In Data Science Engineering (ICDSE),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Feature-cost sensitive learning with submodular trees of classifiers", "author": ["M Kusner", "W Chen", "Q Zhou", "E Zhixiang", "K Weinberger", "Y Chen"], "venue": "AAAI Conference on Artificial Intelligence", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), volume 2, pages 2169\u20132178", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Object Bank: A High-Level Image Representation for Scene Classification and Semantic Feature Sparsification", "author": ["Li-Jia Li", "Hao Su", "Eric P. Xing", "Li Fei-Fei"], "venue": "In Advances in Neural Information Processing Systems. MIT Press,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "A dynamic programming based pruning method for decision trees", "author": ["Xiao-Bai Li", "James Sweigart", "James Teng", "Joan Donohue", "Lori Thombs"], "venue": "INFORMS J. on Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Fast margin-based cost-sensitive classification", "author": ["F Nan", "J Wang", "K Trapeznikov", "V Saligrama"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature-budgeted random forest", "author": ["Feng Nan", "Joseph Wang", "Venkatesh Saligrama"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Integer and Combinatorial Optimization", "author": ["George L. Nemhauser", "Laurence A. Wolsey"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1988}, {"title": "An analysis of approximations for maximizing submodular set functions", "author": ["George L Nemhauser", "Laurence A Wolsey", "Marshall L Fisher"], "venue": "Mathematical Programming,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1978}, {"title": "An optimal constrained pruning strategy for decision trees", "author": ["Hanif D. Sherali", "Antoine G. Hobeika", "Chawalit Jeenanunta"], "venue": "INFORMS Journal on Computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Supervised sequential classification under budget constraints", "author": ["K Trapeznikov", "V Saligrama"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 581\u2013589", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Model selection by linear programming", "author": ["J. Wang", "T. Bolukbasi", "K Trapeznikov", "V Saligrama"], "venue": "European Conference on Computer Vision, pages 647\u2013662", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "An LP for sequential learning under budgets", "author": ["J Wang", "K Trapeznikov", "V Saligrama"], "venue": "Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, AISTATS 2014", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "An lp for sequential learning under budgets", "author": ["J Wang", "K Trapeznikov", "V Saligrama"], "venue": "International Conference on Artificial Intelligence and Statistics", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient learning by directed acyclic graph for resource constrained prediction", "author": ["Joseph Wang", "Kirill Trapeznikov", "Venkatesh Saligrama"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Cost-sensitive tree of classifiers", "author": ["Z Xu", "M Kusner", "M Chen", "K. Q Weinberger"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "The greedy miser: Learning under test-time budgets", "author": ["Zhixiang Eddie Xu", "Kilian Q. Weinberger", "Olivier Chapelle"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Decision tree pruning via integer programming", "author": ["Yi Zhang", "Huang Huei-chuen"], "venue": "Working paper,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}], "referenceMentions": [{"referenceID": 22, "context": "Many modern classification systems, including internet applications (such as web-search engines, recommendation systems, and spam filtering) and security & surveillance applications (such as widearea surveillance and classification on large video corpora), face the challenge of prediction-time budget constraints [26].", "startOffset": 314, "endOffset": 318}, {"referenceID": 13, "context": "In the first stage, we train a random forest (RF) of trees using an impurity function such as entropy or more specialized cost-adaptive impurity [17].", "startOffset": 145, "endOffset": 149}, {"referenceID": 12, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 19, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 18, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 20, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 6, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 23, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 17, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 21, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 8, "context": "Related Work: Learning decision rules to minimize error subject to a budget constraint during prediction-time is an area of recent interest, with many approaches proposed to solve the predictiontime budget constrained problem [16, 23, 22, 24, 9, 27, 21, 25, 12].", "startOffset": 226, "endOffset": 261}, {"referenceID": 1, "context": "Our work is based on RF classifiers [3].", "startOffset": 36, "endOffset": 39}, {"referenceID": 13, "context": "Traditionally, feature cost is not incorporated when constructing RFs, however recent work has involved approximation of budget constraints to learn budgeted RFs [17].", "startOffset": 162, "endOffset": 166}, {"referenceID": 13, "context": "The tree-growing algorithm in [17] does not take feature re-use into account.", "startOffset": 30, "endOffset": 34}, {"referenceID": 2, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[15], CCP has undesirable \u201cjumps\" in the sequence of pruned tree sizes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[28, 20] proposed to solve the pruning problem as a 0-1 integer program; again, their formulations do not account for feature costs that we focus on in this paper.", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "[28, 20] proposed to solve the pruning problem as a 0-1 integer program; again, their formulations do not account for feature costs that we focus on in this paper.", "startOffset": 0, "endOffset": 8}, {"referenceID": 7, "context": "Kulkarni and Sinha [11] provide a survey of methods to prune RFs in order to reduce ensemble size.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "The rest of proof follows directly from the construction in Proposition 3 of [20].", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "The main idea is to show the constraints are still totally unimodular even after adding the coupling constraints and the LP relaxed polyhedron has only integral extreme points [19].", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "(i) BUDGETRF[17]: the recursive node splitting process for each tree is stopped as soon as node impu-", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "GREEDYMISER dominates ASTC [12], CSTC [26] and DAG [25] significantly on all datasets.", "startOffset": 27, "endOffset": 31}, {"referenceID": 22, "context": "GREEDYMISER dominates ASTC [12], CSTC [26] and DAG [25] significantly on all datasets.", "startOffset": 38, "endOffset": 42}, {"referenceID": 21, "context": "GREEDYMISER dominates ASTC [12], CSTC [26] and DAG [25] significantly on all datasets.", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "(ii) CostComplexity Pruning (CCP) [4]: it iteratively prunes subtrees such that the resulting tree has low error and small size.", "startOffset": 34, "endOffset": 37}, {"referenceID": 23, "context": "We also compare against the state-of-the-art methods in budgeted learning (iv) GREEDYMISER [27]: it is a modification of gradient boosted regression tree [8] to incorporate feature cost.", "startOffset": 91, "endOffset": 95}, {"referenceID": 5, "context": "We also compare against the state-of-the-art methods in budgeted learning (iv) GREEDYMISER [27]: it is a modification of gradient boosted regression tree [8] to incorporate feature cost.", "startOffset": 154, "endOffset": 157}, {"referenceID": 8, "context": "Other prediction-time budget algorithms such as ASTC [12], CSTC [26] and cost-weighted l-1 classifiers are shown to perform strictly worse than GREEDYMISER by a significant amount [12, 17] so we omit them in our plots.", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": "Other prediction-time budget algorithms such as ASTC [12], CSTC [26] and cost-weighted l-1 classifiers are shown to perform strictly worse than GREEDYMISER by a significant amount [12, 17] so we omit them in our plots.", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "Other prediction-time budget algorithms such as ASTC [12], CSTC [26] and cost-weighted l-1 classifiers are shown to perform strictly worse than GREEDYMISER by a significant amount [12, 17] so we omit them in our plots.", "startOffset": 180, "endOffset": 188}, {"referenceID": 13, "context": "Other prediction-time budget algorithms such as ASTC [12], CSTC [26] and cost-weighted l-1 classifiers are shown to perform strictly worse than GREEDYMISER by a significant amount [12, 17] so we omit them in our plots.", "startOffset": 180, "endOffset": 188}, {"referenceID": 4, "context": "Yahoo! Learning to Rank:[6] This ranking dataset consists of 473134 web documents and 19944 queries.", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "As in [17], we use the Average Precision@5 as the performance metric, which gives a high reward for ranking the relevant documents on top.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "Our base RF consists of 140 trees using cost weighted entropy split criteria as in [17] and choosing from a random subset of 400 features at each split.", "startOffset": 83, "endOffset": 87}, {"referenceID": 9, "context": "Scene15 [13]: This scene recognition dataset contains 4485 images from 15 scene classes (labels).", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "Following [27] we divide it into 1500/300/2685 examples for training/validation/test sets.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "We use a diverse set of visual descriptors and object detectors from the Object Bank [14].", "startOffset": 85, "endOffset": 89}, {"referenceID": 8, "context": "Recent work has established that GREEDYMISER and BUDGETRF are among the state-of-the-art methods dominating a number of other methods [12, 26, 25] on these benchmarked datasets.", "startOffset": 134, "endOffset": 146}, {"referenceID": 22, "context": "Recent work has established that GREEDYMISER and BUDGETRF are among the state-of-the-art methods dominating a number of other methods [12, 26, 25] on these benchmarked datasets.", "startOffset": 134, "endOffset": 146}, {"referenceID": 21, "context": "Recent work has established that GREEDYMISER and BUDGETRF are among the state-of-the-art methods dominating a number of other methods [12, 26, 25] on these benchmarked datasets.", "startOffset": 134, "endOffset": 146}, {"referenceID": 8, "context": "On the other hand, as we described earlier, [12, 25, 26] are fundamentally \"tree-growing\" approaches, namely they are top-down methods acquiring features sequentially based on a surrogate utility value.", "startOffset": 44, "endOffset": 56}, {"referenceID": 21, "context": "On the other hand, as we described earlier, [12, 25, 26] are fundamentally \"tree-growing\" approaches, namely they are top-down methods acquiring features sequentially based on a surrogate utility value.", "startOffset": 44, "endOffset": 56}, {"referenceID": 22, "context": "On the other hand, as we described earlier, [12, 25, 26] are fundamentally \"tree-growing\" approaches, namely they are top-down methods acquiring features sequentially based on a surrogate utility value.", "startOffset": 44, "endOffset": 56}, {"referenceID": 3, "context": "This is a fundamentally combinatorial problem that is known to be NP hard [5, 26] and thus requires a number of relaxations and heuristics with no guarantees on performance.", "startOffset": 74, "endOffset": 81}, {"referenceID": 22, "context": "This is a fundamentally combinatorial problem that is known to be NP hard [5, 26] and thus requires a number of relaxations and heuristics with no guarantees on performance.", "startOffset": 74, "endOffset": 81}, {"referenceID": 13, "context": "We experiment BUDGETPRUNE with different impurity functions such as entropy and Pairs [17] criteria.", "startOffset": 86, "endOffset": 90}, {"referenceID": 0, "context": "An elegant approach has been suggested by [2], who propose an adversarial feature cost proportional to feature utility value.", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "7, Part 3 of [18] with partition Q1 and Q2.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "2, Part 3 of [18] we can conclude the proof.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "For the Scene15 dataset, we use a diverse set of visual discriptors varying in computation time: GIST, spatial HOG, Local Binary Pattern, self-similarity, texton histogram, geometric texton, geometric color and 177 object detectors from the Object Bank [14].", "startOffset": 253, "endOffset": 257}, {"referenceID": 13, "context": "Entropy Vs Pairs How does BUDGETPRUNE depend on the splitting criteria used in the underlying random forest? On two data sets we build RFs using the popular entropy splitting criteria and the mini-max Pairs criteria used in [17] and the results are shown in Figure 6.", "startOffset": 224, "endOffset": 228}, {"referenceID": 13, "context": "This is expected as using Pairs biases to more balanced splits and thus provably low cost [17].", "startOffset": 90, "endOffset": 94}], "year": 2016, "abstractText": "We propose to prune a random forest (RF) for resource-constrained prediction. We first construct a RF and then prune it to optimize expected feature cost & accuracy. We pose pruning RFs as a novel 0-1 integer program with linear constraints that encourages feature re-use. We establish total unimodularity of the constraint set to prove that the corresponding LP relaxation solves the original integer program. We then exploit connections to combinatorial optimization and develop an efficient primal-dual algorithm, scalable to large datasets. In contrast to our bottom-up approach, which benefits from good RF initialization, conventional methods are top-down acquiring features based on their utility value and is generally intractable, requiring heuristics. Empirically, our pruning algorithm outperforms existing state-of-the-art resource-constrained algorithms.", "creator": "LaTeX with hyperref package"}}}