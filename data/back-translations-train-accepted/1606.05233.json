{"id": "1606.05233", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Learning feed-forward one-shot learners", "abstract": "One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.", "histories": [["v1", "Thu, 16 Jun 2016 15:49:26 GMT  (2553kb,D)", "http://arxiv.org/abs/1606.05233v1", "The first three authors contributed equally, and are listed in alphabetical order"]], "COMMENTS": "The first three authors contributed equally, and are listed in alphabetical order", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["luca bertinetto", "jo\u00e3o f henriques", "jack valmadre", "philip h s torr", "andrea vedaldi"], "accepted": true, "id": "1606.05233"}, "pdf": {"name": "1606.05233.pdf", "metadata": {"source": "CRF", "title": "Learning feed-forward one-shot learners", "authors": ["Luca Bertinetto", "Jo\u00e3o F. Henriques", "Jack Valmadre"], "emails": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "jvlmdr@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that it will be able to fix and correct the mentioned bugs before they are able to retaliate."}, {"heading": "1.1 Related work", "text": "A very recent example is Rezende et al. [18], which uses a recurring spatial attention model to generate images, and which learns by optimizing a reconstruction error by means of varying conclusions [8]. The results are shown by scanning images from this generative model, not by solving discriminatory tasks. Another notable work is Lake et al. [12], which instead uses a probabilistic program as a generative model. This model constructs written characters as compositions of pen strokes so that it is able to capture them."}, {"heading": "2 One-shot learning as dynamic parameter prediction", "text": "Since we view single learning as a discriminatory task, our starting point is standard discriminatory learning. It usually consists of searching for parameters W, which minimize the average loss of predictor function. (1) Unless the model space is very small, generalization must also limit the choice of model, usually by means of regulation. (2) However, in extreme cases, where the goal is to learn W from a single copy class called single learning, additional prior information must be injected into the learning process. (3) The main challenge in discriminatory single learning is to find a mechanism to integrate domain-specific information into the learner, i.e.."}, {"heading": "2.1 The challenge of naive parameter prediction", "text": "To analyze the practical difficulties of implementing a learning network, we start with a one-line prediction of a completely connected layer, as it is easier to analyze, given by y = Wx + b, (4) by an input x (z) and b (z) representing two outputs of the learning network. (5) While eq. (5) seems to be a drop-in replacement for linear layers, careful analysis shows that it scales extremely poorly, the main cause being the unusually large output of the learning network w: Rm \u2192 Rd \u00d7 k."}, {"heading": "2.2 Factorized linear layers", "text": "A simple way to reduce the size of the output space is to consider a factorized set of weights by replacing Gl. (5) with: y = M \u2032 diag (w (z)) Mx + b (z). (6) The product M \u2032 diag (w (z))) M can be considered as a factorized representation of weights, analogous to Singular Value Decomposition. The matrix M \u2032 Rd \u00b7 d projects x into a space in which the elements of w (z) represent untangled variation factors. The second projection M \u2032 Rd \u00b7 k returns the result from this space. Both M \u2032 and M \u2032 contain additional parameters to be learned, but they are modest compared to the case discussed in Section 2.1. Importantly, the single branch w (z) now only needs to predict a series of diagonal elements (see Gl. (6) so that its output space grows in line with the number of units (z)."}, {"heading": "2.3 Factorized convolutional layers", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3 Experiments", "text": "We evaluate learning networks based on bullet architectures (Section 3.1) on two bullet learning problems in the areas of Optical Character Recognition (OCR; Section 3.2) and Visual Object Tracking (Section 3.3)."}, {"heading": "3.1 Architectures", "text": "As mentioned in Section 2, the closest competitors of our method are embedded learning using discriminatory, single-layer Siamese architectures, so we structure the experiments to compare them against this baseline. Specifically, we opt to implement learning networks using similar network topologies for a fairer comparation.The basic Siamese architecture consists of two parallel streams composed of a number of layers, such as convolution, max pooling and ReLU, with the common parameters W (fig. 2.a).The results of the two streams are compared by a layer whose results are predicted by a layer (x; W), a layer (z; W) that calculates a measure of similarity or dissimilarity. In particular, we consider: the dot product < a > between vectors a and b, the euclidean distance that a \u2212 b \u2212 b \u00b2, and the weighted l1 b \u00b2 norm \u2212."}, {"heading": "3.2 Character recognition in foreign alphabets", "text": "This describes our experiments in the background with a total of 19,280 images in the background and 13,180 in the background. To this end, we use the Omniglot datasets, which each contain images of handwritten characters from 50 different alphabets. These alphabets are divided into 30 background and 20 valuation alphabets. However, the associated learning problem is to develop a method to determine whether or not every single copy of a character in a rating alphabet, every other image in that alphabet represents the same character. Importantly, all methods are only trained with background alphabets and tested for the evaluation of alphabets.The datasets and rating log are scaled to 28 \u00d7 28 pixels to efficiently explore several variants of the proposed architectures. There are exactly 20 sample images for each character and an average of 32 characters per alphabet. The datasets contain a total of 19,280 images in the background of the alphabets and 13,180 in recognition."}, {"heading": "3.3 Object tracking", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "4 Conclusions", "text": "In this paper, we have demonstrated that it is possible to obtain the parameters of a deep neural network from a single forward-looking prediction from a second network, which is desirable when iterative methods are too slow and large amounts of commented training samples are not available. We have demonstrated the feasibility of predicting feed-forward parameters in two challenging one-time learning tasks in OCR and visual tracking, and our results point to a promising research path in learning to learn by solving millions of small discriminatory problems in an offline phase. Possible extensions include domain adaptations and sharing a single learning network between different student networks."}, {"heading": "A Basis filters", "text": "This appendix provides an additional interpretation for the role of the predicted filters in a factorized q q layer (Section 2.3). To make the representation short and concise, we use a notation that differs slightly from the main text. If x is a tensor for activations, xi denotes channel i of x. If a is a multi-channel filter, aij denotes the filter for output channel i and input channel j. That is, if a m \u00b7 n \u00b7 p \u00b7 q is then aij, aij m \u00b7 n is for i [p], j \u0432 [q]. The set {1,..., n} is called [n]. Factorized convolution is y = Ax = M \u2032 WMx. (9) where M and M \u2032 predictions are pixel-by-pixel projections and W is a diagonal convolution. While a general convolution (Av) calculates the convolution of jate (Av) i = yy yy (10), each one is a single-channel filter."}, {"heading": "B Additional results on object tracking", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Signature verification using a \u201csiamese\u201d time delay neural network", "author": ["J. Bromley", "J.W. Bentz", "L. Bottou", "I. Guyon", "Y. LeCun", "C. Moore", "E. S\u00e4ckinger", "R. Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "Accurate scale estimation for robust visual tracking", "author": ["M. Danelljan", "G. H\u00e4ger", "F. Khan", "M. Felsberg"], "venue": "BMVC,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Learning deep face representation", "author": ["H. Fan", "Z. Cao", "Y. Jiang", "Q. Yin", "C. Doudou"], "venue": "arXiv CoRR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ICCV,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-store tracker (MUSTER): A cognitive psychology inspired approach to object tracking", "author": ["Z. Hong", "Z. Chen", "C. Wang", "X. Mei", "D. Prokhorov", "D. Tao"], "venue": "CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv CoRR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv CoRR,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Siamese neural networks for one-shot image recognition", "author": ["G. Koch", "R. Zemel", "R. Salakhutdinov"], "venue": "ICML 2015 Deep Learning Workshop,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "The visual object tracking VOT2015 challenge results", "author": ["M. Kristan", "J. Matas", "A. Leonardis", "M. Felsberg", "L. Cehovin", "G. Fernandez", "T. Vojir", "G. Hager", "G. Nebehay", "R. Pflugfelder"], "venue": "ICCV Workshop,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "Science, 350(6266):1332\u20131338,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Bilinear CNN models for fine-grained visual recognition", "author": ["T.-Y. Lin", "A. RoyChowdhury", "S. Maji"], "venue": "ICCV,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Ensemble of exemplar-SVMs for object detection and beyond", "author": ["T. Malisiewicz", "A. Gupta", "A.A. Efros"], "venue": "ICCV,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning multi-domain convolutional neural networks for visual tracking", "author": ["H. Nam", "B. Han"], "venue": "arXiv CoRR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep face recognition", "author": ["O.M. Parkhi", "A. Vedaldi", "A. Zisserman"], "venue": "BMVC,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "In defense of color-based model-free tracking", "author": ["H. Possegger", "T. Mauthner", "H. Bischof"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "One-shot generalization in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "I. Danihelka", "K. Gregor", "D. Wierstra"], "venue": "arXiv CoRR,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "C.D. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Transferring rich feature hierarchies for robust visual tracking", "author": ["N. Wang", "S. Li", "A. Gupta", "D.-Y. Yeung"], "venue": "arXiv CoRR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "MEEM: Robust tracking via multiple experts using entropy minimization", "author": ["J. Zhang", "S. Ma", "S. Sclaroff"], "venue": "ECCV.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 17, "context": "These problems are usually tackled by using generative models [18, 12] or, in a discriminative setting, using ad-hoc solutions such as exemplar support vector machines (SVMs) [14].", "startOffset": 62, "endOffset": 70}, {"referenceID": 11, "context": "These problems are usually tackled by using generative models [18, 12] or, in a discriminative setting, using ad-hoc solutions such as exemplar support vector machines (SVMs) [14].", "startOffset": 62, "endOffset": 70}, {"referenceID": 13, "context": "These problems are usually tackled by using generative models [18, 12] or, in a discriminative setting, using ad-hoc solutions such as exemplar support vector machines (SVMs) [14].", "startOffset": 175, "endOffset": 179}, {"referenceID": 3, "context": "Perhaps the most common discriminative approach to one-shot learning is to learn off-line a deep embedding function and then to define on-line simple classification rules such as nearest neighbors in the embedding space [4, 16, 13].", "startOffset": 220, "endOffset": 231}, {"referenceID": 15, "context": "Perhaps the most common discriminative approach to one-shot learning is to learn off-line a deep embedding function and then to define on-line simple classification rules such as nearest neighbors in the embedding space [4, 16, 13].", "startOffset": 220, "endOffset": 231}, {"referenceID": 12, "context": "Perhaps the most common discriminative approach to one-shot learning is to learn off-line a deep embedding function and then to define on-line simple classification rules such as nearest neighbors in the embedding space [4, 16, 13].", "startOffset": 220, "endOffset": 231}, {"referenceID": 17, "context": "[18], which uses a recurrent spatial attention model to generate images, and learns by optimizing a measure of reconstruction error using variational inference [8].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[18], which uses a recurrent spatial attention model to generate images, and learns by optimizing a measure of reconstruction error using variational inference [8].", "startOffset": 160, "endOffset": 163}, {"referenceID": 11, "context": "[12], which instead uses a probabilistic program as a generative model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "A different approach to one-shot-learning is to learn an embedding space, which is typically done with a siamese network [1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "Training is usually performed by classifying pairs according to distance [4], or by enforcing a distance ranking with a triplet loss [16].", "startOffset": 73, "endOffset": 76}, {"referenceID": 15, "context": "Training is usually performed by classifying pairs according to distance [4], or by enforcing a distance ranking with a triplet loss [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "A variant is to combine embeddings using the outer-product, which yields a bilinear classification rule [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": "It is usually framed as a modality transfer problem and solved through transfer learning [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "[3], who showed that it is possible to linearly predict as much as 95% of the parameters in a layer given the remaining 5%.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Note that the training data is reminiscent of that of siamese networks [1], which also learn from labeled sample pairs.", "startOffset": 71, "endOffset": 74}, {"referenceID": 12, "context": "A similar argument can be made of bilinear networks [13].", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "In practice, this can be achieved with filter tensors that are diagonal in the third and fourth dimensions, or using d filter groups [11], each group containing a single filter.", "startOffset": 133, "endOffset": 137}, {"referenceID": 3, "context": "3 or 5 [4, 21]).", "startOffset": 7, "endOffset": 14}, {"referenceID": 20, "context": "3 or 5 [4, 21]).", "startOffset": 7, "endOffset": 14}, {"referenceID": 11, "context": "For this, we use the Omniglot dataset [12], which contains images of handwritten characters from 50 different alphabets.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "[9] using much deeper networks applied to images of size 105 \u00d7 105.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "A more powerful algorithm for one-shot learning, Hierarchical Bayesian Program Learning [12], is able to achieve human-level performance.", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "The method is trained using the ImageNet Large Scale Visual Recognition Challenge 2015 [19], with 3,862 videos totalling more than one million annotated frames.", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "Testing uses the VOT 2015 benchmark [10].", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "Each siamese stream has five convolutional layers and we test three variants of those: variant (A) has the same configuration as AlexNet [11] but with stride 2 in the first layer, and variants (B) and (C) reduce to 50% the number of filters in the first two convolutional layers and, respectively, to 25% and 12.", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "497 93 DAT [17] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "442 113 SO-DLT [21] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 1, "context": "491 106 DSST [2] 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 21, "context": "483 163 MEEM [22] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "458 107 MUSTer [6] 0.", "startOffset": 15, "endOffset": 18}, {"referenceID": 9, "context": "471 132 Table 2: Tracking accuracy and number of tracking failures in the VOT 2015 Benchmark, as reported by the toolkit [10].", "startOffset": 121, "endOffset": 125}, {"referenceID": 4, "context": "The weights are initialized using the improved Xavier [5] method, and we use batch normalization [7] after all linear layers.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "The weights are initialized using the improved Xavier [5] method, and we use batch normalization [7] after all linear layers.", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "3 compares the methods in terms of the official metrics (accuracy and number of failures) for the VOT 2015 benchmark [10].", "startOffset": 117, "endOffset": 121}, {"referenceID": 16, "context": "An elementary tracker based on learnet compares favourably against recent tracking systems, which make use of different features and online model update strategies: DAT [17], DSST [2], MEEM [22], MUSTer [6] and SO-DLT [21].", "startOffset": 169, "endOffset": 173}, {"referenceID": 1, "context": "An elementary tracker based on learnet compares favourably against recent tracking systems, which make use of different features and online model update strategies: DAT [17], DSST [2], MEEM [22], MUSTer [6] and SO-DLT [21].", "startOffset": 180, "endOffset": 183}, {"referenceID": 21, "context": "An elementary tracker based on learnet compares favourably against recent tracking systems, which make use of different features and online model update strategies: DAT [17], DSST [2], MEEM [22], MUSTer [6] and SO-DLT [21].", "startOffset": 190, "endOffset": 194}, {"referenceID": 5, "context": "An elementary tracker based on learnet compares favourably against recent tracking systems, which make use of different features and online model update strategies: DAT [17], DSST [2], MEEM [22], MUSTer [6] and SO-DLT [21].", "startOffset": 203, "endOffset": 206}, {"referenceID": 20, "context": "An elementary tracker based on learnet compares favourably against recent tracking systems, which make use of different features and online model update strategies: DAT [17], DSST [2], MEEM [22], MUSTer [6] and SO-DLT [21].", "startOffset": 218, "endOffset": 222}, {"referenceID": 9, "context": "We consider, however, that our implementation serves mostly as a proof-of-concept, using tracking as an interesting demonstration of one-shot-learning, and is orthogonal to many technical improvements found in the tracking literature [10].", "startOffset": 234, "endOffset": 238}], "year": 2016, "abstractText": "One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.", "creator": "LaTeX with hyperref package"}}}