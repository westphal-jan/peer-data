{"id": "1606.05250", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "abstract": "We present a new reading comprehension dataset, SQuAD, consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset in both manual and automatic ways to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We built a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.", "histories": [["v1", "Thu, 16 Jun 2016 16:36:00 GMT  (307kb,D)", "http://arxiv.org/abs/1606.05250v1", null], ["v2", "Fri, 7 Oct 2016 03:48:29 GMT  (307kb,D)", "http://arxiv.org/abs/1606.05250v2", "10 pages"], ["v3", "Tue, 11 Oct 2016 02:42:36 GMT  (307kb,D)", "http://arxiv.org/abs/1606.05250v3", "To appear in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["pranav rajpurkar", "jian zhang", "konstantin lopyrev", "percy liang"], "accepted": true, "id": "1606.05250"}, "pdf": {"name": "1606.05250.pdf", "metadata": {"source": "CRF", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "authors": ["Pranav Rajpurkar"], "emails": ["pranavsr@cs.stanford.edu", "zjian@cs.stanford.edu", "klopyrev@cs.stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Reading comprehension (RC et al., 1993), or the ability to read text and then answer questions about it, is a challenging task for machines that require both understanding natural language and knowledge of the world. Considering the question \"What causes precipitation to fall?,\" which is posed in the passage in Figure 1. To answer the question, one could first find the relevant part of the passage \"Precipitation... falls under gravity,\" then reason that \"below\" refers to a cause (not place), and thus determine the correct answer: \"gravity.\" How can we get a machine to make progress in the demanding task of reading comprehension? SQuAD is freely available at https: / / stanford-qa.com. Historically, large, realistic datasets play a crucial role in driving fields - famous examples include ImageNet for object recognition (Deng et al., 2009) and the Penn Treebank for syntactical parsing (al Marcus et al, 1993)."}, {"heading": "2 Existing Datasets", "text": "It is a question of what the future of the world is like, and it is a question of what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future is like, what the future is like, what the future is like, what is like the future of the world, what is like the future of the world, what is like the future of the world, what is like the future of the future of the world, what is like the future of the future of the world, what is like the future of the future of the world, what is like the future of the future of the world, what is like the future of the future of the future of the world, what is like the future of the future of the future of the world, what is like the future of the future of the future of the future of the world, what is like the future of the future of the future of the future of the world, what is like the future of the future of the future of the future of the future of the future of the world, the future of the future of the world, the future of the future of the future of the world, what is like the future of the future, the future of the future of the future, the future of the future of the future of the future, the future of the future, the future of the future, the future of the future of the future of the world like the future, what is like the future, the future, the future, the future of the future, the future of the future of the future of the future, the future of the future, the future of the future, the future, about the future of the future, about the future, about the future, about the future, about the future, about the future of the future of the future, about the future, about the future of the future, about the future of the"}, {"heading": "3 Dataset Collection", "text": "We collect our records in three stages: curating passages, crowdsourcing questions-answers to those passages, and getting additional answers. Passage curating. To retrieve high-quality articles, we used Project Nayuki's internal PageRanks to obtain the top 10,000 articles in English Wikipedia, from which we retrieve 536 articles uniformly at random, we extracted individual paragraphs by adding images, numbers, tables, and discarding paragraphs shorter than 500 characters. The result was 23,215 paragraphs for the 536 articles covering a wide range of topics, from musical celebrities to abstract concepts."}, {"heading": "4 Dataset Analysis", "text": "In order to understand the characteristics of SQuAD, we analyze the questions and answers in SQuAD's development group. Specifically, we explore the (i) diversity of answer types, (ii) the difficulty of the questions in terms of the type of reasoning required to answer them, and (iii) the degree of syntactical divergence between the questions and answer sentences. We automatically categorize the answers as follows: We first separate the numerical and non-numerical answers, and the non-numerical answers are categorized using the labels generated by Stan-Q. Which department store is considered the first in the world? S: Bainbridge's is often categorized first in the store.ford CoreNLP department."}, {"heading": "5 Methods", "text": "We developed a logistic regression model and compared it to the accuracy of three basic methods. Generating the candidate answers. In all methods, we first truncate the list of spans using constituency trees. Suppose there are M signs in a passage, there are O (M2) possible answers to each question. To limit the wide range of possible answers, we consider those that are constituents in parses generated by Stanford CoreNLP for all of our models as candidate answers. After removing punctuation and articles, we find that 77.3% of the correct answers in the development group are constituents."}, {"heading": "5.1 Sliding Window Baseline", "text": "For each candidate answer, we calculate the overlap between the sentence contained therein (without the candidate himself) and the question. We record all candidates that have the maximum overlap, among which we select the best with the sliding window approach proposed in Richardson et al. (2013). In addition to the basic sliding window approach, we also implemented the distance-based extension (Richardson et al., 2013). While Richardson et al. (2013) used the entire passage as the context of an answer, for efficiency reasons, we used only the sentence that contains the candidate response."}, {"heading": "5.2 Logistic Regression", "text": "In our logistics regression model, we extract several types of characteristics for each candidate response. The descriptions and examples of characteristics are summarized in Table 4.The matching word and bigram frequencies and root match characteristics help the model select the right sentences. Length characteristics tend the model to select common lengths and positions for the response spans, while keyword frequencies distort the model over uninformative words. Constituent identifiers and span POS characteristics guide the model to the correct response types. In addition to these basic characteristics, we resolve lexical variations based on lexicalized characteristics, and syntactic variations based on dependency tree paths. Multi-class protocol loss is performed at an initial learning rate of 0.1. Each update is performed using the batch of all questions in one paragraph, and syntactical variation is performed using dependency tree pathways."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Model Evaluation", "text": "We use two different measures to evaluate model performance. Both measures ignore string differences in punctuation and the inclusion / exclusion of certain and indefinite articles. Exact agreement. This measure measures the percentage of predictions that accurately correspond to one of the basic truths. (Macro-averaged) F1 value. This measure loosely measures the average overlap between the prediction and the basic truth answer. We treat the prediction and the basic truth like bags of chips and calculate its formula 1. We take the maximum F1 for all the basic truth answers for a given question and then the average for all questions."}, {"heading": "6.2 Human Performance", "text": "To evaluate human performance, we consider the second answer to each question to be a human prediction and consider the other answers to be basic truth answers. The resulting value of human performance on the test set is 77.0% for the exact match size and 86.8% for formula 1. The mismatch occurs mainly when using optional phrases (e.g. monsoon trough versus monsoon trough movement), not when disagreeing on the answer."}, {"heading": "6.3 Model Performance", "text": "Table 5 shows the performance of our models in addition to human performance on the development and test sets. The logistic regression model clearly exceeds baselines, but falls short of people's expectations. We note that the model is able to correctly select the sentence that contains the answer with an accuracy of 79.3%; however, the real difficulty lies in finding the exact margin within the set. To understand the characteristics that are responsible for the performance of the logistic regression model, we perform a trait ablation, in which we remove a group of characteristics from each of our models. The results shown in Table 6 suggest that lexicalized characteristics and characteristics of the dependency tree are most important. If we compare our analysis with that of Chen et al. (2016), we find that the characteristics of the dependence tree play a much greater role in our data sets."}, {"heading": "7 Conclusion", "text": "We see data sets as a way to guide progress toward the ultimate goal of understanding natural language. To this end, we are introducing SQuAD, a large reading comprehension data set on Wikipedia articles with crowdsourced question-answer pairs. SQuAD has a wide range of questions and answer types. The performance of our logistics regression model at 51.0% F1 over human F1 at 86.8% points to ample room for improvement. We are making our data set freely available to facilitate research into more meaningful models."}], "references": [{"title": "Modeling biological processes for reading comprehension", "author": ["Berant et al.2014] J. Berant", "V. Srikumar", "P. Chen", "A.V. Linden", "B. Harding", "B. Huang", "P. Clark", "C.D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "An analysis of the AskMSR question-answering system", "author": ["Brill et al.2002] E. Brill", "S. Dumais", "M. Banko"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Brill et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brill et al\\.", "year": 2002}, {"title": "A thorough examination of the CNN / Daily Mail reading comprehension", "author": ["Chen et al.2016] D. Chen", "J. Bolton", "C.D. Manning"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "My computer is an honor student but how intelligent is it? standardized tests as a measure of AI", "author": ["Clark", "Etzioni2016] P. Clark", "O. Etzioni"], "venue": null, "citeRegEx": "Clark et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2016}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["Deng et al.2009] J. Deng", "W. Dong", "R. Socher", "L. Li", "K. Li", "L. Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Building Watson: An overview of the DeepQA project", "author": ["Ferrucci et al.2013] D. Ferrucci", "E. Brown", "J. ChuCarroll", "J. Fan", "D. Gondek", "A.A. Kalyanpur", "A. Lally", "J.W. Murdock", "E. Nyberg", "J. Prager", "N. Schlaefer", "C. Welty"], "venue": null, "citeRegEx": "Ferrucci et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2013}, {"title": "Daemo: A self-governed crowdsourcing marketplace", "author": ["D. Morina", "R. Nistala", "M. Agarwal", "A. Cossette", "R. Bhanu", "S. Savage", "V. Narwal", "K. Rajpal", "J. Regino"], "venue": "In Proceedings of the 28th Annual ACM", "citeRegEx": "Gaikwad et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gaikwad et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["T. Ko\u010disk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Hill et al.2015] F. Hill", "A. Bordes", "S. Chopra", "J. Weston"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Deep read: A reading comprehension system", "author": ["M. Light", "E. Breck", "J.D. Burger"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Hirschman et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hirschman et al\\.", "year": 1999}, {"title": "Learning to solve arithmetic word problems with verb categorization", "author": ["H. Hajishirzi", "O. Etzioni", "N. Kushman"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Hosseini et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hosseini et al\\.", "year": 2014}, {"title": "Learning to automatically solve algebra word problems", "author": ["Kushman et al.2014] N. Kushman", "Y. Artzi", "L. Zettlemoyer", "R. Barzilay"], "venue": null, "citeRegEx": "Kushman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kushman et al\\.", "year": 2014}, {"title": "Building a large annotated corpus of English: the Penn Treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Machine comprehension with discourse relations. In Association for Computational Linguistics (ACL)", "author": ["Narasimhan", "Barzilay2015] K. Narasimhan", "R. Barzilay"], "venue": null, "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "A machine learning approach to answering questions for reading comprehension tests. In Joint SIGDAT conference on empirical methods in natural language processing and very large corpora - Volume", "author": ["H.T. Ng", "L.H. Teo", "J.L.P. Kwan"], "venue": null, "citeRegEx": "Ng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2000}, {"title": "Learning surface text patterns for a question answering system", "author": ["Ravichandran", "Hovy2002] D. Ravichandran", "E. Hovy"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Ravichandran et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ravichandran et al\\.", "year": 2002}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["C.J. Burges", "E. Renshaw"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "A rule-based question answering system for reading comprehension tests. In ANLP/NAACL Workshop on reading comprehension tests as evaluation for computer-based language understanding sytems", "author": ["Riloff", "Thelen2000] E. Riloff", "M. Thelen"], "venue": null, "citeRegEx": "Riloff et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Riloff et al\\.", "year": 2000}, {"title": "Learning answer-entailing structures for machine comprehension", "author": ["Sachan et al.2015] M. Sachan", "A. Dubey", "E.P. Xing", "M. Richardson"], "venue": null, "citeRegEx": "Sachan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sachan et al\\.", "year": 2015}, {"title": "Exploring correlation of dependency relation paths for answer extraction", "author": ["Shen", "Klakow2006] D. Shen", "D. Klakow"], "venue": "In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL),", "citeRegEx": "Shen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2006}, {"title": "N-gram idf: A global term weighting scheme based on information distance", "author": ["T. Hara", "S. Nishio"], "venue": "In World Wide Web (WWW),", "citeRegEx": "Shirakawa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shirakawa et al\\.", "year": 2015}, {"title": "Answer extraction from passage graph for question answering", "author": ["H. Sun", "N. Duan", "Y. Duan", "M. Zhou"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "Sun et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2013}, {"title": "Building a question answering test collection", "author": ["Voorhees", "Tice2000] E.M. Voorhees", "D.M. Tice"], "venue": null, "citeRegEx": "Voorhees et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Voorhees et al\\.", "year": 2000}, {"title": "Machine comprehension with syntax, frames, and semantics. In Association for Computational Linguistics (ACL)", "author": ["H. Wang", "M. Bansal", "K. Gimpel", "D. McAllester"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks. arXiv", "author": ["Weston et al.2015] J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "WikiQA: A challenge dataset for open-domain question answering", "author": ["Y. Yang", "W. Yih", "C. Meek"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "Historically, large, realistic datasets have played a critical role for driving fields forward\u2014famous examples include ImageNet for object recognition (Deng et al., 2009) and the Penn Treebank for syntactic parsing (Marcus et al.", "startOffset": 151, "endOffset": 170}, {"referenceID": 12, "context": ", 2009) and the Penn Treebank for syntactic parsing (Marcus et al., 1993).", "startOffset": 52, "endOffset": 73}, {"referenceID": 16, "context": "Existing datasets for RC have one of two shortcomings: (i) those that are high in quality (Richardson et al., 2013; Berant et al., 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al.", "startOffset": 90, "endOffset": 136}, {"referenceID": 0, "context": "Existing datasets for RC have one of two shortcomings: (i) those that are high in quality (Richardson et al., 2013; Berant et al., 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al.", "startOffset": 90, "endOffset": 136}, {"referenceID": 7, "context": ", 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al., 2015; Hill et al., 2015) are semi-synthetic and do not share the same characteristics as explicit reading comprehension questions.", "startOffset": 97, "endOffset": 138}, {"referenceID": 8, "context": ", 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al., 2015; Hill et al., 2015) are semi-synthetic and do not share the same characteristics as explicit reading comprehension questions.", "startOffset": 97, "endOffset": 138}, {"referenceID": 16, "context": "SQuAD contains 107,785 question-answer pairs on 536 articles, and is almost two orders of magnitude larger than previous manually labeled RC datasets such as MCTest (Richardson et al., 2013).", "startOffset": 165, "endOffset": 190}, {"referenceID": 9, "context": "A data-driven approach to reading comprehension goes back to Hirschman et al. (1999), who curated a dataset of 600 real 3rd\u2013 Dataset Question source Formulation Size", "startOffset": 61, "endOffset": 85}, {"referenceID": 16, "context": "MCTest (Richardson et al., 2013) crowdsourced RC, multiple choice 2640", "startOffset": 7, "endOffset": 32}, {"referenceID": 11, "context": "Algebra (Kushman et al., 2014) standardized tests computation 514", "startOffset": 8, "endOffset": 30}, {"referenceID": 25, "context": "WikiQA (Yang et al., 2015) query logs IR, sentence selection 3047", "startOffset": 7, "endOffset": 26}, {"referenceID": 7, "context": "CNN/Daily Mail (Hermann et al., 2015) summary + cloze RC, fill in single entity 1.", "startOffset": 15, "endOffset": 37}, {"referenceID": 8, "context": "CBT (Hill et al., 2015) cloze RC, fill in single word 688K", "startOffset": 4, "endOffset": 23}, {"referenceID": 14, "context": "Their pattern matching baseline was subsequently improved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000).", "startOffset": 139, "endOffset": 156}, {"referenceID": 18, "context": "Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015).", "startOffset": 182, "endOffset": 253}, {"referenceID": 23, "context": "Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015).", "startOffset": 182, "endOffset": 253}, {"referenceID": 14, "context": "Their pattern matching baseline was subsequently improved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000). More recently, Richardson et al. (2013) curated MCTest, which contains 660 stories created by crowdworkers, with 4 questions per story and 4 answer choices per question.", "startOffset": 140, "endOffset": 198}, {"referenceID": 11, "context": "Algebra word problems require understanding a story well enough to turn it into a system of equations, which can be easily solved to produce the answer (Kushman et al., 2014; Hosseini et al., 2014).", "startOffset": 152, "endOffset": 197}, {"referenceID": 10, "context": "Algebra word problems require understanding a story well enough to turn it into a system of equations, which can be easily solved to produce the answer (Kushman et al., 2014; Hosseini et al., 2014).", "startOffset": 152, "endOffset": 197}, {"referenceID": 24, "context": "BAbI (Weston et al., 2015), a fully synthetic RC dataset, is stratified by different types of reasoning required to solve each task.", "startOffset": 5, "endOffset": 26}, {"referenceID": 10, "context": ", 2014; Hosseini et al., 2014). BAbI (Weston et al., 2015), a fully synthetic RC dataset, is stratified by different types of reasoning required to solve each task. Clark and Etzioni (2016) describe the task of solving 4th grade science exams, and stress the need to reason with world knowledge.", "startOffset": 8, "endOffset": 190}, {"referenceID": 5, "context": "The annual evaluations at the Text REtreival Conference (TREC) (Voorhees and Tice, 2000) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy! (Ferrucci et al., 2013).", "startOffset": 181, "endOffset": 204}, {"referenceID": 5, "context": "The annual evaluations at the Text REtreival Conference (TREC) (Voorhees and Tice, 2000) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy! (Ferrucci et al., 2013). Recently, Yang et al. (2015) created the WikiQA dataset, which, like SQuAD, use Wikipedia passages as a source of answers, but their task is sentence selection, while ours requires selecting a specific span in the sentence.", "startOffset": 182, "endOffset": 235}, {"referenceID": 21, "context": "Selecting the span of text that answers a question is similar to answer extraction, the final step in the open-domain QA pipeline, methods for which include bootstrapping surface patterns (Ravichandran and Hovy, 2002), using dependency trees (Shen and Klakow, 2006), and using a factor graph over multiple sentences (Sun et al., 2013).", "startOffset": 316, "endOffset": 334}, {"referenceID": 1, "context": "One key difference between our RC setting and answer extraction is that answer extraction typically exploits the fact that the answer occurs in multiple documents (Brill et al., 2002), which is more lenient than in our setting, where a system only has access to a single reading passage.", "startOffset": 163, "endOffset": 183}, {"referenceID": 8, "context": "The Children\u2019s Book Test (CBT) (Hill et al., 2015), for example, involves predicting a blanked-out word of a sentence given the 20 previous sentences.", "startOffset": 31, "endOffset": 50}, {"referenceID": 6, "context": "Hermann et al. (2015) constructed a corpus of cloze style questions by blanking out entities in abstractive summaries of CNN / Daily News articles; the goal is to fill in the entity based on the original article.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "While the size of this dataset is impressive, Chen et al. (2016) showed that the dataset requires less reasoning than previously thought, and concluded that performance is almost saturated.", "startOffset": 46, "endOffset": 65}, {"referenceID": 6, "context": "We used the Daemo platform (Gaikwad et al., 2015), with Amazon Mechanical Turk at its backend.", "startOffset": 27, "endOffset": 49}, {"referenceID": 16, "context": "Among these, we select the best one using the sliding-window approach proposed in Richardson et al. (2013).", "startOffset": 82, "endOffset": 107}, {"referenceID": 16, "context": "In addition to the basic sliding window approach, we also implemented the distance-based extension (Richardson et al., 2013).", "startOffset": 99, "endOffset": 124}, {"referenceID": 16, "context": "In addition to the basic sliding window approach, we also implemented the distance-based extension (Richardson et al., 2013). Whereas Richardson et al. (2013) used the entire passage as the context of an answer, we used only the sentence containing the candidate answer for efficiency.", "startOffset": 100, "endOffset": 159}, {"referenceID": 20, "context": "We use the generalization of the TF-IDF described in Shirakawa et al. (2015). Span: [0 \u2264 sum < 2.", "startOffset": 53, "endOffset": 77}, {"referenceID": 2, "context": "Comparing our analysis to the one in Chen et al. (2016), we note that the dependency tree path features play a much bigger role in our dataset.", "startOffset": 37, "endOffset": 56}], "year": 2016, "abstractText": "We present a new reading comprehension dataset, SQuAD, consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset in both manual and automatic ways to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We built a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.", "creator": "LaTeX with hyperref package"}}}