{"id": "1606.05302", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Generalized Direct Change Estimation in Ising Model Structure", "abstract": "We consider the problem of estimating change in the dependency structure between two $p$-dimensional Ising models, based on respectively $n_1$ and $n_2$ samples drawn from the models. The change is assumed to be structured, e.g., sparse, block sparse, node-perturbed sparse, etc., such that it can be characterized by a suitable (atomic) norm. We present and analyze a norm-regularized estimator for directly estimating the change in structure, without having to estimate the structures of the individual Ising models. The estimator can work with any norm, and can be generalized to other graphical models under mild assumptions. We show that only one set of samples, say $n_2$, needs to satisfy the sample complexity requirement for the estimator to work, and the estimation error decreases as $\\frac{c}{\\sqrt{\\min(n_1,n_2)}}$, where $c$ depends on the Gaussian width of the unit norm ball. For example, for $\\ell_1$ norm applied to $s$-sparse change, the change can be accurately estimated with $\\min(n_1,n_2)=O(s \\log p)$ which is sharper than an existing result $n_1= O(s^2 \\log p)$ and $n_2 = O(n_1^2)$. Experimental results illustrating the effectiveness of the proposed estimator are presented.", "histories": [["v1", "Thu, 16 Jun 2016 18:21:45 GMT  (437kb,D)", "http://arxiv.org/abs/1606.05302v1", null]], "reviews": [], "SUBJECTS": "math.ST cs.LG stat.TH", "authors": ["farideh fazayeli", "arindam banerjee"], "accepted": true, "id": "1606.05302"}, "pdf": {"name": "1606.05302.pdf", "metadata": {"source": "CRF", "title": "Generalized Direct Change Estimation in Ising Model Structure", "authors": ["Farideh Fazayeli", "Arindam Banerjee"], "emails": ["farideh@cs.umn.edu}", "banerjee@cs.umn.edu}"], "sections": [{"heading": null, "text": "min (n1, n2), where c depends on the Gaussian width of the unit standard ball. For example, for \"1 standard applied to s-sparse variation, the change can be accurately estimated with min (n1, n2) = O (s log p), which is sharper than an existing result n1 = O (s2 log p) and n2 = O (n21). Experimental results illustrating the effectiveness of the proposed estimator are presented."}, {"heading": "1 Introduction", "text": "Over the past decade, considerable progress has been made in estimating the statistical structure of dependence on graphical models based on samples from the model. (In particular, such progress has been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multivariate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 22, 23, 34]. In this paper, we look at Ising models and focus on the problem of estimating changes in the Ising model structure: in the light of two sets of samples Xn11 = {x1i} n1 and X n2 = {x2i} n2 = true parameter 1 and 2 Ising models drawn from two p-dimensional models each, taking into account the changes in the structure of the Ising models Xn11 = {x1i} n2 and X n2 of the changes. (The aim is to estimate the change)."}, {"heading": "2 Generalized Direct Change Estimation", "text": "We will consider the following optimization problems (1), where Xn11 = {x1i} n1 i = 1 and X n2 = {x2i} n2 i = 1 are two sets of i.i.d binary samples taken from the Ising models with the parameters \u03b8 1 and \u03b8 \u0445 2, where x 1 i and x 2 i are p \u2212 dimensional vectors, and n1, n2 are the respective sample sizes. In this section we will first give a brief background on the selection of the Ising model. Then we will explain how to develop the loss function L (\u0441\u0430; Xn11, X n2) on the basis of the density ratio [9, 10, 25, 31] in order to directly estimate how to solve the optimization problem (1) for any standard R."}, {"heading": "2.1 Ising Model", "text": "Let G = (V, E) be an undirected graph with the vertice V = {1, \u00b7 \u00b7, p} and the edge group E, whose elements are disordered pairs of different vertices. Ising-Markov edge field mapped in pairs to the diagram G over the random vector X isP (X = x | \u03b8) = 1 Z (\u03b8) exp {s, t \u00b2 E \u03b8 s, txsxt} s, txsxt} s, txsxt} (2) = 1Z (\u03b8, T (x) >} (3) exp {xTafrica x} (4), where T (x) = {xsxt} ps, t = 1 is a vector of size m = p2, profile, t \u00b2 s, t = 1 x."}, {"heading": "2.2 Loss Function", "text": "Similarly, the loss function can be rewritten on the basis of (4) if the regulation function lies above matrices. Let's look at two Ising models with parameters. < p = < p = p and p = p = p = p and p = p = p = p and p = p = p = p = p and p = p = p = p = p = p (2) p (12, 13], a direct estimate of the problem of detecting the changes. (T (x), p = p = p = p = p (X = x | p) p (X = x | p) p (X = x | p) p (X = x | p) p (X = x | p) p (X = p) p (p = p < T (x), p (x), p = p = p (X), p (X = p)."}, {"heading": "2.3 Optimization", "text": "The optimization problem (1) has a compound target with a smooth convex term corresponding to the loss function (11) and a potentially non-smooth convex term corresponding to the regulator. \u2212 In this section, we introduce an algorithm in the class of Fast Iterative Shrinkage Thresholding Algorithms (FISTA) to efficiently solve the problem (1) [3]. \u2212 For convenience, we refer to the loss function L (eg; Xn11, X n2) as L (eg) and we leave the subscript {n1, n2} of 1, n2.One of the most popular methods for compound objective functions is in the class of FISTA, where we linearize the smooth term and minimize the square approximation of formQL (eg)."}, {"heading": "2.4 Regularization Function", "text": "We assume that the optimal structure is thin or suitably \"structured,\" with such a structure being characterized by a low value in accordance with a suitable standard R (2). Here are some examples of such a standard.L1 Standard: An example of R (.) that we will consider throughout the work is the L1 norm regularization. We use the L1 standard if only a few edges have changed (1st line in Figure 1). In particular, we consider R (2) = [1] if the number of non-zeros entries in the [2] group is s < p2. The prox standard L (.) is indicated as [prox threshold L] by the elementary soft threshold of operation [24]."}, {"heading": "3 Statistical Recovery for Generalized Direct Change Estimation", "text": "Our goal is to set non-asymptotic boundaries between the actual parameter \u03b4\u043d and the minimization function of (1). In this section we will describe different aspects of the problem, present notations and highlight our main result."}, {"heading": "3.1 Background and Assumption", "text": "Gaussian width (A) can be seen as the expectation of the Gaussian process on the Gaussian process of the Gaussian process on the Gaussian process of the Gaussian process of the Gaussian. (A) and the Gaussian widthw (A) provides a geometric description of the size of the setA. (0, Ip \u00b7 p), a vector of independent zero-mean unit-variance Gaussian random variable.The Gaussian widthw (A) provides a geometric character of the setA. (0, Ip \u00b7 p) and the setA), where the constituent Gaussian process {Zu} where the constituent Gaussian random variables Zu = < u > are indexed by u. (0, Ip \u00b7 p). Then the Gaussian width w (A) can be seen as the expectation of the Gaussian process."}, {"heading": "3.2 Bounds on the regularization parameter", "text": "To achieve the recovery (27), one must have a clear boundary (27). However, the boundary on both sides depends on the unknown size of the samples. (Xn11, X n2 2 and is therefore random.) To overcome the above challenges, one can draw the expectation E [R] (R) (X) (X) (Xn11, X n2)) over all samples of size n1 and n2 (and thus obtain highly probable limits of deviation. The aim is to draw a sharp boundary on both sides, since the margin of error in (27) is directly proportional to the value. (1) In Theorem 1, we describe the expectation E [R) (L) (Xn11, X n2)."}, {"heading": "3.3 RSC Condition", "text": "In this section we note that the RSC condition for detecting direct changes (1) > u > Error (1). Simplifying the expression and applying the mean theorem twice on the left side of the RSC condition (26), on the left side of the RSC condition (0, 1) and on the right side (33) thus depends on the nonlinear terms of the loss function. Consider that the nonlinear term, the second term, in the loss function (1), which is the approximation of the protocol partition functions, depends only on n2. As a result, only samples of Xn22 affect the RSC conditions. Our analysis is an extension of the results to [1] using generic concatenation. We show that the RSC condition is most likely fulfilled."}, {"heading": "3.4 Statistical Recovery", "text": "Theorem 3: Consider two sets of i.i.d samples Xn11 = {x1i} n1 i = 1 and X n2 = {x2i} n2 i = 1. Then, with a probability of at least 1 \u2212 \u03b70e \u2212 2, define the following errors: n1, n2 \u2212 1 \u221a min (n1, n2) (w = R) +) (35) and for n2 \u2264 cw2 (Cr = Sd \u2212 1) with a high probability that the estimate is non1, n2 \u00b0 1 (n1, n2) (w = R) +) (35) and for n2 \u2264 cw2 (Cr = Sd \u2212 1), with a high probability that the estimate is nonsatisfactory."}, {"heading": "4 Experiments", "text": "In this section, we evaluate the generalized direct change estimator (directly) with three different standards. Then, we compare our direct approach with the indirect approach. For the indirect approach, we first estimate the Ising model structures \u03b8 1 and \u03b8 2 with the L1 standard regulator separately [22]. Then, from each output model, we obtain n = n1 = n2 = {20, 50, 100}.L1 standard: Here, we first create \u03b8 1 with three separate star subgraphs (Figure 4-a) with p = 50. We generate the weights uniformly randomly between {0.3 \u2212 0.5}. We then generate the weight structure 2 by removing 10 random edges from the target curve 1 (Figure 4-b).It is interesting that although individual diagrams are sparse, but direct approach has a better ROC curve. In this section, we create the weight structure 2 by removing 10 random edges from the target curve (Figure 1-4), which is more interesting."}, {"heading": "5 Conclusion", "text": "This paper presents the statistical analysis of the problem of direct change in graphic models of Ising, in which each standard can be used to characterize the parameter structure. An optimization algorithm based on FISTA-style algorithms is proposed with the convergence rate of O (1 / T 2). We provide the statistical analysis for each standard such as L1 standard, group-sparse standard, node disturbance, etc. Our analysis is based on generic concatenation and illustrates the important role of Gaussian widths (a geometrical measure of the size of suitable sets) in such results. In the specific case of sparseness, we obtain a sharper result than previous results [13] under the same assumption of smooth density ratio. Liu et al. [13] obtained the same result with an assumption of the limited density ratio, which is a more restrictive assumption, which represents a more restrictive assumption. Although we presented the results for the issue model, each of our graphical assumption can be applied to the assumption."}, {"heading": "A Background and Preliminaries", "text": "(Definition 2) Sub-Gaussian Random Vector of X is defined as a random variable. (Definition 2) Sub-Gaussian Random Vector of X is defined as a random variable. (Definition 2) Sub-Gaussian Vector of X (Definition 2). (Definition 3 Sub-Gaussian Random Vector of X: We say that a random vector X in Rn is sub-Gaussian if the onedimensional margins < X > are sub-Gaussian Random Variables for all x-Gaussian Rn. (Definition 3 Sub-Gaussian Random Vector of X). (Definition 2) Sub-Gaussian Random Vector of X, X > are sub-Gaussian Random Variables for all x-Gaussian Rn."}, {"heading": "B Regularization Parameter", "text": "Lemma 5 Let us consider two Ising models with real parameters."}, {"heading": "C RSC condition", "text": "s ri = r (X = x2i), and is smaller than the first hand in the second hand (I =)."}, {"heading": "Acknowledgment", "text": "The research was supported by NSF grants IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, NASA grants NNX12AQ39A, and donations from Adobe, IBM, and Yahoo. F. F. commends the support of the IDF (2014-2015) and DDF (2015-2016) by the University of Minnesota."}], "references": [{"title": "Estimation with Norm Regularization", "author": ["A. Banerjee", "S. Chen", "F. Fazayeli", "V. Sivakumar"], "venue": "Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "and A", "author": ["O. Banerjee", "L. El Ghaoui"], "venue": "d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. The Journal of Machine Learning Research, 9:485\u2013516", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, 2(1):183\u2013202", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Concentration Inequalities: A Nonasymptotic Theory of Independence", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": "Oxford University Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "A constrained l1 minimization approach to sparse precision matrix estimation", "author": ["T. Cai", "W. Liu", "X. Luo"], "venue": "Journal of the American Statistical Association, 106(494):594\u2013607", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "The Convex Geometry of Linear Inverse Problems", "author": ["V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky"], "venue": "Foundations of Computational Mathematics, 12(6):805\u2013849", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Structured estimation with atomic norms: General bounds and applications", "author": ["S. Chen", "A. Banerjee"], "venue": "Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "On Milman\u2019s Inequality and Random Subspaces Which Escape Through a Mesh inR", "author": ["Y. Gordon"], "venue": "Geometric Aspects of Functional Analysis, volume 1317 of Lecture Notes in Mathematics, pages 84\u2013106. Springer Berlin", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1988}, {"title": "Covariate shift by kernel mean matching", "author": ["A. Gretton", "A. Smola", "J. Huang", "M. Schmittfull", "K. Borgwardt", "B. Sch\u00f6lkopf"], "venue": "Dataset shift in machine learning, 3(4):5", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "A least-squares approach to direct importance estimation", "author": ["T. Kanamori", "S. Hido", "M. Sugiyama"], "venue": "The Journal of Machine Learning Research, 10:1391\u20131445", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes", "author": ["M. Ledoux", "M. Talagrand"], "venue": "Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Direct learning of sparse changes in markov networks by density ratio estimation", "author": ["S. Liu", "J.A. Quinn", "M.U. Gutmann", "T. Suzuki", "M. Sugiyama"], "venue": "Neural computation, 26(6):1169\u20131197", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Support consistency of direct sparse-change learning in markov networks", "author": ["S. Liu", "T. Suzuki", "M. Sugiyama"], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Support consistency of direct sparse-change learning in markov networks", "author": ["S. Liu", "T. Suzuki", "M. Sugiyama"], "venue": "Joutnal of Annals of Statistics", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Group sparse priors for covariance estimation", "author": ["B.M. Marlin", "M. Schmidt", "K.P. Murphy"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 383\u2013392. AUAI Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "High-dimensional graphs and variable selection with the lasso", "author": ["N. Meinshausen", "P. B\u00fchlmann"], "venue": "The Annals of Statistics, pages 1436\u20131462", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Reconstruction and subGaussian operators in asymptotic geometric analysis", "author": ["S. Mendelson", "A. Pajor", "N. Tomczak-Jaegermann"], "venue": "Geometric and Functional Analysis, 17:1248\u20131282", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Node-based learning of multiple gaussian graphical models", "author": ["K. Mohan", "P. London", "M. Fazel", "D. Witten", "S.-I. Lee"], "venue": "The Journal of Machine Learning Research, 15(1):445\u2013488", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers", "author": ["S.N. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Statistical Science, 27(4):538\u2013557", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. Nesterov"], "venue": "Mathematical programming, 103(1):127\u2013152", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Proximal algorithms", "author": ["N. Parikh", "S.P. Boyd"], "venue": "Foundations and Trends in Optimization, 1(3):127\u2013239", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "High-dimensional ising model selection using 1-regularized logistic regression", "author": ["P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty"], "venue": "The Annals of Statistics, 38(3):1287\u20131319", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "High-dimensional covariance estimation by minimizing 1-penalized log-determinant divergence", "author": ["P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B. Yu"], "venue": "Electronic Journal of Statistics, 5:935\u2013980", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient learning using forward-backward splitting", "author": ["Y. Singer", "J.C. Duchi"], "venue": "Neural Information Processing Systems, pages 495\u2013503", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Direct importance estimation with model selection and its application to covariate shift adaptation", "author": ["M. Sugiyama", "S. Nakajima", "H. Kashima", "P.V. Buenau", "M. Kawanabe"], "venue": "Advances in neural information processing systems, pages 1433\u20131440", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Density ratio estimation in machine learning", "author": ["M. Sugiyama", "T. Suzuki", "T. Kanamori"], "venue": "Cambridge University Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Majorizing measures: the generic chaining", "author": ["M. Talagrand"], "venue": "The Annals of Probability, pages 1049\u20131103", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Majorizing measures without measures", "author": ["M. Talagrand"], "venue": "Annals of probability, pages 411\u2013417", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "The Generic Chaining", "author": ["M. Talagrand"], "venue": "Springer Monographs in Mathematics. Springer Berlin", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Upper and Lower Bounds for Stochastic Processes", "author": ["M. Talagrand"], "venue": "Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical inference problems and their rigorous solutions", "author": ["V. Vapnik", "R. Izmailov"], "venue": "Statistical Learning and Data Sciences, pages 33\u201371. Springer International Publishing", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimation in high dimensions: a geometric perspective", "author": ["R. Vershynin"], "venue": "Sampling Theory, a Renaissance", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programmming ( Lasso )", "author": ["M.J. Wainwright"], "venue": "IEEE Transactions on Information Theory, 55(5):2183\u20132201", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Graphical models via generalized linear models", "author": ["E. Yang", "A. Genevera", "Z. Liu", "P.K. Ravikumar"], "venue": "Advances in Neural Information Processing Systems, pages 1358\u20131366", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient methods for overlapping group lasso", "author": ["L. Yuan", "J. Liu", "J. Ye"], "venue": "Advances in Neural Information Processing Systems, pages 352\u2013360", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Direct estimation of differential networks", "author": ["S. Zhao", "T. Cai", "H. Li"], "venue": "Biometrika, 101(2):253\u2013268", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].", "startOffset": 233, "endOffset": 256}, {"referenceID": 9, "context": "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].", "startOffset": 233, "endOffset": 256}, {"referenceID": 15, "context": "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].", "startOffset": 233, "endOffset": 256}, {"referenceID": 21, "context": "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].", "startOffset": 233, "endOffset": 256}, {"referenceID": 22, "context": "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].", "startOffset": 233, "endOffset": 256}, {"referenceID": 33, "context": "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].", "startOffset": 233, "endOffset": 256}, {"referenceID": 5, "context": "In particular, we focus on the situation when the change \u03b4\u03b8\u2217 has structure, such as sparsity, block sparsity, or node-perturbed sparsity, which can be characterized by a suitable (atomic) norm [6, 18].", "startOffset": 193, "endOffset": 200}, {"referenceID": 17, "context": "In particular, we focus on the situation when the change \u03b4\u03b8\u2217 has structure, such as sparsity, block sparsity, or node-perturbed sparsity, which can be characterized by a suitable (atomic) norm [6, 18].", "startOffset": 193, "endOffset": 200}, {"referenceID": 4, "context": "In a high dimensional setting, recent advances [5, 22, 23] illustrate that accurate estimation of the parameter \u03b8\u2217 of an Ising model depends on how sparse or otherwise structured the true parameter \u03b8\u2217 is.", "startOffset": 47, "endOffset": 58}, {"referenceID": 21, "context": "In a high dimensional setting, recent advances [5, 22, 23] illustrate that accurate estimation of the parameter \u03b8\u2217 of an Ising model depends on how sparse or otherwise structured the true parameter \u03b8\u2217 is.", "startOffset": 47, "endOffset": 58}, {"referenceID": 22, "context": "In a high dimensional setting, recent advances [5, 22, 23] illustrate that accurate estimation of the parameter \u03b8\u2217 of an Ising model depends on how sparse or otherwise structured the true parameter \u03b8\u2217 is.", "startOffset": 47, "endOffset": 58}, {"referenceID": 21, "context": "are sparse and the samples n1, n2 are sufficient to estimate them accurately [22], indirect estimation of \u03b4\u03b8\u0302 should be accurate.", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "However, if the individual parameters \u03b8\u2217 1 and \u03b8 \u2217 2 are somewhat dense, and the change \u03b4\u03b8 \u2217 has considerably more structure, such as block sparsity (only a small block has changed) or node perturbation sparsity (only edges from a few nodes have changed) [18], direct estimation may be considerably more efficient both in terms of the number of samples required as well as the computation time.", "startOffset": 255, "endOffset": 259}, {"referenceID": 12, "context": "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].", "startOffset": 125, "endOffset": 144}, {"referenceID": 9, "context": "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].", "startOffset": 125, "endOffset": 144}, {"referenceID": 24, "context": "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].", "startOffset": 125, "endOffset": 144}, {"referenceID": 25, "context": "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].", "startOffset": 125, "endOffset": 144}, {"referenceID": 30, "context": "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].", "startOffset": 125, "endOffset": 144}, {"referenceID": 13, "context": "[14] improved the sample complexity to min(n1, n2) = O(s log p) when a bounded density ratio model is assumed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] considered estimating direct sparse changes in Gaussian graphical models (GGMs).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19].", "startOffset": 184, "endOffset": 198}, {"referenceID": 5, "context": "In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19].", "startOffset": 184, "endOffset": 198}, {"referenceID": 17, "context": "In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19].", "startOffset": 184, "endOffset": 198}, {"referenceID": 18, "context": "In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19].", "startOffset": 184, "endOffset": 198}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In particular, when \u03b4\u03b8\u2217 is sparse and our estimator is run with L1 norm, we get a sample complexity of n1 = n2 = O(s log p) which is sharper than n1 = O(s 2 log p) and n2 = O(n1) in [13].", "startOffset": 182, "endOffset": 186}, {"referenceID": 12, "context": "The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter \u03bbn1,n2 depends on the sample size for both Ising models.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter \u03bbn1,n2 depends on the sample size for both Ising models.", "startOffset": 104, "endOffset": 114}, {"referenceID": 5, "context": "The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter \u03bbn1,n2 depends on the sample size for both Ising models.", "startOffset": 104, "endOffset": 114}, {"referenceID": 18, "context": "The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter \u03bbn1,n2 depends on the sample size for both Ising models.", "startOffset": 104, "endOffset": 114}, {"referenceID": 12, "context": "[13] build on the primal-dual witness approach of Wainwright [33], which is effective for the special case of L1 norm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[13] build on the primal-dual witness approach of Wainwright [33], which is effective for the special case of L1 norm.", "startOffset": 61, "endOffset": 65}, {"referenceID": 29, "context": "Our analysis is largely geometric, where generic chaining [30] plays a key role, and our results are in terms of Gaussian widths of suitable sets associated with the norm [1, 6].", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "Our analysis is largely geometric, where generic chaining [30] plays a key role, and our results are in terms of Gaussian widths of suitable sets associated with the norm [1, 6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 5, "context": "Our analysis is largely geometric, where generic chaining [30] plays a key role, and our results are in terms of Gaussian widths of suitable sets associated with the norm [1, 6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 8, "context": "Then, we explain how to develop the loss function L(\u03b4\u03b8;X1 1 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate \u03b4\u03b8 = \u03b81 \u2212 \u03b82, and finally we describe how to solve the optimization problem (1) for any norm R(\u03b4\u03b8).", "startOffset": 97, "endOffset": 112}, {"referenceID": 9, "context": "Then, we explain how to develop the loss function L(\u03b4\u03b8;X1 1 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate \u03b4\u03b8 = \u03b81 \u2212 \u03b82, and finally we describe how to solve the optimization problem (1) for any norm R(\u03b4\u03b8).", "startOffset": 97, "endOffset": 112}, {"referenceID": 24, "context": "Then, we explain how to develop the loss function L(\u03b4\u03b8;X1 1 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate \u03b4\u03b8 = \u03b81 \u2212 \u03b82, and finally we describe how to solve the optimization problem (1) for any norm R(\u03b4\u03b8).", "startOffset": 97, "endOffset": 112}, {"referenceID": 30, "context": "Then, we explain how to develop the loss function L(\u03b4\u03b8;X1 1 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate \u03b4\u03b8 = \u03b81 \u2212 \u03b82, and finally we describe how to solve the optimization problem (1) for any norm R(\u03b4\u03b8).", "startOffset": 97, "endOffset": 112}, {"referenceID": 11, "context": "al [12, 13], a direct estimate for the changes detection problem based on density ratio can be posed as follows r(X = x|\u03b4\u03b8) = p(X = x|\u03b81) p(X = x|\u03b82) = exp{\u3008T (x), \u03b81\u3009} exp{\u3008T (x), \u03b82\u3009} } {{ } r\u2217(x|\u03b4\u03b8) Z(\u03b82) Z(\u03b81) } {{ } 1/Z(\u03b4\u03b8) = exp{\u3008T (x), \u03b4\u03b8\u3009)} Z(\u03b4\u03b8) , (6)", "startOffset": 3, "endOffset": 11}, {"referenceID": 12, "context": "al [12, 13], a direct estimate for the changes detection problem based on density ratio can be posed as follows r(X = x|\u03b4\u03b8) = p(X = x|\u03b81) p(X = x|\u03b82) = exp{\u3008T (x), \u03b81\u3009} exp{\u3008T (x), \u03b82\u3009} } {{ } r\u2217(x|\u03b4\u03b8) Z(\u03b82) Z(\u03b81) } {{ } 1/Z(\u03b4\u03b8) = exp{\u3008T (x), \u03b4\u03b8\u3009)} Z(\u03b4\u03b8) , (6)", "startOffset": 3, "endOffset": 11}, {"referenceID": 2, "context": "In this section, we present an algorithm in the class of Fast Iterative Shrinkage-Thresholding Algorithms (FISTA) for efficiently solving the problem (1) [3].", "startOffset": 154, "endOffset": 157}, {"referenceID": 20, "context": "In general, the proximal operator proxh(x) of a closed proper convex function h : R 7\u2192 R \u222a {+\u221e} [21] is defined as proxh(x) = argmin u ( h(u) + 1 2 \u2016u\u2212 x\u20162 ) .", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "(14) Thus, the unique minimizer (13) correspond to prox \u03bb LR ( \u03b4\u03b8t \u2212 1 L\u2207L(\u03b4\u03b8t) ) which has rate of convergence ofO(1/t) [20, 21].", "startOffset": 121, "endOffset": 129}, {"referenceID": 20, "context": "(14) Thus, the unique minimizer (13) correspond to prox \u03bb LR ( \u03b4\u03b8t \u2212 1 L\u2207L(\u03b4\u03b8t) ) which has rate of convergence ofO(1/t) [20, 21].", "startOffset": 121, "endOffset": 129}, {"referenceID": 2, "context": "To improve the rate of convergence, we adapt the idea of FISTA algorithm [3].", "startOffset": 73, "endOffset": 76}, {"referenceID": 19, "context": "The choice of \u03b1t+1 follows Nesterov\u2019s accelerated gradient descent [20, 21] and is detailed in Algorithm 1.", "startOffset": 67, "endOffset": 75}, {"referenceID": 20, "context": "The choice of \u03b1t+1 follows Nesterov\u2019s accelerated gradient descent [20, 21] and is detailed in Algorithm 1.", "startOffset": 67, "endOffset": 75}, {"referenceID": 2, "context": "(16) The algorithm has a rate of convergence of O(1/t) [3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 23, "context": ") is given by the elementwise soft-thresholding operation [24] as [ prox \u03bb L\u2016.", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "We will focus on the case when R(\u03b4\u0398) = \u2211NG g=1 \u2016\u03b4\u0398(s, t) : s, t \u2208 Gg\u2016F [15].", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "Node perturbation: Another example is the row-column overlap norm (RCON) [18] to capture perturbed nodes i.", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "This norm can be viewed as overlapping group lasso [18] and thus can be solved by applying Algorithm 1 with proximal operator for overlapping group lasso [35].", "startOffset": 51, "endOffset": 55}, {"referenceID": 34, "context": "This norm can be viewed as overlapping group lasso [18] and thus can be solved by applying Algorithm 1 with proximal operator for overlapping group lasso [35].", "startOffset": 154, "endOffset": 158}, {"referenceID": 17, "context": "\u03b4\u0398 = V + V T , (23) and solve it by applying in-exact ADMM techniques [18].", "startOffset": 70, "endOffset": 74}, {"referenceID": 5, "context": "1 Background and Assumption Gaussian Width: In several of our proofs, we use the concept of Gaussian width [6, 8], which is defined as follows.", "startOffset": 107, "endOffset": 113}, {"referenceID": 7, "context": "1 Background and Assumption Gaussian Width: In several of our proofs, we use the concept of Gaussian width [6, 8], which is defined as follows.", "startOffset": 107, "endOffset": 113}, {"referenceID": 3, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].", "startOffset": 179, "endOffset": 194}, {"referenceID": 10, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].", "startOffset": 179, "endOffset": 194}, {"referenceID": 28, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].", "startOffset": 179, "endOffset": 194}, {"referenceID": 29, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].", "startOffset": 179, "endOffset": 194}, {"referenceID": 0, "context": "[1] show that for any convex loss function the error vector \u2206 = (\u03b4\u03b8\u2217 \u2212 \u03b4\u03b8\u0302) lies in a restricted set that is characterized as Er = Er(\u03b4\u03b8 \u2217, \u03b2) = { \u2206 \u2208 R \u2223\u2223\u2223\u2223 R(\u03b4\u03b8\u2217 + \u2206) \u2264 R(\u03b4\u03b8\u2217) + 1 \u03b2 } .", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "Restricted Strong Convexity (RSC) Condition: The sample complexity of the problem (1) depends on the RSC condition [19], which ensures that the estimation problem is strongly convex in the neighborhood of the optimal parameter [1, 19].", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "Restricted Strong Convexity (RSC) Condition: The sample complexity of the problem (1) depends on the RSC condition [19], which ensures that the estimation problem is strongly convex in the neighborhood of the optimal parameter [1, 19].", "startOffset": 227, "endOffset": 234}, {"referenceID": 18, "context": "Restricted Strong Convexity (RSC) Condition: The sample complexity of the problem (1) depends on the RSC condition [19], which ensures that the estimation problem is strongly convex in the neighborhood of the optimal parameter [1, 19].", "startOffset": 227, "endOffset": 234}, {"referenceID": 0, "context": "[1] show a deterministic upper bound for \u2016\u2206\u20162 in terms of \u03bbn1,n2 , \u03ba, and the norm compatibility constant \u03a8(Cr) = supu\u2208Cr R(u) \u2016u\u20162 , as \u2016\u2206\u20162 \u2264 1 + \u03b2 \u03b2 \u03bbn1,n2 \u03ba \u03a8(Cr) .", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "The upper bound on Gaussian width of the unit norm-ball of R for atomic norms which covers a wide range of norms is provided in [6, 7].", "startOffset": 128, "endOffset": 134}, {"referenceID": 6, "context": "The upper bound on Gaussian width of the unit norm-ball of R for atomic norms which covers a wide range of norms is provided in [6, 7].", "startOffset": 128, "endOffset": 134}, {"referenceID": 0, "context": "Simplifying the expression and applying mean value theorem twice on the left side of RSC condition (26), for \u2200\u03b3i \u2208 [0, 1], we have \u03b4L(\u03b4\u03b8\u2217, u) := L(\u03b4\u03b8\u2217 + u)\u2212 L(\u03b4\u03b8\u2217)\u2212 \u3008\u2207L(\u03b4\u03b8\u2217), u\u3009 \u2265 uT\u22072L(\u03b4\u03b8\u2217 + \u03b3iu)u.", "startOffset": 115, "endOffset": 121}, {"referenceID": 0, "context": "Our analysis is an extension of the results on [1] using the generic chaining.", "startOffset": 47, "endOffset": 50}, {"referenceID": 6, "context": "The bound on Gaussian width of the error set for atomic norms has been provided in [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 21, "context": "For indirect approach, we first estimate Ising model structures \u03b8\u03021 and \u03b8\u03022 with L1 norm regularizer, separately [22].", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "For the special case of sparsity, we obtain a sharper result than previous results [13] under the same smooth density ratio assumption.", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "[13] obtained the same result with a bounded density ratio assumption which is a more restrictive assumption.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "1 Generic Chaining Definition 4 (Majorizing measure [27]) Given \u03b1 > 0, and a metric space (T, d) (that need not be finite), we define \u03b3\u03b1(T, d) = inf sup t \u2211 n\u22650 2\u2206(An(T )).", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "We use the traditional definition of majorizing measure \u03b3\u03b1,1(T, d) from [27] \u03b3\u03b1,1(T, d) = inf sup t (\u222b \u221e", "startOffset": 72, "endOffset": 76}, {"referenceID": 27, "context": "Note that \u03b3\u03b1,1(T, d) coincides with the functional \u03b3\u03b1(T, d) [28] as K(\u03b1)\u03b3\u03b1(T, d) \u2264 \u03b3\u03b1,1(T, d) \u2264 K(\u03b1)\u03b3\u03b1(T, d), (46) where K(\u03b1) is a constant depending on \u03b1 only.", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "7] in [29] Consider a set T provided with two distances d1 and d2.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "9] in [29] Under the conditions of Theorem 4, for all values of u1, u2 > 0 we have P (|Xs \u2212Xt0| \u2265 L(\u03b31(T, d1) + \u03b32(T, d2)) + u1D1 + u2D2) \u2264 L exp(\u2212min(u2, u1)), (51) where Dj = 2 \u2211 n\u22650 en(T, dj).", "startOffset": 6, "endOffset": 10}, {"referenceID": 31, "context": "2 (Fernique-Talagrand\u2019s comparison theorem)] in [32] Let T be an arbitrary set.", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "Theorem 7 (Mendelson, Pajor, Tomczak-Jaegermann [17]) There exist absolute constants c1, c2, c3 for which the following holds.", "startOffset": 48, "endOffset": 52}, {"referenceID": 29, "context": "27 in [30] and we have", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "Recall, RSC condition definition as \u03b4L(\u03b4\u03b8\u2217,u) := L(\u03b4\u03b8\u2217 + u;X1 1 ,X n2 2 )\u2212 L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 )\u2212 \u3008\u2207L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 ),u\u3009 \u2265 \u03ba\u2016u\u20162 (118) Simplifying the expression and applying mean value theorem twice on the left side of RSC condition (26), for \u2200\u03b3i \u2208 [0, 1], we have \u03b4L(\u03b4\u03b8\u2217,u) := L(\u03b4\u03b8\u2217 + u;X1 1 ,X n2 2 )\u2212 L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 )\u2212 \u3008\u2207L(\u03b4\u03b8\u2217;X n1 1 ,X n2 2 ),u\u3009 \u2265 u\u2207L(\u03b4\u03b8\u0303;X1 1 ,X n2 2 )u, (119) where \u03b4\u03b8\u0303 = \u03b4\u03b8\u2217 + \u03b3iu.", "startOffset": 256, "endOffset": 262}], "year": 2016, "abstractText": "We consider the problem of estimating change in the dependency structure between two p-dimensional Ising models, based on respectively n1 and n2 samples drawn from the models. The change is assumed to be structured, e.g., sparse, block sparse, node-perturbed sparse, etc., such that it can be characterized by a suitable (atomic) norm. We present and analyze a norm-regularized estimator for directly estimating the change in structure, without having to estimate the structures of the individual Ising models. The estimator can work with any norm, and can be generalized to other graphical models under mild assumptions. We show that only one set of samples, say n2, needs to satisfy the sample complexity requirement for the estimator to work, and the estimation error decreases as c \u221a min(n1,n2) , where c depends on the Gaussian width of the unit norm ball. For example, for `1 norm applied to s-sparse change, the change can be accurately estimated with min(n1, n2) = O(s log p) which is sharper than an existing result n1 = O(s 2 log p) and n2 = O(n1). Experimental results illustrating the effectiveness of the proposed estimator are presented.", "creator": "LaTeX with hyperref package"}}}