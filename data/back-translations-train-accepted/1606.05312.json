{"id": "1606.05312", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Successor Features for Transfer in Reinforcement Learning", "abstract": "Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. Our focus is on transfer where the reward functions vary across tasks while the environment's dynamics remain the same. The method we propose rests on two key ideas: \"successor features,\" a value function representation that decouples the dynamics of the environment from the rewards, and \"generalized policy improvement,\" a generalization of dynamic programming's policy improvement step that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows transfer to take place between tasks without any restriction. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice.", "histories": [["v1", "Thu, 16 Jun 2016 18:45:32 GMT  (84kb,D)", "http://arxiv.org/abs/1606.05312v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["andr\\'e barreto", "r\\'emi munos", "tom schaul", "david silver"], "accepted": true, "id": "1606.05312"}, "pdf": {"name": "1606.05312.pdf", "metadata": {"source": "CRF", "title": "Successor Features for Transfer in Reinforcement Learning", "authors": ["Andr\u00e9 Barreto", "R\u00e9mi Munos", "Tom Schaul", "David Silver"], "emails": ["andrebarreto@google.com", "munos@google.com", "schaul@google.com", "davidsilver@google.com"], "sections": [{"heading": "1 Introduction", "text": "It is in such a way that it concerns a pure \"yes,\" a \"yes,\" a \"yes,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"\" no, \"a\" \"no,\" a \"\" no, \"a\" \"no,\" a \"\" no, \"a\" \"no,\" a \"\" \"no,\" \"a\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "2 Background and Problem Formulation", "text": "We look at the framework of RL outlined in the introduction: an agent interacts with an environment and selects measures to maximize the expected reward in the long run. [21] As usual, we assume that this interaction can be modelled as a Markov decision-making process (MDP, Puterman, [17]). An MDP is defined as a tuple of M \u2261 (S, A, p, r, \u03b3). Sets S and A are respectively the state and action spaces; here we assume that S and A are finite whenever such adoption facilitates presentation, but most of the ideas willingly extend to continuous spaces. For each s, S, r, r, and a, the function is p (\u00b7 s, a) giving the distribution of the next state according to actions in the state. We will often refer to p (\u00b7 s, a) as the dynamics of the MDP. The reward received during transition is an \u2212 s, is given by r (s)."}, {"heading": "3 Successor Features", "text": "In this section, we present the concept that will serve as the cornerstone for the rest of the paper. We begin by presenting a simple reward model = > rt = > + and then show how it naturally leads to a generalization of Dayan's [7] successor presentation (SR). Let's assume that the one-step expected reward associated with the state-action pair (s, a) is given by us. Let's assume that (2) true is not restrictive because we don't make assumptions about these assumptions (s, a): If we have such an arrangement (s, a), we can be the properties of (s, a) and w) namiRd, we will clearly recreate each reward function accurately. To simplify the notation, let's leave it (st, at) by simply rewriting the definition of the action value function."}, {"heading": "4 Transfer Via Successor Features", "text": "In this section we return to our discussion of the transfer in RL. As described, we are interested in the scenario in which all components of an MDP are fixed, except for the reward function. One way to formalize this model is through (2): if we assume that this behavior is fixed, any type of artificial behavior leads to a new MDP. Based on this observation, we can define that we have a new MDP task (S, A, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E,"}, {"heading": "4.1 Generalized Policy Improvement", "text": "This is the driving force behind DP, and any RL algorithm that uses the notion of a value function exploits Bellman's result in one way or another. In this section, we expand the policy enhancement theorem to include the scenario in which the new policy is to be calculated based on the value functions of a set of strategies. We show that this expansion can be done in a very natural way by simply being greedy with regard to the maximum value functions available. Our result is extended to the scenario in which the new policy is to be calculated based on the value functions of a set of strategies. We show that this expansion can be done in a very natural way by simply accessing the maximum available value functions."}, {"heading": "4.2 Generalized Policy Improvement with Successor Features", "text": "We begin this section by slightly expanding our notation to make it easier to refer to the quantities involved in learning transfer. Let Mi be a task defined by wi-Rd. We will use these quantities to refer to an optimal policy of MDP Mi and use Q functions to refer to its value function. The value function of \u03c0 i when performed in Mj-Rd is designated by Q-Rp, whose performance is performed by Q-Rp. Let us now assume that an agent continues to apply the optimal policy for tasks M1, M2,..., Mn-Mp-Mp tasks when subjected to a new task, the agent will compute Q-Rp tasks whose performance is calculated by i-Rp numbers - the value functions of the policy induced by wn + 1. In this case, we will apply the value functions recalculated on theorem 1."}, {"heading": "5 Experiments", "text": "In this section, we use experiments to illustrate how transfer is promoted by combining generalized policy iterations and SFs in practice. (To do this, we present a generalized version of a classic RL task known as \"Puddle World.\" [20] Puddle World is a simple two-dimensional problem with a target position and two elliptical \"puddle images,\" a vertical and a horizontal task [20]. An action fails with a 0.1 probability of randomly selecting an action, the goal being to achieve the goal while avoiding the puddle world along the path by changing the position of the puddle worlds and the target state in arbitrary time steps. More specifically, we implemented the task as a 15-15-15 grid, limiting the position of the elements to a subset of cells."}, {"heading": "6 Related Work", "text": "In this paper, we present SFs, a representation scheme for value functions, and show how they provide a natural framework for the implementation of transfers in RL. Both representation and transfer are active areas of research in RL; in what follows, we briefly describe the previous work, which we consider more closely related, to represent the value function [14, 10, 16, 15]. Many of these approaches are based on the fact that if S is limited, the value function of a political function of a political model given by v\u03c0 = 0 (in relation to the development of methods for automatically calculating good properties to represent the value function [14, 10, 16, 15]. Many of these approaches build on the fact that if S is limited, the value function of a political model sp. [xp] is given a political function given by v\u03c0 = 0 (in relation to P\u03c0)."}, {"heading": "7 Conclusion", "text": "This paper builds on two concepts, both of which are generalizations of earlier ideas: the first concept is SFs, a generalization of Dayan's [7] SR that extends the original definition of discrete to continuous spaces and also facilitates the inclusion of functional alignments; the second concept is general policy improvement formalized in Theorem 1. As the name suggests, this result broadens Bellman's [4] classical policy improvement theorem from a single policy to several policies. Although SFs and general policy improvement are of interest in themselves, in this paper we focus on their combination to bring about a transfer; the resulting framework is an elegant extension of the basic framework of DP that provides a solid basis for transferring to RL. We derive a theoretical result, Theorem 2, which formalizes the intuition that an agent should function well on a novel task when he has already seen a similar task."}, {"heading": "Acknowledgments", "text": "The authors thank Joseph Modayil and Hado van Hasselt for the valuable discussions during the development of the ideas described in this paper. We also thank Peter Dayan, Matt Botvinick, Marc Bellemare and Guy Lever for all the excellent comments. Finally, we thank Dan Horgan and Alexander Pritzel for their help with the experiments."}, {"heading": "A Proofs", "text": "Theorem 1 (generalized political improvement) Let us start with that Q2,..., Q1, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, i, i, i, i, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi, Qi"}], "references": [{"title": "Convex multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Effective control knowledge transfer through learning skill and representation hierarchies", "author": ["Mehran Asadi", "Manfred Huber"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "A comparison of direct and model-based reinforcement learning", "author": ["Christopher G. Atkeson", "J. Santamaria"], "venue": "In Proceedings of the IEEE International Conference on Robotics and Automation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Dynamic Programming", "author": ["Richard E. Bellman"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1957}, {"title": "Technical update: Least-squares temporal difference learning", "author": ["Justin A. Boyan"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Improving generalization for temporal difference learning: The successor representation", "author": ["Peter Dayan"], "venue": "Neural Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Predictive representations of state", "author": ["Michael L. Littman", "Richard S. Sutton", "Satinder Singh"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes", "author": ["Sridhar Mahadevan", "Mauro Maggioni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Andrew Ng", "Stuart Russell"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Analyzing feature generation for value-function approximation", "author": ["Ronald Parr", "Christopher Painter-Wakefield", "Lihong Li", "Michael Littman"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning", "author": ["Ronald Parr", "Lihong Li", "Gavin Taylor", "Christopher Painter-Wakefield", "Michael L. Littman"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "An analysis of laplacian methods for value function approximation in MDPs", "author": ["Marek Petrik"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Markov Decision Processes\u2014Discrete Stochastic Dynamic Programming", "author": ["Martin L. Puterman"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Universal Value Function Approximators", "author": ["Tom Schaul", "Daniel Horgan", "Karol Gregor", "David Silver"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["Alexander L. Strehl", "Michael L. Littman"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["Richard S. Sutton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1996}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Between MDPs and semi-MDPs: a framework for temporal abstraction in reinforcement learning", "author": ["Richard S. Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Matthew E. Taylor", "Peter Stone"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}], "referenceMentions": [{"referenceID": 19, "context": "Reinforcement learning (RL) provides a framework for the development of situated agents that learn how to behave while interacting with the environment [21].", "startOffset": 152, "endOffset": 156}, {"referenceID": 21, "context": "Transfer in reinforcement learning can be defined in many different ways, but in general it refers to the notion that generalization should occur not only within a task but also across tasks [23].", "startOffset": 191, "endOffset": 195}, {"referenceID": 5, "context": "The first one is a generalization of a concept proposed by Dayan [7] called successor representation.", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "The first one is a generalization of Bellman\u2019s [4] classic policy improvement theorem that extends the original result from one to multiple decision policies.", "startOffset": 47, "endOffset": 50}, {"referenceID": 19, "context": "We consider the framework of RL outlined in the introduction: an agent interacts with an environment and selects actions in order to maximize the expected amount of reward received in the long run [21].", "startOffset": 197, "endOffset": 201}, {"referenceID": 15, "context": "As usual, we assume that this interaction can be modeled as a Markov decision process (MDP, Puterman, [17]).", "startOffset": 102, "endOffset": 106}, {"referenceID": 15, "context": "One way to address this problem is to use methods derived from dynamic programming (DP), which heavily rely on the concept of a value function [17].", "startOffset": 143, "endOffset": 147}, {"referenceID": 19, "context": "These two steps, policy evaluation and policy improvement, define the basic mechanics of RL algorithms based on DP; under certain conditions their successive application leads to an optimal policy \u03c0\u2217 that maximizes the expected return from every state in S [21].", "startOffset": 257, "endOffset": 261}, {"referenceID": 5, "context": "We start by presenting a simple reward model and then show how it naturally leads to a generalization of Dayan\u2019s [7] successor representation (SR).", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "This is essentially the concept of SR extended from the space S to the set S\u00d7A [7].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "Although this fact is hinted at in Dayan\u2019s [7] paper, it becomes much more apparent when we look at SR as a particular case of SFs.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "SFs extend Dayan\u2019s [7] SR in two ways.", "startOffset": 19, "endOffset": 22}, {"referenceID": 6, "context": "Note that r(s, a) \u2248 \u03c6(s, a)>w is a supervised learning problem, so one can resort to one of the many wellunderstood techniques from the field to learn w (and potentially \u03c6, too)[8].", "startOffset": 177, "endOffset": 180}, {"referenceID": 5, "context": "that is, SFs satisfy a Bellman equation in which \u03c6i play the role of rewards\u2014something also noted by Dayan [7] regarding SR.", "startOffset": 107, "endOffset": 110}, {"referenceID": 19, "context": "Therefore, in principle any RL method can be used to compute \u03c8 [21, 6].", "startOffset": 63, "endOffset": 70}, {"referenceID": 4, "context": "Therefore, in principle any RL method can be used to compute \u03c8 [21, 6].", "startOffset": 63, "endOffset": 70}, {"referenceID": 5, "context": "This helps explain the good performance of SR in Dayan\u2019s [7] experiments and may also serve as an argument in favor of adopting SFs as a general approximation scheme for RL.", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "2 Despite the fact that similar assumptions have been made in the literature [1], we now describe some illustrative examples that suggest that our formulation ofM is a natural way of modeling some scenarios of interest.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "One of the key results in DP is Bellman\u2019s [4] policy improvement theorem.", "startOffset": 42, "endOffset": 45}, {"referenceID": 18, "context": "In order to do so we introduce a generalized version of a classic RL task known as the \u201cpuddle world\u201d [20].", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "The puddle world is a simple two-dimensional problem with a goal position and two elliptical \u201cpuddles,\u201d one vertical and one horizontal [20].", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "15 [21].", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "The former was learned using temporal-difference updates to solve (4) [21], while the latter was learned as a least-squares minimization of the difference between the two sides of (2).", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "When it comes to representation, a lot of effort in previous research has been directed towards the development of methods to automatically compute good features to represent the value function [14, 10, 16, 15].", "startOffset": 194, "endOffset": 210}, {"referenceID": 8, "context": "When it comes to representation, a lot of effort in previous research has been directed towards the development of methods to automatically compute good features to represent the value function [14, 10, 16, 15].", "startOffset": 194, "endOffset": 210}, {"referenceID": 14, "context": "When it comes to representation, a lot of effort in previous research has been directed towards the development of methods to automatically compute good features to represent the value function [14, 10, 16, 15].", "startOffset": 194, "endOffset": 210}, {"referenceID": 13, "context": "When it comes to representation, a lot of effort in previous research has been directed towards the development of methods to automatically compute good features to represent the value function [14, 10, 16, 15].", "startOffset": 194, "endOffset": 210}, {"referenceID": 10, "context": "Although the methods above decouple the construction of features from the actual RL problem, it is also possible to tackle both problems concomitantly, using general nonlinear function approximators to incrementally learn \u03c8(s, a) [12].", "startOffset": 230, "endOffset": 234}, {"referenceID": 0, "context": "Another interesting possibility is the definition of a clear protocol to also learn \u03c6(s, a), which is closely related to the problem known as \u201cmulti-task feature learning\u201d [1].", "startOffset": 172, "endOffset": 175}, {"referenceID": 9, "context": "Here again the use of nonlinear approximators may be useful, since with them it may be possible to embed an arbitrary family of MDPs into a modelM with the structure shown in (5) [11].", "startOffset": 179, "endOffset": 183}, {"referenceID": 7, "context": "\u2019s [9] predictive state representation (PSR).", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "A scheme that is perhaps closer to SFs is the value function representation sometimes adopted in inverse RL [13].", "startOffset": 108, "endOffset": 112}, {"referenceID": 21, "context": "As mentioned in the introduction, the problem of transfer has many definitions in the literature [23].", "startOffset": 97, "endOffset": 101}, {"referenceID": 2, "context": "One of them is to learn a model of the MDPs\u2019 dynamics [3].", "startOffset": 54, "endOffset": 57}, {"referenceID": 16, "context": "\u2019s [18] universal value function approximators (UVFAs) are particularly relevant to our work.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "\u2019s [22] options, acting greedily with respect to the maximum over their value functions corresponds in some sense to planning at a higher level of temporal abstraction.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "The first one is SFs, a generalization of Dayan\u2019s [7] SR that extends the original definition from discrete to continuous spaces and also facilitates the incorporation of function approximation.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "As the name suggests, this result extends Bellman\u2019s [4] classic policy improvement theorem from a single to multiple policies.", "startOffset": 52, "endOffset": 55}], "year": 2016, "abstractText": "Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. Our focus is on transfer where the reward functions vary across tasks while the environment\u2019s dynamics remain the same. The method we propose rests on two key ideas: \u201csuccessor features,\u201d a value function representation that decouples the dynamics of the environment from the rewards, and \u201cgeneralized policy improvement,\u201d a generalization of dynamic programming\u2019s policy improvement step that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows transfer to take place between tasks without any restriction. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice.", "creator": "LaTeX with hyperref package"}}}