{"id": "1606.05313", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Unsupervised Risk Estimation Using Only Conditional Independence Structure", "abstract": "We show how to estimate a model's test error from unlabeled data, on distributions very different from the training distribution, while assuming only that certain conditional independencies are preserved between train and test. We do not need to assume that the optimal predictor is the same between train and test, or that the true distribution lies in any parametric family. We can also efficiently differentiate the error estimate to perform unsupervised discriminative learning. Our technical tool is the method of moments, which allows us to exploit conditional independencies in the absence of a fully-specified model. Our framework encompasses a large family of losses including the log and exponential loss, and extends to structured output settings such as hidden Markov models.", "histories": [["v1", "Thu, 16 Jun 2016 18:48:51 GMT  (785kb,D)", "http://arxiv.org/abs/1606.05313v1", "15 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["jacob steinhardt", "percy s liang"], "accepted": true, "id": "1606.05313"}, "pdf": {"name": "1606.05313.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Risk Estimation Using Only Conditional Independence Structure", "authors": ["Jacob Steinhardt", "Percy Liang"], "emails": ["jsteinhardt@cs.stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "How can we assess the accuracy of a model when the test distribution is very different from the training distribution? To answer this question, we need to investigate the problem of unsupervised risk assessment (Donmez et al., 2010) - that is, a loss function is predicted that includes only access to m unmarked examples x (1: m), the risk R (\u03b8) def = Ex, y \u00b2) regardless of the risk assessment allows us to estimate the accuracy of the model on a novel input distribution and is therefore important for building reliable machine learning systems. In addition, it also provides a way to use unmarked data for learning: by minimizing the estimated risk above the risks, we can perform unattended learning and domain adjustment."}, {"heading": "2 Framework and Estimation Algorithm", "text": "We focus on the division into several classes; we assume an unknown true distribution p \u0445 (x, y) over X \u00b7 Y = views, where Y = {1,.., k}, and are drawn with unmarked samples x (1),.., x (m) as a rule, drawn by p \u0445 (x, y). In view of the parameters \u03b8 Rd and a loss function L (., y), our goal is to estimate the risk of disturbances on p: R (empirical) def = Ex, y. [L (empirical; x, y)]. Throughout the course, we will make the 3-view of assumption x: Assumption 1 (3-view). Under p, x, the risk can be divided into x2, x3, which are conditionally independent. y (see Figure 2). In addition, we will decompose the loss: L (empirical; x, y) = A (empirical) = A (empirical)."}, {"heading": "3 Extensions", "text": "Theorem 1 provides a basic building block that allows multiple extensions to more complex model structures. We go beyond multiple cases, leaving out most of the evidence to avoid tedium.Extension 1 (Hidden Markov Model). Most importantly, the latent variable y does not need to belong to a small discrete group; we can treat structured output spaces like a hidden Markov model as long as the HMM structure matches. This is an essential generalization of previous work on uncontrolled risks that was limited to a multiclass classification. Assuming that we can handle structured output spaces like a hidden Markov model as long as the HMM structure cannot be extended (yt \u2212 1, yt \u2212 1)."}, {"heading": "4 From Estimation to Learning", "text": "We will now turn our attention to the difficulty of learning, i.e., we will focus on the case of logistics and only learn what we have learned. (...) Unsupervised learning is impossible without additional information, because even if we could learn the k-classes, we would not know which class would have which label. (...) We assume that we have a small amount of information to break this symmetry in the form of a seed model. (...) We only ask about the way in which we can be aligned to mean with the true labels. (...) We can obtain a small amount of labeled data. (...) We define a gap (...) around the difference between R (...) and the next smallest permutation of the classes that will affect the difficulty of learning. (...) We will focus on the case of logistical adjustment. (...) We will define the difference between R (...) and the next smallest permutation of the classes."}, {"heading": "5 Experiments", "text": "To better understand the behavior of our algorithms, we conduct experiments on a version of the MNIST dataset that is modified to ensure that the 3-view assumption holds. To create an image, we try a class in {0,.., 9}, then randomly select 3 images from that class, with one in three pixels coming from the respective image, guaranteeing that there will be 3 conditionally independent views. To explore the pull test variation, we dim pixelp in the image by exp (a) p \u2212 p0, 2 \u2212 0.4))), where p0 is the image center and the distance is normalized to have maximum value. We show sample images for a = 0 (pull) and a = 5 (a possible test distribution) in Figure 3.Risk Estimation. We use non-supervised risk assessment (theorem 1) to estimate the risk we have trained on a = 0 and tested on different values of a model."}, {"heading": "6 Discussion", "text": "We have presented a method for assessing the risk from unlabeled data, which relies only on a conditional structure of independence and therefore does not make parametric assumptions about true distribution. Our approach applies to a large family of losses and extends from classification tasks to hidden Markov models. We can also perform unsupervised learning, since only one seed model can distinguish between classes in anticipation; the seed model can be trained on a related domain, on a small amount of labeled data, or any combination of the two, and thus provides a pleasantly general formulation that highlights the similarities between domain adaptation and semi-supervised learning. Previous approaches to domain adaptation and semi-supervised learning also have an exploited multi-view structure. Given two views, Blitzer et al. (2011), we perform domain adaptations with zero source / target overlap (co-variable shifting is assumed), two co-training approaches (e.g., semi-supervised) are also used."}, {"heading": "A Details of Computing R\u0303 from M and \u03c0", "text": "In this section we will show how to perform calculations efficiently taking into account M and \u03c0. (9) The only bottleneck is the maximum excess over n \u00b2 sym (k), which naively would require the consideration of k! possibilities. However, we can consider this instead as a form of maximum match. In particular, we form the k \u00b2 k matrix Xi, j = 2 x 3 x v = 1 (Mv) j, i. (10) Then we search for the permutation so that we can maximize k = 1 x \u00b2 (j), j. If we consider each Xi, j as edge weight (i, j) in a complete two-part graph, then this is equivalent to the question of a match of i with j with the maximum weight. Thus, if we consider each Xi, j as edge weight (i, j), we can maximize the maximum correspondence with the Hungarian algorithm (1972)."}, {"heading": "B Proof of Theorem 1", "text": "M & amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp;"}, {"heading": "C Proof of Lemma 1", "text": "Let us first note that the minimization of R (BA) versus B (FR), we call R (BA) = E [A (BA; x)] - the minimization of R (BA) versus B (FR), we get R (BA) = E [A (BA; x)] - the minimization of R (BA; x) - the minimization of R (BA; x) - the minimization of R (BA; x) - the minimization of R (BA; x) - the minimization of R (BA; x) - the minimization of R (BA; x) - the minimization of R (BA; x) - the minimization of R (BA; x) - the minimization of R (BA; x) - the minimization of R (BA; x) - the minimization of R (BA; x) - the minimization of R (BA; x)."}, {"heading": "D Proof of Theorem 2", "text": "We note that Theorem 7 by Anandkumar et al. (2012) (and thus Theorem 1 above) does not require that the Mv be k \u00b7 k, but only that they have k \u2212 columns (the number of rows can be arbitrary). In our particular case, we take h \u00b2: Xv \u2192 Rk (d + 1), where the first k-coordinates of h \u00b2 (xv) are equal to h (xv) (i.e., (fv \u00b2, i) ki = 1), and the remaining kd \u00b2 coordinates of h \u00b2 (xv) are equal (B \u00b2 Rk (d + 1), where the first k-coordinates of h \u00b2 (xv) are equal (xv \u00b2 perv) (i.e., (fv \u00b2, i) ki = 1), and the remaining kd \u00b2 coordinates of h \u00b2 (xv) are equal (xv \u00b2), we have the same k \u00b2 coordinates of h (v \u00b2) and skv \u00b2 of Gv \u00b2 are equal (where Gv \u00b2 is)."}, {"heading": "E Learning with General Losses", "text": "In section 4 we formed the matrix of the conditional moment Gv, in which the conditional expectation E [\u03c6v (xv, i) | y = [2] is stored for each individual event and each individual event. However, there was nothing special about the calculation of the event (in contrast to some other moments), and for general losses the conditional progression matrix Gv (\u03b8) can be formed, defined by Gv (\u03b8) i + kr, j = E [\u2202; xv, i) | y = j]. (21) Theorem 2 applies identically to the matrix Gv (\u03b8) for each defined event. We can then calculate the course of the events."}], "references": [{"title": "A method of moments for mixture models and hidden Markov models", "author": ["A. Anandkumar", "D. Hsu", "S.M. Kakade"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": null, "citeRegEx": "Anandkumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2013}, {"title": "Estimation of the parameters of a single equation in a complete system of stochastic equations", "author": ["T.W. Anderson", "H. Rubin"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Anderson and Rubin.,? \\Q1949\\E", "shortCiteRegEx": "Anderson and Rubin.", "year": 1949}, {"title": "The asymptotic properties of estimates of the parameters of a single equation in a complete system of stochastic equations", "author": ["T.W. Anderson", "H. Rubin"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Anderson and Rubin.,? \\Q1950\\E", "shortCiteRegEx": "Anderson and Rubin.", "year": 1950}, {"title": "Two-view feature generation model for semi-supervised learning", "author": ["R.K. Ando", "T. Zhang"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Ando and Zhang.,? \\Q2007\\E", "shortCiteRegEx": "Ando and Zhang.", "year": 2007}, {"title": "Unsupervised supervised learning II: Margin-based classification without labels", "author": ["K. Balasubramanian", "P. Donmez", "G. Lebanon"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Balasubramanian et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Balasubramanian et al\\.", "year": 2011}, {"title": "A discriminative model for semi-supervised learning", "author": ["M. Balcan", "A. Blum"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Balcan and Blum.,? \\Q2010\\E", "shortCiteRegEx": "Balcan and Blum.", "year": 2010}, {"title": "Domain adaptation with coupled subspaces", "author": ["J. Blitzer", "S. Kakade", "D.P. Foster"], "venue": "In Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Blitzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2011}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Blum and Mitchell.,? \\Q1998\\E", "shortCiteRegEx": "Blum and Mitchell.", "year": 1998}, {"title": "Two high stakes challenges in machine learning", "author": ["L. Bottou"], "venue": "Invited talk at the 32nd International Conference on Machine Learning,", "citeRegEx": "Bottou.,? \\Q2015\\E", "shortCiteRegEx": "Bottou.", "year": 2015}, {"title": "Spectral experts for estimating mixtures of linear regressions", "author": ["A. Chaganty", "P. Liang"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Chaganty and Liang.,? \\Q2013\\E", "shortCiteRegEx": "Chaganty and Liang.", "year": 2013}, {"title": "Estimating latent-variable graphical models using moments and likelihoods", "author": ["A. Chaganty", "P. Liang"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Chaganty and Liang.,? \\Q2014\\E", "shortCiteRegEx": "Chaganty and Liang.", "year": 2014}, {"title": "Tensor decompositions, alternating least squares and other tales", "author": ["P. Comon", "X. Luciani", "A.L.D. Almeida"], "venue": "Journal of Chemometrics,", "citeRegEx": "Comon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Comon et al\\.", "year": 2009}, {"title": "Risks of semi-supervised learning: How unlabeled data can degrade performance of generative classifiers", "author": ["F. Cozman", "I. Cohen"], "venue": "In Semi-Supervised Learning", "citeRegEx": "Cozman and Cohen.,? \\Q2006\\E", "shortCiteRegEx": "Cozman and Cohen.", "year": 2006}, {"title": "Maximum likelihood estimation of observer error-rates using the EM algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Applied Statistics,", "citeRegEx": "Dawid and Skene.,? \\Q1979\\E", "shortCiteRegEx": "Dawid and Skene.", "year": 1979}, {"title": "Unsupervised supervised learning I: Estimating classification and regression errors without labels", "author": ["P. Donmez", "G. Lebanon", "K. Balasubramanian"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Donmez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Donmez et al\\.", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Theoretical improvements in algorithmic efficiency for network flow problems", "author": ["J. Edmonds", "R.M. Karp"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Edmonds and Karp.,? \\Q1972\\E", "shortCiteRegEx": "Edmonds and Karp.", "year": 1972}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.-G. Martinsson", "J. Tropp"], "venue": "SIAM Review,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Large sample properties of generalized method of moments estimators", "author": ["L.P. Hansen"], "venue": "Econometrica: Journal of the Econometric Society,", "citeRegEx": "Hansen.,? \\Q1982\\E", "shortCiteRegEx": "Hansen.", "year": 1982}, {"title": "Uncertainty outside and inside economic models", "author": ["L.P. Hansen"], "venue": "Journal of Political Economy,", "citeRegEx": "Hansen.,? \\Q2014\\E", "shortCiteRegEx": "Hansen.", "year": 2014}, {"title": "Identifiability and unmixing of latent parse trees", "author": ["D. Hsu", "S.M. Kakade", "P. Liang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "Estimating the accuracies of multiple classifiers without labeled data", "author": ["A. Jaffe", "B. Nadler", "Y. Kluger"], "venue": "In Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Jaffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaffe et al\\.", "year": 2015}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Joachims.,? \\Q1999\\E", "shortCiteRegEx": "Joachims.", "year": 1999}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["S.M. Kakade", "D.P. Foster"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Kakade and Foster.,? \\Q2007\\E", "shortCiteRegEx": "Kakade and Foster.", "year": 2007}, {"title": "Tensor factorization via matrix factorization", "author": ["V. Kuleshov", "A. Chaganty", "P. Liang"], "venue": "In Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Kuleshov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kuleshov et al\\.", "year": 2015}, {"title": "A link between the canonical decomposition in multilinear algebra and simultaneous matrix diagonalization", "author": ["L.D. Lathauwer"], "venue": "SIAM Journal of Matrix Analysis and Applications,", "citeRegEx": "Lathauwer.,? \\Q2006\\E", "shortCiteRegEx": "Lathauwer.", "year": 2006}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "S.H. Seung"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lee and Seung.,? \\Q2001\\E", "shortCiteRegEx": "Lee and Seung.", "year": 2001}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["L. Li", "M.L. Littman", "T.J. Walsh", "A.L. Strehl"], "venue": "Machine learning,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Towards making unlabeled data never hurt", "author": ["Y. Li", "Z. Zhou"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Li and Zhou.,? \\Q2015\\E", "shortCiteRegEx": "Li and Zhou.", "year": 2015}, {"title": "Analyzing the errors of unsupervised learning", "author": ["P. Liang", "D. Klein"], "venue": "In Human Language Technology and Association for Computational Linguistics (HLT/ACL),", "citeRegEx": "Liang and Klein.,? \\Q2008\\E", "shortCiteRegEx": "Liang and Klein.", "year": 2008}, {"title": "Tagging English text with a probabilistic model", "author": ["B. Merialdo"], "venue": "Computational Linguistics,", "citeRegEx": "Merialdo.,? \\Q1994\\E", "shortCiteRegEx": "Merialdo.", "year": 1994}, {"title": "Learning to classify text from labeled and unlabeled documents", "author": ["K. Nigam", "A. McCallum", "S. Thrun", "T. Mitchell"], "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),", "citeRegEx": "Nigam et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Nigam et al\\.", "year": 1998}, {"title": "Estimating accuracy from unlabeled data", "author": ["E.A. Platanios"], "venue": "Master\u2019s thesis,", "citeRegEx": "Platanios.,? \\Q2015\\E", "shortCiteRegEx": "Platanios.", "year": 2015}, {"title": "Estimation of semiparametric models", "author": ["J.L. Powell"], "venue": "In Handbook of Econometrics,", "citeRegEx": "Powell.,? \\Q1994\\E", "shortCiteRegEx": "Powell.", "year": 1994}, {"title": "Dataset shift in machine learning", "author": ["J. Qui\u00f1onero-Candela", "M. Sugiyama", "A. Schwaighofer", "N.D. Lawrence"], "venue": null, "citeRegEx": "Qui\u00f1onero.Candela et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Qui\u00f1onero.Candela et al\\.", "year": 2009}, {"title": "The estimation of economic relationships using instrumental variables", "author": ["J.D. Sargan"], "venue": null, "citeRegEx": "Sargan.,? \\Q1958\\E", "shortCiteRegEx": "Sargan.", "year": 1958}, {"title": "The estimation of relationships with autocorrelated residuals by the use of instrumental variables", "author": ["J.D. Sargan"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Sargan.,? \\Q1959\\E", "shortCiteRegEx": "Sargan.", "year": 1959}, {"title": "Hidden technical debt in machine learning systems", "author": ["D. Sculley", "G. Holt", "D. Golovin", "E. Davydov", "T. Phillips", "D. Ebner", "V. Chaudhary", "M. Young", "J. Crespo", "D. Dennison"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sculley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sculley et al\\.", "year": 2015}, {"title": "A tutorial on conformal prediction", "author": ["G. Shafer", "V. Vovk"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Shafer and Vovk.,? \\Q2008\\E", "shortCiteRegEx": "Shafer and Vovk.", "year": 2008}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodaira"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Shimodaira.,? \\Q2000\\E", "shortCiteRegEx": "Shimodaira.", "year": 2000}, {"title": "Memory, communication, and statistical queries", "author": ["J. Steinhardt", "G. Valiant", "S. Wager"], "venue": "Electronic Colloquium on Computational Complexity (ECCC),", "citeRegEx": "Steinhardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Steinhardt et al\\.", "year": 2015}, {"title": "On some techniques useful for solution of transportation network", "author": ["N. Tomizawa"], "venue": "problems. Networks,", "citeRegEx": "Tomizawa.,? \\Q1971\\E", "shortCiteRegEx": "Tomizawa.", "year": 1971}, {"title": "Spectral methods meet EM: A provably optimal algorithm for crowdsourcing", "author": ["Y. Zhang", "X. Chen", "D. Zhou", "M.I. Jordan"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Proof of Theorem 2 We note that Theorem 7 of Anandkumar et al. (2012) (and hence Theorem 1 above) does not require that the Mv be k \u00d7 k, but only that they have k columns (the number of rows can be arbitrary)", "author": ["D claimed"], "venue": null, "citeRegEx": "claimed.,? \\Q2012\\E", "shortCiteRegEx": "claimed.", "year": 2012}], "referenceMentions": [{"referenceID": 15, "context": "How can we assess the accuracy of a model when the test distribution is very different than the training distribution? To address this question, we study the problem of unsupervised risk estimation (Donmez et al., 2010)\u2014that is, given a loss function L(\u03b8;x, y) and a fixed model \u03b8, estimate the risk R(\u03b8) def = Ex,y\u223cp\u2217 [L(\u03b8;x, y)] with respect to a test distribution p\u2217(x, y), given access only to m unlabeled examples x \u223c p\u2217(x).", "startOffset": 198, "endOffset": 219}, {"referenceID": 33, "context": "Several others have extended this idea (e.g. Zhang et al., 2014; Platanios, 2015; Jaffe et al., 2015), but continue to focus on the 0/1 loss (with a single exception that we discuss below).", "startOffset": 39, "endOffset": 101}, {"referenceID": 22, "context": "Several others have extended this idea (e.g. Zhang et al., 2014; Platanios, 2015; Jaffe et al., 2015), but continue to focus on the 0/1 loss (with a single exception that we discuss below).", "startOffset": 39, "endOffset": 101}, {"referenceID": 2, "context": ", 2012), it has a much older history in the econometrics literature, where it has been used as a tool for making causal identifications under structural assumptions, even when an explicit form for the likelihood is not known (Anderson and Rubin, 1949; 1950; Sargan, 1958; 1959; Hansen, 1982; Powell, 1994; Hansen, 2014).", "startOffset": 225, "endOffset": 319}, {"referenceID": 36, "context": ", 2012), it has a much older history in the econometrics literature, where it has been used as a tool for making causal identifications under structural assumptions, even when an explicit form for the likelihood is not known (Anderson and Rubin, 1949; 1950; Sargan, 1958; 1959; Hansen, 1982; Powell, 1994; Hansen, 2014).", "startOffset": 225, "endOffset": 319}, {"referenceID": 19, "context": ", 2012), it has a much older history in the econometrics literature, where it has been used as a tool for making causal identifications under structural assumptions, even when an explicit form for the likelihood is not known (Anderson and Rubin, 1949; 1950; Sargan, 1958; 1959; Hansen, 1982; Powell, 1994; Hansen, 2014).", "startOffset": 225, "endOffset": 319}, {"referenceID": 34, "context": ", 2012), it has a much older history in the econometrics literature, where it has been used as a tool for making causal identifications under structural assumptions, even when an explicit form for the likelihood is not known (Anderson and Rubin, 1949; 1950; Sargan, 1958; 1959; Hansen, 1982; Powell, 1994; Hansen, 2014).", "startOffset": 225, "endOffset": 319}, {"referenceID": 20, "context": ", 2012), it has a much older history in the econometrics literature, where it has been used as a tool for making causal identifications under structural assumptions, even when an explicit form for the likelihood is not known (Anderson and Rubin, 1949; 1950; Sargan, 1958; 1959; Hansen, 1982; Powell, 1994; Hansen, 2014).", "startOffset": 225, "endOffset": 319}, {"referenceID": 40, "context": "If p\u2217(x) and p0(x) are close together, we can approximate p\u2217 by p0 via sample re-weighting (Shimodaira, 2000; Qui\u00f1onero-Candela et al., 2009).", "startOffset": 91, "endOffset": 141}, {"referenceID": 35, "context": "If p\u2217(x) and p0(x) are close together, we can approximate p\u2217 by p0 via sample re-weighting (Shimodaira, 2000; Qui\u00f1onero-Candela et al., 2009).", "startOffset": 91, "endOffset": 141}, {"referenceID": 7, "context": "If p\u2217 and p0 are not close, another approach is to assume a well-specified discriminative model family \u0398, such that p0(y | x) = p\u2217(y | x) = p\u03b8\u2217(y | x) for some \u03b8\u2217 \u2208 \u0398; then we need only heed finite-sample error in the estimation of \u03b8\u2217 (Blitzer et al., 2011; Li et al., 2011).", "startOffset": 235, "endOffset": 274}, {"referenceID": 28, "context": "If p\u2217 and p0 are not close, another approach is to assume a well-specified discriminative model family \u0398, such that p0(y | x) = p\u2217(y | x) = p\u03b8\u2217(y | x) for some \u03b8\u2217 \u2208 \u0398; then we need only heed finite-sample error in the estimation of \u03b8\u2217 (Blitzer et al., 2011; Li et al., 2011).", "startOffset": 235, "endOffset": 274}, {"referenceID": 31, "context": "Indeed, many authors report that mis-specification can lead to severe issues in semi-supervised settings (Merialdo, 1994; Nigam et al., 1998; Cozman and Cohen, 2006; Liang and Klein, 2008; Li and Zhou, 2015).", "startOffset": 105, "endOffset": 207}, {"referenceID": 32, "context": "Indeed, many authors report that mis-specification can lead to severe issues in semi-supervised settings (Merialdo, 1994; Nigam et al., 1998; Cozman and Cohen, 2006; Liang and Klein, 2008; Li and Zhou, 2015).", "startOffset": 105, "endOffset": 207}, {"referenceID": 13, "context": "Indeed, many authors report that mis-specification can lead to severe issues in semi-supervised settings (Merialdo, 1994; Nigam et al., 1998; Cozman and Cohen, 2006; Liang and Klein, 2008; Li and Zhou, 2015).", "startOffset": 105, "endOffset": 207}, {"referenceID": 30, "context": "Indeed, many authors report that mis-specification can lead to severe issues in semi-supervised settings (Merialdo, 1994; Nigam et al., 1998; Cozman and Cohen, 2006; Liang and Klein, 2008; Li and Zhou, 2015).", "startOffset": 105, "endOffset": 207}, {"referenceID": 29, "context": "Indeed, many authors report that mis-specification can lead to severe issues in semi-supervised settings (Merialdo, 1994; Nigam et al., 1998; Cozman and Cohen, 2006; Liang and Klein, 2008; Li and Zhou, 2015).", "startOffset": 105, "endOffset": 207}, {"referenceID": 7, "context": "intuition is formalized by Dawid and Skene (1979) when the fv take values in a discrete set (e.", "startOffset": 27, "endOffset": 50}, {"referenceID": 0, "context": "Anandkumar et al., 2012), it has a much older history in the econometrics literature, where it has been used as a tool for making causal identifications under structural assumptions, even when an explicit form for the likelihood is not known (Anderson and Rubin, 1949; 1950; Sargan, 1958; 1959; Hansen, 1982; Powell, 1994; Hansen, 2014). It is upon this older literature that we draw conceptual inspiration, though our technical tools are more closely based on the newer machine learning approaches. The key insight is that certain moment equations\u2013e.g., E[f1f2 | y] = E[f1 | y]E[f2 | y]\u2013 can be derived from the assumed independencies; we then show how to estimate the risk while relying only on these moment conditions, and not on any parametric assumptions about the xv or fv. Moreover, these moment equations also hold for the gradient of fv , which enables efficient unsupervised learning. Our paper is structured as follows. In Section 2, we present our basic framework, and state and prove our main result on estimating the risk given f1, f2, and f3. In Section 3, we extend our framework in several directions, including to hidden Markov models. In Section 4, we present a gradient-based learning algorithm and show that the sample complexity needed for learning is d \u00b7 poly(k)/ , where d is the dimension of \u03b8. In Section 5, we investigate how our method performs empirically. Related Work. While the formal problem of unsupervised risk estimation was only posed recently by Donmez et al. (2010), several older ideas from domain adaptation and semi-supervised learning are also relevant.", "startOffset": 0, "endOffset": 1505}, {"referenceID": 0, "context": "Anandkumar et al., 2012), it has a much older history in the econometrics literature, where it has been used as a tool for making causal identifications under structural assumptions, even when an explicit form for the likelihood is not known (Anderson and Rubin, 1949; 1950; Sargan, 1958; 1959; Hansen, 1982; Powell, 1994; Hansen, 2014). It is upon this older literature that we draw conceptual inspiration, though our technical tools are more closely based on the newer machine learning approaches. The key insight is that certain moment equations\u2013e.g., E[f1f2 | y] = E[f1 | y]E[f2 | y]\u2013 can be derived from the assumed independencies; we then show how to estimate the risk while relying only on these moment conditions, and not on any parametric assumptions about the xv or fv. Moreover, these moment equations also hold for the gradient of fv , which enables efficient unsupervised learning. Our paper is structured as follows. In Section 2, we present our basic framework, and state and prove our main result on estimating the risk given f1, f2, and f3. In Section 3, we extend our framework in several directions, including to hidden Markov models. In Section 4, we present a gradient-based learning algorithm and show that the sample complexity needed for learning is d \u00b7 poly(k)/ , where d is the dimension of \u03b8. In Section 5, we investigate how our method performs empirically. Related Work. While the formal problem of unsupervised risk estimation was only posed recently by Donmez et al. (2010), several older ideas from domain adaptation and semi-supervised learning are also relevant. The covariate shift assumption assumes access to labeled samples from a base distribution p0(x, y) for which p\u2217(y | x) = p0(y | x). If p\u2217(x) and p0(x) are close together, we can approximate p\u2217 by p0 via sample re-weighting (Shimodaira, 2000; Qui\u00f1onero-Candela et al., 2009). If p\u2217 and p0 are not close, another approach is to assume a well-specified discriminative model family \u0398, such that p0(y | x) = p\u2217(y | x) = p\u03b8\u2217(y | x) for some \u03b8\u2217 \u2208 \u0398; then we need only heed finite-sample error in the estimation of \u03b8\u2217 (Blitzer et al., 2011; Li et al., 2011). Both assumptions are somewhat stringent \u2014 re-weighting only allows small perturbations, and mis-specified models are common in practice. Indeed, many authors report that mis-specification can lead to severe issues in semi-supervised settings (Merialdo, 1994; Nigam et al., 1998; Cozman and Cohen, 2006; Liang and Klein, 2008; Li and Zhou, 2015). As mentioned above, our approach is closer in spirit to that of Dawid and Skene (1979) and its extensions.", "startOffset": 0, "endOffset": 2581}, {"referenceID": 0, "context": "Anandkumar et al., 2012), it has a much older history in the econometrics literature, where it has been used as a tool for making causal identifications under structural assumptions, even when an explicit form for the likelihood is not known (Anderson and Rubin, 1949; 1950; Sargan, 1958; 1959; Hansen, 1982; Powell, 1994; Hansen, 2014). It is upon this older literature that we draw conceptual inspiration, though our technical tools are more closely based on the newer machine learning approaches. The key insight is that certain moment equations\u2013e.g., E[f1f2 | y] = E[f1 | y]E[f2 | y]\u2013 can be derived from the assumed independencies; we then show how to estimate the risk while relying only on these moment conditions, and not on any parametric assumptions about the xv or fv. Moreover, these moment equations also hold for the gradient of fv , which enables efficient unsupervised learning. Our paper is structured as follows. In Section 2, we present our basic framework, and state and prove our main result on estimating the risk given f1, f2, and f3. In Section 3, we extend our framework in several directions, including to hidden Markov models. In Section 4, we present a gradient-based learning algorithm and show that the sample complexity needed for learning is d \u00b7 poly(k)/ , where d is the dimension of \u03b8. In Section 5, we investigate how our method performs empirically. Related Work. While the formal problem of unsupervised risk estimation was only posed recently by Donmez et al. (2010), several older ideas from domain adaptation and semi-supervised learning are also relevant. The covariate shift assumption assumes access to labeled samples from a base distribution p0(x, y) for which p\u2217(y | x) = p0(y | x). If p\u2217(x) and p0(x) are close together, we can approximate p\u2217 by p0 via sample re-weighting (Shimodaira, 2000; Qui\u00f1onero-Candela et al., 2009). If p\u2217 and p0 are not close, another approach is to assume a well-specified discriminative model family \u0398, such that p0(y | x) = p\u2217(y | x) = p\u03b8\u2217(y | x) for some \u03b8\u2217 \u2208 \u0398; then we need only heed finite-sample error in the estimation of \u03b8\u2217 (Blitzer et al., 2011; Li et al., 2011). Both assumptions are somewhat stringent \u2014 re-weighting only allows small perturbations, and mis-specified models are common in practice. Indeed, many authors report that mis-specification can lead to severe issues in semi-supervised settings (Merialdo, 1994; Nigam et al., 1998; Cozman and Cohen, 2006; Liang and Klein, 2008; Li and Zhou, 2015). As mentioned above, our approach is closer in spirit to that of Dawid and Skene (1979) and its extensions. Similarly to Zhang et al. (2014) and Jaffe et al.", "startOffset": 0, "endOffset": 2634}, {"referenceID": 0, "context": "Anandkumar et al., 2012), it has a much older history in the econometrics literature, where it has been used as a tool for making causal identifications under structural assumptions, even when an explicit form for the likelihood is not known (Anderson and Rubin, 1949; 1950; Sargan, 1958; 1959; Hansen, 1982; Powell, 1994; Hansen, 2014). It is upon this older literature that we draw conceptual inspiration, though our technical tools are more closely based on the newer machine learning approaches. The key insight is that certain moment equations\u2013e.g., E[f1f2 | y] = E[f1 | y]E[f2 | y]\u2013 can be derived from the assumed independencies; we then show how to estimate the risk while relying only on these moment conditions, and not on any parametric assumptions about the xv or fv. Moreover, these moment equations also hold for the gradient of fv , which enables efficient unsupervised learning. Our paper is structured as follows. In Section 2, we present our basic framework, and state and prove our main result on estimating the risk given f1, f2, and f3. In Section 3, we extend our framework in several directions, including to hidden Markov models. In Section 4, we present a gradient-based learning algorithm and show that the sample complexity needed for learning is d \u00b7 poly(k)/ , where d is the dimension of \u03b8. In Section 5, we investigate how our method performs empirically. Related Work. While the formal problem of unsupervised risk estimation was only posed recently by Donmez et al. (2010), several older ideas from domain adaptation and semi-supervised learning are also relevant. The covariate shift assumption assumes access to labeled samples from a base distribution p0(x, y) for which p\u2217(y | x) = p0(y | x). If p\u2217(x) and p0(x) are close together, we can approximate p\u2217 by p0 via sample re-weighting (Shimodaira, 2000; Qui\u00f1onero-Candela et al., 2009). If p\u2217 and p0 are not close, another approach is to assume a well-specified discriminative model family \u0398, such that p0(y | x) = p\u2217(y | x) = p\u03b8\u2217(y | x) for some \u03b8\u2217 \u2208 \u0398; then we need only heed finite-sample error in the estimation of \u03b8\u2217 (Blitzer et al., 2011; Li et al., 2011). Both assumptions are somewhat stringent \u2014 re-weighting only allows small perturbations, and mis-specified models are common in practice. Indeed, many authors report that mis-specification can lead to severe issues in semi-supervised settings (Merialdo, 1994; Nigam et al., 1998; Cozman and Cohen, 2006; Liang and Klein, 2008; Li and Zhou, 2015). As mentioned above, our approach is closer in spirit to that of Dawid and Skene (1979) and its extensions. Similarly to Zhang et al. (2014) and Jaffe et al. (2015), we use the method of moments for estimating latent-variable models However, those papers use it as a tool for parameter estimation in the face of non-convexity, rather than as a way to sidestep model mis-specification.", "startOffset": 0, "endOffset": 2658}, {"referenceID": 0, "context": "Anandkumar et al., 2012), it has a much older history in the econometrics literature, where it has been used as a tool for making causal identifications under structural assumptions, even when an explicit form for the likelihood is not known (Anderson and Rubin, 1949; 1950; Sargan, 1958; 1959; Hansen, 1982; Powell, 1994; Hansen, 2014). It is upon this older literature that we draw conceptual inspiration, though our technical tools are more closely based on the newer machine learning approaches. The key insight is that certain moment equations\u2013e.g., E[f1f2 | y] = E[f1 | y]E[f2 | y]\u2013 can be derived from the assumed independencies; we then show how to estimate the risk while relying only on these moment conditions, and not on any parametric assumptions about the xv or fv. Moreover, these moment equations also hold for the gradient of fv , which enables efficient unsupervised learning. Our paper is structured as follows. In Section 2, we present our basic framework, and state and prove our main result on estimating the risk given f1, f2, and f3. In Section 3, we extend our framework in several directions, including to hidden Markov models. In Section 4, we present a gradient-based learning algorithm and show that the sample complexity needed for learning is d \u00b7 poly(k)/ , where d is the dimension of \u03b8. In Section 5, we investigate how our method performs empirically. Related Work. While the formal problem of unsupervised risk estimation was only posed recently by Donmez et al. (2010), several older ideas from domain adaptation and semi-supervised learning are also relevant. The covariate shift assumption assumes access to labeled samples from a base distribution p0(x, y) for which p\u2217(y | x) = p0(y | x). If p\u2217(x) and p0(x) are close together, we can approximate p\u2217 by p0 via sample re-weighting (Shimodaira, 2000; Qui\u00f1onero-Candela et al., 2009). If p\u2217 and p0 are not close, another approach is to assume a well-specified discriminative model family \u0398, such that p0(y | x) = p\u2217(y | x) = p\u03b8\u2217(y | x) for some \u03b8\u2217 \u2208 \u0398; then we need only heed finite-sample error in the estimation of \u03b8\u2217 (Blitzer et al., 2011; Li et al., 2011). Both assumptions are somewhat stringent \u2014 re-weighting only allows small perturbations, and mis-specified models are common in practice. Indeed, many authors report that mis-specification can lead to severe issues in semi-supervised settings (Merialdo, 1994; Nigam et al., 1998; Cozman and Cohen, 2006; Liang and Klein, 2008; Li and Zhou, 2015). As mentioned above, our approach is closer in spirit to that of Dawid and Skene (1979) and its extensions. Similarly to Zhang et al. (2014) and Jaffe et al. (2015), we use the method of moments for estimating latent-variable models However, those papers use it as a tool for parameter estimation in the face of non-convexity, rather than as a way to sidestep model mis-specification. The insight that moments are robust to model mis-specification lets us extend beyond the simple discrete settings they consider in order to handle more complex continuous and structured losses. Another approach to handling continuous losses is given in the intriguing work of Balasubramanian et al. (2011), who show that the distribution of losses L | y is often close to Gaussian in practice, and use this to estimate", "startOffset": 0, "endOffset": 3184}, {"referenceID": 27, "context": "1 For v = 2 views, recovering R is related to non-negative matrix factorization (Lee and Seung, 2001).", "startOffset": 80, "endOffset": 101}, {"referenceID": 26, "context": "The left-hand-side of each equation can be estimated from unlabeled data; we can then solve for Mv and \u03c0 using tensor decomposition (Lathauwer, 2006; Comon et al., 2009; Anandkumar et al., 2012; 2013; Kuleshov et al., 2015).", "startOffset": 132, "endOffset": 223}, {"referenceID": 12, "context": "The left-hand-side of each equation can be estimated from unlabeled data; we can then solve for Mv and \u03c0 using tensor decomposition (Lathauwer, 2006; Comon et al., 2009; Anandkumar et al., 2012; 2013; Kuleshov et al., 2015).", "startOffset": 132, "endOffset": 223}, {"referenceID": 0, "context": "The left-hand-side of each equation can be estimated from unlabeled data; we can then solve for Mv and \u03c0 using tensor decomposition (Lathauwer, 2006; Comon et al., 2009; Anandkumar et al., 2012; 2013; Kuleshov et al., 2015).", "startOffset": 132, "endOffset": 223}, {"referenceID": 25, "context": "The left-hand-side of each equation can be estimated from unlabeled data; we can then solve for Mv and \u03c0 using tensor decomposition (Lathauwer, 2006; Comon et al., 2009; Anandkumar et al., 2012; 2013; Kuleshov et al., 2015).", "startOffset": 132, "endOffset": 223}, {"referenceID": 11, "context": "It would be interesting to extend our results to other structures such as more general graphical models (Chaganty and Liang, 2014) and parse trees (Hsu et al.", "startOffset": 104, "endOffset": 130}, {"referenceID": 21, "context": "It would be interesting to extend our results to other structures such as more general graphical models (Chaganty and Liang, 2014) and parse trees (Hsu et al., 2012).", "startOffset": 147, "endOffset": 165}, {"referenceID": 18, "context": "We take a standard approach based on random projections (Halko et al., 2011) and described in", "startOffset": 56, "endOffset": 76}, {"referenceID": 0, "context": "2 of Anandkumar et al. (2013). We refer the reader to the aforementioned references for details, and cite only the final sample complexity and runtime (see Section D for a proof sketch): Theorem 2.", "startOffset": 5, "endOffset": 30}, {"referenceID": 16, "context": "We trained the model with AdaGrad (Duchi et al., 2010) on 10, 000 training examples, and used 10, 000 test examples to estimate the risk.", "startOffset": 34, "endOffset": 54}, {"referenceID": 10, "context": "To solve for \u03c0 and M in (4), we first use the tensor power method implemented by Chaganty and Liang (2013) to initialize, and then locally minimize a weighted `-norm of the moment errors with L-BFGS.", "startOffset": 81, "endOffset": 107}, {"referenceID": 8, "context": "co-training and CCA) are also used in semi-supervised learning (Blum and Mitchell, 1998; Ando and Zhang, 2007; Kakade and Foster, 2007; Balcan and Blum, 2010).", "startOffset": 63, "endOffset": 158}, {"referenceID": 4, "context": "co-training and CCA) are also used in semi-supervised learning (Blum and Mitchell, 1998; Ando and Zhang, 2007; Kakade and Foster, 2007; Balcan and Blum, 2010).", "startOffset": 63, "endOffset": 158}, {"referenceID": 24, "context": "co-training and CCA) are also used in semi-supervised learning (Blum and Mitchell, 1998; Ando and Zhang, 2007; Kakade and Foster, 2007; Balcan and Blum, 2010).", "startOffset": 63, "endOffset": 158}, {"referenceID": 6, "context": "co-training and CCA) are also used in semi-supervised learning (Blum and Mitchell, 1998; Ando and Zhang, 2007; Kakade and Foster, 2007; Balcan and Blum, 2010).", "startOffset": 63, "endOffset": 158}, {"referenceID": 23, "context": ", transductive SVMs (Joachims, 1999).", "startOffset": 20, "endOffset": 36}, {"referenceID": 38, "context": "In addition to reliability and unsupervised learning, our work is motivated by the desire to build machine learning system with contracts, a challenge recently posed by Bottou (2015); the goal is for machine learning systems to satisfy a well-defined input-output contract in analogy with software systems (Sculley et al., 2015).", "startOffset": 306, "endOffset": 328}, {"referenceID": 4, "context": "Given two views, Blitzer et al. (2011) perform domain adaptation with zero source/target overlap (covariate shift is still assumed).", "startOffset": 17, "endOffset": 39}, {"referenceID": 4, "context": "co-training and CCA) are also used in semi-supervised learning (Blum and Mitchell, 1998; Ando and Zhang, 2007; Kakade and Foster, 2007; Balcan and Blum, 2010). These methods all assume some form of low noise or low regret, as do, e.g., transductive SVMs (Joachims, 1999). By focusing on the central problem of risk estimation, our work connects multi-view learning approaches for domain adaptation and semi-supervised learning, and removes covariate shift and low-noise/low-regret assumptions (though we make stronger independence assumptions, and specialize to discrete prediction tasks). In addition to reliability and unsupervised learning, our work is motivated by the desire to build machine learning system with contracts, a challenge recently posed by Bottou (2015); the goal is for machine learning systems to satisfy a well-defined input-output contract in analogy with software systems (Sculley et al.", "startOffset": 89, "endOffset": 773}, {"referenceID": 4, "context": "co-training and CCA) are also used in semi-supervised learning (Blum and Mitchell, 1998; Ando and Zhang, 2007; Kakade and Foster, 2007; Balcan and Blum, 2010). These methods all assume some form of low noise or low regret, as do, e.g., transductive SVMs (Joachims, 1999). By focusing on the central problem of risk estimation, our work connects multi-view learning approaches for domain adaptation and semi-supervised learning, and removes covariate shift and low-noise/low-regret assumptions (though we make stronger independence assumptions, and specialize to discrete prediction tasks). In addition to reliability and unsupervised learning, our work is motivated by the desire to build machine learning system with contracts, a challenge recently posed by Bottou (2015); the goal is for machine learning systems to satisfy a well-defined input-output contract in analogy with software systems (Sculley et al., 2015). Theorem 1 provides the contract that under the 3-view assumption the test error is close to our estimate of the test error; this contrasts with the typical weak contract that if train and test are similar, then the test error is close to the training error. One other interesting contract is given by Shafer and Vovk (2008), who provide prediction regions that contain the true prediction with probability 1\u2212 in the online setting, even in the presence of model mis-specification.", "startOffset": 89, "endOffset": 1244}, {"referenceID": 4, "context": "co-training and CCA) are also used in semi-supervised learning (Blum and Mitchell, 1998; Ando and Zhang, 2007; Kakade and Foster, 2007; Balcan and Blum, 2010). These methods all assume some form of low noise or low regret, as do, e.g., transductive SVMs (Joachims, 1999). By focusing on the central problem of risk estimation, our work connects multi-view learning approaches for domain adaptation and semi-supervised learning, and removes covariate shift and low-noise/low-regret assumptions (though we make stronger independence assumptions, and specialize to discrete prediction tasks). In addition to reliability and unsupervised learning, our work is motivated by the desire to build machine learning system with contracts, a challenge recently posed by Bottou (2015); the goal is for machine learning systems to satisfy a well-defined input-output contract in analogy with software systems (Sculley et al., 2015). Theorem 1 provides the contract that under the 3-view assumption the test error is close to our estimate of the test error; this contrasts with the typical weak contract that if train and test are similar, then the test error is close to the training error. One other interesting contract is given by Shafer and Vovk (2008), who provide prediction regions that contain the true prediction with probability 1\u2212 in the online setting, even in the presence of model mis-specification. The most restrictive part of our framework is the three-view assumption, which is inappropriate if the views are not completely independent or if the data have structure that is not captured in terms of multiple views. Since Balasubramanian et al. (2011) obtain results under Gaussianity (which would be implied by many somewhat dependent views), we are optimistic that unsupervised risk estimation is possible for a wider family of structures.", "startOffset": 89, "endOffset": 1656}], "year": 2016, "abstractText": "We show how to estimate a model\u2019s test error from unlabeled data, on distributions very different from the training distribution, while assuming only that certain conditional independencies are preserved between train and test. We do not need to assume that the optimal predictor is the same between train and test, or that the true distribution lies in any parametric family. We can also efficiently differentiate the error estimate to perform unsupervised discriminative learning. Our technical tool is the method of moments, which allows us to exploit conditional independencies in the absence of a fully-specified model. Our framework encompasses a large family of losses including the log and exponential loss, and extends to structured output settings such as hidden Markov models.", "creator": "LaTeX with hyperref package"}}}