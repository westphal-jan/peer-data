{"id": "1606.05316", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Learning Infinite-Layer Networks: Without the Kernel Trick", "abstract": "Infinite--Layer Networks (ILN) have recently been proposed as an architecture that mimics neural networks while enjoying some of the advantages of kernel methods. ILN are networks that integrate over infinitely many nodes within a single hidden layer. It has been demonstrated by several authors that the problem of learning ILN can be reduced to the kernel trick, implying that whenever a certain integral can be computed analytically they are efficiently learnable.", "histories": [["v1", "Thu, 16 Jun 2016 19:02:14 GMT  (20kb)", "http://arxiv.org/abs/1606.05316v1", null], ["v2", "Fri, 28 Jul 2017 04:13:58 GMT  (33kb,D)", "http://arxiv.org/abs/1606.05316v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["roi livni", "daniel carmon", "amir globerson"], "accepted": true, "id": "1606.05316"}, "pdf": {"name": "1606.05316.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["gamir@cs.tau.ac.il", "roi.livni@mail.huji.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.05 316v 1 [cs.L G] 16. Jun 20In this paper, we provide an online algorithm for ILN that avoids the assumption of a kernel trick. More generally, and of independent interest, we show that kernel methods can generally be exploited even if the kernel cannot be efficiently calculated, but can only be estimated by sampling. We provide a regret analysis for our algorithm that shows that it matches the sample complexity of methods that have access to kernel values. Therefore, our method is the first to show that the kernel trick as such is not necessary and that random features are sufficient to achieve comparable performance."}, {"heading": "1 Introduction", "text": "With the increasing success of highly non-convex and complex learning architectures such as neural networks, we can increasingly strive to further understand and explain the limits of such hierarchical structures."}, {"heading": "2 Problem Setup", "text": "We look at learning algorithms that map from the input instances x-X to the designations y-Y. We focus on the case of regression in which Y is the interval [\u2212 1, 1]. Our starting point is a class of attribute functions that consist of convolution and pooling layers. Our only assumption with respect to the properties (w; x) is that all characteristics parameterized by vectors w-X are valid that the functions (w; x) can contain highly complex nonlinearities, such as multilayer networks that consist of convolution and pooling layers. Our only assumption with respect to the characteristics (w; x) is that all w values and x-X values are valid, that the x values (w; x) | < 1.Taking into account a distribution of f values, we call the class of L2 (p; \u00b5) the class of quadratable functions above."}, {"heading": "3 Main Results", "text": "Theorem 1 gives our result for the online model. The corresponding result for the statistical setting is given in episode 1. We will explain the structure of the algorithm later, but first provide the main result.Algorithm 1: The SHRINKING GRADIENT algorithm. Data: T, B > 1, \u03b7, m Result: Weights \u03b1 (1),..., \u03b1 (T + 1). Functions ft = EST SCALAR PROD (\u03b1, \u00b5) defines asft = \u2211 t i = 1 \u03b1 (t) i \u03a6 (xi); Initialize \u03b1 (1) = 0 \u0445 RT.; for t = 1,..., T doObserve xt, yt; Set Et = EST SCALAR PROD (\u03b1 (t), x1: t \u2212 1, xt, m); if | Et | < 16B then \u03b1 (t + 1) (t + 1) t = ar SCALAR PROT (1m, m)."}, {"heading": "Then:", "text": "1. For each sequence of square losses we observe for f1,..., fT: E [T \u2211 t = 1\u0445 t (ft) \u2212 min f \u0445 Hbp (f)] = O (B \u221a T) 2. The running time of the algorithm is O (\u03c1B4T 2).33. For each t = 1. T and a new test example x, we can estimate < ft, \u0445 (x) > within the accuracy limit 0 by using algorithm 2 with the parameters \u03b1 (t), {xi} ti = 1, x and m = O (B4T 2 0 log 1 / 3). The resulting running time for the test is then O (\u03c1m) > within the accuracy settings in which we set limits for the expected performance. Following the online conversion and theorem 1, we can determine the following correlation (e.g., [ft]: 1 > setpoint = 1 setpoint = 1 setpoint)."}, {"heading": "4 Related Work", "text": "In recent work, we have taken a similar approach to our work, but we focus on calculating the kernel for certain feature classes to call the kernel trick. By contrast, our work avoids the kernel trick and applies it to any feature class that can be randomly generated. All of the above work is part of a broader attempt to circumvent the hardship of deep learning by migrating the kernel trick [21, 4, 3], and developing a general duality between neural networks and kernels [11]. From a different perspective, the relationship between random features and kernels is related where the authors represent the transformation."}, {"heading": "5 Algorithm", "text": "The algorithm is similar to Online Gradient Descent (OGD) [31], but with some important modifications that are necessary for our analysis. We will first present the problem in the terminology of online convex optimization, as in [31]. The aim of the algorithm is to minimize regret against a benchmark of B-bounded functions, as in Eq. 6.A classic approach to the problem is to exploit the OGD algorithms. Its simplest version would be to update ft + 1."}, {"heading": "5.1 Analysis", "text": "In the following, we analyze regret for algorithm 1. We begin by modifying regret for OGD in Equation 10 to take into account steps that differ from the standard gradient update, such as shrinkage. Lemma 1. Let us assume that it is an arbitrary sequence of convex loss functions, and let f1,..., fT be random vectors generated by an online algorithm. Suppose that BT for all i \u2264 T. There should be no unbiased estimate of failure functions for each. Denote f = ft \u2212 1 \u2212 \u03b7 = t \u2212 1 and letPt (f \u0445) = P [p] ft \u2212 f \u2212 f \u2212 f \u2212 f \u2012 f \u2012 t. (13)."}, {"heading": "For every \u2016f\u2217\u2016 \u2264 B it holds that:", "text": "(14) As already discussed, the first two terms in the RHS are the default limit for OGD of equivalents. 10. Note that in the OGD we always choose ft = f so Pt (f) = 0 and the last term disappears; the third term is limited by controlling the first term; the last term Pt + 1 (f) is a penalty resulting from updates that move away from the default update step f-t; this will actually happen for the shrink step; the next problem is limited to that term; the last term Pt + 1 (f) is a penalty resulting from updates that move away from the default update step f-t."}, {"heading": "Denote f\u0302t = ft \u2212 \u03b7\u2207\u0304t and define Pt(f\u2217) as in Eq. 13.", "text": "Pt (f) \u2264 2 exp (\u2212 m (3\u03b7t) 2) Taking Lemma 2 and Lemma 1 into account, we obtain that the last term in Equation 14 can be limited by \u03b7T if we take into account m = O (\u03b7T) 2 log (BT \u03b7) 2) estimates (again using simplicity and treating B as constant).It can be shown that BT = O (\u03b7T), i.e. the neglect of logarithmic factors and the choice of \u03b7 = O (1 \u221a T), are sufficient to bind the fourth term. Complete proof for Theorem 1 and the two above-mentioned Lemms is given in the appendix."}, {"heading": "6 Discussion", "text": "We have presented a new online algorithm that implicitly uses cores, but avoids the kernel trick assumption, because the algorithm can be called even if you only have access to estimates of the scalar product. The problem was motivated by cores resulting from neural networks, but it can of course be applied to any scalar product of the form we described. To sum it up, we can pour our result into a larger model that could be of independent interest. Let's consider a setting in which a learner can observe an unbiased estimate of a coordinate in a kernel matrix, or alternatively the scalar product between any two observations. Our results imply that in this setting the above rates are applicable, and at least for the square loss of not having access to the true values in the kernel matrix."}, {"heading": "Acknowledgement", "text": "The authors thank Tomer Koren for his helpful comments and suggestions. Roi Livni is the recipient of the Google Europe Fellowship in Learning Theory, and this research is supported in part by this Google Fellowship and a Google Research Award."}, {"heading": "A Estimation Procedure \u2013 Concentration Bounds", "text": "In this section, we offer concentration limits for the estimation method in algorithm 2.Lemma 3. Run algorithm 2 with \u03b1 and, {xi} Ti = 1 x and m. Let us leave f = \u2211 \u03b1i\u0445 (xi). Suppose that | \u0432 (x; w) | < 1 for all w and x. Let E be the output of algorithm 2. Then, E is an unbiased estimator for < f, p (x) > and: P [| E \u2212 < f, p (x) >. You can show that it is IID (x). You can show that it is E [...] [...] [...] 1E (...) (...) (...) (...] not performance."}, {"heading": "B Proofs of Lemmas", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proof of Lemma 1", "text": "First, after convexity, we have 2 (t) \u2212 t (f) \u2212 t (f)) \u2264 2 < t, ft \u2212 f) >. (17) Next, we have the upper limit < t, ft \u2212 f) >. Use E to label the event. (17) Name the event. (17) Note that E [t + 1 \u2212 f) \u2264 2] \u2264 E [t + 1 \u2212 f) + E [t + 1 \u2212 f) + E [t + 1 \u2212 f). (17) \u00b7 Pt + 1 (f)."}, {"heading": "B.2 Proof for Lemma 2", "text": "We assume that we first use the event Pt (f) < p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p) p (f) p (f) p (f) p (f) p (f) p (f) p (f) p) p (f) p (f) p (f) p (f) p (f) p) p (f) p (f) p (f) p (f) p) p (f) p (f) p) p (f) p (f) p) p (f) p (f) p) p (f) p) p (f) p) p (f) p) p (f) p) p (f) p) p (f) p) p) \"p (f) p) p (f) p) p) p)\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\""}, {"heading": "C Proof of Main Result", "text": "C.1 Some more technical lemmasWe begin with the derivation of corollars2 limiting the first two terms in the limit of deplorabilty. As discussed, this section follows standard techniques. We begin with an upper limit for e (e) lemmas (2).Lemma begins with the determination that derives from the definitions of deplorabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabelabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabelabilabilabilabilabilabilabilabilabilabilabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabilabelabelabelabelabelabelabelabelabelabelabelabelabilabelabelabelabelabelabelabelabelabelabelabilabelabelabilabelabilabelabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilabilab"}, {"heading": "C.2 Proof of Theorem 1", "text": "Remember that Lemma 1 and Corollary 2 assume a BT upper limit (p). We start with the fact that BT can be limited as follows by using Lemma 4: BT = max t)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>Infinite\u2013Layer Networks (ILN) have recently been proposed as an architecture<lb>that mimics neural networks while enjoying some of the advantages of kernel meth-<lb>ods. ILN are networks that integrate over infinitely many nodes within a single<lb>hidden layer. It has been demonstrated by several authors that the problem of<lb>learning ILN can be reduced to the kernel trick, implying that whenever a certain<lb>integral can be computed analytically they are efficiently learnable.<lb>In this work we give an online algorithm for ILN, which avoids the kernel trick<lb>assumption. More generally and of independent interest, we show that kernel meth-<lb>ods in general can be exploited even when the kernel cannot be efficiently computed<lb>but can only be estimated via sampling.<lb>We provide a regret analysis for our algorithm, showing that it matches the<lb>sample complexity of methods which have access to kernel values. Thus, our method<lb>is the first to demonstrate that the kernel trick is not necessary as such, and random<lb>features suffice to obtain comparable performance.", "creator": "LaTeX with hyperref package"}}}