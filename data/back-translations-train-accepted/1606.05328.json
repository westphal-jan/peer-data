{"id": "1606.05328", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Conditional Image Generation with PixelCNN Decoders", "abstract": "This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder, creating. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.", "histories": [["v1", "Thu, 16 Jun 2016 19:40:56 GMT  (3016kb,D)", "http://arxiv.org/abs/1606.05328v1", null], ["v2", "Sat, 18 Jun 2016 15:44:24 GMT  (3016kb,D)", "http://arxiv.org/abs/1606.05328v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["a\u00e4ron van den oord", "nal kalchbrenner", "lasse espeholt", "koray kavukcuoglu", "oriol vinyals", "alex graves"], "accepted": true, "id": "1606.05328"}, "pdf": {"name": "1606.05328.pdf", "metadata": {"source": "CRF", "title": "Conditional Image Generation with PixelCNN Decoders", "authors": ["A\u00e4ron van den Oord"], "emails": ["avdnoord@google.com", "nalk@google.com", "vinyals@google.com", "espeholt@google.com", "gravesa@google.com", "korayk@google.com"], "sections": [{"heading": "1 Introduction", "text": "Recent advances in image modeling using neural networks [30, 26, 20, 10, 9, 28, 6] have made it possible to generate diverse natural images that capture the high-level structure of training data. While such unconditional models are fascinating in themselves, many practical applications of image modeling require that the model be conditioned on prior information: for example, an image model used to amplify image planning in a visual environment would have to predict future scenes in specific states and actions [17]. Similarly, image processing tasks such as denoizing, blurring, painting, superresolution and coloring may rely on the generation of improved images conditioned on noisy or incomplete data. Neural artworks [18, 5] and content generation represent potential future conditional generation.This paper examines the potential for conditional image modeling by adapting and improving a conventional variant of pixel [30] architecture."}, {"heading": "2 Gated PixelCNN", "text": "PixelCNNs (and PixelRNNs) [30] model the common distribution of pixels over an image x as the following product of conditional distributions, where xi is a single pixel: p (x) = n2, i = 1 p (xi | x1,..., xi \u2212 1). (1) The arrangement of pixel dependencies is done in grid scan order: line by line and pixel by pixel within each line. Therefore, each pixel depends on all pixels above and to the left, and not on any other pixel. The dependency field of a pixel is visualized in Figure 1 (left). A similar setup was used by other autoregressive models such as NADE [14] and RIDE [26]. The difference lies in the way in which the conditional distributions p (xi | x1,..., xi \u2212 1) are multidimensioned."}, {"heading": "2.1 Gated Convolutional Layers", "text": "PixelRNNs that use spatial LSTM layers instead of revolutionary stacks surpass PixelCNNs as generative models [30]. One possible reason for this advantage is that the recurring connections in LSTM allow each layer in the network to access the entire neighborhood of previous pixels, while the neighborhood region available to PixelCNN grows linearly with the depth of the revolutionary stack. To change this, we have replaced the reflected linear units between the masked rotations in the original PixelCNN model with the following gated activation unit (in the form of the LSTM gates), which in turn can model more complex interactions."}, {"heading": "2.2 Blind spot in the receptive field", "text": "In Figure 1 (top right), we show the progressive growth of the effective receptive field of a 3 \u00d7 3 masked filter above the input image. Note that a significant portion of the input image is ignored by the masked Convolutionary Architecture. This \"blind spot\" can cover up to a quarter of the potential receptive field (e.g. when using 3x3 filters), which means that no content to the right of the current pixel would be taken into account. In this work, we remove the blind spot by combining two revolutionary network stacks: one that has conditioned the current line so far (horizontal stack) and one that is conditioned to all rows on top (vertical stack). In Figure 1 (bottom right), the arrangement is illustrated by combining two Constitutional Network Stacks. The vertical stack, which has no masking, allows the receptive field to grow in a receptive line (horizontal stack), without picking up a stack on the xontal layer, or combining the two xontal layers, respectively, in the xontal layer below the xellayers."}, {"heading": "2.3 Conditional PixelCNN", "text": "Faced with a high-level image description represented as a latent vector h, we try to model the conditional distribution p (x | h) of images that fit this description. Formally, the conditional pixelCNN models the following distribution: p (x | h) = n2-i = 1 p (xi | x1,..., xi \u2212 1, h). (3) We model the conditional distribution by adding terms that depend on h to the activations before the nonlinearity in Equation 2, which is now: y = tanh (Wk, f \u00b2 x + V Tk, fh) \u03c3 (Wk, g \u00b2 x + V Tk, gh), (4) where k is the number of layers. If h is a uniform encoding specifying a class, this corresponds to the addition of a class-dependent bias on each layer. Note that the conditioning does not depend on the position of the pixel in the image, which is an appropriate one in the image, as long as it contains information about it and where h is appropriate."}, {"heading": "2.4 PixelCNN Auto-Encoders", "text": "Since conditional pixel CNNs are able to model multiple multimodal image distributions p (x | h), it is possible to apply them as image decoders to existing neural architectures such as autoencoders. An auto encoder consists of two parts: an encoder that takes an input image x and assigns it to a (usually) low-dimensional representation h, and a decoder that attempts to reconstruct the original image. Starting from a traditional revolutionary auto-encoder architecture [16], we replace the deconvolutionary decoder with a conditional pixel CNN and train the entire network end-to-end. Since PixelCNN has proven to be a strong unconditional generative model, we would expect this change to improve reconstructions. Perhaps more interesting is that it also changes the representations that the encoder will learn to extract from the data: Since such a large portion of the low pixel statistics can be processed from the pixel level by CNN at the higher level, this information should instead be processed by CNN at the pixel level."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Unconditional Modeling with Gated PixelCNN", "text": "Table 1 compares Gated PixelCNN with the published results on the CIFAR-10 dataset. These architectures have all been optimized for the best possible validation value, which means that models that get a lower score actually generalize better. In Table 2, we compare the performance of Gated PixelCNN with other models on the ImageNet dataset by 0.11 bits / dim, which has a very significant effect on the visual quality of the samples produced and comes close to the performance of PixelRNN. In Table 2, we compare the performance of Gated PixelCNN with other models on the ImageNet dataset. Here, Gated PixelCNN exceeds PixelRNN [30]; we believe this is because the models fit insufficiently, larger models perform better and the simpler PixelCNN model scales better. We were able to achieve a similar performance to the PixelRNN (Row LSTM [30]) in less than half of the training time to achieve 60 hours (with table G2 GUs per G5 units) for each of G5 with the results traced in 38,000 units."}, {"heading": "3.2 Conditioning on ImageNet Classes", "text": "In our second experiment, we investigated class-related modeling of ImageNet images using gated PixelCNNs. With uniform coding of hi for the i-class, we model p (x | hi). The amount of information the model receives is only log (1000) \u2248 0.003 bits / pixel (for an image of 32 x 32 pixels). Nevertheless, one might expect that conditioning the image generation on the class label could significantly improve the protocol probability results, but we did not observe any major differences. On the other hand, as mentioned in [27], we observed large improvements in the visual quality of the generated samples. In Figure 3, we show samples from a class-related model for 8 different classes. We see that the generated classes are very different and that the corresponding objects, animals and backgrounds are clearly generated. In addition, the images of a single class are very diverse: Thus, the model was able to generate similar scenes from different angles and flash conditions for each animal image, it is encouraging to see approximately 1000 background images."}, {"heading": "3.3 Conditioning on Portrait Embeddings", "text": "In our next experiment, we took the latent representations from the top layer of a winding network trained on a large database of portraits that were automatically cropped with the help of a face detector from Flickr images, and the quality of the images varied wildly, as many of the images were taken with mobile phones under poor lightning conditions, and the network was trained with a tripling loss function [23] that ensured that the embedding h for an x-image of a particular person was closer to the embedding for all other images of the same person than any embedding of another person. After the supervised network was trained, we took the (image = x, embedding = h) tuples and trained the conditional pixel CNN on the p (x | h) model. Given a new image of a person who was not in the training set, we can calculate h = f (x) and generate new portraits of the same person. Samples from the model are shown in Figure 4."}, {"heading": "3.4 PixelCNN Auto Encoder", "text": "This experiment explores the possibility of training both the encoder and the decoder (PixelCNN) as an auto-encoder at the end. We trained a PixelCNN auto-encoder on 32x32 ImageNet patches and compared the results with those of a conventional auto-encoder trained to optimize MSE. Both models used a 10- or 100-dimensional bottleneck. Figure 6 shows the reconstructions from both models. For the PixelCNN we use several conditional reconstructions. These images support our prediction in Section 2.4 that the information encoded in the bottleneck representation h will be qualitatively different than with a more conventional decoder. For example, in the bottom row we can see that the model creates different but similar-looking interior scenes with humans, rather than trying to accurately reconstruct the input."}, {"heading": "4 Conclusion", "text": "This work introduced the gated PixelCNN, an improvement over the original PixelCNN, which is able to match or exceed PixelRNN [30] and is more computationally efficient. In our new architecture, we use two stacks of CNNs to deal with \"blind spots\" in the receptive field that limited the original PixelCNN. In addition, we use a gating mechanism that improves performance and convergence speed. We have shown that the architecture performs similar to PixelRNN on CIFAR-10 and is now state of the art on the ImageNet 32x32 and 64x64 datasets. In addition, we have studied conditional modeling of natural images in three different settings. In the class-based generation, we showed that a single model is capable of producing diverse and realistic-looking images that fit different classes."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Marc G Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "Remi Munos"], "venue": "arXiv preprint arXiv:1606.01868,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "NICE: Non-linear independent components estimation", "author": ["Laurent Dinh", "David Krueger", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1410.8516,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "A neural algorithm of artistic style", "author": ["Leon A Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Towards conceptual compression", "author": ["Karol Gregor", "Frederic Besse", "Danilo J Rezende", "Ivo Danihelka", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep autoregressive networks", "author": ["Karol Gregor", "Ivo Danihelka", "Andriy Mnih", "Charles Blundell", "Daan Wierstra"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Neural gpus learn algorithms", "author": ["\u0141ukasz Kaiser", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "The neural autoregressive distribution estimator", "author": ["Hugo Larochelle", "Iain Murray"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Generating images from captions with attention", "author": ["Elman Mansimov", "Emilio Parisotto", "Jimmy Lei Ba", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1511.02793,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["Jonathan Masci", "Ueli Meier", "Dan Cire\u015fan", "J\u00fcrgen Schmidhuber"], "venue": "In Artificial Neural Networks and Machine Learning\u2013ICANN", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["Junhyuk Oh", "Xiaoxiao Guo", "Honglak Lee", "Richard L Lewis", "Satinder Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Inceptionism: Going deeper into neural networks", "author": ["Christopher Olah", "Mike Tyka"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Generative adversarial text to image synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee"], "venue": "arXiv preprint arXiv:1605.05396,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo J Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "One-shot generalization in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Ivo Danihelka", "Karol Gregor", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1603.05106,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Learning with hierarchical-deep models", "author": ["Ruslan Salakhutdinov", "Joshua B Tenenbaum", "Antonio Torralba"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1958}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Deep unsupervised learning using nonequilibrium thermodynamics", "author": ["Jascha Sohl-Dickstein", "Eric A. Weiss", "Niru Maheswaranathan", "Surya Ganguli"], "venue": "Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Training very deep networks", "author": ["Rupesh K Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Generative image modeling using spatial LSTMs", "author": ["Lucas Theis", "Matthias Bethge"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "A note on the evaluation of generative models", "author": ["Lucas Theis", "Aaron van den Oord", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1511.01844,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Neural autoregressive distribution estimation", "author": ["Benigno Uria", "Marc-Alexandre C\u00f4t\u00e9", "Karol Gregor", "Iain Murray", "Hugo Larochelle"], "venue": "arXiv preprint arXiv:1605.02226,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Locally-connected transformations for deep gmms", "author": ["Aaron van den Oord", "Joni Dambre"], "venue": "In International Conference on Machine Learning (ICML) : Deep learning Workshop,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Pixel recurrent neural networks", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Factoring variations in natural images with deep gaussian mixture models", "author": ["A\u00e4ron van den Oord", "Benjamin Schrauwen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "The student-t mixture as a natural image patch prior with application to image compression", "author": ["Aaron van den Oord", "Benjamin Schrauwen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "Recent advances in image modelling with neural networks [30, 26, 20, 10, 9, 28, 6] have made it feasible to generate diverse natural images that capture the high-level structure of the training data.", "startOffset": 56, "endOffset": 82}, {"referenceID": 25, "context": "Recent advances in image modelling with neural networks [30, 26, 20, 10, 9, 28, 6] have made it feasible to generate diverse natural images that capture the high-level structure of the training data.", "startOffset": 56, "endOffset": 82}, {"referenceID": 19, "context": "Recent advances in image modelling with neural networks [30, 26, 20, 10, 9, 28, 6] have made it feasible to generate diverse natural images that capture the high-level structure of the training data.", "startOffset": 56, "endOffset": 82}, {"referenceID": 9, "context": "Recent advances in image modelling with neural networks [30, 26, 20, 10, 9, 28, 6] have made it feasible to generate diverse natural images that capture the high-level structure of the training data.", "startOffset": 56, "endOffset": 82}, {"referenceID": 8, "context": "Recent advances in image modelling with neural networks [30, 26, 20, 10, 9, 28, 6] have made it feasible to generate diverse natural images that capture the high-level structure of the training data.", "startOffset": 56, "endOffset": 82}, {"referenceID": 27, "context": "Recent advances in image modelling with neural networks [30, 26, 20, 10, 9, 28, 6] have made it feasible to generate diverse natural images that capture the high-level structure of the training data.", "startOffset": 56, "endOffset": 82}, {"referenceID": 5, "context": "Recent advances in image modelling with neural networks [30, 26, 20, 10, 9, 28, 6] have made it feasible to generate diverse natural images that capture the high-level structure of the training data.", "startOffset": 56, "endOffset": 82}, {"referenceID": 16, "context": "While such unconditional models are fascinating in their own right, many of the practical applications of image modelling require the model to be conditioned on prior information: for example, an image model used for reinforcement learning planning in a visual environment would need to predict future scenes given specific states and actions [17].", "startOffset": 343, "endOffset": 347}, {"referenceID": 17, "context": "Neural artwork [18, 5] and content generation represent potential future uses for conditional generation.", "startOffset": 15, "endOffset": 22}, {"referenceID": 4, "context": "Neural artwork [18, 5] and content generation represent potential future uses for conditional generation.", "startOffset": 15, "endOffset": 22}, {"referenceID": 29, "context": "This paper explores the potential for conditional image modelling by adapting and improving a convolutional variant of the PixelRNN architecture [30].", "startOffset": 145, "endOffset": 149}, {"referenceID": 5, "context": "As well as providing excellent samples, this network has the advantage of returning explicit probability densities (unlike alternatives such as generative adversarial networks [6, 3, 19]), making it straightforward to apply in domains such as compression [32] and probabilistic planning and exploration [2].", "startOffset": 176, "endOffset": 186}, {"referenceID": 2, "context": "As well as providing excellent samples, this network has the advantage of returning explicit probability densities (unlike alternatives such as generative adversarial networks [6, 3, 19]), making it straightforward to apply in domains such as compression [32] and probabilistic planning and exploration [2].", "startOffset": 176, "endOffset": 186}, {"referenceID": 18, "context": "As well as providing excellent samples, this network has the advantage of returning explicit probability densities (unlike alternatives such as generative adversarial networks [6, 3, 19]), making it straightforward to apply in domains such as compression [32] and probabilistic planning and exploration [2].", "startOffset": 176, "endOffset": 186}, {"referenceID": 31, "context": "As well as providing excellent samples, this network has the advantage of returning explicit probability densities (unlike alternatives such as generative adversarial networks [6, 3, 19]), making it straightforward to apply in domains such as compression [32] and probabilistic planning and exploration [2].", "startOffset": 255, "endOffset": 259}, {"referenceID": 1, "context": "As well as providing excellent samples, this network has the advantage of returning explicit probability densities (unlike alternatives such as generative adversarial networks [6, 3, 19]), making it straightforward to apply in domains such as compression [32] and probabilistic planning and exploration [2].", "startOffset": 303, "endOffset": 306}, {"referenceID": 6, "context": "Two variants were proposed in the original paper: PixelRNN, where the pixel distributions are modeled with two-dimensional LSTM [7, 26], and PixelCNN, where they are modelled with convolutional networks.", "startOffset": 128, "endOffset": 135}, {"referenceID": 25, "context": "Two variants were proposed in the original paper: PixelRNN, where the pixel distributions are modeled with two-dimensional LSTM [7, 26], and PixelCNN, where they are modelled with convolutional networks.", "startOffset": 128, "endOffset": 135}, {"referenceID": 29, "context": "PixelCNNs (and PixelRNNs) [30] model the joint distribution of pixels over an image x as the following product of conditional distributions, where xi is a single pixel:", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "A similar setup has been used by other autoregressive models such as NADE [14] and RIDE [26].", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "A similar setup has been used by other autoregressive models such as NADE [14] and RIDE [26].", "startOffset": 88, "endOffset": 92}, {"referenceID": 29, "context": "1 Gated Convolutional Layers PixelRNNs, which use spatial LSTM layers instead of convolutional stacks, have previously been shown to outperform PixelCNNs as generative models [30].", "startOffset": 175, "endOffset": 179}, {"referenceID": 24, "context": "Feed-forward neural networks with gates have been explored in previous works, such as highway networks [25], grid LSTM [13] and neural GPUs [12], and have generally proved beneficial to performance.", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "Feed-forward neural networks with gates have been explored in previous works, such as highway networks [25], grid LSTM [13] and neural GPUs [12], and have generally proved beneficial to performance.", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": "Feed-forward neural networks with gates have been explored in previous works, such as highway networks [25], grid LSTM [13] and neural GPUs [12], and have generally proved beneficial to performance.", "startOffset": 140, "endOffset": 144}, {"referenceID": 29, "context": "As proposed in [30] we also use a residual connection [11] in the horizontal stack.", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "As proposed in [30] we also use a residual connection [11] in the horizontal stack.", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "Starting with a traditional convolutional auto-encoder architecture [16], we replace the deconvolutional decoder with a conditional PixelCNN and train the complete network end-to-end.", "startOffset": 68, "endOffset": 72}, {"referenceID": 29, "context": "Model NLL Test (Train) Uniform Distribution: [30] 8.", "startOffset": 45, "endOffset": 49}, {"referenceID": 29, "context": "00 Multivariate Gaussian: [30] 4.", "startOffset": 26, "endOffset": 30}, {"referenceID": 3, "context": "70 NICE: [4] 4.", "startOffset": 9, "endOffset": 12}, {"referenceID": 23, "context": "48 Deep Diffusion: [24] 4.", "startOffset": 19, "endOffset": 23}, {"referenceID": 8, "context": "20 DRAW: [9] 4.", "startOffset": 9, "endOffset": 12}, {"referenceID": 30, "context": "13 Deep GMMs: [31, 29] 4.", "startOffset": 14, "endOffset": 22}, {"referenceID": 28, "context": "13 Deep GMMs: [31, 29] 4.", "startOffset": 14, "endOffset": 22}, {"referenceID": 7, "context": "00 Conv DRAW: [8] 3.", "startOffset": 14, "endOffset": 17}, {"referenceID": 25, "context": "57) RIDE: [26, 30] 3.", "startOffset": 10, "endOffset": 18}, {"referenceID": 29, "context": "57) RIDE: [26, 30] 3.", "startOffset": 10, "endOffset": 18}, {"referenceID": 29, "context": "47 PixelCNN: [30] 3.", "startOffset": 13, "endOffset": 17}, {"referenceID": 29, "context": "08) PixelRNN: [30] 3.", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": "We were able to achieve similar performance to the PixelRNN (Row LSTM [30]) in less than half the training time (60 hours using 32 GPUs).", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "We used 200K synchronous updates over 32 GPUs in TensorFlow [1] using a total batch size of 128.", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "32x32 Model NLL Test (Train) Conv Draw: [8] 4.", "startOffset": 40, "endOffset": 43}, {"referenceID": 29, "context": "35) PixelRNN: [30] 3.", "startOffset": 14, "endOffset": 18}, {"referenceID": 7, "context": "77) 64x64 Model NLL Test (Train) Conv Draw: [8] 4.", "startOffset": 44, "endOffset": 47}, {"referenceID": 29, "context": "04) PixelRNN: [30] 3.", "startOffset": 14, "endOffset": 18}, {"referenceID": 26, "context": "On the other hand, as noted in [27], we observed great improvements in the visual quality of the generated samples.", "startOffset": 31, "endOffset": 35}, {"referenceID": 22, "context": "The network was trained with a triplet loss function [23] that ensured that the embedding h produced for an image x of a specific person was closer to the embeddings for all other images of the same person than it was to any embedding of another person.", "startOffset": 53, "endOffset": 57}, {"referenceID": 29, "context": "This work introduced the Gated PixelCNN, an improvement over the original PixelCNN that is able to match or outperform PixelRNN [30], and is computationally more efficient.", "startOffset": 128, "endOffset": 132}, {"referenceID": 20, "context": "In the future it might be interesting to try and generate new images with a certain animal or object solely from a single example image [21, 22].", "startOffset": 136, "endOffset": 144}, {"referenceID": 21, "context": "In the future it might be interesting to try and generate new images with a certain animal or object solely from a single example image [21, 22].", "startOffset": 136, "endOffset": 144}], "year": 2016, "abstractText": "This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder, creating . Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.", "creator": "LaTeX with hyperref package"}}}