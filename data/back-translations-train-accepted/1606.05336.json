{"id": "1606.05336", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "On the Expressive Power of Deep Neural Networks", "abstract": "We study the expressivity of deep neural networks with random weights. We provide several results, both theoretical and experimental, precisely characterizing their functional properties in terms of the depth and width of the network. In doing so, we illustrate inherent connections between the length of a latent trajectory, local neuron transitions, and network activation patterns. The latter, a notion defined in this paper, is further studied using properties of hyperplane arrangements, which also help precisely characterize the effect of the neural network on the input space. We further show dualities between changes to the latent state and changes to the network weights, and between the number of achievable activation patterns and the number of achievable labellings over input data. We see that the depth of the network affects all of these quantities exponentially, while the width appears at most as a base. These results also suggest that the remaining depth of a neural network is an important determinant of expressivity, supported by experiments on MNIST and CIFAR-10.", "histories": [["v1", "Thu, 16 Jun 2016 19:55:29 GMT  (1051kb,D)", "http://arxiv.org/abs/1606.05336v1", null], ["v2", "Fri, 24 Jun 2016 20:26:47 GMT  (1789kb,D)", "http://arxiv.org/abs/1606.05336v2", null], ["v3", "Wed, 17 Aug 2016 22:21:25 GMT  (2267kb,D)", "http://arxiv.org/abs/1606.05336v3", null], ["v4", "Mon, 3 Oct 2016 15:44:39 GMT  (951kb,D)", "http://arxiv.org/abs/1606.05336v4", null], ["v5", "Wed, 1 Mar 2017 03:00:26 GMT  (3798kb,D)", "http://arxiv.org/abs/1606.05336v5", null], ["v6", "Sun, 18 Jun 2017 13:24:34 GMT  (4061kb,D)", "http://arxiv.org/abs/1606.05336v6", "Accepted to ICML 2017"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["maithra raghu", "ben poole", "jon m kleinberg", "surya ganguli", "jascha sohl-dickstein"], "accepted": true, "id": "1606.05336"}, "pdf": {"name": "1606.05336.pdf", "metadata": {"source": "CRF", "title": "On the expressive power of deep neural networks", "authors": ["Maithra Raghu", "Ben Poole", "Surya Ganguli"], "emails": [], "sections": [{"heading": null, "text": "We study the expressivity of deep neural networks with random weighting. We provide several theoretical and experimental results to accurately characterize their functional properties with respect to the depth and width of the network, illustrating inherent relationships between the length of a latent trajectory, local neuron transitions, and network activation patterns. The latter, an idea defined in this paper, is further investigated using properties of hyperplanetary arrangements that also contribute to the precise characterization of the effect of the neural network on the input space. Furthermore, we show dualities between changes in the latent state and changes in network weights, as well as between the number of achievable activation patterns and the number of achievable labeling via input data. We see that the depth of the network affects all these quantities exponentially, while the width appears at most as a basis. These results also suggest that the remaining depth of a neural network is an important determinant of expressivity that is determined by experiments supported by Nan MIST-C10 and IAR."}, {"heading": "1 Introduction", "text": "In fact, it is such that most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "1.1 Expressivity", "text": "In fact, most of them are able to determine for themselves how they have behaved."}, {"heading": "2 Exponential Expressivity", "text": "In this section, we establish a fundamental relationship between this length and a local notion of expressivity, the number of transitions that neurons pass through along an input curve. We then link these properties to a global notion of expressivity, the number of activation patterns of the entire network, which is examined by a novel linkage with the theory of hyper-plane alignments. Theory sequences are merged in Section 3, where we examine dichotomy - labeling via inputs. Theoretical and experimental measurements of all these properties show an exponential dependence on depth, but not width. This is illustrated in Figure 1, which shows how the number of dichotomy grows with both depth and width."}, {"heading": "2.1 Notation and Definitions", "text": "In this section we will consider architectures with input dimension m, n hidden layers of total width k and a scalar read layer. (So, FW: Rm \u2192 R.) We will use v (d) i to put the i-th neuron in hidden layer d. We will also let x = z (0) be an input layer, h (d) the hidden representation on layer d, and \u03c6 the non-linearity. (1) Our results are usually referred to as W (d) and b (d) respectively. So we have the Relationsh (d + 1) = W (d) + b (d), z (d + 1) = D (d). Our results examine the cases where it is a hard tanh sign Collobert and Bengio (2004)."}, {"heading": "2.2 Neuron transitions and trajectory length", "text": "In this section, we analyze the number of character transitions of FW, a random neural hard-tanh network, as the input x (t) is swept through a one-dimensional trajectory. We rigorously deduce how the length of the input curve x (t) changes as it propagates through FW, and then verify experimentally and theoretically deduce a linear relationship between trajectory length and number of transitions for the large weight limit."}, {"heading": "2.2.1 Bound on trajectory length growth", "text": "We would like to understand how the arc length of a one-dimensional orbit x (t) changes when it is pushed through a network FW. We prove: (with a more precise lower limit in the appendix) Theorem 1 (t) = z (d) (t) = z (t) = to be the imageof the orbit in the layer d of FW, and let l (d) (t) be a one-dimensional orbit in the input space. (t) Define z (d) (t) = to be the entire layer d of FW, and let l (d) (t) be the layer of growth in the layer d, and let l (t) prove the layer of growth in the layer d. (t) The layer of growth in the layer d is. (t) The layer of growth in the layer d is."}, {"heading": "2.2.2 Relation to number of transitions", "text": "To understand the relationship to the number of character transitions, note the following: For FW with n hidden layers as above, the linear, one-dimensional read layer outputs a value by calculating the inner product (z (n)) TW (n). The sign of the output is then determined by whether this quantity is \u2265 0 or not. Specifically, the decision limit is a hyperplane, with equation (z (n)) TW (n) = 0. The number of transitions we see in the input as x (t) is then exactly the number of times that z (n) (t) (t) exceeds the decision limit. Since FW is a random neural network, with purely randomly distributed weight entries between \u00b1 1, we expect a direct proportionality between the length of the curve z (n) (t) and the number of times it exceeds the decision limit."}, {"heading": "2.2.3 Trajectory length growth in different regimes", "text": "Returning to the statement of Theorem 2, we can look at how the path length growth (and thus the number of transitions) looks in different decisions of k and \u03c3w. We see that for very large k the path length is exponential in depth, with the base depending only on \u03c3w. This seems intuitively correct, since we expect a normalizing effect caused by the Central Limit Theorem on very large k - we add a very large number of independent random variables (for each neuron) and perform an implicit averaging by setting the variance of each weight coordinate to \u03c32w / k. For very large \u03c3w, we see that the basis of exponential growth is now \u221a k. This \"large weight limit\" results in very traceable behavior, because: (1) For each input (which is delimited from zero) almost all neurons are saturated (2)."}, {"heading": "2.3 A Global Perspective: Transitions and Activation Patterns", "text": "In fact, this local measure of expressivity has an elegant connection to the \"global\" notion of expression patterns of the number of activation patterns of a network. We can show that for many well-behaved trajectories, the number of transitions (which is directly proportional to the trajectory length) is exactly the same as the number of unique activation patterns. A helpful mental image is obtained by the language of the hyperplane alignments. A hyperplane alignment consists of a series of hyperplanes in an ambient space, in our case the alignment of the input space Rm in regions. A hyperplane alignment of this kind refers to neural networks in the following way: Looking at a random neural network. In view of a certain neuron alignment in the first layer, we can ask for which inputs it is \"on\" (the linear regime) and for which inputs it \"is."}, {"heading": "3 Function Space", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The Dual Perspective", "text": "This year, we have reached the stage where we feel we are in a position to achieve the objectives I have mentioned, but we do not feel we are in a position to achieve them."}, {"heading": "4 Experiments", "text": "We implemented the random network architecture described in Section 2.1. In separate experiments, we then swept an input vector along a large circular orbit for fixed weights and swept weights along a large circular orbit for a fixed set of inputs, as described in Section 3.1. In both cases, the trajectory was divided into 106 segments. We repeated this for a grid of network widths k, weight deviations \u03c32w and number of inputs. Unless otherwise noted, \u03c3b = 0 for all experiments. We repeated each experiment ten times and averaged the results. Simulation results are discussed and plotted throughout the text. We also investigated the effects of these results on trained networks. In the experiments related to Figures 7, 8, 9, we randomly initiated a neural network and froze all layers, except one we trained (on MNIST and CIFAR-10). The results show that higher training performance results from the training than the lower layer result from the 6."}, {"heading": "5 Conclusion", "text": "This framework enabled an average case analysis and illustrated the relationship between orbit length, neuron transitions, activation patterns and attainable classification dichotomies (see Table 1). Our analysis also provided precise theoretical and experimental results on the effect of depth and width on neural networks. In particular, greater depth leads to an exponential increase in these values. That exponentially large changes in performance can be caused by small changes in input could help explain the prevalence of contrarian examples in deep networks Szegedy et al. [2013]. Furthermore, understanding this structure could inspire new optimization schemes that take into account the different leverage effects of weights in different layers."}, {"heading": "Acknowledgements", "text": "We thank Samy Bengio, Ian Goodfellow, Laurent Dinh and Quoc Le for the extremely helpful discussion."}, {"heading": "A Proofs and additional results from Section 2.2", "text": "The result is: \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W,\" W, \"W, W, W,\" W, W, W, \"W, W, W, W, W,\" W, W, W, W, W, W, \"W, W, W, W, W, W, W, W, W,\", W, W, W, W, W, W, W, W, W, W, W, \""}, {"heading": "B Proofs and additional results from Section 2.2.3", "text": "The proof of theory 3proof. For \u03c3b = 0: For hidden layer d > > Y = > Y (> Y =) we believe that neuron v (d) 1. This has as input quantity k = 1W (d \u2212 1) i1 z (d \u2212 1) i1. Since we are in the case of major \u03c3, we assume that z (d \u2212 1) i | = 1. Furthermore, as characters for z (d \u2212 1) i and W (d \u2212 1) i1 both are completely random, we can assume that z (d \u2212 1) i = 1. For a certain input layer, we can define v (d) 1 as sensitivity to v (d \u2212 1) i if v (d \u2212 1) i transitioning (to wlog \u2212 1) i1) cause a transition in the layer v (d). A sufficient condition for this is that this happens."}, {"heading": "C Proofs and additional results from Section 2.3", "text": "The proof of theorem 5 Proof. Let the hyperplane arrangement be called H, and let H-H be a particular hyperplane. Then, the number of regions in H-H is exactly the number of regions in H-H plus the number of regions in H-H. (This results from the fact that H is divided into two regions exactly into H-H and does not affect any of the other regions.) In particular, we have the recursive form (k, m) = r (k \u2212 1, m) + r (k \u2212 1, m \u2212 1) + r (k \u2212 1) (We now have k + m to assert the claim. The base cases of r (1, 0) = r (0, 1) = 1 are trivial, and assuming the claim for \u2264 k + m \u2212 1 as an induction hypothesis, we have (k \u2212 1, m \u2212 1) + r (k \u2212 1, m \u2212 1) \u2264 m, i = 0 (k \u2212 1) + m \u2212 i = 0 (k \u2212 i = 1, 1) are equal (1), and assuming the claim for \u2264 k + m \u2212 1 (m \u2212 1, m \u2212 1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1"}, {"heading": "D Proofs and additional results from Section 3", "text": "D.1 Upper limit for dichotomies The Vapnik-Chervonenko (VC) dimension of a functional class is the cardinality of the largest number of points it can destroy. VC dimension represents an upper (worst) limit for the generalization error of a functional class Vapnik and Vapnik [1998]. Motivated by a generalization error, the VC dimension for neural networks Sontag [1998], Bartlett and Maass [2003] was investigated. In Bartlett et al. [1998] an upper limit for the VC dimension v of a neural network with piecemeal polynmic activation function and binary output was derived. For hard tanh units, this limit is isv = 2 | W | n log (4e | W | nk) + 2 | W | n2 log 2 2 2 + 2n, (4) where | W | is the total number of weights, n the depth and k the width of the mesh. VC dimension provides an upper limit for growth of the sotomies (4)."}], "references": [{"title": "The unreasonable effectiveness of deep learning", "author": ["Yann LeCun"], "venue": "In Seminar. Johns Hopkins University,", "citeRegEx": "LeCun.,? \\Q2014\\E", "shortCiteRegEx": "LeCun.", "year": 2014}, {"title": "The unreasonable effectiveness of recurrent neural networks", "author": ["Andrej Karpathy"], "venue": "In Andrej Karpathy blog,", "citeRegEx": "Karpathy.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Searching for exotic particles in high-energy physics with deep learning", "author": ["Pierre Baldi", "Peter Sadowski", "Daniel Whiteson"], "venue": "Nature communications,", "citeRegEx": "Baldi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 2014}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Deep knowledge tracing", "author": ["Chris Piech", "Jonathan Bassen", "Jonathan Huang", "Surya Ganguli", "Mehran Sahami", "Leonidas J Guibas", "Jascha Sohl-Dickstein"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Piech et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Piech et al\\.", "year": 2015}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["Ian J Goodfellow", "Oriol Vinyals", "Andrew M Saxe"], "venue": "arXiv preprint arXiv:1412.6544,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "The loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "arXiv preprint arXiv:1412.0233,", "citeRegEx": "Choromanska et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2014}, {"title": "Optimizing neural networks with kronecker-factored approximate curvature", "author": ["James Martens", "Roger Grosse"], "venue": "arXiv preprint arXiv:1503.05671,", "citeRegEx": "Martens and Grosse.,? \\Q2015\\E", "shortCiteRegEx": "Martens and Grosse.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "Vc dimension of neural networks", "author": ["Eduardo D Sontag"], "venue": "NATO ASI SERIES F COMPUTER AND SYSTEMS SCIENCES,", "citeRegEx": "Sontag.,? \\Q1998\\E", "shortCiteRegEx": "Sontag.", "year": 1998}, {"title": "Vapnik-chervonenkis dimension of neural nets", "author": ["Peter L Bartlett", "Wolfgang Maass"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "Bartlett and Maass.,? \\Q2003\\E", "shortCiteRegEx": "Bartlett and Maass.", "year": 2003}, {"title": "Almost linear vc-dimension bounds for piecewise polynomial networks", "author": ["Peter L Bartlett", "Vitaly Maiorov", "Ron Meir"], "venue": "Neural computation,", "citeRegEx": "Bartlett et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 1998}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Kurt Hornik", "Maxwell Stinchcombe", "Halbert White"], "venue": "Neural networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko"], "venue": "Mathematics of control, signals and systems,", "citeRegEx": "Cybenko.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko.", "year": 1989}, {"title": "A comparison of the computational power of sigmoid and Boolean threshold circuits", "author": ["Wolfgang Maass", "Georg Schnitger", "Eduardo D Sontag"], "venue": null, "citeRegEx": "Maass et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Maass et al\\.", "year": 1994}, {"title": "Expressiveness of rectifier networks", "author": ["Xingyuan Pan", "Vivek Srikumar"], "venue": "arXiv preprint arXiv:1511.05678,", "citeRegEx": "Pan and Srikumar.,? \\Q2015\\E", "shortCiteRegEx": "Pan and Srikumar.", "year": 2015}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1512.03965,", "citeRegEx": "Eldan and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Eldan and Shamir.", "year": 2015}, {"title": "Representation benefits of deep feedforward networks", "author": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1509.08101,", "citeRegEx": "Telgarsky.,? \\Q2015\\E", "shortCiteRegEx": "Telgarsky.", "year": 2015}, {"title": "On the representational efficiency of restricted boltzmann machines", "author": ["James Martens", "Arkadev Chattopadhya", "Toni Pitassi", "Richard Zemel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Martens et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2013}, {"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "author": ["Monica Bianchini", "Franco Scarselli"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "Bianchini and Scarselli.,? \\Q2014\\E", "shortCiteRegEx": "Bianchini and Scarselli.", "year": 2014}, {"title": "On the number of response regions of deep feed forward networks with piece-wise linear activations", "author": ["Razvan Pascanu", "Guido Montufar", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1312.6098,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity", "author": ["Amit Daniely", "Roy Frostig", "Yoram Singer"], "venue": "arXiv preprint arXiv:1602.05897,", "citeRegEx": "Daniely et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2016}, {"title": "Exponential expressivity", "author": ["Ben Poole", "Subhaneil Lahiri", "Maithra Raghu", "Jascha Sohl-Dickstein", "Surya Ganguli"], "venue": null, "citeRegEx": "Poole et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Poole et al\\.", "year": 2015}, {"title": "Links between perceptrons, mlps and svms", "author": ["2016. Ronan Collobert", "Samy Bengio"], "venue": "In Proceedings of the twenty-first", "citeRegEx": "Collobert and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "Collobert and Bengio.", "year": 2016}, {"title": "Hyperplane arrangements", "author": ["Richard Stanley"], "venue": "international conference on Machine learning,", "citeRegEx": "Stanley.,? \\Q2004\\E", "shortCiteRegEx": "Stanley.", "year": 2004}, {"title": "On some inequalities for the gamma function", "author": ["Andrea Laforgia", "Pierpaolo Natalini"], "venue": "Advances in Dynamical", "citeRegEx": "Laforgia and Natalini.,? \\Q1983\\E", "shortCiteRegEx": "Laforgia and Natalini.", "year": 1983}], "referenceMentions": [{"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al.", "startOffset": 66, "endOffset": 79}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al.", "startOffset": 66, "endOffset": 96}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al.", "startOffset": 66, "endOffset": 167}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al.", "startOffset": 66, "endOffset": 233}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al.", "startOffset": 66, "endOffset": 266}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al. [2015]. Despite their power, we have limited understanding of how and why neural networks work, and much of this understanding is qualitative and heuristic rather than precise and quantitative.", "startOffset": 66, "endOffset": 323}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al. [2015]. Despite their power, we have limited understanding of how and why neural networks work, and much of this understanding is qualitative and heuristic rather than precise and quantitative. While even qualitative knowledge is helpful, a more formal and fundamental framework is desperately needed to ease the laborious process of designing and training these networks, as well as to enable us to to better predict and protect against their failures. In order to develop theoretical underpinnings for deep networks, it is first necessary to disentangle the many factors that influence their effectiveness. The effectiveness of deep neural networks on real tasks can be interpreted in terms of three broad properties: their trainability, or how well they can be fit to data; their generalizability, or how well they perform on novel examples; and their expressivity, or the set of functions they can compute. All three of these properties are crucial for understanding the performance of neural networks. First, neural networks are useful only to the extent they can be sucessfully trained. Previous work has explored the objective function landscape of neural networks Dauphin et al. [2014], Goodfellow et al.", "startOffset": 66, "endOffset": 1510}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al. [2015]. Despite their power, we have limited understanding of how and why neural networks work, and much of this understanding is qualitative and heuristic rather than precise and quantitative. While even qualitative knowledge is helpful, a more formal and fundamental framework is desperately needed to ease the laborious process of designing and training these networks, as well as to enable us to to better predict and protect against their failures. In order to develop theoretical underpinnings for deep networks, it is first necessary to disentangle the many factors that influence their effectiveness. The effectiveness of deep neural networks on real tasks can be interpreted in terms of three broad properties: their trainability, or how well they can be fit to data; their generalizability, or how well they perform on novel examples; and their expressivity, or the set of functions they can compute. All three of these properties are crucial for understanding the performance of neural networks. First, neural networks are useful only to the extent they can be sucessfully trained. Previous work has explored the objective function landscape of neural networks Dauphin et al. [2014], Goodfellow et al. [2014], Choromanska et al.", "startOffset": 66, "endOffset": 1536}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al. [2015]. Despite their power, we have limited understanding of how and why neural networks work, and much of this understanding is qualitative and heuristic rather than precise and quantitative. While even qualitative knowledge is helpful, a more formal and fundamental framework is desperately needed to ease the laborious process of designing and training these networks, as well as to enable us to to better predict and protect against their failures. In order to develop theoretical underpinnings for deep networks, it is first necessary to disentangle the many factors that influence their effectiveness. The effectiveness of deep neural networks on real tasks can be interpreted in terms of three broad properties: their trainability, or how well they can be fit to data; their generalizability, or how well they perform on novel examples; and their expressivity, or the set of functions they can compute. All three of these properties are crucial for understanding the performance of neural networks. First, neural networks are useful only to the extent they can be sucessfully trained. Previous work has explored the objective function landscape of neural networks Dauphin et al. [2014], Goodfellow et al. [2014], Choromanska et al. [2014], designed optimization procedures specifically suited to neural networks Martens and Grosse [2015], and proposed architectural components such as ReLUs Nair and Hinton [2010], which we analyze in this paper, specifically designed to make them more trainable.", "startOffset": 66, "endOffset": 1563}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al. [2015]. Despite their power, we have limited understanding of how and why neural networks work, and much of this understanding is qualitative and heuristic rather than precise and quantitative. While even qualitative knowledge is helpful, a more formal and fundamental framework is desperately needed to ease the laborious process of designing and training these networks, as well as to enable us to to better predict and protect against their failures. In order to develop theoretical underpinnings for deep networks, it is first necessary to disentangle the many factors that influence their effectiveness. The effectiveness of deep neural networks on real tasks can be interpreted in terms of three broad properties: their trainability, or how well they can be fit to data; their generalizability, or how well they perform on novel examples; and their expressivity, or the set of functions they can compute. All three of these properties are crucial for understanding the performance of neural networks. First, neural networks are useful only to the extent they can be sucessfully trained. Previous work has explored the objective function landscape of neural networks Dauphin et al. [2014], Goodfellow et al. [2014], Choromanska et al. [2014], designed optimization procedures specifically suited to neural networks Martens and Grosse [2015], and proposed architectural components such as ReLUs Nair and Hinton [2010], which we analyze in this paper, specifically designed to make them more trainable.", "startOffset": 66, "endOffset": 1662}, {"referenceID": 0, "context": "Neural network architectures have proven \u201cunreasonably effective\u201d LeCun [2014], Karpathy [2015] on many tasks, including image classification Krizhevsky et al. [2012], identifying particles in high energy physics Baldi et al. [2014], playing Go Silver et al. [2016], and modeling human student learning Piech et al. [2015]. Despite their power, we have limited understanding of how and why neural networks work, and much of this understanding is qualitative and heuristic rather than precise and quantitative. While even qualitative knowledge is helpful, a more formal and fundamental framework is desperately needed to ease the laborious process of designing and training these networks, as well as to enable us to to better predict and protect against their failures. In order to develop theoretical underpinnings for deep networks, it is first necessary to disentangle the many factors that influence their effectiveness. The effectiveness of deep neural networks on real tasks can be interpreted in terms of three broad properties: their trainability, or how well they can be fit to data; their generalizability, or how well they perform on novel examples; and their expressivity, or the set of functions they can compute. All three of these properties are crucial for understanding the performance of neural networks. First, neural networks are useful only to the extent they can be sucessfully trained. Previous work has explored the objective function landscape of neural networks Dauphin et al. [2014], Goodfellow et al. [2014], Choromanska et al. [2014], designed optimization procedures specifically suited to neural networks Martens and Grosse [2015], and proposed architectural components such as ReLUs Nair and Hinton [2010], which we analyze in this paper, specifically designed to make them more trainable.", "startOffset": 66, "endOffset": 1738}, {"referenceID": 11, "context": "A bound on generalization error in terms of the number of stochastic gradient descent steps performed is developed in Hardt et al. [2015]. VC dimension provides an upper bound on generalization error, and has been studied for neural networks Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 118, "endOffset": 138}, {"referenceID": 11, "context": "A bound on generalization error in terms of the number of stochastic gradient descent steps performed is developed in Hardt et al. [2015]. VC dimension provides an upper bound on generalization error, and has been studied for neural networks Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 118, "endOffset": 256}, {"referenceID": 11, "context": "A bound on generalization error in terms of the number of stochastic gradient descent steps performed is developed in Hardt et al. [2015]. VC dimension provides an upper bound on generalization error, and has been studied for neural networks Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 118, "endOffset": 283}, {"referenceID": 11, "context": "A bound on generalization error in terms of the number of stochastic gradient descent steps performed is developed in Hardt et al. [2015]. VC dimension provides an upper bound on generalization error, and has been studied for neural networks Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998]. In model distillation, more efficient models are trained to emulate costly models which possess better generalization error Hinton et al.", "startOffset": 118, "endOffset": 307}, {"referenceID": 11, "context": "A bound on generalization error in terms of the number of stochastic gradient descent steps performed is developed in Hardt et al. [2015]. VC dimension provides an upper bound on generalization error, and has been studied for neural networks Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998]. In model distillation, more efficient models are trained to emulate costly models which possess better generalization error Hinton et al. [2015]. In this paper, we focus on the third of these properties, expressivity \u2014 the power of the class of deep neural networks to represent highly complex functions.", "startOffset": 118, "endOffset": 453}, {"referenceID": 13, "context": "This approach has yielded many fascinating results: neural networks have been shown to be universal approximators Hornik et al. [1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 114, "endOffset": 135}, {"referenceID": 13, "context": "This approach has yielded many fascinating results: neural networks have been shown to be universal approximators Hornik et al. [1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 114, "endOffset": 151}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 51, "endOffset": 65}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al.", "startOffset": 51, "endOffset": 92}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al.", "startOffset": 51, "endOffset": 116}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015].", "startOffset": 51, "endOffset": 222}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.", "startOffset": 51, "endOffset": 247}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al.", "startOffset": 51, "endOffset": 638}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al.", "startOffset": 51, "endOffset": 656}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al. [2013], Bianchini and Scarselli [2014].", "startOffset": 51, "endOffset": 679}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al. [2013], Bianchini and Scarselli [2014]. This is often used to suggest the benefits of deep networks over shallow networks.", "startOffset": 51, "endOffset": 711}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al. [2013], Bianchini and Scarselli [2014]. This is often used to suggest the benefits of deep networks over shallow networks. Other work examines the complexity of a single function computed by a deep network, rather than the complexity of a set of functions. The number of linear regions was introduced as a measure of expressivity in Pascanu et al. [2013]. In Montufar et al.", "startOffset": 51, "endOffset": 1027}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al. [2013], Bianchini and Scarselli [2014]. This is often used to suggest the benefits of deep networks over shallow networks. Other work examines the complexity of a single function computed by a deep network, rather than the complexity of a set of functions. The number of linear regions was introduced as a measure of expressivity in Pascanu et al. [2013]. In Montufar et al. [2014], it was shown that a specific network achieves an exponential number of linear regions with depth.", "startOffset": 51, "endOffset": 1054}, {"referenceID": 12, "context": "[1989], Cybenko [1989], their VC dimension studied Sontag [1998], Bartlett and Maass [2003], Bartlett et al. [1998], and connections between boolean and threshold networks to ReLU networks developed in Maass et al. [1994], Pan and Srikumar [2015]. VC dimension provides an upper bound on one of the properties we measure, and is further discussed in Appendix D.1. Looking at achievable functions also provides a method to compare different neural network architectures. A typical approach to this constructs classes of functions from one network architecture that another is incapable of approximating efficiently Eldan and Shamir [2015], Telgarsky [2015], Martens et al. [2013], Bianchini and Scarselli [2014]. This is often used to suggest the benefits of deep networks over shallow networks. Other work examines the complexity of a single function computed by a deep network, rather than the complexity of a set of functions. The number of linear regions was introduced as a measure of expressivity in Pascanu et al. [2013]. In Montufar et al. [2014], it was shown that a specific network achieves an exponential number of linear regions with depth. In Section 2.3 we generalize linear regions to network activation patterns, and develop a tight upper bound on the number of achievable activation patterns which the construction in Montufar et al. [2014] saturates.", "startOffset": 51, "endOffset": 1358}, {"referenceID": 26, "context": "The expressivity of these random networks is largely unexplored, though a connection between random networks and compositional kernels is developed in Daniely et al. [2016]. The class of random networks enables tractable theoretical and experimental analysis of interrelated aspects of expressivity: trajectory length, neuron transitions, network activation patterns, and number of dichotomies (number of labellings on inputs).", "startOffset": 151, "endOffset": 173}, {"referenceID": 0, "context": "convolutional networks LeCun et al. [1998], and explored theoretically in Cohen et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 0, "context": "convolutional networks LeCun et al. [1998], and explored theoretically in Cohen et al. [2015]. In particular the depth of the neural network is often seen to take advantage of the hierarchical and compositional structure of features in datasets.", "startOffset": 23, "endOffset": 94}, {"referenceID": 27, "context": "In a companion paper Poole et al. [2016] we explore closely related questions by using mean field theory to explore the propagation of curvature through deep networks.", "startOffset": 21, "endOffset": 41}, {"referenceID": 28, "context": "Our results mostly examine the cases where \u03c6 is a hard-tanh Collobert and Bengio [2004] or ReLU nonlinearity.", "startOffset": 60, "endOffset": 88}, {"referenceID": 27, "context": "This phase transition behavior is explored further in our companion paper Poole et al. [2016]. The proof can be found in the Appendix.", "startOffset": 74, "endOffset": 94}, {"referenceID": 29, "context": "The question of the number of regions for a specific hyperplane arrangement is well studied Stanley [2011], with", "startOffset": 92, "endOffset": 107}, {"referenceID": 25, "context": "In Montufar et al. [2014], it is observed that a (trivial) upper bound is given by 2 (all possible subsets of all possible neurons) and a construction of linear regions increasing exponentially with depth is given.", "startOffset": 3, "endOffset": 26}, {"referenceID": 25, "context": "In Montufar et al. [2014], it is observed that a (trivial) upper bound is given by 2 (all possible subsets of all possible neurons) and a construction of linear regions increasing exponentially with depth is given. This upper bound is exponentially looser than the bound given through regions of a hyperplane arrangement, which is shown asymptotically tight with the construction in Montufar et al. [2014].", "startOffset": 3, "endOffset": 406}, {"referenceID": 27, "context": "This is further supported by an exponential decrease in autocorrelation length in function space, which we derive in the Supplemental Material of our companion paper Poole et al. [2016]. Our results further suggest the following conjectures: Conjecture 2.", "startOffset": 166, "endOffset": 186}, {"referenceID": 27, "context": ") In our companion paper Poole et al. [2016], we explore the effect of width and total depth when training only the final layer in the network.", "startOffset": 25, "endOffset": 45}], "year": 2016, "abstractText": "We study the expressivity of deep neural networks with random weights. We provide several results, both theoretical and experimental, precisely characterizing their functional properties in terms of the depth and width of the network. In doing so, we illustrate inherent connections between the length of a latent trajectory, local neuron transitions, and network activation patterns. The latter, a notion defined in this paper, is further studied using properties of hyperplane arrangements, which also help precisely characterize the effect of the neural network on the input space. We further show dualities between changes to the latent state and changes to the network weights, and between the number of achievable activation patterns and the number of achievable labellings over input data. We see that the depth of the network affects all of these quantities exponentially, while the width appears at most as a base. These results also suggest that the remaining depth of a neural network is an important determinant of expressivity, supported by experiments on MNIST and CIFAR-10.", "creator": "LaTeX with hyperref package"}}}