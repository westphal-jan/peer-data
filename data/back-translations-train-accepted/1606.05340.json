{"id": "1606.05340", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Exponential expressivity in deep neural networks through transient chaos", "abstract": "We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in generic, deep neural networks with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted to the analysis of single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions.", "histories": [["v1", "Thu, 16 Jun 2016 19:59:57 GMT  (5829kb,D)", "http://arxiv.org/abs/1606.05340v1", null], ["v2", "Fri, 17 Jun 2016 18:13:20 GMT  (6482kb,D)", "http://arxiv.org/abs/1606.05340v2", "Fixed equation references"]], "reviews": [], "SUBJECTS": "stat.ML cond-mat.dis-nn cs.LG", "authors": ["ben poole", "subhaneil lahiri", "maithreyi raghu", "jascha sohl-dickstein", "surya ganguli"], "accepted": true, "id": "1606.05340"}, "pdf": {"name": "1606.05340.pdf", "metadata": {"source": "CRF", "title": "Exponential expressivity in deep neural networks through transient chaos", "authors": ["Ben Poole", "Surya Ganguli"], "emails": ["poole@cs.stanford.edu", "sulahiri@stanford.edu", "maithrar@gmail.com", "jaschasd@google.com", "sganguli@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Deep-feed networks, with several hidden layers, have performed remarkably in many areas. A key factor behind their success is their high expressivity. This informal term has manifested itself primarily in two forms of intuition."}, {"heading": "2 A mean field theory of deep nonlinear signal propagation", "text": "(So let's look at a deep network with random meshes in which each of the synaptic weights is drawn.) This network is essentially a Nl \u00b7 Nl \u00b7 Nl \u2212 1 weight matrix. (D \u00b7 Nl \u00b7 1 weight matrix). (D \u00b7 Nl \u00b7 1 weight matrix. (D), where there is a vector of distortions, is the pattern of input of neurons at layer l, namely a single neuron calar nonlinearity that acts component-wise to transform the activities xl. We want to understand the nature of typical computable functions through such networks as a result of their depth. (We therefore examine random networks in which each of the synaptic weights is drawn.).We want to understand the type of typical functions computable through such networks."}, {"heading": "3 Transient chaos in deep networks", "text": "Let us now consider the layered propagation of two inputs x0,1 and x0,2. The geometry of these two inputs as they propagate through the network is determined by the 2 x 2 matrix of internal products: qlab = 1NlNl \u2211 i = 1hli (x 0, a) hli (x 0, b) a, b) {1, 2}. (4) The dynamics of the two diagonal terms are predicted theoretically by the length map in (13). We derive (see SM) a nearby map C which predicts the layered dynamics of ql12: ql12 = C (cl \u2212 112, q \u2212 l \u2212 1) is a quickly predicted relationship between the two diagonal terms. (see SM) a correlation map which predicts the layered dynamics of ql12: ql12 = C (cl \u2212 1, q \u2212 1)."}, {"heading": "4 The propagation of manifold geometry through deep networks", "text": "Now let us consider a manifold prediction of equations (13) and 15 (15) is in a sense a quantitative simulation. (This is an intrinsic scalar coordinate at the manifold level.) This manifold approach expands to a new manifold level. (This is a typical geometry of the manifold in the l'th layer is summarized by ql (2), which is defined for all other phenomena 1 and 2. (14) with the choice x0 (1) and x0 (0), b = x0 (2). The theory for the propagation of points applies to all pairs of points at the manifold level, so that we intuitively expect that in the chaotic phase of a sigmoid network, the multiplicity should in a sense become de-correlatte, and more complex, while in the ordered phase the manifold prediction of central equations should be contrasted by a point."}, {"heading": "5 Shallow networks cannot achieve exponential expressivity", "text": "Let us consider a flat network with a hidden layer x1, an input layer x0, with x1 = \u03c6 (W1x0) + b1 and a linear read layer. How complex can the hidden representation be in relation to its width N1, in relation to the above results in terms of depth? We prove a general upper limit for LE (see SM): Theory 1. Let us assume that \u03c6 (h) is monotonous not decreasing with a limited dynamic range R, i.e., for each choice of W1 and b1, the euclidean length of x1 (\u03b8) = R. Further, let us assume that x0 (\u03b8) is a curve in the entrance space, so that no 1D projection of phenomena changes more than s times over the range of \u03b8. Then, for each choice of W1 and b1, the euclidean length of x1 (\u03b8) = minimal; the euclidean length of x1 (\u03b8) satisfies LE N1 (+ 1)."}, {"heading": "6 Classification boundaries acquire exponential local curvature with depth", "text": "So far, we have focused on how simple manifolds in the entrance space can have both exponential Euclidean and Grassman lengths with depths, exponentially correlating and filling a hidden representation space. Another natural question is how the complexity of a decision boundary increases when propagated back to the input level. Consider a linear classifier y = sgn (\u03b2 \u00b7 xD \u2212 \u03b20) that acts on the last layer. In this layer, the N \u2212 1 dimensional decision boundary is the hyperplane \u03b2 \u00b7 xD \u2212 \u03b20 = 0. However, in the input layer x0, the decision boundary is a curved N \u2212 1 dimensional manifoldM that results as a solution to the nonlinear equation G (x0)."}, {"heading": "7 Discussion", "text": "Basically, neural networks calculate nonlinear cards between high-dimensional spaces, for example, from RN1 \u2192 RND, and it is unclear what is the most appropriate math for understanding such scary card spaces. Previous work has addressed this problem by limiting the nature of the nonlinearity involved (e.g. piecemeal linear, sum-productive, or Pfaffsche) and thus limiting the space of the cards to those specific theoretical analysis methods (combinatorics, polynomic relationships, or topological invariants).We have begun a preliminary investigation of the expressivity of such deep functions based on Riemannian geometry and dynamic center theory. We show that networks in a chaotic phase exhibit compact input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-in"}, {"heading": "A Derivation of a transient dynamical mean field theory for deep networks", "text": "We study a deep network with D-layers of weights W1,.., WD and D-1 layer of neural activity vectors 1hl,., xD, with Nl neurons in each layer l, so that xl, RNl and Wl form an Nl-1 weight matrix., D, (10), where there is a vector of distortion, is the pattern of inputs to neurons in layer l, and the number of neurons in layer l, and which is non-linear, acting component by component to transformed inputs to activities xl. Synaptic weights Wlij are i.e. drawn by a zero medium Gaussian with variance 2 w / Nl \u2212 1, while the biases i.i."}, {"heading": "B Derivation of evolution equations for Riemannian curvature", "text": "Here we derive recursion relations for Riemannian curvature quantities. B.1 Curvature and length in relation to internal productsConsider a translation invariant manifold or 1D curve h (\u03b8), which is based on any constant radius sphere, so that q (\u03b81, \u03b82) = Q (\u03b81) = h (\u03b81) \u00b7 h (\u03b82), (20) with Q (0) = Nq. With large N, the internal product structure of translation invariant manifolds remains approximately translation invariant as it spreads through the network. Therefore, with large N, we can derive internal products of the derivatives of h in relation to Q. For example, the Euclidean metric gE is given as BygE."}, {"heading": "C Upper bounds on the complexity of shallow neural representations", "text": "Consider a flat network with a hidden layer x1 and an input layer x0, so that x1 = constant (W1x0) + b is assumed. The network can calculate functions by linear reading of the hidden layer. In particular, we are interested in how the length and curvature of an input manual x0 (\u03b8) changes as it propagates to become x1 (\u03b8) in the hidden layer. We would like to change the maximum achievable length and curvature across all possible selections of W1 and b.C.1 upper limit of an input manual x. Here, we derive such an upper limit to the euclidean indicator. Length for a very general class of nonlinearity."}, {"heading": "D Simulation details", "text": "All neural network simulations were implemented in Keras and Theano. For all simulations (except Figure 5C) we used inputs and hidden layers with a width of 1,000 and tanh activations. We found that our results were largely insensitive to width, but by using larger widths the fluctuations in the averaged quantities were reduced. Simulation error bars are all standard deviations, whereby the variance across the different inputs was calculated, h1 (\u03b8). If not mentioned, the weights in the network are initialized in the chaotic regime with \u03c3b = 0.3, \u03c3w = 4.0. Calculation of all standard deviations (\u03b8) requires the calculation of the velocity and acceleration vectors corresponding to the first and second derivatives of the neural network hl. In relation to this component, an error is initialized with \u03c3b = 0.3, \u03c3w = 4.0."}, {"heading": "E Additional visualization of hidden actions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "F A view from the function space perspective", "text": "We have shown above that for a fixed set of weights and distortions in the chaotic order, the only dependence in this family is fixed: the internal representation hl (x0) at great depth l, rapidly de-correlated by itself as input x0 changes (see e.g. Figure 3B in the main work). Here, we ask a double question: How does a deep network in a functional space move over this multiplicity than the weights in a single layer? For example, let us consider a random parameter family of deep networks parameterized by any parameters [\u2212 1, 1]. In this family, we assume that the bias vectors in each layer are chosen as i.i.d. random Gaussian vectors with zero mean and variance in each layer, regardless of size. Furthermore, we assume that the weight matrix Wl has elements drawn i.i.e.. of zero mean gaussian with variance in each layer, independent of all layers."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Deep knowledge tracing", "author": ["Chris Piech", "Jonathan Bassen", "Jonathan Huang", "Surya Ganguli", "Mehran Sahami", "Leonidas J Guibas", "Jascha Sohl-Dickstein"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pierre Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Untangling invariant object recognition", "author": ["James J DiCarlo", "David D Cox"], "venue": "Trends in cognitive sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Shallow vs. deep sum-product networks", "author": ["Olivier Delalleau", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1512.03965,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Representation benefits of deep feedforward networks", "author": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1509.08101,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "On the representational efficiency of restricted boltzmann machines", "author": ["James Martens", "Arkadev Chattopadhya", "Toni Pitassi", "Richard Zemel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "author": ["Monica Bianchini", "Franco Scarselli"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Learning real and boolean functions: When is deep better than shallow", "author": ["Hrushikesh Mhaskar", "Qianli Liao", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1603.00988,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Riemannian manifolds: an introduction to curvature, volume 176", "author": ["John M Lee"], "venue": "Springer Science & Business Media,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Chaos in random neural networks", "author": ["Haim Sompolinsky", "A Crisanti", "HJ Sommers"], "venue": "Physical Review Letters,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1988}], "referenceMentions": [{"referenceID": 0, "context": "Deep feedforward neural networks, with multiple hidden layers, have achieved remarkable performance across many domains [1\u20134].", "startOffset": 120, "endOffset": 125}, {"referenceID": 1, "context": "Deep feedforward neural networks, with multiple hidden layers, have achieved remarkable performance across many domains [1\u20134].", "startOffset": 120, "endOffset": 125}, {"referenceID": 2, "context": "Deep feedforward neural networks, with multiple hidden layers, have achieved remarkable performance across many domains [1\u20134].", "startOffset": 120, "endOffset": 125}, {"referenceID": 3, "context": "Deep feedforward neural networks, with multiple hidden layers, have achieved remarkable performance across many domains [1\u20134].", "startOffset": 120, "endOffset": 125}, {"referenceID": 4, "context": "The second piece of intuition, which has captured the imagination of machine learning [5] and neuroscience [6] alike, is that deep neural networks can disentangle highly curved manifolds in input space into flattened manifolds in hidden space, to aid the performance of simple linear readouts.", "startOffset": 86, "endOffset": 89}, {"referenceID": 5, "context": "The second piece of intuition, which has captured the imagination of machine learning [5] and neuroscience [6] alike, is that deep neural networks can disentangle highly curved manifolds in input space into flattened manifolds in hidden space, to aid the performance of simple linear readouts.", "startOffset": 107, "endOffset": 110}, {"referenceID": 6, "context": "For the first intuition, seminal works have exhibited examples of particular functions that can be computed with a polynomial number of neurons (in the input dimension) in a deep network but require an exponential number of neurons in a shallow network [7\u201311].", "startOffset": 253, "endOffset": 259}, {"referenceID": 7, "context": "For the first intuition, seminal works have exhibited examples of particular functions that can be computed with a polynomial number of neurons (in the input dimension) in a deep network but require an exponential number of neurons in a shallow network [7\u201311].", "startOffset": 253, "endOffset": 259}, {"referenceID": 8, "context": "For the first intuition, seminal works have exhibited examples of particular functions that can be computed with a polynomial number of neurons (in the input dimension) in a deep network but require an exponential number of neurons in a shallow network [7\u201311].", "startOffset": 253, "endOffset": 259}, {"referenceID": 9, "context": "For the first intuition, seminal works have exhibited examples of particular functions that can be computed with a polynomial number of neurons (in the input dimension) in a deep network but require an exponential number of neurons in a shallow network [7\u201311].", "startOffset": 253, "endOffset": 259}, {"referenceID": 10, "context": "For the first intuition, seminal works have exhibited examples of particular functions that can be computed with a polynomial number of neurons (in the input dimension) in a deep network but require an exponential number of neurons in a shallow network [7\u201311].", "startOffset": 253, "endOffset": 259}, {"referenceID": 6, "context": "For example [7] focused on ReLu nonlinearities and number of linear regions as a complexity measure, while [8] focused on sum-product networks and the number of monomials as complexity measure, and [12] focused on Pfaffian nonlinearities and topological measures of complexity, like the sum of Betti numbers of a decision boundary.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "For example [7] focused on ReLu nonlinearities and number of linear regions as a complexity measure, while [8] focused on sum-product networks and the number of monomials as complexity measure, and [12] focused on Pfaffian nonlinearities and topological measures of complexity, like the sum of Betti numbers of a decision boundary.", "startOffset": 107, "endOffset": 110}, {"referenceID": 11, "context": "For example [7] focused on ReLu nonlinearities and number of linear regions as a complexity measure, while [8] focused on sum-product networks and the number of monomials as complexity measure, and [12] focused on Pfaffian nonlinearities and topological measures of complexity, like the sum of Betti numbers of a decision boundary.", "startOffset": 198, "endOffset": 202}, {"referenceID": 12, "context": "see [13] for an interesting analysis of a general class of compositional functions.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "The limits of prior theoretical techniques raise another central question: is there a unifying theoretical framework for deep neural expressivity that is simultaneously applicable to arbitrary nonlinearities, generic networks, and a natural, general measure of functional complexity? Here we attack both central problems of deep neural expressivity by combining a very different set of tools, namely Riemannian geometry [14] and dynamical mean field theory [15].", "startOffset": 420, "endOffset": 424}, {"referenceID": 14, "context": "The limits of prior theoretical techniques raise another central question: is there a unifying theoretical framework for deep neural expressivity that is simultaneously applicable to arbitrary nonlinearities, generic networks, and a natural, general measure of functional complexity? Here we attack both central problems of deep neural expressivity by combining a very different set of tools, namely Riemannian geometry [14] and dynamical mean field theory [15].", "startOffset": 457, "endOffset": 461}, {"referenceID": 13, "context": "To quantitatively understand the layer-wise growth of complexity of this manifold, it is useful to turn to concepts in Riemannian geometry [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "For any parameterization, an explicit expression for \u03ba(\u03b8) is given by \u03ba(\u03b8) = (v \u00b7v)\u22123/2 \u221a (v \u00b7 v)(a \u00b7 a)\u2212 (v \u00b7 a)2 [14].", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "Furthermore, the Gauss metric is related to the extrinsic curvature and the Euclidean metric via the relation g(\u03b8) = \u03ba(\u03b8)g(\u03b8) [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "These principal curvatures arise as the eigenvalues of a normalized Hessian operator projected onto the tangent plane Tx\u2217M: H = ||~ \u2207G||\u22121 2 P \u2202 G \u2202x\u2202xT P, where P = I\u2212 \u2207\u0302G\u2207\u0302G is the projection operator onto Tx\u2217M and \u2207\u0302G is the unit normal vector [14].", "startOffset": 247, "endOffset": 251}], "year": 2016, "abstractText": "We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in generic, deep neural networks with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted to the analysis of single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions.", "creator": "LaTeX with hyperref package"}}}