{"id": "1606.05378", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Simpler Context-Dependent Logical Forms via Model Projections", "abstract": "We consider the task of learning a context-dependent mapping from utterances to denotations. With only denotations at training time, we must search over a combinatorially large space of logical forms, which is even larger with context-dependent utterances. To cope with this challenge, we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms. Though less expressive, we find that these simpler models are much faster and can be surprisingly effective. Moreover, they can be used to bootstrap the full model. Finally, we collected three new context-dependent semantic parsing datasets, and develop a new left-to-right parser.", "histories": [["v1", "Thu, 16 Jun 2016 21:57:11 GMT  (5085kb,D)", "http://arxiv.org/abs/1606.05378v1", "10 pages, ACL 2016"]], "COMMENTS": "10 pages, ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["reginald long", "panupong pasupat", "percy liang"], "accepted": true, "id": "1606.05378"}, "pdf": {"name": "1606.05378.pdf", "metadata": {"source": "CRF", "title": "Simpler Context-Dependent Logical Forms via Model Projections", "authors": ["Reginald Long", "Panupong Pasupat", "Percy Liang"], "emails": ["reggylong@cs.stanford.edu", "ppasupat@cs.stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules that they have shown in recent years, and they will be able to play by the rules that they have shown in the past."}, {"heading": "2 Task", "text": "In this section, we formalize the task and describe the new rows we have created for the task."}, {"heading": "2.1 Setup", "text": "First, we define the context-dependent semantic parsing task. We define w0 as the initial state of the world, which consists of a series of entities (cups in ALCHEMY) and properties (place, color (s) and filled amount). Text x is a sequence of expressions x1,..., xL. For each utterance xi (e.g. \"mix\") we have a latent logical form zi (e.g. mix (args [1] [2]). We define the context ci = (w0, z1: i \u2212 1) to describe the initial state of the world w0 and the history of past logical forms z1: i \u2212 1. Each logical form zi is executed on the context ci to create the next state: wi = Exec (ci, zi) for each i = 1, L. Overloading the notation, we write wL = Exec (w0, z), where z = (z1, z.L) is wapping, the learning problem is wapping (L), wapping (a series of wapping, wapping, etc.)."}, {"heading": "2.2 Datasets", "text": "We have created three new contextual datasets, ALCHEMY, SCENE and TANGAMS (see Table 1 for a summary), which aim to capture a range of contextual linguistic phenomena such as \"mix\" in the US. \"It,\" the study says, \"is another step in the right direction.\""}, {"heading": "3 Model", "text": "We will now describe Model A, our complete context-dependent semantic parsing model. First, let Z show the number of candidates logical forms (e.g.: 1 color (green), color (red)), etc. Each logical form consists of a top-level action with arguments that are either primitive values (green, 3, etc.), or composed by selection and superlative operations. [1] A notable feature of logical forms is context dependence: for example, certain contexts (w0, z1: 4), predicate actions [2] refer to the action of z2 and args [2] refer to the first argument of z2.1We use the term anchored logical forms (a.k.a. derivatives) to refer to logical forms that we refer to with alignments between sub-logical forms of zi and spans of utterance xi."}, {"heading": "4 Left-to-right parsing", "text": "3. Like a shift-reduce parser, we proceed from left to right, but each shift operation advances an entire utterance rather than a word. We then sit on the utterance for a while and perform a sequence of build operations that either combine two logical shapes on the stack (like the reduction operation) or generate new logical shapes, similar to what is done in the flowing parser of Pasupat and Liang (2015). Our parser has two desirable properties: First, the left-to-right approach allows us to build and evaluate logical shapes that depend on the world statewi-1, which is a function of the previous logical shapes. Note that wi \u2212 1 is a random variable in our setting while fixed in Zettlemoyer and Collins (2009). Second, the build operation allows us the flexibility to handle elliptical shapes (e.g. \"aphorical mix\" and \"aphorical\")."}, {"heading": "5 Model Projections", "text": "Model A is ambitious as it tries to learn from the ground up how each word is aligned to some part of the logical form. For example, when Model A analyzes \"Mix it,\" one derivative will correctly mix \"Mix,\" but others will align \"Mix\" to args [1], \"Mix\" to pos (2), and so on (Figure 2). Since we are not assuming a seed lexicon that can mix \"Mix,\" the amount of logical forms is exponentially large. For example, only the first sentence of Figure 1 would generate 1,216,140 intermediate logical formulas. How can we reduce the search space when the space of logical forms is much smaller than the space of anchored logical forms. Even if both grow exponentially, we can deal directly with logical forms that generate us without combinatorial selection of alignments."}, {"heading": "6 Experiments", "text": "Our experiments aim to explore the mathematical compromise in the transition from model A to model B to model C. We would expect that model A will suffer the most from the computational limitation of a finite beam size, but with an infinite beam, model A should perform better. We evaluate all models for accuracy, the fraction of the examples that a model correctly predicts. A predicted logical form z is considered correct for an example (w0, x, wL) if the predicted logical form z performs wL to the correct end-world state. We also measure oracle accuracy, which is a fraction of the examples in which at least one z executes on the beam to wL. All experiments train for 6 iterations using AdaGrad (Duchi et al., 2010) and L1 regulation with a coefficient of 0.001."}, {"heading": "6.1 Real data experiments", "text": "For the first two iterations, models B and C train only on the first utterance of each example (L = 1). In the remaining iterations, the models train on the basis of two utterances. However, we compare models B and C on the three real datasets for L = 3 and L = 5 utterances (model A was too expensive to use). Table 4 shows that on 5 utterances, the flatter model C achieves an average accuracy of 20% higher than the more compositional model B. Similarly, the average accuracy of the oracle is 39% higher, suggesting that (i) the correct logical shape of the model often exceeds the beam of model B due to a larger search space, and (ii) the utterance of the model GRAN.C for the search for model S is always. \""}, {"heading": "6.2 Artificial data experiments", "text": "In order to evaluate Model A practically, we constructed an artificial dataset. The worlds are created using the procedure described in Section 2.2. We use a simple template to generate expressions (e.g. \"Drain 1 from the 2 green beaker\"). To reduce the search space for Model A, we only allow actions (e.g. Drain) to align with verbs and property values (e.g. Green) to align with adjectives. Using these linguistic constraints provides a slightly optimistic assessment of the performance of Model A. We train on a dataset of 500 training examples and evaluate 500 test examples. We repeat this procedure for different beam sizes, from 40 to 260. The model uses only features (F1) through (F3). The accuracy of models under infinite beam. Since Model A is more expressive, we expect it to be more powerful when we do not have compatible sizes."}, {"heading": "6.3 Error Analysis", "text": "We randomly categorized 20 false predictions based on 3 examples of utterances from each of the three real data sets for Model B and Model C. We categorized each prediction error into one of the following categories: (i) logical shapes falling off the bar; (ii) the choice of wrong action (e.g. mapping \"drain\" to pour); (iii) the choice of wrong argument due to a misunderstanding of the description (e.g. mapping \"third cup\" to item (1)); (iv) the choice of wrong action or wrong argument due to a misunderstanding of the context (see Figure 8); (v) the noise in the data set. Table 5 shows a fraction of each error category."}, {"heading": "7 Related Work and Discussion", "text": "Context-dependent semantic parsing. Utterances can depend on either linguistic context or world-state context. Zettlemoyer and Collins (2009) developed a model that processes references to previous logical forms; Artzi and Zettlemoyer (2013) developed a model that handles references to the current state of the world. However, our system considers both types of context, dealing with linguistic phenomena such as ellipsis and anaphora, which refer to both previous states of the world and logical forms. Traditional semantic parsers generate logical forms by aligning each part of the logical form to the utterance (Cell and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). Generally, such systems rely on a lexicon that can be developed and extracted by hand (Cai and Yates, 2013)."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their constructive feedback. The third author is supported by a Microsoft Research Faculty Fellowship."}], "references": [{"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Y. Artzi", "L. Zettlemoyer."], "venue": "Transactions of the Association for Computational Linguistics (TACL), 1:49\u201362.", "citeRegEx": "Artzi and Zettlemoyer.,? 2013", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2013}, {"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Question answering with subgraph embeddings", "author": ["A. Bordes", "S. Chopra", "J. Weston."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Can recursive neural tensor networks learn logical reasoning", "author": ["S.R. Bowman", "C. Potts", "C.D. Manning"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "Bowman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2014}, {"title": "Large-scale semantic parsing via schema matching and lexicon extension", "author": ["Q. Cai", "A. Yates."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Cai and Yates.,? 2013", "shortCiteRegEx": "Cai and Yates.", "year": 2013}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["D.L. Chen", "R.J. Mooney."], "venue": "Association for the Advancement of Artificial Intelligence (AAAI), pages 859\u2013865.", "citeRegEx": "Chen and Mooney.,? 2011", "shortCiteRegEx": "Chen and Mooney.", "year": 2011}, {"title": "Fast online lexicon learning for grounded language acquisition", "author": ["D.L. Chen."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Chen.,? 2012", "shortCiteRegEx": "Chen.", "year": 2012}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["J. Clarke", "D. Goldwasser", "M. Chang", "D. Roth."], "venue": "Computational Natural Language Learning (CoNLL), pages 18\u201327.", "citeRegEx": "Clarke et al\\.,? 2010", "shortCiteRegEx": "Clarke et al\\.", "year": 2010}, {"title": "Expanding the scope of the ATIS task: The ATIS-3 corpus", "author": ["D.A. Dahl", "M. Bates", "M. Brown", "W. Fisher", "K. Hunicke-Smith", "D. Pallett", "C. Pao", "A. Rudnicky", "E. Shriberg."], "venue": "Workshop on Human Language Technology, pages 43\u201348.", "citeRegEx": "Dahl et al\\.,? 1994", "shortCiteRegEx": "Dahl et al\\.", "year": 1994}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer."], "venue": "Conference on Learning Theory (COLT).", "citeRegEx": "Duchi et al\\.,? 2010", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Traversing knowledge graphs in vector space", "author": ["K. Guu", "J. Miller", "P. Liang."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Guu et al\\.,? 2015", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Inducing probabilistic CCG grammars from logical form with higher-order unification", "author": ["T. Kwiatkowski", "L. Zettlemoyer", "S. Goldwater", "M. Steedman."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1223\u20131233.", "citeRegEx": "Kwiatkowski et al\\.,? 2010", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2010}, {"title": "Lexical generalization in CCG grammar induction for semantic parsing", "author": ["T. Kwiatkowski", "L. Zettlemoyer", "S. Goldwater", "M. Steedman."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1512\u20131523.", "citeRegEx": "Kwiatkowski et al\\.,? 2011", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2011}, {"title": "Learning semantic correspondences with less supervision", "author": ["P. Liang", "M.I. Jordan", "D. Klein."], "venue": "Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 91\u201399.", "citeRegEx": "Liang et al\\.,? 2009", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "Lambda dependency-based compositional semantics", "author": ["P. Liang."], "venue": "arXiv.", "citeRegEx": "Liang.,? 2013", "shortCiteRegEx": "Liang.", "year": 2013}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["A. Neelakantan", "Q.V. Le", "I. Sutskever."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Neelakantan et al\\.,? 2016", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney."], "venue": "Computational Linguistics, 29:19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Zero-shot entity extraction from web pages", "author": ["P. Pasupat", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Pasupat and Liang.,? 2014", "shortCiteRegEx": "Pasupat and Liang.", "year": 2014}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["P. Pasupat", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Pasupat and Liang.,? 2015", "shortCiteRegEx": "Pasupat and Liang.", "year": 2015}, {"title": "Neural programmerinterpreters", "author": ["S. Reed", "N. de Freitas."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Reed and Freitas.,? 2016", "shortCiteRegEx": "Reed and Freitas.", "year": 2016}, {"title": "Learning with relaxed supervision", "author": ["J. Steinhardt", "P. Liang."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Steinhardt and Liang.,? 2015", "shortCiteRegEx": "Steinhardt and Liang.", "year": 2015}, {"title": "A new corpus and imitation learning framework for context-dependent semantic parsing", "author": ["A. Vlachos", "S. Clark."], "venue": "Transactions of the Association for Computational Linguistics (TACL), 2:547\u2013559.", "citeRegEx": "Vlachos and Clark.,? 2014", "shortCiteRegEx": "Vlachos and Clark.", "year": 2014}, {"title": "Building a semantic parser overnight", "author": ["Y. Wang", "J. Berant", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov."], "venue": "arXiv.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Y.W. Wong", "R.J. Mooney."], "venue": "Association for Computational Linguistics (ACL), pages 960\u2013967.", "citeRegEx": "Wong and Mooney.,? 2007", "shortCiteRegEx": "Wong and Mooney.", "year": 2007}, {"title": "Freebase QA: Information extraction or semantic parsing", "author": ["X. Yao", "J. Berant", "B. Van-Durme."], "venue": "Workshop on Semantic parsing.", "citeRegEx": "Yao et al\\.,? 2014", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["M. Zelle", "R.J. Mooney."], "venue": "Association for the Advancement of Artificial Intelligence (AAAI), pages 1050\u20131055.", "citeRegEx": "Zelle and Mooney.,? 1996", "shortCiteRegEx": "Zelle and Mooney.", "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["L.S. Zettlemoyer", "M. Collins."], "venue": "Uncertainty in Artificial Intelligence (UAI), pages 658\u2013 666.", "citeRegEx": "Zettlemoyer and Collins.,? 2005", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}, {"title": "Online learning of relaxed CCG grammars for parsing to logical form", "author": ["L.S. Zettlemoyer", "M. Collins."], "venue": "Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 678\u2013687.", "citeRegEx": "Zettlemoyer and Collins.,? 2007", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2007}, {"title": "Learning context-dependent mappings from sentences to logical form", "author": ["L.S. Zettlemoyer", "M. Collins."], "venue": "Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP).", "citeRegEx": "Zettlemoyer and Collins.,? 2009", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2009}], "referenceMentions": [{"referenceID": 26, "context": "We start with the classic paradigm of training semantic parsers that map utterances to logical forms, which are executed to produce the denotation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010).", "startOffset": 147, "endOffset": 282}, {"referenceID": 27, "context": "We start with the classic paradigm of training semantic parsers that map utterances to logical forms, which are executed to produce the denotation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010).", "startOffset": 147, "endOffset": 282}, {"referenceID": 24, "context": "We start with the classic paradigm of training semantic parsers that map utterances to logical forms, which are executed to produce the denotation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010).", "startOffset": 147, "endOffset": 282}, {"referenceID": 29, "context": "We start with the classic paradigm of training semantic parsers that map utterances to logical forms, which are executed to produce the denotation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010).", "startOffset": 147, "endOffset": 282}, {"referenceID": 11, "context": "We start with the classic paradigm of training semantic parsers that map utterances to logical forms, which are executed to produce the denotation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010).", "startOffset": 147, "endOffset": 282}, {"referenceID": 7, "context": "More recent work learns directly from denotations (Clarke et al., 2010; Liang, 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but in this setting, a constant struggle is to contain the exponential explosion of possible logical forms.", "startOffset": 50, "endOffset": 134}, {"referenceID": 14, "context": "More recent work learns directly from denotations (Clarke et al., 2010; Liang, 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but in this setting, a constant struggle is to contain the exponential explosion of possible logical forms.", "startOffset": 50, "endOffset": 134}, {"referenceID": 1, "context": "More recent work learns directly from denotations (Clarke et al., 2010; Liang, 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but in this setting, a constant struggle is to contain the exponential explosion of possible logical forms.", "startOffset": 50, "endOffset": 134}, {"referenceID": 0, "context": "More recent work learns directly from denotations (Clarke et al., 2010; Liang, 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but in this setting, a constant struggle is to contain the exponential explosion of possible logical forms.", "startOffset": 50, "endOffset": 134}, {"referenceID": 27, "context": ", Zettlemoyer and Collins (2005)).", "startOffset": 2, "endOffset": 33}, {"referenceID": 14, "context": "This \u201cfloating\u201d approach was used in Pasupat and Liang (2015) and Wang et al.", "startOffset": 49, "endOffset": 62}, {"referenceID": 14, "context": "This \u201cfloating\u201d approach was used in Pasupat and Liang (2015) and Wang et al. (2015).", "startOffset": 49, "endOffset": 85}, {"referenceID": 24, "context": "This model is in the spirit of Yao et al. (2014) and Bordes et al.", "startOffset": 31, "endOffset": 49}, {"referenceID": 2, "context": "(2014) and Bordes et al. (2014), who directly predicted concrete paths in a knowledge graph for question answering.", "startOffset": 11, "endOffset": 32}, {"referenceID": 8, "context": "In the context-dependent ATIS dataset (Dahl et al., 1994) used by Zettlemoyer and Collins (2009), logical forms of utterances depend on previous logical forms, though there is no world state and the linguistic phenomena is limited to nominal references.", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": "In the map navigation dataset (Chen and Mooney, 2011), used by Artzi and Zettlemoyer (2013), utterances only reference the current world state.", "startOffset": 30, "endOffset": 53}, {"referenceID": 5, "context": "In the context-dependent ATIS dataset (Dahl et al., 1994) used by Zettlemoyer and Collins (2009), logical forms of utterances depend on previous logical forms, though there is no world state and the linguistic phenomena is limited to nominal references.", "startOffset": 39, "endOffset": 97}, {"referenceID": 0, "context": "In the map navigation dataset (Chen and Mooney, 2011), used by Artzi and Zettlemoyer (2013), utterances only reference the current world state.", "startOffset": 63, "endOffset": 92}, {"referenceID": 0, "context": "In the map navigation dataset (Chen and Mooney, 2011), used by Artzi and Zettlemoyer (2013), utterances only reference the current world state. Vlachos and Clark (2014) released a corpus of annotated dialogues, which has interesting linguistic contextdependence, but there is no world state.", "startOffset": 63, "endOffset": 169}, {"referenceID": 27, "context": "These special predicates play the role of references in Zettlemoyer and Collins (2009). They perform contextindependent parsing and resolve references, whereas we resolve them jointly while parsing.", "startOffset": 56, "endOffset": 87}, {"referenceID": 14, "context": "We then sit on the utterance for a while, performing a sequence of build operations, which either combine two logical forms on the stack (like the reduce operation) or generate fresh logical forms, similar to what is done in the floating parser of Pasupat and Liang (2015).", "startOffset": 260, "endOffset": 273}, {"referenceID": 27, "context": "Note that wi\u22121 is a random variable in our setting, whereas it is fixed in Zettlemoyer and Collins (2009). Second, the build operation allows us the flexibility to handle ellipsis (e.", "startOffset": 75, "endOffset": 106}, {"referenceID": 14, "context": "This is similar to the model of Pasupat and Liang (2015). Note that most of the derivation conditions (F2)\u2013(F7) already depend on properties of the denotations of the arguments, so in Model C, we can directly reason over the space of flat logical forms zC (e.", "startOffset": 44, "endOffset": 57}, {"referenceID": 9, "context": "All experiments train for 6 iterations using AdaGrad (Duchi et al., 2010) and L1 regularization with a coefficient of 0.", "startOffset": 53, "endOffset": 73}, {"referenceID": 26, "context": "Zettlemoyer and Collins (2009) developed a model that handles references to previous logical forms; Artzi and Zettlemoyer (2013) developed a model that handles references to the current world state.", "startOffset": 0, "endOffset": 31}, {"referenceID": 0, "context": "Zettlemoyer and Collins (2009) developed a model that handles references to previous logical forms; Artzi and Zettlemoyer (2013) developed a model that handles references to the current world state.", "startOffset": 100, "endOffset": 129}, {"referenceID": 26, "context": "Traditional semantic parsers generate logical forms by aligning each part of the logical form to the utterance (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011).", "startOffset": 111, "endOffset": 215}, {"referenceID": 24, "context": "Traditional semantic parsers generate logical forms by aligning each part of the logical form to the utterance (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011).", "startOffset": 111, "endOffset": 215}, {"referenceID": 28, "context": "Traditional semantic parsers generate logical forms by aligning each part of the logical form to the utterance (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011).", "startOffset": 111, "endOffset": 215}, {"referenceID": 12, "context": "Traditional semantic parsers generate logical forms by aligning each part of the logical form to the utterance (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011).", "startOffset": 111, "endOffset": 215}, {"referenceID": 4, "context": "In general, such systems rely on a lexicon, which can be hand-engineered, extracted (Cai and Yates, 2013; Berant et al., 2013), or automatically learned from annotated logical forms (Kwiatkowski et al.", "startOffset": 84, "endOffset": 126}, {"referenceID": 1, "context": "In general, such systems rely on a lexicon, which can be hand-engineered, extracted (Cai and Yates, 2013; Berant et al., 2013), or automatically learned from annotated logical forms (Kwiatkowski et al.", "startOffset": 84, "endOffset": 126}, {"referenceID": 11, "context": ", 2013), or automatically learned from annotated logical forms (Kwiatkowski et al., 2010; Chen, 2012).", "startOffset": 63, "endOffset": 101}, {"referenceID": 6, "context": ", 2013), or automatically learned from annotated logical forms (Kwiatkowski et al., 2010; Chen, 2012).", "startOffset": 63, "endOffset": 101}, {"referenceID": 13, "context": "Pasupat and Liang (2014) and Wang et al.", "startOffset": 12, "endOffset": 25}, {"referenceID": 13, "context": "Pasupat and Liang (2014) and Wang et al. (2015) proposed generating logical forms without alignments, similar to our Model B.", "startOffset": 12, "endOffset": 48}, {"referenceID": 13, "context": "Pasupat and Liang (2014) and Wang et al. (2015) proposed generating logical forms without alignments, similar to our Model B. Yao et al. (2014) and Bordes et al.", "startOffset": 12, "endOffset": 144}, {"referenceID": 2, "context": "(2014) and Bordes et al. (2014) have explored predicting paths in a knowledge graph directly, which is similar to the flat logical forms of Model C.", "startOffset": 11, "endOffset": 32}, {"referenceID": 16, "context": "In the unsupervised learning of generative models, bootstrapping can help escape local optima and provide helpful regularization (Och and Ney, 2003; Liang et al., 2009).", "startOffset": 129, "endOffset": 168}, {"referenceID": 13, "context": "In the unsupervised learning of generative models, bootstrapping can help escape local optima and provide helpful regularization (Och and Ney, 2003; Liang et al., 2009).", "startOffset": 129, "endOffset": 168}, {"referenceID": 13, "context": "In the unsupervised learning of generative models, bootstrapping can help escape local optima and provide helpful regularization (Och and Ney, 2003; Liang et al., 2009). When it is difficult to even find one logical form that reaches the denotation, one can use the relaxation technique of Steinhardt and Liang (2015).", "startOffset": 149, "endOffset": 318}, {"referenceID": 3, "context": "One could go one step further and bypass logical forms altogether, performing all the logical reasoning in a continuous space (Bowman et al., 2014; Weston et al., 2015; Guu et al., 2015; Reed and de Freitas, 2016).", "startOffset": 126, "endOffset": 213}, {"referenceID": 23, "context": "One could go one step further and bypass logical forms altogether, performing all the logical reasoning in a continuous space (Bowman et al., 2014; Weston et al., 2015; Guu et al., 2015; Reed and de Freitas, 2016).", "startOffset": 126, "endOffset": 213}, {"referenceID": 10, "context": "One could go one step further and bypass logical forms altogether, performing all the logical reasoning in a continuous space (Bowman et al., 2014; Weston et al., 2015; Guu et al., 2015; Reed and de Freitas, 2016).", "startOffset": 126, "endOffset": 213}, {"referenceID": 13, "context": "Indeed, Neelakantan et al. (2016) use recurrent neural networks attempt to perform logical operations.", "startOffset": 8, "endOffset": 34}], "year": 2016, "abstractText": "We consider the task of learning a contextdependent mapping from utterances to denotations. With only denotations at training time, we must search over a combinatorially large space of logical forms, which is even larger with context-dependent utterances. To cope with this challenge, we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms. Though less expressive, we find that these simpler models are much faster and can be surprisingly effective. Moreover, they can be used to bootstrap the full model. Finally, we collected three new contextdependent semantic parsing datasets, and develop a new left-to-right parser.", "creator": "TeX"}}}