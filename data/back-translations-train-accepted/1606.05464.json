{"id": "1606.05464", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "Stance Detection with Bidirectional Conditional Encoding", "abstract": "Stance detection is the task of classifying the attitude expressed in a text towards a target such as \"Climate Change is a Real Concern\" to be \"positive\", \"negative\" or \"neutral\". Previous work has assumed that either the target is mentioned in the text or that training data for every target is given. This paper considers the more challenging version of this task, where targets are not always mentioned and no training data is available for the test targets. We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms the independent encoding of tweet and target. Performance improves even further when the conditional model is augmented with bidirectional encoding. The method is evaluated on the SemEval 2016 Task 6 Twitter Stance Detection corpus and achieves performance second best only to a system trained on semi-automatically labelled tweets for the test target. When such weak supervision is added, our approach achieves state-of-the-art results.", "histories": [["v1", "Fri, 17 Jun 2016 09:39:47 GMT  (27kb)", "http://arxiv.org/abs/1606.05464v1", null], ["v2", "Mon, 26 Sep 2016 20:49:16 GMT  (23kb)", "http://arxiv.org/abs/1606.05464v2", "10 pages"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["isabelle augenstein", "tim rockt\u00e4schel", "andreas vlachos", "kalina bontcheva"], "accepted": true, "id": "1606.05464"}, "pdf": {"name": "1606.05464.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["i.augenstein@ucl.ac.uk,", "t.rocktaschel@cs.ucl.ac.uk", "k.bontcheva}@sheffield.ac.uk", "@realDonaldTrump", "@GOP"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.05 464v 1 [cs.C L] 17 JuStance detection is the task of classifying as \"positive,\" \"negative,\" or \"neutral\" the attitudes expressed in a text towards a target such as \"climate change is a real concern.\" Previous work has assumed that either the target is mentioned in the text or that training data is given for each target. In this paper, the more sophisticated version of this task is considered, where targets are not always mentioned and training data is not available for the test targets. We experiment with conditional LSTM coding, which forms a target-dependent representation of the tweet, and show that it exceeds the independent coding of tweet and target. Performance improves even further when the conditional model is extended to include bi-directional coding. It is evaluated at SemEval 2016 Task 6 Twitter Stance Detection and achieves the second-best performance only after a system trained on semi-automatic tweets for the test target."}, {"heading": "1 Introduction", "text": "The goal of position recognition is to classify the attitude expressed in a text toward a particular goal as \"positive,\" \"negative,\" or \"neutral.\" Such information can be useful for a variety of tasks, e.g. Mendoza et al. (2010) showed that tweets that indicate actual facts were confirmed by 90% of tweets related to it, while tweets that convey false information were overwhelmingly challenged or denied. The focus of this paper is on a novel position recognition task, namely tweet detection toward previously invisible targets (mostly entities such as politicians or issues of public interest) as defined in the SemEval Stance Detection for Twitter task (Mohammad et al., 2016). This task is quite difficult, firstly because of the lack of training data for the targets in the test set, and secondly because of the goals not mentioned in the tweet, and secondly because of the goals not always mentioned in the tweet. For example, the positive tweet is the GOP @ aldTrump voice."}, {"heading": "2 Task Setup", "text": "The SemEval 2016 Stance Detection for Twitter task (Mohammad et al., 2016) consists of two subtasks, Task A and Task B. In Task A, the goal is to identify the attitudes of tweets toward goals marked with training data for all test goals (Climate Change is a Real Concern, Feminist Movement, Atheism, Legalization of Abortion and Hillary Clinton). In Task B, which is at the center of this paper, the goal is to identify attitudes toward an invisible goal that differs from those considered in Task A, namely Donald Trump, for which training / development data are not provided. Systems must classify the attitudes of each tweet as \"positive\" (FAVOR), \"negative\" (AGAINST) or \"neutral\" (NONE) toward the goal. The official metric reported is F1 macro-aged averaged over FAVOR and AGAINST."}, {"heading": "3 Methods", "text": "A common approach to position recognition is to treat it as a sentence classification task, similar to sentiment analysis (Pang and Lee, 2008, Socher et al., 2013). However, such an approach cannot capture a tweet's attitude toward a particular target unless training data is available for each of the test targets. In such cases, we might learn that a tweet mentioning Donald Trump positively expresses a negative attitude toward Hillary Clinton. Despite this limitation, we use two such baselines, one implemented with a support vector machine (SVM) classifier, and one with an LSTM to assess whether we can successfully integrate the target into the position forecast. A na\u00efve approach to integrate the target into the position forecast would be to generate features that associate the target with words from the tweet. In principle, this could allow the classifier to learn that some standpoint algorithms are still available for each of the tweets depending on the target weights."}, {"heading": "3.1 Independent Encoding", "text": "Our initial attempt to learn distributed representations for the tweets and the targets consists in encoding the target and the tweet independently of each other as k-dimensional dense vectors using two LSTMs (Hochreiter and Schmidhuber, 1997). H = [xtht \u2212 1] it = \u03c3 (W i H + bi) ft = \u03c3 (W f H + bf) ot = \u03c3 (W o H + bo) ct = ft-ct \u2212 1 + it-tanh (W c H + bc) ht = ot-tanh (ct) Here is xt an input vector in time step t, ct denotes the LSTM memory, ht-Rk is an output vector and the remaining weight matrices and distortions are comprehensible parameters. We concatenate the two output vector representations and classify the attitude based on the softmax via a non-linear projection softmax (choice target + W thw + tweet +) in the unweighted space of the Wta and the three classes."}, {"heading": "3.2 Conditional Encoding", "text": "To learn target-specific tweet representations, we use conditional encoding, as previously used to recognize text influences (Rockta \ufffd schel et al., 2016). We use an LSTM to encode the target as a fixed-length vector. We then encode the tweet with another LSTM whose state is initialized with the representation of the target. Finally, we use the last output vector of the tweet-LSTM to predict the posture of the target-tweet pair. This effectively enables the second LSTM to read the tweet in a target-specific manner, which is critical because the posture of the tweet depends on the target (remember the example of Donald Trump above)."}, {"heading": "3.3 Bidirectional Conditional Encoding", "text": "It has been shown that bidirectional LSTMs learn improved representations of sequences by encoding a sequence from left to right and from right to left (Graves and Schmidhuber, 2005). Therefore, we adapt the conditional encoding model from Section 3.2 to use bidirectional LSTMs that represent the target and the tweet, using two vectors for each of them, one that reads the tweet from left to right by reading the target and then the tweet from left to right (as in the conditional LSTM encoding) and one that is achieved by reading from right to left. To achieve this, we initialize the state of the bidirectional LSTM that reads the tweet through the last state of the forward and backward encoding of the target (see Figure 1). Bidirectional encoding allows the model to construct target-dependent representations of the tweet in such a way that, when each word is viewed, it takes into account both the left-hand and right-hand context."}, {"heading": "3.4 Unsupervised Pretraining", "text": "In order to counterbalance the relatively small amount of training data available (a total of 5628 cases), an unsupervised pre-training is used. It initializes the word embeddings used in the LSTMs using a suitably trained word2vec model (Mikolov et al., 2013). Note that these embeddings are only used for initialization, as we make it possible to further optimize them during the training.In detail, we train a word2vec model on a body of 395,212 unlabeled tweets collected with the Twitter Keyword Search API1 between November 2015 and January 2016, plus any tweets included in the official SemEval 2016 Stance Detection Datasets (Mohammad et al., 2016).The unlabeled tweets are collected so that they contain the training, dev and test targets, using up to two keywords per target, namely \"hillary,\" \"clinton,\" \"\" killary, \"\" linorti, \"\" femi. \""}, {"heading": "4 Experiments", "text": "We report on experiments for two different experimental constellations: One is the invisible target setup (Section 5), which is the focus of this paper, i.e. determining the attitude of tweets towards previously invisible targets. We show that conditional coding, by reading the tweets specifically, generalizes to invisible targets, which are better than baselines that ignore the target. Next, we compare our approach with previous work in a weakly supervised framework (Section 6) and show that our approach exceeds the state of the art at the SemEval 2016 Stance Detection Subtask B Corpus. Table 1 lists the various companies used in the experiments and their size. TaskA Tr + Dv is the official version of TaskA, TaskA Training and Development Corpus, which contain examples of legalization."}, {"heading": "4.1 Methods", "text": "We compare the following basic methods: \u2022 SVM trains with word and letter tweet-n-grams (SVM-ngramscomb) Mohammad et al. (2016) \u2022 a majority class baseline (majority baseline), reported in (Mohammad et al., 2016) \u2022 a bag of word vectors (BoWV) (see Section 3.4) \u2022 independent encoding of tweet and target with two LSTMs (Concat) (see Section 3.1) \u2022 Tweet encoding with only one LSTM (TweetOnly) (see Section 3.1bis three versions of conditional encoding: \u2022 Target conditioned on tweet (TarCondTweet) \u2022 Tweet conditioned on target (TweetCondTar) \u2022 a bidirectional encoding model (BiCond)"}, {"heading": "5 Unseen Target Stance Detection", "text": "As explained, the challenge is to learn a model without manually labeled training data for the test target, but only with the data of the Task A targets. To avoid any labeled data for Donald Trump and still have a (labeled) development setting for our models, we used the tweets for Hillary Clinton as a development set and the tweets for the remaining four targets as training. We call this a development setting, and all models are matched with this setup. The labeled Donald Trump tweets were only used for reporting our final results. For the final results, we train all data from the development setup and evaluate on the official Task B test set, i.e. the Donald Trump tweets. We call this our test setup. Based on a small grid search using the development setup, the following settings for LSTM-based models were chosen: Input Coating Size 100 (equal to word embedding dimensions), Hidden Layer Size 60, Training for max 50 learning stages with a relatively low AM of 0.1."}, {"heading": "5.1 Results and Discussion", "text": "The results for the Invisible Objective show how well conditioned encoding is suitable for learning bed-dependent representations of tweets, and how well such representations generalize to invisible targets. The most powerful method for both development (Table 2) and test configurations (Table 3) is BiCond, which achieves an F1 of 0.4722 and 0.4901, respectively. It is noteworthy that Concat, which learns invariable encoding of the target and tweets, does not achieve major F1 improvements over TweetOnly, which contains only one representation of tweets, showing that it is not only important to learn targeted encoding, but also the way in which it is learned. Models that learn to condition the encoding of tweets to targets outperform all baselines on the testset.It is also worth noting that the Bag-of-WordVectors basic training achieves comparable tweetOnly results."}, {"heading": "6 Weakly Supervised Stance Detection", "text": "The previous section demonstrated the usefulness of conditional encoding for invisible target position detection and compared the results with internal fundamentals. The goal of the experiments covered in this section is to compare with participants in SemEval 2016 Stance Detection Task B. While we are looking at an invisible target setup, most submissions, including the three highest-level for Task B, pkudblab (Wei et al., 2016b), LitisMind (Zarrella and Marsh, 2016), and INFUFRGS (Wei et al., 2016a) considered another experimental setup. They annotated training data for the test target Donald Trump, perceiving the task as poorly verified. The pkudblab system uses a deep revolutionary neural network that learns to make predictions in two ways that are automatically labeled positive and negative."}, {"heading": "6.1 Results and Discussion", "text": "Table 6 lists our results in the poorly monitored environment. Table 7 shows all of our results, including those using the invisible target structure, compared to the state of the art in position recognition. It also lists baselines reported by Mohammad et al. (2016), namely a majority-class baseline (majority baseline) and a method that uses 1 to 3 gram word and letter-n-gram features (SVM-ngram comb) that are extracted from the tweets and used to train a 3-way SVM classifier. Word bag baselines (BoWV, SVM-ngram comb) achieve results comparable to the majority baseline (F1 of 0.2972), showing how difficult the task is. Baselines that extract only features from the tweets, SVM-ngram comb and TweetOnly achieve results that are comparable to baseline tweets (which are more difficult for baseline F1), such as SVM-ngram comb and TweetOnly, the results are comparable to baseline tweets (which are more difficult for baseline F1)."}, {"heading": "7 Related Work", "text": "Stance Detection: Previous work mostly looked at target-specific position predictions in debates (Hasan and Ng, 2013, Walker et al., 2012) or student papers (Faulkner et al., 2014). Recent work examined Twitter-based position recognition (Rajadesingan et al., 2014), which is also a task at SemEval 2016 (Mohammad et al., 2016), which is more difficult than position recognition in debates because in addition to irregular language, the data set Mohammad et al. (2016) is offered without any context, e.g. conversation structure or tweet metadata. The targets are also not always mentioned in the tweets, which makes the task very difficult (Augenstein et al., 2016) and distinguishes them from target dependence (Vo et Zhang et al., 2015, Zhang et al., 2016, Alghunaim et al., 2015) and the Open Domain Target Sentiment Analysis Bowet, 2015, et al."}, {"heading": "8 Conclusions and Future Work", "text": "This work demonstrated that conditional LSTM encoding is a successful approach to position recognition for invisible targets. Our invisible bi-directional conditional target encoding approach achieves the second best results reported so far on the SemEval 2016 Twitter position recognition corpus. In a minimally monitored target scenario, as considered in previous work, our approach achieves the best results to date on the SemEval Task B dataset. We also show that in the absence of large tagged corpora, unattended pretraining can be used to learn position recognition objectives and improve results on the SemEval corpus. Future work will further explore the challenge of position recognition for tweets that do not contain explicit mentions of the target."}], "references": [{"title": "Scott Cyphers", "author": ["Abdulaziz Alghunaim", "Mitra Mohtarami"], "venue": "and Jim Glass.", "citeRegEx": "Alghunaim et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Andreas Vlachos", "author": ["Isabelle Augenstein"], "venue": "and Kalina Bontcheva.", "citeRegEx": "Augenstein et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Christopher D", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts"], "venue": "Manning.", "citeRegEx": "Bowman et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Automated Classification of Stance in Student Essays: An Approach Using Stance Target Information and the Wikipedia Link-Based Measure", "author": ["Adam Faulkner"], "venue": null, "citeRegEx": "Faulkner.,? \\Q2014\\E", "shortCiteRegEx": "Faulkner.", "year": 2014}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Stance Classification of Ideological Debates: Data, Models, Features, and Constraints. In IJCNLP, pages 1348\u20131356", "author": ["Hasan", "Ng2013] Kazi Saidul Hasan", "Vincent Ng"], "venue": "Asian Federation of Natural Language Processing / ACL", "citeRegEx": "Hasan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hasan et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Adam: A Method for Stochastic Optimization. CoRR, abs/1412.6980", "author": ["Kingma", "Ba2014] Diederik P. Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Barbara Poblete", "author": ["Marcelo Mendoza"], "venue": "and Carlos Castillo.", "citeRegEx": "Mendoza et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Greg Corrado", "author": ["Tomas Mikolov", "Kai Chen"], "venue": "and Jeffrey Dean.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Theresa Wilson", "author": ["Margaret Mitchell", "Jacqui Aguilar"], "venue": "and Benjamin Van Durme.", "citeRegEx": "Mitchell et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Xiaodan Zhu", "author": ["Saif M. Mohammad", "Svetlana Kiritchenko", "Parinaz Sobhani"], "venue": "and Colin Cherry.", "citeRegEx": "Mohammad et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Opinion mining and sentiment analysis", "author": ["Pang", "Lee2008] Bo Pang", "Lillian Lee"], "venue": "Foundations and trends in information retrieval,", "citeRegEx": "Pang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2008}, {"title": "and E", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot"], "venue": "Duchesnay.", "citeRegEx": "Pedregosa et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Identifying Users with Opposing Opinions in Twitter Debates", "author": ["Rajadesingan", "Liu2014] Ashwin Rajadesingan", "Huan Liu"], "venue": null, "citeRegEx": "Rajadesingan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rajadesingan et al\\.", "year": 2014}, {"title": "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann"], "venue": "and Phil Blunsom.", "citeRegEx": "Rockt\u00e4schel et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "2016b. pkudblab at", "author": ["Chen", "Tengjiao Wang"], "venue": null, "citeRegEx": "Chen and Wang.,? \\Q2016\\E", "shortCiteRegEx": "Chen and Wang.", "year": 2016}, {"title": "Neural Networks for Open Do", "author": ["Duy Tin Vo"], "venue": null, "citeRegEx": "Vo.,? \\Q2015\\E", "shortCiteRegEx": "Vo.", "year": 2015}, {"title": "Gated Neural Networks for Targeted", "author": ["Tin Vo"], "venue": null, "citeRegEx": "Vo.,? \\Q2016\\E", "shortCiteRegEx": "Vo.", "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "Stance detection is the task of classifying the attitude expressed in a text towards a target such as \u201cClimate Change is a Real Concern\u201d to be \u201cpositive\u201d, \u201cnegative\u201d or \u201cneutral\u201d. Previous work has assumed that either the target is mentioned in the text or that training data for every target is given. This paper considers the more challenging version of this task, where targets are not always mentioned and no training data is available for the test targets. We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms the independent encoding of tweet and target. Performance improves even further when the conditional model is augmented with bidirectional encoding. The method is evaluated on the SemEval 2016 Task 6 Twitter Stance Detection corpus and achieves performance second best only to a system trained on semi-automatically labelled tweets for the test target. When such weak supervision is added, our approach achieves state\u2013of-the-art results.", "creator": "LaTeX with hyperref package"}}}