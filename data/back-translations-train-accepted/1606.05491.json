{"id": "1606.05491", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings", "abstract": "We present a natural language generator based on the sequence-to-sequence approach that can be trained to produce natural language strings as well as deep syntax dependency trees from input dialogue acts, and we use it to directly compare two-step generation with separate sentence planning and surface realization stages to a joint, one-step approach. We were able to train both setups successfully using very little training data. The joint setup offers better performance, surpassing state-of-the-art with regards to n-gram-based scores while providing more relevant outputs.", "histories": [["v1", "Fri, 17 Jun 2016 11:51:25 GMT  (116kb,D)", "http://arxiv.org/abs/1606.05491v1", "Accepted as a short paper for ACL 2016"]], "COMMENTS": "Accepted as a short paper for ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ond\\v{r}ej du\\v{s}ek", "filip jur\\v{c}\\'i\\v{c}ek"], "accepted": true, "id": "1606.05491"}, "pdf": {"name": "1606.05491.pdf", "metadata": {"source": "CRF", "title": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings", "authors": ["Ond\u0159ej Du\u0161ek"], "emails": ["odusek@ufal.mff.cuni.cz", "jurcicek@ufal.mff.cuni.cz"], "sections": [{"heading": null, "text": "We have been able to successfully train both constellations with very little training data; the common constellation provides better performance, exceeds the state of the art in terms of ngram-based values, and delivers more relevant outputs."}, {"heading": "1 Introduction", "text": "In spoken dialog systems (SDS), the task of natural language generation (NLG) is to transform a meaning representation (MR) produced by the dialog manager into one or more sentences in a natural language. Traditionally, it is divided into two subtasks: sentence planning, which determines the entire sentence structure, and surface realization, which determines the exact word forms and linearizes the structure into a string (Reiter and Dale, 2000). While some generators maintain this division and use a two-step pipeline (Walker et al., 2001; Rieser et al., 2010; Dethlefs et al., 2013), others apply a common model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013), we present a new, conceptually simple NLG system for SDS, which is able to work in both modes: it either produces natural language strings or creates deep syntax dependency trees, which are then processed by an external user interface."}, {"heading": "2 Generator Setting", "text": "The input to our generator are dialog files (DA) (Young et al., 2010) representing an action such as information or request, along with one or more attributes (slots) and their values. Our generator operates in two modes and produces either deep syntax trees (Dus ek et al., 2012) or natural language strings (see fig. 1). The first mode corresponds to the sentence planning NLG level, since it determines the syntactical form of the output set; the resulting deep syntax tree includes content words (Lemmata) and their syntactic form (formulas, violet in fig. 1). The trees are linearized with the help of aar Xiv to strings: 160 6.05 491v 1 [cs.C L] June 17, 2016 surface realizers from the TectoMT translation system (Dus ek et al., 2015). The second generator mode combines sentence planning and surface realization step in a synthetic mode, and the natural modes are produced directly along the two-level structure (both of the two-level modes)."}, {"heading": "3 The Seq2seq Generation Model", "text": "Our generator is based on the seq2seq approach (Cho et al., 2014; Sutskever et al., 2014), a type of encoder decoder RNN architecture that works with sequences of tokens of varying lengths. Section 3.1 discusses the necessary conversion of input DA and output trees / sentences into sequences, followed by Section 3.2 describing the most important seq2seq component, supplemented by a reranchor as explained in Section 3.3."}, {"heading": "3.1 Sequence Representation of DA, Trees, and Sentences", "text": "We present DA, deep syntax trees, and sentences as sequences of tokens to allow their use in the sequence-based RNN components of our generator (see Sections 3.2 and 3.3). Each token is represented by its embedding - a vector of floating point numbers (see Fig. 3). To form a sequence representation of a DA, we generate for each slot in the DA a triple of the structure \"DA type, slot, value,\" and concatenate the triples (see Fig. 3). The deep syntax tree output of the seq2seq generator is represented in a brackets notation similar to that of Vinyals et al. (2015, see Fig. 2)."}, {"heading": "3.2 Seq2seq Generator", "text": "Our attentive seq2seq generator (Bahdanau et al., 2015, see Fig. 3) 1 starts with the encoder stage, which encodes an input sequence x = {x1,..., xn} into a sequence of encoder outputs and hidden states h = {h1,.., hn}, where ht = lstm (xt, ht \u2212 1), a nonlinear function represented by the LSTM cell (Graves, 2013), then uses the hidden states to generate a sequence y = {y1,.., ym} with a second LSTM-based RNN. The probability of each output token is defined as: p (yt | y1,.., yt \u2212 1, x) = softmax ((st \u00b2 ct) WY). Here is st the decoder state in which s0 = hn and st = lqar (WY) are hidden."}, {"heading": "3.3 Reranker", "text": "To ensure that the output trees / strings match semantically with the input DA, we have implemented a classifier to rearrange the n-best bar search results and punish the missing required information and / or add irrelevant ones. Similar to Wen et al. (2015a), the classifier returns a binary decision for an output tree / string on the presence of all dialog types and slot-value combinations shown in the training data, resulting in a 1-hot vector (see Fig. 4). We use implementation in the TensorFlow framework (Abadi et al., 2015). The input DA is converted into a similar 1-hot vector and the ranking penalty of the set is the hamming distance between the two vectors (see Fig. 4). Weighted penalties for all sentences are subtracted from their n-log probabilities."}, {"heading": "4 Experiments", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "5 Results", "text": "The results of our experiments and a comparison with previous work on this dataset are shown in Table 1, although the results of BLEU and NIST scores and the number of semantic errors (erroneous, missing and repeated) that we manually evaluate on a sample of 42 output sets (results of two randomly selected cross-validation runs) show that the models learn to produce domain-style fluent sentences; 9 incoherent sentences are rare, but semantic errors are very common in greedy searches. Most errors involve confusion of semantically close items, e.g. in relation to the French or riverside area of the city (see Table 2); positions that occur more frequently, regardless of their relevance; beam searching brings about a BLEU improvement, but retains most semantic errors in place."}, {"heading": "6 Related Work", "text": "While most newer NLG systems attempt to learn from the data, the choice of a particular approach - pipeline or joint - is often arbitrary and depends on the system architecture or specific generational areas. Work using the pipeline approach in the SDS tends to focus on typesetting, improving a handmade generator (Walker et al., 2001; Stent et al., 2004; Paiva et al., 2005), or using perception-driven A * search (Dus, ek and Jurc, 2001). Generators using the common approach use various methods, such as factored language models (Mairesse et al., 2010), inverted parsing (Wong and Mooney, 2007; Konstas and Lapata, 2013), or a pipeline of discriminatory classifiers (Angeli et al., 2010). Unlike most previous 11The difference is statistically significant at the 99% level, corresponding to bootstrap 2004 (Koresehn)."}, {"heading": "7 Conclusions and Future Work", "text": "We have presented a direct comparison of the two-step generation of deep syntax trees with direct generation in strings, both using the same NLG system based on the seq2seq approach. Although both approaches offer decent performance, their results are quite different.The results show that the direct approach is more advantageous, with significantly higher n-gram values and a similar number of semantic errors in output. We have also shown that our generator can learn to generate meaningful expressions with a much lower amount of training data than what is normally used for RNN-based approaches. The resulting models had virtually no problems generating flowing, coherent sentences or generating valid structures of bracketed deep syntax trees. Our generator was able to surpass the best BLEU- / NIST values on the same data sets previously obtained by a perctron-based generator of dens, coherent sets, or generating valid structures of clear syntax trees."}, {"heading": "Acknowledgments", "text": "This work was funded by the Ministry of Education, Youth and Sport of the Czech Republic under the funding agreement LK11221 and nuclear research funding, the SIA project 260 333 and the GAUK project 2058214 of Charles University in Prague using language resources stored and distributed in the LINDAT / CLARIN project of the Ministry of Education, Youth and Sport of the Czech Republic (project LM2015071). We thank our colleagues and anonymous reviewers for helpful comments."}], "references": [{"title": "A simple domain-independent probabilistic approach to generation", "author": ["G. Angeli", "P. Liang", "D. Klein."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502\u2013512.", "citeRegEx": "Angeli et al\\.,? 2010", "shortCiteRegEx": "Angeli et al\\.", "year": 2010}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A Neural Probabilistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin."], "venue": "Journal of Machine Learning Research, 3:1137\u2013 1155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio."], "venue": "Proceedings of the 2014 Conference", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Conditional Random Fields for Responsive Surface Realisation using Global Features", "author": ["N. Dethlefs", "H. Hastie", "H. Cuay\u00e1huitl", "O. Lemon."], "venue": "Proceedings of ACL, Sofia.", "citeRegEx": "Dethlefs et al\\.,? 2013", "shortCiteRegEx": "Dethlefs et al\\.", "year": 2013}, {"title": "Automatic Evaluation of Machine Translation Quality Using N-gram Cooccurrence Statistics", "author": ["G. Doddington."], "venue": "Proceedings of the Second International Conference on Human Language Technology Research, pages 138\u2013145, San Fran-", "citeRegEx": "Doddington.,? 2002", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "Training a Natural Language Generator From Unaligned Data", "author": ["O. Du\u0161ek", "F. Jur\u010d\u0131\u0301\u010dek"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language", "citeRegEx": "Du\u0161ek and Jur\u010d\u0131\u0301\u010dek.,? \\Q2015\\E", "shortCiteRegEx": "Du\u0161ek and Jur\u010d\u0131\u0301\u010dek.", "year": 2015}, {"title": "Formemes in English-Czech Deep Syntactic MT", "author": ["Ond\u0159ej Du\u0161ek", "Zden\u011bk \u017dabokrtsk\u00fd", "Martin Popel", "Martin Majli\u0161", "Michal Nov\u00e1k", "David Mare\u010dek."], "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 267\u2013274,", "citeRegEx": "Du\u0161ek et al\\.,? 2012", "shortCiteRegEx": "Du\u0161ek et al\\.", "year": 2012}, {"title": "New Language Pairs in TectoMT", "author": ["O. Du\u0161ek", "L. Gomes", "M. Nov\u00e1k", "M. Popel", "R. Rosa."], "venue": "Proceedings of the 10th Workshop on Machine Translation, pages 98\u2013104, Lisbon, Portugal. Association for Computational Linguistics.", "citeRegEx": "Du\u0161ek et al\\.,? 2015", "shortCiteRegEx": "Du\u0161ek et al\\.", "year": 2015}, {"title": "Generating Sequences With Recurrent Neural Networks", "author": ["A. Graves."], "venue": "arXiv:1308.0850 [cs], August.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba."], "venue": "International Conference on Learning Representations. arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["P. Koehn."], "venue": "Proceedings of EMNLP, pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "A Global Model for Concept-to-Text Generation", "author": ["I. Konstas", "M. Lapata."], "venue": "Journal of Artificial Intelligence Research, 48:305\u2013346.", "citeRegEx": "Konstas and Lapata.,? 2013", "shortCiteRegEx": "Konstas and Lapata.", "year": 2013}, {"title": "Phrase-based statistical language generation using graphical models and active learning", "author": ["F. Mairesse", "M. Ga\u0161i\u0107", "F. Jur\u010d\u0131\u0301\u010dek", "S. Keizer", "B. Thomson", "K. Yu", "S. Young"], "venue": "In Proceedings of the 48th Annual Meeting of the Association", "citeRegEx": "Mairesse et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairesse et al\\.", "year": 2010}, {"title": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment", "author": ["H. Mei", "M. Bansal", "M.R. Walter."], "venue": "arXiv:1509.00838 [cs], September.", "citeRegEx": "Mei et al\\.,? 2015", "shortCiteRegEx": "Mei et al\\.", "year": 2015}, {"title": "Empirically-based control of natural language generation", "author": ["D.S. Paiva", "R. Evans."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL \u201905, pages 58\u201365, Stroudsburg, PA, USA. Association for Computa-", "citeRegEx": "Paiva and Evans.,? 2005", "shortCiteRegEx": "Paiva and Evans.", "year": 2005}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu."], "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "TectoMT: modular NLP framework", "author": ["M. Popel", "Z. \u017dabokrtsk\u00fd."], "venue": "Proceedings of IceTAL, 7th International Conference on Natural Language Processing,, pages 293\u2013304, Reykjav\u0131\u0301k.", "citeRegEx": "Popel and \u017dabokrtsk\u00fd.,? 2010", "shortCiteRegEx": "Popel and \u017dabokrtsk\u00fd.", "year": 2010}, {"title": "Sequence Level Training with Recurrent Neural Networks", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba."], "venue": "arXiv:1511.06732 [cs], November.", "citeRegEx": "Ranzato et al\\.,? 2015", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Let\u2019s go public! taking a spoken dialog system to the real world", "author": ["A. Raux", "B. Langner", "D. Bohus", "A.W. Black", "M. Eskenazi."], "venue": "in Proc. of Interspeech 2005. Citeseer.", "citeRegEx": "Raux et al\\.,? 2005", "shortCiteRegEx": "Raux et al\\.", "year": 2005}, {"title": "Building Natural Language Generation Systems", "author": ["E. Reiter", "R. Dale."], "venue": "Cambridge University Press, studies in natural language processing edition.", "citeRegEx": "Reiter and Dale.,? 2000", "shortCiteRegEx": "Reiter and Dale.", "year": 2000}, {"title": "Optimising information presentation for spoken dialogue systems", "author": ["V. Rieser", "O. Lemon", "X. Liu."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1009\u20131018.", "citeRegEx": "Rieser et al\\.,? 2010", "shortCiteRegEx": "Rieser et al\\.", "year": 2010}, {"title": "Trainable sentence planning for complex information presentation in spoken dialog systems", "author": ["A. Stent", "R. Prasad", "M. Walker."], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 79\u201386.", "citeRegEx": "Stent et al\\.,? 2004", "shortCiteRegEx": "Stent et al\\.", "year": 2004}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112. arXiv:1409.3215.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a Foreign Language", "author": ["O. Vinyals", "\u0141. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton."], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, R. Garnett, and R. Garnett, editors, Advances in Neural Information Processing Systems", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "SPoT: a trainable sentence planner", "author": ["M.A. Walker", "O. Rambow", "M. Rogati."], "venue": "Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, pages 1\u20138, Stroudsburg,", "citeRegEx": "Walker et al\\.,? 2001", "shortCiteRegEx": "Walker et al\\.", "year": 2001}, {"title": "Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking", "author": ["T.-H. Wen", "M. Gasic", "D. Kim", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young."], "venue": "Proceedings of the 16th Annual Meeting", "citeRegEx": "Wen et al\\.,? 2015a", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems", "author": ["T.-H. Wen", "M. Gasic", "N. Mrk\u0161i\u0107", "P.-H. Su", "D. Vandyke", "S. Young."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Wen et al\\.,? 2015b", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Generation by inverting a semantic parser that uses statistical machine translation", "author": ["Y.W. Wong", "R.J. Mooney."], "venue": "Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Compu-", "citeRegEx": "Wong and Mooney.,? 2007", "shortCiteRegEx": "Wong and Mooney.", "year": 2007}, {"title": "The Hidden Information State model: A practical framework for POMDP-based spoken dialogue management", "author": ["S. Young", "M. Ga\u0161i\u0107", "S. Keizer", "F. Mairesse", "J. Schatzmann", "B. Thomson", "K. Yu."], "venue": "Computer Speech & Language, 24(2):150\u2013", "citeRegEx": "Young et al\\.,? 2010", "shortCiteRegEx": "Young et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 21, "context": "It is traditionally divided into two subtasks: sentence planning, which decides on the overall sentence structure, and surface realization, determining the exact word forms and linearizing the structure into a string (Reiter and Dale, 2000).", "startOffset": 217, "endOffset": 240}, {"referenceID": 26, "context": "While some generators keep this division and use a two-step pipeline (Walker et al., 2001; Rieser et al., 2010; Dethlefs et al., 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013).", "startOffset": 69, "endOffset": 134}, {"referenceID": 22, "context": "While some generators keep this division and use a two-step pipeline (Walker et al., 2001; Rieser et al., 2010; Dethlefs et al., 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013).", "startOffset": 69, "endOffset": 134}, {"referenceID": 4, "context": "While some generators keep this division and use a two-step pipeline (Walker et al., 2001; Rieser et al., 2010; Dethlefs et al., 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013).", "startOffset": 69, "endOffset": 134}, {"referenceID": 29, "context": ", 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013).", "startOffset": 51, "endOffset": 100}, {"referenceID": 13, "context": ", 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013).", "startOffset": 51, "endOffset": 100}, {"referenceID": 8, "context": "We present a new, conceptually simple NLG system for SDS that is able to operate in both modes: it either produces natural language strings or generates deep syntax dependency trees, which are subsequently processed by an external surface realizer (Du\u0161ek et al., 2015).", "startOffset": 248, "endOffset": 268}, {"referenceID": 23, "context": ", (Stent et al., 2004; Raux et al., 2005; Mairesse et al., 2010)), it", "startOffset": 2, "endOffset": 64}, {"referenceID": 20, "context": ", (Stent et al., 2004; Raux et al., 2005; Mairesse et al., 2010)), it", "startOffset": 2, "endOffset": 64}, {"referenceID": 14, "context": ", (Stent et al., 2004; Raux et al., 2005; Mairesse et al., 2010)), it", "startOffset": 2, "endOffset": 64}, {"referenceID": 28, "context": "We experiment with using much less training data than recent systems based on recurrent neural networks (RNN) (Wen et al., 2015b; Mei et al., 2015), and we find that our genera-", "startOffset": 110, "endOffset": 147}, {"referenceID": 15, "context": "We experiment with using much less training data than recent systems based on recurrent neural networks (RNN) (Wen et al., 2015b; Mei et al., 2015), and we find that our genera-", "startOffset": 110, "endOffset": 147}, {"referenceID": 14, "context": "tor learns successfully to produce both strings and deep syntax trees on the BAGEL restaurant information dataset (Mairesse et al., 2010).", "startOffset": 114, "endOffset": 137}, {"referenceID": 6, "context": "It is able to surpass n-gram-based scores achieved previously by Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015), offering a simpler", "startOffset": 65, "endOffset": 92}, {"referenceID": 30, "context": "The input to our generator are dialogue acts (DA) (Young et al., 2010) representing an action, such as inform or request, along with one or more attributes (slots) and their values.", "startOffset": 50, "endOffset": 70}, {"referenceID": 7, "context": "Our generator operates in two modes, producing either deep syntax trees (Du\u0161ek et al., 2012) or natural language strings (see Fig.", "startOffset": 72, "endOffset": 92}, {"referenceID": 8, "context": "surface realizer from the TectoMT translation system (Du\u0161ek et al., 2015).", "startOffset": 53, "endOffset": 73}, {"referenceID": 6, "context": "away from complex surface syntax and morphology, which can be handled by a handcrafted, domain-independent module to ensure grammatical correctness at all times (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015), and the joint mode does not need to model", "startOffset": 161, "endOffset": 188}, {"referenceID": 13, "context": "structure explicitly and avoids accumulating errors along the pipeline (Konstas and Lapata, 2013).", "startOffset": 71, "endOffset": 97}, {"referenceID": 3, "context": "Our generator is based on the seq2seq approach (Cho et al., 2014; Sutskever et al., 2014), a type of an encoder-decoder RNN architecture operating on variable-length sequences of tokens.", "startOffset": 47, "endOffset": 89}, {"referenceID": 24, "context": "Our generator is based on the seq2seq approach (Cho et al., 2014; Sutskever et al., 2014), a type of an encoder-decoder RNN architecture operating on variable-length sequences of tokens.", "startOffset": 47, "endOffset": 89}, {"referenceID": 2, "context": "Each token is represented by its embedding \u2013 a vector of floatingpoint numbers (Bengio et al., 2003).", "startOffset": 79, "endOffset": 100}, {"referenceID": 9, "context": "cell (Graves, 2013).", "startOffset": 5, "endOffset": 19}, {"referenceID": 24, "context": "On top of this basic seq2seq model, we implemented a simple beam search for decoding (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 85, "endOffset": 132}, {"referenceID": 1, "context": "On top of this basic seq2seq model, we implemented a simple beam search for decoding (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 85, "endOffset": 132}, {"referenceID": 14, "context": "We perform our experiments on the BAGEL data set of Mairesse et al. (2010), which contains 202 DA from the restaurant information domain with two natural language paraphrases each, de-", "startOffset": 52, "endOffset": 75}, {"referenceID": 14, "context": "2Unlike Mairesse et al. (2010), we do not use", "startOffset": 8, "endOffset": 31}, {"referenceID": 13, "context": "We adopt the delexicalization scenario used by Mairesse et al. (2010) and Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015).", "startOffset": 47, "endOffset": 70}, {"referenceID": 6, "context": "(2010) and Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015). manually annotated alignment of slots and values in the input DA to target words and phrases and let the generator learn it from data, which simplifies training data preparation but makes our task", "startOffset": 11, "endOffset": 38}, {"referenceID": 18, "context": "We lowercase the data and treat plural -s as separate tokens for generating into strings, and we apply automatic analysis from the Treex NLP toolkit (Popel and \u017dabokrtsk\u00fd, 2010) to obtain deep syntax trees for training tree-based generator setups.", "startOffset": 149, "endOffset": 177}, {"referenceID": 14, "context": "3 Same as Mairesse et al. (2010), we apply 10-fold cross-validation, with 181 training DA and 21 testing DA.", "startOffset": 10, "endOffset": 33}, {"referenceID": 11, "context": "To train our seq2seq generator, we use the Adam optimizer (Kingma and Ba, 2015) to minimize unweighted sequence cross-entropy.", "startOffset": 58, "endOffset": 79}, {"referenceID": 17, "context": "We treat the two paraphrases for the same DA as separate instances in the training set but use them together as two references to measure BLEU and NIST scores (Papineni et al., 2002; Doddington, 2002) on the validation and test sets.", "startOffset": 159, "endOffset": 200}, {"referenceID": 5, "context": "We treat the two paraphrases for the same DA as separate instances in the training set but use them together as two references to measure BLEU and NIST scores (Papineni et al., 2002; Doddington, 2002) on the validation and test sets.", "startOffset": 159, "endOffset": 200}, {"referenceID": 13, "context": "Setup BLEU NIST ERR Mairesse et al. (2010)\u2217 \u223c67 - 0 Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015) 59.", "startOffset": 20, "endOffset": 43}, {"referenceID": 6, "context": "(2010)\u2217 \u223c67 - 0 Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015) 59.", "startOffset": 16, "endOffset": 43}, {"referenceID": 14, "context": "\u2217Mairesse et al. (2010) use manual alignments in their work, so their result is not directly comparable to ours.", "startOffset": 1, "endOffset": 24}, {"referenceID": 6, "context": "ited domain (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015), leaving the seq2seq generator as the major error source.", "startOffset": 12, "endOffset": 39}, {"referenceID": 6, "context": "sults of both setups surpass the best results on this dataset using training data without manual alignments (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015) in both automatic metrics12 and the number of semantic errors.", "startOffset": 108, "endOffset": 135}, {"referenceID": 26, "context": "Works using the pipeline approach in SDS tend to focus on sentence planning, improving a handcrafted generator (Walker et al., 2001; Stent et al., 2004; Paiva and Evans, 2005) or using perceptron-guided A* search (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015).", "startOffset": 111, "endOffset": 175}, {"referenceID": 23, "context": "Works using the pipeline approach in SDS tend to focus on sentence planning, improving a handcrafted generator (Walker et al., 2001; Stent et al., 2004; Paiva and Evans, 2005) or using perceptron-guided A* search (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015).", "startOffset": 111, "endOffset": 175}, {"referenceID": 16, "context": "Works using the pipeline approach in SDS tend to focus on sentence planning, improving a handcrafted generator (Walker et al., 2001; Stent et al., 2004; Paiva and Evans, 2005) or using perceptron-guided A* search (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015).", "startOffset": 111, "endOffset": 175}, {"referenceID": 6, "context": ", 2004; Paiva and Evans, 2005) or using perceptron-guided A* search (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2015).", "startOffset": 68, "endOffset": 95}, {"referenceID": 14, "context": ", factored language models (Mairesse et al., 2010), inverted parsing (Wong and Mooney, 2007; Konstas and Lapata, 2013), or a pipeline of discriminative clas-", "startOffset": 27, "endOffset": 50}, {"referenceID": 29, "context": ", 2010), inverted parsing (Wong and Mooney, 2007; Konstas and Lapata, 2013), or a pipeline of discriminative clas-", "startOffset": 26, "endOffset": 75}, {"referenceID": 13, "context": ", 2010), inverted parsing (Wong and Mooney, 2007; Konstas and Lapata, 2013), or a pipeline of discriminative clas-", "startOffset": 26, "endOffset": 75}, {"referenceID": 0, "context": "sifiers (Angeli et al., 2010).", "startOffset": 8, "endOffset": 29}, {"referenceID": 12, "context": "The difference is statistically significant at 99% level according to pairwise bootstrap resampling test (Koehn, 2004).", "startOffset": 105, "endOffset": 118}, {"referenceID": 27, "context": "Wen et al. (2015a) combined two RNN with a convolutional network reranker; Wen et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 27, "context": "Wen et al. (2015a) combined two RNN with a convolutional network reranker; Wen et al. (2015b) later replaced basic sigmoid", "startOffset": 0, "endOffset": 94}, {"referenceID": 15, "context": "Mei et al. (2015) present the only seq2seq-based NLG system known to us.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "scores on the same dataset previously achieved by a perceptron-based generator of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2015) while reducing the amount of irrelevant information on the output.", "startOffset": 82, "endOffset": 109}, {"referenceID": 19, "context": ", 2015) and sequence level training (Ranzato et al., 2015).", "startOffset": 36, "endOffset": 58}], "year": 2016, "abstractText": "We present a natural language generator based on the sequence-to-sequence approach that can be trained to produce natural language strings as well as deep syntax dependency trees from input dialogue acts, and we use it to directly compare two-step generation with separate sentence planning and surface realization stages to a joint, one-step approach. We were able to train both setups successfully using very little training data. The joint setup offers better performance, surpassing state-of-the-art with regards to ngram-based scores while providing more relevant outputs.", "creator": "LaTeX with hyperref package"}}}