{"id": "1606.05679", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "Two Discourse Driven Language Models for Semantics", "abstract": "Natural language understanding often requires deep semantic knowledge. Expanding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction. We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities. For each model, we investigate four implementations: a \"standard\" N-gram language model and three discriminatively trained \"neural\" language models that generate embeddings for semantic frames. The quality of the semantic language models (SemLM) is evaluated both intrinsically, using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing.", "histories": [["v1", "Fri, 17 Jun 2016 21:19:35 GMT  (30kb)", "https://arxiv.org/abs/1606.05679v1", "To appear in ACL 16"], ["v2", "Mon, 27 Jun 2016 06:20:52 GMT  (30kb)", "http://arxiv.org/abs/1606.05679v2", "To appear in ACL 16"]], "COMMENTS": "To appear in ACL 16", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haoruo peng", "dan roth"], "accepted": true, "id": "1606.05679"}, "pdf": {"name": "1606.05679.pdf", "metadata": {"source": "CRF", "title": "Two Discourse Driven Language Models for Semantics", "authors": ["Haoruo Peng"], "emails": ["hpeng7@illinois.edu", "danr@illinois.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.05 679v 2 [cs.C L] 27 Jun 2016 Appeared in ACL '16"}, {"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far, until it is so far."}, {"heading": "2 Related Work", "text": "Early work (Schank and Abelson, 1977; Mooney and DeJong, 1985), however, attempted to construct knowledge bases from documents in order to learn scripts. Recent work focused on the use of statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016); other work aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al.; Balasubramanian et al., 2014; Bamman and Smith, 2015), and several papers dealt with neural embedding (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al, 2014; Titov and Khoddam, 2015)."}, {"heading": "3 Two Models for SemLM", "text": "In this section, we describe how to capture semantic information consisting of semantic frames and discourse markers as semantic units (i.e. vocabulary)."}, {"heading": "3.1 Semantic Frames and Discourse Markers", "text": "Semantic frames A semantic frame is made up of a predicate and the corresponding argumentation participants. Here, we demand that the predicate is unambiguous in a certain sense, and we need a certain amount of abstraction of arguments so that we can assign abstract designations. PropBank frames (Kingsbury and Palmer, 2002) and FrameNet frames (Baker et al., 1998) both have a limited set of frames (in the order of thousands), and each frame can be clearly represented by its predicate sense. These frames offer a good degree of generalization, as each frame can be embedded in different surface shapes in natural texts. We use these frames as part of our vocabulary for SemLMs. Formally, we use the notation f to represent a frame. We also denote fa, f # Arg when we refer to an argument discourse within a marker dissectory framework."}, {"heading": "3.2 Frame-Chain SemLM", "text": "For the frame chain SemLM, we model all semantic frames and discourse markers in a document. We build the semantic sequence by first including all semantic frames in the order in which they appear in the text: [f1, f2, f3, f3,...]. Then we insert framed discourse markers into the sequence by placing them in their appearance order. Thus, we obtain a sequence like [f1, dis1, f2, f3, dis2,...]. Note that discourse markers do not necessarily exist between all semantic frames. Moreover, we treat the periodic symbol as a special discourse marker called \"o.\" Since some sentences contain more than one semantic frame (situations such as clauses), we obtain the final semantic sequence like this: [f1, dis1, f2, o, f3, o, dis2,...]."}, {"heading": "3.3 Entity-Centered SemLM", "text": "We generate semantic sequences using co-reference chains for entity-centered SemLMs. From the co-reference resolution, we can obtain a sequence such as [m1, m2, m3,...] in which mentions appear in the order in which they occur in the text. Each mention can be associated with an argument within a semantic framework. Thus, we replace each mention with its argument designation within a semantic framework and get [fa1, fa2, fa3,...]. We then add discourse markers exactly as we do for entity-centered SemLMs, and get the following sequence: [fa1, dis1, fa2, fa3, dis2,...] Comparison of vocabularies between frame chain and entity-centered SemLMs is summarized in Table 1."}, {"heading": "4 Implementations of SemLM", "text": "In this paper, we experiment with four language models: N-gram (NG), Skip-Gram (SG), Continuous Bag-of-Words (CBOW), and Log-bilinear (LB) language model. For a simple explanation, we assume that a semantic unit sequence is s = [w1, w2, w3,.., wk]."}, {"heading": "4.1 N-gram Model", "text": "For an n-gram model, we predict each token based on its n \u2212 1 previous tokens, i.e. we directly model the following conditional probability (in practice, we choose n = 3, Tri-gram (TRI)): p (wt + 2 | wt, wt + 1). Then, the probability of the sequence isp (s) = p (w1) p (w2 | w1) k \u2212 2 \u0445 t = 1p (wt + 2 | wt, wt + 1).To calculate p (w2 | w1) and p (w1), we must revert from tri-gram to bi-gram and Uni-gram."}, {"heading": "4.2 Skip-Gram Model", "text": "The SG model was proposed in Mikolov et al. (2013b). It uses a token to predict its context, i.e. we model the following conditional probability: p (c-c (wt) | wt, \u03b8).Here c (wt) is the context for wt and \u03b8 denotes the learned parameters, which include neural network states and embeddings. Subsequently, the probability of the sequence is calculated with ask-t = 1-c-c (wt) p (c-wt, \u03b8)."}, {"heading": "4.3 Continuous Bag-of-Words Model", "text": "In contrast to Skip-gram, CBOW (Mikolov et al., 2013a) uses context to predict each token, i.e. we model the following conditional probability: p (wt | c (wt), \u03b8). In this case, the probability of the sequence isk \u0445t = 1p (wt | c (wt), \u03b8)."}, {"heading": "4.4 Log-bilinear Model", "text": "LB was introduced in Mnih and Hinton (2007). Similar to CBOW, it also uses context to predict each character, but LB associates a symbol with three components instead of just one vector: a target vector v (w), a context vector v '(w), and a bias b (w), so the conditional probability becomes: p (wt | c (wt)) = exp (v (wt) u (c (wt)) + b (wt))."}, {"heading": "5 Building SemLMs from Scratch", "text": "In this section, we explain how to build SemLMs from uncommented plaintext."}, {"heading": "5.1 Dataset and Preprocessing", "text": "Dataset We use the New York Times Corpus2 (from 1987 to 2007) for training. It contains a total of just over 1.8 million documents. Pre-processing We process all documents with semantic role designation (Punyakanok et al., 2004) and part-of-speech tagger (Roth and Zelenko, 1998). We also implement the explicit discourse-linking identification module in shallow discourse sparing (Song et al., 2015). Additionally, we use within the document entity co-reference (Peng et al., 2015a) to generate correlation chains. To get all annotations, we use the Illinois NLP tools3."}, {"heading": "5.2 Semantic Unit Generation", "text": "This year it is more than ever before."}, {"heading": "5.3 Language Model Training", "text": "NG We implement the N-gram model using the SRILM toolkit (Stolcke, 2002) and use the well-known KneserNey smoothing technique (Kneser and Ney, 1995).SG & CBOW We use the word2vec package to implement both SG and CBOW. In practice, we set the context window size for SG to 10, while we set the number for CBOW (both are common settings for syntactic language models) to 5. We create 300-dimensional embeddings for both models. LB We use the OxLM toolkit (Paul et al., 2014) with Noise-Constrastive Estimation (Gutmann and Hyvarins, 2010) for the LB model. We set the context window size to 5 and produce 150-dimensional embeddings."}, {"heading": "6 Evaluation", "text": "In this section, we first evaluate the quality of the SemLMs through helplessness and a narrative cloze test. More importantly, we show that the proposed SemLMs can help improve the performance of coreference resolution and shallow discourse sparse. It also proves that we successfully capture semantic sequence information that can potentially benefit a wide range of semantically related NLP tasks. We have developed two models for SemLM: Frame Chain (FC) and Entity-Centred (EC). By training on both types of sequences, we each implement four different language models: TRI, SG, CBOW, LB. We focus the evaluation efforts on these eight SemLMs."}, {"heading": "6.1 Quality Evaluation of SemLMs", "text": "We use three sets of data. Initially, we sample 10% of the New York Times Corpus documents (approximately two years of data), which are called NYT hold-out data. All of our SemLMs are trained on the remaining NYT data and tested on that hold-out data. We generate semantic sequences for the training and test data using the methodology described in Section 5.2. We use PropBank data with frame chains as a further test set. In this case, we use SemLM sequences with frame chains. Likewise, we use Ontonotes data (Hovy et al., 2006) with semantic generation techniques for gold frames and co-reference annotations as described in Section 5.2. When testing Gold PropBank data with frame chains, we use SemLMs with frame chains formed from all NYT data. Similarly, we use Gold Ontonotes data (Ontonotes with Data with Frame Chains 2006, third and third)."}, {"heading": "6.1.1 Perplexity", "text": "Since SemLMs are language models, it is natural to evaluate helplessness, which is a measure of how well a language model can predict sequences. SemLM helplessness results are presented in Table 3, and are calculated without taking end-token (EOS) into account. We apply the three gram KneserNey Smoothing to CBOW, BI, and LB. LB consistently shows the least helplessness for both frame chain and holistic SemLMs acrossall test sets. Similar to syntactic language models, helplessness decreases rapidly from UNI, BI to TRI. In addition, CBOW and SG have very close perplexity results, indicating that their language modeling capabilities are on the same level. We can compare the results of our frame chain SemLM to NYT hold-out data and Gold PropBank Data with frame chains (frame chains), where frames-ontones and net interferences do not interact with each other during some of the chains of the chains of the EK."}, {"heading": "6.1.2 Narrative Cloze Test", "text": "We follow the Narrative Cloze Test idea in Script Learning (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009). As Rudinger et al. (2015) shows, the Narrative Cloze Test can be considered a language modeling evaluation. In the Narrative Cloze Test, we randomly use a token from each semantic sequence in the test series. We then use language models to predict the missing token and evaluate its correctness. For all SemLMs that are defined in Sec. 4 to obtain token predictions. We also use ordered PMI as an additional baseline. The Narrative Cloze Test is performed in the same way as the perplexity evaluation. We use means reciprocessional rank (MRR) and recall at 30 (recall the results are in Table 4)."}, {"heading": "6.2 Evaluation of SemLM Applications", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1 Co-reference Resolution", "text": "To improve their performance, we integrate SemLM information as features into an existing co-reference resolution system. We choose the state-of-art Illinois co-reference resolution system (Peng et al., 2015a) as our base system. It deals with a monitored joint mention recognition and co-referencing recognition features. There are a pair of mentions (m1, m2) where m1 appears, we first extract the appropriate semantic framework and the argumentation role label of each mention. We do this by following the procedures in Sec. 5. So we can get a pair of semantic frames with arguments (fa1, m2)."}, {"heading": "6.2.2 Shallow Discourse Parsing", "text": "To show that SemLM can help improve the analysis of the shallow discourse, we evaluate the identification of the right sense of the discourse connections (both explicit and implicit). We choose Song et al. (2015), which uses a monitored pipeline approach as our base system, which extracts contextual features for potential discourse connections and applies the discourse connection sense classifier. Let's consider an explicit connecting \"dis\"; we extract the semantic frames closest to it (left and right), which leads to the sequence [f1, dis, f2] by following the procedures described in Sec. We then add the following conditional probabilities as characteristics. Computeqc = p (dis | f1, f2).and, similar to what we do for co-reference resolution, we add."}, {"heading": "7 Conclusion", "text": "The work builds two types of discourse-driven semantic language models with four different language models that use neural embedding for semantic frames. We use helplessness and a narrative cloze test to prove that the proposed SemLMs have a good level of abstraction and are of high quality, and then successfully apply them to the two challenging tasks of correlation resolution and shallow discourse parsing, with improvements over modern systems. In the future, we plan to apply SemLMs to other semantic NLP tasks, such as machine translation and answering questions."}, {"heading": "Acknowledgments", "text": "The authors thank Christos Christodoulopoulos and Eric Horn for comments that have helped to improve this work, which is supported by the contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the US Government. This material is also based on work supported by the US Department of Homeland Security under the reference number 2009- ST-061-CCI002-07."}], "references": [{"title": "The berkeley framenet project", "author": ["C.F. Baker", "C.J. Fillmore", "J.B. Lowe."], "venue": "COLING/ACL, pages 86\u201390.", "citeRegEx": "Baker et al\\.,? 1998", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Generating coherent event schemas at scale", "author": ["N. Balasubramanian", "S. Soderland", "Mausam", "O. Etzioni."], "venue": "EMNLP, pages 1721\u20131731.", "citeRegEx": "Balasubramanian et al\\.,? 2013", "shortCiteRegEx": "Balasubramanian et al\\.", "year": 2013}, {"title": "Unsupervised discovery of biographical structure from text", "author": ["D. Bamman", "N.A. Smith."], "venue": "TACL, 2:363\u2013376.", "citeRegEx": "Bamman and Smith.,? 2014", "shortCiteRegEx": "Bamman and Smith.", "year": 2014}, {"title": "Unsupervised discovery of event scenarios from texts", "author": ["C.A. Bejan."], "venue": "FLAIRS Conference, pages 124\u2013129.", "citeRegEx": "Bejan.,? 2008", "shortCiteRegEx": "Bejan.", "year": 2008}, {"title": "Understanding the value of features for coreference resolution", "author": ["E. Bengtson", "D. Roth."], "venue": "EMNLP.", "citeRegEx": "Bengtson and Roth.,? 2008", "shortCiteRegEx": "Bengtson and Roth.", "year": 2008}, {"title": "Jointly combining implicit constraints improves temporal ordering", "author": ["N. Chambers", "D. Jurafsky."], "venue": "EMNLP.", "citeRegEx": "Chambers and Jurafsky.,? 2008a", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2008}, {"title": "Unsupervised learning of narrative event chains", "author": ["N. Chambers", "D. Jurafsky."], "venue": "ACL, volume 94305, pages 789\u2013797.", "citeRegEx": "Chambers and Jurafsky.,? 2008b", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2008}, {"title": "Unsupervised learning of narrative schemas and their participants", "author": ["N. Chambers", "D. Jurafsky."], "venue": "ACL, volume 2, pages 602\u2013610.", "citeRegEx": "Chambers and Jurafsky.,? 2009", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2009}, {"title": "Event schema induction with a probabilistic entity-driven model", "author": ["N. Chambers."], "venue": "EMNLP, volume 13, pages 1797\u20131807.", "citeRegEx": "Chambers.,? 2013", "shortCiteRegEx": "Chambers.", "year": 2013}, {"title": "Probabilistic frame induction", "author": ["J.C.K. Cheung", "H. Poon", "L. Vanderwende."], "venue": "arXiv:1302.4813.", "citeRegEx": "Cheung et al\\.,? 2013", "shortCiteRegEx": "Cheung et al\\.", "year": 2013}, {"title": "First-order probabilistic models for coreference resolution", "author": ["A. Culotta", "M. Wick", "R. Hall", "A. McCallum."], "venue": "NAACL.", "citeRegEx": "Culotta et al\\.,? 2007", "shortCiteRegEx": "Culotta et al\\.", "year": 2007}, {"title": "A unified bayesian model of scripts, frames and language", "author": ["Francis Ferraro", "Benjamin Van Durme."], "venue": "AAAI.", "citeRegEx": "Ferraro and Durme.,? 2016", "shortCiteRegEx": "Ferraro and Durme.", "year": 2016}, {"title": "A hierarchical bayesian model for unsupervised induction of script knowledge", "author": ["L. Frermann", "I. Titov", "Pinkal. M."], "venue": "EACL.", "citeRegEx": "Frermann et al\\.,? 2014", "shortCiteRegEx": "Frermann et al\\.", "year": 2014}, {"title": "What happens next? event prediction using a compositional neural network model", "author": ["M. Granroth-Wilding", "S. Clark", "M.T. Llano", "R. Hepworth", "S. Colton", "J. Gow", "J. Charnley", "N. Lavra\u010d", "M. \u017dnidar\u0161i\u010d"], "venue": "Perovs\u030cek", "citeRegEx": "Granroth.Wilding et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Granroth.Wilding et al\\.", "year": 2015}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyvarinen."], "venue": "AISTATS.", "citeRegEx": "Gutmann and Hyvarinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyvarinen.", "year": 2010}, {"title": "Ontonotes: The 90% solution", "author": ["E. Hovy", "M. Marcus", "M. Palmer", "L. Ramshaw", "R. Weischedel."], "venue": "Proceedings of HLT/NAACL.", "citeRegEx": "Hovy et al\\.,? 2006", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Narrative schema as world knowledge for coreference resolution", "author": ["J. Irwin", "M. Komachi", "Y. Matsumoto."], "venue": "CoNLL Shared Task, pages 86\u201392.", "citeRegEx": "Irwin et al\\.,? 2011", "shortCiteRegEx": "Irwin et al\\.", "year": 2011}, {"title": "Skip n-grams and ranking functions for predicting script events", "author": ["B. Jans", "S. Bethard", "I. Vuli\u0107", "M.F. Moens."], "venue": "EACL, pages 336\u2013344.", "citeRegEx": "Jans et al\\.,? 2012", "shortCiteRegEx": "Jans et al\\.", "year": 2012}, {"title": "From Treebank to PropBank", "author": ["P. Kingsbury", "M. Palmer."], "venue": "Proceedings of LREC-2002.", "citeRegEx": "Kingsbury and Palmer.,? 2002", "shortCiteRegEx": "Kingsbury and Palmer.", "year": 2002}, {"title": "Improved backing-off for m-gram language modeling", "author": ["R. Kneser", "H. Ney."], "venue": "ICASSP.", "citeRegEx": "Kneser and Ney.,? 1995", "shortCiteRegEx": "Kneser and Ney.", "year": 1995}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean."], "venue": "arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "NAACL.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton."], "venue": "ICML, pages 641\u2013648.", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Inducing neural models of script knowledge", "author": ["A. Modi", "I. Titov."], "venue": "CoNLL.", "citeRegEx": "Modi and Titov.,? 2014a", "shortCiteRegEx": "Modi and Titov.", "year": 2014}, {"title": "Learning semantic script knowledge with event embeddings", "author": ["A. Modi", "I. Titov."], "venue": "ICLR Workshop.", "citeRegEx": "Modi and Titov.,? 2014b", "shortCiteRegEx": "Modi and Titov.", "year": 2014}, {"title": "Learning schemata for natural language processing", "author": ["R. Mooney", "G. DeJong"], "venue": null, "citeRegEx": "Mooney and DeJong.,? \\Q1985\\E", "shortCiteRegEx": "Mooney and DeJong.", "year": 1985}, {"title": "Generative event schema induction with entity disambiguation", "author": ["K.-H. Nguyen", "X. Tannier", "O. Ferret", "R. Besan\u00e7on."], "venue": "ACL.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "The ace evaluation plan", "author": ["US NIST."], "venue": "US National Institute for Standards and Technology (NIST).", "citeRegEx": "NIST.,? 2004", "shortCiteRegEx": "NIST.", "year": 2004}, {"title": "Oxlm: A neural language modelling framework for machine translation", "author": ["B. Paul", "B. Phil", "H. Hieu."], "venue": "The Prague Bulletin of Mathematical Linguistics, 102(1):81\u201392.", "citeRegEx": "Paul et al\\.,? 2014", "shortCiteRegEx": "Paul et al\\.", "year": 2014}, {"title": "A joint framework for coreference resolution and mention head detection", "author": ["H. Peng", "K. Chang", "D. Roth."], "venue": "CoNLL.", "citeRegEx": "Peng et al\\.,? 2015a", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Solving hard coreference problems", "author": ["H. Peng", "D. Khashabi", "D. Roth."], "venue": "NAACL.", "citeRegEx": "Peng et al\\.,? 2015b", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Statistical script learning with multi-argument events", "author": ["K. Pichotta", "R.J. Mooney."], "venue": "EACL, volume 14, pages 220\u2013229.", "citeRegEx": "Pichotta and Mooney.,? 2014", "shortCiteRegEx": "Pichotta and Mooney.", "year": 2014}, {"title": "Learning statistical scripts with lstm recurrent neural networks", "author": ["K. Pichotta", "R.J. Mooney."], "venue": "AAAI.", "citeRegEx": "Pichotta and Mooney.,? 2016", "shortCiteRegEx": "Pichotta and Mooney.", "year": 2016}, {"title": "CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes", "author": ["S. Pradhan", "A. Moschitti", "N. Xue", "O. Uryupina", "Y. Zhang."], "venue": "CoNLL.", "citeRegEx": "Pradhan et al\\.,? 2012", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "The penn discourse treebank 2.0", "author": ["Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind Joshi", "Bonnie Webber"], "venue": "In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Semantic role labeling via integer linear programming inference", "author": ["V. Punyakanok", "D. Roth", "W. Yih", "D. Zimak."], "venue": "COLING.", "citeRegEx": "Punyakanok et al\\.,? 2004", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2004}, {"title": "Coreference resolution with world knowledge", "author": ["A. Rahman", "V. Ng."], "venue": "ACL.", "citeRegEx": "Rahman and Ng.,? 2011", "shortCiteRegEx": "Rahman and Ng.", "year": 2011}, {"title": "Part of speech tagging using a network of linear separators", "author": ["D. Roth", "D. Zelenko."], "venue": "COLINGACL.", "citeRegEx": "Roth and Zelenko.,? 1998", "shortCiteRegEx": "Roth and Zelenko.", "year": 1998}, {"title": "Script induction as language modeling", "author": ["R. Rudinger", "P. Rastogi", "F. Ferraro", "B. Van Durme."], "venue": "EMNLP.", "citeRegEx": "Rudinger et al\\.,? 2015", "shortCiteRegEx": "Rudinger et al\\.", "year": 2015}, {"title": "Scripts, plans, goals, and understanding: An inquiry into human knowledge structures", "author": ["R.C. Schank", "R.P. Abelson."], "venue": "JMZ.", "citeRegEx": "Schank and Abelson.,? 1977", "shortCiteRegEx": "Schank and Abelson.", "year": 1977}, {"title": "Verbnet: A broad-coverage, comprehensive verb lexicon", "author": ["K.K. Schuler"], "venue": null, "citeRegEx": "Schuler.,? \\Q2005\\E", "shortCiteRegEx": "Schuler.", "year": 2005}, {"title": "Improving a pipeline architecture for shallow discourse parsing", "author": ["Y. Song", "H. Peng", "P. Kordjamshidi", "M. Sammons", "D. Roth."], "venue": "CoNLL Shared Task.", "citeRegEx": "Song et al\\.,? 2015", "shortCiteRegEx": "Song et al\\.", "year": 2015}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["A. Stolcke."], "venue": "INTERSPEECH, volume 2002, page 2002.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Unsupervised induction of semantic roles within a reconstruction-error minimization framework", "author": ["I. Titov", "E. Khoddam."], "venue": "NAACL.", "citeRegEx": "Titov and Khoddam.,? 2015", "shortCiteRegEx": "Titov and Khoddam.", "year": 2015}, {"title": "Automatically identifying the arguments of discourse connectives", "author": ["Ben Wellner", "James Pustejovsky."], "venue": "Proceedings of the 2007 Joint Conference of EMNLP-CoNLL.", "citeRegEx": "Wellner and Pustejovsky.,? 2007", "shortCiteRegEx": "Wellner and Pustejovsky.", "year": 2007}, {"title": "Understanding natural language", "author": ["T. Winograd."], "venue": "Cognitive psychology, 3(1):1\u2013191.", "citeRegEx": "Winograd.,? 1972", "shortCiteRegEx": "Winograd.", "year": 1972}, {"title": "Learning anaphoricity and antecedent ranking features for coreference resolution", "author": ["S. Wiseman", "A.M. Rush", "S.M. Shieber", "J. Weston."], "venue": "ACL.", "citeRegEx": "Wiseman et al\\.,? 2015", "shortCiteRegEx": "Wiseman et al\\.", "year": 2015}, {"title": "The conll-2015 shared task on shallow discourse parsing", "author": ["N. Xue", "H.T. Ng", "S. Pradhan", "R.P.C. Bryant", "A.T. Rutherford."], "venue": "CoNLL.", "citeRegEx": "Xue et al\\.,? 2015", "shortCiteRegEx": "Xue et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 45, "context": "At each level, capturing meaning frequently requires context sensitive abstraction and disambiguation, as shown in the following example (Winograd, 1972):", "startOffset": 137, "endOffset": 153}, {"referenceID": 21, "context": "For both models of SemLM, we study four language model implementations: N-gram, skipgram (Mikolov et al., 2013b), continuous bagof-words (Mikolov et al.", "startOffset": 89, "endOffset": 112}, {"referenceID": 20, "context": ", 2013b), continuous bagof-words (Mikolov et al., 2013a) and log-bilinear language model (Mnih and Hinton, 2007).", "startOffset": 33, "endOffset": 56}, {"referenceID": 22, "context": ", 2013a) and log-bilinear language model (Mnih and Hinton, 2007).", "startOffset": 41, "endOffset": 64}, {"referenceID": 6, "context": "We also follow the script learning literature (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Rudinger et al., 2015) and evaluate on the narrative cloze test, i.", "startOffset": 46, "endOffset": 128}, {"referenceID": 7, "context": "We also follow the script learning literature (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Rudinger et al., 2015) and evaluate on the narrative cloze test, i.", "startOffset": 46, "endOffset": 128}, {"referenceID": 38, "context": "We also follow the script learning literature (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Rudinger et al., 2015) and evaluate on the narrative cloze test, i.", "startOffset": 46, "endOffset": 128}, {"referenceID": 18, "context": "We conduct both evaluations on two test sets: a hold-out dataset from the New York Times Corpus and gold sequence data (for frame-chain SemLMs, we use PropBank (Kingsbury and Palmer, 2002); for entitycentered SemLMs, we use Ontonotes (Hovy et al.", "startOffset": 160, "endOffset": 188}, {"referenceID": 15, "context": "We conduct both evaluations on two test sets: a hold-out dataset from the New York Times Corpus and gold sequence data (for frame-chain SemLMs, we use PropBank (Kingsbury and Palmer, 2002); for entitycentered SemLMs, we use Ontonotes (Hovy et al., 2006) ).", "startOffset": 234, "endOffset": 253}, {"referenceID": 39, "context": "Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to construct knowledge bases from documents to learn scripts.", "startOffset": 12, "endOffset": 63}, {"referenceID": 25, "context": "Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to construct knowledge bases from documents to learn scripts.", "startOffset": 12, "endOffset": 63}, {"referenceID": 5, "context": "Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016).", "startOffset": 111, "endOffset": 258}, {"referenceID": 3, "context": "Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016).", "startOffset": 111, "endOffset": 258}, {"referenceID": 17, "context": "Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016).", "startOffset": 111, "endOffset": 258}, {"referenceID": 31, "context": "Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016).", "startOffset": 111, "endOffset": 258}, {"referenceID": 13, "context": "Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016).", "startOffset": 111, "endOffset": 258}, {"referenceID": 32, "context": "Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016).", "startOffset": 111, "endOffset": 258}, {"referenceID": 8, "context": "Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al.", "startOffset": 64, "endOffset": 197}, {"referenceID": 9, "context": "Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al.", "startOffset": 64, "endOffset": 197}, {"referenceID": 9, "context": "Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al.", "startOffset": 64, "endOffset": 197}, {"referenceID": 1, "context": "Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al.", "startOffset": 64, "endOffset": 197}, {"referenceID": 2, "context": "Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al.", "startOffset": 64, "endOffset": 197}, {"referenceID": 26, "context": "Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al.", "startOffset": 64, "endOffset": 197}, {"referenceID": 24, "context": ", 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015).", "startOffset": 59, "endOffset": 153}, {"referenceID": 23, "context": ", 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015).", "startOffset": 59, "endOffset": 153}, {"referenceID": 12, "context": ", 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015).", "startOffset": 59, "endOffset": 153}, {"referenceID": 43, "context": ", 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015).", "startOffset": 59, "endOffset": 153}, {"referenceID": 1, "context": ", 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015). Ferraro and Van Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence.", "startOffset": 8, "endOffset": 259}, {"referenceID": 7, "context": "In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009).", "startOffset": 99, "endOffset": 128}, {"referenceID": 16, "context": "Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b).", "startOffset": 75, "endOffset": 136}, {"referenceID": 36, "context": "Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b).", "startOffset": 75, "endOffset": 136}, {"referenceID": 30, "context": "Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b).", "startOffset": 75, "endOffset": 136}, {"referenceID": 18, "context": "The design of PropBank frames (Kingsbury and Palmer, 2002) and FrameNet frames (Baker et al.", "startOffset": 30, "endOffset": 58}, {"referenceID": 0, "context": "The design of PropBank frames (Kingsbury and Palmer, 2002) and FrameNet frames (Baker et al., 1998) perfectly fits our needs.", "startOffset": 79, "endOffset": 99}, {"referenceID": 34, "context": "We get the full list from the Penn Discourse Treebank (Prasad et al., 2008) and include them as part of our vocabulary for SemLMs.", "startOffset": 54, "endOffset": 75}, {"referenceID": 44, "context": "More importantly, discourse markers are associated with arguments (Wellner and Pustejovsky, 2007) in text (usually two sentences/clauses, sometimes one).", "startOffset": 66, "endOffset": 97}, {"referenceID": 20, "context": "The SG model was proposed in Mikolov et al. (2013b). It uses a token to predict its context, i.", "startOffset": 29, "endOffset": 52}, {"referenceID": 20, "context": "In contrast to skip-gram, CBOW (Mikolov et al., 2013a) uses context to predict each token, i.", "startOffset": 31, "endOffset": 54}, {"referenceID": 22, "context": "LB was introduced in Mnih and Hinton (2007). Similar to CBOW, it also uses context to predict each token.", "startOffset": 21, "endOffset": 44}, {"referenceID": 35, "context": "Preprocessing We pre-process all documents with semantic role labeling (Punyakanok et al., 2004) and part-of-speech tagger (Roth and Zelenko, 1998).", "startOffset": 71, "endOffset": 96}, {"referenceID": 37, "context": ", 2004) and part-of-speech tagger (Roth and Zelenko, 1998).", "startOffset": 34, "endOffset": 58}, {"referenceID": 41, "context": "We also implement the explicit discourse connective identification module in shallow discourse parsing (Song et al., 2015).", "startOffset": 103, "endOffset": 122}, {"referenceID": 29, "context": "Additionally, we utilize within document entity coreference (Peng et al., 2015a) to produce coreference chains.", "startOffset": 60, "endOffset": 80}, {"referenceID": 40, "context": "As the Illinois SRL package is built upon PropBank frames, we do a mapping to FrameNet frames via VerbNet senses (Schuler, 2005), thus achieving a higher level of abstraction.", "startOffset": 113, "endOffset": 128}, {"referenceID": 42, "context": "NG We implement the N-gram model using the SRILM toolkit (Stolcke, 2002).", "startOffset": 57, "endOffset": 72}, {"referenceID": 19, "context": "We also employ the well-known KneserNey Smoothing (Kneser and Ney, 1995) technique.", "startOffset": 50, "endOffset": 72}, {"referenceID": 28, "context": "LB We use the OxLM toolkit (Paul et al., 2014) with Noise-Constrastive Estimation (Gutmann and Hyvarinen, 2010) for the LB model.", "startOffset": 27, "endOffset": 46}, {"referenceID": 14, "context": ", 2014) with Noise-Constrastive Estimation (Gutmann and Hyvarinen, 2010) for the LB model.", "startOffset": 43, "endOffset": 72}, {"referenceID": 15, "context": "Similarly, we use Ontonotes data (Hovy et al., 2006) with gold frame and co-reference annotations as the third test set, Gold Ontonotes Data with Coref Chains.", "startOffset": 33, "endOffset": 52}, {"referenceID": 17, "context": "We use the ordered PMI (OP) as our baseline, which is a variation of PMI by considering asymmetric counting (Jans et al., 2012).", "startOffset": 108, "endOffset": 127}, {"referenceID": 6, "context": "We follow the Narrative Cloze Test idea used in script learning (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009).", "startOffset": 64, "endOffset": 123}, {"referenceID": 7, "context": "We follow the Narrative Cloze Test idea used in script learning (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009).", "startOffset": 64, "endOffset": 123}, {"referenceID": 5, "context": "We follow the Narrative Cloze Test idea used in script learning (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009). As Rudinger et al. (2015) points out, the narrative cloze test can be regarded as a language modeling evaluation.", "startOffset": 65, "endOffset": 151}, {"referenceID": 38, "context": "8% Rudinger et al. (2015) 0.", "startOffset": 3, "endOffset": 26}, {"referenceID": 38, "context": "This finding is also reflected in the results reported in Rudinger et al. (2015). Though CBOW and SG have similar perplexity results, SG appears to be stronger in the narrative cloze test.", "startOffset": 58, "endOffset": 81}, {"referenceID": 38, "context": "We cannot directly compare with other related works (Rudinger et al., 2015; Pichotta and Mooney, 2016) because of the differences in data and evaluation metrics.", "startOffset": 52, "endOffset": 102}, {"referenceID": 32, "context": "We cannot directly compare with other related works (Rudinger et al., 2015; Pichotta and Mooney, 2016) because of the differences in data and evaluation metrics.", "startOffset": 52, "endOffset": 102}, {"referenceID": 31, "context": ", 2015; Pichotta and Mooney, 2016) because of the differences in data and evaluation metrics. Rudinger et al. (2015) also use the NYT portion of the Gigaword corpus, but with Concrete annotations; Pichotta and Mooney (2016) use the English Wikipedia as their data, and Stanford NLP tools for pre-processing while we use the Illinois NLP tools.", "startOffset": 8, "endOffset": 117}, {"referenceID": 31, "context": ", 2015; Pichotta and Mooney, 2016) because of the differences in data and evaluation metrics. Rudinger et al. (2015) also use the NYT portion of the Gigaword corpus, but with Concrete annotations; Pichotta and Mooney (2016) use the English Wikipedia as their data, and Stanford NLP tools for pre-processing while we use the Illinois NLP tools.", "startOffset": 8, "endOffset": 224}, {"referenceID": 46, "context": "We outperform the state-of-art system (Wiseman et al., 2015), which reports the best results on CoNLL12 dataset.", "startOffset": 38, "endOffset": 60}, {"referenceID": 29, "context": "39 Base (Peng et al., 2015a) 71.", "startOffset": 8, "endOffset": 28}, {"referenceID": 38, "context": "Rudinger et al. (2015) does share a common evaluation metric with us: MRR.", "startOffset": 0, "endOffset": 23}, {"referenceID": 38, "context": "Rudinger et al. (2015) does share a common evaluation metric with us: MRR. If we ignore the data difference and make a rough comparison, we find that the absolute values of our results are better while Rudinger et al. (2015) have higher relative improvement (\u201cRel-Impr\u201d in Table 4).", "startOffset": 0, "endOffset": 225}, {"referenceID": 29, "context": "We choose the state-of-art Illinois Co-reference Resolution system (Peng et al., 2015a) as our base system.", "startOffset": 67, "endOffset": 87}, {"referenceID": 27, "context": "We evaluate the effect of the added SemLM features on two co-reference benchmark datasets: ACE04 (NIST, 2004) and CoNLL12 (Pradhan et al.", "startOffset": 97, "endOffset": 109}, {"referenceID": 33, "context": "We evaluate the effect of the added SemLM features on two co-reference benchmark datasets: ACE04 (NIST, 2004) and CoNLL12 (Pradhan et al., 2012).", "startOffset": 122, "endOffset": 144}, {"referenceID": 10, "context": "We use the standard split of 268 training documents, 68 development documents, and 106 testing documents for ACE04 data (Culotta et al., 2007; Bengtson and Roth, 2008).", "startOffset": 120, "endOffset": 167}, {"referenceID": 4, "context": "We use the standard split of 268 training documents, 68 development documents, and 106 testing documents for ACE04 data (Culotta et al., 2007; Bengtson and Roth, 2008).", "startOffset": 120, "endOffset": 167}, {"referenceID": 46, "context": "By employing log-bilinear model embeddings, we further improve the numbers and we outperform the best reported results on the CoNLL12 dataset (Wiseman et al., 2015).", "startOffset": 142, "endOffset": 164}, {"referenceID": 41, "context": "Base (Song et al., 2015) 89.", "startOffset": 5, "endOffset": 24}, {"referenceID": 41, "context": "We choose Song et al. (2015), which uses a supervised pipeline approach, as our base system.", "startOffset": 10, "endOffset": 29}, {"referenceID": 47, "context": "We evaluate on CoNLL16 (Xue et al., 2015) test and blind sets, following the train and development document split from the Shared Task, and report F1 using the official shared task scorer.", "startOffset": 23, "endOffset": 41}], "year": 2016, "abstractText": "Natural language understanding often requires deep semantic knowledge. Expanding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction. We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities. For each model, we investigate four implementations: a \u201cstandard\u201d N-gram language model and three discriminatively trained \u201cneural\u201d language models that generate embeddings for semantic frames. The quality of the semantic language models (SemLM) is evaluated both intrinsically, using perplexity and a narrative cloze test and extrinsically \u2013 we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}