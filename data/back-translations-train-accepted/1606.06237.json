{"id": "1606.06237", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Online and Differentially-Private Tensor Decomposition", "abstract": "Tensor decomposition is positioned to be a pervasive tool in the era of big data. In this paper, we resolve many of the key algorithmic questions regarding robustness, memory efficiency, and differential privacy of tensor decomposition. We propose simple variants of the tensor power method which enjoy these strong properties. We present the first guarantees for online tensor power method which has a linear memory requirement. Moreover, we present a noise calibrated tensor power method with efficient privacy guarantees. At the heart of all these guarantees lies a careful perturbation analysis derived in this paper which improves up on the existing results significantly.", "histories": [["v1", "Mon, 20 Jun 2016 18:30:10 GMT  (205kb,D)", "https://arxiv.org/abs/1606.06237v1", "20 pages, 9 figures"], ["v2", "Sat, 2 Jul 2016 22:30:01 GMT  (205kb,D)", "http://arxiv.org/abs/1606.06237v2", "20 pages, 9 figures"], ["v3", "Sun, 30 Oct 2016 21:56:58 GMT  (209kb,D)", "http://arxiv.org/abs/1606.06237v3", "19 pages, 9 figures. To appear at the 30th Annual Conference on Advances in Neural Information Processing Systems (NIPS 2016), to be held at Barcelona, Spain"], ["v4", "Thu, 15 Dec 2016 13:35:22 GMT  (208kb,D)", "http://arxiv.org/abs/1606.06237v4", "19 pages, 9 figures. To appear at the 30th Annual Conference on Advances in Neural Information Processing Systems (NIPS 2016), to be held at Barcelona, Spain. Fix small typos in proofs of Lemmas C.5 and C.6"]], "COMMENTS": "20 pages, 9 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yining wang", "anima anandkumar"], "accepted": true, "id": "1606.06237"}, "pdf": {"name": "1606.06237.pdf", "metadata": {"source": "CRF", "title": "Online and Differentially-Private Tensor Decomposition", "authors": ["Yining Wang", "Animashree Anandkumar"], "emails": ["yiningwa@cs.cmu.edu", "a.anandkumar@uci.edu"], "sections": [{"heading": null, "text": "Key words: tensor decomposition, tensor power method, online methods, streaming, differential privacy, fault analysis."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "1.1 Related work", "text": "Online tensor SGD Stochastic Gradient Descent (SGD) is an intuitive approach to online tensor decomposition and has been successful in practice in large-scale tensor decomposition problems [16]. Despite its simplicity, the theoretical properties are particularly difficult to establish. [11] However, the approach in [11] only works for uniform tensors and its sample complexity dependence on tensor dimensions d is poor. Tensor PCA In the statistical tensor PCA [24], a d \u00b7 d tensor T = v 3 + E is observed and one wants to recover component v in the presence of Gaussor dimensions d. [24] shows that the E dimensions (d \u2212 1 / 2) are sufficient to ensure a safe recovery from V dimensions."}, {"heading": "1.2 Notation and Preliminaries", "text": "We use [n] to describe the crowd {1, 2, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11"}, {"heading": "2 Improved Noise Analysis for Tensor Power Method", "text": "If the tensor T has an exact orthogonal decomposition, the power method can demonstrably restore all components with random initialization and deflation. However, the analysis is more subtle and becomes comprehensible only under certain conditions. While matrix fault limits are well understood, it is an open problem in the case of tensors. This is because the problem of tensor decomposition can become intractable again. In [1], guaranteed restoration of components has been demonstrated under certain conditions that the result of decoding of components is detectable, and we will recap the result below.Theorem 2.1 ([1] Theorem 5.1, simplified version)."}, {"heading": "3 Memory-Efficient Streaming Tensor Decomposition", "text": "It is a classic example in which each xi represents documents that can be achieved in streams and consistent estimates of population by displaying variants of population."}, {"heading": "4 Differentially private tensor decomposition", "text": "The aim of private data processing is to publish data summaries, so that no reliable conclusion can be drawn from the published results. Formally, we adopt the popular (\u03b5, \u03b4) -differential criterion of privacy proposed in [9]: Definition 4.1 (\u03b5, \u03b4) -differential privacy [9]. Let M be all symmetrical d-dimensional real tensors and O be an arbitrary output set. A randomized algorithm A: M \u2192 O is (\u03b5, \u03b4) -differential private if for all adjacent tensors T, T \u2032 and measurable theorem O O O we havePr [A). \u2212 O] \u2264 e\u03b5 Pr [A \u2032)."}, {"heading": "11: Deflation: \u03bb\u0302i = \u03bb\u0303(\u03c4", "text": "This is necessary because an arbitrary disturbance of a tensor, even if limited to only one entry, is capable of destroying any warranty of use. In a nutshell, definitions 4.1, 4.2 state that an algorithm A is differentiated private when, due to a number of possible outputs of A, one cannot distinguish with high probability between two \"adjacent\" tensors that differ only in a single entry (up to symmetrical ones), thereby protecting the privacy of a particular element in the original tensor T."}, {"heading": "5 Conclusion", "text": "We look at storage-efficient and differentiated private tensor decomposition issues in this paper and derive efficient algorithms for both online and private tensor decomposition based on the popular tensor power methodology framework. By improving the noise status of the robust tensor power method, we obtain sharper dimensional boundaries of sample complexity for online tensor decomposition and a wider range of privacy parameters for private tensor decomposition while maintaining usefulness. Simulation results basically confirm the density of our noise conditions. An important direction for future research is to expand our online and / or private tensor decomposition algorithms and analytics to practical applications such as topic modeling and community detection, where tensor decomposition is a critical step for data analysis. An end-to-theoretical-to-end analysis of these end-to-end private / end-end-of-use models could also benefit from practical."}, {"heading": "A Simulation results", "text": "We verify our most important theoretical results in Theorem 2.2 on synthetic tensors."}, {"heading": "B Comparison with whitening and matrix SVD decompositions", "text": "Another popular thread of tensor decomposition techniques involves brightening and reducing the problem to a matrix SVD decomposition, which is very effective in reducing the dimensionality of the problem in the k = o (d) undercomplete settings [1, 21, 28]. We show in this section that without additional page information, a standard application and analysis of tensor decomposition of whitening and matrix SVD techniques results in worse error margins than we have in theorem 2.2.2.If only the 3rd order tensor T is available, a common whitening approach is randomly \"marginalized\" a view of tensor decomposition \"a view of T techniques: M (II, IV, IV), randomly drawn to the d-dimensional sphere; and then we evaluate the top-k own decomposition of M (II). Letximan = span span of span IV, span of span of span IV, d d."}, {"heading": "C Proof of Theorem 2.2", "text": "In this section we outline the proof of theorem 2.2. Our proof is largely based on the analysis in [1] of the robust tensor power method. However, we are borrowing new ideas [12] to essentially revise the pro-iteration analysis (Lemma C.2), which subsequently leads to a desired loosening of the noise conditions. Some results and arguments in [1], especially those dealing with absolute constants, are simplified for accessibility. We start with Lemma C1, the random initialization against eigenvectors.Lemma C.1. Fix j."}, {"heading": "D Proof of results for streaming robust tensor power method", "text": "The proof for theorem 3.1. first, it should be noted that if x1, \u00b7 \u00b7 q, xn i.i.d., P, P and SGD (\u03c3), then the distribution of the sample mean x = 1n x i = 1 xi belongs to SGD. To detect this, one must correct an error and show that E [exp (a > x)] = n \u00b2 i = 1 E [exp (a > xi / n)] \u2264 n \u00b2 i = 1 exp (a > 22\u04452 / n2) = exp (a > 22\u04452 / n), the second inequality being due to the fact that xi and i (a > xi / n \u00b2 22 / n2). Under assumptions 3.1, 3.2 and using the arguments above, we know that the second inequality is not due to probability."}, {"heading": "E Proofs of utility results for differentially private tensor decomposition", "text": "Before examining Theorem 4.2, we first present a dilemma that the upper limit cannot be reached if the components V = > Rd \u00b7 k are not contiguous (Assumption 4.1) and the Gaussian noise is not taken into account via the grid updates. (Assume T = = 1, u) and V = (v1, \u00b7 \u00b7 \u00b7, vk) fulfills Assumption 4.1 with the coherence level 0. (Fixu). (Assume T = 1, u) and leave u = T (I, u) + V = 3, where z \u00b2 N (0, Id \u00b7 d) are zero-mean random variables. (Assume) We then have that the lower limit cannot be exceeded. (Assume) We have not reached the lower limit. (Assume) We prove this dilemma by indicating an upper limit for the lower limit and a lower limit for the lower limit. (Assume) We have both with overwhelming probabilities (assumed)."}, {"heading": "F Technical lemmas", "text": "We then have the following answer: F > R is an L-Lipschitz function; that is, f (x) \u2212 f (y) \u2212 f \u2212 j \u2212 x \u2212 y \u2212 2 for all x, y \u2212 Rd. [f (x)].F > R is an L-Lipschitz function; that is, f (x) \u2212 f (x) \u2212 j \u2212 n for all x, y \u2212 Rd. [f (x).F > R is an L-Lipschitz function; that is, we have the thatPr [f (x) \u2212 f (x) \u2212 f (x).M \u2212 n for all x, y \u2212 Rd. [f (x)."}, {"heading": "A\u0303 = A + E be a perturbed version of A and (U\u03031, U\u03032, U\u03033, V\u03031, V\u03032, \u03a3\u03031, \u03a3\u03032) be analogous singular", "text": "Value Decomposition of A. Let us be the matrix of the canonical angles between area (U1) and area (U-1) and the matrix of the canonical angles between area (V1) and area (V-1). If there is such a range, there must be such a range. If there is such a range, there must be such a range. If there is such a range, there must be such a range."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Tensor decomposition is an important tool for big data analysis. In this paper,<lb>we resolve many of the key algorithmic questions regarding robustness, memory<lb>efficiency, and differential privacy of tensor decomposition. We propose simple<lb>variants of the tensor power method which enjoy these strong properties. We present<lb>the first guarantees for online tensor power method which has a linear memory<lb>requirement. Moreover, we present a noise calibrated tensor power method with<lb>efficient privacy guarantees. At the heart of all these guarantees lies a careful<lb>perturbation analysis derived in this paper which improves up on the existing<lb>results significantly.<lb>", "creator": "LaTeX with hyperref package"}}}