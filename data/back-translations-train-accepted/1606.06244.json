{"id": "1606.06244", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Learning in Games: Robustness of Fast Convergence", "abstract": "We show that learning algorithms satisfying a $\\textit{low approximate regret}$ property experience fast convergence to approximate optimality in a large class of repeated games. Our property, which simply requires that each learner has small regret compared to a $(1+\\epsilon)$-multiplicative approximation to the best action in hindsight, is ubiquitous among learning algorithms - it is satisfied even by the vanilla Hedge forecaster. Our results improve upon recent work of Syrgkanis et al. [SALS15] in a number of ways. We improve upon the speed of convergence by a factor of n, the number of players, and require only that the players observe payoffs under other players' realized actions, as opposed to expected payoffs. We further show that convergence occurs with high probability, and under certain conditions show convergence under bandit feedback. Both the scope of settings and the class of algorithms for which our analysis provides fast convergence are considerably broader than in previous work.", "histories": [["v1", "Mon, 20 Jun 2016 18:54:19 GMT  (40kb)", "http://arxiv.org/abs/1606.06244v1", null], ["v2", "Tue, 16 Aug 2016 13:09:10 GMT  (130kb)", "http://arxiv.org/abs/1606.06244v2", null], ["v3", "Wed, 16 Nov 2016 14:55:13 GMT  (32kb)", "http://arxiv.org/abs/1606.06244v3", null], ["v4", "Fri, 16 Dec 2016 20:44:36 GMT  (32kb)", "http://arxiv.org/abs/1606.06244v4", "27 pages. NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.GT cs.LG", "authors": ["dylan j foster", "zhiyuan li", "thodoris lykouris", "karthik sridharan", "\u00e9va tardos"], "accepted": true, "id": "1606.06244"}, "pdf": {"name": "1606.06244.pdf", "metadata": {"source": "CRF", "title": "Fast Convergence of Common Learning Algorithms in Games", "authors": ["Dylan J. Foster", "Zhiyuan Li", "Thodoris Lykouris", "Karthik Sridharan"], "emails": ["djfoster@cs.cornell.edu.", "lizhiyuan13@mails.tsinghua.edu.cn.", "teddlyk@cs.cornell.edu.", "sridharan@cs.cornell.edu.", "eva@cs.cornell.edu."], "sections": [{"heading": null, "text": "ar Xiv: 160 6.06 244v 1 [cs.G T] 20 JuOur framework apply to dynamic population games via a low approximate regret property for shifting experts. At this point, we strengthen the results of [LST16] in two ways: We allow players to select learning algorithms from a larger class that include a minor variant of the basic hedge algorithm, and we increase the maximum exodus of players for whom an approximate level of optimism is achieved. In the bandit environment, we present a novel algorithm that provides a \"small loss limit\" with improved dependence on the number of actions and is both simple and efficient. This result could be of independent interest. \u2022 Cornell University, djfoster @ cs.cornell.edu. Work supported under NSF grant CDS & E-MSS 1521544. \u2020 Tsinghua University, lizhiyuan13 @ mails.tsinghua.edu.nc."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to play by the rules they have applied in the past."}, {"heading": "2 Repeated Games and Learning Dynamics", "text": "We consider a game G among a series of n players. - Each player i has an action space Si and a cost function costi: S1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Sn \u2192 [0, 1], which has an action profile s = (s1,.., sn) at the cost costi (s) experienced by the player. - We assume that the action space of each player has cardinality d, i.e., we let w = (w1,.., wn) specify a list of probability distributions for all actions of the player, where wi (Si) and wi, x is the probability of action x. - Analogous quantities for use maximization games are appended D. We consider the setting in which the game G is played repeatedly for T-time steps. - At any time, each player takes a probability distribution wti (Si) over actions and derives its effect from this distribution."}, {"heading": "2.1 Learning Dynamics", "text": "We assume that players select their actions using a learning algorithm that satisfies a property that we call \"Low Approximate Regret\" = > K = > Regret, which simply requires that the cumulative cost of the learner multiply approximately approximately approximately approximately approximately approximately approximately the cost of the best action that they could have chosen in retrospect approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately the cost of the best action that they could have chosen. We will see in subsequent sections that this property is omnipresent and leads to fast approximation in a robust range of settings. Definition 1. (Low Approximate Regret) A learning algorithm approximately approximates approximately approximately approximately satisfies the property for parameters and function A, if for all action distributions f (Si), (1 \u2212 equative approximation of the population) T, (1 \u2212 quoer) T, which approximates approximately approximately approximates approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately"}, {"heading": "2.2 Smooth Games", "text": "It is well known that in a large class of games referred to by Roughgarden [Rou15] as smooth games, traditional learning dynamics converge to near optimal prosperity. In the following sections, we analyze the convergence of smooth learning dynamics in such smooth games. Before verifying this result, we review social welfare and smooth games. For a specific action profile, the social costs are C (s) = smooth costs. To limit the loss of efficiency due to the selfish behavior of the players, we compare the optimal social costs that can be achieved by some centralized units."}, {"heading": "3 Learning in Games with Full Information Feedback", "text": "We are now analyzing the efficiency of algorithms with the Low Approximate Regret property in our complete informational environment. Our first proposal shows that in smooth games with complete information feedback learners with the Low Approximate Regret property are converging quickly to efficient results. Proposal 1. In every (\u03bb, \u00b5) -smooth game, when all LowApproximate players use Regret algorithms, the Eq. (1) with the parameters Equity and A, then1T \u2211 tE [C (st) amp # amp amp # 160; amp # # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160 amp # 160; amp # 160 amp # 160 amp # 160 amp # 160 amp; amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp; 160 amp # 160 amp # 160 amp; 160 amp # 160 amp # 160 amp; amp; 160 amp # 160 amp; 160 amp # 160 amp # 160 amp; amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; amp # 160 amp # 160 amp; amp # 160 amp # 160 amp # 160 amp; amp # 160 amp # 160 amp; amp # 160 amp; amp; amp # 160 amp; amp # 160"}, {"heading": "3.1 Examples of Simple Low Approximate Regret Algorithms", "text": "One would also hope that such algorithms are relatively simple and easy to find. We now show that the well-known hedge algorithm, as well as basic variants such as Optimistic Hedge and Hedge with online learning rate tuning, satisfy the property with A = O (log d), which will lead to rapid convergence of both n and T. It is important to note that for all the algorithms considered in this paper, we can achieve the Low Approximate Regret Property for each fixed property."}, {"heading": "3.2 Main Result for Full Information Games", "text": "We summarize our results to date into a single theorem on the behavior of players with low remorse that learners expect in complete information games with implemented feedback. Theorem 3. In each (\u03bb, \u00b5) smooth game, if all players achieve approximate reset algorithm satisfaction (1) for parameters 1 and A (d, T) = O (log d), then1T (st) smooth game results, if all players achieve approximate reset algorithm satisfaction (1) for parameters 1 and A (d) = O (log d), with a probability of at least 1 \u2212 3, 1T (st)."}, {"heading": "4 Bandit Feedback", "text": "In many realistic scenarios, the players of a game do not even seem to know what they would have lost or gained if they had deviated from the plot they were playing. (We model this lack of information with feedback in which each player observes a single scalar cost factor to show that players can quickly convert to efficient solutions. (Our results hold with the same generality as in the complete information setting: as long as the learners meet the proposed attribute (1), the complete information setting, in which players can quickly access efficient solutions, is very important. (As long as the learners have the satisfactory attribute), they do not seem to satisfy. (1) Unlike the complete information setting, where most algorithms, hedge, achieves Low Approximate Regret with competing parameters, the most common adarial algorithms do not seem to satisfy."}, {"heading": "5 Dynamic Population Games", "text": "In this section, we look at the dynamic population of repeated game settings introduced in [LST16]. Given the low cost risk described in Section 2, a dynamic population game with Stage GameG is a repetitive game in which a new player is replaced at each step with a fluctuation probability. Specifically, when a player reverses, its strategy and cost function are arbitrarily subjected to the rules of the game. This model of repeated game settings, in which players have to adapt to a changing environment, refers to the cost function of the player i in turn as cost (\u00b7). As in Section 3, we assume that players receive complete information feedback, which is at the end of each round they observe the total cost vector cti = cost i (\u00b7 st \u2212 i), and know their own cost function, but are not aware of the cost of the other players in the game."}, {"heading": "Acknowledgements", "text": "The authors thank Vasilis Syrgkanis for providing his simulation software and for helpful discussions."}, {"heading": "APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Proofs from Section 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Proposition 2", "text": "To obtain the high probability limits, we must use a concentration limit. (<) We will use a refinement of martyrdom-amber inequality due to [BDH + 08], which can be used to obtain highly probable versions of data-dependent repentance limits. (<) We will use a sum of conditional variances for a certain result X1,.., XT is a martyrial difference with a certain result X1,. \u2212 \u2212 \u2212 Let's use 2 = 1 Var (Xt | X1,.,. \u2212 1) the sum of conditional variances for a certain result X1,. \u2212 LT. \u2212 For all other things < 1 / e, T \u2264 4 we have haveP T = 1Xt > 4,."}, {"heading": "A.2 Low Approximate Regret of Specific Algorithms", "text": "In this section we present the evidence for the property Low Approximate Regret for Hedge (Example 1) and Optimistic Hedge (3). The first evidence is given mainly for completeness, but may be helpful as subsequent evidence follows the same framework. The evidence for Optimistic Hedge provides a new analysis that relates the performance of Optimistic Hedge to the performance of Hedge. We omit the evidence for Example 2 and refer the reader instead to Conclusion 2.4 [CBL06]."}, {"heading": "A.2.1 Hedge (Example 1)", "text": "Proposition 12. Hedge with a constant learning rate and a uniform approach fulfills the Low Reasonable Regret property with A (d, T) = Log (d).Hedge with a constant learning rate is an algorithm for online linear optimization via the simplex with the update rule wt + 1% wte \u2212 \u03b7ct. We will write this proof why it fulfills the Low Reasonable Regret property for completeness purposes, as we will use in later proofs. Hedge as an example of online mirror descent with negative entropy regularization R (w) = 1% wp = 1% wp (wi). It is defined by a two-step update rule, which in due time c t: 1. Let us not satisfy w (w) = 1% w (wt)."}, {"heading": "A.2.2 Optimistic Hedge (Example 3)", "text": "Proposition 13: \"Optimistic hedge with constant learning rate\" < < < 1 / 4 fulfills the \"Low Approximate Regret\" property with A (d, T) = 8 log (d). \u2212 \u2212 \u2212 \u2212 \"Optimistic hedge\" is described in two update sequences: \"The first is the hedge update:\" 1t + 1% \"(g) and the second is an alternative that gives more weight to the last step. \u2212 \u2212\" The local standard with respect to w is f = \"T\" 2R (w) f \"and its\" Bregman DivergenceDR \"is the same as the Hessian standard with respect to w\" f \"T\" 2R \"(w) f\" and its dual standard is the \"x\" in the proof of \"Proposition 12.Let\" 2R \"(w)."}, {"heading": "A.3 Proof of Theorem 3", "text": "This theorem is derived from Propositions 1 and 2. Robustness to the opponent is considered 1) we can set the log (d) T, since the Strong LowApproximate Regret property includes ownership for all, and 2) \u2211 t < wti, c t > \u2264 T (the losses are in [0, 1])."}, {"heading": "B Proofs from Section 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proof of Lemma 5", "text": "The proof follows a common scheme for proving the limits of regret in partial information settings (see [AHR08] for a nice discussion): First, we provide a limit of regret for the algorithm in the complete information setting, then we run the algorithm-based unbiased estimator c t for the cost for the partial information setting. An unbiased estimator c \u0442t fulfils Est \u0445 wt = ct, where st is the action selected by the algorithm at step t. We use the known meaning-weighted estimator: c \u0442i (st) = ct iwt i \u00b7 est, where st is the action selected by the algorithm and denotes the ith standard base vector."}, {"heading": "Regret Bound with Estimated Costs", "text": "We will first show that the first three lines of our boundary generally apply to all costs, and the last three are specialized in our estimator. Starting from the standard boundary of the mirror descent, we have this for all t: < wt \u2212 f = 1 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0"}, {"heading": "From Full Information to Partial Information", "text": "As next, note that E [< est \u2212 f, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "C Proofs from Section 5", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Proof of Theorem 7", "text": "At this point, we will outline the proof for Theorem 7, analogous to the proof of Proposition 1. Remember that s * 1: T is a solution sequence that costs at most 1 times the minimum cost, and Ki denotes the number of strategy changes in player i in this order. (1 \u2212 2) \u2211 tE [C (st)] = (1 \u2212 3) \u2211 tE [costi (s t)] \u2264 3 [costi (s, t \u2212 i)] + 1 + E [Ki]. Where the second inequality uses the smoothing property, the first inequality is satisfactory by means of Low Approximate Regret algorithm (2) for each player i against the alternating expert s \u00b2 A (d, T). Where the second inequality uses the smoothing property, the first inequality is satisfactory by means of Low Approximate Regret Algorithm (2)."}, {"heading": "C.2 Proof of Proposition 8", "text": "The rest of the notation will be identical as in the evidence for the low assumed regret for hedge in Appendix A.2.1We will take a similar approach to this evidence. Similar to Equation (8) in each step (1), the equation (8) in each step for each f (d) and the use of this term will be identical as in the evidence for the low assumed regret for hedge in Appendix A.2.1We will take a similar approach to this evidence. Similar to Equation (8) in each step f (d) and the use of this term (1). < wt \u2212 f, ct > = < 1t \u2212 w \u2212 w \u2212 t that the equation (1) in relation to the equation (8) in relation to the equation (d) and the use of this term."}, {"heading": "D Utility Maximization Games and Mechanisms", "text": "In this section, we show how all our results extend to benefit maximization and how benefit maximization affects benefit maximization. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}], "references": [{"title": "Regret bounds and minimax policies under partial monitoring", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Audibert and Bubeck.,? \\Q2010\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2010}, {"title": "Adaptive and self-confident on-line learning algorithms", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Claudio Gentile"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "High-probability regret bounds for bandit online linear optimization", "author": ["Peter L Bartlett", "Varsha Dani", "Thomas Hayes", "Sham Kakade", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In Proceedings of 21st Annual Conference on Learning Theory (COLT),", "citeRegEx": "Bartlett et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2008}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["Nicolo Cesa-Bianchi", "Yishay Mansour", "Gilles Stoltz"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2007}, {"title": "The price of anarchy of finite congestion games", "author": ["Giorgos Christodoulou", "Elias Koutsoupias"], "venue": "In Proceedings of the 37th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Christodoulou and Koutsoupias.,? \\Q2005\\E", "shortCiteRegEx": "Christodoulou and Koutsoupias.", "year": 2005}, {"title": "Near-optimal no-regret algorithms for zero-sum games", "author": ["Constantinos Daskalakis", "Alan Deckelbaum", "Anthony Kim"], "venue": "Games and Economic Behavior,", "citeRegEx": "Daskalakis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Daskalakis et al\\.", "year": 2015}, {"title": "Adaptive online learning", "author": ["Dylan J Foster", "Alexander Rakhlin", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Foster et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2015}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Introduction to Online Convex Optimization", "author": ["Elad Hazan"], "venue": "Foundations and Trends in Optimization,", "citeRegEx": "Hazan.,? \\Q2016\\E", "shortCiteRegEx": "Hazan.", "year": 2016}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["Elad Hazan", "Satyen Kale"], "venue": "Machine learning,", "citeRegEx": "Hazan and Kale.,? \\Q2010\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2010}, {"title": "Efficient learning algorithms for changing environments", "author": ["Elad Hazan", "C. Seshadhri"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning (ICML),", "citeRegEx": "Hazan and Seshadhri.,? \\Q2009\\E", "shortCiteRegEx": "Hazan and Seshadhri.", "year": 2009}, {"title": "Worst-case equilibria", "author": ["Elias Koutsoupias", "Christos Papadimitriou"], "venue": "Computer science review,", "citeRegEx": "Koutsoupias and Papadimitriou.,? \\Q2009\\E", "shortCiteRegEx": "Koutsoupias and Papadimitriou.", "year": 2009}, {"title": "Second-order quantile methods for experts and combinatorial games", "author": ["Wouter M Koolen", "Tim Van Erven"], "venue": "In Proceedings of The 28th Conference on Learning Theory (COLT),", "citeRegEx": "Koolen and Erven.,? \\Q2015\\E", "shortCiteRegEx": "Koolen and Erven.", "year": 2015}, {"title": "Achieving all with no parameters: Adanormalhedge", "author": ["Haipeng Luo", "Robert E Schapire"], "venue": "In Proceedings of The 28th Conference on Learning Theory (COLT),", "citeRegEx": "Luo and Schapire.,? \\Q2015\\E", "shortCiteRegEx": "Luo and Schapire.", "year": 2015}, {"title": "Learning and efficiency in games with dynamic population", "author": ["Thodoris Lykouris", "Vasilis Syrgkanis", "\u00c9va Tardos"], "venue": "InProceedings of the Twenty-SeventhAnnualACM-SIAMSymposium onDiscrete Algorithms (SODA),", "citeRegEx": "Lykouris et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lykouris et al\\.", "year": 2016}, {"title": "First-order regret bounds for combinatorial semi-bandits", "author": ["Gergely Neu"], "venue": "In Proceedings of the 27th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Neu.,? \\Q2015\\E", "shortCiteRegEx": "Neu.", "year": 2015}, {"title": "Intrinsic robustness of the price of anarchy", "author": ["Tim Roughgarden"], "venue": "Journal of the ACM,", "citeRegEx": "Roughgarden.,? \\Q2015\\E", "shortCiteRegEx": "Roughgarden.", "year": 2015}, {"title": "Online learning with predictable sequences", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2013}, {"title": "Optimization, learning, and games with predictable sequences", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2013}, {"title": "How bad is selfish routing", "author": ["Tim Roughgarden", "Eva Tardos"], "venue": "Journal of the ACM,", "citeRegEx": "Roughgarden and Tardos.,? \\Q2002\\E", "shortCiteRegEx": "Roughgarden and Tardos.", "year": 2002}, {"title": "Fast convergence of regularized learning in games", "author": ["Vasilis Syrgkanis", "Alekh Agarwal", "Haipeng Luo", "Robert E Schapire"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Syrgkanis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Syrgkanis et al\\.", "year": 2015}, {"title": "Adaptivity and optimism: An improvedexponentiated gradient algorithm", "author": ["Jacob Steinhardt", "PercyLiang"], "venue": "In Proceedings of the 31st International Conference onMachine Learning (ICML),", "citeRegEx": "Steinhardt and PercyLiang.,? \\Q2014\\E", "shortCiteRegEx": "Steinhardt and PercyLiang.", "year": 2014}, {"title": "Composable and efficient mechanisms", "author": ["Vasilis Syrgkanis", "\u00c9va Tardos"], "venue": "In ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Syrgkanis and Tardos.,? \\Q2013\\E", "shortCiteRegEx": "Syrgkanis and Tardos.", "year": 2013}, {"title": "Incomplete information and internal regret in prediction of individual sequences", "author": ["Gilles Stoltz"], "venue": "PhD thesis, Universite Paris-Sud,", "citeRegEx": "Stoltz.,? \\Q2005\\E", "shortCiteRegEx": "Stoltz.", "year": 2005}, {"title": "How to better use expert advice", "author": ["Rani Yaroshinsky", "Ran El-Yaniv", "Steven S. Seiden"], "venue": "Machine Learning,", "citeRegEx": "Yaroshinsky et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yaroshinsky et al\\.", "year": 2004}], "referenceMentions": [], "year": 2017, "abstractText": "We show that learning algorithms satisfying a low approximate regret property experience fast convergence to approximate optimality in a large class of repeated games. Our property, which simply requires that each learner has small regret compared to a (1 + \u01eb)-multiplicative approximation to the best action in hindsight, is ubiquitous among learning algorithms \u2014 it is satisfied evenby the vanillaHedge forecaster. Our results improveupon recentwork of Syrgkanis et al. [SALS15] in a number of ways. We improve upon the speed of convergence by a factor of n, the number of players, and require only that the players observe payoffs under other players\u2019 realized actions, as opposed to expected payoffs. We further show that convergence occurs with high probability, and under certain conditions show convergence under bandit feedback. Both the scope of settings and the class of algorithms for which our analysis provides fast convergence are considerably broader than in previous work. Our framework applies to dynamic population games via a low approximate regret property for shifting experts. Here we strengthen the results of [LST16] in two ways: We allow players to select learning algorithms from a larger class, which includes a minor variant of the basic Hedge algorithm, and we increase the maximum churn in players for which approximate optimality is achieved. In the bandit setting we present a novel algorithm which provides a \u201csmall loss\u201d-type bound with improved dependence on the number of actions and is both simple and efficient. This result may be of independent interest. \u2217Cornell University, djfoster@cs.cornell.edu. Work supported under NSF grant CDS&E-MSS 1521544. \u2020Tsinghua University, lizhiyuan13@mails.tsinghua.edu.cn. Research performed while author was visiting Cornell University. \u2021Cornell University, teddlyk@cs.cornell.edu. Work supported under ONR grant N00014-08-1-0031, and a Google faculty research award. \u00a7Cornell University, sridharan@cs.cornell.edu. Work supported by NSF grant CDS&E-MSS 1521544. \u00b6Cornell University, eva@cs.cornell.edu. Work supported in part by NSF grant CCF-1563714, ONR grant N00014-081-0031, and a Google faculty research award.", "creator": "LaTeX with hyperref package"}}}