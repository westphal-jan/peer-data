{"id": "1606.06368", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings", "abstract": "Can we train a system that, on any new input, either says \"don't know\" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is well-specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.", "histories": [["v1", "Mon, 20 Jun 2016 23:59:25 GMT  (4978kb)", "https://arxiv.org/abs/1606.06368v1", "ACL 2016"], ["v2", "Thu, 23 Jun 2016 07:33:01 GMT  (242kb,D)", "http://arxiv.org/abs/1606.06368v2", "ACL 2016, Removed the duplicate author name of the previous version"]], "COMMENTS": "ACL 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL", "authors": ["fereshte khani", "martin c rinard", "percy liang"], "accepted": true, "id": "1606.06368"}, "pdf": {"name": "1606.06368.pdf", "metadata": {"source": "CRF", "title": "Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings", "authors": ["Fereshte Khani", "Martin Rinard", "Percy Liang"], "emails": ["fereshte@cs.stanford.edu", "rinard@lcs.mit.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "It is better for the system to say \"do not know\" rather than make a costly erroneous prediction. If the system is learned from data that permeates uncertainty, and we need to manage that uncertainty correctly to meet our precision requirement, it is particularly difficult, since training inputs may not be representative of test inputs due to limited data, covariate shift (Shimodaira, 2000), or adversarial filtering (Nelson et al., 2009). In this non-deformative setting, we can still form a system that is guaranteed to be either abstain or make the correct prediction. Our current work is motivated by the goal of building reliable systems and natural language interfaces. Our goal is to learn a semantic mapping of examples of perfect form pairs (Figure 1). More generally, we assume that the input x is a source of atoms (Multiset)."}, {"heading": "2 Setup", "text": "In the simplest case, source atoms are words and target atoms predicates - see Figure 2 (above) for an example.2 We assume that there is a true assignment from a source atom s (e.g. Ohio) to a bag of target atoms t = M (s) (e.g. {OH}).Note that M * cannot assign a source atom to any target atoms (M * (of) = {}) or multiple target atoms (M * (grandparents) = {parents, parents}).We extend M to bags of source atoms via multiset sum: M * (x) =] s \u00b2 xM \u00b2 (s).Of course, we do not know M * and have to estimate it from training data.Our training examples are source atoms to an accuracy that we set not on one (n) but on one (n) x (x)."}, {"heading": "2.1 Unanimity principle", "text": "Letter M is the set of mappings (which contains the true mappings M *). Let C use the subset of map-2Our semantic parsing experiments (Section 4.2) use more complex source and target atoms to grasp a context and structure that match the training examples. Let F be the group of safe mappings on which all mappings in C match: F def = {x: | {M (x): M \u00b2 | = 1}. (2) The unanimity principle defines an mappings M that delivers the unanimous output to F and \"does not know\" on its supplement. This choice is given the following strong guarantee: For every safe mappings x \u00b2 F we have M \u00b2 (x) = M \u00b2 (x) that delivers the best possible mappings to F and \"does not know\" on its supplement."}, {"heading": "3 Linear algebraic formulation", "text": "s be ns (nt) the number of source (target) atomic types. First, we can represent the bag x (y) as n-dimensional (nt-dimensional) row vector of number; for example, the vector form of the \"area of Ohio\" isar eaof O hioci tiesin Io wa [] 1 1 1 0 0 0. We represent the mapping M as a non-negative integer matrix, where Mst is the number of times the target atom t appears in the bag to which the source atom s is mapped (Figure 3). We also encode the learning examples as matrices: S is an n-ns matrix, where the i-th series xi; T is the n-th matrix, where the i-th series yi is."}, {"heading": "3.1 Integer linear programming", "text": "Finding an element of C, as defined in (3), corresponds to solving an integer linear program (ILP) that is NP-hard at worst, although there are relatively effective standard solvers such as Gurobi. However, one solution is not enough. To check whether an input x is in safe group F (2), we need to check whether all mappings predict M \u00b2 C the same output on x; that is, xM is for all M \u00b2 C. Our finding is that we can check whether x \u00b2 F is by solving only two ILPs. Let's remember that we want to know if the output vector xM can be different for different M \u00b2 C. To do this, we select a random vector v \u00b2 Rnt and look at the scalar projection xMv. The first ILP maximizes this scalar and the second MX \u00b2 MX = probability."}, {"heading": "3.2 Linear programming", "text": "The basic idea is that we have two mappingsM1, M2, CLP \"randomly enough,\" whether xM1, xM2 or xM2 are equivalent to animosities about CLP. \"We cannot assume that we have two mappingsM1, M2, CLP.\" (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\" (\"We.\"). (\"We.\" (. \"We.\"). (. \"We.\" (.). (. \"We.\" (.). (. \"We.\" (.). (. \"We.\" (. \"). (.\" We. \"(.\"). (. \"We.\" (.). \"(.\" We. \"(.\"). \"(.\" (.). \"We. (.\" (.). \"(.\" We. \"(.\"). \"(.\" We. \"(.\"). \"(.\" We. \"(.\"). \"(.\" We. \"(.\"). \"(.\" (. \"). (.\" We. \"). (.\"). (. \"We.\" (. \"). (. (. (.\"). (. \"). (. (.). (. (.). (.\" We.). (. (.). (. (.). (. (. \"We. (.). (. (.\" We. (.). (.). (. (. \"). (. (.). (. (.). (. (.). (. (.). (.). (.). (. (.). (. (.)."}, {"heading": "3.3 Linear system", "text": "To get additional intuition about the unanimity principle, let us further relax CLP (4) by removing the non-negativity constraint that leads to a linear system. Define the relaxed series of consistent mappings to be equivalent to all solutions to the linear system and the relaxed safe sentences: CLS def = {M-Rns \u00b7 nt-SM = T) (7) FLS def = {M (x): M-CLS} | = 1) Note that CLS is an affine subspace so that any M-CLS can be expressed as M0 + BA, where M0 is an arbitrary solution, B is a basis for the zero space of S and A is an arbitrary matrix. Figure 5 represents the linear system for four training examples. In the rare case that S has a complete column (if we have many training examples), then we have the left inversion of S, and there is a true S-inversion."}, {"heading": "3.4 Handling noise", "text": "So far, we have assumed that our training examples are noiseless, so we can directly add the3. There may be more than one set of coefficients (\u03b11, \u03b12) to write x, but they result in the same output: \u03b1 > 1 S = \u03b1 > 2 S = \u21d2 \u03b1 > 1 SM = \u21d2 \u03b1 > 2 SM = \u21d2 1 T. Can we still guarantee 100% precision? The answer for the ILP formulation is yes: we simply replace the exact match condition (SM = T) with a weaker one: \"SM \u2212 T\" 1 \u2264 nError (*). The result is still an ILP, so that the techniques from Section 3.1 are readily applicable."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Artificial data", "text": "We then created 120 training examples and 50 test examples, in which the length of each input is between 5 and 10. Source atoms are divided into 10 clusters, and each input contains only source atoms from a cluster. All methods achieve 100% precision, and as expected, the relaxations result in a lower memory, although they can all achieve 100% recall. Comparison with the score estimation. Recall that M's unanimity principles over the entire set of consistent mappings allow us to be robust changes in input distribution."}, {"heading": "4.2 Semantic parsing on GeoQuery", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to fulfil them."}, {"heading": "5 Extensions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Learning from denotations", "text": "For semantic parsing, this means commenting sentences with logical forms (e.g. area from Ohio to area (OH), which is very expensive. This has motivated previous work to learn from question-and-answer pairs (e.g. area from Ohio to 44825) (Liang et al., 2011). This provides a weaker oversight: For example, 44825 is the area of Ohio (in square miles), but it is also the ZIP code of Chatfield. So the true output could be either area (OH) or ZIP code (Chatfield). In this section, we will show how to deal with this form of weak oversight by asking for unanimity on additional selection variables. Formally, we have D = {(x1, Y1),. (xn, Yn)} as a set of training examples, here each Yi consists of ki candidate outputs for xi. In this case, the unknowns are before we implement the Yi selection."}, {"heading": "5.2 Active learning", "text": "A minor advantage of Linear System Relaxation (Section 3.3) is that it proposes an active learning procedure; the setting is that we want to receive a set of inputs (the matrix S) and (adaptively) select which inputs (rows of S) should receive the output (corresponding row of T). Sentence 4 states that in linear system formulation the set of safe inputs FLS is exactly the same as the line spacing of S. So, if we ask for an input that is already in the line spacing of S, this does not affect FLS at all. The algo rithm is then simple: go through our training inputs x1,... xn one by one and only request the output if it is not in the line spacing of the previously added inputs x1,..., xi \u2212 1.Figure 9 shows the reminder when we select examples that are linearly independent compared to the examples we randomly select. The active learning scheme requires half of the examples as x1..."}, {"heading": "5.3 Paraphrasing", "text": "Another minor advantage of linear system relaxation (Section 3.3) is that we can easily divide the safe amount of FLS (8) into subsets of utterances that paraphrase each other. Two utterances are a paraphrase of each other if both correspond to the same logical form, e.g. \"Texas Capital\" and \"Capital of Texas.\" Given a sentence x-FLS, our goal is to find all of its paraphrases in FLS. As explained in Section 3.3, we can represent each input x for some coefficients \u03b2-Rn as a linear combination of S: x = \u03b1 Rn: x = \u03b1 > S. We want to find all x-FLS in such a way that x \"guarantees the same output as x. We can represent x\" = \u03b2 > S for some coefficients \u03b2-Rn. Therefore, the outputs for x and x \"are \u03b1 > T and \u03b2 > T, respectively, so that x.\" Therefore, we are interested in \u03b2-T = \u03b2-T > T, or in other words, \u03b2-Rn for some paraphrases (i.e., for the space v = T)."}, {"heading": "6 Discussion and related work", "text": "In the last ten years, there has been much work on semantic parsing, especially on learning from weaker people (Liang et al., 2011; Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013), which goes beyond small databases (Cai and Yates, 2013; Pascual and Liang, 2015), and the application of semantic methods to other tasks (Matuschek et al., 2012; Barzilay and Barzilay, 2013)."}], "references": [{"title": "Bootstrapping semantic parsers from conversations", "author": ["Artzi", "Zettlemoyer2011] Y. Artzi", "L. Zettlemoyer"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Artzi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2011}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics (TACL), 1:49\u201362", "author": ["Artzi", "Zettlemoyer2013] Y. Artzi", "L. Zettlemoyer"], "venue": null, "citeRegEx": "Artzi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2013}, {"title": "Learning to abstain from binary prediction. arXiv preprint arXiv:1602.08151", "author": ["A. Balsubramani"], "venue": null, "citeRegEx": "Balsubramani.,? \\Q2016\\E", "shortCiteRegEx": "Balsubramani.", "year": 2016}, {"title": "Semantic parsing on Freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP)", "author": ["Berant et al.2013] J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Largescale semantic parsing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL)", "author": ["Cai", "Yates2013] Q. Cai", "A. Yates"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "On optimum recognition error and reject tradeoff", "author": ["C.K. Chow"], "venue": "IEEE Transactions on Information", "citeRegEx": "Chow.,? \\Q1970\\E", "shortCiteRegEx": "Chow.", "year": 1970}, {"title": "Identifying the set of always-active constraints in a system of linear inequalities by a single linear program", "author": ["R.M. Freund", "R. Roundy", "M.J. Todd"], "venue": null, "citeRegEx": "Freund et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1985}, {"title": "Confidence driven unsupervised semantic parsing", "author": ["R. Reichart", "J. Clarke", "D. Roth"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Goldwasser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldwasser et al\\.", "year": 2011}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["S. Hanneke"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Hanneke.,? \\Q2007\\E", "shortCiteRegEx": "Hanneke.", "year": 2007}, {"title": "Learning to transform natural to formal languages", "author": ["Kate et al.2005] R.J. Kate", "Y.W. Wong", "R.J. Mooney"], "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),", "citeRegEx": "Kate et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kate et al\\.", "year": 2005}, {"title": "Calibrated structured prediction", "author": ["Kuleshov", "Liang2015] V. Kuleshov", "P. Liang"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Kuleshov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kuleshov et al\\.", "year": 2015}, {"title": "Using semantic unification to generate regular expressions from natural language", "author": ["Kushman", "Barzilay2013] N. Kushman", "R. Barzilay"], "venue": "In Human Language Technology and North American Association", "citeRegEx": "Kushman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kushman et al\\.", "year": 2013}, {"title": "Inducing probabilistic CCG grammars from logical form with higher-order unification", "author": ["L. Zettlemoyer", "S. Goldwater", "M. Steedman"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2010}, {"title": "Version space algebra and its application to programming by demonstration", "author": ["T.A. Lau", "P. Domingos", "D.S. Weld"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Lau et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lau et al\\.", "year": 2000}, {"title": "Learning dependency-based compositional semantics", "author": ["Liang et al.2011] P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["Matuszek et al.2012] C. Matuszek", "N. FitzGerald", "L. Zettlemoyer", "L. Bo", "D. Fox"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Matuszek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Using machine teaching to identify optimal training-set attacks on machine learners. In Association for the Advancement of Artificial Intelligence (AAAI)", "author": ["Mei", "Zhu2015] S. Mei", "X. Zhu"], "venue": null, "citeRegEx": "Mei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2015}, {"title": "Version spaces: A candidate elimination approach to rule learning", "author": ["T.M. Mitchell"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Mitchell.,? \\Q1977\\E", "shortCiteRegEx": "Mitchell.", "year": 1977}, {"title": "Misleading learners: Co-opting your spam filter", "author": ["Nelson et al.2009] B. Nelson", "M. Barreno", "F.J. Chi", "A.D. Joseph", "B.I. Rubinstein", "U. Saini", "C. Sutton", "J. Tygar", "K. Xia"], "venue": "In Machine learning in cyber trust,", "citeRegEx": "Nelson et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nelson et al\\.", "year": 2009}, {"title": "Compositional semantic parsing on semistructured tables. In Association for Computational Linguistics (ACL)", "author": ["Pasupat", "Liang2015] P. Pasupat", "P. Liang"], "venue": null, "citeRegEx": "Pasupat et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pasupat et al\\.", "year": 2015}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["J. Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "Platt.,? \\Q1999\\E", "shortCiteRegEx": "Platt.", "year": 1999}, {"title": "Towards a theory of natural language interfaces to databases", "author": ["Popescu et al.2003] A. Popescu", "O. Etzioni", "H. Kautz"], "venue": "In International Conference on Intelligent User Interfaces (IUI),", "citeRegEx": "Popescu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Popescu et al\\.", "year": 2003}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodaira"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Shimodaira.,? \\Q2000\\E", "shortCiteRegEx": "Shimodaira.", "year": 2000}, {"title": "An optimal reject rule for binary classifiers", "author": ["F. Tortorella"], "venue": "In Advances in Pattern Recognition,", "citeRegEx": "Tortorella.,? \\Q2000\\E", "shortCiteRegEx": "Tortorella.", "year": 2000}, {"title": "A version space approach to learning context-free grammars", "author": ["Vanlehn", "Ball1987] K. Vanlehn", "W. Ball"], "venue": "Machine learning,", "citeRegEx": "Vanlehn et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Vanlehn et al\\.", "year": 1987}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Wong", "Mooney2007] Y.W. Wong", "R.J. Mooney"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Wong et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2007}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["Zelle", "Mooney1996] M. Zelle", "R.J. Mooney"], "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),", "citeRegEx": "Zelle et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Zelle et al\\.", "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Zettlemoyer", "Collins2005] L.S. Zettlemoyer", "M. Collins"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Zettlemoyer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 22, "context": "It is particularly challenging since training inputs might not be representative of test inputs due to limited data, covariate shift (Shimodaira, 2000), or adversarial filtering (Nelson et al.", "startOffset": 133, "endOffset": 151}, {"referenceID": 18, "context": "It is particularly challenging since training inputs might not be representative of test inputs due to limited data, covariate shift (Shimodaira, 2000), or adversarial filtering (Nelson et al., 2009; Mei and Zhu, 2015).", "startOffset": 178, "endOffset": 218}, {"referenceID": 6, "context": "We can achieve this by solving the following LP from Freund et al. (1985):", "startOffset": 53, "endOffset": 74}, {"referenceID": 9, "context": "We use the variable-free functional logical forms (Kate et al., 2005), in which each target atom is a predicate conjoined with its argument order (e.", "startOffset": 50, "endOffset": 69}, {"referenceID": 12, "context": "There is a lot of work in semantic parsing that tackles the GeoQuery dataset (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011), and the state-of-the-art is 91.", "startOffset": 77, "endOffset": 201}, {"referenceID": 14, "context": "There is a lot of work in semantic parsing that tackles the GeoQuery dataset (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011), and the state-of-the-art is 91.", "startOffset": 77, "endOffset": 201}, {"referenceID": 14, "context": "1% precision and recall (Liang et al., 2011).", "startOffset": 24, "endOffset": 44}, {"referenceID": 14, "context": ", area of Ohio to 44825) (Liang et al., 2011).", "startOffset": 25, "endOffset": 45}, {"referenceID": 14, "context": "Over the last decade, there has been much work on semantic parsing, mostly focusing on learning from weaker supervision (Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013), scaling up beyond small databases (Cai and Yates, 2013; Berant et al.", "startOffset": 120, "endOffset": 223}, {"referenceID": 7, "context": "Over the last decade, there has been much work on semantic parsing, mostly focusing on learning from weaker supervision (Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013), scaling up beyond small databases (Cai and Yates, 2013; Berant et al.", "startOffset": 120, "endOffset": 223}, {"referenceID": 3, "context": ", 2011; Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013), scaling up beyond small databases (Cai and Yates, 2013; Berant et al., 2013; Pasupat and Liang, 2015), and applying semantic parsing to other tasks (Matuszek et al.", "startOffset": 101, "endOffset": 168}, {"referenceID": 15, "context": ", 2013; Pasupat and Liang, 2015), and applying semantic parsing to other tasks (Matuszek et al., 2012; Kushman and Barzilay, 2013; Artzi and Zettlemoyer, 2013).", "startOffset": 79, "endOffset": 159}, {"referenceID": 3, "context": ", 2011; Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013), scaling up beyond small databases (Cai and Yates, 2013; Berant et al., 2013; Pasupat and Liang, 2015), and applying semantic parsing to other tasks (Matuszek et al., 2012; Kushman and Barzilay, 2013; Artzi and Zettlemoyer, 2013). However, only Popescu et al. (2003) focuses on precision.", "startOffset": 123, "endOffset": 333}, {"referenceID": 17, "context": "The idea of computing consistent hypotheses appears in the classic theory of version spaces for binary classification (Mitchell, 1977) and has been extended to more structured settings (Vanlehn and Ball, 1987; Lau et al.", "startOffset": 118, "endOffset": 134}, {"referenceID": 13, "context": "The idea of computing consistent hypotheses appears in the classic theory of version spaces for binary classification (Mitchell, 1977) and has been extended to more structured settings (Vanlehn and Ball, 1987; Lau et al., 2000).", "startOffset": 185, "endOffset": 227}, {"referenceID": 8, "context": "Our \u201csafe set\u201d of inputs appears in the literature as the complement of the disagreement region (Hanneke, 2007).", "startOffset": 96, "endOffset": 111}, {"referenceID": 5, "context": "There is classic work on learning classifiers that can abstain (Chow, 1970; Tortorella, 2000; Balsubramani, 2016).", "startOffset": 63, "endOffset": 113}, {"referenceID": 23, "context": "There is classic work on learning classifiers that can abstain (Chow, 1970; Tortorella, 2000; Balsubramani, 2016).", "startOffset": 63, "endOffset": 113}, {"referenceID": 2, "context": "There is classic work on learning classifiers that can abstain (Chow, 1970; Tortorella, 2000; Balsubramani, 2016).", "startOffset": 63, "endOffset": 113}, {"referenceID": 20, "context": "Another avenue for providing user confidence is probabilistic calibration (Platt, 1999), which has been explored more recently for structured prediction (Kuleshov and Liang, 2015).", "startOffset": 74, "endOffset": 87}], "year": 2016, "abstractText": "Can we train a system that, on any new input, either says \u201cdon\u2019t know\u201d or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is wellspecified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.", "creator": "LaTeX with hyperref package"}}}