{"id": "1606.06406", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "Incremental Parsing with Minimal Features Using Bi-Directional LSTM", "abstract": "Recently, neural network approaches for parsing have largely automated the combination of individual features, but still rely on (often a larger number of) atomic features created from human linguistic intuition, and potentially omitting important global context. To further reduce feature engineering to the bare minimum, we use bi-directional LSTM sentence representations to model a parser state with only three sentence positions, which automatically identifies important aspects of the entire sentence. This model achieves state-of-the-art results among greedy dependency parsers for English. We also introduce a novel transition system for constituency parsing which does not require binarization, and together with the above architecture, achieves state-of-the-art results among greedy parsers for both English and Chinese.", "histories": [["v1", "Tue, 21 Jun 2016 03:20:59 GMT  (54kb,D)", "http://arxiv.org/abs/1606.06406v1", "Pre-print of paper appearing in ACL 2016"]], "COMMENTS": "Pre-print of paper appearing in ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["james cross", "liang huang 0001"], "accepted": true, "id": "1606.06406"}, "pdf": {"name": "1606.06406.pdf", "metadata": {"source": "CRF", "title": "Incremental Parsing with Minimal Features Using Bi-Directional LSTM", "authors": ["James Cross", "Liang Huang"], "emails": ["crossj@oregonstate.edu", "liang.huang@oregonstate.edu"], "sections": [{"heading": null, "text": "Recently, neural networking approaches to parsing have largely automated the combination of individual traits, but still rely on (often a large number) of atomic traits derived from human linguistic intuition, ignoring potentially important global contexts. To reduce feature engineering to the absolute minimum, we use bidirectional LSTM sentence representations to model a parser state with only three sentence positions that automatically identifies important aspects of the entire sentence. This model achieves state-of-the-art results among greedy dependency savers for English. We also introduce a novel transition system for parsing constituencies that does not require binarization, and together with the architecture above, achieves state-of-the-art results among greedy parsers for both English and Chinese."}, {"heading": "1 Introduction", "text": "Recently, neural network-based parsers have become popular, with the promise to reduce the burden of manual feature engineering. For example, Chen and Manning (2014) and subsequent work replace the enormous amount of manual feature combinations in non-neural network efforts (Nivre et al., 2011) by vector embedding of atomic properties. However, this approach has two related limitations. First, it still depends on a large number of carefully designed atomic features. For example, Chen and Manning (2014) and subsequent work such as Weiss et al. (2015) use of 48 atomic features from Zhang and Nivre (2011), including selected thirtysomething dependencies. This approach inevitably leaves out some non-local information that could be useful. In particular, although such a model can exploit similarities between words and other embedded categories, and learn to reduce interactions among these atomic features, there can be no manual induction to target the need for further texts.We have no further details on this."}, {"heading": "3 Shift-Reduce Dependency Parsing", "text": "Using the LSTM architecture to encode the context, we found that we achieved competitive results by using only three sentence positioning functions to model the parser state: the header of each of the top two trees on the stack (s0 and s1) and the next word in the queue (q0); see Table 1. The usefulness of the headers on the stack is clear enough, as these are the two words that are linked by a dependency when performing a reduction action. Also, the next word on the queue is important because the top tree on the stack should not be reduced if it has children that have not yet been moved. This feature allows the model to delay a right reduction until the top tree on the stack is fully formed, but move instead."}, {"heading": "3.1 Hierarchical Classification", "text": "The structure of our network model based on the calculation of position characteristics is relatively simple and similar to previous approaches to the analysis of neural networks such as Chen and Manning (2014) and Wei\u00df et al. (2015). It consists of a multi-layer perceptron that uses a single hidden ReLU layer, followed by a linear classifier across the action space, with the training target being a negative log softmax. However, we found that performance could be improved by taking into account the decision on structural measures (i.e. shift, left truncation or right-hand guidance) and the decision on which arc to assign to a reduction. We therefore use separate classifiers for these decisions, each with its own fully connected hidden and output layers, but with the underlying recursive architecture. This structure has been used for the results reported in Section 5 and is referred to as \"Hierarchical Actions\" compared to a single action classifier in Table 3."}, {"heading": "4 Shift-Promote-Adjoin Constituency Parsing", "text": "In order to further demonstrate the advantage of our idea of minimal features with bidirectional sentence representations, we expand our work from dependency sparsing to constituency parsing. However, the latter is much more difficult than the former under the paradigm of shift reduction, because: \u2022 we must also predict the non-terminal labels \u2022 the tree is non-binarized (with many simple rules and more than binary branch rules) While most previous work does not require binarization or transformation of constituency trees in a pre-processing step (Zhu et al., 2013; Wang and Xue, 2014; Wed and Huang, 2015), we propose a novel \"Shift PromoteAdjoin\" paradigm that does not require binarization or transformation of constituency trees (see Figure 5). Note in particular that in our case only promoting the action produces a new tree node (with a non-terminal labeling), while the join action is the linguistically motivated \"adjunction,\" i.e. the fixation of the constituency junction."}, {"heading": "5 Experimental Results", "text": "All experiments were performed with minimal hyperparameter tuning, and the settings used for the reported results are summarized in Table 6. Network parameters were updated using gradient backpropagation, including backpropagation over time for the recurring components, using ADADELTA to plan the learning rate (Zeiler, 2012). We tested both types of parsers at Penn Treebank (PTB) and Penn Chinese Treebank (CTB-5) (with p = 0.5), using the standard splits for each LSTM layer (separately for each connection in the case of the two-layer network)."}, {"heading": "5.1 Dependency Parsing: English & Chinese", "text": "Table 2 shows results for the English Penn Treebank using the Stanford dependencies. Despite the minimal representation of features, relatively low training repetitions and lack of pre-calculated embedding, the parser delivered the same performance as state-of-the-art incremental dependency parsers and easily outperformed the state-of-the-art greedy parser. Ablation experiments shown in Table 3 suggest that both forward and backward contexts are very important for each word to achieve strong results. Using only word forms and not language components results in similarly impaired performance. Figure 6 compares our parser to that of Chen and Manning (2014) in terms of arc repetition for different arc lengths. While the two parsers perform similar performance for short arcs, ours clearly outperforms their performance on longer arcs, and more interestingly still our accuracy does not deteriorate much after length 6. This confirms the advantage of global repeat in our model.Table 4 summarizes the Chinese dependency results, our work is highly competitive with those developed and our work is again."}, {"heading": "5.2 Constituency Parsing: English & Chinese", "text": "Table 5 compares the results of our constituency analyses with the latest incremental parsers. Although our work is clearly less precise than that of beam detectors, we achieve the highest accuracy among greedy parsers, in both English and China.1,21 The greedy accuracies for Mi and Huang (2015) come from Haitao Mi, and greedy results for Zhu et al. (2013) stem from the duplication of experiments with the code provided by these authors. 2 The parser of Vinyals et al. (2015) does not use an explicit transition system, but is similar in spirit, as the generation of a right bracket can be considered a reduction action."}, {"heading": "6 Related Work", "text": "Because recurring networks are so natural for language modeling (given the sequential nature of the latter), bi-directional LSTM networks are becoming increasingly common in all kinds of linguistic tasks, such as event detection in Ghaeini et al. (2016). In fact, after the filing, we discovered that Kiperwasser and Goldberg (2016) simultaneously developed an extremely similar approach to our dependency saver. Instead of extending it to parsing constituencies, they also apply the same idea to graph-based dependency parsing."}, {"heading": "7 Conclusions", "text": "We presented a simple bi-directional LSTM sentence representation model for minimal features in both incremental dependence and incremental constituency parsing, using a novel shift-promotion-join algorithm. Experiments show that our method competes with the state-of-the-art greedy parsers in parsing tasks as well as in English and Chinese."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for comments. We also thank Taro Watanabe, Muhua Zhu and Yue Zhang for sharing their code, Haitao Mi for the greedy results of his parser, and Ashish Vaswani and Yoav Goldberg for discussions. The authors were partially supported by DARPA FA8750-13-2-0041 (DEFT), NSF IIS1449278, and a Google Faculty Research Award."}], "references": [{"title": "A fast and accurate dependency parser using neural networks. In Empirical Methods in Natural Language Processing (EMNLP)", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Statistical parsing with an automatically-extracted tree-adjoining grammar", "author": ["David Chiang"], "venue": "In Proc. of ACL", "citeRegEx": "Chiang.,? \\Q2000\\E", "shortCiteRegEx": "Chiang.", "year": 2000}, {"title": "Transition-based dependency parsing with stack long short-term memory. arXiv preprint arXiv:1505.08075", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Event nugget detection with forward-backward recurrent neural networks", "author": ["Ghaeini et al.2016] Reza Ghaeini", "Xiaoli Z. Fern", "Liang Huang", "Prasad Tadepalli"], "venue": "In Proc. of ACL", "citeRegEx": "Ghaeini et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ghaeini et al\\.", "year": 2016}, {"title": "Inducing history representations for broad coverage statistical parsing", "author": ["James Henderson"], "venue": "In Proceedings of NAACL", "citeRegEx": "Henderson.,? \\Q2003\\E", "shortCiteRegEx": "Henderson.", "year": 2003}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations. CoRR, abs/1603.04351", "author": ["Kiperwasser", "Yoav Goldberg"], "venue": null, "citeRegEx": "Kiperwasser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser et al\\.", "year": 2016}, {"title": "Shift-reduce constituency parsing with dynamic programming and pos tag lattice", "author": ["Mi", "Huang2015] Haitao Mi", "Liang Huang"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Mi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2015}, {"title": "Maltparser: A data-driven parsergenerator for dependency parsing", "author": ["Nivre et al.2006] Joakim Nivre", "Johan Hall", "Jens Nilsson"], "venue": "In Proc. of LREC", "citeRegEx": "Nivre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Joakim Nivre"], "venue": "Computational Linguistics,", "citeRegEx": "Nivre.,? \\Q2008\\E", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "Grammar as a foreign language", "author": ["\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Joint pos tagging and transition-based constituent parsing in chinese with non-local features", "author": ["Wang", "Xue2014] Zhiguo Wang", "Nianwen Xue"], "venue": "In Proceedings of ACL", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Structured training for neural network transition-based parsing", "author": ["Weiss et al.2015] David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "In Proceedings of ACL", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Zhang", "Nivre2011] Yue Zhang", "Joakim Nivre"], "venue": "In Proceedings of ACL,", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "Fast and accurate shift-reduce constituent parsing", "author": ["Zhu et al.2013] Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu"], "venue": "In Proceedings of ACL", "citeRegEx": "Zhu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": "For example, Chen and Manning (2014) and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts (Nivre et al., 2006; Zhang and Nivre, 2011) by vector embeddings of the atomic features.", "startOffset": 142, "endOffset": 185}, {"referenceID": 8, "context": "For example, Chen and Manning (2014) and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts (Nivre et al., 2006; Zhang and Nivre, 2011) by vector embeddings of the atomic features. However, this approach has two related limitations. First, it still depends on a large number of carefully designed atomic features. For example, Chen and Manning (2014) and subsequent work such as Weiss et al.", "startOffset": 143, "endOffset": 401}, {"referenceID": 8, "context": "For example, Chen and Manning (2014) and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts (Nivre et al., 2006; Zhang and Nivre, 2011) by vector embeddings of the atomic features. However, this approach has two related limitations. First, it still depends on a large number of carefully designed atomic features. For example, Chen and Manning (2014) and subsequent work such as Weiss et al. (2015) use 48 atomic features from Zhang and Nivre (2011), including select thirdorder dependencies.", "startOffset": 143, "endOffset": 449}, {"referenceID": 8, "context": "For example, Chen and Manning (2014) and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts (Nivre et al., 2006; Zhang and Nivre, 2011) by vector embeddings of the atomic features. However, this approach has two related limitations. First, it still depends on a large number of carefully designed atomic features. For example, Chen and Manning (2014) and subsequent work such as Weiss et al. (2015) use 48 atomic features from Zhang and Nivre (2011), including select thirdorder dependencies.", "startOffset": 143, "endOffset": 500}, {"referenceID": 2, "context": "This effort is similar in motivation to the stack-LSTM of Dyer et al. (2015), but uses a much simpler architecture.", "startOffset": 58, "endOffset": 77}, {"referenceID": 10, "context": "Figure 3: The arc-standard dependency parsing system (Nivre, 2008) (rey omitted).", "startOffset": 53, "endOffset": 66}, {"referenceID": 13, "context": "The structure of our network model after computing positional features is fairly straightforward and similar to previous neural-network parsing approaches such as Chen and Manning (2014) and Weiss et al. (2015). It consists of a multilayer perceptron using a single ReLU hidden layer followed by a linear classifier over the action space, with the training objective being negative log softmax.", "startOffset": 191, "endOffset": 211}, {"referenceID": 16, "context": "While most previous work binarizes the constituency tree in a preprocessing step (Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015), we propose a novel \u201cShift-PromoteAdjoin\u201d paradigm which does not require any binariziation or transformation of constituency trees (see Figure 5).", "startOffset": 81, "endOffset": 139}, {"referenceID": 1, "context": ", attachment (Chiang, 2000; Henderson, 2003).", "startOffset": 13, "endOffset": 44}, {"referenceID": 4, "context": ", attachment (Chiang, 2000; Henderson, 2003).", "startOffset": 13, "endOffset": 44}, {"referenceID": 16, "context": "\u2022 it does not require binarization (Zhu et al., 2013; Wang and Xue, 2014) or compression of unary chains (Mi and Huang, 2015)", "startOffset": 35, "endOffset": 73}, {"referenceID": 4, "context": "There is, however, a more closely-related \u201cshiftproject-attach\u201d paradigm by Henderson (2003). For the example in Figure 5 he would use the following actions:", "startOffset": 76, "endOffset": 93}, {"referenceID": 14, "context": "Networks parameters were updated using gradient backpropagation, including backpropagation through time for the recurrent components, using ADADELTA for learning rate scheduling (Zeiler, 2012).", "startOffset": 178, "endOffset": 192}, {"referenceID": 5, "context": "We also applied dropout (Hinton et al., 2012) (with p = 0.", "startOffset": 24, "endOffset": 45}, {"referenceID": 15, "context": "1 The greedy accuracies for Mi and Huang (2015) are from Haitao Mi, and greedy results for Zhu et al. (2013) come from duplicating experiments with code provided by those authors.", "startOffset": 91, "endOffset": 109}, {"referenceID": 11, "context": "2 The parser of Vinyals et al. (2015) does not use an explicit transition system, but is similar in spirit since generating a right bracket can be viewed as a reduce action.", "startOffset": 16, "endOffset": 38}, {"referenceID": 3, "context": "Because recurrent networks are such a natural fit for modeling languages (given the sequential nature of the latter), bi-directional LSTM networks are becoming increasingly common in all sorts of linguistic tasks, for example event detection in Ghaeini et al. (2016). In fact, we discovered after submission that Kiperwasser and Goldberg (2016) have concurrently developed an extremely similar approach to our dependency parser.", "startOffset": 245, "endOffset": 267}, {"referenceID": 3, "context": "Because recurrent networks are such a natural fit for modeling languages (given the sequential nature of the latter), bi-directional LSTM networks are becoming increasingly common in all sorts of linguistic tasks, for example event detection in Ghaeini et al. (2016). In fact, we discovered after submission that Kiperwasser and Goldberg (2016) have concurrently developed an extremely similar approach to our dependency parser.", "startOffset": 245, "endOffset": 345}], "year": 2016, "abstractText": "Recently, neural network approaches for parsing have largely automated the combination of individual features, but still rely on (often a larger number of) atomic features created from human linguistic intuition, and potentially omitting important global context. To further reduce feature engineering to the bare minimum, we use bi-directional LSTM sentence representations to model a parser state with only three sentence positions, which automatically identifies important aspects of the entire sentence. This model achieves state-of-the-art results among greedy dependency parsers for English. We also introduce a novel transition system for constituency parsing which does not require binarization, and together with the above architecture, achieves state-of-the-art results among greedy parsers for both English and Chinese.", "creator": "LaTeX with hyperref package"}}}