{"id": "1606.06630", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "On Multiplicative Integration with Recurrent Neural Networks", "abstract": "We introduce a general and simple structural design called Multiplicative Integration (MI) to improve recurrent neural networks (RNNs). MI changes the way in which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models.", "histories": [["v1", "Tue, 21 Jun 2016 15:55:29 GMT  (71kb,D)", "http://arxiv.org/abs/1606.06630v1", "11 pages, 2 figures"], ["v2", "Sat, 12 Nov 2016 19:47:10 GMT  (187kb,D)", "http://arxiv.org/abs/1606.06630v2", "10 pages, 2 figures; To appear in NIPS2016"]], "COMMENTS": "11 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuhuai wu", "saizheng zhang", "ying zhang", "yoshua bengio", "ruslan salakhutdinov"], "accepted": true, "id": "1606.06630"}, "pdf": {"name": "1606.06630.pdf", "metadata": {"source": "CRF", "title": "On Multiplicative Integration with Recurrent Neural Networks", "authors": ["Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to behave in a way in which they see themselves in a position in which they are able to outdo themselves, and in which they are able to put themselves in a position in which they are able to be in a position in which they are in a position in which they are able to outlive themselves, in which they are able to outlive themselves, in which they are able to outlive themselves, and in which they are able to put themselves in a position to outlive themselves, in which they are able to outlive themselves, in which they are able to outlive themselves, in which they are able to outlive themselves, in which they are able to outlive themselves, in which they are able to outlive themselves."}, {"heading": "2 Structure Description and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 General Formulation of Multiplicative Integration", "text": "The key idea behind the multiplicative integration is the integration of different information flows Wx and Uz by the Hadamard product. \"A more general formulation of the multiplicative integration includes two further bias vectors \u03b21 and \u03b22 added to Wx and Uz: \u03c6 (Wx + \u03b21) (Uz + \u03b22) + b) (3), where \u03b21, \u03b22 and Rd are bias vectors. Note that such a formulation contains the terms of the first order as in an additive block, i.e. \u03b21 Uz + \u03b22 Wxt. To make the multiplicative integration more flexible, we introduce another bias vector \u03b1-Rd to gate2 the term Wx Uz and get the following formulation: \u03c6 (\u03b1 Wx Uz + \u03b21 Uz + \u03b22 Wx + \u03b22 Wxt), (4) Note that the number of parameters of the multiplicative integration can be approximately equal to the number of the GRx and the number of blocks of the GRX + 1."}, {"heading": "2.2 Gradient Properties", "text": "The multiplicative integration has different gradient properties compared to the additive component. To illustrate the representation, we first consider vanilla RNN and RNN with embedded multiplicative integration, which is called MI-RNN. That is, ht = \u03c6 (Wxt + Uht \u2212 1 + b) versus ht = \u03c6 (Wxt Uht \u2212 1 + b). In a vanilla RNN, the gradient \u2202 ht \u2212 n can be calculated as follows: 8,000 ht \u2212 n = t \u0445 k = t \u2212 n + 1 UTdiag (\u03c6 \u2032 k), (5) where it is possible to imply a gradient (Wxk + Uhk \u2212 1 + b). The above equation shows that the flow of the gradient over time strongly depends on the hidden to hidden matrix properties U, but W and xk seem to play a limited role: they come only in the derivative of Manhattan, mixed with Whk \u2212 1."}, {"heading": "3 Experiments", "text": "In all our experiments we use the general form of multiplicative integration (Eq.4) for all hidden state / goal calculations, unless otherwise stated."}, {"heading": "3.1 Exploratory Experiments", "text": "To better understand the functionality of multiplicative integration, we perform a simple RNN for illustration and several exploratory experiments to model the sign language using the Penn Treebank dataset [13] after the data partition in [14]. The length of the training sequence is 50. All models have a single hidden layer of size 2048, and we use Adam optimization algorithm [15] with learning rate 1e \u2212 4. Weights are initialized on samples taken from uniform [\u2212 0.02, 0.02]. Performance is evaluated using the bit-per-character metric (BPC) corresponding to log2 of perplexity."}, {"heading": "3.1.1 Gradient Properties", "text": "In order to analyze the gradient flow of the model, we divide the gradient in Eq. 6 into two parts: 1. the gated matrix products: UTdiag (Wxk), and 2. the derivative of nonlinearity \u03c6 \u2032, We analyze separately the properties of each term compared to the additive building block. First, we focus on the gating effect caused by diag (Wxk). To distinguish the effect of nonlinearity, we chose \u03c6 to activate the identity card, therefore both vanilla RNN and MI-RNN reduce to linear models called lin-RNN and lin-MI-RNN. For each model, we monitor the log-L2 standard of the gradient protocol."}, {"heading": "3.1.2 Scaling Problem", "text": "However, if you add two numbers of different magnitude, the smaller one could be negligible for the sum. In this experiment, we test whether the multiplicative integration is more robust for the scales of the weights. Following the same models as in Section 3.1.1, we first calculated the norms of Wxk and Uhk \u2212 1 for both vanilla RNN and MI-RNN for different k after the training. We found that Wxk is much smaller in both structures than Uhk \u2212 1 in the order of magnitude. This could be due to the fact that xk \u2212 1 is a single vector for both vanilla RNN and MI-RNN, whereby the number of updates for (columns of) W is smaller than U. As a result, in vanilla RNN, the pre-activation term Wxk \u2212 1 is largely consistent by the value of Wxk \u2212 1 and not by Wxk \u2212 W-1, but by Wxk-values."}, {"heading": "3.1.3 On different choices of the formulation", "text": "In our third experiment, we investigated the performance of various computational components, namely Gl. 1 (vanilla RNN), Gl. 2 (MI-RNN-simple) and Gl. 4 (MI-RNN-general) 5. The validation curves in Figure 1 (b) show that both MI-RNN, simple and MI-RNN-general have significantly better performance compared to vanilla RNN and MI-RNN-general have a faster convergence rate compared to MI-RNN-simple. We also compared our results with the previously published models in Table 1, lower left panel, where MI-RNN-general achieves a test BPC of 1.39, which, to our knowledge, is the best result for RNNNs in this task without complex rating / cell mechanisms."}, {"heading": "3.2 Character Level Language Modeling", "text": "In addition to the Penn Treebank dataset, we also perform character-level linguistic modeling on two larger datasets: text86 and Hutter Challenge Wikipedia7. Both contain 100M characters from Wikipedia, while text8 has an alphabet size of 27 and Hutter Challenge Wikipedia has an alphabet size of 205. If the BPC (bits-per-character) validation for both datasets does not decrease in [14] and [1], we achieve half the learning rate.We implemented multiplicative integration on both vanilla RNN and LSTM, which are referred to as MIRNN and MI-LSTM. Results for the text8 datasets are presented in Table 1, lower center table. All five models, including some of the previously published models, have the same number of 5b analyses."}, {"heading": "3.3 Speech Recognition", "text": "Next, we evaluate our models at the Wall Street Journal (WSJ) corpus (available as LDC corpus LDC93S6B and LDC94S13B), where we use the full 81-hour rate \"si284\" for training, set \"dev93\" for validation, and set \"eval92\" for testing. We follow the same data preparation process and model setting as in [20], and we use 59 characters as the target for acoustic modeling. Decoding is done using the CTC [23] -based, weighted finite-state transducers (WFSTs) [24] as proposed in [20]. Our model (referred to as MI-LSTM + CTC + WFST) consists of 4 bidirectional MI-LSTM layers, each with 320 units for each direction. CTC is performed at the top to solve the problem of alignment in language transcription."}, {"heading": "3.4 Learning Skip-Thought Vectors", "text": "Next, we evaluate our multiplicative integration based on the Skip-Thought model of [25].Skip-Thought is an encoder decoder model that attempts to learn generic, distributed sentence representations, which produces sentence representations that are robust and work well in practice as it achieves excellent results in many different NLP tasks. It was trained on the basis of the BookCorpus dataset, which consists of 11,038 books with 74,000,000 sentences. Not surprisingly, a single pass yields better results, but they use much larger models (about 16M) that are not directly compared with each other. Training data can last up to a week in which a high-end GPU is evaluated (as reported).Such training speed limits careful hyper-parameter search. However, with multiplicative integration not only shortens the training time by a factor of two, but also significantly improves final performance."}, {"heading": "3.5 Teaching Machines to Read and Comprehend", "text": "In our last experiment, we show that the use of multiplicative integration can be combined with other techniques for training RNNs, and the benefits of using MI still exist. [7] Batch normalization [26] was recently introduced for RNNNs, but they evaluated their proposed technique on a unidirectional 9https: / / github.com / ryankiros / skip-thoughtsAttentive Reader Model [27] for answering the question using the CNN corpus10. To test our approach, we evaluated the following four models: 1. An attentive LSTM reading model with a single hidden layer size 240 (the same as [7]) as the starting point, referred to as LSTM (ours); 2. Multiplicative integration LSTM with a single hidden size 240, referred to as MI-LSTM models; 3. MILSTM models with a batch standard, referred to as STM validation."}, {"heading": "4 Relationship to Previous Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Relationship to Hidden Markov Models", "text": "It can be shown that, under certain constraints, MI-RNN effectively implements the forward algorithm of the Hidden Markov Model (HMM). Direct mapping can be constructed as follows (see [28] for a similar derivative): Let U-Rm \u00b7 m be the probability matrix for the transition with Uij = Pr [ht + 1 = i | ht = j], W-Rm \u00b7 n be the probability matrix for observation with Wij = Pr [xt = i | ht = j]. If xt is a uniform vector (e.g. in many of the language modeling tasks), multiply it with W, effectively select a column of the observation matrix. Namely, if the jste input of xt is one, then Wxt = Pr [xt | ht = j]. If h0 is the original state distribution with h0 = Pr [h0] and {ht} t \u2265 1 is the alpha values in the forward process with Wx1, then Wxt = Iht = 1."}, {"heading": "4.2 Relations to Second Order RNNs and Multiplicative RNNs", "text": "MI-RNN is related to the second order RNN [9, 6] and the multiplicative RNN (MRNN) [10]. We first describe the similarities with these two models: The second order RNN contains a second order st in a vanilla RNN, whereas the element st, i is calculated by the bilinear form: st, i = xTt T (i) ht \u2212 1, where T (i) approximate order Rn \u00b7 m (1 \u2264 i \u2264 m) is10Note, however, that [7] a truncated version of the original dataset is used to store the compilation. 11Learning curves and the final number of results are obtained by correspondence with authors of [7]. 12https: / github.com / cooijmanstim / recurrent-batch-normalization.git.the ith slice of a tensor T-Rm \u00d7 n m."}, {"heading": "4.3 General Multiplicative Integration", "text": "Multiplicative integration can be seen as a general way to combine information flows from two different sources. Specifically, [29] suggested the conductor network, which achieves promising results in semi-monitored learning. In their model, they combine the lateral connections and the backward links via the \"combator\" function of a Hadamard product. Performance would deteriorate greatly without this product, as shown empirically [30]. [31] They examined neural embedding approaches in knowledge databases by formulating relationships as bilinear and / or linear mapping functions, and compared a variety of embedding models in the link prediction task. Surprisingly, the single-weighted Hadamard product is the best result among all bilinear functions. Furthermore, they carefully compare the multiplicative and additive interactions and show that multiplicative interaction dominates the additive ones."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed using Multiplicative Integration (MI), a simple Hadamard product, to combine the flow of information in recurrent neural networks. MI is easy to integrate into many popular RNN models, including LSTMs and GRUs, and introduces almost no additional parameters. In fact, implementing MI requires almost no additional work beyond implementing RNN models. We also show that MI performs cutting-edge performance for four different tasks or 11 datasets of different sizes and scales. We believe that multiplicate integration can be a standard building block for forming different types of RNN models."}, {"heading": "Acknowledgments", "text": "The authors thank the following agencies for funding and support: NSERC, Canada Research Chairs, CIFAR, Calcul Quebec, Compute Canada, Disney Research and ONR Grant N00014-14-10232. The authors thank the developers of Theano [32] and Keras [33] and Jimmy Ba for many stimulating discussions."}], "references": [{"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.02367,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "First-order versus second-order single-layer recurrent neural networks", "author": ["Mark W Goudreau", "C Lee Giles", "Srimat T Chakradhar", "D Chen"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Recurrent batch normalization", "author": ["Tim Cooijmans", "Nicolas Ballas", "C\u00e9sar Laurent", "Aaron Courville"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "An inequality with application to statistical estimation for probabilistic functions of markov processes and to a model for ecology", "author": ["LE Baum", "JA Eagon"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1967}, {"title": "Second-order recurrent neural networks for grammatical inference", "author": ["C Lee Giles", "D Chen", "CB Miller", "HH Chen", "GZ Sun", "YC Lee"], "venue": "In Neural Networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Architectural complexity measures of recurrent neural networks", "author": ["Saizheng Zhang", "Yuhuai Wu", "Tong Che", "Zhouhan Lin", "Roland Memisevic", "Ruslan Salakhutdinov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.08210,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "Subword language modeling with neural networks", "author": ["Tom\u00e1\u0161 Mikolov", "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink"], "venue": "preprint, (http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Regularizing rnns by stabilizing activations", "author": ["David Krueger", "Roland Memisevic"], "venue": "arXiv preprint arXiv:1511.08400,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "First-pass large vocabulary continuous speech recognition using bi-directional recurrent dnns", "author": ["Awni Y Hannun", "Andrew L Maas", "Daniel Jurafsky", "Andrew Y Ng"], "venue": "arXiv preprint arXiv:1408.2873,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1508.04395,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "arXiv preprint arXiv:1507.08240,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Regularization and nonlinearities for neural language models: when are they needed", "author": ["Marius Pachitariu", "Maneesh Sahani"], "venue": "arXiv preprint arXiv:1301.5650,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Weighted finite-state transducers in speech recognition", "author": ["Mehryar Mohri", "Fernando Pereira", "Michael Riley"], "venue": "Computer Speech & Language,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Refining hidden markov models with recurrent neural networks", "author": ["T. Wessels", "C.W. Omlin"], "venue": "In Neural Networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "Semi-supervised learning with ladder network", "author": ["Antti Rasmus", "Harri Valpola", "Mikko Honkala", "Mathias Berglund", "Tapani Raiko"], "venue": "arXiv preprint arXiv:1507.02672,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Deconstructing the ladder network architecture", "author": ["Mohammad Pezeshki", "Linxi Fan", "Philemon Brakel", "Aaron Courville", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1511.06430,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "arXiv preprint arXiv:1412.6575,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Theano: A python framework for fast computation of mathematical expressions, 2016", "author": ["Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Recently there has been a resurgence of new structural designs for recurrent neural networks (RNNs) [1, 2, 3].", "startOffset": 100, "endOffset": 109}, {"referenceID": 1, "context": "Recently there has been a resurgence of new structural designs for recurrent neural networks (RNNs) [1, 2, 3].", "startOffset": 100, "endOffset": 109}, {"referenceID": 2, "context": "Recently there has been a resurgence of new structural designs for recurrent neural networks (RNNs) [1, 2, 3].", "startOffset": 100, "endOffset": 109}, {"referenceID": 3, "context": "Most of these designs are derived from popular structures including vanilla RNNs, Long Short Term Memory networks (LSTMs) [4] and Gated Recurrent Units (GRUs) [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "Most of these designs are derived from popular structures including vanilla RNNs, Long Short Term Memory networks (LSTMs) [4] and Gated Recurrent Units (GRUs) [5].", "startOffset": 159, "endOffset": 162}, {"referenceID": 5, "context": "The result of this modification changes the RNN from first order to second order [6], while introducing no extra parameters.", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "It can also be combined with other RNN training techniques such as Recurrent Batch Normalization [7].", "startOffset": 97, "endOffset": 100}, {"referenceID": 7, "context": "We further discuss its relationship to existing models, including Hidden Markov Models (HMMs) [8], second order RNNs [9, 6] and Multiplicative RNNs [10].", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "We further discuss its relationship to existing models, including Hidden Markov Models (HMMs) [8], second order RNNs [9, 6] and Multiplicative RNNs [10].", "startOffset": 117, "endOffset": 123}, {"referenceID": 5, "context": "We further discuss its relationship to existing models, including Hidden Markov Models (HMMs) [8], second order RNNs [9, 6] and Multiplicative RNNs [10].", "startOffset": 117, "endOffset": 123}, {"referenceID": 9, "context": "We further discuss its relationship to existing models, including Hidden Markov Models (HMMs) [8], second order RNNs [9, 6] and Multiplicative RNNs [10].", "startOffset": 148, "endOffset": 152}, {"referenceID": 10, "context": "RNN with multiple skip connections [11] or in feedforward models like residual networks [12]), one can implement pairwise Multiplicative Integration for integrating all k information sources.", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "RNN with multiple skip connections [11] or in feedforward models like residual networks [12]), one can implement pairwise Multiplicative Integration for integrating all k information sources.", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "1 Exploratory Experiments To further understand the functionality of Multiplicative Integration, we take a simple RNN for illustration, and perform several exploratory experiments on the character level language modeling task using Penn-Treebank dataset [13], following the data partition in [14].", "startOffset": 254, "endOffset": 258}, {"referenceID": 13, "context": "1 Exploratory Experiments To further understand the functionality of Multiplicative Integration, we take a simple RNN for illustration, and perform several exploratory experiments on the character level language modeling task using Penn-Treebank dataset [13], following the data partition in [14].", "startOffset": 292, "endOffset": 296}, {"referenceID": 14, "context": "All models have a single hidden layer of size 2048, and we use Adam optimization algorithm [15] with learning rate 1e\u22124.", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "48, which is comparable to a vanilla-RNN with stabilizing regularizer [16], while lin-RNN performs rather poorly, achieving a test BPC of over 2.", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "For both datasets, we follow the training protocols in [14] and [1] respectively.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "For both datasets, we follow the training protocols in [14] and [1] respectively.", "startOffset": 64, "endOffset": 67}, {"referenceID": 16, "context": "DRNN+CTCbeamsearch [17] 10.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "1 Encoder-Decoder [18] 6.", "startOffset": 18, "endOffset": 22}, {"referenceID": 18, "context": "3 LSTM+CTCbeamsearch [19] 9.", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "7 Eesen [20] - 7.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "RNN [14] 1.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "42 HF-MRNN [14] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "41 RNN+stabalization [16] 1.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "RNN+smoothReLu [21] 1.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "55 HF-MRNN [14] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 21, "context": "stacked-LSTM [22] 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "67 GF-LSTM [1] 1.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "58 grid-LSTM [2] 1.", "startOffset": 13, "endOffset": 16}, {"referenceID": 21, "context": "On Hutter Challenge Wikipedia dataset, we compare our MI-LSTM (single layer with 2048 unit, \u224817M, with {\u03b1,\u03b21,\u03b22,b} initialized as {1, 1, 1, 0}) to the previous stacked LSTM (7 layers, \u224827M) [22], GF-LSTM (5 layers, \u224820M) [1], and grid-LSTM (6 layers, \u224817M) [2].", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "On Hutter Challenge Wikipedia dataset, we compare our MI-LSTM (single layer with 2048 unit, \u224817M, with {\u03b1,\u03b21,\u03b22,b} initialized as {1, 1, 1, 0}) to the previous stacked LSTM (7 layers, \u224827M) [22], GF-LSTM (5 layers, \u224820M) [1], and grid-LSTM (6 layers, \u224817M) [2].", "startOffset": 221, "endOffset": 224}, {"referenceID": 1, "context": "On Hutter Challenge Wikipedia dataset, we compare our MI-LSTM (single layer with 2048 unit, \u224817M, with {\u03b1,\u03b21,\u03b22,b} initialized as {1, 1, 1, 0}) to the previous stacked LSTM (7 layers, \u224827M) [22], GF-LSTM (5 layers, \u224820M) [1], and grid-LSTM (6 layers, \u224817M) [2].", "startOffset": 257, "endOffset": 260}, {"referenceID": 19, "context": "We follow the same data preparation process and model setting as in [20], and we use 59 characters as the targets for the acoustic modelling.", "startOffset": 68, "endOffset": 72}, {"referenceID": 22, "context": "Decoding is done with the CTC [23] based weighted finite-state transducers (WFSTs) [24] as proposed by [20].", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": "Decoding is done with the CTC [23] based weighted finite-state transducers (WFSTs) [24] as proposed by [20].", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "Decoding is done with the CTC [23] based weighted finite-state transducers (WFSTs) [24] as proposed by [20].", "startOffset": 103, "endOffset": 107}, {"referenceID": 24, "context": "Next, we evaluate our Multiplicative Integration on the Skip-Thought model of [25].", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "[7] reports better results but they use much larger models (\u224816M) which is not directly comparable.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "uni-skip [25] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "2872 bi-skip [25] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "2995 combine-skip [25] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "uni-skip [25] 73.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "9 bi-skip [25] 71.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "2 combine-skip [25] 73.", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "uni-skip [25] 75.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "9 bi-skip [25] 73.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "3 combine-skip [25] 76.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "LSTM [7] 0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "5033 BN-LSTM [7] 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "4951 BN-everywhere [7] 0.", "startOffset": 19, "endOffset": 22}, {"referenceID": 24, "context": "the training data can take up to a week on a high-end GPU (as reported in [25]).", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "We also train a baseline model with the same size, referred to as uni-skip(ours), which essentially reproduces the original model of [25].", "startOffset": 133, "endOffset": 137}, {"referenceID": 24, "context": "We also evaluated both models after one week of training, with the best results being reported on six out of eight tasks reported in [25]: semantic relatedness task on SICK dataset, paraphrase detection task on Microsoft Research Paraphrase Corpus, and four classification benchmarks: movie review sentiment (MR), customer product reviews (CR), subjectivity/objectivity classification (SUBJ), and opinion polarity (MPQA).", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "Recently, [7] introduced Batch-Normalization [26] for RNNs.", "startOffset": 10, "endOffset": 13}, {"referenceID": 25, "context": "Recently, [7] introduced Batch-Normalization [26] for RNNs.", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "LSTM [7]", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "BN-LSTM [7]", "startOffset": 8, "endOffset": 11}, {"referenceID": 26, "context": "Attentive Reader Model [27] for the question answering task using the CNN corpus10.", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "A vanilla LSTM attentive reader model with a single hidden layer size 240 (same as [7]) as our baseline, referred to as LSTM (ours), 2.", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "MI-LSTM with Batch-Norm everywhere (as detailed in [7]), referred to as MI-LSTM+BN-everywhere.", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "We compared our models to results reported in [7] (referred to as LSTM, BN-LSTM and BN-LSTM everywhere) 11.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "We follow the experimental protocol of [7]12 and use exactly the same settings as theirs, except we remove the gradient clipping for MI-LSTMs.", "startOffset": 39, "endOffset": 42}, {"referenceID": 27, "context": "A direct mapping can be constructed as follows (see [28] for a similar derivation).", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "MI-RNN is related to the second order RNN [9, 6] and the multiplicative RNN (MRNN) [10].", "startOffset": 42, "endOffset": 48}, {"referenceID": 5, "context": "MI-RNN is related to the second order RNN [9, 6] and the multiplicative RNN (MRNN) [10].", "startOffset": 42, "endOffset": 48}, {"referenceID": 9, "context": "MI-RNN is related to the second order RNN [9, 6] and the multiplicative RNN (MRNN) [10].", "startOffset": 83, "endOffset": 87}, {"referenceID": 6, "context": "Note that [7] used a truncated version of the original dataset in order to save computation.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "Learning curves and the final result number are obtained by emails correspondence with authors of [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 9, "context": "(2) Easier Optimization: In tensor decomposition methods, the products of three different (low-rank) matrices generally makes it hard to optimize [10].", "startOffset": 146, "endOffset": 150}, {"referenceID": 28, "context": "In particular, [29] proposed the ladder network that achieves promising results on semi-supervised learning.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "The performance would severely degrade without this product as empirically shown by [30].", "startOffset": 84, "endOffset": 88}, {"referenceID": 30, "context": "[31] explored neural embedding approaches in knowledge bases by formulating relations as bilinear and/or linear mapping functions, and compared a variety of embedding models on the link prediction task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "The authors thank the developers of Theano [32] and Keras [33], and also thank Jimmy Ba for many thought-provoking discussions.", "startOffset": 43, "endOffset": 47}], "year": 2016, "abstractText": "We introduce a general and simple structural design called \u201cMultiplicative Integration\u201d (MI) to improve recurrent neural networks (RNNs). MI changes the way in which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models.", "creator": "LaTeX with hyperref package"}}}