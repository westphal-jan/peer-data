{"id": "1606.07046", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Semantic Parsing to Probabilistic Programs for Situated Question Answering", "abstract": "Situated question answering is the problem of answering questions about an environment such as an image. This problem requires interpreting both a question and the environment, and is challenging because the set of interpretations is large, typically superexponential in the number of environmental objects. Existing models handle this challenge by making strong -- and untrue -- independence assumptions. We present Parsing to Probabilistic Programs (P3), a novel situated question answering model that utilizes approximate inference to eliminate these independence assumptions and enable the use of global features of the question/environment interpretation. Our key insight is to treat semantic parses as probabilistic programs that execute nondeterministically and whose possible executions represent environmental uncertainty. We evaluate our approach on a new, publicly-released data set of 5000 science diagram questions, finding that our approach outperforms several competitive baselines.", "histories": [["v1", "Wed, 22 Jun 2016 19:19:29 GMT  (306kb,D)", "http://arxiv.org/abs/1606.07046v1", "EMNLP 2016 submission, 10 pages"], ["v2", "Sat, 24 Sep 2016 00:37:43 GMT  (3772kb,D)", "http://arxiv.org/abs/1606.07046v2", "EMNLP 2016, 11 pages"]], "COMMENTS": "EMNLP 2016 submission, 10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jayant krishnamurthy", "oyvind tafjord", "aniruddha kembhavi"], "accepted": true, "id": "1606.07046"}, "pdf": {"name": "1606.07046.pdf", "metadata": {"source": "CRF", "title": "Semantic Parsing to Probabilistic Programs for Situated Question Answering", "authors": ["Jayant Krishnamurthy", "Oyvind Tafjord"], "emails": ["jayantk@allenai.org", "oyvindt@allenai.org"], "sections": [{"heading": "1 Introduction", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green.\" rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc ide for the green for the green for the green.\" rE \"s tis rf\u00fc ide rf\u00fc ide for the green for the green for the green"}, {"heading": "2 Prior Work", "text": "Most work assumes that these models are independent and that environmental interpretation consists of independent predicate instances (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Malinowski and Fritz, 2014). Seo etal., (2015) contains hard constraints on the common question / environmental interpretation, but their approach does not generalize soft constraints or arbitrary logical forms.Another line of work assumes that there is a semantic parser that focuses exclusively on the interpretation of the environment, which includes robot orientation following the execution of the environment, 2010; Tellex et al., 2011; Howard et al., 2014b; Howard et al., 2014a) and answers the question against knowledge representations extracted from the text. (Berant et al, 2014; Krishnamurthy et al, 2015)."}, {"heading": "3 Parsing to Probabilistic Programs (P 3)", "text": "This section describes our model, Parsing to Probabilistic Programs (P 3). Input to the model q is a question and an environment and its output is a denotation, which is a formal answer to the question. P 3 has two factors, a semantic parser and an execution model. The semantic parser represents syntactic parses and logical forms for the question (Figure 2 shows a sample parse. These logical forms are probabilistic programs with several possible executions, each of which can return a different density. The set of executions is determined by an initialization program that defines the functions of the logical form using the choice. Figure 4 shows pseudo-code for the food web initialization program and Figure 3 shows how a logical form is executed. The execution model assigns a score to each of these executions. We shift additional application-specific discussions to Section 4.Formally, P 3 is a logical model that predicts a denotation for an environment."}, {"heading": "3.1 Semantic Parsing", "text": "The factor fprs represents a semantic combinatorial categorical grammar (CCG) (Zettlemoyer and Collins, 2005) that evaluates logical forms for a question. Considering a lexicon that maps words to syntactical categories and logical forms, CCG defines a set of possible syntactic parses t and logical forms' for a question q. Figure 2 shows an example of a CCG parse. fprs is a loglinear model of parses (', t): fprs (', t | q; \u03b8prs) = exp {\u03b8Tprs\u03c6 (', t, q)} 1In our experiments we learn the lexicon automatically in a reprocessing step (Section 5.2)."}, {"heading": "3.2 Execution Model", "text": "The factor fex is a loglinear model of the execution of a logical form in a given environment. Remember that logical forms in P 3 are probabilistic programs with a series of possible executions, each of which can return a different value. Each execution is a sequence, e = [e0, e1, e2,..., en], where e0 is the starting state of the program, ei stands for the state immediately after the ith call, and en is the state at completion. The value of an execution is: fex (e, '| v; successex) = n, i = 1 exp {\u03b8Tex\u03c6 (ei \u2212 1, ei', v)} In the above equation, successex represents the parameters of the model and \u03c6 a function that produces a characteristic vector representing the difference between the sequential program states ei \u2212 1 and a given environmental states v and the logical form. It is important that these characteristics may contain arbitrary characteristics, for example, in the 4.logical environment."}, {"heading": "3.3 Inference", "text": "P 3 is designed to be based on approximate conclusions: Our goal is to use rich features to make local decisions accurately, as with linear time parsers (Nivre et al., 2006). We perform approximate conclusions using a two-step bar search. In response to a question q, the first stage performs a bar search using CCG parses to create a list of logical forms evaluated by fprs. This step is done by using a CKY-style chart parsing algorithm that then marginalizes the syntactic parses. The second stage performs a bar search using executions of any logical form. This search first rewrites the logical form into continuous form, then receives a bar of continuations - each of which represents a partial execution - achieved by fex. Passing transformation allows it to be implemented as a function that adds multiple continuations - one per return value - to the search queue (2014 and 2014)."}, {"heading": "3.4 Training", "text": "The training data {(qi, vi, ci)} ni = 1 are a collection of questions qi and environments vi paired with monitoring oracles ci. ci (e) = 1 for correct execution e and ci (e) = 0 otherwise. The oracle ci can be used to implement various types of monitoring, including: (1) labeled labels by verifying the return value of e at completion and (2) labeled environments by verifying decisions of e on environmental interpretation. The chart question answer oracle combines both forms of monitoring (Section 4.5). The objective function O is the logical probability of predicting correct execution: O (TB) = n \u2211 i = 1 protocol \u0445 e, l l l, l, t ci (e) P (e, ', t | qi, vi; 2000) We optimize this objective function using stochastic gradients ascent by improving the approximate match between the individual margins by marginalizing the steps above 3."}, {"heading": "4 Diagram Question Answering with P 3", "text": "As a case study, we apply P 3 to the task of answering diagram questions in the field of 8th grade sciences. A few steps are required to apply P 3. First, we write an initialization program that defines the constitutive functions of logical forms, the representation of the environment and the corresponding uncertainty. Second, we create a component for selecting an answer based on a designation. Finally, we define the characteristics of the model and the supervision oracle."}, {"heading": "4.1 Food Web Diagram Questions", "text": "The input consists of an image representing a food web, a question in natural language, and a list of answers in natural language (Figure 1).The goal is to select the correct answer from these options. A food web represents the flow of energy in an ecosystem as a directed graph, with each vertex indicating an organism and an edge from x to y indicating that the organism is eating the organism x. Many different types of questions can be asked via a food web, ranging from what eats what, to the roles of animals and the consequences of population changes. This task has many regularities that require global characteristics: Food webs are typically acyclic, and certain animals usually have certain roles (e.g. mice are herbivores).We have collected and published a data set for this task (Section 5.1).We prepare the images in the data set using a computer vision system that identifies candidate diagrams (Khaet 2016)."}, {"heading": "4.2 Initialization Program", "text": "The diagram initialization program defines a distribution over partial food webs that take a logical form. It assumes that each diagram has a known group of units, which in our case are a series of extracted text labels. It defines a collection of learned predicates about these units to represent represented food webs; these predicates evoke uncertainty. It also includes deterministic functions that encode background knowledge. Figure 4 shows pseudo code for part of the initialization program. Food webs are represented by two learned predicates: ORGANISM (x) indicates whether the text label x is an organism (as opposed to the image title); and EATS (x, y) indicates that the organism eats x organism. One way to represent uncertainty about food webs is to determine the truth value of each predicate substance in its embodiment."}, {"heading": "4.3 Answer Selection", "text": "The solution selection component uses string-match heuristics and an LSTM to generate a distribution over multiple-choice responses that have a distribution over the terms predicted by P 3. String-match heuristics evaluates each answer option with a score and then selects the answer with the highest score, which is not taken into account in the case of a tie. Calculation of the score depends on the type of the score. If the score is a group of entities, grading is an approximate number of entities in the score that are mentioned in the answer based on a blurred string match. If the grading is a series of change events, grading is a blurred agreement of both the direction of change and the animal name. If the grading is a number, the string matching comparison is simply the application of this and the distribution of the hayrification results in a denomination."}, {"heading": "4.4 Execution Features", "text": "The execution model uses three types of attributes: instance traits, predicate traits, and identifiers. Instance traits treat each predicate instance independently of each other, while the other traits are functions of multiple predicate instances and the logical form. We provide a complete listing of attributes in an online appendix. Instance traits are always present when an executor selects a truth value for a predicate instance. These traits are similar to the traits per predicate instance used in previous work to distribute across possible worlds. For ORGANISM (x), our attributes include the extraction value of the vision model for x and indicator traits for the number of tokens in x. For EATS (x, y), our attributes are different combinations of the values of the vision model for arrows containing the text labels x and y.Predicate traits for the number of tokens in x."}, {"heading": "4.5 Supervision Oracle", "text": "We assume that each diagram has been labeled with a food web. An execution is correct only if (1) all of its global variables encoding the food web match the labeled food web, and (2) the selection of string match responses applied to its label selects the correct response. The first constraint guarantees that each logical form has at most one correct execution for each given diagram."}, {"heading": "5 Evaluation", "text": "Our evaluation compares P 3 with both possible worlds and neural networking approaches in our dataset of web diagram questions on food. An ablation study shows that both sets of global characteristics demonstrate accuracy. Finally, we demonstrate the universality of P 3 by applying it to a previously published dataset, obtaining state-of-the-art results. We have attached the dataset to the submission and will release the code when the work is published."}, {"heading": "5.1 FOODWEBS Data Set", "text": "FOODWEBS consists of \u0445 500 Food Web Diagrams and \u0445 5000 Questions, which are designed to imitate the actual questions that arose in the 8th grade science exams. The pull / validation / test sets contain \u0445 300 / 100 / 100 diagrams and the related questions. In addition to the correct answer for each question, the data set contains three types of comments. First, each chart is commented on the Food Web, which it depicts with the ORGANISM and EATS predicates. Second, each chart has predictions from a vision system for various chart elements such as arrows and text labels (Kembhavi et al., 2016). These are loud predictions, not fundamentals. Finally, each question is commented on by the authors with a logical form or zero if its meaning cannot be represented by our predicates. These logical forms are not used to train P 3, but are useful per component to measure errors."}, {"heading": "5.2 Baseline Comparison", "text": "The first baseline, WORLDS, is a possible world model based on Malinowski and Fritz (2014). This baseline learns a semantic parser P (', t | q) and a distribution via food webs P (w | v), then evaluates \"to w to generate a distribution via denotations, which is implemented by the independent application of P 3's CCG parser (question / answer pairs and labeled food webs) and a possible execution model (on labeled food webs). The CCG lexicon for both P 3 and WORLDS was generated by applying PAL (Krishnamurthy, 2016) to the same data. Finally, both models choose answers that are compared in Section 4.3.We have compared P 3 with several neural network bases."}, {"heading": "5.3 Ablation Study", "text": "We conducted an ablation study to better understand the effects of LSTM response selection and global characteristics. Table 2 shows the accuracy of P 3, which is trained without these components. We note that the LSTM response selection improves accuracy by 9 points as expected, which is due to the importance of linguistic prior knowledge. Global characteristics improve accuracy by 7 points, which is roughly comparable to the delta between P 3 and WORLDS in Table 1."}, {"heading": "5.4 Component Error Analysis", "text": "Our third experiment analyzes the sources of error by training and evaluating P 3 while providing the logical form of gold, the food web, or both as input. Table 3 shows the accuracy of these three models. The final entry shows the greatest possible accuracy given our logical formal language and response selection. The greater performance improvement in golden food webs suggests that the execution model is responsible for more errors than semantic parsing, although both components contribute to this."}, {"heading": "5.5 SCENE Experiments", "text": "Our last experiment applied P 3 to the SCENE dataset of Krishnamurthy and Kollar (2013). The task in this dataset is to identify the set of objects in an image characterized by a natural language expression, e.g. \"blue cup to the left of the screen.\" We use the provided CCG lexicon and predicate vocabulary and create a learned predicate for each in the initialization program. We use the provided instance characteristics, add predicate and denotation size characteristics. Table 4 compares P 3 with previous work on SCENE. We consider three different supervision conditions: QA moves with question-answer pairs, QA + E includes further labeled surroundings, and QA + E + LF includes additional labeled logical forms. We trained P 3 in the first two conditions, while previous work in the first and third condition was trained better than previous work with labeled A and P 3 fills better."}, {"heading": "6 Conclusion", "text": "P 3 trains a semantic parser to predict logical forms, which are probabilistic programs whose possible execution presents environmental uncertainty. We demonstrate this model against a challenging new dataset of 5000 scientific diagram questions and find that it exceeds several competitive baselines and that its global properties improve accuracy. P 3 has several beneficial properties. First, P 3 can easily be applied to new problems: all you have to do is write an initialization program and define the execution characteristics. Second, the initialization program can be used to encode a broad class of environmental assumptions. Thus, the model can assume that each noun refers to a single object. Combining semantic parsing and probabilistic programming makes P 3 an expressive and flexible model with many potential applications."}], "references": [{"title": "2016a. Deep compositional question answering with neural module", "author": ["Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": null, "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": null, "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "VQA: Visual question answering", "author": ["Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In International Conference on Computer Vision (ICCV)", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics", "author": ["Artzi", "Zettlemoyer2013] Yoav Artzi", "Luke Zettlemoyer"], "venue": null, "citeRegEx": "Artzi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2013}, {"title": "Modeling biological processes for reading comprehension", "author": ["Vivek Srikumar", "PeiChun Chen", "Abby Vander Linden", "Brittany Harding", "Brad Huang", "Peter Clark", "Christopher D. Manning"], "venue": "Proceedings of EMNLP", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["Chen", "Mooney2011] David L. Chen", "Raymond J. Mooney"], "venue": "In Proceedings of the 25th AAAI Conference on Artificial Intelligence", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Wide-coverage efficient statistical parsing with CCG and log-linear models", "author": ["Clark", "Curran2007] Stephen Clark", "James R. Curran"], "venue": null, "citeRegEx": "Clark et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2007}, {"title": "The Design and Implementation of Probabilistic Programming Languages. http://dippl.org", "author": ["Goodman", "Stuhlm\u00fcller2014] Noah D Goodman", "Andreas Stuhlm\u00fcller"], "venue": null, "citeRegEx": "Goodman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2014}, {"title": "Church: A language for generative models", "author": ["Vikash K. Mansinghka", "Daniel M. Roy", "Keith Bonawitz", "Joshua B. Tenenbaum"], "venue": "In Uncertainty in Artificial Intelligence", "citeRegEx": "Goodman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2008}, {"title": "Efficient natural language interfaces for assistive robots", "author": ["Istvan Chung", "Oron Propp", "Matthew R. Walter", "Nicholas Roy"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "Howard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Howard et al\\.", "year": 2014}, {"title": "A natural language planner interface for mobile manipulators", "author": ["Stefanie Tellex", "Nicholas Roy"], "venue": "IEEE International Conference on Robotics and Automation (ICRA)", "citeRegEx": "Howard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Howard et al\\.", "year": 2014}, {"title": "A diagram is worth a dozen images. CoRR, abs/1603.07396", "author": ["Mike Salvato", "Eric Kolve", "Min Joon Seo", "Hannaneh Hajishirzi", "Ali Farhadi"], "venue": null, "citeRegEx": "Kembhavi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kembhavi et al\\.", "year": 2016}, {"title": "Toward understanding natural language directions", "author": ["Kollar et al.2010] Thomas Kollar", "Stefanie Tellex", "Deb Roy", "Nicholas Roy"], "venue": "In Proceedings of the 5th ACM/IEEE International Conference on HumanRobot Interaction", "citeRegEx": "Kollar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kollar et al\\.", "year": 2010}, {"title": "Jointly learning to parse and perceive: Connecting natural language to the physical world", "author": ["Krishnamurthy", "Kollar2013] Jayant Krishnamurthy", "Thomas Kollar"], "venue": "Transactions of the Association of Computational Linguistics \u2013 Volume", "citeRegEx": "Krishnamurthy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2013}, {"title": "Weakly supervised training of semantic parsers", "author": ["Krishnamurthy", "Tom M. Mitchell"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Krishnamurthy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2012}, {"title": "Learning a compositional semantics for freebase with an open predicate vocabulary. Transactions of the Association for Computational Linguistics, 3:257\u2013270", "author": ["Krishnamurthy", "Tom M. Mitchell"], "venue": null, "citeRegEx": "Krishnamurthy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2015}, {"title": "Probabilistic models for learning a semantic parser lexicon", "author": ["Jayant Krishnamurthy"], "venue": "In NAACL", "citeRegEx": "Krishnamurthy.,? \\Q2016\\E", "shortCiteRegEx": "Krishnamurthy.", "year": 2016}, {"title": "Learning dependency-based compositional semantics", "author": ["Liang et al.2011] Percy Liang", "Michael I. Jordan", "Dan Klein"], "venue": "In Proceedings of the Association for Computational Linguistics", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Malinowski", "Fritz2014] Mateusz Malinowski", "Mario Fritz"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Malinowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Marcus Rohrbach", "Mario Fritz"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["Nicholas FitzGerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Matuszek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Maltparser: A data-driven parsergenerator for dependency parsing", "author": ["Nivre et al.2006] Joakim Nivre", "Johan Hall", "Jens Nilsson"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Associa-", "citeRegEx": "Nivre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "Solving geometry problems: Combining text and diagram interpretation", "author": ["Seo et al.2015] Minjoon Seo", "Hannaneh Hajishirzi", "Ali Farhadi", "Oren Etzioni", "Clint Malcolm"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Seo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["Thomas Kollar", "Steven Dickerson", "Matthew Walter", "Ashis Banerjee", "Seth Teller", "Nicholas Roy"], "venue": "In AAAI Conference on Artifi-", "citeRegEx": "Tellex et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Stacked attention networks for image question answering", "author": ["Yang et al.2015] Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alexander J. Smola"], "venue": "arXiv preprint arXiv:1511.02274", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Learning to map sentences to logical form: structured classification with probabilistic categorial grammars", "author": ["Zettlemoyer", "Collins2005] Luke S. Zettlemoyer", "Michael Collins"], "venue": "In UAI \u201905, Proceedings of the 21st Conference in Uncertainty in Artificial In-", "citeRegEx": "Zettlemoyer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "Examples of this problem include visual question answering (Antol et al., 2015), robot direction following (Kollar et al.", "startOffset": 59, "endOffset": 79}, {"referenceID": 12, "context": ", 2015), robot direction following (Kollar et al., 2010), and even text question answering (Berant et al.", "startOffset": 35, "endOffset": 56}, {"referenceID": 4, "context": ", 2010), and even text question answering (Berant et al., 2014).", "startOffset": 42, "endOffset": 63}, {"referenceID": 20, "context": "In order to reason about the large set of interpretations, existing possible worlds models for situated question answering make strong independence assumptions that preclude them from representing these complex dependencies (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Malinowski and Fritz, 2014).", "startOffset": 224, "endOffset": 307}, {"referenceID": 20, "context": "Most work has assumed that these models are independent and the environment interpretation consists of independent predicate instances (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Malinowski and Fritz, 2014).", "startOffset": 135, "endOffset": 218}, {"referenceID": 16, "context": ", 2012; Krishnamurthy and Kollar, 2013; Malinowski and Fritz, 2014). Seo et al., (2015) incorporate hard constraints on the joint question/environment interpretation; however, their approach does not generalize to soft constraints or arbitrary logical forms.", "startOffset": 8, "endOffset": 88}, {"referenceID": 12, "context": "This work includes robot direction following (Kollar et al., 2010; Tellex et al., 2011; Howard et al., 2014b; Howard et al., 2014a) and question answering against knowledge representations extracted from text (Berant et al.", "startOffset": 45, "endOffset": 131}, {"referenceID": 24, "context": "This work includes robot direction following (Kollar et al., 2010; Tellex et al., 2011; Howard et al., 2014b; Howard et al., 2014a) and question answering against knowledge representations extracted from text (Berant et al.", "startOffset": 45, "endOffset": 131}, {"referenceID": 4, "context": ", 2014a) and question answering against knowledge representations extracted from text (Berant et al., 2014; Krishnamurthy and Mitchell, 2015).", "startOffset": 86, "endOffset": 141}, {"referenceID": 2, "context": "Neural networks have been recently used for visual question answering (Antol et al., 2015; Malinowski et al., 2015; Yang et al., 2015; Andreas et al., 2016a).", "startOffset": 70, "endOffset": 157}, {"referenceID": 19, "context": "Neural networks have been recently used for visual question answering (Antol et al., 2015; Malinowski et al., 2015; Yang et al., 2015; Andreas et al., 2016a).", "startOffset": 70, "endOffset": 157}, {"referenceID": 25, "context": "Neural networks have been recently used for visual question answering (Antol et al., 2015; Malinowski et al., 2015; Yang et al., 2015; Andreas et al., 2016a).", "startOffset": 70, "endOffset": 157}, {"referenceID": 17, "context": ", a database), for applications such as question answering (Zettlemoyer and Collins, 2005; Liang et al., 2011), direction following (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), and information extraction (Krishnamurthy and Mitchell, 2012).", "startOffset": 59, "endOffset": 110}, {"referenceID": 8, "context": "We express logical forms in a probabilistic variant of Scheme similar to Church (Goodman et al., 2008); however, in this paper we use Python-like pseudocode for clarity.", "startOffset": 80, "endOffset": 102}, {"referenceID": 21, "context": "P 3 is designed to rely on approximate inference: our goal is to use rich features to accurately make local decisions, as in linear-time parsers (Nivre et al., 2006).", "startOffset": 145, "endOffset": 165}, {"referenceID": 11, "context": "We preprocess the images in the data set using a computer vision system that identifies candidate diagram elements (Kembhavi et al., 2016).", "startOffset": 115, "endOffset": 138}, {"referenceID": 2, "context": "Such knowledge is known to be essential for visual question answering (Antol et al., 2015; Andreas et al., 2016b) and important in our task as well.", "startOffset": 70, "endOffset": 113}, {"referenceID": 11, "context": "Second, each diagram has predictions from a vision system for various diagram elements such as arrows and text labels (Kembhavi et al., 2016).", "startOffset": 118, "endOffset": 141}, {"referenceID": 16, "context": "The CCG lexicon for both P 3 and WORLDS was generated by applying PAL (Krishnamurthy, 2016) to the same data.", "startOffset": 70, "endOffset": 91}, {"referenceID": 11, "context": "The second baseline, DQA, is a diagram question answering model that uses a predicted \u201cdiagram parse\u201d of the image (Kembhavi et al., 2016).", "startOffset": 115, "endOffset": 138}, {"referenceID": 2, "context": "with other multiple-choice situated question answering tasks (Antol et al., 2015).", "startOffset": 61, "endOffset": 81}, {"referenceID": 16, "context": "Our final experiment applies P 3 to the SCENE data set of Krishnamurthy and Kollar (2013). The task in this data set is to identify the set of objects in an image denoted by a natural language expression, such as \u201cblue mug to the left of the monitor.", "startOffset": 58, "endOffset": 90}, {"referenceID": 16, "context": "KK2013 results are from Krishnamurthy and Kollar (2013).", "startOffset": 24, "endOffset": 56}], "year": 2016, "abstractText": "Situated question answering is the problem of answering questions about an environment such as an image. This problem requires interpreting both a question and the environment, and is challenging because the set of interpretations is large, typically superexponential in the number of environmental objects. Existing models handle this challenge by making strong \u2013 and untrue \u2013 independence assumptions. We present Parsing to Probabilistic Programs (P ), a novel situated question answering model that utilizes approximate inference to eliminate these independence assumptions and enable the use of global features of the question/environment interpretation. Our key insight is to treat semantic parses as probabilistic programs that execute nondeterministically and whose possible executions represent environmental uncertainty. We evaluate our approach on a new, publicly-released data set of 5000 science diagram questions, finding that our approach outperforms several competitive baselines.", "creator": "LaTeX with hyperref package"}}}