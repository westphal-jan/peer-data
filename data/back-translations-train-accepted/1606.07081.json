{"id": "1606.07081", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Finite Sample Prediction and Recovery Bounds for Ordinal Embedding", "abstract": "The goal of ordinal embedding is to represent items as points in a low-dimensional Euclidean space given a set of constraints in the form of distance comparisons like \"item $i$ is closer to item $j$ than item $k$\". Ordinal constraints like this often come from human judgments. To account for errors and variation in judgments, we consider the noisy situation in which the given constraints are independently corrupted by reversing the correct constraint with some probability. This paper makes several new contributions to this problem. First, we derive prediction error bounds for ordinal embedding with noise by exploiting the fact that the rank of a distance matrix of points in $\\mathbb{R}^d$ is at most $d+2$. These bounds characterize how well a learned embedding predicts new comparative judgments. Second, we investigate the special case of a known noise model and study the Maximum Likelihood estimator. Third, knowledge of the noise model enables us to relate prediction errors to embedding accuracy. This relationship is highly non-trivial since we show that the linear map corresponding to distance comparisons is non-invertible, but there exists a nonlinear map that is invertible. Fourth, two new algorithms for ordinal embedding are proposed and evaluated in experiments.", "histories": [["v1", "Wed, 22 Jun 2016 20:06:10 GMT  (987kb,D)", "http://arxiv.org/abs/1606.07081v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["lalit jain", "kevin g jamieson", "robert d nowak"], "accepted": true, "id": "1606.07081"}, "pdf": {"name": "1606.07081.pdf", "metadata": {"source": "CRF", "title": "Finite Sample Prediction and Recovery Bounds for Ordinal Embedding", "authors": ["Lalit Jain", "Kevin Jamieson"], "emails": [], "sections": [{"heading": "1 Ordinal Embedding", "text": "Ordinary embedding, also known as non-metric multidimensional scaling, aims to represent items as points in Rd, so that the distances between items correspond as much as possible to a given set of ordinals comparisons, as item i is closer to item j than item k. This is a classic problem that is often used to visualize perceptual similarities [1, 2]. Recently, several authors have proposed a variety of new algorithms for learning metric embedding from ordinary information [3, 4, 5, 6, 7]. There has also been some theoretical progress in characterizing the consistency of ordinary embedding methods. For example, it has been shown that correct embedding in the border can be learned when the number of items grows [8, 9, 10]. However, a major shortcoming of all previous work is the lack of generalization and embedding of observation boundaries, and the number of problems for limiting the number of items."}, {"heading": "1.1 Ordinal Embedding from Noisy Data", "text": "(Consider n points x1, x2,. \u2212 n) Leave X = [x1] \u00b7 \u00b7 xn] \u2022 Rd \u00b7 n. The Euclidean distance matrix D? is defined in such a way that it contains the elements D? ij = [xj] 22. Ordinary embedding, however, is the problem of restoring ordinal constraints to distances. (This work focuses on \"triplet\" constraints of the form D? ij < D? ik, where 1 \u2264 i = 6 = k \u2264 n. Moreover, we observe only noisy references to these constraints, as follows: Each triplet t t = (i, j, k) has an associated probability pt pt > 1 / 2."}, {"heading": "1.2 Organization of Paper", "text": "These limits take advantage of the fact that the rank of a distance matrix of points in Rd is d + 2 at most. Then, we will consider the specific case of a known observation model and the maximum likelihood estimator in Section 3. The link function allows us to correlate prediction errors with error limits related to estimates of differences. In Section 4, we will examine the non-trivial problem of recovering D? and thus X from {D? ij \u2212 D? ik}. Finally, in Section 5, two new algorithms for ordinal embedding are proposed and evaluated experimentally."}, {"heading": "1.3 Notation and Assumptions", "text": "We will use (D?, G?) to denote the distance and gram matrices of the latent embedding, and (D, G) to denote an arbitrary distance matrix and the associated gram matrix. Observations {yt} contain information about D?, but distance matrices are invariant to rotation and translation, and thus it may only be possible to restore X to a rigid transformation. Therefore, throughout the paper we assume the following assumption. Assumption 1. To eliminate the translational ambiguity, we assume that the points x1,.. xn, that the gram matrices are centered at origin (i.e., the input n = 1 xi = 0).Define the centrix V: = I \u2212 1n11 T. Note that under the assumption above, XV = X matrix. Let us note that D? is determined by the gram matrix."}, {"heading": "2 Prediction Error Bounds", "text": "For t-T = (i, j, k), we define Lt as the linear operator (G = >). < G (S) < G (S) < G (S) < S (S) < S (S) < S (S) < S (S). < S (S). (Gjj \u2212 2Gij \u2212 Gkk + 2Gik.S (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S)............................................................................................................................................................................................ (S).................................................................................................................... (S)."}, {"heading": "3 Maximum Likelihood Estimation", "text": "We are now turning our attention to restoring metric information about G? & \u2212 S is a collection of triplets (< < G = > Q = > Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q (= Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q"}, {"heading": "4 Maximum Likelihood Embedding", "text": "In this section we have shown that the maximum probability estimator enables us to work in addition to gram matrices. Similar to the operators Lt (G), which have been defined above, we define the operators as operators. (D) The operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-operators-"}, {"heading": "5 Experimental Study", "text": "It is not the first time that we have to deal with the question of what the future of humanity is like. (...) It is not the first time that we have to deal with the question of what the future of humanity is like. (...) It is the second time that we have to deal with the question of what the future of humanity is like. (...) It is the first time that we have to deal with the future of humanity. (...) It is the second time that we have to deal with the future of humanity. (...) It is the second time that we have to deal with the future of humanity. (...) It is the second time that we have to deal with the future of humanity. (...) It is the first time that we have to deal with the future of humanity. (...) It is the second time that we have to deal with the future of humanity. (...) It is the second time that we have to deal with the future of humanity. (...) It is the second time that we have to deal with the future of humanity. (...) It is the first time that we have to deal with the future of the future of the world."}, {"heading": "6 Future work", "text": "For all specified n points in Rd and randomly selected distance comparison queries, our results show that the component of G? (the gram matrix associated with the points) can be accurately restored in the range of all possible distance comparisons from O (dn log n) queries, and we suspect that these limits are narrow. Furthermore, we have proven the existence of an estimator G *, so that we have G \u00b2 G? as the number of queries increases. A focus of our ongoing work is the characterization of finite sample limits for the velocity at which G \u00b2 G?. One way to approach such a result is to represent c1% D \u2212 D \u00b2 2F."}, {"heading": "7 Supplementary Materials for \u201cFinite Sample Error Bounds for Ordinal", "text": "Embedding \""}, {"heading": "7.1 Proof of Lemma 1", "text": "(c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c))) c))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))"}, {"heading": "7.2 Proof of Theorem 2", "text": "For y, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z, z"}, {"heading": "7.3 Proof of Lemma 3", "text": "Lemma 3. Let D and D \u00b2 be two different distance matrices of n points in Rd and Rd \u00b2 i. (Let C and C \u00b2 be orthogonal to J the components of D and D \u00b2. (Let D \u2212 \u2212 \u2212 n \u2212 C \u00b2 2F \u2264 Vis (C) \u2212 p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p"}, {"heading": "7.4 Proofs of Lemmas 4 and 5", "text": "Lemma 4. Let D be a Euclidean distance matrix on n points. \u2022 Then D is negative semidefinitive on the subspace. \u2022 The associated gram matrix G = \u2212 12V DV is a positive semidefinitive matrix. For x-1, Jx = \u2212 x so xT (\u2212 12V DV) x = \u2212 12 xTDx \u2264 0the definition of the first part of the theorem. Well, if x-kerD, 0-1 2 xTV DV x = \u2212 1 2 xT11Tx = \u2212 1 1TD1 (1Tx) 2 \u2264 0, the last inequality follows from the fact that 1TD1 > 0 is not negative because D."}], "references": [{"title": "The analysis of proximities: Multidimensional scaling with an unknown distance function", "author": ["Roger N Shepard"], "venue": "i. Psychometrika,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1962}, {"title": "Nonmetric multidimensional scaling: a numerical method", "author": ["Joseph B Kruskal"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1964}, {"title": "Generalized non-metric multidimensional scaling", "author": ["Sameer Agarwal", "Josh Wills", "Lawrence Cayton", "Gert Lanckriet", "David J Kriegman", "Serge Belongie"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Adaptively learning the crowd kernel", "author": ["Omer Tamuz", "Ce Liu", "Ohad Shamir", "Adam Kalai", "Serge J Belongie"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Low-dimensional embedding using adaptively selected ordinal data", "author": ["Kevin G Jamieson", "Robert D Nowak"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Stochastic triplet embedding", "author": ["Laurens Van Der Maaten", "Kilian Weinberger"], "venue": "In Machine Learning for Signal Processing (MLSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Learning multi-modal similarity", "author": ["Brian McFee", "Gert Lanckriet"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Uniqueness of ordinal embedding", "author": ["Matth\u00e4us Kleindessner", "Ulrike von Luxburg"], "venue": "In COLT, pages 40\u201367,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Local ordinal embedding", "author": ["Yoshikazu Terada", "Ulrike V Luxburg"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Some theory for ordinal embedding", "author": ["Ery Arias-Castro"], "venue": "arXiv preprint arXiv:1501.02861,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "1-bit matrix completion", "author": ["Mark A Davenport", "Yaniv Plan", "Ewout van den Berg", "Mary Wootters"], "venue": "Information and Inference, page iau006,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Convex Optimization & Euclidean Distance Geometry", "author": ["Jon Dattorro"], "venue": "Meboo Publishing USA,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Theory of classification: A survey of some recent advances", "author": ["St\u00e9phane Boucheron", "Olivier Bousquet", "G\u00e1bor Lugosi"], "venue": "ESAIM: probability and statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "An introduction to matrix concentration inequalities", "author": ["Joel A. Tropp"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Euclidean distance matrices: new characterization and boundary properties", "author": ["Pablo Tarazaga", "Juan E. Gallardo"], "venue": "Linear and Multilinear Algebra,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Next: A system for real-world development, evaluation, and application of active learning", "author": ["Kevin G Jamieson", "Lalit Jain", "Chris Fernandez", "Nicholas J Glattard", "Rob Nowak"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Conditional gradient with enhancement and truncation for atomic norm regularization", "author": ["Nikhil Rao", "Parikshit Shah", "Stephen Wright"], "venue": "In NIPS workshop on Greedy Algorithms,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Sharp time\u2013data tradeoffs for linear inverse problems", "author": ["Samet Oymak", "Benjamin Recht", "Mahdi Soltanolkotabi"], "venue": "arXiv preprint arXiv:1507.04793,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "This is a classic problem that is often used to visualize perceptual similarities [1, 2].", "startOffset": 82, "endOffset": 88}, {"referenceID": 1, "context": "This is a classic problem that is often used to visualize perceptual similarities [1, 2].", "startOffset": 82, "endOffset": 88}, {"referenceID": 2, "context": "Recently, several authors have proposed a variety of new algorithms for learning a metric embedding from ordinal information [3, 4, 5, 6, 7].", "startOffset": 125, "endOffset": 140}, {"referenceID": 3, "context": "Recently, several authors have proposed a variety of new algorithms for learning a metric embedding from ordinal information [3, 4, 5, 6, 7].", "startOffset": 125, "endOffset": 140}, {"referenceID": 4, "context": "Recently, several authors have proposed a variety of new algorithms for learning a metric embedding from ordinal information [3, 4, 5, 6, 7].", "startOffset": 125, "endOffset": 140}, {"referenceID": 5, "context": "Recently, several authors have proposed a variety of new algorithms for learning a metric embedding from ordinal information [3, 4, 5, 6, 7].", "startOffset": 125, "endOffset": 140}, {"referenceID": 6, "context": "Recently, several authors have proposed a variety of new algorithms for learning a metric embedding from ordinal information [3, 4, 5, 6, 7].", "startOffset": 125, "endOffset": 140}, {"referenceID": 7, "context": "For example, it has been shown that the correct embedding can be learned in the limit as the number of items grows [8, 9, 10].", "startOffset": 115, "endOffset": 125}, {"referenceID": 8, "context": "For example, it has been shown that the correct embedding can be learned in the limit as the number of items grows [8, 9, 10].", "startOffset": 115, "endOffset": 125}, {"referenceID": 9, "context": "For example, it has been shown that the correct embedding can be learned in the limit as the number of items grows [8, 9, 10].", "startOffset": 115, "endOffset": 125}, {"referenceID": 0, "context": "where the link function f : R\u2192 [0, 1] is known.", "startOffset": 31, "endOffset": 37}, {"referenceID": 10, "context": "We note that a problem known as one-bit matrix completion has similarly used link functions to recover structure [11].", "startOffset": 113, "endOffset": 117}, {"referenceID": 11, "context": "We refer the reader to [12] for an insightful and thorough treatment of the properties of distance matrices.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "The proof follows from standard statistical learning theory techniques, see for instance [13].", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "The above display says that |S| must scale like dn log(n) which is consistent with known finite sample bounds [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 14, "context": "In preparing this paper for publication we became aware of an alterntive proof of this theorem [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "It is not an attempt to perform an exhaustive empirical evaluation of different embedding techniques; for that see [17, 4, 6, 3].", "startOffset": 115, "endOffset": 128}, {"referenceID": 3, "context": "It is not an attempt to perform an exhaustive empirical evaluation of different embedding techniques; for that see [17, 4, 6, 3].", "startOffset": 115, "endOffset": 128}, {"referenceID": 5, "context": "It is not an attempt to perform an exhaustive empirical evaluation of different embedding techniques; for that see [17, 4, 6, 3].", "startOffset": 115, "endOffset": 128}, {"referenceID": 2, "context": "It is not an attempt to perform an exhaustive empirical evaluation of different embedding techniques; for that see [17, 4, 6, 3].", "startOffset": 115, "endOffset": 128}, {"referenceID": 16, "context": "This last algorithm is motivated by the fact observation that heuristics like minimizing \u2016 \u00b7 \u20161 or \u2016 \u00b7 \u2016\u2217 are good at identifying the true support or basis of a signal, but output biased magnitudes [18].", "startOffset": 198, "endOffset": 202}, {"referenceID": 15, "context": "A popular technique for recovering rank d embeddings is to perform (stochastic) gradient descent on R\u0302S(UU) with objective variable U \u2208 Rn\u00d7d taken as the embedding [17, 4, 6].", "startOffset": 164, "endOffset": 174}, {"referenceID": 3, "context": "A popular technique for recovering rank d embeddings is to perform (stochastic) gradient descent on R\u0302S(UU) with objective variable U \u2208 Rn\u00d7d taken as the embedding [17, 4, 6].", "startOffset": 164, "endOffset": 174}, {"referenceID": 5, "context": "A popular technique for recovering rank d embeddings is to perform (stochastic) gradient descent on R\u0302S(UU) with objective variable U \u2208 Rn\u00d7d taken as the embedding [17, 4, 6].", "startOffset": 164, "endOffset": 174}, {"referenceID": 17, "context": "Also, in light of our isometry theorem, we can show that the Hessian of E[R\u0302S(G)] is nearly a scaled identity, which leads us to hypothesize that a globally optimal linear convergence result for this non-convex optimization may be possible using the techniques of [19, 20].", "startOffset": 264, "endOffset": 272}, {"referenceID": 3, "context": "Finally, we note that previous literature has reported that nuclear norm optimizations like Nuclear Norm PGD tend to produce less accurate embeddings than those of non-convex methods [4, 6].", "startOffset": 183, "endOffset": 189}, {"referenceID": 5, "context": "Finally, we note that previous literature has reported that nuclear norm optimizations like Nuclear Norm PGD tend to produce less accurate embeddings than those of non-convex methods [4, 6].", "startOffset": 183, "endOffset": 189}], "year": 2016, "abstractText": "The goal of ordinal embedding is to represent items as points in a low-dimensional Euclidean space given a set of constraints in the form of distance comparisons like \u201citem i is closer to item j than item k\u201d. Ordinal constraints like this often come from human judgments. To account for errors and variation in judgments, we consider the noisy situation in which the given constraints are independently corrupted by reversing the correct constraint with some probability. This paper makes several new contributions to this problem. First, we derive prediction error bounds for ordinal embedding with noise by exploiting the fact that the rank of a distance matrix of points in R is at most d+ 2. These bounds characterize how well a learned embedding predicts new comparative judgments. Second, we investigate the special case of a known noise model and study the Maximum Likelihood estimator. Third, knowledge of the noise model enables us to relate prediction errors to embedding accuracy. This relationship is highly non-trivial since we show that the linear map corresponding to distance comparisons is non-invertible, but there exists a nonlinear map that is invertible. Fourth, two new algorithms for ordinal embedding are proposed and evaluated in experiments.", "creator": "LaTeX with hyperref package"}}}