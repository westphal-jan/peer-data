{"id": "1606.07315", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Nearly-optimal Robust Matrix Completion", "abstract": "In this paper, we consider the problem of Robust Matrix Completion (RMC) where the goal is to recover a low-rank matrix by observing a small number of its entries out of which a few can be arbitrarily corrupted. We propose a simple projected gradient descent method to estimate the low-rank matrix that alternately performs a projected gradient descent step and cleans up a few of the corrupted entries using hard-thresholding. Our algorithm solves RMC using nearly optimal number of observations as well as nearly optimal number of corruptions. Our result also implies significant improvement over the existing time complexity bounds for the low-rank matrix completion problem. Finally, an application of our result to the robust PCA problem (low-rank+sparse matrix separation) leads to nearly linear time (in matrix dimensions) algorithm for the same; existing state-of-the-art methods require quadratic time. Our empirical results corroborate our theoretical results and show that even for moderate sized problems, our method for robust PCA is an an order of magnitude faster than the existing methods.", "histories": [["v1", "Thu, 23 Jun 2016 13:57:56 GMT  (1562kb)", "http://arxiv.org/abs/1606.07315v1", null], ["v2", "Wed, 7 Dec 2016 20:05:13 GMT  (1588kb)", "http://arxiv.org/abs/1606.07315v2", null], ["v3", "Thu, 8 Dec 2016 19:48:40 GMT  (1569kb)", "http://arxiv.org/abs/1606.07315v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["yeshwanth cherapanamjeri", "kartik gupta", "prateek jain"], "accepted": true, "id": "1606.07315"}, "pdf": {"name": "1606.07315.pdf", "metadata": {"source": "CRF", "title": "Nearly-optimal Robust Matrix Completion", "authors": ["Yeshwanth Cherapanamjeri", "Kartik Gupta", "Prateek Jain"], "emails": ["t-yecher@microsoft.com", "t-kagu@microsoft.com", "prajain@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.07 315v 1 [cs.L G] 23 Jun 20"}, {"heading": "1 Introduction", "text": "In this thesis, we examine the problem of robust matrix completion (RMC), where the goal is to reconstruct an underlying low matrix by observing a small number of less corrupt entries from the matrix. (1) In this thesis, we will observe the sparse corruption of the observed entries, i.e., Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S), Supp (S)."}, {"heading": "2 Algorithm", "text": "In this section we present our algorithms for solving the RMC question (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L).). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L). (for L)."}, {"heading": "3 Analysis", "text": "We introduce our analysis for both of our algorithms PG-RMC (algorithm 1) and R-RMC (algorithm 2). Generally, the problem of PCA ruggedness with the missing entries (3) is more difficult than the standard matrix completion problem and therefore NP-hard [HMRW14]. Therefore, we have to impose certain (standard) assumptions about L-Rank-Rank-Rank-Incoherence of L-Rm-n, i.e., Rank-Rank-Rack-Rank-Rack-Rack-Rack-Rack-Rack-Rack-Rack-Rack-Rack-Rack-Rack-Rack-Rack-Rack-Rack-Rack-Rack-R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@ R@-@"}, {"heading": "3.1 Proof of Theorem 1", "text": "We present our proof for theorem 1; the proof for theorem 2 follows in a similar manner. (The proofs of all but one of the lammas used are moved to the appendix to improve readability.) We remember that we assume that M = L + S (t)) (see line 9 of algorithms 1), i.e., S (t) is the set of iterates we could get \"if the whole M (t) cannot be calculated to simplify our analyses.We first write the projected gradient steps for L (t + 1) as described in (4): L (t + 1) = Pkq (L)."}, {"heading": "Base Case: q = 1 and t = 0", "text": "We start by proving an upper limit based on \"L\" and \"S.\" We do this as follows: \"L\" and \"K\" = \"K\" = \"K\" and \"K\" = \"K\" and \"K\" = \"K\" and \"K\" = \"K\" and \"K.\" This directly supports the third assertion of the problem in the basic case. We also find that due to the threshold level and the incoherence assumption based on \"L\": 1. \"E (0) and the incoherence of\" U. \"This directly proves the third assertion of the problem in the basic case. We also note that due to the threshold level and the incoherence assumption based on\" L \": 1.\" E (0) and \"E (S)."}, {"heading": "Induction over t", "text": "We first prove the inductive step above t (for a fixed q-probability). Based on the inductive hypothesis, we assume that: a)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "Induction Over Stages q", "text": "Suppose the hypothesis applies to level q. At the end of level q q q we have: 1. Q E (T)."}, {"heading": "4 Experiments", "text": "In this section we discuss the performance of algorithm 1 based on synthetic data and its use in the foreground-background separations. The aim of the section is twofold: a) to demonstrate the practicality and effectiveness of algorithm 1 for the RMC problem, b) to show that algorithm 1 actually solves the problem of RPCA in significantly less time than is required by the existing state-of-the-art algorithm (St-NcRPCA + 14). To this end, we use synthetic data where the goal is to perform separation between the individual levels [CLMW11]. We implemented our algorithms in MATLAB and the results for the synthetic data were achieved by averaging over 20 runs. We have a matlab implementation of St-NcRPCA [NUNS + 14] by the authors of [NUNS + 14]."}, {"heading": "5 Appendix", "text": "In the second part we give the convergence guarantee for PG-RMC. In the third part we present another algorithm which has an example complexity of O (\u00b54r3n log2 n log \u00b52r\u03c3) and its convergence guarantees. In the fourth part we demonstrate a generalised form of the lemm1. In the fifth part we present some additional experiments. For the sake of simplicity we define some notation here. We define p = | k, t | mn and consider the following equivalent update step for L (t + 1) in the analysis: L (t): = Pk (M (t)))) M (t): = L + H H: = E (t) + \u03b2G E (t) + \u03b2G E (t): = S (t) - S (t)."}, {"heading": "5.1 Common Lemmas", "text": "We will begin by revisiting some of the lemmas from previous work we use in our proofs. First, we will reproduce Weyl's error dilemma of [Bha97], a key tool in our analysis: lemmas 6. Suppose, B = A + E + Rm \u00b7 n matrix. Then we will find an answer to the question of whether we define the singular values of B and A respectively in such a way that there is an answer to the question of whether there is an answer to the question of whether we give an answer to the question. Then we will find an answer to the question of whether we have an answer to the question."}, {"heading": "5.2 Algorithm R-RMC", "text": "The proof for Theorem 2: From Lemma 13 we know that T \u2265 log (3\u00b5 q \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (22) When the loop comes to an end, we can combine this with Lemmas 6 and 11. \u2212 Then we get: \u03c3q + 1 (M) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (22) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "Base Case: q = 1 and t = 0", "text": "We start by proving an upper limit on the basis of L-2pp. We do this as follows: L-2r-2r-2r-2rm-2rm-2rm-2rm-2rm-1where the last imbalance of Cauchy-Schwartz and the incoherence of U-2rm follows. This directly proves the third assertion of the problem for the base case. We also note that due to the threshold step and the incoherence assumption on L-2rm-2rm-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2pp-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-2er-"}, {"heading": "Induction over t", "text": "(G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) ("}, {"heading": "Induction Over Stages q", "text": "We assume that the hypothesis applies to step q. At the end of the step q we have: 1: 1: 1: 1: 1: 2: 2: 3: 3: 3: 4: 4: 4: 4: 4: 4: 4: 4: 4: 5: 5: 5: 5: 5: 5: 6: 6: 6: 7: 7: 7: 7: 7: 7: 7: 7: 7: 8: 8: 8: 8: 8: 8: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9: 9 9: 9: 9 9: 9: 9.: 9.: 9.: 9.: 9.: 9. 9: 9.: 9.: 9. 9.: 9. 9.: 9.: 9. 9.: 9.: 9.: 9.: 9. 9.: 9. 9.: 9.: 9. 9.: 9.: 9. 9. 9.: 9.: 9.: 9. 9.: 9.: 9. 9. 9.: 9.: 9.: 9. 9.: 9. 9. 9.: 9. 9.: 9.: 9.: 9. 9. 9.: 9. 9. 9.: 9. 9. 9.: 9. 9.: 9. 9. 9. 9.: 9. 9. 9. 9.: 9.: 9. 9. 9. 9. 9.: 9. 9. 9. 9. 9. 9. 9. 9. 9.: 9.: 9.: 9. 9. 9.: 9. 9. 9. 9.: 9. 9. 9. 9.: 9. 9. 9. 9.: 9. 9. 9.: 9.: 9.: 9.: 9. 9.: 9.: 9. 9. 9.: 9.: 9. 9. 9.: 9. 9. 9.: 9.: 9.: 9.: 9. 9.: 9. 9. 9. 9.: 9. 9.: 9.: 9. 9.: 9. 9.: 9.: 9. 9. 9.: 9."}, {"heading": "5.3 Proof of a generalized form of Lemma 1", "text": "Lemma 15: Let's assume H = H1 + H2 and H = H2 = H1 = H1 corresponds to definition 1 (definition 7 of [JN15]) and H2 is a matrix with column and row. Let's leave U a matrix with rows that are referred to as U1. (To define 7 of [JN15] and H2 is a matrix with rows that are referred to as V1. (Let's) a matrix with rows that are referred to as U1. (Let's) a matrix with rows that are referred to as V1. (Let's) a matrix that is referred to as a Qth vector from the default base. (Let's) a matrix with rows that are used as a vector. (Let's) a matrix that are used as a vector. (Let's use a matrix, let's use a matrix). (Let's use a matrix)."}, {"heading": "5.4 Additional Experimental Results", "text": "In this section we explain some additional experiments performed with algorithm 1. Experiments were performed with synthetic data and real datasets. Synthetic data. We create a random matrix with the same probability as in Section 4. In these experiments, our goal is to analyze the behavior of the algorithm in extreme.We consider two such cases: 1) The probability of sampling is very low (Figure 3 (a), 2) the number of corruption is very high (Figure 3 (b)). In the first case, we see that we get a reasonably good probability of sampling (Figure 0.8), even with very low sampling probability (0.07). In the second case, we observe that the time to recovery appears almost independent of the number of corruption as long as they are below a certain threshold. In our experiments, we saw that when the probability of sampling number increased to 0.2 the probability of sampling probability fell to the value 0.00 (Figure 1) In order to have the probability of recalculating the results (804-1p), we recompute with Figure 1 (1)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we consider the problem of Robust Matrix Completion (RMC) where the goal is to recover a low-rank matrix by observing a small number of its entries out of which a few can be arbitrarily corrupted. We propose a simple projected gradient descent method to estimate the low-rank matrix that alternately performs a projected gradient descent step and cleans up a few of the corrupted entries using hard-thresholding. Our algorithm solves RMC using nearly optimal number of observations as well as nearly optimal number of corruptions. Our result also implies significant improvement over the existing time complexity bounds for the low-rank matrix completion problem. Finally, an application of our result to the robust PCA problem (low-rank+sparse matrix separation) leads to nearly linear time (in matrix dimensions) algorithm for the same; existing state-of-the-art methods require quadratic time. Our empirical results corroborate our theoretical results and show that even for moderate sized problems, our method for robust PCA is an an order of magnitude faster than the existing methods.", "creator": "LaTeX with hyperref package"}}}