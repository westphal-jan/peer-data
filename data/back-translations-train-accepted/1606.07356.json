{"id": "1606.07356", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Analyzing the Behavior of Visual Question Answering Models", "abstract": "Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze the best performing models from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models.", "histories": [["v1", "Thu, 23 Jun 2016 16:05:16 GMT  (8531kb,D)", "http://arxiv.org/abs/1606.07356v1", "13 pages, 20 figures; Under review at EMNLP 2016"], ["v2", "Tue, 27 Sep 2016 19:56:22 GMT  (5647kb,D)", "http://arxiv.org/abs/1606.07356v2", "13 pages, 20 figures; To appear in EMNLP 2016"]], "COMMENTS": "13 pages, 20 figures; Under review at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG", "authors": ["aishwarya agrawal", "dhruv batra", "devi parikh"], "accepted": true, "id": "1606.07356"}, "pdf": {"name": "1606.07356.pdf", "metadata": {"source": "CRF", "title": "Analyzing the Behavior of Visual Question Answering Models", "authors": ["Aishwarya Agrawal", "Dhruv Batra", "Devi Parikh"], "emails": ["parikh}@vt.edu"], "sections": [{"heading": null, "text": "In this paper, we propose systematic methods to analyze the behavior of these models as a first step in identifying their strengths and weaknesses and identifying the most fruitful directions for progress. We analyze the most powerful models from two major classes of VQA models - with and without attention, and point out the similarities and differences in the behavior of these models. Our behavioral analysis shows that, despite recent advances, today's VQA models are \"myopic\" (tending to fail in sufficiently new cases), often \"hasty in reaching conclusions\" (convergence to a predicted answer after having \"listened\" to only half of the question) and \"persistent\" (not changing their answers via images)."}, {"heading": "1 Introduction", "text": "Visual Question Answering (VQA) is a recently introduced (Antol et al., 2015; Geman et al., 2014; Malinowski et al., 2014) problem where - in the face of an image and a question in natural language (e.g. \"What kind of business is that?,\" \"How many people are waiting in the queue?\") - the task is to automatically produce a precise answer in natural language (\"Bakery,\" \"\" 5 \"). A flood of more recent models based on deep learning has been proposed for VQA (Antol et al., 2015; Chen et al., 2015; Xuand Saenko, 2015; Jiang et al., 2015; Andreas et al., 2015; Wang et al., 2015; Kafle et al and Kanan, 2016; Lu et al., al., 2016; and Han., 2016; and Qal.; two; Qal.; 2016, al., 2016; and Qan.; two; Qal."}, {"heading": "2 Related Work", "text": "Our work is inspired by earlier work in which the fault modes of models for different tasks were diagnosed. (Karpathy et al., 2015) This Paper1http: / / www.visualqa.org / challenge.htmlar Xiv: 160 6.07 356v 1 [cs.C L] 23 Jun 20aims to conduct behavioral analysis as a first step towards error diagnosis for VQA. (Yang et al., 2015) Categorize the errors of your VQA model into four categories - the model focuses on wrong regions, the model focuses on suitable regions, but predicts wrong answers, predicted answers differ from labels, but may be acceptable, labels are wrong. While these are crude but useful fault modes, we are interested in understanding the behavior of VQA models along certain dimensions - whether they generalize the totality, whether they generalize, whether they generalize or whether they generalize."}, {"heading": "3 Behavior Analyses", "text": "We analyze the behavior of VQA models along the following three dimensions - generalization to new instances: We examine whether the test instances that are answered incorrectly are those that are \"new,\" i.e., not comparable to training instances. The novelty of the test instances can be twofold - 1) the test questionnaire image (QI) pair is \"new,\" i.e., too different from training QI pairs; and 2) the test QI pair is \"familiar,\" but the answer required for the test is \"new,\" i.e., the answers seen during the training are different from what needs to be produced for the test QI pairs. Complete understanding of the question: To examine whether a VQA model understands the input question or not, we analyze whether the model \"hears only a few words of the question or the entire question,\" whether it \"listens\" is just a question, \"listens\" to \"the only\" question A \"on the gap between the only words in the language or the only word."}, {"heading": "3.1 Generalization to novel instances", "text": "In fact, most people who are able to determine for themselves what they want and what they want to do have to do it to put the world in order, \"he said.\" But it's not as if the world is in order. \"He pointed out that most people who are able to determine for themselves are not ready to decide what they want to do.\" But it's not as if the world is in order. \"He pointed out that people are able to determine for themselves what they want."}, {"heading": "3.2 Complete question understanding", "text": "We feed sub-questions with increasing length (from 0-100% of the question from left to right) and then calculate what percentage of answers does not change as more and more words are typed.Figure 2 shows the test accuracy and the percentage of questions where the answers remain the same (compared to the total number of questions), as a function of the sub-question duration. We can see that the CNN + LSTM model appears to have agreed on a predicted answer for 40% of the questions after having heard only half of the question, showing that the model hears the first words of the question more often than the words towards the end. In addition, the model has 68% of the final accuracy (54%) when making predictions based on half of the original question. The ATT model appears to have agreed on a predicted answer after hearing only half of the question more often (49% of the time), and reaches 74% of the final accuracy (57%). See Figure 3 for qualitative examples. We also analyze the change in the OS of the answers, where most of the answers consist of the figure 4."}, {"heading": "3.3 Complete image understanding", "text": "To analyze this, we calculate the percentage of times (say X) in which the answer does not change (i.e., the answer for all images is \"2\"), in all images for a specific question (say, \"How many zebras?\"), and in the diagram histogram of X for all questions (see Figure 5). We do this analysis only for the questions in the VQA validation set that were asked for at least 25 images, resulting in a total of 263 questions. The cumulative graph shows that for 56% of the questions, the CNN + LSTM model gives the same answer for at least half of the images, which is quite high, suggesting that the model chooses the same answer no matter what the image is. Promisingly, the ATT model (which does not work with a holistic full-frame representation and supposedly draws attention to specific spatial regions in an image) is the same model as the STS model for fewer questions (42%)."}, {"heading": "4 Conclusion", "text": "As a first step to understanding these models, we are developing novel techniques to characterize the behavior of VQA models. Our behavioral analysis shows that, despite recent advances, today's VQA models are often \"rash\" (agreeing on a predicted answer after \"listening\" to only half the question), \"stubborn\" (not changing their answers alternately), and less \"stubborn\" than non-attention-based models, and \"short-sighted\" (usually failing to find enough new cases)."}, {"heading": "Appendix Overview", "text": "In the appendix we find: I - Behavioral Analysis for question-only and image-only question VQA models (Appendix I).II - Scatter plot of average distance of test cases from next neighbor training instances w.r.t. VQA accuracy (Appendix II).III - Further qualitative examples of \"generalization to novel test cases\" (Appendix III).IV - The analyses of \"complete image understanding\" for different questions (Appendix IV).V - Further qualitative examples of \"complete question understanding\" (Appendix V).VI - The analyses of \"complete image understanding\" for different question types (Appendix VI).VII - Further qualitative examples of \"complete image understanding\" for."}], "references": [{"title": "Deep compositional question answering with neural module", "author": ["Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "networks. CoRR,", "citeRegEx": "Andreas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2015}, {"title": "Learning to compose neural networks for question answering", "author": ["Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "CoRR, abs/1601.01705", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In International Conference on Computer Vision (ICCV)", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "ABC-CNN: an attention based convolutional neural network for visual question answering", "author": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia"], "venue": "CoRR, abs/1511.05960", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Fukui et al.2016] Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach"], "venue": "CoRR, abs/1606.01847", "citeRegEx": "Fukui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fukui et al\\.", "year": 2016}, {"title": "A Visual Turing Test for Computer Vision Systems", "author": ["Geman et al.2014] Donald Geman", "Stuart Geman", "Neil Hallonquist", "Laurent Younes"], "venue": "In PNAS", "citeRegEx": "Geman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Geman et al\\.", "year": 2014}, {"title": "Diagnosing error in object detectors", "author": ["Hoiem et al.2012] Derek Hoiem", "Yodsawalai Chodpathumwan", "Qieyun Dai"], "venue": "In Proceedings of the 12th European Conference on Computer Vision - Volume Part III,", "citeRegEx": "Hoiem et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hoiem et al\\.", "year": 2012}, {"title": "A focused dynamic attention model for visual question answering", "author": ["Shuicheng Yan", "Jiashi Feng"], "venue": "CoRR, abs/1604.01485", "citeRegEx": "Ilievski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ilievski et al\\.", "year": 2016}, {"title": "Compositional memory for visual question answering", "author": ["Jiang et al.2015] Aiwen Jiang", "Fang Wang", "Fatih Porikli", "Yi Li"], "venue": "CoRR, abs/1511.05676", "citeRegEx": "Jiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2015}, {"title": "Answer-type prediction for visual question answering", "author": ["Kafle", "Kanan2016] Kushal Kafle", "Christopher Kanan"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Kafle et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kafle et al\\.", "year": 2016}, {"title": "Visualizing and understanding recurrent networks", "author": ["Justin Johnson", "Fei-Fei Li"], "venue": "CoRR, abs/1506.02078", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Multimodal residual learning for visual", "author": ["Kim et al.2016] Jin-Hwa Kim", "Sang-Woo Lee", "DongHyun Kwak", "Min-Oh Heo", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "venue": "qa. CoRR,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Deeper lstm and normalized cnn visual question answering model", "author": ["Lu et al.2015] Jiasen Lu", "Xiao Lin", "Dhruv Batra", "Devi Parikh"], "venue": "https://github.com/VT-vision-lab/ VQA_LSTM_CNN", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Hierarchical question-image coattention", "author": ["Lu et al.2016] Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": "CoRR, abs/1606.00061v1", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input", "author": ["Malinowski", "Fritz2014] Mateusz Malinowski", "Mario Fritz"], "venue": "In NIPS", "citeRegEx": "Malinowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Training recurrent answering units with joint loss minimization for vqa", "author": ["Noh", "Han2016] Hyeonwoo Noh", "Bohyung Han"], "venue": "CoRR, abs/1606.03647", "citeRegEx": "Noh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2016}, {"title": "Dualnet: Domain-invariant network for visual question answering", "author": ["Saito et al.2016] Kuniaki Saito", "Andrew Shin", "Yoshitaka Ushiku", "Tatsuya Harada"], "venue": "CoRR, abs/1606.06108", "citeRegEx": "Saito et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Saito et al\\.", "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["Shih et al.2015] Kevin J. Shih", "Saurabh Singh", "Derek Hoiem"], "venue": "CoRR, abs/1511.07394", "citeRegEx": "Shih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shih et al\\.", "year": 2015}, {"title": "Explicit knowledge-based reasoning for visual question answering", "author": ["Wang et al.2015] Peng Wang", "Qi Wu", "Chunhua Shen", "Anton van den Hengel", "Anthony R. Dick"], "venue": "CoRR, abs/1511.02570", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Ask me anything: Free-form visual question answering based on knowledge from external sources", "author": ["Wu et al.2015] Qi Wu", "Peng Wang", "Chunhua Shen", "Anton van den Hengel", "Anthony R. Dick"], "venue": "CoRR, abs/1511.06973", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Xiong et al.2016] Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "CoRR, abs/1603.01417", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Ask, attend and answer: Exploring questionguided spatial attention for visual question answering", "author": ["Xu", "Saenko2015] Huijuan Xu", "Kate Saenko"], "venue": "CoRR, abs/1511.05234", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Yang et al.2015] Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alexander J. Smola"], "venue": "CoRR, abs/1511.02274", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Simple baseline for visual question answering", "author": ["Zhou et al.2015] Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "venue": "CoRR, abs/1512.02167", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Visual Question Answering (VQA) is a recentlyintroduced (Antol et al., 2015; Geman et al., 2014; Malinowski and Fritz, 2014) problem where \u2013 given an image and a natural language question (e.", "startOffset": 56, "endOffset": 124}, {"referenceID": 5, "context": "Visual Question Answering (VQA) is a recentlyintroduced (Antol et al., 2015; Geman et al., 2014; Malinowski and Fritz, 2014) problem where \u2013 given an image and a natural language question (e.", "startOffset": 56, "endOffset": 124}, {"referenceID": 12, "context": "els ((Lu et al., 2015),(Lu et al.", "startOffset": 5, "endOffset": 22}, {"referenceID": 13, "context": ", 2015),(Lu et al., 2016)), one from each of the two major classes of VQA models \u2013 with-attention and without-attention.", "startOffset": 8, "endOffset": 25}, {"referenceID": 10, "context": "(Karpathy et al., 2015) constructed a series of ora-", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "(Hoiem et al., 2012) provided analysis tools to facilitate detailed and meaningful investigation of object detector performance.", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "(Yang et al., 2015) categorize the errors made by their VQA model into four categories \u2013 model focuses attention on incorrect regions, model focuses attention on appropriate regions but predicts incorrect answers, predicted answers are different from labels but might be acceptable, labels are wrong.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Complete image understanding: The absence of a large gap in performance of language-alone and language + vision VQA models (Antol et al., 2015)", "startOffset": 123, "endOffset": 143}, {"referenceID": 2, "context": "We present our behavioral analyses on the VQA dataset (Antol et al., 2015).", "startOffset": 54, "endOffset": 74}, {"referenceID": 2, "context": "CNN + LSTM based model without-attention (CNN+LSTM): We use the best performing model of (Antol et al., 2015) (code provided by (Lu et al.", "startOffset": 89, "endOffset": 109}, {"referenceID": 12, "context": ", 2015) (code provided by (Lu et al., 2015)), which achieves an accuracy of 54.", "startOffset": 26, "endOffset": 43}, {"referenceID": 13, "context": "leaderboard (as of the time of this submission) (Lu et al., 2016), which achieves an accuracy of 57.", "startOffset": 48, "endOffset": 65}, {"referenceID": 15, "context": "The k-NNs are computed in the space of average Word2Vec (Mikolov et al., 2013) vectors of answers.", "startOffset": 56, "endOffset": 78}, {"referenceID": 2, "context": "most \u2013 over the three major categories of questions \u2013 \u201cyes/no\u201d, \u201cnumber\u201d and \u201cother\u201d as categorized in (Antol et al., 2015).", "startOffset": 103, "endOffset": 123}], "year": 2017, "abstractText": "Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze the best performing models from two major classes of VQA models \u2013 with-attention and without-attention and show the similarities and differences in the behavior of these models. Our behavior analysis reveals that despite recent progress, today\u2019s VQA models are \u201cmyopic\u201d (tend to fail on sufficiently novel instances), often \u201cjump to conclusions\u201d (converge on a predicted answer after \u2018listening\u2019 to just half the question), and are \u201cstubborn\u201d (do not change their answers across images).", "creator": "LaTeX with hyperref package"}}}