{"id": "1606.07419", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Learning to Poke by Poking: Experiential Learning of Intuitive Physics", "abstract": "We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 50K pokes on different objects. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels. Our experiments show that this joint modeling approach outperforms alternative methods. We also demonstrate that active data collection using the learned model further improves performance.", "histories": [["v1", "Thu, 23 Jun 2016 19:42:57 GMT  (3812kb,D)", "http://arxiv.org/abs/1606.07419v1", null], ["v2", "Wed, 15 Feb 2017 22:53:52 GMT  (1756kb,D)", "http://arxiv.org/abs/1606.07419v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.RO", "authors": ["pulkit agrawal", "ashvin nair", "pieter abbeel", "jitendra malik", "sergey levine"], "accepted": true, "id": "1606.07419"}, "pdf": {"name": "1606.07419.pdf", "metadata": {"source": "CRF", "title": "Learning to Poke by Poking: Experiential Learning of Intuitive Physics", "authors": ["Pulkit Agrawal", "Ashvin Nair", "Pieter Abbeel", "Jitendra Malik"], "emails": ["pulkitag@berkeley.edu", "anair17@berkeley.edu", "pabbeel@berkeley.edu", "malik@berkeley.edu", "svlevine@cs.washington.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2 Related Work", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to understand the rules that they have given themselves. (...) In fact, it is the case that they are able to break the rules. (...) In fact, it is the case that they are able to break the rules. (...) In fact, it is the case that they are able to determine themselves. (...) It is as if they are able to determine for themselves what they want. (...)"}, {"heading": "3 Method", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4 Experimental Setup", "text": "Manipulating real-world objects through actions such as poking is very difficult because objects have complex geometries and material properties that cause them to behave in unexpected ways, as shown in Figure 2. So far, our robot has executed 50K tusks, including 30K tusks for the Nutella bottle, 20K for the hammer, and 5K each for the peptito bottle and a coffee glass. Videos of our facility can be found at: link.Our experimental setup is shown in Figure 1. The robot has a table in front of it on which it can manipulate objects. It is equipped with a Kinect camera, a gripper for poking, and a white bar to reset the objects in the middle of the work area. Kinect's point cloud is used to randomly select a target point for poking. Kinect's point cloud is only used during training to distort the robot rather than poking an object in open space."}, {"heading": "4.1 Training Details", "text": "The representation of Poke The poke is parameterized as a tuple of poke location (pt), the angle of poke (\u03b8t) and the length of poke (lt). For the purpose of forming the reverse model, we discredit the poke position to a 20 \u00d7 20 grid, and the angle of poke and the length of poke 36 and 11 containers, respectively. The eleventh container of the poke length is used not to name any poke parameters. This discrediting allows us to predict multimodal distributions about the poke in the reverse model, which we found crucial for dealing with the ambiguity inherent in the inverse dynamics. For the forward model, we simply use the really evaluated poke parameters. Shorthand for Neural Network Architecture The abbreviations Ck, Fk represent a convolutional (C) layer with k filters, a full-connected (F) with non-filters each Lwe-used linity respectively."}, {"heading": "4.2 Evaluation Procedure", "text": "Once the robot has learned a model of its interaction with objects, we test the quality of the learned model by asking the robot to move the objects from an initial configuration to the final configuration. Note that during the training process, the robot does not receive any explicit reward for this task and therefore has no incentive to move the object into a desired configuration. Even if the same object is present in a nearby location in the initial and target image, it is often not possible to move the object to the target image in a single push, because the push causes both objects to transform and rotate at the same time. To move the object to the target position, we use the reverse model to predict a push from a given pair of input and target images. This push is executed to create the next image. The pair of this new image and target image are fed back into the input point in the reverse model."}, {"heading": "5 Simulation Experiments", "text": "To test the hypothesis that forward models can regulate inverse models, we first conducted extensive simulation experiments using environments consisting of a single rectangle that could be freely transformed and rotated (Figure 4).The agent interacted with the rectangle by poking it with small forces. During the unattended training phase, the displacement of the rectangle between the initial and final images was limited to a maximum of 5 pixels (for reference, the world size was 48 x 48 pixels and the rectangles 14 x 18 pixels).The training was performed with an architecture similar to that described in Section 4.1. The inverse single model was trained by setting \u03bb = 0 in Equation 3. Further details on the experimental setup, network architecture and training methods for the simulation experiments are provided in the supplementary experiments."}, {"heading": "6 Real World Experiments", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "7 Discussion and Future Work", "text": "We presented a method for collectively learning inverse and forward-looking models to predict the outcome of actions from raw visual inputs, as well as an empirical evaluation of an experimental approach to learning intuitive physical models in the context of robotic manipulation. However, our results suggest that the proposed common interaction episodes are more accurate than other standard methods, and also that the active data collection in which the robot achieves its own goals with the latest learned model produces more precisely predictable models. Nevertheless, manipulation is a tough problem, and although our models work better than the basic method, they are far from giving the robot complete control over the object state, making it more difficult to predict the outcome of the action."}, {"heading": "Acknowledgement", "text": "First of all, we thank Alyosha Efros for inspiration and fruitful discussions during this work. The title of this essay was partly influenced by the term \"Pokebot,\" which Alyosha has been using for several years. We would like to thank Ruzena Bajcsy for access to the Baxter robot and Shubham Tulsiani for helpful comments. This work was partially supported by ONR MURI N00014-14-1-0671 and ONR YIP. Sergey Levine was partially supported by ARL through the MAST program. We thank NVIDIA Corporation for donating K40 GPUs and access to the NVIDIA PSG cluster for this research.Additional materials and videos of our data collection can be found at: link."}], "references": [{"title": "A planning framework for non-prehensile manipulation under clutter and uncertainty", "author": ["Dogar", "Mehmet R", "Srinivasa", "Siddhartha S"], "venue": "Autonomous Robots,", "citeRegEx": "Dogar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dogar et al\\.", "year": 2012}, {"title": "Deep spatial autoencoders for visuomotor learning", "author": ["Finn", "Chelsea", "Tan", "Xin Yu", "Duan", "Yan", "Darrell", "Trevor", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Finn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2015}, {"title": "Learning visual predictive models of physics for playing billiards", "author": ["Fragkiadaki", "Katerina", "Agrawal", "Pulkit", "Levine", "Sergey", "Malik", "Jitendra"], "venue": "arXiv preprint arXiv:1511.07404,", "citeRegEx": "Fragkiadaki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fragkiadaki et al\\.", "year": 2015}, {"title": "Internal physics models guide probabilistic judgments about object dynamics", "author": ["Hamrick", "Jessica", "Battaglia", "Peter", "Tenenbaum", "Joshua B"], "venue": "In Proceedings of the 33rd annual conference of the cognitive science society,", "citeRegEx": "Hamrick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hamrick et al\\.", "year": 2011}, {"title": "Forward models: Supervised learning with a distal teacher", "author": ["Jordan", "Michael I", "Rumelhart", "David E"], "venue": "Cognitive science,", "citeRegEx": "Jordan et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1992}, {"title": "The neuro slot car racer: Reinforcement learning in a real world setting", "author": ["Kietzmann", "Tim C", "Riedmiller", "Martin"], "venue": "In Machine Learning and Applications,", "citeRegEx": "Kietzmann et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kietzmann et al\\.", "year": 2009}, {"title": "Physically consistent state estimation and system identification for contacts", "author": ["Kolev", "Svetoslav", "Todorov", "Emanuel"], "venue": "In Humanoid Robots (Humanoids),", "citeRegEx": "Kolev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kolev et al\\.", "year": 2015}, {"title": "Learning to predict how rigid objects behave under simple manipulation", "author": ["Kopicki", "Marek", "Zurek", "Sebastian", "Stolkin", "Rustam", "M\u00f6rwald", "Thomas", "Wyatt", "Jeremy"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "Kopicki et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kopicki et al\\.", "year": 2011}, {"title": "Deep learning of visual control policies", "author": ["Lange", "Sascha", "Riedmiller", "Martin A"], "venue": "In ESANN. Citeseer,", "citeRegEx": "Lange et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lange et al\\.", "year": 2010}, {"title": "Autonomous reinforcement learning on raw visual input data in a real world application", "author": ["Lange", "Stanislav", "Riedmiller", "Martin", "Voigtlander", "Arne"], "venue": "In Neural Networks (IJCNN), The 2012 International Joint Conference on,", "citeRegEx": "Lange et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lange et al\\.", "year": 2012}, {"title": "Automatic learning of pushing strategy for delivery of irregularshaped objects", "author": ["Lau", "Manfred", "Mitani", "Jun", "Igarashi", "Takeo"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "Lau et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lau et al\\.", "year": 2011}, {"title": "Deepmpc: Learning deep latent features for model predictive control", "author": ["Lenz", "Ian", "Knepper", "Ross", "Saxena", "Ashutosh"], "venue": "In RSS,", "citeRegEx": "Lenz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lenz et al\\.", "year": 2015}, {"title": "Learning physical intuition of block towers by example", "author": ["Lerer", "Adam", "Gross", "Sam", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1603.01312,", "citeRegEx": "Lerer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lerer et al\\.", "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "CoRR, abs/1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["Levine", "Sergey", "Pastor", "Peter", "Krizhevsky", "Alex", "Quillen", "Deirdre"], "venue": null, "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Model predictive control: Recent developments and future", "author": ["Mayne", "David Q"], "venue": "promise. Automatica,", "citeRegEx": "Mayne and Q.,? \\Q2014\\E", "shortCiteRegEx": "Mayne and Q.", "year": 2014}, {"title": "Intuitive physics", "author": ["McCloskey", "Michael"], "venue": "Scientific american,", "citeRegEx": "McCloskey and Michael.,? \\Q1983\\E", "shortCiteRegEx": "McCloskey and Michael.", "year": 1983}, {"title": "Push-manipulation of complex passive mobile objects using experimentally acquired motion models", "author": ["Meri\u00e7li", "Tekin", "Veloso", "Manuela", "Ak\u0131n", "H Levent"], "venue": "Autonomous Robots,", "citeRegEx": "Meri\u00e7li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meri\u00e7li et al\\.", "year": 2015}, {"title": "The perception of causality", "author": ["Michotte", "Albert"], "venue": null, "citeRegEx": "Michotte and Albert.,? \\Q1963\\E", "shortCiteRegEx": "Michotte and Albert.", "year": 1963}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Newtonian image understanding: Unfolding the dynamics of objects in static images", "author": ["Mottaghi", "Roozbeh", "Bagherinezhad", "Hessam", "Rastegari", "Mohammad", "Farhadi", "Ali"], "venue": "arXiv preprint arXiv:1511.04048,", "citeRegEx": "Mottaghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mottaghi et al\\.", "year": 2015}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["Oh", "Junhyuk", "Guo", "Xiaoxiao", "Lee", "Honglak", "Lewis", "Richard", "Singh", "Satinder"], "venue": "arXiv preprint arXiv:1507.08750,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["Pinto", "Lerrel", "Gupta", "Abhinav"], "venue": "arXiv preprint arXiv:1509.06825,", "citeRegEx": "Pinto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pinto et al\\.", "year": 2015}, {"title": "The development of embodied cognition: Six lessons from babies", "author": ["Smith", "Linda", "Gasser", "Michael"], "venue": "Artificial life,", "citeRegEx": "Smith et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2005}, {"title": "Anticipating the future by watching unlabeled video", "author": ["Vondrick", "Carl", "Pirsiavash", "Hamed", "Torralba", "Antonio"], "venue": "arXiv preprint arXiv:1504.08023,", "citeRegEx": "Vondrick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vondrick et al\\.", "year": 2015}, {"title": "From pixels to torques: Policy learning with deep dynamical models", "author": ["Wahlstr\u00f6m", "Niklas", "Sch\u00f6n", "Thomas B", "Deisenroth", "Marc Peter"], "venue": "CoRR, abs/1502.02251,", "citeRegEx": "Wahlstr\u00f6m et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wahlstr\u00f6m et al\\.", "year": 2015}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["Watter", "Manuel", "Springenberg", "Jost", "Boedecker", "Joschka", "Riedmiller", "Martin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Watter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watter et al\\.", "year": 2015}, {"title": "An internal model for sensorimotor integration", "author": ["Wolpert", "Daniel M", "Ghahramani", "Zoubin", "Jordan", "Michael I"], "venue": "Science-AAAS-Weekly Paper Edition,", "citeRegEx": "Wolpert et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Wolpert et al\\.", "year": 1995}, {"title": "Galileo: Perceiving physical object properties by integrating a physics engine with deep learning", "author": ["Wu", "Jiajun", "Yildirim", "Ilker", "Lim", "Joseph J", "Freeman", "Bill", "Tenenbaum", "Josh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "One popular way of building a model is to represent the world in terms of a fixed set of physical parameters such as mass, friction coefficient, normal forces etc and use a physics simulator for computing object dynamics (Kolev & Todorov, 2015; Mottaghi et al., 2015; Wu et al., 2015; Hamrick et al., 2011) from this representation.", "startOffset": 221, "endOffset": 306}, {"referenceID": 29, "context": "One popular way of building a model is to represent the world in terms of a fixed set of physical parameters such as mass, friction coefficient, normal forces etc and use a physics simulator for computing object dynamics (Kolev & Todorov, 2015; Mottaghi et al., 2015; Wu et al., 2015; Hamrick et al., 2011) from this representation.", "startOffset": 221, "endOffset": 306}, {"referenceID": 3, "context": "One popular way of building a model is to represent the world in terms of a fixed set of physical parameters such as mass, friction coefficient, normal forces etc and use a physics simulator for computing object dynamics (Kolev & Todorov, 2015; Mottaghi et al., 2015; Wu et al., 2015; Hamrick et al., 2011) from this representation.", "startOffset": 221, "endOffset": 306}, {"referenceID": 2, "context": "We call this alternative approach as \u201cintuitive\u201d physics (Fragkiadaki et al., 2015; McCloskey, 1983; Michotte, 1963).", "startOffset": 57, "endOffset": 116}, {"referenceID": 20, "context": "Several recent works have proposed to learn visual control policies using reinforcement learning for tasks such as playing Atari games (Mnih et al., 2015) and controlling robots in simulation (Lillicrap et al.", "startOffset": 135, "endOffset": 154}, {"referenceID": 15, "context": ", 2015) and controlling robots in simulation (Lillicrap et al., 2015) and in the real world (Levine et al.", "startOffset": 45, "endOffset": 69}, {"referenceID": 13, "context": ", 2015) and in the real world (Levine et al., 2015).", "startOffset": 30, "endOffset": 51}, {"referenceID": 9, "context": "A number of recent methods have also been proposed for learning representations for vision-based control using autoencoders to transform visual inputs into a low-dimensional state space (Lange et al., 2012; Finn et al., 2015; Lange & Riedmiller, 2010; Kietzmann & Riedmiller, 2009).", "startOffset": 186, "endOffset": 281}, {"referenceID": 1, "context": "A number of recent methods have also been proposed for learning representations for vision-based control using autoencoders to transform visual inputs into a low-dimensional state space (Lange et al., 2012; Finn et al., 2015; Lange & Riedmiller, 2010; Kietzmann & Riedmiller, 2009).", "startOffset": 186, "endOffset": 281}, {"referenceID": 25, "context": "Recently (Vondrick et al., 2015) proposed to build prediction models in the space of features learnt by pretraining on image classification on Imagenet.", "startOffset": 9, "endOffset": 32}, {"referenceID": 14, "context": "(Pinto & Gupta, 2015; Levine et al., 2016) learn how to grasp objects by trial and error from a large number of attempts.", "startOffset": 0, "endOffset": 42}, {"referenceID": 28, "context": "A good review of model based control can be found in (Mayne, 2014) and (Jordan & Rumelhart, 1992; Wolpert et al., 1995) provide interesting perspectives.", "startOffset": 71, "endOffset": 119}, {"referenceID": 11, "context": "(Lenz et al., 2015) used deep learning based model predictive control for cutting vegetables.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Only very recently, (Fragkiadaki et al., 2015; Wahlstr\u00f6m et al., 2015; Watter et al., 2015; Oh et al., 2015) addressed the problem of model based control from vision in synthetic domains of manipulating two degree of freedom robotic arm, inverted pendulum, billiards and Atari games.", "startOffset": 20, "endOffset": 108}, {"referenceID": 26, "context": "Only very recently, (Fragkiadaki et al., 2015; Wahlstr\u00f6m et al., 2015; Watter et al., 2015; Oh et al., 2015) addressed the problem of model based control from vision in synthetic domains of manipulating two degree of freedom robotic arm, inverted pendulum, billiards and Atari games.", "startOffset": 20, "endOffset": 108}, {"referenceID": 27, "context": "Only very recently, (Fragkiadaki et al., 2015; Wahlstr\u00f6m et al., 2015; Watter et al., 2015; Oh et al., 2015) addressed the problem of model based control from vision in synthetic domains of manipulating two degree of freedom robotic arm, inverted pendulum, billiards and Atari games.", "startOffset": 20, "endOffset": 108}, {"referenceID": 22, "context": "Only very recently, (Fragkiadaki et al., 2015; Wahlstr\u00f6m et al., 2015; Watter et al., 2015; Oh et al., 2015) addressed the problem of model based control from vision in synthetic domains of manipulating two degree of freedom robotic arm, inverted pendulum, billiards and Atari games.", "startOffset": 20, "endOffset": 108}, {"referenceID": 29, "context": "(Wu et al., 2015; Mottaghi et al., 2015; Lerer et al., 2016) proposed using Newtonian physics in combination with neural networks to predict the dynamics of objects in the future.", "startOffset": 0, "endOffset": 60}, {"referenceID": 21, "context": "(Wu et al., 2015; Mottaghi et al., 2015; Lerer et al., 2016) proposed using Newtonian physics in combination with neural networks to predict the dynamics of objects in the future.", "startOffset": 0, "endOffset": 60}, {"referenceID": 12, "context": "(Wu et al., 2015; Mottaghi et al., 2015; Lerer et al., 2016) proposed using Newtonian physics in combination with neural networks to predict the dynamics of objects in the future.", "startOffset": 0, "endOffset": 60}, {"referenceID": 7, "context": "In robotic manipulation, a number of prior methods have been proposed that use hand-designed visual features and known object poses or key locations to plan and execute pushes and other non-prehensile manipulations (Kopicki et al., 2011; Lau et al., 2011; Meri\u00e7li et al., 2015).", "startOffset": 215, "endOffset": 277}, {"referenceID": 10, "context": "In robotic manipulation, a number of prior methods have been proposed that use hand-designed visual features and known object poses or key locations to plan and execute pushes and other non-prehensile manipulations (Kopicki et al., 2011; Lau et al., 2011; Meri\u00e7li et al., 2015).", "startOffset": 215, "endOffset": 277}, {"referenceID": 18, "context": "In robotic manipulation, a number of prior methods have been proposed that use hand-designed visual features and known object poses or key locations to plan and execute pushes and other non-prehensile manipulations (Kopicki et al., 2011; Lau et al., 2011; Meri\u00e7li et al., 2015).", "startOffset": 215, "endOffset": 277}, {"referenceID": 25, "context": "The higher layers of a deep neural networks trained on the ImageNet challenge provide one such feature space (Vondrick et al., 2015).", "startOffset": 109, "endOffset": 132}, {"referenceID": 13, "context": "The output of the third convolution layer is fed into a spatial softmax layer (Levine et al., 2015) to extract the mean activation location of each of the conv-3 feature maps.", "startOffset": 78, "endOffset": 99}, {"referenceID": 13, "context": "Spatial softmax layer (Levine et al., 2015) operates on conv-3 outputs to extract the mean activation location of each of the conv-3 feature maps (also called feature points) - xt, xt+1 from images It, It+1, respectively.", "startOffset": 22, "endOffset": 43}, {"referenceID": 14, "context": "However, this is not a limitation of our approach as data collection process can be easily parallelized across robots (Levine et al., 2016).", "startOffset": 118, "endOffset": 139}], "year": 2016, "abstractText": "We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 50K pokes on different objects. We propose a novel approach based on deep neural networks for modeling the dynamics of robot\u2019s interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels. Our experiments show that this joint modeling approach outperforms alternative methods. We also demonstrate that active data collection using the learned model further improves performance.", "creator": "LaTeX with hyperref package"}}}