{"id": "1606.07839", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles", "abstract": "Many practical perception systems exist within larger processes which often include interactions with users or additional components that are capable of evaluating the quality of predicted solutions. In these contexts, it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction. In this work, we pose the task of producing multiple outputs as a learning problem over an ensemble of deep networks -- introducing a novel stochastic gradient descent based approach to minimize the loss with respect to an oracle. Our method is simple to implement, agnostic to both architecture and loss function, and parameter-free. Our approach achieves lower oracle error compared to existing methods on a wide range of tasks and deep architectures. We also show qualitatively that solutions produced from our approach often provide interpretable representations of task ambiguity.", "histories": [["v1", "Fri, 24 Jun 2016 21:48:55 GMT  (7242kb,D)", "https://arxiv.org/abs/1606.07839v1", null], ["v2", "Wed, 28 Sep 2016 20:44:55 GMT  (12050kb,D)", "http://arxiv.org/abs/1606.07839v2", null], ["v3", "Wed, 5 Oct 2016 17:12:00 GMT  (6655kb,D)", "http://arxiv.org/abs/1606.07839v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["stefan lee", "senthil purushwalkam", "michael cogswell", "viresh ranjan", "david j crandall", "dhruv batra"], "accepted": true, "id": "1606.07839"}, "pdf": {"name": "1606.07839.pdf", "metadata": {"source": "CRF", "title": "Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles", "authors": ["Stefan Lee", "Senthil Purushwalkam", "Michael Cogswell", "Viresh Ranjan"], "emails": ["steflee@vt.edu", "spurushw@andrew.cmu.edu", "cogswell@vt.edu", "rviresh@vt.edu", "djcran@indiana.edu", "dbatra@vt.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live."}, {"heading": "2 Related Work", "text": "Ensemble learning. Much of the existing work on training ensembles focuses on diversity between individual models as a means of improving performance by reducing the error correlation. This is often achieved by retrieving existing training data for each member model [27] or by producing artificial data that encourages new models to relate to the existing ensemble [21]. Other approaches train or combine ensemble members at a common loss [19, 26]. More recently, work by Hinton et al. [12] and Ahmed et al. [2] has been studied using \"generalist\" network performance statistics to shape the design of ensemble architectures for classification. In contrast, sMCL discovers specialization as a result of minimizing oracle losses. Importantly, most existing methods do not generalize into structured output labels, while sMCL is seamlessly adapted to the design of ensemble architectures for classification."}, {"heading": "3 Multiple-Choice Learning as Stochastic Block Gradient Descent", "text": "We consider the task of training an ensemble of differentiated learners who together produce a series of solutions with minimal loss in relation to an oracle that selects only the lowest error prediction. (D) We use [n] to name the plan {1, 2,.., n}. Given a series of input-output pairs D = (xi, yi) referring to a series of input-output pairs D = (xi, yi), our goal is to learn a function g: X \u2192 YM that maps any input-output to M-outputs. We fix the shape of the ensemble of M-learners f = (x),., fM (x)., which measures task-related losses (y), the error between true and predicted outputs y and y, we define the oracular loss of g via the dataset D asLO (D) = n."}, {"heading": "4 Experiments", "text": "In this section, we present the results for sMCL ensembles trained for the tasks and deep architectures shown in Figure 3, including CNN ensembles for image classification, FCN ensembles for semantic segmentation, and a CNN + RNN ensembles for captions generation. We compare our proposed methods against: - classical ensembles, in which each model is trained at an independent loss with different random initializations. We refer to these as Indp. Ensembles in numbers. - MCL [8], which alternate between assigned examples and assigning examples to their lowest error model. We repeat this process for 5 meta-iterations and initialize ensembles with different random weights."}, {"heading": "4.1 Image Classification", "text": "model. We begin our experiments with sMCL on the CIFAR10 [17] dataset using the small convolutionary neural network \"CIFAR10-Quick,\" which is provided with the Caffe deep learning framework [13]. CIFAR10 is a ten-fold classification task with small 32 x 32 images. For these experiments, the reference model is based on a batch size of 350 for 5,000 iterations with an impulse of 0.9, a weight loss of 0.004 and an initial learning rate of 0.001, which drops to 0.0001 after 4000 iterations. Results. The oracle accuracy for sMCL and baseline ensembles of sizes 1 to 6 are shown in Figure 4a. The sMCL-trained ensembles result in a higher oracle accuracy than the baseline methods, and are comparable to MCL, while the method of Dey et al. [5] results in a worse than independent ensembles as an ensemble size."}, {"heading": "4.2 Semantic Segmentation", "text": "We present our results for the semantic segmentation task on the Pascal VOC dataset [6].We model the fully revolutionary network (FCN) architecture presented by Long et al. [20] as our base model. As [20] we extended on the Pascal VOC 2011 training set with additional segmentations included in [10] and we test on a subset of the VOC 2011 validation theory. we initialize our sMCL models from a standard ensemble that is trained for a learning rate of 10 \u2212 3. The sMCL group is then finely tuned for another 15 epochs with a reduced learning rate of 10 \u2212 5.Results."}, {"heading": "4.3 Image Captioning", "text": "In this section, we show that sMCL-trained ensembles can produce sentences of high quality and diverse sentences that are essential for improving memory and capturing ambiguities in language and perception. We take the model and training method of Karpathy et al. [14], use their publicly available implementations of neural parasites [16], which represent the input image as a fixed representation for a long-term memory (LSTM), train and test on the MSCOCO dataset [18], using the same splits as [14], perform two experimental setups by initiating the freezing or fining of the CNN language model, freeze the parameters of the CNN models, and train multiple LSTM ensembles."}, {"heading": "5 Conclusion", "text": "In summary, we propose Stochastic Multiple Choice Learning (sMCL), an SGD-based technique for training different deep ensembles that follows a winner-take-gradient training strategy. Our experiments demonstrate the broad applicability and effectiveness of sMCL for training different deep ensembles. In all experimental constellations, sMCL significantly outperforms classic ensembles and other strong baselines, including the 5x slower MCL process. Our analysis shows that the very same algorithm (sMCL) automatically generates specializations among ensemble members along different task-specific dimensions. sMCL is easy to implement, both architecturally agnostic and loss-free, and involves only the introduction of a new sMCL layer into existing ensemble architectures."}, {"heading": "Acknowledgments", "text": "This work was supported in part by a National Science Foundation CAREER Award, a YIP Award from the Army Research Office, an ICTAS Junior Faculty Award, a CAREER Award from the Office of Naval Research N00014-14-1-0679, a Google Faculty Research Award, an AWS in Education Research Award, and an NVIDIA GPU Donation, all of which were awarded to DB, and an NSF CAREER Award (IIS-1253549), the Intelligence Advanced Research Projects Activity (IARPA) through the Air Force Research Laboratory Contract FA8650-12-C-7212, a Google Faculty Research Award, and an NVIDIA GPU Donation, all of which were awarded to DC."}], "references": [{"title": "Network of experts for large-scale image categorization", "author": ["K. Ahmed", "M.H. Baig", "L. Torresani"], "venue": "arXiv preprint arXiv:1604.06119", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Diverse M-Best Solutions in Markov Random Fields", "author": ["D. Batra", "P. Yadollahpour", "A. Guzman-Rivera", "G. Shakhnarovich"], "venue": "Proceedings of European Conference on Computer Vision (ECCV)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1405.3531", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting multiple structured visual interpretations", "author": ["D. Dey", "V. Ramakrishna", "M. Hebert", "J. Andrew Bagnell"], "venue": "Proceedings of IEEE International Conference on Computer Vision (ICCV)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "and A", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn"], "venue": "Zisserman. The PASCAL Visual Object Classes Challenge 2011 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Vision meets Robotics: The KITTI Dataset", "author": ["A. Geiger", "P. Lenz", "C. Stiller", "R. Urtasun"], "venue": "International Journal of Robotics Research (IJRR)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiple Choice Learning: Learning to Produce Multiple Structured Outputs", "author": ["A. Guzman-Rivera", "D. Batra", "P. Kohli"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficiently enforcing diversity in multi-output structured prediction", "author": ["A. Guzman-Rivera", "P. Kohli", "D. Batra", "R. Rutenbar"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic contours from inverse detectors", "author": ["B. Hariharan", "P. Arbelaez", "L. Bourdev", "S. Maji", "J. Malik"], "venue": "Proceedings of IEEE International Conference on Computer Vision (ICCV)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["G.E. Hinton", "O. Vinyals", "J. Dean"], "venue": "Advances in Neural Information Processing Systems (NIPS) - Deep Learning Workshop", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": "http://caffe. berkeleyvision.org/", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Inferring m-best diverse solutions in a single one", "author": ["A. Kirillov", "B. Savchynskyy", "D. Schlesinger", "D. Vetrov", "C. Rother"], "venue": "Proceedings of IEEE International Conference on Computer Vision (ICCV)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "M-best-diverse labelings for submodular energies and beyond", "author": ["A. Kirillov", "D. Schlesinger", "D. Vetrov", "C. Rother", "B. Savchynskyy"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "and C", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r"], "venue": "L. Zitnick. Microsoft COCO: Common objects in context", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Ensemble learning via negative correlation", "author": ["Y. Liu", "X. Yao"], "venue": "Neural Networks, 12(10):1399\u20131404", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Creating diversity in ensembles using artificial data", "author": ["P. Melville", "R.J. Mooney"], "venue": "Information Fusion, 6(1):99\u2013111", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "N-best maximal decoders for part models", "author": ["D. Park", "D. Ramanan"], "venue": "Proceedings of IEEE International Conference on Computer Vision (ICCV), pages 2627\u20132634", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Submodular meets structured: Finding diverse subsets in exponentiallylarge structured item sets", "author": ["A. Prasad", "S. Jegelka", "D. Batra"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "and L", "author": ["O. Russakovsky", "J. Deng", "J. Krause", "A. Berg"], "venue": "Fei-Fei. The ImageNet Large Scale Visual Recognition Challenge 2012 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions", "author": ["A. Strehl", "J. Ghosh"], "venue": "The Journal of Machine Learning Research, 3:583\u2013617", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "Error correlation and error reduction in ensemble classifiers", "author": ["K. Tumer", "J. Ghosh"], "venue": "Connection Science, 8(3-4):385\u2013404", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "For instance, the task of recognizing and segmenting objects in an image (semantic segmentation [6]) might be embedded in an autonomous vehicle [7], while the task of describing an image with a sentence (image captioning [18]) might be part of a system to assist visually-impaired users [22, 29].", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "For instance, the task of recognizing and segmenting objects in an image (semantic segmentation [6]) might be embedded in an autonomous vehicle [7], while the task of describing an image with a sentence (image captioning [18]) might be part of a system to assist visually-impaired users [22, 29].", "startOffset": 144, "endOffset": 147}, {"referenceID": 16, "context": "For instance, the task of recognizing and segmenting objects in an image (semantic segmentation [6]) might be embedded in an autonomous vehicle [7], while the task of describing an image with a sentence (image captioning [18]) might be part of a system to assist visually-impaired users [22, 29].", "startOffset": 221, "endOffset": 225}, {"referenceID": 6, "context": "Such a learning setting is called Multiple Choice Learning (MCL) [8], where the goal for the learner is to minimize oracle loss achieved by a set of M solutions.", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "In this paper, we generalize the Multiple Choice Learning paradigm [8, 9] to jointly learn ensembles of deep networks that minimize the oracle loss directly.", "startOffset": 67, "endOffset": 73}, {"referenceID": 7, "context": "In this paper, we generalize the Multiple Choice Learning paradigm [8, 9] to jointly learn ensembles of deep networks that minimize the oracle loss directly.", "startOffset": 67, "endOffset": 73}, {"referenceID": 6, "context": "We are the first to adapt these ideas to deep networks and we present a novel training algorithm that avoids costly retraining [8] and learning difficulty [5] of past methods.", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "We are the first to adapt these ideas to deep networks and we present a novel training algorithm that avoids costly retraining [8] and learning difficulty [5] of past methods.", "startOffset": 155, "endOffset": 158}, {"referenceID": 15, "context": "We demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles with interpretable emergent expertise on a wide range of problem domains and network architectures, including Convolutional Neural Network (CNN) [1] ensembles for image classification [17], FullyConvolutional Network (FCN) [20] ensembles for semantic segmentation [6], and combined CNN and Recurrent Neural Network (RNN) ensembles [14] for image captioning [18].", "startOffset": 280, "endOffset": 284}, {"referenceID": 18, "context": "We demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles with interpretable emergent expertise on a wide range of problem domains and network architectures, including Convolutional Neural Network (CNN) [1] ensembles for image classification [17], FullyConvolutional Network (FCN) [20] ensembles for semantic segmentation [6], and combined CNN and Recurrent Neural Network (RNN) ensembles [14] for image captioning [18].", "startOffset": 319, "endOffset": 323}, {"referenceID": 4, "context": "We demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles with interpretable emergent expertise on a wide range of problem domains and network architectures, including Convolutional Neural Network (CNN) [1] ensembles for image classification [17], FullyConvolutional Network (FCN) [20] ensembles for semantic segmentation [6], and combined CNN and Recurrent Neural Network (RNN) ensembles [14] for image captioning [18].", "startOffset": 360, "endOffset": 363}, {"referenceID": 12, "context": "We demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles with interpretable emergent expertise on a wide range of problem domains and network architectures, including Convolutional Neural Network (CNN) [1] ensembles for image classification [17], FullyConvolutional Network (FCN) [20] ensembles for semantic segmentation [6], and combined CNN and Recurrent Neural Network (RNN) ensembles [14] for image captioning [18].", "startOffset": 427, "endOffset": 431}, {"referenceID": 16, "context": "We demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles with interpretable emergent expertise on a wide range of problem domains and network architectures, including Convolutional Neural Network (CNN) [1] ensembles for image classification [17], FullyConvolutional Network (FCN) [20] ensembles for semantic segmentation [6], and combined CNN and Recurrent Neural Network (RNN) ensembles [14] for image captioning [18].", "startOffset": 453, "endOffset": 457}, {"referenceID": 24, "context": "This is often accomplished by resampling existing training data for each member model [27] or by producing artificial data that encourages new models to be decorrelated with the existing ensemble [21].", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "This is often accomplished by resampling existing training data for each member model [27] or by producing artificial data that encourages new models to be decorrelated with the existing ensemble [21].", "startOffset": 196, "endOffset": 200}, {"referenceID": 17, "context": "Other approaches train or combine ensemble members under a joint loss [19, 26].", "startOffset": 70, "endOffset": 78}, {"referenceID": 23, "context": "Other approaches train or combine ensemble members under a joint loss [19, 26].", "startOffset": 70, "endOffset": 78}, {"referenceID": 10, "context": "[12] and Ahmed et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[2] explores using \u2018generalist\u2019 network performance statistics to inform the design of ensemble-of-expert architectures for classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "There is a large body of work on the topic of extracting multiple diverse solutions from a single model [3, 15, 16, 23, 24]; however, these approaches are designed for probabilistic structured-output models and are not directly applicable to general deep architectures.", "startOffset": 104, "endOffset": 123}, {"referenceID": 13, "context": "There is a large body of work on the topic of extracting multiple diverse solutions from a single model [3, 15, 16, 23, 24]; however, these approaches are designed for probabilistic structured-output models and are not directly applicable to general deep architectures.", "startOffset": 104, "endOffset": 123}, {"referenceID": 14, "context": "There is a large body of work on the topic of extracting multiple diverse solutions from a single model [3, 15, 16, 23, 24]; however, these approaches are designed for probabilistic structured-output models and are not directly applicable to general deep architectures.", "startOffset": 104, "endOffset": 123}, {"referenceID": 20, "context": "There is a large body of work on the topic of extracting multiple diverse solutions from a single model [3, 15, 16, 23, 24]; however, these approaches are designed for probabilistic structured-output models and are not directly applicable to general deep architectures.", "startOffset": 104, "endOffset": 123}, {"referenceID": 21, "context": "There is a large body of work on the topic of extracting multiple diverse solutions from a single model [3, 15, 16, 23, 24]; however, these approaches are designed for probabilistic structured-output models and are not directly applicable to general deep architectures.", "startOffset": 104, "endOffset": 123}, {"referenceID": 6, "context": "[8, 9] which explicitly minimizes oracle loss over the outputs of an ensemble, formalizing this setting as the Multiple Choice Learning (MCL) paradigm.", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "[8, 9] which explicitly minimizes oracle loss over the outputs of an ensemble, formalizing this setting as the Multiple Choice Learning (MCL) paradigm.", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[5] reformulated this problem as a submodular optimization task in which ensemble members are learned sequentially in a boosting-like manner to maximize marginal gain in oracle performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] present an objective which forms a (potentially tight) upper-bound.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Note that this approach is not feasible with training deep networks, since modern architectures [11] can take weeks or months to train a single model once.", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "Figure 2: The MCL approach of [8] (Alg.", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "- MCL [8] that alternates between training models to convergence on assigned examples and allocating examples to their lowest error model.", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "[5] train models sequentially in a boosting-like fashion, each time reweighting examples to maximize marginal increase of the evaluation metric.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "(a) Convolutional classification model of [1] for CIFAR10 [17] (b) Fully-convolutional segmentation model of Long et al.", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "[20] (c) CNN+RNN based captioning model of Karpathy et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Figure 3: We experiment with three problem domains using the various architectures shown above.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "weights, [5] requires an error measure bounded above by 1: accuracy (for classification) and IoU (for segmentation) satisfy this; the CIDEr-D score [28] divided by 10 guarantees this for captioning.", "startOffset": 9, "endOffset": 12}, {"referenceID": 25, "context": "weights, [5] requires an error measure bounded above by 1: accuracy (for classification) and IoU (for segmentation) satisfy this; the CIDEr-D score [28] divided by 10 guarantees this for captioning.", "startOffset": 148, "endOffset": 152}, {"referenceID": 22, "context": "For example, in classification tasks, oracle accuracy is exactly the top-k criteria of ImageNet [25], i.", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "Oracle metrics allow the evaluation of multiple-prediction systems separately from downstream re-ranking or selection systems, and have been extensively used in previous work [3, 5, 8, 9, 15, 16, 23, 24].", "startOffset": 175, "endOffset": 203}, {"referenceID": 3, "context": "Oracle metrics allow the evaluation of multiple-prediction systems separately from downstream re-ranking or selection systems, and have been extensively used in previous work [3, 5, 8, 9, 15, 16, 23, 24].", "startOffset": 175, "endOffset": 203}, {"referenceID": 6, "context": "Oracle metrics allow the evaluation of multiple-prediction systems separately from downstream re-ranking or selection systems, and have been extensively used in previous work [3, 5, 8, 9, 15, 16, 23, 24].", "startOffset": 175, "endOffset": 203}, {"referenceID": 7, "context": "Oracle metrics allow the evaluation of multiple-prediction systems separately from downstream re-ranking or selection systems, and have been extensively used in previous work [3, 5, 8, 9, 15, 16, 23, 24].", "startOffset": 175, "endOffset": 203}, {"referenceID": 13, "context": "Oracle metrics allow the evaluation of multiple-prediction systems separately from downstream re-ranking or selection systems, and have been extensively used in previous work [3, 5, 8, 9, 15, 16, 23, 24].", "startOffset": 175, "endOffset": 203}, {"referenceID": 14, "context": "Oracle metrics allow the evaluation of multiple-prediction systems separately from downstream re-ranking or selection systems, and have been extensively used in previous work [3, 5, 8, 9, 15, 16, 23, 24].", "startOffset": 175, "endOffset": 203}, {"referenceID": 20, "context": "Oracle metrics allow the evaluation of multiple-prediction systems separately from downstream re-ranking or selection systems, and have been extensively used in previous work [3, 5, 8, 9, 15, 16, 23, 24].", "startOffset": 175, "endOffset": 203}, {"referenceID": 21, "context": "Oracle metrics allow the evaluation of multiple-prediction systems separately from downstream re-ranking or selection systems, and have been extensively used in previous work [3, 5, 8, 9, 15, 16, 23, 24].", "startOffset": 175, "endOffset": 203}, {"referenceID": 3, "context": "[5] (typical improvements of 6-10%), and MCL (while providing a 5x speedup over MCL).", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "We begin our experiments with sMCL on the CIFAR10 [17] dataset using the small convolutional neural network \u201cCIFAR10-Quick\u201d provided with the Caffe deep learning framework [13].", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "We begin our experiments with sMCL on the CIFAR10 [17] dataset using the small convolutional neural network \u201cCIFAR10-Quick\u201d provided with the Caffe deep learning framework [13].", "startOffset": 172, "endOffset": 176}, {"referenceID": 3, "context": "[5] performs worse than independent ensembles as ensemble size grows.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "2 Semantic Segmentation We now present our results for the semantic segmentation task on the Pascal VOC dataset [6].", "startOffset": 112, "endOffset": 115}, {"referenceID": 18, "context": "[20] as our base model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Like [20], we train on the Pascal VOC 2011 training set augmented with extra segmentations provided in [10] and we test on a subset of the VOC 2011 validation set.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "Like [20], we train on the Pascal VOC 2011 training set augmented with extra segmentations provided in [10] and we test on a subset of the VOC 2011 validation set.", "startOffset": 103, "endOffset": 107}, {"referenceID": 3, "context": "sMCL MCL Dey [5] Indp.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "[5] saturates more quickly \u2013 resulting in performance worse than classical ensembles as ensemble size grows.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Ensemble Size M O ra cl e M ea n Io U sMCL MCL Dey [5] Indp.", "startOffset": 51, "endOffset": 54}, {"referenceID": 12, "context": "[14], utilizing their publicly available implementation neuraltalk2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The model consists of an VGG16 network [4] which encodes the input image as a fixed-length representation for a Long Short-Term Memory (LSTM) language model.", "startOffset": 39, "endOffset": 42}, {"referenceID": 16, "context": "We train and test on the MSCOCO dataset [18], using the same splits as [14].", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "We train and test on the MSCOCO dataset [18], using the same splits as [14].", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "We generate sentences for testing by performing beam search with a beam width of two (following [14]).", "startOffset": 96, "endOffset": 100}, {"referenceID": 25, "context": "Table 1 presents the oracle CIDEr-D [28] scores for all methods on the validation set.", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "21 MCL [8] - 0.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "87 Dey [5] - 0.", "startOffset": 7, "endOffset": 10}], "year": 2016, "abstractText": "Many practical perception systems exist within larger processes that include interactions with users or additional components capable of evaluating the quality of predicted solutions. In these contexts, it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction. In this work, we pose the task of producing multiple outputs as a learning problem over an ensemble of deep networks \u2013 introducing a novel stochastic gradient descent based approach to minimize the loss with respect to an oracle. Our method is simple to implement, agnostic to both architecture and loss function, and parameter-free. Our approach achieves lower oracle error compared to existing methods on a wide range of tasks and deep architectures. We also show qualitatively that the diverse solutions produced often provide interpretable representations of task ambiguity.", "creator": "LaTeX with hyperref package"}}}