{"id": "1606.07901", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2016", "title": "Corpus-level Fine-grained Entity Typing Using Contextual Information", "abstract": "This paper addresses the problem of corpus-level entity typing, i.e., inferring from a large corpus that an entity is a member of a class such as \"food\" or \"artist\". The application of entity typing we are interested in is knowledge base completion, specifically, to learn which classes an entity is a member of. We propose FIGMENT to tackle this problem. FIGMENT is embedding-based and combines (i) a global model that scores based on aggregated contextual information of an entity and (ii) a context model that first scores the individual occurrences of an entity and then aggregates the scores. In our evaluation, FIGMENT strongly outperforms an approach to entity typing that relies on relations obtained by an open information extraction system.", "histories": [["v1", "Sat, 25 Jun 2016 12:22:05 GMT  (36kb)", "http://arxiv.org/abs/1606.07901v1", "Accepted at EMNLP2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing"]], "COMMENTS": "Accepted at EMNLP2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yadollah yaghoobzadeh", "hinrich sch\u00fctze"], "accepted": true, "id": "1606.07901"}, "pdf": {"name": "1606.07901.pdf", "metadata": {"source": "CRF", "title": "Corpus-level Fine-grained Entity Typing Using Contextual Information", "authors": ["Yadollah Yaghoobzadeh"], "emails": ["yadollah@cis.lmu.de"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.07 901v 1 [cs.C L] 25 Jun 2016"}, {"heading": "1 Introduction", "text": "eSi rf\u00fc ide nlrrsrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2 Related work", "text": "In fact, most of them are able to survive on their own without having to stray into another world."}, {"heading": "3 Motivation and problem definition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Freebase", "text": "Large KBs such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007) and Google Knowledge Graph are important NLP resources. Their structure roughly corresponds to a graph in which entities are nodes and edges are relationships between entities. Each node is also associated with one or more semantic classes, so-called types, which are at the center of this paper. We use Freebase, the largest KB available, in this paper. In Freebase, an entity can belong to several classes, e.g. \"Barack Obama\" is a member of 37 types, including \"US President\" and \"Author.\" A notable type is also defined for each entity, e.g. \"US President\" for \"Obama,\" as it is considered to be its most prominent feature and the one that would be used to apply unambiguous references to it, e.g. to distinguish it from someone else by the same name."}, {"heading": "3.2 Incompleteness of knowledge base", "text": "Although Freebase is the largest publicly available database of its kind, it still has significant coverage problems; for example, 78.5% of people in Freebase have no nationality (Min et al., 2013). This is inevitable, partly because Freebase is user-generated, partly because the world is changing and Freebase needs to be updated to reflect these changes. All existing KBs that attempt to model much of the world suffer from this problem of incompleteness. The approach we take in this paper to address the incompleteness of KBs is to extract information from large corpora of text, such as Freebase. The more and more fine-grained types are added, the more difficult it becomes to achieve good coverage for these new types when only human editors are used."}, {"heading": "3.3 Entity linking", "text": "This problem of entity linkage has some interdependencies with the input of entities. In fact, some recent work shows great improvements when entity types and linkages are modeled together (Ling et al., 2015; Durrett et al., 2014). However, there are limitations that are important for a powerful linkage of entities, but are of little relevance for the typing of entities. For example, there is a large literature on linkage of entities that deals with the resolution of entities and constraints between entities - for example, \"Naples\" refers more to a US city (or an Italian city) in a context in which \"Fort Myers\" (or \"Sicily\") is mentioned. Therefore, we will only deal with the typing of entities in this paper and consider linking entities as an independent module that includes contexts of FITs for ENT, the FITs we say."}, {"heading": "3.4 FIGER types", "text": "Our goal is fine-grained typing of companies, but types like \"Vietnamese Borough\" are too fine-grained. To provide a reliable framework for evaluation, and to ensure that all types have an appropriate number of instances, we use the FIGER type set (Ling and Weld, 2012), which was created with the same objectives. FIGER consists of 112 tags and was created in an attempt to preserve the variety of freebase types while consolidating rare and unusual types through filtering and fusing. For example, the freebase types \"dish,\" \"\" ingredient, \"\" food \"and\" cheese \"are mapped to a type of\" food. \"See (Ling and Weld, 2012) for a full list of FIGER types. We use\" type \"to refer to FIGER types in the rest of the paper. 1lemurproject.org / cluewebF12 / C1"}, {"heading": "4 Global, context and joint models", "text": "We address a problem in which the following points are given: a KB with a group of units E, a set of types T, and a membership function m: E \u00b7 T 7 \u2192 {0, 1}, so that m (e, t) = 1 iff unit e has type t; and a large commented corpus C, in which mentions of E are linked to each other. As already mentioned, we use FACC1 as a body part. In this problem we deal with the task of typing fine-grained units at the body level: We want to derive and type from the corpus for each pair of unit e whether m (e, t) = 1, i.e. whether unit e is a member of the type. We use three scoring models in FIGMENT: global model, context model, and common model. The models provide a score S (e, t) for a unit-type pair (e, t) as a unit."}, {"heading": "4.1 Global model", "text": "The global model (GM) evaluates possible types of entities e based on a distributed vector representation or embedding of ~ v (e) and e. The embedding of a word usually results from the distribution of its context words. The hypothesis is that words with similar meanings occur in similar contexts (Harris, 1954) and therefore coexist with similar context words. In the further course of our model, it is assumed that entities with similar types tend to cooperate with similar context words. To learn a score function, we use a multi-layer perceptron (MLP) with a common hidden layer and an output layer containing for each type t in T a logistic regression classifier that predicts the probability of t: SGM (e, t) = Gt tanh (wind) and an output layer in which is used for the distribution of the layer (wind)."}, {"heading": "4.2 Context model", "text": "For the context model (CM), we first learn a scoring function Sc2t (c, t) for individual contexts c in the corpus. Sc2t (c, t) is an assessment of how likely it is that an entity occurring in the context c has type t. For example, consider the contexts c1 = \"he served SLOT in wine\" and c2 = \"she loves SLOT more than anything else.\" SLOT denotes the occurrence of an entity and it also shows that we do not care about the entity itself, but only about its context. For type t = \"food,\" Sc2t (c1, t) the use of SLOT is high, while Sc2t (c2, t) is low. This example shows that some contexts of an entity such as \"beef\" allow specific conclusions about its type while others do not. We aim to learn a scoring function Sc2t that can distinguish these cases."}, {"heading": "4.3 Joint model", "text": "The strength of CM is that it is a direct model for the only source of reliable evidence we have: the context in which the company occurs. This is also the way in which a person typifies a company: it would determine whether a certain context in which the company occurs is applicable. In particular, if the company contains only a small number of companies of a certain type, the corpus contains a large number of contexts and takes CM into account. A well-trained CM will also work in cases where GM is not applicable."}, {"heading": "5 Experimental setup and results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Setup", "text": "In fact, it is the case that people in the United States and other countries in crisis are able to help themselves. (...) It is not the case that they are able to help themselves. (...) It is not the case that they are able to help themselves. (...) It is not the case that they are able to help themselves. (...) It is the case that they are able to do it. (...) It is the case that they are able to do it. (...) It is the case that they are able to do it. (...) It is the case that they are able to do it. (...) It is the case that they are able to do it. (...) It is the case. (...) It is the case. (...) It is the case. (...). (It is. (...). (It is.) (It is. (It is.) (It is. (...). (It is.). (It is. (It is.). (It is. (It is.). (It is. (It is.). (It is. (it.). (It is. (it is.). (It is. (it.). (It is.). (It is. (it.). (It is. (it.). (it is. (it.). (it.)"}, {"heading": "5.2 Results", "text": "Table 1 shows the results for the ranking as well as for the first three measurements of the classification evaluation. MFT is the most common type baseline, which classifies types according to their frequency in the train. We also show the results for head units (frequency higher than 100) and tail units (frequency lower than 5). The performance of the systems is in this order: JM > GM > CM > NPLB > MFT.Table 2 shows the results of the fourth classification measure, type macro average F1, for all, head (more than 3000 train units, 11 types) and tail (less than 200 train units, 36 types) types. The order of models for Table 2 is in line with Table 1: JM > GM > CM > NPLB > MFT.We can easily perform FIGMENT for non-subjective units (NSE) exactly as we do for subjects. We test our JM for the 67,000 units with a frequency greater than 10%."}, {"heading": "6 Analysis", "text": "This year is the highest in the history of the country."}, {"heading": "7 Future work", "text": "We plan to address this problem in the future (i) by running FIGMENT on larger corporations, (ii) by refining the FIGER type to cover more freebase units, (iii) by exploiting a hierarchy of individual types, and (iv) by examining more complex input representations of the context for the CM.FIGMENT context model, which can in principle be based on any system that provides entity-type ratings for individual contexts. Therefore, as an alternative to our Sc2t (c, t) scoring model, we could use sentence-based entity classification systems such as FIGER (Ling and Weld, 2012) and (Yogatama et al., 2015), which are based on linguistic characteristics that differ from the input representation we use, so a comparison with our embedding-based approach is interesting. Our assumption is that certain FIENT organizations are robust against GMENT, but the FIENT is not."}, {"heading": "8 Conclusion", "text": "We introduced FIGMENT, a corpus-level system that uses context information to enter entities. We developed two scoring models for pairs of entities and types: a global model based on aggregated context information and a context model that aggregates entities of individual contexts. We used embedding of words, entities and types to represent context information. Our experimental results show that global model and context model provide complementary information for the typing of entities. We showed that FIGMENT performs well compared to an OpenIE-based system on noisy web pages. Thanks to the anonymous reviewers for their valuable comments. This work was supported by the German Research Foundation (DFG SCHU 2246 / 8-2, SPP 1335)."}], "references": [{"title": "Don\u2019t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Against intellectual monopoly", "author": ["Boldrin", "Levine2008] Michele Boldrin", "David K. Levine"], "venue": null, "citeRegEx": "Boldrin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Boldrin et al\\.", "year": 2008}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the ACM SIGMOD International Con-", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Irreflexive and hierarchical relations as translations. CoRR, abs/1304.7158", "author": ["Nicolas Usunier", "Alberto Garc\u0131\u0301a-Dur\u00e1n", "Jason Weston", "Oksana Yakhnenko"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "A hybrid neural model for type classification of entity mentions", "author": ["Dong et al.2015] Li Dong", "Furu Wei", "Hong Sun", "Ming Zhou", "Ke Xu"], "venue": "In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John C. Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A joint model for entity analysis: Coreference, typing, and linking", "author": ["Durrett", "Klein2014] Greg Durrett", "Dan Klein"], "venue": null, "citeRegEx": "Durrett et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2014}, {"title": "Identifying relations for open information extraction", "author": ["Fader et al.2011] Anthony Fader", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Fader et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2011}, {"title": "Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1535\u20131545", "author": ["John McIntyre"], "venue": null, "citeRegEx": "2011 and McIntyre,? \\Q2011\\E", "shortCiteRegEx": "2011 and McIntyre", "year": 2011}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["Trond Grenager", "Christopher D. Manning"], "venue": "In ACL 2005,", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Facc1: Freebase annotation of clueweb corpora", "author": ["Michael Ringgaard", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gabrilovich et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gabrilovich et al\\.", "year": 2013}, {"title": "Improved pattern learning for bootstrapped entity extraction", "author": ["Gupta", "Manning2014] Sonal Gupta", "Christopher D. Manning"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Gupta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2014}, {"title": "Link prediction in multi-relational graphs using additive models", "author": ["Jiang et al.2012] Xueyan Jiang", "Volker Tresp", "Yi Huang", "Maximilian Nickel"], "venue": "In Proceedings of the International Workshop on Semantic Technologies meet Recommender", "citeRegEx": "Jiang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2012}, {"title": "No noun phrase left behind: Detecting and typing unlinkable entities", "author": ["Lin et al.2012] Thomas Lin", "Mausam", "Oren Etzioni"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Lin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Fine-grained entity recognition", "author": ["Ling", "Weld2012] Xiao Ling", "Daniel S. Weld"], "venue": "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, July 22-26,", "citeRegEx": "Ling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2012}, {"title": "Design challenges for entity linking", "author": ["Ling et al.2015] Xiao Ling", "Sameer Singh", "Daniel S. Weld"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Overview of the tac 2009 knowledge base population track", "author": ["McNamee", "Dang2009] Paul McNamee", "Hoa Trang Dang"], "venue": "In Text Analysis Conference (TAC),", "citeRegEx": "McNamee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McNamee et al\\.", "year": 2009}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction with an incomplete knowledge base", "author": ["Min et al.2013] Bonan Min", "Ralph Grishman", "Li Wan", "Chang Wang", "David Gondek"], "venue": "In Human Language Technologies: Conference of the North American Chapter", "citeRegEx": "Min et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Min et al\\.", "year": 2013}, {"title": "Fine-grained semantic typing of emerging entities", "author": ["Tomasz Tylenda", "Gerhard Weikum"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Nakashole et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2013}, {"title": "Inferring missing entity type instances for knowledge base completion: New dataset and methods", "author": ["Neelakantan", "Chang2015] Arvind Neelakantan", "Ming-Wei Chang"], "venue": "In NAACL HLT 2015,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Factorizing YAGO: scalable machine learning for linked data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In World Wide Web Conference,", "citeRegEx": "Nickel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "In Human Language Technologies: Conference of the North American Chapter", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Yago: a core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th International Conference on World Wide Web,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D. Manning"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "A bootstrapping method for learning semantic lexicons using extraction pattern contexts", "author": ["Thelen", "Riloff2002] Michael Thelen", "Ellen Riloff"], "venue": "In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Thelen et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Thelen et al\\.", "year": 2002}, {"title": "Knowledge graph and text jointly embedding", "author": ["Wang et al.2014] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Connecting language and knowledge bases with embedding models for relation extraction", "author": ["Weston et al.2013] Jason Weston", "Antoine Bordes", "Oksana Yakhnenko", "Nicolas Usunier"], "venue": "In Proceedings of the 2013 Conference on Empirical Meth-", "citeRegEx": "Weston et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2013}, {"title": "Embedding methods for fine grained entity type classification", "author": ["Daniel Gillick", "Nevena Lazic"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-", "citeRegEx": "Yogatama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2015}, {"title": "HYENA: hierarchical type classification for entity names", "author": ["Sandro Bauer", "Johannes Hoffart", "Marc Spaniol", "Gerhard Weikum"], "venue": "COLING", "citeRegEx": "Yosef et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yosef et al\\.", "year": 2012}, {"title": "Representation learning for measuring entity relatedness with rich information", "author": ["Zhao et al.2015] Yu Zhao", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Multi-instance multi-label learning with application to scene classification", "author": ["Zhou", "Zhang2006] Zhi-Hua Zhou", "Min-Ling Zhang"], "venue": "In Advances in Neural Information Processing Systems 19,", "citeRegEx": "Zhou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 30, "context": "(2005)) only consider a small number of entity classes, recent work has addressed fine-grained NER (Yosef et al., 2012; Ling and Weld, 2012).", "startOffset": 99, "endOffset": 140}, {"referenceID": 8, "context": ", Finkel et al. (2005)) only consider a small number of entity classes, recent work has addressed fine-grained NER (Yosef et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 4, "context": "Dong et al. (2015) use distributed representations of words in a hybrid classifier to classify mentions to 20 types.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Dong et al. (2015) use distributed representations of words in a hybrid classifier to classify mentions to 20 types. Yogatama et al. (2015) classify mentions to more fine-grained types by using different features for mentions and embedding labels in the same space.", "startOffset": 0, "endOffset": 140}, {"referenceID": 13, "context": "More closely related to our work are the OpenIE systems NNPLB (Lin et al., 2012) and PEARL (Nakashole et al.", "startOffset": 62, "endOffset": 80}, {"referenceID": 19, "context": ", 2012) and PEARL (Nakashole et al., 2013) for fine-grained typing of unlinkable and emerging entities.", "startOffset": 18, "endOffset": 42}, {"referenceID": 21, "context": "Some generalize the patterns of relationships within the KB (Nickel et al., 2012; Bordes et al., 2013) while others use a combina-", "startOffset": 60, "endOffset": 102}, {"referenceID": 3, "context": "Some generalize the patterns of relationships within the KB (Nickel et al., 2012; Bordes et al., 2013) while others use a combina-", "startOffset": 60, "endOffset": 102}, {"referenceID": 28, "context": "tion of within-KB generalization and information extraction from text (Weston et al., 2013; Socher et al., 2013; Jiang et al., 2012; Riedel et al., 2013; Wang et al., 2014).", "startOffset": 70, "endOffset": 172}, {"referenceID": 23, "context": "tion of within-KB generalization and information extraction from text (Weston et al., 2013; Socher et al., 2013; Jiang et al., 2012; Riedel et al., 2013; Wang et al., 2014).", "startOffset": 70, "endOffset": 172}, {"referenceID": 12, "context": "tion of within-KB generalization and information extraction from text (Weston et al., 2013; Socher et al., 2013; Jiang et al., 2012; Riedel et al., 2013; Wang et al., 2014).", "startOffset": 70, "endOffset": 172}, {"referenceID": 22, "context": "tion of within-KB generalization and information extraction from text (Weston et al., 2013; Socher et al., 2013; Jiang et al., 2012; Riedel et al., 2013; Wang et al., 2014).", "startOffset": 70, "endOffset": 172}, {"referenceID": 27, "context": "tion of within-KB generalization and information extraction from text (Weston et al., 2013; Socher et al., 2013; Jiang et al., 2012; Riedel et al., 2013; Wang et al., 2014).", "startOffset": 70, "endOffset": 172}, {"referenceID": 12, "context": ", 2013; Jiang et al., 2012; Riedel et al., 2013; Wang et al., 2014). Neelakantan and Chang (2015) address entity typing in a way that is similar to FIGMENT.", "startOffset": 8, "endOffset": 98}, {"referenceID": 31, "context": "(2013)) and entity and type embeddings (Zhao et al., 2015) has mainly used KB information as opposed to text corpora.", "startOffset": 39, "endOffset": 58}, {"referenceID": 17, "context": "We learn all our embeddings using word2vec (Mikolov et al., 2013).", "startOffset": 43, "endOffset": 65}, {"referenceID": 0, "context": "Learning embeddings for words is standard in a large body of NLP literature (see Baroni et al. (2014) for an overview).", "startOffset": 81, "endOffset": 102}, {"referenceID": 0, "context": "Learning embeddings for words is standard in a large body of NLP literature (see Baroni et al. (2014) for an overview). In addition to words, we also learn embeddings for entities and types. Most prior work on entity embeddings (e.g., Weston et al. (2013), Bordes et al.", "startOffset": 81, "endOffset": 256}, {"referenceID": 0, "context": "Learning embeddings for words is standard in a large body of NLP literature (see Baroni et al. (2014) for an overview). In addition to words, we also learn embeddings for entities and types. Most prior work on entity embeddings (e.g., Weston et al. (2013), Bordes et al. (2013)) and entity and type embeddings (Zhao et al.", "startOffset": 81, "endOffset": 278}, {"referenceID": 0, "context": "Learning embeddings for words is standard in a large body of NLP literature (see Baroni et al. (2014) for an overview). In addition to words, we also learn embeddings for entities and types. Most prior work on entity embeddings (e.g., Weston et al. (2013), Bordes et al. (2013)) and entity and type embeddings (Zhao et al., 2015) has mainly used KB information as opposed to text corpora. Wang et al. (2014) learn embeddings of words and entities in the same space by replacing Wikipedia anchors with their corresponding entities.", "startOffset": 81, "endOffset": 408}, {"referenceID": 25, "context": "Our problem can be formulated as multi-instance multi-label (MIML) learning (Zhou and Zhang, 2006), similar to the formulation for relation extraction by Surdeanu et al. (2012). In our problem, each example (entity) can have several instances (contexts) and each instance can have several labels (types).", "startOffset": 154, "endOffset": 177}, {"referenceID": 25, "context": "Our problem can be formulated as multi-instance multi-label (MIML) learning (Zhou and Zhang, 2006), similar to the formulation for relation extraction by Surdeanu et al. (2012). In our problem, each example (entity) can have several instances (contexts) and each instance can have several labels (types). Similar to Zhou and Zhang (2006)\u2019s work on scene classification, we also transform MIML into easier tasks.", "startOffset": 154, "endOffset": 338}, {"referenceID": 2, "context": "Large scale KBs like Freebase (Bollacker et al., 2008), YAGO (Suchanek et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 24, "context": ", 2008), YAGO (Suchanek et al., 2007) and Google knowledge graph are important NLP resources.", "startOffset": 14, "endOffset": 37}, {"referenceID": 18, "context": "5% of persons in Freebase do not have nationality (Min et al., 2013).", "startOffset": 50, "endOffset": 68}, {"referenceID": 15, "context": "Indeed, some recent work shows large improvements when entity typing and linking are jointly modeled (Ling et al., 2015; Durrett and Klein, 2014).", "startOffset": 101, "endOffset": 145}, {"referenceID": 10, "context": "More specifically, we build FIGMENT on top of the output of an existing entity linking system and use FACC1,1 an automatic Freebase annotation of ClueWeb (Gabrilovich et al., 2013).", "startOffset": 154, "endOffset": 180}, {"referenceID": 5, "context": "Stochastic gradient descent (SGD) with AdaGrad (Duchi et al., 2011) and minibatches are used to learn the parameters.", "startOffset": 47, "endOffset": 67}, {"referenceID": 13, "context": "Baseline: Our baseline system is the OpenIE system no-noun-phrase-left-behind (NNPLB) by Lin et al. (2012) (see Section 2).", "startOffset": 89, "endOffset": 107}, {"referenceID": 13, "context": "The precision of our implementation on the dataset of three million relation triples distributed by (Lin et al., 2012) is 60.", "startOffset": 100, "endOffset": 118}, {"referenceID": 13, "context": "The precision of our implementation on the dataset of three million relation triples distributed by (Lin et al., 2012) is 60.7% compared to 59.8% and 61% for tail and head entities reported by Lin et al. (2012). http://lemurproject.", "startOffset": 101, "endOffset": 211}, {"referenceID": 7, "context": "We run the OpenIE system Reverb (Fader et al., 2011) to extract relation triples of the form <subject, relation, object>.", "startOffset": 32, "endOffset": 52}, {"referenceID": 13, "context": "to the evaluation performed by Lin et al. (2012), we use precision at 1 (P@1) and breakeven point (BEP, Boldrin and Levine (2008)).", "startOffset": 31, "endOffset": 49}, {"referenceID": 13, "context": "to the evaluation performed by Lin et al. (2012), we use precision at 1 (P@1) and breakeven point (BEP, Boldrin and Levine (2008)).", "startOffset": 31, "endOffset": 130}, {"referenceID": 29, "context": "Thus, as an alternative to our scoring model Sc2t(c, t), we could use sentence-level entity classification systems such as FIGER (Ling and Weld, 2012) and (Yogatama et al., 2015)\u2019s system.", "startOffset": 155, "endOffset": 178}], "year": 2016, "abstractText": "This paper addresses the problem of corpus-level entity typing, i.e., inferring from a large corpus that an entity is a member of a class such as \u201cfood\u201d or \u201cartist\u201d. The application of entity typing we are interested in is knowledge base completion, specifically, to learn which classes an entity is a member of. We propose FIGMENT to tackle this problem. FIGMENT is embedding-based and combines (i) a global model that scores based on aggregated contextual information of an entity and (ii) a context model that first scores the individual occurrences of an entity and then aggregates the scores. In our evaluation, FIGMENT strongly outperforms an approach to entity typing that relies on relations obtained by an open information extraction system.", "creator": "LaTeX with hyperref package"}}}