{"id": "1606.07902", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2016", "title": "Intrinsic Subspace Evaluation of Word Embedding Representations", "abstract": "We introduce a new methodology for intrinsic evaluation of word representations. Specifically, we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria. Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words and thus view vector representations as points. We show the limits of these point-based intrinsic evaluations. We apply our evaluation methodology to the comparison of a count vector model and several neural network models and demonstrate important properties of these models.", "histories": [["v1", "Sat, 25 Jun 2016 12:27:17 GMT  (36kb)", "http://arxiv.org/abs/1606.07902v1", "Long paper accepted in ACL2016"]], "COMMENTS": "Long paper accepted in ACL2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yadollah yaghoobzadeh", "hinrich sch\u00fctze"], "accepted": true, "id": "1606.07902"}, "pdf": {"name": "1606.07902.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["yadollah@cis.lmu.de"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.07 902v 1 [cs.C L] 25 Jun 2016"}, {"heading": "1 Introduction", "text": "This year it has come to the point where it has never worked out as well as it did before."}, {"heading": "2 Related Work", "text": "Baroni et al. (2014) evaluate embedding using different intrinsic tests: similarity, analogy, synonym recognition, categorization and selective preferences. Schnabel et al. (2015) introduce tasks with finer-grained data sets. These tasks are unattended and generally based on cosinal similarity, which means that only the general direction of vectors is taken into account or, equivalent, that words are modeled as points in a room and only their spatial distance / proximity is taken into account. In contrast, we test embedding in a classification scheme and in different subdivisions of embedding. Tsvetkov et al. (2015) evaluate embedding based on their correlations with linguistic embedding on a WordNet basis. Correlation, however, does not directly evaluate how accurately and completely an application can filter embedding out of an embedding."}, {"heading": "3 Criteria for word representations", "text": "Each word is a combination of different characteristics. Depending on the language, these characteristics include lexical, syntactic, semantic, global knowledge and other characteristics. We call these characteristics facets. The ultimate goal is to learn representations for words that contain these facets precisely and consistently. Let's take the multi-faceted gender (GEN) as an example. We now present four important criteria that a representation must meet in order to present facets accurately and uniformly. These criteria are applied across various problems that NLP applications face in the effective use of embedding. Nevertheless, a word that contains a gene facet must contain this information. We present four important criteria that a representation must meet in order to present facets accurately and uniformly."}, {"heading": "4 Experimental setup and results", "text": "In fact, most people in the world are able to understand themselves and what they are doing. (...) Most people in the world do not know what they are doing. (...) Most people in the world do not know what they are doing. (...) Most people in the world do not know what they are doing. (...) People in the world do not know what they are doing. (...) People in the world do not know what they are doing. (...) People in the world do not know what they are doing. (...) People in the world do not know what they are doing. (...) People in the world do not know what they are doing. (...) People in the world do not know what they are doing. (...) People in the world do not know what they are doing. (...) People in the world do not know what they are doing. (...) People in the world do not know what they are doing. \""}, {"heading": "4.1 Nonconflation", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they are able to"}, {"heading": "4.2 Robustness against sparseness", "text": "The grammar shown in Figure 2 generates frequent vi and rare uzm context (lines 1 and 9); and frequent wi and rare xi in the contexts (lines 2 and 10). The language generated by the PCFG is in lines 9-11 with the ten contexts a0u0b0.. This is how to model the phenomenon of thrift in the contexts c0x0d0. That is, each of the two contexts is accurate in the contexts uzm and in the contexts."}, {"heading": "4.3 Robustness against ambiguity", "text": "The grammar in Figure 3 generates two types of contexts, which we interpret as two different meanings: a-b contexts (lines 1.4) and c-d contexts (lines 2, 3). vi occur only in a-b contexts (line 1), w49 occur only in c-d contexts (line 2), so they are unambiguous. w0.. w4 are ambiguous and most likely \u03b2 in c-d contexts (line 3) and most likely \u03b2 (line 1 \u2212 \u03b2) in a-b contexts (line 3, 4). The parameter \u03b2 controls the sketchiness of the distribution of meaning; for example, the two senses for \u03b2 = 0.5 and the second sense (line 4) are three times as likely as the first sense (line 3) for \u03b2 = 0.25 dataset. The grammar specified in Figure 3 was used to generate a training body of 100,000 sentences."}, {"heading": "4.4 Accurate and consistent representation of multifacetedness", "text": "The grammar shown in Figure 5 shows two syntactic categories, nouns and adjectives, the left context of which is highly predictable: it is one of five left context words ni (resp. ai) for nouns, see lines 1, 3, 5 (resp. for adjectives, see lines 2, 4, 6). However, there are two grammatical genders: female (corresponding to the two symbols Fn and Fa) and masculine (corresponding to the two symbols Mn and Ma). The four combinations of syntactic categories and genders are equally probable (lines 1-4). In addition to gender, nouns and adjectives, morphological paradigms are distinguished. Line 7 generates one of five feminine nouns (xnfi) and the corresponding paradigms of marker U nfi."}, {"heading": "5 Analysis", "text": "In this section, we first summarize and analyze the lessons we learned through experiments in Section 4, and then show how these lessons are supported by a real natural language corpus."}, {"heading": "5.1 Learned lessons", "text": "(i) Two words with distinctly different context distributions should have different representations; aggregation models cannot do this by calculating global statistics. (ii) The embedding of learning may have different effectiveness in sparse or non-sparse events. Therefore, models of representations should be evaluated in terms of their ability to deal with scarcity; evaluation data sets should include both rare and frequent words. (iii) Our results in Section 4.3 suggest that individual representation approaches can actually represent different senses of a word and therefore have little impact on fullspace similarity values. Such failure does not necessarily mean that a particular meaning is not present in the representation, and it does not necessarily mean that individual representation approaches occupy a small part of the capacity of the representation and therefore have little impact on those values."}, {"heading": "5.2 Extrinsic evaluation: entity typing", "text": "This year it is more than ever before in the history of the city."}, {"heading": "6 Conclusion and future work", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to hide ourselves, and that we are able, that we are able to hide ourselves."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Agirre et al.2009] Eneko Agirre", "Enrique Alfonseca", "Keith B. Hall", "Jana Kravalova", "Marius Pasca", "Aitor Soroa"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Baroni", "Lenci2010] Marco Baroni", "Alessandro Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Don\u2019t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Language universals and linguistic typology: Syntax and morphology", "author": ["Bernard Comrie"], "venue": null, "citeRegEx": "Comrie.,? \\Q1989\\E", "shortCiteRegEx": "Comrie.", "year": 1989}, {"title": "Indexing by latent semantic analysis", "author": ["Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman"], "venue": "Journal of the American Society for Information", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan et al.2008] Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Facc1: Freebase annotation of clueweb corpora", "author": ["Michael Ringgaard", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gabrilovich et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gabrilovich et al\\.", "year": 2013}, {"title": "Improving word representations via global context and multiple word prototypes. In The 50th Annual Meeting of the Association for Computational Lin", "author": ["Huang et al.2012] Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "What?s in an embedding? analyzing word embeddings through multilingual evaluation", "author": ["Arne K\u00f6hn"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "K\u00f6hn.,? \\Q2015\\E", "shortCiteRegEx": "K\u00f6hn.", "year": 2015}, {"title": "How to generate a good word embedding? CoRR, abs/1507.05523", "author": ["Lai et al.2015] Siwei Lai", "Kang Liu", "Liheng Xu", "Jun Zhao"], "venue": null, "citeRegEx": "Lai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Goldberg2014a] Omer Levy", "Yoav Goldberg"], "venue": "In CoNLL", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014b] Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Levy et al.2015] Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Do multi-sense embeddings improve natural language understanding", "author": ["Li", "Jurafsky2015] Jiwei Li", "Dan Jurafsky"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan W. Black", "Isabel Trancoso"], "venue": "In NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Mnih", "Kavukcuoglu2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Reisinger", "Mooney2010] Joseph Reisinger", "Raymond J Mooney"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Word embedding calculus in meaningful ultradense subspaces", "author": ["Rothe", "Sch\u00fctze2016] Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Rothe et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rothe et al\\.", "year": 2016}, {"title": "Ultradense embeddings by orthogonal transformation", "author": ["Rothe et al.2016] Sascha Rothe", "Sebastian Ebert", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Rothe et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rothe et al\\.", "year": 2016}, {"title": "How well do distributional models capture different types of semantic knowledge", "author": ["Effi Levi", "Roy Schwartz", "Ari Rappoport"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Rubinstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2015}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Igor Labutov", "David Mimno", "Thorsten Joachims"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Schnabel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Tsvetkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Corpus-level fine-grained entity typing using contextual information", "author": ["Yaghoobzadeh", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural", "citeRegEx": "Yaghoobzadeh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yaghoobzadeh et al\\.", "year": 2015}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhong", "Ng2010] Zhi Zhong", "Hwee Tou Ng"], "venue": "ACL", "citeRegEx": "Zhong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "(Baroni et al., 2014) for an overview.", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": ", Mikolov et al. (2013), Pennington et al.", "startOffset": 2, "endOffset": 24}, {"referenceID": 13, "context": ", Mikolov et al. (2013), Pennington et al. (2014), Baroni et al.", "startOffset": 2, "endOffset": 50}, {"referenceID": 1, "context": "(2014), Baroni et al. (2014)).", "startOffset": 8, "endOffset": 29}, {"referenceID": 8, "context": "K\u00f6hn (2015) gives similar suggestions and also recommends the use of supervised methods for evaluation.", "startOffset": 0, "endOffset": 12}, {"referenceID": 8, "context": "K\u00f6hn (2015) gives similar suggestions and also recommends the use of supervised methods for evaluation. Lai et al. (2015) evaluate embeddings", "startOffset": 0, "endOffset": 122}, {"referenceID": 3, "context": "on the following linguistic phenomenon, a phenomenon that occurs frequently crosslinguistically (Comrie, 1989).", "startOffset": 96, "endOffset": 110}, {"referenceID": 5, "context": "The embeddings obtained are then evaluated in a classification setting, in which we apply a linear SVM (Fan et al., 2008) to classify embeddings.", "startOffset": 103, "endOffset": 121}, {"referenceID": 4, "context": ", Deerwester et al. (1990), Levy and Goldberg (2014b)) and supervised methods such as neural networks (e.", "startOffset": 2, "endOffset": 27}, {"referenceID": 4, "context": ", Deerwester et al. (1990), Levy and Goldberg (2014b)) and supervised methods such as neural networks (e.", "startOffset": 2, "endOffset": 54}, {"referenceID": 15, "context": "The learning-based models are: (i) vLBL (henceforth: LBL) (vectorized log-bilinear language model) (Mnih and Kavukcuoglu, 2013), (ii) SkipGram (henceforth: SKIP) (skipgram bag-ofword model), (iii) CBOW (continuous bag-ofword model (Mikolov et al., 2013), (iv) Structured SkipGram (henceforth SSKIP), (Ling et al.", "startOffset": 231, "endOffset": 253}, {"referenceID": 14, "context": ", 2013), (iv) Structured SkipGram (henceforth SSKIP), (Ling et al., 2015) and CWindow (henceforth CWIN) (continuous window model) (Ling et al.", "startOffset": 54, "endOffset": 73}, {"referenceID": 14, "context": ", 2015) and CWindow (henceforth CWIN) (continuous window model) (Ling et al., 2015).", "startOffset": 64, "endOffset": 83}, {"referenceID": 9, "context": "We use word2vec1 for SKIP and CBOW, wang2vec2 for SSKIP and CWIN, and Lai et al. (2015)\u2019s implementation3 for LBL.", "startOffset": 70, "endOffset": 88}, {"referenceID": 0, "context": "The reason that the model of Agirre et al. (2009) is rarely used is precisely its", "startOffset": 29, "endOffset": 50}, {"referenceID": 7, "context": "The main motivation for this approach is the assumption that singleword distributional representations cannot represent all senses of a word well (Huang et al., 2012).", "startOffset": 146, "endOffset": 166}, {"referenceID": 7, "context": "For example, Reisinger and Mooney (2010) and Huang et al. (2012) cluster the contexts of each word and then learn a different representation for each cluster.", "startOffset": 45, "endOffset": 65}, {"referenceID": 21, "context": "Learning taxonomic properties or types of words has been used as an evaluation method for word embeddings (Rubinstein et al., 2015).", "startOffset": 106, "endOffset": 131}, {"referenceID": 1, "context": "Baroni et al. (2014), Rubinstein et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Baroni et al. (2014), Rubinstein et al. (2015)), entity typing can be a promising alternative, which enables to do supervised classification instead of unsupervised clustering.", "startOffset": 0, "endOffset": 47}, {"referenceID": 6, "context": "We take part of ClueWeb, which is annotated with Freebase entities using automatic annotation of FACC17 (Gabrilovich et al., 2013), as our corpus.", "startOffset": 104, "endOffset": 130}, {"referenceID": 12, "context": "See (Levy et al., 2015) for more information about the meaning of hyperparameters.", "startOffset": 4, "endOffset": 23}, {"referenceID": 19, "context": "See Rothe et al. (2016) and Rothe and Sch\u00fctze (2016) for work that makes", "startOffset": 4, "endOffset": 24}, {"referenceID": 19, "context": "See Rothe et al. (2016) and Rothe and Sch\u00fctze (2016) for work that makes", "startOffset": 4, "endOffset": 53}], "year": 2016, "abstractText": "We introduce a new methodology for intrinsic evaluation of word representations. Specifically, we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria. Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words and thus view vector representations as points. We show the limits of these point-based intrinsic evaluations. We apply our evaluation methodology to the comparison of a count vector model and several neural network models and demonstrate important properties of these models.", "creator": "LaTeX with hyperref package"}}}