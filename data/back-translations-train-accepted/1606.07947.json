{"id": "1606.07947", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2016", "title": "Sequence-Level Knowledge Distillation", "abstract": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al, 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with only a decrease of 0.2 BLEU. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search.", "histories": [["v1", "Sat, 25 Jun 2016 18:16:39 GMT  (229kb,D)", "http://arxiv.org/abs/1606.07947v1", null], ["v2", "Thu, 4 Aug 2016 17:24:18 GMT  (231kb,D)", "http://arxiv.org/abs/1606.07947v2", "EMNLP 2016"], ["v3", "Mon, 8 Aug 2016 15:02:54 GMT  (231kb,D)", "http://arxiv.org/abs/1606.07947v3", "EMNLP 2016"], ["v4", "Thu, 22 Sep 2016 01:17:12 GMT  (232kb,D)", "http://arxiv.org/abs/1606.07947v4", "EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["yoon kim", "alexander m rush"], "accepted": true, "id": "1606.07947"}, "pdf": {"name": "1606.07947.pdf", "metadata": {"source": "CRF", "title": "Sequence-Level Knowledge Distillation", "authors": ["Yoon Kim", "Alexander M. Rush"], "emails": ["yoonkim@seas.harvard.edu", "srush@seas.harvard.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to move to a different world in which they are able to escape than to another world in which they are able to escape."}, {"heading": "2 Background", "text": "For notation, we refer to vectors with bold lowercase letters (e.g. hi, b), matrices with bold uppercase letters (e.g. W, U), and sets with italic uppercase letters (e.g. V, T). We assume that words are represented by their indexes."}, {"heading": "2.1 Sequence-to-Sequence with Attention", "text": "We start with the description of our base set NMT model proposed by Luong et al. (2015) and achieve state-of-the-art results in English \u2192 German translation. Let s = [s1,.] and t = [t1,.., tJ] be (random variable sequences represent) the source / target sentence, with I and J respectively being the source / target lengths. Machine translation involves finding the most likely target sentence given the source: argmax t Tp (t | s) where T is the set of all possible sequences. NMT models parameterize p (t | s) with an encoder neural network which reads the source sentence and a decoder neural network which produce a distribution over the target sentence (one word at a time) given the source.Encoder In attention-based models (Bahdanau et al., 2015; Luong et al.), the encoder source sentence and outputs a hector tors."}, {"heading": "2.2 Knowledge Distillation", "text": "Knowledge distillation describes a method class for training a smaller student network in order to achieve better results by learning from a larger teacher network (in addition to learning from the training data set). We generally assume that the teacher was previously trained and that we value the parameters for the student. Knowledge distillation suggests training by matching the student's predictions with the teacher's predictions. In classification, this usually means that we determine the probabilities either via L2 on the log scale (Ba and Caruana, 2014) or through cross-entropy (Li et al., 2014; Hinton et al., 2015). Specifically, we assume that we determine a multi-class classifier using a data set of examples of the form (x, y) with possible classes V. The usual training criteria are to minimize NLL for each example from the training data, LNLL for each example, LNL for the training data."}, {"heading": "3 Knowledge Distillation for NMT", "text": "The large size of neural machine translation systems makes them an ideal candidate for knowledge distillation approaches. In this section, we examine three different ways in which this technique can be applied to NMT."}, {"heading": "3.1 Word-Level Knowledge Distillation", "text": "NMT systems are trained directly to minimize the word NLL, LWORD-NLL, at any point. So, if we have a teacher model, standard knowledge distillation can be applied to the multi-level cross entropy. We define this distillation for one sentence as, LWORD-KD = \u2212 J-J-J = 1 | V-K = 1 q (tj = k | s, t < j) \u00b7 log p (tj = k | s, t < j) and the student can be trained to optimize the mix of LWORD-KD and LWORD-NLL. In the context of NMT, we call this approach word-based knowledge distillation and illustrate this in Figure 1 (left)."}, {"heading": "3.2 Sequence-Level Knowledge Distillation", "text": "The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"Why?\" The question of \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The answer \"The answer\" The answer \"The answer\" The answer \"The answer\" The question \"The question\" The question The question \"The question The question\" The question The question The question \"The question The question The question The question The question The question The question\" The question The question The question The question The question The question The question \"The question The question The question The question The question The question The question The question The question The question The question\" The question The question The question The question The question The question The question The question The question The question The question The question \"The question The question The question The question The question The question The question The question The question The question The question\" The question The question The question The question The question The question The question The question The question The question \"The question The question The question The question The question The question\" The question The question The question The question The question The question \"The question The question The question The question The question The question\" The question The question The question The question The question The question The question The question The question The question The question The question The question The question The question The question The question The question \"Why."}, {"heading": "3.3 Sequence-Level Interpolation", "text": "Next, we consider integrating the training data back into the process, so that we see the student model as a mixture of our sequence level (LSEQ-KD) with the original training data (LSEQ-NLL), L = (1 \u2212 \u03b1) LSEQ-NLL + (LSEQ-KD = (1 \u2212 \u03b1) Log (1 \u2212) Log (s) Log (t) Log (t) Log (t), where the gold target sequence is. Since the second term is intractable, we could reapply the mode approximation from the previous section, L = \u2212 (1 \u2212) Log (y) Log (y) p (y) and pull on both observed (y) and observed (y) data generated. However, this process is noniideal for two reasons: (1) unlike the standard knowledge distribution, it requires a doubling of training data (2)."}, {"heading": "4 Experimental Setup", "text": "To test these approaches, we perform two sets of NMT experiments: High Resource (English \u2192 German) and Low Resource (Thai \u2192 English). The English-German data is from WMT 2014.6 The training set has 4m sets and we take newstest2012 / newstest2013 as a dev set and newstest2014 as a test set. We keep the top 50k most common words and replace the rest with UNK. The teacher model is a 4 \u00d7 1000 LSTM (as in Luong et al. (2015) and we train two student models: 2 \u00d7 300 and 2 \u00d7 500. The Thai-English data is from IWSLT 2015.7 There are 90k sets in the training set and we take the data as a dev set and 2012 / 2013 as a test set. We take the top 25k most common words. Size of the teacher model is 2 \u00d7 500 (the better than 4 \u00d7 1000, 3 \u00d7 750, 2 \u00d7 750 models)."}, {"heading": "5 Results and Discussion", "text": "The results of this study can be found in Table 1, whereas it is able to rely on the foundations, in the form in which it is able, in the form in which it is able, in the condition in which it is able, in the condition in which it is able, in the condition in which it is able, in the condition in which it is able, in the condition in which it is able, in the condition in which it is able, in the condition in which it is able, in the condition in which it is able, in the condition in which it is able to remain in the world."}, {"heading": "5.1 Decoding Speed", "text": "The computational complexity of beam search grows linearly with the beam size. Therefore, the fact that distillation of knowledge at the sequence level enables greedy decoding is of considerable importance, which has practical implications for the operation of NMT systems on different devices. To test the speed gains, we run the teacher-student models on GPU, CPU, and cell phone and verify the average number of translated source words per second (Table 2). We use a GeForce GTX Titan X for the GPU and a Samsung Galaxy 6 smartphone. We find that we can run the student model with greedy decoding ten times faster than the teacher model with beam search (1051.3 vs 101.9 words / sec), without major performance losses."}, {"heading": "5.2 Further Observations", "text": "We report on some other experiments and observations: \u2022 We have tried to develop very small student models (1 x 100, 2 x 50) and found that performance has deteriorated significantly compared to the teacher model. \u2022 For models trained with the distillation of knowledge at the word level, we have also tried to trace the top hidden layer of the student network at each step back to the top hidden layer of the teacher network, noting that Romero et al. (2015) made improvements using a similar technique in advance models. We found that this yields comparable results in standard knowledge distillation and therefore did not pursue this further. \u2022 The number of parameters for the student models (Table 1: parameters) is still somewhat large, as the majority of parameters are dominated by the embedding of word embedding that scale with the embedding size (compared to RNN parameters that are square with the hidden dimension size)."}, {"heading": "6 Conclusion", "text": "In this paper, we have examined existing methods of knowledge distillation for NMT (which work at the word level) and introduced two sequential variants of knowledge distillation. We show that our method is effective and offers improvements over the base model without any knowledge distillation, as well as over models trained with word-based knowledge distillation. We observe that the methods are complementary and lead to further improvements. We also note that models trained with sequence-based knowledge distillation do not seem to require beam search during inference, which has significant practical implications for the use of NMT systems in the wild. Our method can even be used to improve the original teacher model, and likewise eliminates the need for beam search with the teacher model. We have chosen to focus on translation, as this domain usually requires the greatest capacity of in-depth learning models, but sequence-sequence-sequence-pillar models are successfully applied."}], "references": [{"title": "Do Deep Nets Really Need to be Deep", "author": ["Ba", "Caruana2014] Lei Jimma Ba", "Rich Caruana"], "venue": "In Proceedings of NIPS", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU", "author": ["Chen", "Cherry2014] Boxing Chen", "Colin Cherry"], "venue": "In Proceedings of the Ninth Workshop on Statistical Machine Translation", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Compressing Neural Networks with the Hashing Trick", "author": ["Chen et al.2015] Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "Proceedings of ICML", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Hope and Fear for Discriminative Training of Statistical Translation Models", "author": ["David Chiang"], "venue": "In JMLR", "citeRegEx": "Chiang.,? \\Q2012\\E", "shortCiteRegEx": "Chiang.", "year": 2012}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Character-based Neural Machine Translation", "author": ["Costa-Jussa", "Jose A.R. Fonollosa"], "venue": null, "citeRegEx": "Costa.Jussa et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Costa.Jussa et al\\.", "year": 2016}, {"title": "Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained", "author": ["Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": null, "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Predicting Parameters in Deep Learning", "author": ["Denil et al.2013] Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc\u2019Aurelio Ranzato", "Nando de Freitas"], "venue": "In Proceedings of NIPS", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Multilingual Language Processing from Bytes", "author": ["Gillick et al.2016] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": "In Proceedings of NAACL", "citeRegEx": "Gillick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2016}, {"title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding", "author": ["Han et al.2016] Song Han", "Huizi Mao", "William J. Dally"], "venue": "Proceedings of ICLR", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Hassibi", "Stork1993] Babak Hassibi", "David G. Stork"], "venue": "In Proceedings of NIPS", "citeRegEx": "Hassibi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Reshaping Deep Neural Network for Fast Decoding by Node-Pruning", "author": ["He et al.2014] Tianxing He", "Yuchen Fan", "Yanmin Qian", "Tian Tan", "Kai Yu"], "venue": "Proceedings of ICASSP", "citeRegEx": "He et al\\.,? \\Q2014\\E", "shortCiteRegEx": "He et al\\.", "year": 2014}, {"title": "Distilling the Knowledge in a Neural Network", "author": ["Oriol Vinyals", "Jeff Deam"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u2019\u0301urgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Exploring the Limits of Language Modeling", "author": ["Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": null, "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Recurrent Continuous Translation Models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Character-Aware Neural Language Models", "author": ["Kim et al.2016] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": "In Proceedings of AAAI", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Optimal Brain Damage", "author": ["LeCun et al.1990] Yann LeCun", "John S. Denker", "Sara A. Solla"], "venue": "In Proceedings of NIPS", "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "Learning Small-Size DNN with Output-Distribution-Based Criteria", "author": ["Li et al.2014] Jinyu Li", "Rui Zhao", "Jui-Ting Huang", "Yifan Gong"], "venue": "In Proceedings of INTERSPEECH", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "A DiversityPromoting Objective Function for Neural Conversational Models", "author": ["Li et al.2016] Jiwei Li", "Michael Galley", "Chris Brockett", "Jianfeg Gao", "Bill Dolan"], "venue": "In Proceedings of NAACL 2016", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "An End-toEnd Discriminative Approach to Machine Translation", "author": ["Liang et al.2006] Percy Liang", "Alexandre BouchardCote", "Dan Klein", "Ben Taskar"], "venue": "In Proceedings of COLING-ACL", "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Finding Function in Form: Composition Character Models for Open Vocabulary Word Representation", "author": ["Ling et al.2015a] Wang Ling", "Tiago Lui", "Luis Marujo", "Ramon Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Effective Approaches to Attention-based Neural Machine Translation", "author": ["Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of ACL", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distilling Word Embeddings: An Encoding Approach. arXiv:1506.04488", "author": ["Mou et al.2015] Lili Mou", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": null, "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["Slim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of ICML", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "FitNets: Hints for Thin Deep Nets", "author": ["Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio"], "venue": "Proceedings of ICLR", "citeRegEx": "Romero et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2015}, {"title": "A Neural Attention Model for Abstractive Sentence Summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models", "author": ["Allesandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "Proceedings of AAAI", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Unsupervised Learning of Video Representations using LSTMs", "author": ["Elman Mansimov", "Ruslan Salakhutdinov"], "venue": "Proceedings of ICML", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Oriol Vinyals", "Quoc Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a Foreign Language", "author": ["Lukasz Kaiser", "Terry Koo", "Slave Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "2015b. Show and Tell: A Neural Image Caption Generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimma Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "Proceedings of ICML", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "author": ["Zhou et al.2016] Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu"], "venue": "Proceedings of TACL", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT.", "startOffset": 69, "endOffset": 111}, {"referenceID": 5, "context": "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) is a deep learning-based method for translation that has recently shown promising results as an alternative to statistical approaches.", "startOffset": 33, "endOffset": 107}, {"referenceID": 30, "context": "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) is a deep learning-based method for translation that has recently shown promising results as an alternative to statistical approaches.", "startOffset": 33, "endOffset": 107}, {"referenceID": 30, "context": "While both simple and surprisingly accurate, NMT systems typically need to be very high capacity in order to perform well: Sutskever et al. (2014) used a 4-layer LSTM with 1000 hidden units per layer (herein 4 \u00d7 1000) and Zhou et al.", "startOffset": 123, "endOffset": 147}, {"referenceID": 30, "context": "While both simple and surprisingly accurate, NMT systems typically need to be very high capacity in order to perform well: Sutskever et al. (2014) used a 4-layer LSTM with 1000 hidden units per layer (herein 4 \u00d7 1000) and Zhou et al. (2016) obtained state-of-the-art results on English \u2192 French with a 16-layer LSTM with 512 units per layer.", "startOffset": 123, "endOffset": 241}, {"referenceID": 8, "context": "Researchers have particularly noted that large networks seem to be necessary for training, but learn redundant representations in the process (Denil et al., 2013).", "startOffset": 142, "endOffset": 162}, {"referenceID": 18, "context": "1 Pruning methods (LeCun et al., 1990; Hassibi and Stork, 1993; He et al., 2014; Han et al., 2016; Mariet and Sra, 2016), zero-out weights based", "startOffset": 18, "endOffset": 120}, {"referenceID": 12, "context": "1 Pruning methods (LeCun et al., 1990; Hassibi and Stork, 1993; He et al., 2014; Han et al., 2016; Mariet and Sra, 2016), zero-out weights based", "startOffset": 18, "endOffset": 120}, {"referenceID": 10, "context": "1 Pruning methods (LeCun et al., 1990; Hassibi and Stork, 1993; He et al., 2014; Han et al., 2016; Mariet and Sra, 2016), zero-out weights based", "startOffset": 18, "endOffset": 120}, {"referenceID": 3, "context": "There has also been work on reducing the memory footprint of models through quantization of weights (Chen et al., 2015; Han et al., 2016; Courbariaux et al., 2016) as a postprocessing step.", "startOffset": 100, "endOffset": 163}, {"referenceID": 10, "context": "There has also been work on reducing the memory footprint of models through quantization of weights (Chen et al., 2015; Han et al., 2016; Courbariaux et al., 2016) as a postprocessing step.", "startOffset": 100, "endOffset": 163}, {"referenceID": 7, "context": "There has also been work on reducing the memory footprint of models through quantization of weights (Chen et al., 2015; Han et al., 2016; Courbariaux et al., 2016) as a postprocessing step.", "startOffset": 100, "endOffset": 163}, {"referenceID": 19, "context": "Knowledge distillation approaches (Bucila et al., 2006; Ba and Caruana, 2014; Li et al., 2014; Hinton et al., 2015; Romero et al., 2015; Mou et al., 2015) learn a smaller student network to mimic the original teacher network by minimizing the loss (typically L2 or cross-entropy) between the student and teacher output.", "startOffset": 34, "endOffset": 154}, {"referenceID": 13, "context": "Knowledge distillation approaches (Bucila et al., 2006; Ba and Caruana, 2014; Li et al., 2014; Hinton et al., 2015; Romero et al., 2015; Mou et al., 2015) learn a smaller student network to mimic the original teacher network by minimizing the loss (typically L2 or cross-entropy) between the student and teacher output.", "startOffset": 34, "endOffset": 154}, {"referenceID": 26, "context": "Knowledge distillation approaches (Bucila et al., 2006; Ba and Caruana, 2014; Li et al., 2014; Hinton et al., 2015; Romero et al., 2015; Mou et al., 2015) learn a smaller student network to mimic the original teacher network by minimizing the loss (typically L2 or cross-entropy) between the student and teacher output.", "startOffset": 34, "endOffset": 154}, {"referenceID": 24, "context": "Knowledge distillation approaches (Bucila et al., 2006; Ba and Caruana, 2014; Li et al., 2014; Hinton et al., 2015; Romero et al., 2015; Mou et al., 2015) learn a smaller student network to mimic the original teacher network by minimizing the loss (typically L2 or cross-entropy) between the student and teacher output.", "startOffset": 34, "endOffset": 154}, {"referenceID": 16, "context": "on an importance criterion: LeCun et al. (1990) use (a diagonal approximation to) the Hessian to identify weights whose removal minimally impact the objective function, while Han et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 10, "context": "(1990) use (a diagonal approximation to) the Hessian to identify weights whose removal minimally impact the objective function, while Han et al. (2016) remove weights based on thresholding their absolute values.", "startOffset": 134, "endOffset": 152}, {"referenceID": 23, "context": "We begin by describing our baseline NMT model, which was proposed by Luong et al. (2015) and achieved state-of-the-art results on English \u2192 German translation.", "startOffset": 69, "endOffset": 89}, {"referenceID": 1, "context": "Encoder In attention-based models (Bahdanau et al., 2015; Luong et al., 2015), the encoder reads the source sentence and outputs a sequence of vectors (one vector for each time step) to be attended to during decoding.", "startOffset": 34, "endOffset": 77}, {"referenceID": 23, "context": "Encoder In attention-based models (Bahdanau et al., 2015; Luong et al., 2015), the encoder reads the source sentence and outputs a sequence of vectors (one vector for each time step) to be attended to during decoding.", "startOffset": 34, "endOffset": 77}, {"referenceID": 23, "context": "Finally, as in Luong et al. (2015) we feed cj as additional input to the decoder for the next time step by concatenating it with xj , so the decoder equation is modified to,", "startOffset": 15, "endOffset": 35}, {"referenceID": 19, "context": "For classification this usually means matching the probabilities either via L2 on the log scale (Ba and Caruana, 2014) or by cross-entropy (Li et al., 2014; Hinton et al., 2015).", "startOffset": 139, "endOffset": 177}, {"referenceID": 13, "context": "For classification this usually means matching the probabilities either via L2 on the log scale (Ba and Caruana, 2014) or by cross-entropy (Li et al., 2014; Hinton et al., 2015).", "startOffset": 139, "endOffset": 177}, {"referenceID": 13, "context": "similarity between classes) and has less variance in gradients (Hinton et al., 2015).", "startOffset": 63, "endOffset": 84}, {"referenceID": 21, "context": "This approach is inspired by local updating (Liang et al., 2006) and hope/fear training (Chiang, 2012), which are commonly-used methods for discriminative training in statistical machine translation (although to our knowledge not for knowledge distillation).", "startOffset": 44, "endOffset": 64}, {"referenceID": 4, "context": ", 2006) and hope/fear training (Chiang, 2012), which are commonly-used methods for discriminative training in statistical machine translation (although to our knowledge not for knowledge distillation).", "startOffset": 31, "endOffset": 45}, {"referenceID": 25, "context": "Jaccard similarity or BLEU (Papineni et al., 2002)).", "startOffset": 27, "endOffset": 50}, {"referenceID": 23, "context": "The teacher model is a 4 \u00d7 1000 LSTM (as in Luong et al. (2015)) and we train two student models: 2\u00d7 300 and 2\u00d7 500.", "startOffset": 44, "endOffset": 64}, {"referenceID": 21, "context": "Difficulty of training on reference translations has also been observed in discriminative statistical machine translation (Liang et al., 2006)", "startOffset": 122, "endOffset": 142}, {"referenceID": 26, "context": "\u2022 For models trained with word-level knowledge distillation, we also tried regressing the student network\u2019s top-most hidden layer at each time step to the teacher network\u2019s top-most hidden layer as a pretraining step, noting that Romero et al. (2015) obtained improvements with a similar technique on feed-forward models.", "startOffset": 230, "endOffset": 251}, {"referenceID": 17, "context": "There have been promising recent results on eliminating word embeddings completely and obtaining word representations directly from characters with character composition models, which have many fewer parameters than word embedding lookup tables (Ling et al., 2015a; Kim et al., 2016; Ling et al., 2015b; Jozefowicz et al., 2016; Costa-Jussa and Fonollosa, 2016).", "startOffset": 245, "endOffset": 361}, {"referenceID": 15, "context": "There have been promising recent results on eliminating word embeddings completely and obtaining word representations directly from characters with character composition models, which have many fewer parameters than word embedding lookup tables (Ling et al., 2015a; Kim et al., 2016; Ling et al., 2015b; Jozefowicz et al., 2016; Costa-Jussa and Fonollosa, 2016).", "startOffset": 245, "endOffset": 361}, {"referenceID": 15, "context": ", 2015b; Jozefowicz et al., 2016; Costa-Jussa and Fonollosa, 2016). For example Jozefowicz et al. (2016) compress a state-of-the-art language model by a factor of 20 by replacing input/output word embeddings with a character model.", "startOffset": 9, "endOffset": 105}, {"referenceID": 27, "context": ", 2015a), summarization (Rush et al., 2015), dialogue (Vinyals and Le, 2015; Serban et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 28, "context": ", 2015), dialogue (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016), NER/POS-tagging (Gillick et al.", "startOffset": 18, "endOffset": 78}, {"referenceID": 20, "context": ", 2015), dialogue (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016), NER/POS-tagging (Gillick et al.", "startOffset": 18, "endOffset": 78}, {"referenceID": 9, "context": ", 2016), NER/POS-tagging (Gillick et al., 2016), image captioning (Vinyals et al.", "startOffset": 25, "endOffset": 47}, {"referenceID": 33, "context": ", 2016), image captioning (Vinyals et al., 2015b; Xu et al., 2015), and video generation (Srivastava et al.", "startOffset": 26, "endOffset": 66}, {"referenceID": 29, "context": ", 2015), and video generation (Srivastava et al., 2015).", "startOffset": 30, "endOffset": 55}], "year": 2016, "abstractText": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with only a decrease of 0.2 BLEU. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search.", "creator": "LaTeX with hyperref package"}}}