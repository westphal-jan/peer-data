{"id": "1606.08359", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Lifted Rule Injection for Relation Embeddings", "abstract": "Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.", "histories": [["v1", "Mon, 27 Jun 2016 16:39:23 GMT  (292kb,D)", "http://arxiv.org/abs/1606.08359v1", null], ["v2", "Fri, 23 Sep 2016 20:40:25 GMT  (294kb,D)", "http://arxiv.org/abs/1606.08359v2", "Camera-ready version for EMNLP 2016 Conference"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL", "authors": ["thomas demeester", "tim rockt\u00e4schel", "sebastian riedel"], "accepted": true, "id": "1606.08359"}, "pdf": {"name": "1606.08359.pdf", "metadata": {"source": "CRF", "title": "Lifted Rule Injection for Relation Embeddings", "authors": ["Thomas Demeester", "Tim Rockt\u00e4schel", "Sebastian Riedel"], "emails": ["tdmeeste@intec.ugent.be", "t.rocktaschel@cs.ucl.ac.uk", "s.riedel@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them are able to abide by the rules that they have given themselves. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. \"(...)"}, {"heading": "2 Background", "text": "In this section, we look at the matrix factorization relationship model developed by Riedel et al. (2013) and present the notation used in the work. We choose the matrix factorization model for its simplicity as the basis on which we develop the implication injection. Riedel et al. (2013) represent each relation r \u00b2 R (selected from Freebase et al., 2008) or extract it as a textual surface pattern) by a dimensional latent representation r \u00b2 Rk. A certain relation or fact is the combination of a relation r and a tuple of entities involved in this relationship and those as < r, t >. We write O as a set of all such input facts available for the training. In addition, each entity tuple t \u00b2 T is represented by a latent vector t \u00b2 Rk (with T representing the set of all entity tuples in O).F model of Riedel's respective product (2013)."}, {"heading": "3 Lifted Injection of Implications", "text": "In this section, we will show how an implication such as T: < rp, t > \u21d2 < rq, t >, (4) can be imposed independently of the entity tuples. For simplicity, we will abbreviate implications such as rp \u21d2 rq (e.g. professorAt \u21d2 emploeAt)."}, {"heading": "3.1 Grounded Loss Formulation", "text": "The implication rule can be imposed by requiring that each tuple t-T is at least as compatible with relation rp as with rq. If < rp, t > is a true fact with a high score r > p t and the fact < rq, t > has an even higher score, it must also be true, but not vice versa. We can therefore introduce an implication rule by minimizing both sides of the above inequality by a separate contribution from each t-T and adding the total loss if the corresponding inequality is not satisfied. To make the contribution of each tuple t to that loss regardless of the magnitude of the tuple embedding, we divide both sides of the above inequality by a separate contribution, adding the total loss if the corresponding inequality is not satisfied."}, {"heading": "3.2 Lifted Loss Formulation", "text": "The problems mentioned above can be avoided by minimizing a tuple-independent upper limit q instead of LI. Such a limit can be constructed, provided that all components of t are limited to a non-negative embedding space, i.e., T Rk, +. If this is true, Jensen's inequality allows us to transform eq. (6) As follows LI = \u2211 T'I (k i = 1 t i [rp \u2212 rq] > 1i) (7) \u2264 k i = 1I ([rp \u2212 rq] > 1i)."}, {"heading": "3.3 Approximately Boolean Entity Tuples", "text": "In order to impose implications by minimizing an increased loss of LUI, the tuple embedding space must be limited to Rk, +. (Kruszewski et al., 2015) The tuple embedding space is constructed even more strongly than required, namely to the hypercube t-0, 1] k, than, say, Boolean embedding (Kruszewski et al., 2015). The tuple embedding is constructed from real vectors e, with the component-by-component sigmoid function t = \u03c3 (e), e-Rk. (12) To minimize the loss, the gradients are therefore calculated with respect to e, and the L2 regulation is applied to the components of e instead of t.Other options to ensure the constraint t-0 in eq. (11) are possible, but we have found that our approach works better in practice than that (e.g. the exponential transformation proposed by Demeester et al (2016) <)."}, {"heading": "3.4 Convex Implication Loss", "text": "The logistical loss \"R\" (see \u00a7 2) is not suitable to enforce implications, because once the inequality in equivalents (11) is satisfied, the components of rp and rq need not be separated any further. However, with \"R this would continue to happen due to the low imbalance between zero and zero. In the reconstruction loss LR, this is a desirable effect that further separates the values for positive and negative examples. However, if an implication is imposed between two relationships, which according to the training data are almost equivalent, we still want to find almost equivalent embedding vectors. Therefore, we propose to use the loss I = max (0, s + \u03b4) (15) with a small positive margin to ensure that the gradient does not disappear before the inequality is actually satisfied. We use the reverse for all experiments = 0.01."}, {"heading": "4 Related Work", "text": "In recent years, it has been shown that different types of rules can be incorporated using an integer linear programming approach. Rockta \u00bc schel et al. (2015) provided a framework for collectively maximizing the probability of observed facts and logical rules. These approaches establish the rules in the training data and limit their scalability towards large rule sets and KBs with many entities. As argued in the introduction, this is an important motivation for the current work. Wu et al. (2015) proposed to use a path ranking approach to capture long-term interactions between entities, and to add it as an additional loss concept that models relationship by pair. Our FSL model differs substantially from their approach by looking at tuples instead of separate entities, and we inject a predetermined set of rules."}, {"heading": "5 Experiments and Results", "text": "Before turning to the injection of rules, we compare the model F with the model FS and show that the limitation of the embedding space in the tuple has a regularizing effect, instead of limiting the meaningfulness of the model (\u00a7 5.1). We then show that the model FSL is capable of zero-shot learning (\u00a7 5.2), and show that the injection of high-quality WordNet rules leads to improved precision (\u00a7 5.3). We continue with a visual illustration of the embedding with and without injected rules (\u00a7 5.4), provide details on the time efficiency of the method of injecting upscale rules (\u00a7 5.5), and show that it correctly detects the asymmetry of the implication rules (\u00a7 5.6). All models were implemented in TensorFlow (Abadi et al., 2015). We use the hyperparameters of Riedel et (100) with 0.000k \u2212 0.000k (2013) and 0.000k = 1 loss."}, {"heading": "5.1 Restricted Embedding Space", "text": "Before incorporating external sound knowledge into relation representations, we were curious to see how much we were losing by limiting the entity tupel space to approximately Boolean embedding. We evaluated our models on the data set introduced by Riedel et al. (2013) in The New York Times. Surprisingly, we found that the expressivity of the model does not suffer from this strong constraint. Table 1 shows that the constraint of the tupel embedding space seems to work slightly better (FS) than a newly evaluated tupel embedding space (F), suggesting that this constraint has a regulating effect that improves generalization. We also provide the original results for Model F by Riedel et al. (2013) (referred to as R13-F) for comparison. Due to a different implementation and optimization process, the results for our Model F and R13-F are not identical."}, {"heading": "5.2 Zero-shot Learning", "text": "The zero-shot learning experiment conducted in Rockta \u00bc schel et al. (2015) leads to an important finding: when injecting right-hand side implications for freebase relationships for which no or very limited training factors are available, the model should be able to deduce the validity of freebase facts for these relationships based on rules and correlations between textual surface patterns. We inject the same hand-picked relationships as those of Rockta \u00bc schel et al. (2015), after removing all freebase training factors. The reversed rule injection (FSL model) achieves a weighted MAP of 0.35, comparable to 0.38 by the joint model of Rockta \u00bc schel et al. (2015) (led by R15-Joint)."}, {"heading": "5.3 Injecting Knowledge from WordNet", "text": "We use WordNet hypernyms to generate rules for the NYT dataset. To this end, we iterate over all the surface shape patterns in the dataset and try to replace words in the pattern with their hypernyms. If the resulting pattern is included in the dataset, we create the corresponding rule. For example, we create a rule appos- > Diplomate- > Amod \u21d2 Appos- > Offices- > Amod, because both patterns are included in the NYT dataset and we know from WordNet that a diplomat is an official. This results in 427 rules from WordNet, which we then comment on manually to obtain 36 high-quality rules. Note that none of these rules directly implies a freebase relationship. Although the test relationships are all from the freebase, we still hope for improvements through transitive effects, i.e., better surface shape representations, which in turn help to achieve logical predictions. These freebase results are partially weighted by W1 application."}, {"heading": "5.4 Visualizing Relation Embeddings", "text": "Figures 2a and 2b show the difference between model F (without injected rules) and FSL (with rules). Values of embedding in model FSL are more polarized, i.e. we observe stronger negative or positive components than in model F. In addition, FSL also shows a clearer difference between the most left (mostly negative, more specific) and most right (mostly positive, more general) embedding (i.e. a clearer separation between positive and negative values in the diagram), which results from the sequence relationship being imposed in Equation (11) when implications are injected."}, {"heading": "5.5 Efficiency of Lifted Injection of Rules", "text": "To get an idea of the time efficiency of rule injection, we measure the time per epoch when we limit program execution to a single 2.4GHz CPU core. We measure an average of 6.33s per epoch without rules (model FS), compared to 6.76s and 6.97s when injecting the 36 high-quality WordNet rules or the unfiltered 427 rules (model FSL). Increasing the number of injected rules from 36 to 427 results in an increase in computing time of only 3%, although all rule losses are used in each training batch in our setup, confirming the high efficiency of our upscale rule injection method."}, {"heading": "5.6 Asymmetric Character of Implications", "text": "To show that injecting implications maintains their asymmetrical character, we conduct the following experiment: After incorporating high-quality WordNet rules rp \u21d2 rq into the FSL model, we select all tuples tp that occur with implications < rp, tp >. Assigning these values to relation rq should result in high values for the values r > q tp if the implication holds. However, if the tuples tq are selected from the training facts < rq, tq >, and matched with relation rp, the values r > p tq should be much lower if the reverse implication does not hold (in other words, if rq and rp are not equivalent). Table 3 lists the averaged results for 5 example rules, and the average for all relationships in WordNet rules, both in the case of injected rules (model FSL) and without rules (model FS)."}, {"heading": "6 Conclusions", "text": "We presented a new, rapid approach to incorporating first-order implication rules into distributed representations of relationships. We called our approach a \"superseded rule injection\" because it avoids the costly grounding of first-order implication rules and is therefore independent of the size of the range of units. By constructing it, these rules are met for any observed or unobserved fact. The approach presented requires a limitation of the embedding space of units. However, experiments on a real data set show that this does not affect the meaningfulness of the representations learned; on the contrary, it appears to have a beneficial regulating effect. By incorporating rules generated from WordNet hypernyms, our model improved over a matrix factoring basis for completing the knowledge base, especially in areas where annotations are costly and only small amounts of training facts are available, our approach provides a way to leverage external sources of knowledge for the follow-up."}], "references": [{"title": "Tim Sturge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh"], "venue": "and Jamie Taylor.", "citeRegEx": "Bollacker et al.2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Christopher Potts", "author": ["Samuel R Bowman"], "venue": "and Christopher D Manning.", "citeRegEx": "Bowman et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Lifted Firstorder Probabilistic Inference", "author": ["Rodrigo De Salvo Braz"], "venue": "Ph.D. thesis,", "citeRegEx": "Braz.,? \\Q2007\\E", "shortCiteRegEx": "Braz.", "year": 2007}, {"title": "Bishan Yang", "author": ["Kai-Wei Chang", "Wen-tau Yih"], "venue": "and Christopher Meek.", "citeRegEx": "Chang et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "W", "author": ["William"], "venue": "Cohen.", "citeRegEx": "Cohen2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Tim Rockt\u00e4schel", "author": ["Thomas Demeester"], "venue": "and Sebastian Riedel.", "citeRegEx": "Demeester et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Eduard Hovy", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K Jauhar", "Chris Dyer"], "venue": "and Noah A Smith.", "citeRegEx": "Faruqui et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Benjamin Van Durme", "author": ["Juri Ganitkevitch"], "venue": "and Chris Callison-Burch.", "citeRegEx": "Ganitkevitch et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Eduard Hovy", "author": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu"], "venue": "and Eric Xing.", "citeRegEx": "Hu et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Denis Paperno", "author": ["German Kruszewski"], "venue": "and Marco Baroni.", "citeRegEx": "Kruszewski et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Benjamin Roth", "author": ["Arvind Neelakantan"], "venue": "and Andrew McCallum.", "citeRegEx": "Neelakantan et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Volker Tresp", "author": ["Maximilian Nickel"], "venue": "and Hans-Peter Kriegel.", "citeRegEx": "Nickel et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Volker Tresp", "author": ["Maximilian Nickel", "Kevin Murphy"], "venue": "and Evgeniy Gabrilovich.", "citeRegEx": "Nickel et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "First-order probabilistic inference", "author": ["David Poole"], "venue": "In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Poole.,? \\Q2003\\E", "shortCiteRegEx": "Poole.", "year": 2003}, {"title": "Zeno Gantner", "author": ["Steffen Rendle", "Christoph Freudenthaler"], "venue": "and Lars Schmidt-Thieme.", "citeRegEx": "Rendle et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Andrew McCallum", "author": ["Sebastian Riedel", "Limin Yao"], "venue": "and Benjamin M Marlin.", "citeRegEx": "Riedel et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning knowledge base inference with neural theorem provers", "author": ["Rockt\u00e4schel", "Riedel2016] Tim Rockt\u00e4schel", "Sebastian Riedel"], "venue": "In NAACL Workshop on Automated Knowledge Base Construction (AKBC)", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "Sameer Singh", "author": ["Tim Rockt\u00e4schel", "Matko Bosnjak"], "venue": "and Sebastian Riedel.", "citeRegEx": "Rockt\u00e4schel et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Sameer Singh", "author": ["Tim Rockt\u00e4schel"], "venue": "and Sebastian Riedel.", "citeRegEx": "Rockt\u00e4schel et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Christopher D Manning", "author": ["Richard Socher", "Danqi Chen"], "venue": "and Andrew Ng.", "citeRegEx": "Socher et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Pallavi Choudhury", "author": ["Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon"], "venue": "and Michael Gamon.", "citeRegEx": "Toutanova et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sanja Fidler", "author": ["Ivan Vendrov", "Ryan Kiros"], "venue": "and Raquel Urtasun.", "citeRegEx": "Vendrov et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Row-less universal schema", "author": ["Verga", "McCallum2016] Patrick Verga", "Andrew McCallum"], "venue": null, "citeRegEx": "Verga et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Verga et al\\.", "year": 2016}, {"title": "Benjamin Roth", "author": ["Patrick Verga", "David Belanger", "Emma Strubell"], "venue": "and Andrew McCallum.", "citeRegEx": "Verga et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Kathryn Mazaitis", "author": ["William Yang Wang"], "venue": "and William W Cohen.", "citeRegEx": "Wang et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Bin Wang", "author": ["Quan Wang"], "venue": "and Li Guo.", "citeRegEx": "Wang et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Zhongfei Zhang", "author": ["Fei Wu", "Jun Song", "Yi Yang", "Xi Li"], "venue": "and Yueting Zhuang.", "citeRegEx": "Wu et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.", "creator": "LaTeX with hyperref package"}}}