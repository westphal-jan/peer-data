{"id": "1606.08425", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Predicting the Relative Difficulty of Single Sentences With and Without Surrounding Context", "abstract": "The problem of accurately predicting relative reading difficulty across a set of sentences arises in a number of important natural language applications, such as finding and curating effective usage examples for intelligent language tutoring systems. Yet while significant research has explored document- and passage-level reading difficulty, the special challenges involved in assessing aspects of readability for single sentences have received much less attention, particularly when considering the role of surrounding passages. We introduce and evaluate a novel approach for estimating the relative reading difficulty of a set of sentences, with and without surrounding context. Using different sets of lexical and grammatical features, we explore models for predicting pairwise relative difficulty using logistic regression, and examine rankings generated by aggregating pairwise difficulty labels using a Bayesian rating system to form a final ranking. We also compare rankings derived for sentences assessed with and without context, and find that contextual features can help predict differences in relative difficulty judgments across these two conditions.", "histories": [["v1", "Mon, 27 Jun 2016 19:48:40 GMT  (149kb)", "https://arxiv.org/abs/1606.08425v1", null], ["v2", "Wed, 29 Jun 2016 14:54:57 GMT  (33kb)", "http://arxiv.org/abs/1606.08425v2", null], ["v3", "Tue, 25 Oct 2016 00:11:06 GMT  (41kb,D)", "http://arxiv.org/abs/1606.08425v3", "EMNLP 2016 Long Paper"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["elliot schumacher", "maxine esk\u00e9nazi", "gwen a frishkoff", "kevyn collins-thompson"], "accepted": true, "id": "1606.08425"}, "pdf": {"name": "1606.08425.pdf", "metadata": {"source": "CRF", "title": "Predicting the Relative Difficulty of Single Sentences With and Without Surrounding Context", "authors": ["Elliot Schumacher", "Maxine Eskenazi", "Gwen Frishkoff"], "emails": ["eschumac@cs.cmu.edu", "max@cs.cmu.edu", "gfrishkoff@gmail.com", "kevynct@umich.edu"], "sections": [{"heading": null, "text": "The problem of accurately predicting relative reading difficulties within a group of sentences occurs in a number of important applications of natural language, such as searching for and curating effective examples of the use of intelligent speech support systems. However, while significant research has examined the difficulty of reading documents and passages, the particular challenges in assessing aspects of the legibility of individual sentences have received much less attention, especially when it comes to the role of surrounding passages. We are introducing and evaluating a novel approach to estimating the relative reading difficulty of a set of sentences, with and without context. Using various lexical and grammatical characteristics, we are examining models for predicting paired relative difficulties using logistic regression and are examining rankings generated by aggregating pairs of difficulty markers using a Bayesian rating system to form a definitive ranking. We are also comparing those for sentences evaluated with and without context, and finding that relative difficulty markers can contribute to predicting contextual differences within the two."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 Related Work", "text": "The Lexile Framework (Stenner, 1996) uses word frequency estimates in a large corpus as a proxy for lexical difficulty and sentence length as a grammatical feature. Methods based on statistical machine learning, such as the measures of reading difficulty developed by Collins-Thompson and Callan for other texts (CollinsThompson and Callan, 2004) and (Swarm and Ostendorf, 2005) used features based on language models. Later work (Heilman et al., 2008) integrated grammatical features by analyzing the sentences in a text and creating sub-trees of one- to three-level depth as separate features. Such features allow for more detailed, direct analysis of the sentence structure itself instead of traditional proxies of syntactic complexity like sentence length. The linguistic features proposed in these works capture specific aspects of language difficulty applied at the document level while our work examines the effectiveness of these features."}, {"heading": "3 Data Collection and Processing", "text": "We will now describe methods that are used to create our set data set, to collect pair-by-pair assessments of difficulty, and to summarize these pair-by-pair preferences into a complete ranking."}, {"heading": "3.1 Data Set", "text": "The sentences were drawn from a corpus that combines the American National Corpus (Reppen et al., 2005), the New York Times Corpus (Sandhaus, 2008), and the North American News Text Corpus (McClosky et al., 2008). The domain of this corpora is largely news text, but also other topics, such as travel guides and other issues. In total, this database contains 60,663,803 sentences that serve as starting candidates. Sentences were selected by the third author for a study of the teaching vocabulary."}, {"heading": "3.2 Crowdsourcing", "text": "Both of these tasks were performed on the Crowdflower platform. Workers were first given instructions for each task, which included a description of the general purpose of the task. In the one-sentence task, workers were asked to choose which of the two sentences was more difficult. In the \"sentence-within-sentence\" task, workers were similarly asked to decide which sentence was more difficult. Instructions for the latter required workers to judge only the sentence, not the entire context. In both tasks, there was an option for \"I don't know or can't decide.\" Workers were asked to make their decision based on the vocabulary and grammatical structure of the sentences. Finally, examples of each task were provided with explanations for each answer. For each task, at least 40 gold standard questions were created from sentence pairs, which were judged to be sufficiently distinct from each other, so that they could easily be answered correctly. For the task-in-within the passage, several gold standard questions were written to verify that the instructions were followed from sentence pairs, since it was easier for judges to answer a passage of 85 questions."}, {"heading": "3.3 Ranking Generation", "text": "To aggregate these paired preferences into an overall ranking of sentences, we use a simple, publicly available approach that Chen et al. rates as competitive with their own crowd-BT aggregation method: the Microsoft Trueskill algorithm (Herbrich et al., 2007). Trueskill is a Bayesian skill rating system that generalizes the well-known Elo rating system by generating a ranking from paired decisions. Since Trueskill's ranking algorithm depends on the order in which the samples are processed, we report the ranking as an average of 50 runes. The judgments were not aggregated for each comparison, but each of the judgments was treated separately, allowing Trueskill to take into account the degree of match among workers, since a high-match verdict reflects a larger difference in ranking than a lower-match verdict."}, {"heading": "4 Modeling Pairwise Relative Difficulty", "text": "Our first step in researching the relative difficulty of determining a set of sentences was to develop a model that could accurately predict the relative difficulty for a single pair of sentences, corresponding to the paired judgments of the relative difficulty we extracted from the set. We did this for both the sentence-in-passage and the sentence-in-sentence method. In predicting a paired judgment for the sentence-in-sentence task, the model uses only the sentence texts. In the sentence-in-passage task model, the Stanford Deterministic Coreference Resolution System (Raghunathan et al., 2010) takes into account corner chains within the passage. From these corner chains, sentences with references to and from the target sentence are identified. If additional sentences are found, they are used in a separate function set included in the model; for all possible features, they are calculated for the target sentence, and set separately for the additional sentence."}, {"heading": "4.1 Lexical Features", "text": "For lexical characteristics that are partially based on the work of (Song and Cohn 2011), we have included the percentage of non-stop words (using the NLTK list), the total number of words and the total number of characters as characteristics. We have included the percentage of words in the text of the revised Dale Chall word list (Dale and Chall, 2000) to capture the presence of more difficult words in the sentences. Since sentences that contain rarer word strings are likely to be more difficult, and the probability of the sentence that is based on a large corpus should reflect this, we have included the n-gram probability of each sentence over each of 1-5 n-grams as a characteristic. The Microsoft WebLM service (Wang et al., 2010) has also been used to calculate the n-gram probability. In the field of psycholinguistics, the age of acquisition skills (AoA) refers to the age of each of 1-5 n-gram as a characteristic. The Microsoft WebLM service (Wang et al., 2010) was also used to calculate the n-gram probability based on a large corpus, which should reflect the n-gram probability of a large corpus."}, {"heading": "4.2 Syntactic Features", "text": "We analyzed each sentence in the dataset using the BLLIP parser (Charniak and Johnson, 2005), which contains a pre-trained model based on the Wall Street Journal Corpus. This yielded both a syntactic tree and a portion of the language markers for the sentence. Since part of the language marker is often used as a high-level linguistic trait, we calculated percentages for each POS tag present, since percentages can vary between difficult sentences and simpler sentences. The percentage for each part of the language marker is defined as the number of times that a given day occurred, divided by the total partial markers. The variety of language markers was used, as they can vary between difficult sentences and simpler sentences. Using the syntactic tree provided by the parser, we obtained the probability of the parse, and the probability that the re-ranker produced a given day, as syntactical characteristics. If a sentence has a comparatively high probability, it is easier to read the total, therefore, because it is more likely that the structure of the three tactical characteristics has been added and the phrase is more frequent."}, {"heading": "5 Pairwise Difficulty Prediction Results", "text": "The performance of logistic regression, which is equipped with different characteristics, is shown in Table 1. We reported on the mean and mean deviation of the accuracy of each model over 200 randomly selected trainings and tests. Each test set consisted of 20% of the data and contained 60 aggregated pairs that were unable to make the optimal prediction for each aggregated pair. Test sets contain the same set pairs, but the individual judgments are different."}, {"heading": "6 Ranking Results", "text": "Using the method of pairwise aggregation described in paragraph 3.3, we evaluated sentences by relative difficulty for both pure sentence and sentence insertion tasks. By observing how the overall rank or ranking of sentences changes under these conditions, we could see differences in how workers assessed the relative difficulty of sentences with and without context."}, {"heading": "6.1 Rank Differences", "text": "Comparisons between the rankings for each task are made in Table 3.When comparing the rankings for the sentence-only and sentence-in-passage task, the results show a statistically significant overall difference in the way the crowd classifies sentence difficulty with and without the surrounding passage. While the correlation between the two rankings is high and the average normalized change in ranking position is 7.7%, several sentences showed a large change in ranking order. For example, the sentence \"As a result, the police had little incentive to make concessions.\" was classified much more easily out of context than in context (ranking change: -30 positions). For example, the surrounding passage explained the complex political environment to which this sentence referred indirectly."}, {"heading": "6.2 Feature Correlation with Rank Differences", "text": "To investigate why sentences can be classified as more or less difficult depending on the context, we examined the correlation between the change in the rank of a sentence (sentence-only ranking minus the sentence-passage ranking) and the normalized difference in characteristic values between the sentence representation and the remaining context representation. We found that percentage changes in parser and reranchor models have the most significant correlation (-0.33) with a change in rank, as in Table 4. To interpret this result, note that the parser and reranchor probabilities represent the probability that the parser and reranchor models assign to the syntactic parse generated by the sentence. In other words, they are a measure of how likely it is that the sentence structure occurs based on the training data of the model. If the difficulty of the sentence is classified higher in the passage than the sentence alone, this probability correlates to the target tactic structure with the average surrounding the sentence tactical structure."}, {"heading": "6.3 Review of Data", "text": "The paired prediction results indicate that a large proportion of crowdsourced pair orders can be decided based on vocabulary characteristics, based on the strong performance of the Age of Acquisition characteristics. To determine the relative importance of vocabulary and syntax in our data, we reviewed each pair and assessed whether the syntax or vocabulary of the sentence or the combination of the two were required to correctly predict the more difficult sentence. In many pairs, either vocabulary or vocabulary could be used to correctly predict the more difficult sentence, as each factor indicated that the same sentence was more difficult. We found that 19% of the pairs had only one vocabulary distinction and 65% of the pairs could correctly be judged based on either vocabulary or syntax. Therefore, 84% of the pairs could be judged based on the vocabulary, which explains the high performance of the Age of Acquisition feature. The level of the source document of a teaching sentence was determined to be used as a substitute for the high level of the sentence in the set."}, {"heading": "7 Conclusion", "text": "Using a rich sentence representation based on lexical and syntactical features used in previous work on document readability, we introduced and evaluated several models for predicting the relative reading difficulty of individual sentences with and without environmental context. We found that while the best predictive performance was achieved by using all characteristic classes, simpler representations based on lexical features such as the age of acquisition norms were effective. The accuracy achieved by the best predictive model was 6% of the oracle accuracy for both tasks. Many of the identified characteristics showed a high correlation with the mass-generated rankings, suggesting that these characteristics can be used to build a model of sentence difficulty. Since the rankings were created based on crowd-sourced judgments of sentence difficulty, small but significant differences in the order of precedence of sentences with and without the surrounding passages of a sentence may be found to suggest that this additional fit suggests that the accuracy of the sentence may be determined in our application."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their suggestions and Ann Schumacher for her role as a music writer. This work has been partially supported by the Department of Education of the University of Michigan. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors."}], "references": [{"title": "Natural language processing with Python. ", "author": ["Bird et al.2009] Steven Bird", "Ewan Klein", "Edward Loper"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Coarse-to-fine n-best parsing and maxent discriminative reranking", "author": ["Charniak", "Johnson2005] Eugene Charniak", "Mark Johnson"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Charniak et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Charniak et al\\.", "year": 2005}, {"title": "Pairwise ranking aggregation in a crowdsourced setting", "author": ["Chen et al.2013] Xi Chen", "Paul N Bennett", "Kevyn Collins-Thompson", "Eric Horvitz"], "venue": "In Proceedings of the sixth ACM international conference on Web search and data mining,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "A language modeling approach to predicting reading difficulty", "author": ["Collins-Thompson", "James P Callan"], "venue": "In HLT-NAACL,", "citeRegEx": "Collins.Thompson et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Collins.Thompson et al\\.", "year": 2004}, {"title": "Computational assessment of text readability: a survey of current and future research", "author": ["Kevyn Collins-Thompson"], "venue": "International Journal of Applied Linguistics,", "citeRegEx": "Collins.Thompson.,? \\Q2014\\E", "shortCiteRegEx": "Collins.Thompson.", "year": 2014}, {"title": "Readability revisited: The new dale-chall readability", "author": ["Dale", "Chall2000] Edgar Dale", "Jeanne S Chall"], "venue": null, "citeRegEx": "Dale et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Dale et al\\.", "year": 2000}, {"title": "The readability of tweets and their geographic correlation with education", "author": ["Davenport", "Robert DeLine"], "venue": null, "citeRegEx": "Davenport et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Davenport et al\\.", "year": 2014}, {"title": "Using the crowd for readability prediction", "author": ["Veronique Hoste", "Bart Desmet", "Philip Van Oosten", "Martine De Cock", "Lieve Macken"], "venue": "Natural Language Engineering,", "citeRegEx": "Clercq et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Clercq et al\\.", "year": 2014}, {"title": "Mining words in the minds of second language learners: Learner-specific word difficulty", "author": ["Ehara et al.2012] Yo Ehara", "Issei Sato", "Hidekazu Oiwa", "Hiroshi Nakagawa"], "venue": "COLING", "citeRegEx": "Ehara et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ehara et al\\.", "year": 2012}, {"title": "Coh-metrix providing multilevel analyses of text characteristics", "author": ["Danielle S McNamara", "Jonna M"], "venue": "Kulikowich", "citeRegEx": "Graesser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Graesser et al\\.", "year": 2011}, {"title": "An analysis of statistical models and features for reading difficulty prediction", "author": ["Kevyn CollinsThompson", "Maxine Eskenazi"], "venue": "In Proceedings of the Third Workshop", "citeRegEx": "Heilman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2008}, {"title": "Trueskill(tm): A bayesian skill rating system", "author": ["Tom Minka", "Thore Graepel"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Herbrich et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 2007}, {"title": "Corpus-based acquisition of sentence readability ranking models for deaf people", "author": ["Inui", "Yamamoto2001] Kentaro Inui", "Satomi Yamamoto"], "venue": "In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium,", "citeRegEx": "Inui et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Inui et al\\.", "year": 2001}, {"title": "Predicting the readability of short web summaries", "author": ["Kanungo", "Orr2009] Tapas Kanungo", "David Orr"], "venue": "In Proceedings of the Second ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Kanungo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kanungo et al\\.", "year": 2009}, {"title": "Statistical estimation of word acquisition with application to readability prediction", "author": ["Kidwell et al.2011] Paul Kidwell", "Guy Lebanon", "Kevyn Collins-Thompson"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Kidwell et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kidwell et al\\.", "year": 2011}, {"title": "Gdex: Automatically finding good dictionary examples in a corpus", "author": ["Milos Hus\u00e1k", "Katy McAdam", "Michael Rundell", "Pavel Rychl\u1ef3"], "venue": "In Proceedings of the XIII EURALEX International Congress (Barcelona,", "citeRegEx": "Kilgarriff et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kilgarriff et al\\.", "year": 2008}, {"title": "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel", "author": ["Robert P Fishburne Jr.", "Richard L Rogers", "Brad S Chissom"], "venue": null, "citeRegEx": "Kincaid et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Kincaid et al\\.", "year": 1975}, {"title": "Age-of-acquisition ratings for 30,000 english words", "author": ["Hans Stadthagen-Gonzalez", "Marc Brysbaert"], "venue": "Behavior Research Methods,", "citeRegEx": "Kuperman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kuperman et al\\.", "year": 2012}, {"title": "Effects of causal text revisions on more-and less-skilled readers", "author": ["Liederholm", "Michelle Gaddy Everson", "Paul van den Broek", "Maureen Mischinski", "Alex Crittenden", "Jay Samuels"], "venue": null, "citeRegEx": "Liederholm et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Liederholm et al\\.", "year": 2000}, {"title": "Reading both high-coherence and low-coherence texts: Effects", "author": ["Danielle S McNamara"], "venue": null, "citeRegEx": "McNamara.,? \\Q2001\\E", "shortCiteRegEx": "McNamara.", "year": 2001}, {"title": "The neural bases of text and discourse processing", "author": ["Perfetti", "Frishkoff2008] Charles Perfetti", "Gwen A. Frishkoff"], "venue": "In B. Stemmer and H. A. Whitaker (Eds.) Handbook of the Neuroscience of Language,", "citeRegEx": "Perfetti et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Perfetti et al\\.", "year": 2008}, {"title": "Rule-based and machine learning approaches for second language sentencelevel readability", "author": ["Pil\u00e1n et al.2014] Ildik\u00f3 Pil\u00e1n", "Elena Volodina", "Richard Johansson"], "venue": "In Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educa-", "citeRegEx": "Pil\u00e1n et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pil\u00e1n et al\\.", "year": 2014}, {"title": "Revisiting readability: A unified framework for predicting text quality", "author": ["Pitler", "Nenkova2008] Emily Pitler", "Ani Nenkova"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pitler et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pitler et al\\.", "year": 2008}, {"title": "A multi-pass sieve for coreference resolution", "author": ["Heeyoung Lee", "Sudarshan Rangarajan", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky", "Christopher Manning"], "venue": "In Proceedings of the 2010 Conference", "citeRegEx": "Raghunathan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raghunathan et al\\.", "year": 2010}, {"title": "American national corpus (anc) second release", "author": ["Reppen et al.2005] Randi Reppen", "Nancy Ide", "Keith Suderman"], "venue": null, "citeRegEx": "Reppen et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Reppen et al\\.", "year": 2005}, {"title": "The new york times annotated corpus. Linguistic Data Consortium, Philadelphia, 6(12):e26752", "author": ["Evan Sandhaus"], "venue": null, "citeRegEx": "Sandhaus.,? \\Q2008\\E", "shortCiteRegEx": "Sandhaus.", "year": 2008}, {"title": "Reading level assessment using support vector machines and statistical language models", "author": ["Schwarm", "Ostendorf2005] Sarah E Schwarm", "Mari Ostendorf"], "venue": "In Proceedings of the 43rd Annual Meeting on Association", "citeRegEx": "Schwarm et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schwarm et al\\.", "year": 2005}, {"title": "A two-stage approach for generating unbiased estimates of text complexity", "author": ["Michael Flor", "Diane Napolitano"], "venue": "In Proceedings of the Workshop on Natural Language Processing", "citeRegEx": "Sheehan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sheehan et al\\.", "year": 2013}, {"title": "The textevaluator tool", "author": ["Irene Kostin", "Diane Napolitano", "Michael Flor"], "venue": null, "citeRegEx": "Sheehan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sheehan et al\\.", "year": 2014}, {"title": "Regression and ranking based optimisation for sentence level machine translation evaluation", "author": ["Song", "Cohn2011] Xingyi Song", "Trevor Cohn"], "venue": "In Proceedings of the Sixth Workshop on Statistical Machine Translation,", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}, {"title": "Measuring reading comprehension with the lexile framework", "author": ["A Jackson Stenner"], "venue": null, "citeRegEx": "Stenner.,? \\Q1996\\E", "shortCiteRegEx": "Stenner.", "year": 1996}, {"title": "Sorting texts by readability", "author": ["Satoshi Tezuka", "Hiroshi Terada"], "venue": "Comput. Linguist.,", "citeRegEx": "Tanaka.Ishii et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tanaka.Ishii et al\\.", "year": 2010}, {"title": "Learning from history text: The interaction of knowledge and comprehension skill with text structure", "author": ["Voss", "Silfies1996] James Voss", "Laurie Silfies"], "venue": "Cognition and Instruction,", "citeRegEx": "Voss et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Voss et al\\.", "year": 1996}, {"title": "An overview of microsoft web n-gram corpus and applications. June", "author": ["Wang et al.2010] Kuansan Wang", "Christopher Thrasher", "Evelyne Viegas", "Xiaolong Li", "Paul Hsu"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Learning to simplify sentences with quasi-synchronous grammar and integer programming", "author": ["Woodsend", "Lapata2011] Kristian Woodsend", "Mirella Lapata"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Woodsend et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "The reading difficulty, or readability, of a text is an estimate of linguistic complexity and is typically based on lexical and syntactic features, such as text length, word frequency, and grammatical complexity (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Kidwell et al., 2011; Kanungo and Orr, 2009).", "startOffset": 212, "endOffset": 321}, {"referenceID": 14, "context": "well as the paragraph or passage level (Kidwell et al., 2011; Kanungo and Orr, 2009).", "startOffset": 39, "endOffset": 84}, {"referenceID": 21, "context": "However, much less work has attempted to characterize the readability of single sentences (Pil\u00e1n et al., 2014).", "startOffset": 90, "endOffset": 110}, {"referenceID": 19, "context": "In general, studies have shown a link between reading comprehension and the presence of such cross-sentence relationships in the text (McNamara, 2001; Liederholm et al., 2000; Voss and Silfies, 1996).", "startOffset": 134, "endOffset": 199}, {"referenceID": 18, "context": "In general, studies have shown a link between reading comprehension and the presence of such cross-sentence relationships in the text (McNamara, 2001; Liederholm et al., 2000; Voss and Silfies, 1996).", "startOffset": 134, "endOffset": 199}, {"referenceID": 4, "context": "Recent approaches to estimating readability have used a variety of linguistic features and prediction models (Collins-Thompson, 2014).", "startOffset": 109, "endOffset": 133}, {"referenceID": 30, "context": "The Lexile Framework (Stenner, 1996) uses word frequency estimates in a large corpus as a proxy for lexical difficulty, and sentence length as a grammati-", "startOffset": 21, "endOffset": 36}, {"referenceID": 10, "context": "Later work (Heilman et al., 2008) incorporated grammatical features by parsing the sentences", "startOffset": 11, "endOffset": 33}, {"referenceID": 12, "context": "The approach most similar to ours is the prediction of relative sentence difficulty (with associated readability ranking) for the deaf introduced by Inui et al. (2001). That work focused on effective morphosyntactic features for that target population with an SVM binary clas-", "startOffset": 149, "endOffset": 168}, {"referenceID": 14, "context": "In Kidwell et al. (2011), a set of Age", "startOffset": 3, "endOffset": 25}, {"referenceID": 21, "context": "For example, (Pil\u00e1n et al., 2014) classified individual sentences that would be understood by second-language learners.", "startOffset": 13, "endOffset": 33}, {"referenceID": 15, "context": "Another work (Kilgarriff et al., 2008) identified sentences that would be good dictionary examples by looking for specific desirable features.", "startOffset": 13, "endOffset": 38}, {"referenceID": 9, "context": "(Graesser et al., 2011) measures text cohesiveness, accounting for both the reading difficulty of the text and other lexical and syntactic measures as well as a measure of prior knowledge needed for comprehension, and the genre of the text.", "startOffset": 0, "endOffset": 23}, {"referenceID": 27, "context": "TextEvaluator (Sheehan et al., 2013; Sheehan et al., 2014) is designed to help educators select materials for instruction.", "startOffset": 14, "endOffset": 58}, {"referenceID": 28, "context": "TextEvaluator (Sheehan et al., 2013; Sheehan et al., 2014) is designed to help educators select materials for instruction.", "startOffset": 14, "endOffset": 58}, {"referenceID": 29, "context": "Tanaka-Ishii et al. (2010) explored an approach for sorting texts by readability based on pairwise preferences.", "startOffset": 0, "endOffset": 27}, {"referenceID": 2, "context": "Later, Chen et al. (2013) also proposed a model to obtain passage readability ranking by aggregating pairwise comparisons made by crowdworkers.", "startOffset": 7, "endOffset": 26}, {"referenceID": 2, "context": "Later, Chen et al. (2013) also proposed a model to obtain passage readability ranking by aggregating pairwise comparisons made by crowdworkers. In De Clercq et al.(2014), pairwise judgments of whole passages were obtained from crowdworkers and were found to give comparable", "startOffset": 7, "endOffset": 170}, {"referenceID": 24, "context": "bining the American National Corpus (Reppen et al., 2005), the New York Times Corpus (Sandhaus, 2008), and the North American News Text Corpus (McClosky et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 25, "context": ", 2005), the New York Times Corpus (Sandhaus, 2008), and the North American News Text Corpus (McClosky et al.", "startOffset": 35, "endOffset": 51}, {"referenceID": 11, "context": "as being competitive with their own Crowd-BT aggregation method: the Microsoft Trueskill algorithm (Herbrich et al., 2007).", "startOffset": 99, "endOffset": 122}, {"referenceID": 23, "context": "In the model for the sentence-in-passage task, the Stanford Deterministic Coreference Resolution System (Raghunathan et al., 2010) is used to find coreference chains", "startOffset": 104, "endOffset": 130}, {"referenceID": 8, "context": "when deploying a system that recommends highquality items for learners), its connection to the Rasch psychometric model used with reading assessments (Ehara et al., 2012), and the interpretable nature of the resulting parameter weights.", "startOffset": 150, "endOffset": 170}, {"referenceID": 0, "context": "We used the NLTK library (Bird et al., 2009) to tokenize the sentence for feature processing.", "startOffset": 25, "endOffset": 44}, {"referenceID": 33, "context": "The Microsoft WebLM service (Wang et al., 2010) was used to calculate the n-gram likelihood.", "startOffset": 28, "endOffset": 47}, {"referenceID": 17, "context": "A database of 51,715 words collected by (Kuperman et al., 2012) provides a rich resource for use in reading difficulty measures.", "startOffset": 40, "endOffset": 63}, {"referenceID": 16, "context": "Since the data set also includes the number of syllables in each word, and as (Kincaid et al., 1975) proposes that words with more syllables are more difficult, we also included the average and maximum syllable count as", "startOffset": 78, "endOffset": 100}, {"referenceID": 10, "context": "Therefore, as was done in (Heilman et al., 2008),", "startOffset": 26, "endOffset": 48}], "year": 2016, "abstractText": "The problem of accurately predicting relative reading difficulty across a set of sentences arises in a number of important natural language applications, such as finding and curating effective usage examples for intelligent language tutoring systems. Yet while significant research has explored documentand passage-level reading difficulty, the special challenges involved in assessing aspects of readability for single sentences have received much less attention, particularly when considering the role of surrounding passages. We introduce and evaluate a novel approach for estimating the relative reading difficulty of a set of sentences, with and without surrounding context. Using different sets of lexical and grammatical features, we explore models for predicting pairwise relative difficulty using logistic regression, and examine rankings generated by aggregating pairwise difficulty labels using a Bayesian rating system to form a final ranking. We also compare rankings derived for sentences assessed with and without context, and find that contextual features can help predict differences in relative difficulty judgments across these two conditions.", "creator": "LaTeX with hyperref package"}}}