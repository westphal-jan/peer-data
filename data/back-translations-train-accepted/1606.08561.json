{"id": "1606.08561", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2016", "title": "Estimating the class prior and posterior from noisy positives and unlabeled data", "abstract": "We develop a classification algorithm for estimating posterior distributions from positive-unlabeled data, that is robust to noise in the positive labels and effective for high-dimensional data. In recent years, several algorithms have been proposed to learn from positive-unlabeled data; however, many of these contributions remain theoretical, performing poorly on real high-dimensional data that is typically contaminated with noise. We build on this previous work to develop two practical classification algorithms that explicitly model the noise in the positive labels and utilize univariate transforms built on discriminative classifiers. We prove that these univariate transforms preserve the class prior, enabling estimation in the univariate space and avoiding kernel density estimation for high-dimensional data. The theoretical development and both parametric and nonparametric algorithms proposed here constitutes an important step towards wide-spread use of robust classification algorithms for positive-unlabeled data.", "histories": [["v1", "Tue, 28 Jun 2016 05:29:25 GMT  (80kb,D)", "http://arxiv.org/abs/1606.08561v1", null], ["v2", "Tue, 31 Jan 2017 19:25:14 GMT  (81kb,D)", "http://arxiv.org/abs/1606.08561v2", "Fixed a typo in the MSGMM update equations in the appendix. Other minor changes"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["shantanu jain", "martha white", "predrag radivojac"], "accepted": true, "id": "1606.08561"}, "pdf": {"name": "1606.08561.pdf", "metadata": {"source": "CRF", "title": "Estimating the class prior and posterior from noisy positives and unlabeled data", "authors": ["Shantanu Jain", "Martha White", "Predrag Radivojac"], "emails": ["predrag}@indiana.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to survive themselves are able to survive themselves, and they are able to survive themselves, \"he said in an interview with The New York Times,\" I don't think they are able to survive themselves. \"In other words,\" I don't think they are able to survive themselves. \"In other words,\" I don't think they are able to survive themselves. \"In other words,\" I don't think they are able to survive themselves. \""}, {"heading": "2 Problem formulation", "text": "Consider a binary classification problem when mapping an input space to an output space Y = \u03b2 = {0, 1}. Let f be the true distribution of the inputs. It can be represented as the following mixture f (x) = \u03b1f1 (x) + (1 \u2212 \u03b1) f0 (x), (1), where x-X, y-Y, fy are distributions over X for the positive (y = 1) and negative (y = 0) class; and \u03b1 [0, 1) is the class before or the portion of the positive examples in f. We point to a sample of f as unlabeled data. Let g be the distribution of the inputs for the labeled data. Since the labeled sample contains some incorrectly labeled examples, the corresponding distribution is also a mixture of f1 and a small portion, say 1 \u2212 \u03b2, of f0. That is, g (x) = \u03b2f1 (x) f0 (x) components (x) f0 (projected), Y (2), each containing different component (\u03b2)."}, {"heading": "3 Identifiability", "text": "The pre-identifiability class is identifiable if there is a unique pre-identifiable class that makes the two components as different as possible; much of the identifiability characterization in this section has already been considered the case of asymmetric noise (Scott et al., 2013); see Section 7 on related work. We are creating these results here with the aim of introducing the necessary notation to highlight several important results for later algorithm development and include some missing results for our approach. Although the evidential techniques themselves are very different and may be of interest, there are typically two aspects to highlight the identifiableness. First, one must determine whether a problem is identifiable, and second, if there is no canonical form that is identifiable. In this section, we will see that the class is previously unidentifiable because f0 can be a mixture that contains f1 and vice versa. To ensure identifiability, it is necessary to select a pre-identifiable class that makes the two components as different as possible."}, {"heading": "It follows that", "text": "1. (\u03b1 \u043d, \u03b2 \u043d, \u00b5 \u0432 0, \u00b5 \u043c 1) generate (\u00b5, \u03bd) 2. (\u00b5 \u0445 0, \u00b5 \u0445 1) \u0441\u0441\u0442\u0440\u0438\u0439 and consequently \u0432 = a \u00b5 \u0432 1 \u00b5, \u03b2 \u0432 = a \u00b5 \u0432 1 \u03bd.3. F (\u043fres) contains all the mixing pairs in F (\u043fall). 4. A + (\u00b5, \u03bd, \u0442res) = {(\u03b1, \u03b2 \u043a)}. 5. F (\u0445res) is identifiable in (\u03b1, \u03b2); i.e. (\u03b1, \u03b2) is clearly determined from (\u00b5, \u03bd). We refer to the expressions of \u00b5 and \u03bd as mixtures of components \u00b50 and \u00b51 as a max-canonical form when (\u00b50, \u00b51) is selected from systems. This form forces that \u00b51 is not a mixture containing \u00b50 and vice versa, which results in maximum separation, while still a combination of \u00b5 and \u0441\u043eli\u043c justifies distribution in systems."}, {"heading": "4 Univariate Transformation", "text": "The theory and algorithms for the prior estimation of the class are independent of the dimensionality of the data. In practice, however, this dimensionality can have important consequences. (Parametric Gaussian mixture models, which were trained via Expectation Maximization (EM), are known to suffer greatly from the colinearity in high-dimensional data. We address the curse of dimensionality by transforming the data into a single dimension. Transformation: X \u2192 R is surprisingly simply an output of a non-traditional classification class, which is trained into separate samples, L, from an unmarked sample, U. The transformation is similar to that in (Jain et al., 2016), except that it is not necessary to be calibrated like a posterior distribution."}, {"heading": "5 Algorithms", "text": "In this section, we derive a parametric and a non-parametric algorithm for estimating \u03b2 components for both components. In theory, both approaches can handle multivariate samples; in practice, however, to avoid the curse of dimensionality, we use the theory of \u03b1 conservation of universal transformations to transform the samples. The parametric approach is derived by modelling each sample as a two-component Gaussian mixture, sharing the same components but with different mixing ratios: Xui application of uniform transformations to transform the samples."}, {"heading": "6 Empirical investigation", "text": "In this section we systematically evaluate the new algorithms in a controlled synthetic environment as well as on a variety of data sets from the UCI Machine Learning Repository (Lichman, 2013).Experiments on synthetic data: We begin by evaluating all algorithms in a univariate environment, in which both the mixing ratios and the overlaps between the mixing components are known. We create unit variance Gaussian and unit scale Laplace distributed i.i.d. samples and investigate effects on the accuracy of the mixing ratios, the size of the component sample and the overlap between the mixing components. The class before that was varied from {0.05, 0.25, 0.50, 0.95} and the noise component \u03b2 of {1.00, 0.95, 0.75}. The size of the labeled sample L was varied from {100, 1000}, while the size of the unlabeled sample U was fixed at 10000.00."}, {"heading": "7 Related work", "text": "Class predictions in a semi-monitored environment, including positive-non-labeled learning processes, have already been extensively discussed; see Saerens et al. (2002); Cortes et al. (2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al. (2013); Jain et al. (2016) and references therein. Recently, a general environment for label noise has also been introduced, the so-called reciprocal contamination model. The goal of this model is to estimate several unknown base distributions using several random samples composed of different convex combinations of these base distributions (Katz-Samuels and Scott, 2016).The setting of asymmetric label noise is a subset of this more general environment, which is treated under general conditions by Scott et al. (2013) and previously investigated under a more restrictive environment as co-training."}, {"heading": "8 Conclusion", "text": "In this thesis, we developed a practical algorithm for classifying positively blank data with noise in the labeled data set. In particular, we focused on a strategy for high-dimensional data that provides a univariate transformation that reduces the dimension of the data that the class receives beforehand, so that the estimate remains valid in this reduced space and is then useful for classification. This approach represents a simple algorithm that simultaneously improves the assessment of the class beforehand and delivers a resulting classifier. We derived a parametric and a non-parametric algorithm and then demonstrated the performance on a variety of learning scenarios and data sets. To the best of knowledge, this algorithm represents one of the first practical and easy-to-use approaches to learning with high-dimensional positively blank data with noise in the labels."}, {"heading": "It follows that", "text": "(1). (1)..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "It follows that", "text": "1. a\u00b51\u00b5 = inf R (\u00b5, \u00b51).2. If a\u00b51\u00b50 = 0, then \u03b1 = a \u00b51 \u00b5.Proof: The proof results from Lemma 4 and Theorem 3 of Jain et al. (2016).Theorem 4 (Restatement of Theorem 9 in Jain et al. (2016) LetX and X1 are random variables with densities f and f1 and measure \u00b5 and \u00b51 respectively. For R + = R + 1 {0, \u221e} and an abstract space X, if any one-to-one function G: R + \u2192 X\u03c4 is given, define the function condition \u03c4: X = X\u03c4\u0442 = G, whereas d (x) = {f (x) / f1 (x) > 0 \u221e if f1 (x) = 0.Let the measurements be for the random variables \u03c4 (X), dging (X1) or for disturbed algebra."}, {"heading": "A\u03c4 on X\u03c4 . Then a\u00b51\u00b5 = a\u00b5\u03c41\u00b5\u03c4 .", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B MSGMM", "text": "The parametric approach is derived from the modelling of each sample as a two-component Gaussian mix. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "C AlphaMax", "text": "For a mixed sample M and a component sample C AlphaMax (M, C) is then estimated the maximum proportion of C in M. (AlphaMax is based on the limited maximization of the probability of samples M and C, derived using non-parametric estimates of their density m and c, respectively. We list the most important steps of AlphaMax using Example C. Determine the weights, vi, and components \u03bai from non-parametric density estimates of m as k component mixture, m (x) = deterioration of k i = 1 viception i (x), using M.2. Construct two density functions c, vi, and m, from vi, i and c, parameterized by a k-dimensional weight vector vi = x x, 0."}, {"heading": "D Results for synthetic data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "E Results for multivariate AlphaMax-N and MSGMM", "text": "To demonstrate the effectiveness of class-pre-conservation transformation, we implemented the multivariate versions of AlphaMax-N and MSGMM and evaluated them on the twelve real datasets without applying the transformation. There was significant stability and arithmetic problems related to the high-dimensional nature of the datasets. MSGMM was numerically unstable due to the singular / near-singular covariance matrix; AlphaMax-N became mathematically demanding because the number of containers (for histogram-based density estimation) grows exponentially with the dimension, leading to a large parameter vector, and consequently to a large optimization problem, even after removing the zero-dimension containers. This is expected since density estimation for multivariate data is known to be problematic, which is one of the main reasons for introducing our transform. In order to make the estimation feasible under this stability and we do not have any component computation problems using GMM at the same level, although we used three component dimensions."}], "references": [{"title": "High breakdown mixture discriminant analysis", "author": ["S. Bashir", "E.M. Carter"], "venue": "J Multivar Anal,", "citeRegEx": "Bashir and Carter.,? \\Q2005\\E", "shortCiteRegEx": "Bashir and Carter.", "year": 2005}, {"title": "Semi-supervised novelty detection", "author": ["G. Blanchard", "G. Lee", "C. Scott"], "venue": "J Mach Learn Res,", "citeRegEx": "Blanchard et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Blanchard et al\\.", "year": 2010}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In Proceedings of the 11th Annual Conference on Computational Learning Theory, COLT", "citeRegEx": "Blum and Mitchell.,? \\Q1998\\E", "shortCiteRegEx": "Blum and Mitchell.", "year": 1998}, {"title": "Robust supervised classification with mixture models: learning from data with uncertain labels", "author": ["C. Bouveyron", "S. Girard"], "venue": "Pattern Recognit,", "citeRegEx": "Bouveyron and Girard.,? \\Q2009\\E", "shortCiteRegEx": "Bouveyron and Girard.", "year": 2009}, {"title": "Sample selection bias correction theory", "author": ["C. Cortes", "M. Mohri", "M. Riley", "A. Rostamizadeh"], "venue": "In Proceedings of the 19th International Conference on Algorithmic Learning Theory, ALT", "citeRegEx": "Cortes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2008}, {"title": "Learning from positive and unlabeled examples", "author": ["F. Denis", "R. Gilleron", "F. Letouzey"], "venue": "Theor Comput Sci,", "citeRegEx": "Denis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Denis et al\\.", "year": 2005}, {"title": "Class prior estimation from positive and unlabeled data", "author": ["M.C. du Plessis", "M. Sugiyama"], "venue": "IEICE Transactions on Information and Systems,", "citeRegEx": "Plessis and Sugiyama.,? \\Q2014\\E", "shortCiteRegEx": "Plessis and Sugiyama.", "year": 2014}, {"title": "Learning classifiers from only positive and unlabeled data", "author": ["C. Elkan", "K. Noto"], "venue": "In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Elkan and Noto.,? \\Q2008\\E", "shortCiteRegEx": "Elkan and Noto.", "year": 2008}, {"title": "High-breakdown linear discriminant analysis", "author": ["D.M. Hawkins", "G.J. McLachlan"], "venue": "J Am Stat Assoc,", "citeRegEx": "Hawkins and McLachlan.,? \\Q1997\\E", "shortCiteRegEx": "Hawkins and McLachlan.", "year": 1997}, {"title": "Nonparametric semi-supervised learning of class proportions", "author": ["S. Jain", "M. White", "M.W. Trosset", "P. Radivojac"], "venue": "arXiv preprint arXiv:1601.01944,", "citeRegEx": "Jain et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2016}, {"title": "A mutual contamination analysis of mixed membership and partial label models", "author": ["J. Katz-Samuels", "C. Scott"], "venue": "arXiv preprint arXiv:1602.06235,", "citeRegEx": "Katz.Samuels and Scott.,? \\Q2016\\E", "shortCiteRegEx": "Katz.Samuels and Scott.", "year": 2016}, {"title": "Estimating a kernel Fisher discriminant in the presence of label noise", "author": ["N.D. Lawrence", "B. Scholkopf"], "venue": "In Proceedings of the 18th International Conference on Machine Learning,", "citeRegEx": "Lawrence and Scholkopf.,? \\Q2001\\E", "shortCiteRegEx": "Lawrence and Scholkopf.", "year": 2001}, {"title": "Sparse nonparametric density estimation in high dimensions using the rodeo", "author": ["H. Liu", "J.D. Lafferty", "L.A. Wasserman"], "venue": "In Proceedings of the 10th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Random classification noise defeats all convex potential boosters", "author": ["P.M. Long", "R.A. Servedio"], "venue": "Mach Learn,", "citeRegEx": "Long and Servedio.,? \\Q2010\\E", "shortCiteRegEx": "Long and Servedio.", "year": 2010}, {"title": "Noise tolerance under risk minimization", "author": ["N. Manwani", "P.S. Sastry"], "venue": "IEEE T Cybern,", "citeRegEx": "Manwani and Sastry.,? \\Q2013\\E", "shortCiteRegEx": "Manwani and Sastry.", "year": 2013}, {"title": "Obtaining calibrated probabilities from boosting", "author": ["A. Niculescu-Mizil", "R. Caruana"], "venue": "In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Niculescu.Mizil and Caruana.,? \\Q2005\\E", "shortCiteRegEx": "Niculescu.Mizil and Caruana.", "year": 2005}, {"title": "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods, pages 61\u201374", "author": ["J.C. Platt"], "venue": null, "citeRegEx": "Platt.,? \\Q1999\\E", "shortCiteRegEx": "Platt.", "year": 1999}, {"title": "Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure", "author": ["M. Saerens", "P. Latinne", "C. Decaestecker"], "venue": "Neural Comput,", "citeRegEx": "Saerens et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Saerens et al\\.", "year": 2002}, {"title": "Classification with asymmetric label noise: consistency and maximal denoising", "author": ["C. Scott", "G. Blanchard", "G. Handy"], "venue": "J Mach Learn Res W&CP,", "citeRegEx": "Scott et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Scott et al\\.", "year": 2013}, {"title": "The curse of dimensionality and dimension reduction. Multivariate Density Estimation: Theory, Practice, and Visualization, pages", "author": ["D.W. Scott"], "venue": null, "citeRegEx": "Scott.,? \\Q2008\\E", "shortCiteRegEx": "Scott.", "year": 2008}, {"title": "The ABC\u2019s (and XYZ\u2019s) of peptide sequencing", "author": ["H. Steen", "M. Mann"], "venue": "Nat Rev Mol Cell Biol,", "citeRegEx": "Steen and Mann.,? \\Q2004\\E", "shortCiteRegEx": "Steen and Mann.", "year": 2004}, {"title": "Theorem 4 (Restatement of Theorem 9 in Jain et al. (2016)) LetX andX1 be random variables with densities f and f1 and measures \u03bc and \u03bc1", "author": ["Jain"], "venue": null, "citeRegEx": "Jain,? \\Q2016\\E", "shortCiteRegEx": "Jain", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "In many domains, however, a sample from one of the classes (say, negatives) may not be available, leading to the setting of learning from positive and unlabeled data (Denis et al., 2005).", "startOffset": 166, "endOffset": 186}, {"referenceID": 7, "context": "If the class priors in the unlabeled data are known, positive-unlabeled learning can straightforwardly translate into learning of non-traditional classifiers (Elkan and Noto, 2008); i.", "startOffset": 158, "endOffset": 180}, {"referenceID": 9, "context": ", those that discriminate between positive and negative data (Jain et al., 2016).", "startOffset": 61, "endOffset": 80}, {"referenceID": 9, "context": "Under mild assumptions, even when the class priors are unknown, there exists a monotonic relationship between the outputs of these classifiers (Jain et al., 2016) and, hence, the models trained for information retrieval and ranking generally do not suffer when trained on labeled vs.", "startOffset": 143, "endOffset": 162}, {"referenceID": 1, "context": "Class prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al.", "startOffset": 146, "endOffset": 228}, {"referenceID": 18, "context": "Class prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al.", "startOffset": 146, "endOffset": 228}, {"referenceID": 9, "context": "Class prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al.", "startOffset": 146, "endOffset": 228}, {"referenceID": 7, "context": ", 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al., 2009; du Plessis and Sugiyama, 2014; Jain et al., 2016).", "startOffset": 38, "endOffset": 129}, {"referenceID": 9, "context": ", 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al., 2009; du Plessis and Sugiyama, 2014; Jain et al., 2016).", "startOffset": 38, "endOffset": 129}, {"referenceID": 20, "context": "For example, in the process of peptide identification (Steen and Mann, 2004), bioinformatics methods are usually set to report results with specified false discovery rate thresholds (e.", "startOffset": 54, "endOffset": 76}, {"referenceID": 18, "context": "Further, the only approach that does consider similar such noise (Scott et al., 2013) requires density estimation, which is known to be problematic for high-dimensional data.", "startOffset": 65, "endOffset": 85}, {"referenceID": 18, "context": "Much of the identifiability characterization in this section has already been considered as the case of asymmetric noise (Scott et al., 2013); see Section 7 on related work.", "startOffset": 121, "endOffset": 141}, {"referenceID": 18, "context": "To ensure identifiability, it is necessary to choose a canonical form that prefers a class prior that makes the two components as different as possible; this canonical form was independently introduced as the mutual irreducibility principle (Scott et al., 2013) or the max-canonical form (Jain et al.", "startOffset": 241, "endOffset": 261}, {"referenceID": 9, "context": ", 2013) or the max-canonical form (Jain et al., 2016).", "startOffset": 34, "endOffset": 53}, {"referenceID": 12, "context": "Nonparametric (kernel) density estimation is also known to have curse-of-dimensionality issues, both in theory (Liu et al., 2007) and in practice (Scott, 2008).", "startOffset": 111, "endOffset": 129}, {"referenceID": 19, "context": ", 2007) and in practice (Scott, 2008).", "startOffset": 24, "endOffset": 37}, {"referenceID": 9, "context": "The transform is similar to that in (Jain et al., 2016), except that it is not required to be calibrated like a posterior distribution; as shown below, a good ranking function is sufficient.", "startOffset": 36, "endOffset": 55}, {"referenceID": 16, "context": "The posterior probability \u03c4p can be estimated directly by using a probabilistic classifier or by calibrating a classifier\u2019s score (Platt, 1999; Niculescu-Mizil and Caruana, 2005); |U |/|L| serves as an estimate of p(S=0)/p(S=1); section 5 gives parametric and nonparametric approaches for estimation of \u03b1\u2217 and \u03b2\u2217.", "startOffset": 130, "endOffset": 178}, {"referenceID": 15, "context": "The posterior probability \u03c4p can be estimated directly by using a probabilistic classifier or by calibrating a classifier\u2019s score (Platt, 1999; Niculescu-Mizil and Caruana, 2005); |U |/|L| serves as an estimate of p(S=0)/p(S=1); section 5 gives parametric and nonparametric approaches for estimation of \u03b1\u2217 and \u03b2\u2217.", "startOffset": 130, "endOffset": 178}, {"referenceID": 9, "context": "For this purpose, we use the AlphaMax algorithm (Jain et al., 2016), briefly summarized in the Appendix.", "startOffset": 48, "endOffset": 67}, {"referenceID": 7, "context": "Algorithms: We compare the AlphaMax-N and MSGMM algorithms to the Elkan-Noto algorithm (Elkan and Noto, 2008) as well as the noiseless version of AlphaMax (Jain et al.", "startOffset": 87, "endOffset": 109}, {"referenceID": 9, "context": "Algorithms: We compare the AlphaMax-N and MSGMM algorithms to the Elkan-Noto algorithm (Elkan and Noto, 2008) as well as the noiseless version of AlphaMax (Jain et al., 2016).", "startOffset": 155, "endOffset": 174}, {"referenceID": 6, "context": "The algorithm proposed by du Plessis and Sugiyama (2014) minimizes the same objective as the e1 Elkan-Noto estimator and, thus, was not implemented.", "startOffset": 29, "endOffset": 57}, {"referenceID": 10, "context": "The aim under this model is to estimate multiple unknown base distributions, using multiple random samples that are composed of different convex combinations of those base distributions (Katz-Samuels and Scott, 2016).", "startOffset": 186, "endOffset": 216}, {"referenceID": 2, "context": "(2013), and previously investigated under a more restrictive setting as co-training (Blum and Mitchell, 1998).", "startOffset": 84, "endOffset": 109}, {"referenceID": 13, "context": "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.", "startOffset": 165, "endOffset": 216}, {"referenceID": 14, "context": "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.", "startOffset": 165, "endOffset": 216}, {"referenceID": 8, "context": "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.", "startOffset": 233, "endOffset": 287}, {"referenceID": 0, "context": "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.", "startOffset": 233, "endOffset": 287}, {"referenceID": 11, "context": "Generative mixture model approaches have also been developed, which explicitly model the noise (Lawrence and Scholkopf, 2001; Bouveyron and Girard, 2009); these algorithms, however, assume labeled data for each class.", "startOffset": 95, "endOffset": 153}, {"referenceID": 3, "context": "Generative mixture model approaches have also been developed, which explicitly model the noise (Lawrence and Scholkopf, 2001; Bouveyron and Girard, 2009); these algorithms, however, assume labeled data for each class.", "startOffset": 95, "endOffset": 153}, {"referenceID": 5, "context": "Class prior estimation in a semi-supervised setting including positive-unlabeled learning, has been extensively discussed previously; see Saerens et al. (2002); Cortes et al.", "startOffset": 138, "endOffset": 160}, {"referenceID": 0, "context": "(2002); Cortes et al. (2008); Elkan and Noto (2008); Blanchard et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "(2002); Cortes et al. (2008); Elkan and Noto (2008); Blanchard et al.", "startOffset": 8, "endOffset": 52}, {"referenceID": 0, "context": "(2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al.", "startOffset": 31, "endOffset": 55}, {"referenceID": 0, "context": "(2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al. (2013); Jain et al.", "startOffset": 31, "endOffset": 76}, {"referenceID": 0, "context": "(2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al. (2013); Jain et al. (2016) and references therein.", "startOffset": 31, "endOffset": 96}, {"referenceID": 0, "context": "(2008); Elkan and Noto (2008); Blanchard et al. (2010); Scott et al. (2013); Jain et al. (2016) and references therein. Recently, a general setting for label noise has also been introduced, called the mutual contamination model. The aim under this model is to estimate multiple unknown base distributions, using multiple random samples that are composed of different convex combinations of those base distributions (Katz-Samuels and Scott, 2016). The setting of asymmetric label noise is a subset of this more general setting, treated under general conditions by Scott et al. (2013), and previously investigated under a more restrictive setting as co-training (Blum and Mitchell, 1998).", "startOffset": 31, "endOffset": 583}, {"referenceID": 0, "context": "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise. Generative mixture model approaches have also been developed, which explicitly model the noise (Lawrence and Scholkopf, 2001; Bouveyron and Girard, 2009); these algorithms, however, assume labeled data for each class. As the most related work, though Scott et al. (2013) did not explicitly treat the positive-unlabeled learning with noisy positives, their formulation can incorporate this setting by using \u03c00 = \u03b1 and \u03b2 = 1 \u2212 \u03c01.", "startOffset": 263, "endOffset": 611}, {"referenceID": 9, "context": "Proof: The proof follows from Lemma 4 and Theorem 3 of Jain et al. (2016).", "startOffset": 55, "endOffset": 74}, {"referenceID": 9, "context": "Theorem 4 (Restatement of Theorem 9 in Jain et al. (2016)) LetX andX1 be random variables with densities f and f1 and measures \u03bc and \u03bc1 respectively.", "startOffset": 39, "endOffset": 58}, {"referenceID": 9, "context": "The pseudo code for elbow detection is provided in (Jain et al., 2016).", "startOffset": 51, "endOffset": 70}], "year": 2017, "abstractText": "We develop a classification algorithm for estimating posterior distributions from positive-unlabeled data, that is robust to noise in the positive labels and effective for high-dimensional data. In recent years, several algorithms have been proposed to learn from positive-unlabeled data; however, many of these contributions remain theoretical, performing poorly on real high-dimensional data that is typically contaminated with noise. We build on this previous work to develop two practical classification algorithms that explicitly model the noise in the positive labels and utilize univariate transforms built on discriminative classifiers. We prove that these univariate transforms preserve the class prior, enabling estimation in the univariate space and avoiding kernel density estimation for high-dimensional data. The theoretical development and both parametric and nonparametric algorithms proposed here constitutes an important step towards wide-spread use of robust classification algorithms for positive-unlabeled data.", "creator": "LaTeX with hyperref package"}}}