{"id": "1606.08571", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2016", "title": "Alternating Back-Propagation for Generator Network", "abstract": "The supervised learning of the discriminative convolutional neural network (ConvNet) is powered by back-propagation on the parameters. In this paper, we show that the unsupervised learning of a popular top-down generative ConvNet model with latent continuous factors can be accomplished by a learning algorithm that consists of alternatively performing back-propagation on both the latent factors and the parameters. The model is a non-linear generalization of factor analysis, where the high-dimensional observed data, is assumed to be the noisy version of a vector generated by a non-linear transformation of a low-dimensional vector of continuous latent factors. Furthermore, it is assumed that these latent factors follow known independent distributions, such as standard normal distributions, and the non-linear transformation is assumed to be parametrized by a top-down ConvNet, which is capable of approximating the highly non-linear mapping from the latent factors to the image. We explore a simple and natural learning algorithm for this model that alternates between the following two steps: (1) inferring the latent factors by Langevin dynamics or gradient descent, and (2) updating the parameters of the ConvNet by gradient descent. Step (1) is based on the gradient of the reconstruction error with respect to the latent factors, which is available by back-propagation. We call this step inferential back-propagation. Step (2) is based on the gradient of the reconstruction error with respect to the parameters, and is also obtained by back-propagation. We refer to this step as learning back-propagation. The code for inferential back-propagation is actually part of the code for learning back-propagation, and thus the inferential back-propagation is actually a by-product of the learning back-propagation. We show that our algorithm can learn realistic generative models of images and sounds.", "histories": [["v1", "Tue, 28 Jun 2016 06:46:05 GMT  (5581kb,D)", "https://arxiv.org/abs/1606.08571v1", null], ["v2", "Sat, 2 Jul 2016 15:11:00 GMT  (5585kb,D)", "http://arxiv.org/abs/1606.08571v2", null], ["v3", "Thu, 15 Sep 2016 04:38:01 GMT  (7268kb,D)", "http://arxiv.org/abs/1606.08571v3", null], ["v4", "Tue, 6 Dec 2016 04:04:19 GMT  (7393kb,D)", "http://arxiv.org/abs/1606.08571v4", null]], "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG cs.NE", "authors": ["tian han", "yang lu", "song-chun zhu", "ying nian wu"], "accepted": true, "id": "1606.08571"}, "pdf": {"name": "1606.08571.pdf", "metadata": {"source": "CRF", "title": "Alternating Back-Propagation for Generator Network", "authors": ["Tian Han", "Yang Lu", "Song-Chun Zhu", "Ying Nian Wu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This paper examines the fundamental problem of learning and conclusions in the generator network (Goodfellow et al., 2014), a generative model that has recently become popular. Specifically, we propose an alternating back-propagation algorithm for learning and conclusions in this model."}, {"heading": "1.1 Non-linear factor analysis", "text": "The generator network is a non-linear generalization of factor analysis. Factor analysis is a prototype model in the unattended learning of distributed representations. There are two directions that can be followed to generalize the factor analysis model. One direction is to generalize the previous model or the previous assumption about the latent factors, which led to methods such as independent component analysis (Hyv\u00e4rinen, Karhunen and Volinsky, 2004), sparse coding (Olshausen and Field, 1997), non-negative matrix factorization (Lee and Seung, 2001), matrix factorization and completion for recommendation systems (Koren, Bell and Volinsky, 2009), etc. The other direction to generalize the factor analysis model is the generalization of the mapping factors from the continuous latent factors to the observed signal. The generator network is an example in this direction. It does not linear the mapping of the factor analysis into an image."}, {"heading": "1.2 Alternating back-propagation", "text": "The factor analysis model can be learned from the Rubin-Thayer algorithm (Rubin and Thayer, 1982; Dempster, Laird and Rubin, 1977), where both the E-step and the M-step are based on multivariate linear regression. Inspired by this algorithm, we propose an alternating back-propagation algorithm that iterates the following two steps: (1) Inferential back-propagation: For each training example, we exclude the continuous latent factors by the Langevin dynamics or the origin of the generator. (2) Learning back-propagation: Update the parameters, the latent factors by gradient descent. Langevin dynamics (Neal, 2011) is a stochastic sampling dynamic."}, {"heading": "1.3 Explaining-away inference", "text": "Inferential reverse propagation solves an inverse problem by an explanatory process in which the latent factors compete with each other to explain each training example. Advantages of the explanatory inference of the latent factors are: (1) The latent factors can follow complex earlier models. For example, in textured movements (Wang and Zhu, 2003) or dynamic textures (Doretto et al., 2003), the latent factors can follow a dynamic model, such as vector authority regression. By deriving the latent factors that explain the observed examples, we can learn the earlier model. (2) The observed data may be incomplete or indirect. For example, the training images may contain locked objects. In this case, the latent factors can still be determined by explaining the incomplete or indirect observations, and the model can still be learned as before."}, {"heading": "1.4 Learning from incomplete or indirect data", "text": "We dare to suggest that one of the main advantages of a generative model is that it learns from incomplete or indirect data, which is not rare in practice, and the generative model can then be evaluated on the basis of how well it recovers the original unobserved data, while still learning a model that can generate new data. Learning the generator network from incomplete data can be considered a nonlinear generalization of matrix completion. We also propose to evaluate the learned generator network on the basis of the reconstruction error of the test data."}, {"heading": "1.5 Contribution and related work", "text": "The main contribution of this paper is to propose the alternating back-propagation algorithm for the formation of the generator network. Another contribution is the evaluation of the generative models by learning incomplete or indirect training data. One method is the variable auto-encoder (UAE), the other was recently developed to achieve this. Both methods involve a support network with a separate set of parameters in addition to the original network that generates the signals. One method is the variable auto-encoder (VAE), 2014; Rezende, Mohamed, and Wierstra, 2014; Mnih and Gregor, where the assistance network is an inferential or recognition network that seeks to adopt the posterior distribution of the latent factors."}, {"heading": "2 Factor analysis with ConvNet", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Factor analysis and beyond", "text": "Let Y be a D-dimensional observed data vector, like an image. Let Z be the d-dimensional vector of continuous latent factors, Z = (zk, k = 1,..., d). The traditional factor analysis model is Y = WZ +, where W is the D-dimensional matrix and is a D-dimensional error vector or observation noise. Let us assume that Z \u00b2 N (0, Id), where Id stands for the d-dimensional identity matrix. Let us also assume that \"N\" (0, \u03c32ID), i.e. the observation errors are Gaussian white noise. There are three perspectives to look at W (1) base vectors. Write W = (W1,..., Wd), where each Wk is a D-dimensional column vector, where each Wk is a D-dimensional vector. Then Y = vector."}, {"heading": "2.2 ConvNet mapping", "text": "In addition to generalizing the previous model of latent factors Z, we can also generalize the mapping of Z to Y. In this paper, we consider the generator network model (Goodfellow et al., 2014), which maintains the assumptions that d < D, Z \u0445 N (0, Id) and \u0445 N (0, \u03c32ID) as in traditional factor analysis, but maintains the linear mapping Z to a nonlinear mapping f (Z; W) where f is a ConvNet, and W collects all connection weights and bias terms of ConvNet. Then, the model becomes a multiple mapping of X (Z; W) +, Z \u0445 N (0, Id), \u0445 N (0, \u03c32ID), d < D. (1) The reconstruction error is | Y \u2212 f (Z; W)."}, {"heading": "3 Alternating back-propagation", "text": "If we observe a set of data vectors (Yi = 1, n = Y), then each Yi has a corresponding Zi, but all Yi share the same ConvNet W. Intuitively, we should conclude that the model can be written as \"Z\" to minimize the reconstruction error, the n = 1 | | Yi \u2212 f (Zi; W) | 2 plus a regularization term, the previous one on Z. Formally, the model can be written as \"Z\" and [Y | Z] p (Y, W] p (Z) p (Z, W) p (Z) p (Z) p (Z) p (Z) p (Z) Z). Adopting the language of the painting algorithms (Dempster, Laird, and Rubin, 1977), the complete data model becomes a bylog p (Y, Z; W) = Log [p (Z) p (Y | Z) p (Z) p (Z) p (Z), W) = \u2212 f (Z)."}, {"heading": "4 Experiments", "text": "The code in our experiments is based on the MatConvNet package of (Vedaldi and Lenc, 2015).The training images and sounds are scaled so that the intensities are in the range [\u2212 1, 1].We adopt the structure of the generator network of (Radford, Metz and Chintala, 2016; Dosovitskiy, Springenberg and Brox, 2015), where the topdown network consists of several layers of deconvolution by linear overlay, ReLU nonlinearity and up-sampling, with tanh nonlinearity at the bottom layer (Radford, Metz and Chintala, 2016) to drop the signals within [\u2212 1, 1].We also adopt the batch normalization (Ioffe and Szegedy, 2015).We fix the course for the standard deviation of the noise vector with l = 10 or 30 steps of the dynamics of Langevin."}, {"heading": "4.1 Qualitative experiments", "text": "In fact, most of them are able to outdo themselves."}, {"heading": "4.2 Quantitative experiments", "text": "This year it is more than ever before."}, {"heading": "5 Conclusion", "text": "This paper proposes an alternating back-propagation algorithm for training the generator network. We acknowledge that the generator network is a nonlinear generalization of the factor analysis model, and develop the alternating back-propagation algorithm as a nonlinear generalization of the alternating regression scheme of the Rubin-Thayer EM algorithm for adapting the factor analysis model. The alternating back-propagation algorithm iterates the inferential back-propagation for inferencing the latent factors and the learning back-propagation for updating the parameters. Both back-propagation steps share most of their calculation steps in the chain control calculations. Our learning algorithm is perhaps the most canonical algorithm for training the generator network. It is based on maximum probability, which is theoretically the most accurate estimator. Maximum probability learning attempts to explain the entire data head propagation model in a uniform way or with little regard to adaptation."}, {"heading": "Acknowledgement", "text": "We thank Yifei (Jerry) Xu for his help with the experiments during his summer visit in 2016. We thank Jianwen Xie for helpful discussions. The work is supported by NSF DMS 1310391, DARPA SIMPLEX N66001-15-C-4035, ONR MURI N00014-16-12007 and DARPA ARO W911NF-16-1-0579."}, {"heading": "6 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 ReLU and piecewise factor analysis", "text": "The generator network is Y = f (Z; W) +, Z (l \u2212 1) = fl (WlZ = 1), L = 1,..., L, with Z (0) = f (Z; W), and Z (L) = Z (L) = Z, 2012) or the leaky ReLU (Maas, Hannun, and Ng, 2013; Xu et al., 2015). Each ReLU unit corresponds to a binary switch. In the case of non-leaky ReLU, we follow the analysis of (Pascanu, Montufar, and Bengio, 2013), we can write Z (l \u2212 1)."}, {"heading": "6.2 EM, density mapping, and density shifting", "text": "Suppose the training data {Yi, i = 1, n} comes from a data distribution Pdata (Y) (Y). To understand how the alternating back-propagation algorithm or its EM idealization seeks the prior distribution of the latent factors p (Z) on the data distribution Pdata (Y) by the learned g (Z; W), we define {Pdata (Z, Y; W) = Pdata (Y; W) Pdata (Y), (7) where the Pdata (Z; W) the words (Z; W) p (Z; W) p) = Pdata (Z) p (Z; W) dY is considered by averaging the back data p (Z | Y; W) over the observed data Y (Z; W) the Pdata (Z; W) can be considered as the data before Ydata (Z; W)."}, {"heading": "6.3 Factor analysis and alternating regression", "text": "The alternating back propagation algorithm is inspired by Rubin-Thayer EM = > Y algorithm = > Y distribution (Y = \u03b2), where both the observed data model p (Y | W) and the posterior distribution p (Z | Y, W) are available in closed form. EM algorithm for factor analysis can be interpreted as alternating linear regression (Rubin and Thayer, 1982; Liu, Rubin and Wu, 1998).In the factor analysis model, the operator Z \u0445 N (0, Id), Y = regression N (0, \u03c32ID).The common distribution of (Z, Y) is [Z Y].The common distribution of (Z Y].Z-ression is [0].Z \u2212 Zip > W > + Zip 2ID])."}], "references": [{"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E.J. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "IEEE Transactions on information theory 52(2):489\u2013509.", "citeRegEx": "Cand\u00e8s et al\\.,? 2006", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society: B 1\u201338.", "citeRegEx": "Dempster et al\\.,? 1977", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "R Fergus"], "venue": null, "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Density estimation using real nvp", "author": ["L. Dinh", "J. Sohl-Dickstein", "S. Bengio"], "venue": "CoRR abs/1605.08803.", "citeRegEx": "Dinh et al\\.,? 2016", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Dynamic textures", "author": ["G. Doretto", "A. Chiuso", "Y. Wu", "S. Soatto"], "venue": "IJCV 51(2):91\u2013109.", "citeRegEx": "Doretto et al\\.,? 2003", "shortCiteRegEx": "Doretto et al\\.", "year": 2003}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["E. Dosovitskiy", "J.T. Springenberg", "T. Brox"], "venue": "CVPR.", "citeRegEx": "Dosovitskiy et al\\.,? 2015", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2015}, {"title": "Riemann manifold langevin and hamiltonian monte carlo methods", "author": ["M. Girolami", "B. Calderhead"], "venue": "Journal of the Royal Statistical Society: B 73(2):123\u2013214.", "citeRegEx": "Girolami and Calderhead,? 2011", "shortCiteRegEx": "Girolami and Calderhead", "year": 2011}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. WardeFarley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS, 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Independent component analysis", "author": ["A. Hyv\u00e4rinen", "J. Karhunen", "E. Oja"], "venue": "John Wiley & Sons.", "citeRegEx": "Hyv\u00e4rinen et al\\.,? 2004", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2004}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method", "author": ["H. Kim", "H. Park"], "venue": "SIAM Journal on Matrix Analysis and Applications 30(2):713\u2013730.", "citeRegEx": "Kim and Park,? 2008", "shortCiteRegEx": "Kim and Park", "year": 2008}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "ICLR.", "citeRegEx": "Kingma and Welling,? 2014", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer 42(8):30\u201337.", "citeRegEx": "Koren et al\\.,? 2009", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Algorithms for nonnegative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "NIPS, 556\u2013562.", "citeRegEx": "Lee and Seung,? 2001", "shortCiteRegEx": "Lee and Seung", "year": 2001}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "ICCV, 3730\u20133738.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Parameter expansion to accelerate em: The px-em algorithm", "author": ["C. Liu", "D.B. Rubin", "Y.N. Wu"], "venue": "Biometrika 85(4):755\u2013770.", "citeRegEx": "Liu et al\\.,? 1998", "shortCiteRegEx": "Liu et al\\.", "year": 1998}, {"title": "Learning FRAME models using CNN filters", "author": ["Y. Lu", "S.-C. Zhu", "Y.N. Wu"], "venue": "AAAI.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "ICML.", "citeRegEx": "Maas et al\\.,? 2013", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Sound texture perception via statistics of the auditory periphery: evidence from sound synthesis", "author": ["J.H. McDermott", "E.P. Simoncelli"], "venue": "Neuron 71(5):926\u2013940.", "citeRegEx": "McDermott and Simoncelli,? 2011", "shortCiteRegEx": "McDermott and Simoncelli", "year": 2011}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "ICML.", "citeRegEx": "Mnih and Gregor,? 2014", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Mcmc using hamiltonian dynamics", "author": ["R.M. Neal"], "venue": "Handbook of Markov Chain Monte Carlo 2.", "citeRegEx": "Neal,? 2011", "shortCiteRegEx": "Neal", "year": 2011}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Research 37(23):3311\u20133325", "author": ["B.A. Olshausen", "D.J. Field"], "venue": null, "citeRegEx": "Olshausen and Field,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field", "year": 1997}, {"title": "On the number of response regions of deep feed forward networks with piece-wise linear activations", "author": ["R. Pascanu", "G. Montufar", "Y. Bengio"], "venue": "arXiv:1312.6098.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "ICLR.", "citeRegEx": "Radford et al\\.,? 2016", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "NIPS, 1278\u20131286.", "citeRegEx": "Rezende et al\\.,? 2014", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science 290(5500):2323\u20132326.", "citeRegEx": "Roweis and Saul,? 2000", "shortCiteRegEx": "Roweis and Saul", "year": 2000}, {"title": "Em algorithms for ml factor analysis", "author": ["D.B. Rubin", "D.T. Thayer"], "venue": "Psychometrika 47(1):69\u201376.", "citeRegEx": "Rubin and Thayer,? 1982", "shortCiteRegEx": "Rubin and Thayer", "year": 1982}, {"title": "Multiple imputation for nonresponse in surveys, volume 81", "author": ["D.B. Rubin"], "venue": "John Wiley & Sons.", "citeRegEx": "Rubin,? 2004", "shortCiteRegEx": "Rubin", "year": 2004}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Int. Conf. on Multimedia.", "citeRegEx": "Vedaldi and Lenc,? 2015", "shortCiteRegEx": "Vedaldi and Lenc", "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "ICML, 1096\u20131103.", "citeRegEx": "Vincent et al\\.,? 2008", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Modeling textured motion: Particle, wave and sketch", "author": ["Y. Wang", "Zhu", "S.-C."], "venue": "ICCV, 213\u2013220.", "citeRegEx": "Wang et al\\.,? 2003", "shortCiteRegEx": "Wang et al\\.", "year": 2003}, {"title": "A theory of generative convnet", "author": ["J. Xie", "Y. Lu", "S.-C. Zhu", "Y.N. Wu"], "venue": "ICML.", "citeRegEx": "Xie et al\\.,? 2016", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["B. Xu", "N. Wang", "T. Chen", "M. Li"], "venue": "CoRR abs/1505.00853.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates", "author": ["L. Younes"], "venue": "Stochastics: An International Journal of Probability and Stochastic Processes 65(3-4):177\u2013228.", "citeRegEx": "Younes,? 1999", "shortCiteRegEx": "Younes", "year": 1999}], "referenceMentions": [{"referenceID": 7, "context": "This paper studies the fundamental problem of learning and inference in the generator network (Goodfellow et al., 2014), which is a generative model that has become popular recently.", "startOffset": 94, "endOffset": 119}, {"referenceID": 23, "context": "This led to methods such as independent component analysis (Hyv\u00e4rinen, Karhunen, and Oja, 2004), sparse coding (Olshausen and Field, 1997), non-negative matrix factorization (Lee and Seung, 2001), matrix factorization and completion for recommender systems (Koren, Bell, and Volinsky, 2009), etc.", "startOffset": 111, "endOffset": 138}, {"referenceID": 15, "context": "This led to methods such as independent component analysis (Hyv\u00e4rinen, Karhunen, and Oja, 2004), sparse coding (Olshausen and Field, 1997), non-negative matrix factorization (Lee and Seung, 2001), matrix factorization and completion for recommender systems (Koren, Bell, and Volinsky, 2009), etc.", "startOffset": 174, "endOffset": 195}, {"referenceID": 14, "context": "a convolutional neural network (ConvNet or CNN) (LeCun et al., 1998; Krizhevsky, Sutskever, and Hinton, 2012; Dosovitskiy, Springenberg, and Brox, 2015).", "startOffset": 48, "endOffset": 152}, {"referenceID": 2, "context": "It has been shown recently that the generator network is capable of generating realistic images (Denton et al., 2015; Radford, Metz, and Chintala, 2016).", "startOffset": 96, "endOffset": 152}, {"referenceID": 28, "context": "The factor analysis model can be learned by the Rubin-Thayer EM algorithm (Rubin and Thayer, 1982; Dempster, Laird, and Rubin, 1977), where both the E-step and the M-step are based on multivariate linear regression.", "startOffset": 74, "endOffset": 132}, {"referenceID": 22, "context": "The Langevin dynamics (Neal, 2011) is a stochastic sampling counterpart of gradient descent.", "startOffset": 22, "endOffset": 34}, {"referenceID": 10, "context": "The alternating back-propagation algorithm follows the tradition of alternating operations in unsupervised learning, such as alternating linear regression in the EM algorithm for factor analysis, alternating least squares algorithm for matrix factorization (Koren, Bell, and Volinsky, 2009; Kim and Park, 2008), and alternating gradient descent algorithm for sparse coding (Olshausen and Field, 1997).", "startOffset": 257, "endOffset": 310}, {"referenceID": 23, "context": "The alternating back-propagation algorithm follows the tradition of alternating operations in unsupervised learning, such as alternating linear regression in the EM algorithm for factor analysis, alternating least squares algorithm for matrix factorization (Koren, Bell, and Volinsky, 2009; Kim and Park, 2008), and alternating gradient descent algorithm for sparse coding (Olshausen and Field, 1997).", "startOffset": 373, "endOffset": 400}, {"referenceID": 4, "context": "For instance, in textured motions (Wang and Zhu, 2003) or dynamic textures (Doretto et al., 2003), the latent factors may follow a dynamic model such as vector auto-regression.", "startOffset": 75, "endOffset": 97}, {"referenceID": 11, "context": "One method is variational auto-encoder (VAE) (Kingma and Welling, 2014; Rezende, Mohamed, and Wierstra, 2014; Mnih and Gregor, 2014), where the assisting network is an inferential or recognition network that seeks to approximate the posterior distribution of the latent factors.", "startOffset": 45, "endOffset": 132}, {"referenceID": 21, "context": "One method is variational auto-encoder (VAE) (Kingma and Welling, 2014; Rezende, Mohamed, and Wierstra, 2014; Mnih and Gregor, 2014), where the assisting network is an inferential or recognition network that seeks to approximate the posterior distribution of the latent factors.", "startOffset": 45, "endOffset": 132}, {"referenceID": 7, "context": "The other method is the generative adversarial network (GAN) (Goodfellow et al., 2014; Denton et al., 2015; Radford, Metz, and Chintala, 2016), where the assisting network is a discriminator network that plays an adversarial role against the generator network.", "startOffset": 61, "endOffset": 142}, {"referenceID": 2, "context": "The other method is the generative adversarial network (GAN) (Goodfellow et al., 2014; Denton et al., 2015; Radford, Metz, and Chintala, 2016), where the assisting network is a discriminator network that plays an adversarial role against the generator network.", "startOffset": 61, "endOffset": 142}, {"referenceID": 33, "context": "One can also obtain a probabilistic model based on a bottomup ConvNet that defines descriptive features (Xie et al., 2016; Lu, Zhu, and Wu, 2016).", "startOffset": 104, "endOffset": 145}, {"referenceID": 28, "context": "The factor analysis model can be learned by the RubinThayer EM algorithm, which involves alternating regressions of Z on Y in the E-step and of Y on Z in the M-step, with both steps powered by the sweep operator (Rubin and Thayer, 1982; Liu, Rubin, and Wu, 1998).", "startOffset": 212, "endOffset": 262}, {"referenceID": 23, "context": "(Olshausen and Field, 1997), d > D, and Z is assumed to be a redundant but sparse vector, i.", "startOffset": 0, "endOffset": 27}, {"referenceID": 15, "context": "(3) Non-negative matrix factorization (Lee and Seung, 2001), it is assumed that zk \u2265 0.", "startOffset": 38, "endOffset": 59}, {"referenceID": 7, "context": "In this paper, we consider the generator network model (Goodfellow et al., 2014) that retains the assumptions that d < D, Z \u223c N(0, Id), and \u223c N(0, \u03c3ID) as in traditional factor analysis, but generalizes the linear mapping WZ to a non-linear mapping f(Z;W ), where f is a ConvNet, and W collects all the connection weights and bias terms of the ConvNet.", "startOffset": 55, "endOffset": 80}, {"referenceID": 35, "context": "The stochastic gradient algorithm of (Younes, 1999) can be used for learning, where in each iteration, for each Zi, only a single copy of Zi is sampled from p(Zi|Yi,W ) by running a finite number of steps of Langevin dynamics starting from the current value of Zi, i.", "startOffset": 37, "endOffset": 51}, {"referenceID": 22, "context": "The Langevin dynamics can be extended to Hamiltonian Monte Carlo (Neal, 2011) or more sophisticated versions (Girolami and Calderhead, 2011).", "startOffset": 65, "endOffset": 77}, {"referenceID": 6, "context": "The Langevin dynamics can be extended to Hamiltonian Monte Carlo (Neal, 2011) or more sophisticated versions (Girolami and Calderhead, 2011).", "startOffset": 109, "endOffset": 140}, {"referenceID": 30, "context": "The code in our experiments is based on the MatConvNet package of (Vedaldi and Lenc, 2015).", "startOffset": 66, "endOffset": 90}, {"referenceID": 9, "context": "We also adopt batch normalization (Ioffe and Szegedy, 2015).", "startOffset": 34, "endOffset": 59}, {"referenceID": 20, "context": "A sound signal can be treated as a one-dimensional texture image (McDermott and Simoncelli, 2011).", "startOffset": 65, "endOffset": 97}, {"referenceID": 34, "context": "2 (Maas, Hannun, and Ng, 2013; Xu et al., 2015).", "startOffset": 2, "endOffset": 47}, {"referenceID": 16, "context": "In the second experiment, we learn a model with d = 100 from 1000 face images randomly selected from the CelebA dataset (Liu et al., 2015).", "startOffset": 120, "endOffset": 138}, {"referenceID": 7, "context": "We also provide qualitative comparison with Deep Convolutional Generative Adversarial Net (DCGAN) (Goodfellow et al., 2014; Radford, Metz, and Chintala, 2016).", "startOffset": 98, "endOffset": 158}, {"referenceID": 4, "context": "We model a textured motion (Wang and Zhu, 2003) or a dynamic texture (Doretto et al., 2003) by a non-linear dynamic system", "startOffset": 69, "endOffset": 91}, {"referenceID": 4, "context": "This model is a direct generalization of the linear dynamic system of (Doretto et al., 2003), where Yt is reduced to Zt by principal component analysis (PCA) via singular value decomposition (SVD).", "startOffset": 70, "endOffset": 92}, {"referenceID": 4, "context": "(2) Treat {Zt} as the training data, learn A and Q as in (Doretto et al., 2003).", "startOffset": 57, "endOffset": 79}, {"referenceID": 4, "context": "The first row is a segment of the sequence generated by our model, and the second row is generated by the method of (Doretto et al., 2003), with the same dimensionality of Z.", "startOffset": 116, "endOffset": 138}, {"referenceID": 4, "context": "Row 2: a sequence by the method of (Doretto et al., 2003).", "startOffset": 35, "endOffset": 57}, {"referenceID": 31, "context": "Our experiments are different from (1) de-noising auto-encoder (Vincent et al., 2008), where the training images are fully observed, and noises are added as a matter of regularization, (2) in-painting or de-noising, where the prior model or regularization has already been learned or given.", "startOffset": 63, "endOffset": 85}, {"referenceID": 34, "context": "The element-wise non-linearity fl in modern ConvNet is usually the two-piece linearity, such as rectified linear unit (ReLU) (Krizhevsky, Sutskever, and Hinton, 2012) or the leaky ReLU (Maas, Hannun, and Ng, 2013; Xu et al., 2015).", "startOffset": 185, "endOffset": 230}, {"referenceID": 27, "context": "The generator model can be considered an explicit implementation of the local linear embedding (Roweis and Saul, 2000), where Z is the embedding of Y .", "startOffset": 95, "endOffset": 118}, {"referenceID": 29, "context": "From Rubin\u2019s multiple imputation point of view (Rubin, 2004) of the EM algorithm, the E-step of EM infers", "startOffset": 47, "endOffset": 60}, {"referenceID": 28, "context": "The EM algorithm for factor analysis can be interpreted as alternating linear regression (Rubin and Thayer, 1982; Liu, Rubin, and Wu, 1998).", "startOffset": 89, "endOffset": 139}], "year": 2016, "abstractText": "This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a nonlinear generalization of factor analysis. In this model, the mapping from the continuous latent factors to the observed signal is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dynamics or gradient descent. (2) Learning back-propagation, which updates the parameters given the inferred latent factors by gradient descent. The gradient computations in both steps are powered by back-propagation, and they share most of their code in common. We show that the alternating back-propagation algorithm can learn realistic generator models of natural images, video sequences, and sounds. Moreover, it can also be used to learn from incomplete or indirect training data.", "creator": "LaTeX with hyperref package"}}}