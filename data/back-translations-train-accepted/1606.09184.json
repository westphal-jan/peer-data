{"id": "1606.09184", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "Disease Trajectory Maps", "abstract": "Medical researchers are coming to appreciate that many diseases are in fact complex, heterogeneous syndromes composed of subpopulations that express different variants of a related complication. Time series data extracted from individual electronic health records (EHR) offer an exciting new way to study subtle differences in the way these diseases progress over time. In this paper, we focus on answering two questions that can be asked using these databases of time series. First, we want to understand whether there are individuals with similar disease trajectories and whether there are a small number of degrees of freedom that account for differences in trajectories across the population. Second, we want to understand how important clinical outcomes are associated with disease trajectories. To answer these questions, we propose the Disease Trajectory Map (DTM), a novel probabilistic model that learns low-dimensional representations of sparse and irregularly sampled time series. We propose a stochastic variational inference algorithm for learning the DTM that allows the model to scale to large modern medical datasets. To demonstrate the DTM, we analyze data collected on patients with the complex autoimmune disease, scleroderma. We find that DTM learns meaningful representations of disease trajectories and that the representations are significantly associated with important clinical outcomes.", "histories": [["v1", "Wed, 29 Jun 2016 17:06:45 GMT  (1245kb,D)", "http://arxiv.org/abs/1606.09184v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.AP", "authors": ["peter schulam", "raman arora"], "accepted": true, "id": "1606.09184"}, "pdf": {"name": "1606.09184.pdf", "metadata": {"source": "CRF", "title": "Disease Trajectory Maps", "authors": ["Peter Schulam"], "emails": ["pschulam@cs.jhu.edu", "arora@cs.jhu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, we are talking about just under half a million euros."}, {"heading": "1.1 Background and Related Work", "text": "The result is that time series are often sampled irregularly (the time between samples varies within and across individuals) and can therefore be extremely sparse (it is not uncommon to have a single observation for an individual).To support the following discussion, we briefly present a notation for this type of data.We use m to name the number of individual disease progressions recorded in a given dataset.For each individual, we use ni to name the number of observations. We collect the observation times for subjects i into a column vector ti (sorted in no decreasing order) and the corresponding measurements into a column vector yi: ti, [ti1,., tini] > and yi, [yi1,."}, {"heading": "2 Disease Trajectory Maps", "text": "To motivate the disease trajectory maps, we start with the reduced ranking of linear mixed models, as proposed by James et al. [2000]. In particular, we allow \u00b5-R to be the marginal mean of the observations, where F-Rd \u00b7 q can be a Rangq matrix and \u03c32 can be the variance of measurement errors. Remember: yi-Rni denotes the vector of the observed orbit measurements, Bi-Rni \u00b7 d denotes the design matrix of the subject, and xi-Rq denotes the representation or embedding of the subject. We start with the reduced ranking model: yi | Bi, xi, F, \u03c32 \u2012 N (BiFxi, 2Ini). In the reduced ranking model, we assume that we have an isotropic standard before xi and marginalization in order to obtain the observed data."}, {"heading": "2.1 Learning and Inference in the DTM", "text": "In this section we approach the logarithm of the techniques of Hensman et al. [2013], which allows us to apply stochastic conclusions (SVI). [Hoffman et al., 2013]. [Hoffman et al.].Recent work in scaling Gaussian processes to large datasets to inducational points [Snelson and Ghahramani, 2005, Titsias, 2009], which are a relatively small number of the Gaussian process that act as a bottleneck and circulated information in the data. [Snelson and Ghahramani, 2005, Titsias, 2009], which are a relatively small number of the Gaussian process that act as a large datasets."}, {"heading": "3 Experiments", "text": "We use DTM to analyze clinical marker histories of individuals with the autoimmune disease scleroderma [Allanore et al., 2015]. Scleroderma is a heterogeneous and complex chronic autoimmune disease that can potentially affect many of the visceral organs, such as the heart, lungs, kidneys, and vessels. Each individual can experience only a subset of complications, and the timing of symptoms relative to the onset of the disease can vary considerably from individual to individual. Furthermore, there are no known biomarkers that accurately predict the disease progression of an individual. Clinicians and medical researchers are therefore interested in characterizing and understanding disease progression. In addition, there are a number of clinical outcomes that account for the majority of morbidity in patients with scleroderma, including constipated heart failure, pulmonary hypertension, and arterial arterial hypertension, complications, and varsitis []."}, {"heading": "3.1 Experimental Setup", "text": "For our experiments, we extract trajectories from one of the largest scleroderma patient registries in the nation. For both PFVC and TSS, we examine the trajectories from the time of the first symptom to ten years of follow-up. The PFVC dataset contains trajectories for 2,323 individuals and the TSS dataset contains 2,239 individuals. The mean number of observations per individual is 3 for the PFVC data and 2 for the TSS data. The maximum number of observations is 55 and 22 for both PFVC and TSS. We present two sets of results. In the first, we visualize groups of similar trajectories obtained through clusters of representations acquired through DTM. Although not quantitative, we use these visualizations as a way to verify that the DTM detects subpopulations that correspond to what is currently known about scleroderma."}, {"heading": "3.2 Qualitative Analysis of Representations", "text": "We study these disadvantages for both the PFVC and TSS datasets in order to verify consistency with what is currently known about the pathways of scleroderma. In Figure 1 (A), we show groups of pathways revealed by clusters of learned pathways shown in Figure 1 (B). Many of the groups shown here agree with other work on subtypes of scleroderma lung disease (e.g. Schulam et al. [2015]). In particular, we see rapidly declining pathways (group [5]), slowly declining pathways (group [22]), recovering pathways (group [23]) and stable pathways (group [34]). Surprisingly, we also see a group of individuals that we describe as \"late declines\" (group [28]). These individuals are progressing for the first 5 years, but are currently stable for the 6-11 years."}, {"heading": "3.3 Associations between Representations and Clinical Outcomes", "text": "In order to quantify the low-dimensional representations learned through the DTM, we statistically test the relationships between the representations of the clinical marker trajectories and important clinical outcomes. We compare the conclusions of the hypotheses test with those made using representations from the LFPM and FPCA baselines. To determine that the models are all equally meaningful and achieve comparable generalization errors, we present the data log liquidities presented in Table 1, which are estimated using a 10-fold cross-validation. We see that the models are all consistent with generalization errors.To test the correlations between clinical outcomes and learned representations, we use a kernel density estimator test [DCP], which estimates the hyperfixation of the skin monotheses relationship between the clinical outcomes and the learned representations (DCP) by only 0.0- 0.0- 0.0- 0.0- 0- 0- 0- 0- 0- 0- 0- 0- ------------------------------------------------------------------------------------------------------------------------- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- ----------------------------------------------------------------------------------------------------------------- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- 0,0- ----------------------------------------------------------------------------------------------------------------- 0,0- 0,0- 0,0- 0,0- 0,0- 0,"}, {"heading": "4 Conclusion", "text": "We present the Disease Trajectory Map (DTM), a novel probability model that learns the low-dimensional embedding of sparse and irregularly sampled clinical time series data. DTM is a reformulation of the LMM that focuses on the representations that the model learns, a view comparable to that taken by Lawrence [2004] in deriving the Gaussian process latent variable model (GPLVM) from the probabilistic principal component analysis (PPCA) [Tipping and Bishop, 1999], and in fact the DTM can be interpreted as a \"twin core\" GPLVM (briefly discussed in the final paragraphs) on functional observations. DTM can also be considered as an LMM with a \"distorted\" Gaussian precursor on the random effects (see e.g. Damianou et al. [2015] for a discussion of the distributions caused by random Gaussian variables mapping)."}, {"heading": "A Derivation of Evidence Lower Bound", "text": "When we talk about the lines of the Gaussian processes, we have a Gaussian process about the trajectories, but we do this implicitly to induce a Gaussian process about the subject-specific base coefficients. (F) We see that the vector of the coefficients w:, k: a) We have a Gaussian process distribution with the mean 0 and covarianceCov (wik, wjk) = (14) Moreover, the Gaussian processes about the coefficients are statistically independent of each other. To construct our approximate objective, we must favor each of the d-coefficient Gaussian processes by introducing points (see e.g. Gaussian processes)."}, {"heading": "B Optimizing the Evidence Lower Bound", "text": "In this section and in our experiments, we assume that the kernel has a radial basic function (RBF) with scale \u03b1 and length scale (or bandwidth). \"We assume that the normal distributions are over two, three, and\" with the mean parameters ms, ma, m \"or precision parameters (RBF), each with a scale of two, three, and four (or bandwidth). Our goal is the thereforeJSA-DTM (m, m1: m, S1: m, \u00b5, 2, \u03b1\"). We: \"2 Eq (xi). (Bi, p, p).We:.\" (Bi, p). (Bi, p, p)."}], "references": [{"title": "Systemic sclerosis. Nature Reviews Disease Primers, page", "author": ["Allanore"], "venue": null, "citeRegEx": "Allanore,? \\Q2015\\E", "shortCiteRegEx": "Allanore", "year": 2015}, {"title": "Time series: data analysis and theory, volume 36", "author": ["Carvalho"], "venue": "American Statistical Association,", "citeRegEx": "Carvalho,? \\Q2012\\E", "shortCiteRegEx": "Carvalho", "year": 2012}, {"title": "The varimax criterion for analytic rotation in factor analysis", "author": ["E. Keogh"], "venue": null, "citeRegEx": "Keogh,? \\Q2000\\E", "shortCiteRegEx": "Keogh", "year": 2000}, {"title": "Experiencing SAX: a novel symbolic representation", "author": ["Jessica Lin", "Eamonn Keogh", "Li Wei", "Stefano Lonardi"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2013}, {"title": "Unsupervised pattern discovery in electronic health care data using probabilistic clustering", "author": ["B.M. Marlin"], "venue": null, "citeRegEx": "Marlin,? \\Q2009\\E", "shortCiteRegEx": "Marlin", "year": 2009}, {"title": "Applied functional data analysis: methods and case studies", "author": ["James Ramsay"], "venue": "Proc. ACM SIGHIT International Health Informatics Symposium,", "citeRegEx": "Ramsay,? \\Q2012\\E", "shortCiteRegEx": "Ramsay", "year": 2012}, {"title": "Subtyping: What it is and its role in precision medicine", "author": ["S. Saria", "A. Goldenberg"], "venue": "Int. Sys.,", "citeRegEx": "Saria and Goldenberg.,? \\Q2015\\E", "shortCiteRegEx": "Saria and Goldenberg.", "year": 2015}, {"title": "Clustering longitudinal clinical marker trajectories from electronic health", "author": ["P. Schulam", "F. Wigley", "S. Saria"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Schulam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulam et al\\.", "year": 2015}, {"title": "Applications to phenotyping and endotype discovery", "author": ["E. Snelson", "Z. Ghahramani"], "venue": "In AAAI,", "citeRegEx": "Snelson and Ghahramani.,? \\Q2005\\E", "shortCiteRegEx": "Snelson and Ghahramani.", "year": 2005}, {"title": "Variational learning of inducing variables in sparse gaussian processes", "author": ["B. Varadarajan"], "venue": "Series B (Statistical Methodology),", "citeRegEx": "Varadarajan,? \\Q1999\\E", "shortCiteRegEx": "Varadarajan", "year": 1999}, {"title": "Linear mixed models for longitudinal data", "author": ["G. Verbeke", "G. Molenberghs"], "venue": null, "citeRegEx": "Verbeke and Molenberghs.,? \\Q2009\\E", "shortCiteRegEx": "Verbeke and Molenberghs.", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "Saria and Goldenberg [2015]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 6, "context": "Schulam et al. [2015]).", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Allanore et al. [2015]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "In the statistics literature, trajectory data is often referred to as unbalanced longitudinal data, and it is commonly analyzed in that community using linear mixed models (LMMs) [Verbeke and Molenberghs, 2009].", "startOffset": 179, "endOffset": 210}, {"referenceID": 1, "context": "Carvalho et al. [2012]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "trajectories) assumed to be sampled from a stochastic process and the goal is to find a parsimonious representation for the data [Ramsay et al., 2002]. Functional principal component analysis (FPCA), one of the most standard techniques in functional data analysis, expresses functional data in the orthonormal basis given by the eigenfunctions of the auto-covariance operator. This representation is optimal in the sense that no other representation captures more variation [Ramsay, 2006]. The idea itself can be traced back to early independent work by Karhunen and Loeve and is also referred to as the Karhunen-Loeve expansion [Watanabe, 1965]. While numerous variants of FPCA have been proposed, the one that is most relevant to the problem at hand is that of sparse FPCA [Castro et al., 1986, Rice and Wu, 2001] where we allow sparse irregularly sampled data as in longitudinal data analysis. To deal with the sparsity, Rice and Wu [2001] proposed the mixed effect model which leverages statistical strength from all observations for function estimation.", "startOffset": 130, "endOffset": 943}, {"referenceID": 2, "context": "trajectories) assumed to be sampled from a stochastic process and the goal is to find a parsimonious representation for the data [Ramsay et al., 2002]. Functional principal component analysis (FPCA), one of the most standard techniques in functional data analysis, expresses functional data in the orthonormal basis given by the eigenfunctions of the auto-covariance operator. This representation is optimal in the sense that no other representation captures more variation [Ramsay, 2006]. The idea itself can be traced back to early independent work by Karhunen and Loeve and is also referred to as the Karhunen-Loeve expansion [Watanabe, 1965]. While numerous variants of FPCA have been proposed, the one that is most relevant to the problem at hand is that of sparse FPCA [Castro et al., 1986, Rice and Wu, 2001] where we allow sparse irregularly sampled data as in longitudinal data analysis. To deal with the sparsity, Rice and Wu [2001] proposed the mixed effect model which leverages statistical strength from all observations for function estimation. The mixed effect model often suffers from numerical instability of covariance matrices in high dimensions; James et al. [2000] addressed this by constraining the rank of the covariance matrices\u2014this is often referred to as the reduced rank model.", "startOffset": 130, "endOffset": 1186}, {"referenceID": 2, "context": "trajectories) assumed to be sampled from a stochastic process and the goal is to find a parsimonious representation for the data [Ramsay et al., 2002]. Functional principal component analysis (FPCA), one of the most standard techniques in functional data analysis, expresses functional data in the orthonormal basis given by the eigenfunctions of the auto-covariance operator. This representation is optimal in the sense that no other representation captures more variation [Ramsay, 2006]. The idea itself can be traced back to early independent work by Karhunen and Loeve and is also referred to as the Karhunen-Loeve expansion [Watanabe, 1965]. While numerous variants of FPCA have been proposed, the one that is most relevant to the problem at hand is that of sparse FPCA [Castro et al., 1986, Rice and Wu, 2001] where we allow sparse irregularly sampled data as in longitudinal data analysis. To deal with the sparsity, Rice and Wu [2001] proposed the mixed effect model which leverages statistical strength from all observations for function estimation. The mixed effect model often suffers from numerical instability of covariance matrices in high dimensions; James et al. [2000] addressed this by constraining the rank of the covariance matrices\u2014this is often referred to as the reduced rank model. The reduced rank model was further extended by Zhou et al. [2008] to a two-dimensional sparse principal component model.", "startOffset": 130, "endOffset": 1372}, {"referenceID": 2, "context": "Marlin et al. [2012] cluster time series data from the ICU using a mixture model and use cluster membership to predict outcomes.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Marlin et al. [2012] cluster time series data from the ICU using a mixture model and use cluster membership to predict outcomes. Schulam and Saria [2015] describe a probabilistic model that represents trajectories using a hierarchy of features, which includes \u201csubtype\u201d or cluster membership.", "startOffset": 0, "endOffset": 154}, {"referenceID": 2, "context": "Marlin et al. [2012] cluster time series data from the ICU using a mixture model and use cluster membership to predict outcomes. Schulam and Saria [2015] describe a probabilistic model that represents trajectories using a hierarchy of features, which includes \u201csubtype\u201d or cluster membership. LMMs have also been extended to have nonparametric Dirichlet process priors over the coefficients (e.g. Kleinman and Ibrahim [1998]), which implicitly induce clusters in the data.", "startOffset": 0, "endOffset": 425}, {"referenceID": 2, "context": "Marlin et al. [2012] cluster time series data from the ICU using a mixture model and use cluster membership to predict outcomes. Schulam and Saria [2015] describe a probabilistic model that represents trajectories using a hierarchy of features, which includes \u201csubtype\u201d or cluster membership. LMMs have also been extended to have nonparametric Dirichlet process priors over the coefficients (e.g. Kleinman and Ibrahim [1998]), which implicitly induce clusters in the data. Although these approaches flexibly model trajectory data, the structure they recover is a partition, which does not allow us to compare all trajectories in a coherent way as we can in a vector space. Lexicon-based representations. Another line of research has investigated the discovery of motifs or repeated patterns in continuous time-series data for the purposes of succinctly representing the data as a string of elements of the discovered lexicon. These include efforts in the speech processing community to identify sub-word units (parts of the words at the same level as phonemes) in a datadriven manner [Varadarajan et al., 2008, Levin et al., 2013]. In computational healthcare, Saria et al. [2011] propose a method for discovering deformable motifs that are repeated in continuous time-series data.", "startOffset": 0, "endOffset": 1181}, {"referenceID": 8, "context": "Recent work in scaling Gaussian processes to large datasets focuses on the idea of inducing points [Snelson and Ghahramani, 2005, Titsias, 2009], which are a relatively small number of artificial observations of the Gaussian process that act as a bottleneck and approximately capture the information contained in the training data. Let f \u2208 R denote observations of the GP at inputs {xi}i=1 and u \u2208 R denote inducing points at inputs {zi} p i=1. Titsias [2009] constructs the inducing points as variational parameters by introducing an augmented probability model: u \u223c N (0,Kpp) , f | u \u223c N (KmpK pp u, K\u0303mm), (6)", "startOffset": 100, "endOffset": 460}, {"referenceID": 7, "context": "Schulam et al. [2015]).", "startOffset": 0, "endOffset": 22}], "year": 2016, "abstractText": "Medical researchers are coming to appreciate that many diseases are in fact complex, heterogeneous syndromes composed of subpopulations that express different variants of a related complication. Time series data extracted from individual electronic health records (EHR) offer an exciting new way to study subtle differences in the way these diseases progress over time. In this paper, we focus on answering two questions that can be asked using these databases of time series. First, we want to understand whether there are individuals with similar disease trajectories and whether there are a small number of degrees of freedom that account for differences in trajectories across the population. Second, we want to understand how important clinical outcomes are associated with disease trajectories. To answer these questions, we propose the Disease Trajectory Map (DTM), a novel probabilistic model that learns low-dimensional representations of sparse and irregularly sampled time series. We propose a stochastic variational inference algorithm for learning the DTM that allows the model to scale to large modern medical datasets. To demonstrate the DTM, we analyze data collected on patients with the complex autoimmune disease, scleroderma. We find that DTM learns meaningful representations of disease trajectories that the representations are significantly associated with important clinical outcomes.", "creator": "LaTeX with hyperref package"}}}