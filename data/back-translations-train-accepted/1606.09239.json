{"id": "1606.09239", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "Learning Concept Taxonomies from Multi-modal Data", "abstract": "We study the problem of automatically building hypernym taxonomies from textual and visual data. Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Instead, we propose a probabilistic model for taxonomy induction by jointly leveraging text and images. To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representations of images and words. The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evaluate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap.", "histories": [["v1", "Wed, 29 Jun 2016 19:52:53 GMT  (630kb,D)", "http://arxiv.org/abs/1606.09239v1", "To appear in ACL 2016"]], "COMMENTS": "To appear in ACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG", "authors": ["hao zhang", "zhiting hu", "yuntian deng", "mrinmaya sachan", "zhicheng yan", "eric p xing"], "accepted": true, "id": "1606.09239"}, "pdf": {"name": "1606.09239.pdf", "metadata": {"source": "CRF", "title": "Learning Concept Taxonomies from Multi-modal Data", "authors": ["Hao Zhang", "Zhiting Hu", "Yuntian Deng", "Mrinmaya Sachan", "Zhicheng Yan", "Eric P. Xing"], "emails": ["hao@cs.cmu.edu", "zhitingh@cs.cmu.edu", "yuntiand@cs.cmu.edu", "mrinmays@cs.cmu.edu", "epxing@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to abide by the rules they have established in relation to their work on themselves."}, {"heading": "2 Related Work", "text": "Many approaches have recently been developed that build hierarchies by identifying either lexical patterns or statistical features in text corpora (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Zhu et al., 2013; Fu et al., 2014; Tuan et al., 2014; Tuan et al., 2015; Kiela et al., 2015). The approaches in Yang and Callan (2009) and Snow et al., (2006) start from an incomplete hierarchy and attempt to expand it by inserting new terms. Kozareva and Hovy (2010) and Navigli et al. (2011) first find leaf nodes and then use lexical patterns to find intermedial terms and all certified hypernistic terms."}, {"heading": "3 Taxonomy Induction Model", "text": "Our model is motivated by the key observation that in a semantically significant taxonomy, a category tends to be closely related to its children and siblings. Thus, for example, there is a hypernym-hyponym relationship between the name of the category shark and that of its parent fish. Furthermore, images of sharks tend to be visually similar to those of rays, which are both marine fish. Therefore, our model is designed to promote such local semantic consistency; and by taking all categories into account in the conclusion, a globally optimal structure is achieved. A key advantage of the model is that we include both visual and textual characteristics that arise from distributed representations of images and text (Section 4). These characteristics capture the rich underlying semantics and facilitate taxonomy induction. Furthermore, we distinguish the relative importance of visual and textual characteristics that may be similar in different layers of a taxonomy (general intuitive characteristics are increasingly similar in the deeper categories)."}, {"heading": "3.1 The Problem", "text": "Let us assume a series of N-categories x = {x1, x2,.., xN}, each category xn consisting of a text term tn as a name and a series of images in = {i1, i2,...}. Our goal is to construct a taxonomy tree T above these categories, so that categories of specific object types (e.g. sharks) are grouped and assigned to general concepts (e.g. marine fish). Since the categories in x can come from several fragmented taxonomy trees, we add a pseudo-category x0 as hyperroot, so that the optimal taxonomy is ensured as a single tree. Let us leave zn,..."}, {"heading": "3.2 Model", "text": "We formulate the distribution p (z) x (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n) n (n) n (n) n (n) n (n) n) n (n) n (n) n (n) n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n) n (n) n (n) n (n) n) n (n) n (n) n (n) n (n) n) n (n) n (n (n) n (n) n (n) n) n (n) n (n (n) n (n) n (n) n) n (n (n) n) n (n) n (n) n) n (n (n) n) n (n) n (n) n (n (n) n (n) n (n) n (n) n) n) n (n"}, {"heading": "4 Features", "text": "In this section, we describe the character vector f used in our model and move further details into the complementary material. Compared to previous taxonomic induction work, which relied on purely linguistic information, we use both perceptual and text features to capture the rich spectrum of image and text encoded semantics. In addition, we use the distributed representations of images and words to construct compact and effective characteristics. Specifically, each image i is represented as an embedding vector vi-Ra extracted by deep revolutionary neural networks, which has been successfully used in various visual tasks. On the other hand, the category name t is represented by its word \"embedding vt,\" a low-dimensional dense vector induced by the Skip-gram model (Mikolov etal., 2013), which is also widely used in various NLP applications. Then, we design f (xn, xcategory, textual model\\ n) based on the parent scale and xox."}, {"heading": "4.1 Image Features", "text": "As already mentioned, close neighbors in a taxonomy tend to be visually similar, suggesting that the embedding of images of sibling categories should be close to each other in the vector space Ra. For a category xn and its set image, we adjust the visual similarity between the image vectors in which Vin, Vin, Visa, Ra, the middle vector and Vim are the covariance matrix. For a sibling category xm of xn, we define the visual similarity between xn and xm asvissim, where the visual vectors Vim, Vim, Vim, n and N are the covariance matrix. For a sibling category xm of xn, we define the visual similarity between xn and xm asvissim, the visual similarity between the image vectors (xn, xm, xm, xm), the visual vector, vector, vector, vector, and vector, and vector in the comatrix, is N)."}, {"heading": "4.2 Word Features", "text": "More details on text property extraction can be found in the supplementary material. Text Embedding Functions. PC-V1, We induce functions that use word vectors to measure sibling and parent-child proximity in the text area (Fu et al., 2014). An exception is that, since each category has only one word, sibling similarity is calculated as a cosinal distance between two word vectors (instead of center vectors). This results in two additional features, namely the word-word kinship characteristic (PC-T1) and the word-word kinship characteristic of the siblings (S-T1). Word surface characteristics. In addition to the embedding-based characteristics, we use other lexical characteristics based on the surface shapes of child-parent category names. Specifically, we use uppercase, endings with, content, suffix, Length and other common features of 2009 and Yength."}, {"heading": "5 Experiments", "text": "First, we disclose our implementation details in Section 5.1 and the supplementary material for the reproducibility of bets. Then, we compare our model with previous state-of-the-art methods (Fu et al., 2014; Bansal et al., 2014) with two taxonomic induction tasks. Finally, we analyze the induced weights and taxonomies."}, {"heading": "5.1 Implementation Details", "text": "We conduct our experiments with the ImageNet2011 dataset (Deng et al., 2009), which overlays a large collection of category elements (synsets) with associated images and a label hierarchy (sampled from WordNet).The original ImageNet taxonomy is pre-processed, resulting in a tree structure with 28231 nodes. Word embedding training. We train Word embedding for synsets by replacing every word / phrase in a synset with a unique token and then using Google's word2vec tool (Mikolov et al., 2013).We combine three publicly available companies with each other, including the latest Wikipedia dump (Wikipedia, 2014), the One Billion Word Language Modeling Benchmark (Chelba et al., 2013) and the UMBC Webbase tool Corpus (Han et al., 2013), which adds a total of 12 billion to the corpus of the SVIL6."}, {"heading": "5.2 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 Experimental Settings", "text": "We evaluate our model using three sub-trees taken from the ImageNet taxonomy. To collect the sub-trees, we start from a specific root (e.g. consumer goods) and traverse the complete taxonomy using BFS and collect all subsequent nodes within a depth h (number of nodes in the longest path). We vary to obtain a series of sub-trees with increasing height h {4, 5, 6, 7} and different yardsticks (maximum 1326 nodes) in different domains. Statistics of the assessment sets are in Table 1. To avoid ambiguity, all nodes used in ILSVRC 2012 are removed as the CNN feature extractor is trained on them. We design two different tasks to evaluate our model. (1) In the hierarchy completion task, we randomly remove some nodes from a tree and use the remaining hierarchy for training."}, {"heading": "5.2.2 Results", "text": "We compare the following three systems: (1) Fu20144 (Fu et al., 2014); (2) Our model with both language characteristics and visual characteristics 5. The average performance on three trees is shown in Table 2. We observe that performance gradually decreases as the number of nodes increases, resulting in a more complex and difficult taxonomy. Our model with both language characteristics and visual characteristics 5. The average performance on three trees is shown in Table 2. We observe that performance gradually decreases as more nodes are inserted, leading to a higher and more difficult taxonomy. Overall, our model outperforms Fu2014 in terms of the F1 score, even without visual characteristics. In the most difficult case, with h = 7, our model still maintains an F1 score of 0.42 (2 x Fuhierarchy), demonstrating the superiority of our design strategy."}, {"heading": "5.3 Qualitative Analysis", "text": "This year, it has come to the point that there is only one occasion when there is a scandal before there is a scandal."}, {"heading": "6 Conclusion", "text": "In this paper, we examine the problem of automatically generating semantically meaningful concept taxonomies from multimodal data. We propose a probabilistic Bayesian model that uses distributed representations for images and words. We compare our model and its features with previous ones to two different tasks using the ImageNet hierarchies, and demonstrate the superior performance of our model and the effectiveness of using visual content for taxonomy generation. We continue to conduct qualitative studies and distinguish the relative importance of visual and textual features in the construction of different parts of a taxonomy."}, {"heading": "Acknowledgements", "text": "We thank anonymous reviewers for their valuable feedback and Mohit Bansal for helpful suggestions. We thank NVIDIA for GPU donations. The work is supported by NSF Big Data IIS1447676."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We study the problem of automatically building hypernym taxonomies from textual and visual data. Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Instead, we propose a probabilistic model for taxonomy induction by jointly leveraging text and images. To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representations of images and words. The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evaluate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap.", "creator": "TeX"}}}