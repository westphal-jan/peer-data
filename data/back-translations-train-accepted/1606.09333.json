{"id": "1606.09333", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Dimension-Free Iteration Complexity of Finite Sum Optimization Problems", "abstract": "Many canonical machine learning problems boil down to a convex optimization problem with a finite sum structure. However, whereas much progress has been made in developing faster algorithms for this setting, the inherent limitations of these problems are not satisfactorily addressed by existing lower bounds. Indeed, current bounds focus on first-order optimization algorithms, and only apply in the often unrealistic regime where the number of iterations is less than $\\mathcal{O}(d/n)$ (where $d$ is the dimension and $n$ is the number of samples). In this work, we extend the framework of (Arjevani et al., 2015) to provide new lower bounds, which are dimension-free, and go beyond the assumptions of current bounds, thereby covering standard finite sum optimization methods, e.g., SAG, SAGA, SVRG, SDCA without duality, as well as stochastic coordinate-descent methods, such as SDCA and accelerated proximal SDCA.", "histories": [["v1", "Thu, 30 Jun 2016 03:10:54 GMT  (223kb,D)", "http://arxiv.org/abs/1606.09333v1", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG math.NA", "authors": ["yossi arjevani", "ohad shamir"], "accepted": true, "id": "1606.09333"}, "pdf": {"name": "1606.09333.pdf", "metadata": {"source": "CRF", "title": "Dimension-Free Iteration Complexity of Finite Sum Optimization Problems", "authors": ["Yossi Arjevani", "Ohad Shamir"], "emails": ["yossi.arjevani@weizmann.ac.il", "ohad.shamir@weizmann.ac.il"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us are able to go in search of a solution."}, {"heading": "2 Framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Motivation", "text": "However, due to the difficulties encountered in investigating the complexity of general optimization problems within the framework of discrete calculation models, it is customary to analyze the computational hardness of optimization algorithms by modelling the way in which a given algorithm interacts with problem cases (without limiting its computational resources).In the pioneering work of Nemirovsky and Yudin [12] it is shown that algorithms that access the given method require at least an equivalent solution solely by querying a primary oracle (min {d, \u0430} ln (1 /), \u00b5 > 0 (3)."}, {"heading": "2.2 Definitions", "text": "When considering the lower boundaries, one must be very precise about the scope of optimization algorithms to which they are applied. (In the following, we give formal definitions for oblivious stochastic CLI optimization algorithms and iteration complexity (which serve as rough proxy for their computational complexity).Definition 1 (class of optimization problems).Definition 1 (class of optimization problems).Definition 2 (class of optimization problems).Definition 1 (class of optimization problems).Definition 1 (class of optimization problems).Definition 2 (class of optimization problems).Definition 2 (class of optimization problems).Definition 2 (class of optimization problems).Definition 2 (class of optimization problems).Definition 1 (class of optimization problems).Definition.Definition.Definition.1 (class of optimization problems).Definition.Definition.1 (class of optimization problems).Definition.Definition.1 (class of optimization problems).Definition.1 (class of optimization problems).Definition.Definition.1 (class of optimization problems).Definition.1 (class of optimization problems).1 (class of optimier.Optimier.Optimier.Optimier..Optimier.....Optimier...Optimier....Optimier...Optimier....Optimier...Optimier.......Optimier.....Optimier.........Optimier...........Optimier.............Optimier....Optimier............Optimier.....................Optimier.............................Optimier..........Optimier..."}, {"heading": "2.3 Proof Technique - Deriving Lower Bounds via Approximation Theory", "text": "Consider the following parameterized class of L-smooth and p-strongly convex optimization problems, min x-Rf\u03b7 (x): p-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-Sp-"}, {"heading": "3 Lower Bound for Finite Sums Minimization Methods", "text": "After describing our analytical approach, we now turn to some concrete applications, starting with iteration complexity of lower limits in the context of FSM problems (1). In what follows, we deduce a lower limit on the iteration complexity of Oblivian (possibly stochastic) CLI algorithms equipped with the first order and coordinate descent oracle for FSM. (1) We focus on optimization algorithms equipped with two generalized first oracles, O (w; A, C, j) = A, fj (w) + C, A, B, C, Rd \u00b7 d, j [n] equipped with two generalized first oracles, O (w; A, C, J, J) = A, Bw + Bw + C, A, B, C, Rd \u00b7 d, j [n]."}, {"heading": "4 Lower Bound for Dual Regularized Loss Minimization with Linear Predictors", "text": "The form of the functions (12) discussed in the previous section does not easily adapt to the general RLM problems with linear predictors (i.e., min w-RdP (w): = 1n n-D = 1-D (< xi, w >) + \u03bb 2-W-2, (17) where the loss functions \u03c6i are smooth and convex, the samples x1,. \u2212 xn d-dimensional vectors in Rd and \u03bb are a positive constant. Therefore, dual methods that exploit the additional structure of this constellation through the dual problem [18], min \u03b1-RnD (\u03b1) = 1n n-D = 1-D (\u2212 \u03b1i) + 2-D-2-D."}, {"heading": "A Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Theorem 1", "text": "Evidence According to the way A generates iterates, for some polynomial sk (\u03b7) we have at most k a degree. For Lemma 6 we have a value of s (\u03b7), for Lemma 6 a value of s (\u03b7), for Pk."}, {"heading": "A.2 Proof of Theorem 2 - Finite Sums", "text": "When we deal with multivariate polynomials, it is convenient to define multi-indices i = (i1,..). (i1,..) When we deal with the multivariates, it is easier to define the multivariates. (i1,.) If we look at the multivariates in the multivariate distribution (.) as a whole, it is easier than if we apply the total regions across all terms. (1,.) If we apply the multivariate polynomials across the total regions (.) as a whole, it is less than or equal to k. (.) Finally, we give s (.) Pnk is the total regions of all multivariates, their total regions. (.) We assume that we define them. (.)"}, {"heading": "A.3 Proof of Theorem 3 - Smooth Functions", "text": "The following applicationPk: = {p-Pk | p-p (0) = p-p (0) = p-p (2) = p-p-p (25) = p-p-p (2) = p-p-p (0) = p-p (2) = p-p (2) = p-p (2) = p-p (2) = p-p (2) = p-p (1) = p-p (1) = p-p (0) = p-p (0) = p-p (0) = p-p (1) = p-p (1) = p-p (0)."}, {"heading": "A.4 Proof of Theorem 4 - Regularized Empirical Loss Minimization", "text": "For the sake of simplicity of presentation, we assume that these are non-negative values and non-negative values (0) \u2264 1. In addition, we assume that n = 1 is uniform and that L = 1 (the proof applies to odd n and general L > 0. First, we give an explicit definition of the parameterized function set on which we will focus, as well as the oracles below which our limits lie. We designate the set of all (1,..) that we do not define so that all entries are 0, except for some j [n / 2] for which we do not use any other values."}, {"heading": "A.5.1 Approximation w.r.t. L\u221e", "text": "Lemma 6 (b > a > 0 and c > \u2212 \u2212 b \u2212 a) (b + a + a) (b + a) (b \u2212 a) (b \u2212 a) (b + a) (b \u2212 a) (b \u2212 b) (b \u2212 a) (b \u2212 a) (b \u2212 a) (b \u2212 a) (b \u2212 a) (b \u2212 b) (b + c \u2212 a) (b + c \u2212 a) (b \u2212 b) (b \u2212 b) (b \u2212 b \u2212 a) (b \u2212 b) (b \u2212 b) (b \u2212 b) (b) (b \u2212 b) (b) (b \u2212 b) (b) (b \u2212 b) (b \u2212 b) (b \u2212 b) (b) (b) (b) (b) \u2212 b) (b) (b) (b) (b) (b) (b) \u2212 b) (b) (b) (b) (b) \u2212 b) (b) (b) \u2212 b) (b) (a) (b) (b) (b)) (b) (b) (b) (b)) (b) (b) (b)) (b) (b) (b) (b)) (b) (b) (b)) (b) (b) (b) (b)) (b) (b) (b)) (b) (b) (b) (b)) (b) (b) (b) (b)) (b) (b) (b) (b) (b)) (b) (b) (b) (b) (b) (b) (b)) (b) (b) (b) (b) (b) (b)) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b)) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b"}, {"heading": "A.5.2 Approximation w.r.t. L1", "text": "Let's leave the orthogonality of sgn (Uk) in relation to Pk \u2212 1 over [\u2212 1, 1]. Let's leave p (then) p = 1 over [\u2212 1, 1]. Lemma 7. Let's leave p (then) p (then) p (then) p (then) p (then) p (then) p (then) p (then) p (then) p (then) p (then) p (then) p (then) p (then) p). Let's first consider the orthogonality of sgn (Uk) in relation to Pk \u2212 1 over [\u2212 1, 1]. Lemma 7. Let's leave p (then) p \u2212 1, then (then) p (then) p (sgn) sgn (Uk)."}, {"heading": "A.5.3 Approximation w.r.t. L2", "text": "Lemma 9: 1 (\u2212 1, 0 \u2212 1), min s (p), p. 16], we havemin s (p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p (n), p), p), p (n), p (n), p (n), p), p), p (n), p), p), p), p), p), p (n), p), p (n), p), p), p), p), p), p), p)."}, {"heading": "A.6 Technical Lemmas", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>Many canonical machine learning problems boil down to a convex optimization problem with a finite<lb>sum structure. However, whereas much progress has been made in developing faster algorithms for<lb>this setting, the inherent limitations of these problems are not satisfactorily addressed by existing lower<lb>bounds. Indeed, current bounds focus on first-order optimization algorithms, and only apply in the often<lb>unrealistic regime where the number of iterations is less than O(d/n) (where d is the dimension and n<lb>is the number of samples). In this work, we extend the framework of Arjevani et al. [3, 5] to provide<lb>new lower bounds, which are dimension-free, and go beyond the assumptions of current bounds, thereby<lb>covering standard finite sum optimization methods, e.g., SAG, SAGA, SVRG, SDCA without duality, as<lb>well as stochastic coordinate-descent methods, such as SDCA and accelerated proximal SDCA.", "creator": "LaTeX with hyperref package"}}}