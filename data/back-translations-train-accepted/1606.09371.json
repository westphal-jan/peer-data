{"id": "1606.09371", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Recurrent neural network models for disease name recognition using domain invariant features", "abstract": "Hand-crafted features based on linguistic and domain-knowledge play crucial role in determining the performance of disease name recognition systems. Such methods are further limited by the scope of these features or in other words, their ability to cover the contexts or word dependencies within a sentence. In this work, we focus on reducing such dependencies and propose a domain-invariant framework for the disease name recognition task. In particular, we propose various end-to-end recurrent neural network (RNN) models for the tasks of disease name recognition and their classification into four pre-defined categories. We also utilize convolution neural network (CNN) in cascade of RNN to get character-based embedded features and employ it with word-embedded features in our model. We compare our models with the state-of-the-art results for the two tasks on NCBI disease dataset. Our results for the disease mention recognition task indicate that state-of-the-art performance can be obtained without relying on feature engineering. Further the proposed models obtained improved performance on the classification task of disease names.", "histories": [["v1", "Thu, 30 Jun 2016 07:15:56 GMT  (91kb,D)", "http://arxiv.org/abs/1606.09371v1", "This work has been accepted in ACL-2016 as long paper"]], "COMMENTS": "This work has been accepted in ACL-2016 as long paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sunil kumar sahu", "ashish anand"], "accepted": true, "id": "1606.09371"}, "pdf": {"name": "1606.09371.pdf", "metadata": {"source": "CRF", "title": "Recurrent neural network models for disease name recognition using domain invariant features", "authors": ["Sunil Kumar Sahu", "Ashish Anand"], "emails": ["anand.ashish}@iitg.ernet.in"], "sections": [{"heading": null, "text": "Recurrent neural network models for disease name recognition using domain invariant traits Sunil Kumar Sahu and Ashish Anand Department of Computer Science and EngineeringIndian Institute of Technology Guwahati Assam, India - 781039 {sunil.sahu, anand.ashish} @ iitg.ernet.in - AbstractHandmade traits based on language and domain knowledge play a critical role in determining the performance of disease name recognition systems. Such methods are further limited by the scope of these traits, or in other words, their ability to cover the context or word dependencies within a sentence. In this work, we focus on reducing such dependencies and propose a domain invariant framework for the task of disease name recognition. Specifically, we propose various end-to-end-to-end recurrent neurural network (RNN) models that are pre-defined based on disease classification and their four categories."}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Methods", "text": "We first give an overview of the complete model used for the two tasks, then we explain embedded features used in different models of neural networks. In Section 2.3 we give a brief description of different RNN models. Training and inference strategies are explained in Section 2.4."}, {"heading": "2.1 Model Architectures", "text": "We choose the BIO model of marking, where B stands for beginning, I for intermediate and O for outsider. In this way, we have two possible markers for all stakeholders, i.e. for all disease mentions, and a tag for other units. Generic neural architecture is presented in Figure 1. In the very first step, each word is assigned to its embedded characteristics. We call this layer an embedding layer, which acts as an input into hidden layers of the RNN model. We examine the three different RNN models and have briefly described them in Section 2.3. The output of the hidden layers is then fed to the output layer to compress the scores for all relevant markers (Collobert et al., 2011; Huang et al., 2015). In the output layer, we use the log paper to briefly describe all probabilities in Table 1."}, {"heading": "2.2 Features", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Distributed Word Representation (WE)", "text": "Distributed word representation or word embedding or simply word vector (Bengio et al., 2003; Collobert and Weston, 2008) is the technique for learning the vector representation of a word in a Givencorpus. Word vectors are present in columns of the matrix Mwe. We can obtain this vector by taking the product of the matrix Mwe and a hot vector of vi.w (i) = Mwe h (i) (1). Here, h (i) is the only hot vector that represents the ith word in V. We use pre-trained 50-dimensional word vectors learned by skipgram method on a biomedical corpus (Mikolov et al., 2013b; Mikolov et al., 2013a; TH et al., 2015)."}, {"heading": "Character Level Word Embedding (CE)", "text": "Word embedding preserve syntactic and semantic information well but does not capture morphologicaland shape information. However, such information can play an important role in the detection task of the disease. For example, letter -o- in the word gastroenteritis is used to combine different body parts for stomach, intestine and Itis. Therefore, it implies inflammation of the stomach and intestine, where -itis plays an important role in determining that it is actually a disease name.Character level word embedding was first introduced by (dos Santos and Zadrozny, 2014) with the motivation to capture word shape and morphological features in word embedding. Character level word embedding also automatically alleviates the problem of word embedding from vocabulary words, as we can embed each word by its characters. In this case, a vector for each sign in the corpus is initialized."}, {"heading": "2.3 Recurrent Neural Network Models", "text": "Recurrent Neural Network (RNN) is a class of artificial neural networks that uses sequential information and maintains the story through its intermediate layers (Graves et al., 2009; Graves, 2013). We experiment with three different variants of RNN, which are briefly described in the following sections."}, {"heading": "Bi-directional Recurrent Neural Network", "text": "In Bi-RNN, the context of the word is captured by past and future words. This is achieved by linking two hidden components in the intermediate layer and feeding them to the output level to obtain the score for all tags of the considered word. One component processes the information forward (from left to right) and another in the reverse direction. Subsequently, the output of these components is concatenated and fed to the output level to obtain the score for all tags of the considered word. Let x (t) is a feature vector for the tth word in the sentence (concatenation of the corresponding embedding features wti and yti) and h (t \u2212 1) l is the calculation of the last hidden state for (t) l: h (t) th word, then the calculation of the values of the hide and output layer would be: h (t) l = tanh (U lx (t) + h (t) l (h) l (h) (h) l (h) (l) (l) ()."}, {"heading": "Bi-directional Long Short Term Memory Network", "text": "Traditional RNN models suffer from both disappearing and exploding gradients (Pascanu et al., 2012; Bengio et al., 2013). Such models are likely to fail where we need longer contexts to get the job done. These questions were the main motivation behind the LSTM model (Hochreiter and Schmidhuber, 1997). LSTM layer is just another way to calculate a hidden state that introduces a new structure that includes a memory cell (ct) and three gates called input (it), output (ot) and forget.These gates consist of sigmoid activation function and are responsible for regulating information in the memory cell. The incoming gate, by allowing an incoming signal to change the state of the memory cell, regulates the ratio of history information storage cell. On the other hand, the output gate regulates the share of stored information in the memory cell and the state of the memory cell."}, {"heading": "Bi-directional Gated Recurrent Unit Network", "text": "A gated recurrent unit (GRU) was proposed by (Cho et al., 2014) to use any recurrent unit to adaptively capture dependencies of different time scales. The resulting model is simpler than the standard LSTM model. Let's follow the (Chung et al., 2014) model of GRU to transform the extracted word embedding and character embedding characteristics to point for all tags. Let's leave x (t) l (t) l (z) l \u2212 h (z \u2212 tanh) l (t \u2212 1) l is the calculation of the hidden state for (t \u2212 1) th word then the calculation of GRU: z (t) l = profile value (U (t) l (t) l (t) l (t) l (W) l l (W) l l (W) l l l (W) l l l (W) l l l (W) l l l l (W) l l l (l l l l l l l) l (W) l l (l l l l l) l (W) l (l l l) l (W) l (W) l (W) l l (l l l l (W) l l (W) l (W) l l (l l l l (W) l (l l l l l (l) l (W) l l (l l (l) l l l (l) l l (l) l (l l (l) l (l) l (l) l (l) l (l) l (l (l) l (l) l (l) l (l) l (l (l) l (l) l (l) l (l) l (l (l) l (l) l (l) l (l (l) l) l (l (l) l (l (l) l (l (l) l) l (l (l) l) l (l (l) l (l (l) l (l) l) l) l (l (l) l (l (l) l) l (l (l) l (l (l (l) l) l (l (l)) l (l (l)) l (l (l (l)))"}, {"heading": "2.4 Training and Inference", "text": "Equations 3, 4 and 5 are the values of all possible tags for the phrase of the test word. We follow the tenentele log-likelihood (SLL) approach (Collobert et al., 2011), which corresponds to the linear chain CRF, in order to derive the values of a certain tag sequence for the given word sequence. If [w] | s | 1 is the sentence and [t] | s | 1 is the tag sequence for which we want to find the common score, then the score for the entire sentence with the given tag sequence would be: s (w) | s | 1, [t] | 1) = \u2211 1 \u2264 i \u2264 | s | (W transti \u2212 1, ti + z (i) ti) ti) ti), (6) where W trans is transitional score matrix and W transi, j indicates the transition from day ti to tj."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Dataset", "text": "The NCBI dataset has been manually commented by a group of physicians to identify diseases and their types in biomedical articles. All disease mentions have been divided into four distinct categories, namely specific diseases, disease classes, compound diseases and modifiers. A word is commented upon as a specific disease when it refers to a particular disease. Category of diseases refers to a word that describes a family of many specific diseases, such as autoimmune diseases. A string that identifies two or more different disease mentions is commented upon with compound mentions. The modifier category indicates that disease mentions have been used as modifiers for other concepts. This dataset is an extension of the AZDC dataset (Leaman et al., 2009), which was commented upon only with disease mentions and not with their categories. Statistics of disease detection datasets are placed in Category A, where we will place the disease exactly in Category A: In Category A."}, {"heading": "4 Results and Discussion", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to adapt to people's needs, and that we are able to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position."}, {"heading": "Comparison with State-of-art", "text": "At the end, we compare our results with the results of statecraft presented in this dataset using BANNER (Leaman and Gonz\u00e1lez, 2008) in Table 7. BANNER is a CRF-based biounit recognition model that uses general linguistic, orthographic and syntactic dependency characteristics. However, although the result reported in (Dog-Lan and Lu, 2012) (F1 score = 81.8) is better than that of our RNN models, it should be noted that the competitive result (F1 score = 79.13%) is achieved by the proposed Bi-LSTM model, which does not depend on feature engineering or domain-specific resources and only uses uncontrolled word embedding functions on a huge body. For task B, we have not found any paper, except (Li, 2012). Li (2012) used linear soft margin support vector F1 with a number of SVM, including a number of SVM, were thrown by hand."}, {"heading": "5 Failure Analysis", "text": "To see exactly where our models failed to detect diseases, we carefully analyzed the results. We found that a significant proportion of the errors were due to the use of disease acronyms and the use of disease forms that are rare in our corpus. Examples of few such cases are \"CD,\" \"HNPCC,\" \"SCA1.\" We observed that this error occurs because we do not have an exact word embedding for these words. Most of the acronyms in the disease corpus were matched to the embedding of rare words.1 Another large proportion of the errors in our results are due to difficulties in recognizing nested forms of disease names, for example in all the following cases: \"hereditary forms of\" ovarian cancer, \"\" hereditary \"\" breast cancer, \"\" male and female \"breast cancer,\" part of the phrase such as ovarian cancer in hereditary forms of ovarian cancer, breast cancer and male and female breast cancer."}, {"heading": "6 Related Research", "text": "In the biomedical domain, so-called entity recognition has attracted a lot of attention for identifying entities such as genes and proteins (Settles, 2005; Leaman and Gonzalez, 2008; Leaman et al., 2009), but not so much for identifying disease names. Noteworthy work, as by Chowdhury and Lavelli (2010), are mainly contingent random task (CRF) based models that use many manually designed template texts, including linguistic, orthographic, contextual, and dictatorial characteristics. However, they have evaluated their model on the AZDC dataset, which is small compared to 1we obtained word embedding functions (TH et al., 2015) and in its pre-processing strategy, all words of less than 50 were mapped to rare word types. The NCBI dataset we considered in this study."}, {"heading": "7 Conclusions", "text": "In this paper, for the first time, we used three different variants of bidirectional RNN models with word embedding functions for disease names and class recognition tasks. Bidirectional RNN models are used to capture both forward and backward long-term dependencies between words within a sentence. We have shown that these models are able to achieve relatively competitive results compared to the benchmark result in disease name recognition. Furthermore, our results have shown significantly improved results in the relatively difficult task of classifying diseases, which has not been studied much so far. All of our results have been achieved without any effort on feature engineering or requiring domain-specific knowledge. Our results also suggest that RNN-based models work better for these two tasks than window-based neural network models. This could be due to the implicit ability of RNN models to capture variable range dependencies of words compared to explicit network dependencies of window size."}, {"heading": "Acknowledgments", "text": "We acknowledge the use of computer resources sponsored by the Board of Research in Nuclear Science (BRNS), Dept of Atomic Energy (DAE) Govt. of India (No.2013 / 13 / 8-BRNS / 10026) and funded by Dr. Aryabartta Sahu at the Department of Computer Science and Engineering, IIT Guwahati."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Advances in optimizing recurrent networks", "author": ["Bengio et al.2013] Yoshua Bengio", "Nicolas Boulanger-Lewandowski", "Razvan Pascanu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches. CoRR, abs/1409.1259", "author": ["Cho et al.2014] KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555", "author": ["Chung et al.2014] Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "NCBI disease corpus: A resource for disease name recognition and concept normalization", "author": ["Robert Leaman", "Zhiyong Lu"], "venue": "Journal of Biomedical Informatics,", "citeRegEx": "Dogan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dogan et al\\.", "year": 2014}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["dos Santos", "Bianca Zadrozny"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Victor Guimaraes", "RJ Niter\u00f3i", "Rio de Janeiro"], "venue": "Proceedings of NEWS 2015 The Fifth Named Entities Workshop,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "An improved corpus of disease mentions in pubmed citations", "author": ["Do\u011fan", "Lu2012] Rezarta Islamaj Do\u011fan", "Zhiyong Lu"], "venue": "In Proceedings of the 2012 Workshop on Biomedical Natural Language Processing,", "citeRegEx": "Do\u011fan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Do\u011fan et al\\.", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2010] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Technical Report UCB/EECS-2010-24, EECS Department,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves et al.2009] Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bidirectional LSTM-CRF models for sequence tagging. CoRR, abs/1508.01991", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Non-lexical neural architecture for fine-grained pos tagging", "author": ["Kevin Lser", "Alexandre Allauzen"], "venue": "In Llus Mrquez,", "citeRegEx": "Labeau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Labeau et al\\.", "year": 2015}, {"title": "Banner: An executable survey of advances in biomedical named entity recognition", "author": ["Leaman", "Gonzalez2008] Robert Leaman", "Graciela Gonzalez"], "venue": null, "citeRegEx": "Leaman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Leaman et al\\.", "year": 2008}, {"title": "Enabling recognition of diseases in biomedical text with machine learning: corpus and benchmark", "author": ["Leaman et al.2009] Robert Leaman", "Christopher Miller", "G Gonzalez"], "venue": "Proceedings of the 2009 Symposium on Languages", "citeRegEx": "Leaman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Leaman et al\\.", "year": 2009}, {"title": "Disease mention recognition using soft-margin", "author": ["Gang Li"], "venue": "svm. Training,", "citeRegEx": "Li.,? \\Q2012\\E", "shortCiteRegEx": "Li.", "year": 2012}, {"title": "Disease mention recognition with specific features", "author": ["Mahbub Chowdhury", "Alberto Lavelli"], "venue": "In Proceedings of the 2010 Workshop on Biomedical Natural Language Processing,", "citeRegEx": "Chowdhury et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chowdhury et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding", "author": ["Abeed Sarker", "Karen OConnor", "Rachel Ginn", "Graciela Gonzalez"], "venue": null, "citeRegEx": "Nikfarjam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nikfarjam et al\\.", "year": 2015}, {"title": "Understanding the exploding gradient problem. CoRR, abs/1211.5063", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Classifying semantic relations in bioscience texts", "author": ["Rosario", "Hearst2004] Barbara Rosario", "Marti A. Hearst"], "venue": "In Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Rosario et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosario et al\\.", "year": 2004}, {"title": "ABNER: An open source tool for automatically tagging genes, proteins, and other entity names in text", "author": ["B. Settles"], "venue": null, "citeRegEx": "Settles.,? \\Q2005\\E", "shortCiteRegEx": "Settles.", "year": 2005}, {"title": "Evaluating distributed word representations for capturing semantics of biomedical concepts", "author": ["TH et al.2015] MUNEEB TH", "Sunil Sahu", "Ashish Anand"], "venue": "In Proceedings of BioNLP", "citeRegEx": "TH et al\\.,? \\Q2015\\E", "shortCiteRegEx": "TH et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Complicate and inconsistent terminologies, ambiguities caused by use of abbreviations and acronyms, new disease names, multiple names (possibly of varying number of words) for the same disease, complicated syntactic structure referring to multiple related names or entities are some of the major reasons for making automatic identification of the task difficult and challenging (Leaman et al., 2009).", "startOffset": 378, "endOffset": 399}, {"referenceID": 7, "context": "State-ofthe-art disease name recognition systems (Mahbub Chowdhury and Lavelli, 2010; Do\u011fan and Lu, 2012; Dogan et al., 2014) depends on user defined features which in turn try to capture context keeping in mind above mentioned challenges.", "startOffset": 49, "endOffset": 125}, {"referenceID": 6, "context": "Recently window based neural network approach of (Collobert and Weston, 2008; Collobert et al., 2011) got lot of attention in different sequence tagging tasks in NLP.", "startOffset": 49, "endOffset": 101}, {"referenceID": 6, "context": "Output of the hidden layers is then fed to the output layer to compute the scores for all tags of interest (Collobert et al., 2011; Huang et al., 2015).", "startOffset": 107, "endOffset": 151}, {"referenceID": 15, "context": "Output of the hidden layers is then fed to the output layer to compute the scores for all tags of interest (Collobert et al., 2011; Huang et al., 2015).", "startOffset": 107, "endOffset": 151}, {"referenceID": 1, "context": "Distributed word representation or word embedding or simply word vector (Bengio et al., 2003; Collobert and Weston, 2008) is the technique of learning vector representation of a word in a given", "startOffset": 72, "endOffset": 121}, {"referenceID": 27, "context": "We use pre-trained 50 dimensional word vectors learned using skipgram method on a biomedical corpus (Mikolov et al., 2013b; Mikolov et al., 2013a; TH et al., 2015).", "startOffset": 100, "endOffset": 163}, {"referenceID": 12, "context": "Recurrent Neural Network (RNN) is a class of artificial neural networks which utilizes sequential information and maintains history through its intermediate layers (Graves et al., 2009; Graves, 2013).", "startOffset": 164, "endOffset": 199}, {"referenceID": 13, "context": "Recurrent Neural Network (RNN) is a class of artificial neural networks which utilizes sequential information and maintains history through its intermediate layers (Graves et al., 2009; Graves, 2013).", "startOffset": 164, "endOffset": 199}, {"referenceID": 24, "context": "Traditional RNN models suffer from both vanishing and exploding gradient (Pascanu et al., 2012; Bengio et al., 2013).", "startOffset": 73, "endOffset": 116}, {"referenceID": 2, "context": "Traditional RNN models suffer from both vanishing and exploding gradient (Pascanu et al., 2012; Bengio et al., 2013).", "startOffset": 73, "endOffset": 116}, {"referenceID": 13, "context": "In our experiment we used model discussed in (Graves, 2013; Huang et al., 2015).", "startOffset": 45, "endOffset": 79}, {"referenceID": 15, "context": "In our experiment we used model discussed in (Graves, 2013; Huang et al., 2015).", "startOffset": 45, "endOffset": 79}, {"referenceID": 3, "context": "A gated recurrent unit (GRU) was proposed by (Cho et al., 2014) to make each recurrent unit to adaptively capture dependencies of different time scales.", "startOffset": 45, "endOffset": 63}, {"referenceID": 4, "context": "We follow (Chung et al., 2014) model of GRU to transform the extracted word embedding and character embedding features to score for all tags.", "startOffset": 10, "endOffset": 30}, {"referenceID": 6, "context": "We follow sentencelevel log-likelihood (SLL) (Collobert et al., 2011) approach equivalent to linear-chain CRF to infer the scores of a particular tag sequence for the given word sequence.", "startOffset": 45, "endOffset": 69}, {"referenceID": 11, "context": "To train our model we used cross entropy loss function and adagrad (Duchi et al., 2010) approach to optimize the loss function.", "startOffset": 67, "endOffset": 87}, {"referenceID": 0, "context": "Entire code has been implemented using theano (Bastien et al., 2012) library in python language.", "startOffset": 46, "endOffset": 68}, {"referenceID": 18, "context": "This dataset is a extension of the AZDC dataset (Leaman et al., 2009) which was annotated with disease mentions only and not with their categories.", "startOffset": 48, "endOffset": 69}, {"referenceID": 6, "context": "We compare the results of RNN models with window based neural network (Collobert et al., 2011) using sentence level log likelihood approach (NN + CE).", "startOffset": 70, "endOffset": 94}, {"referenceID": 19, "context": "For the task B, we did not find any paper except (Li, 2012).", "startOffset": 49, "endOffset": 59}, {"referenceID": 19, "context": "For the task B, we did not find any paper except (Li, 2012). Li (2012) used linear soft margin support vector (SVM) machine with a number of hand designed features including dictionary based features.", "startOffset": 50, "endOffset": 71}, {"referenceID": 19, "context": "16 SM-SVM(Li, 2012) - - - 66.", "startOffset": 9, "endOffset": 19}, {"referenceID": 26, "context": "In biomedical domain, named entity recognition has attracted much attention for identification of entities such as genes and proteins (Settles, 2005; Leaman and Gonzalez, 2008; Leaman et al., 2009) but not as much for disease name recognition.", "startOffset": 134, "endOffset": 197}, {"referenceID": 18, "context": "In biomedical domain, named entity recognition has attracted much attention for identification of entities such as genes and proteins (Settles, 2005; Leaman and Gonzalez, 2008; Leaman et al., 2009) but not as much for disease name recognition.", "startOffset": 134, "endOffset": 197}, {"referenceID": 17, "context": "In biomedical domain, named entity recognition has attracted much attention for identification of entities such as genes and proteins (Settles, 2005; Leaman and Gonzalez, 2008; Leaman et al., 2009) but not as much for disease name recognition. Notable works, such as of Chowdhury and Lavelli (2010), are mainly conditional random field (CRF) based models using lots of manually designed template features.", "startOffset": 177, "endOffset": 299}, {"referenceID": 27, "context": "we obtained pre-trained word-embedding features from (TH et al., 2015) and in their pre-processing strategy, all words of frequency less than 50 were mapped to rare-word.", "startOffset": 53, "endOffset": 70}, {"referenceID": 23, "context": "Nikfarjam et al. (2015) have proposed a CRF based sequence tagging model, where cluster id of embedded word as an extra feature with manually engineered features is used for adverse drug reaction recognition in tweets.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "Recently deep neural network models with minimal dependency on feature engineering have been used in few studies in NLP including NER tasks (Collobert et al., 2011; Collobert and Weston, 2008).", "startOffset": 140, "endOffset": 192}, {"referenceID": 5, "context": "Recently deep neural network models with minimal dependency on feature engineering have been used in few studies in NLP including NER tasks (Collobert et al., 2011; Collobert and Weston, 2008). dos Santos et al. (2015) used deep neural network based model such as window based network to recognize named entity in Portuguese and Spanish texts.", "startOffset": 141, "endOffset": 219}, {"referenceID": 15, "context": "More recent work of Huang et al. (2015) used LSTM and CRF in variety of combination such as only LSTM, LSTM with CRF and Bi-LSTM with CRF for PoS tagging, chunking and NER tasks in general texts.", "startOffset": 20, "endOffset": 40}], "year": 2016, "abstractText": "Hand-crafted features based on linguistic and domain-knowledge play crucial role in determining the performance of disease name recognition systems. Such methods are further limited by the scope of these features or in other words, their ability to cover the contexts or word dependencies within a sentence. In this work, we focus on reducing such dependencies and propose a domain-invariant framework for the disease name recognition task. In particular, we propose various end-to-end recurrent neural network (RNN) models for the tasks of disease name recognition and their classification into four pre-defined categories. We also utilize convolution neural network (CNN) in cascade of RNN to get character-based embedded features and employ it with word-embedded features in our model. We compare our models with the state-of-the-art results for the two tasks on NCBI disease dataset. Our results for the disease mention recognition task indicate that state-of-the-art performance can be obtained without relying on feature engineering. Further the proposed models obtained improved performance on the classification task of disease names.", "creator": "LaTeX with hyperref package"}}}