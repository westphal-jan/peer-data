{"id": "1606.09375", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering", "abstract": "Convolutional neural networks (CNNs) have greatly improved state-of-the-art performances in a number of fields, notably computer vision and natural language processing. In this work, we are interested in generalizing the formulation of CNNs from low-dimensional regular Euclidean domains, where images (2D), videos (3D) and audios (1D) are represented, to high-dimensional irregular domains such as social networks or biological networks represented by graphs. This paper introduces a formulation of CNNs on graphs in the context of spectral graph theory. We borrow the fundamental tools from the emerging field of signal processing on graphs, which provides the necessary mathematical background and efficient numerical schemes to design localized graph filters efficient to learn and evaluate. As a matter of fact, we introduce the first technique that offers the same computational complexity than standard CNNs, while being universal to any graph structure. Numerical experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs, as long as the graph is well-constructed.", "histories": [["v1", "Thu, 30 Jun 2016 07:42:13 GMT  (676kb,D)", "http://arxiv.org/abs/1606.09375v1", null], ["v2", "Mon, 31 Oct 2016 15:24:49 GMT  (719kb,D)", "http://arxiv.org/abs/1606.09375v2", "NIPS 2016 camera-ready"], ["v3", "Sun, 5 Feb 2017 17:04:39 GMT  (166kb,D)", "http://arxiv.org/abs/1606.09375v3", "NIPS 2016 final revision"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["micha\u00ebl defferrard", "xavier bresson", "pierre vandergheynst"], "accepted": true, "id": "1606.09375"}, "pdf": {"name": "1606.09375.pdf", "metadata": {"source": "CRF", "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering", "authors": ["Micha\u00ebl Defferrard", "Xavier Bresson", "Pierre Vandergheynst"], "emails": ["michael.defferrard@epfl.ch", "xavier.bresson@epfl.ch", "pierre.vandergheynst@epfl.ch"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us are able to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world."}, {"heading": "2 Related Works", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Graph Signal Processing", "text": "GSP is an emerging field that aims to bridge the gap between signal processing techniques such as wave beam analysis [22] and graph theory such as spectral graph analysis [3, 21]. We refer the reader to [32] to introduce the field. Standard operations on grids such as folding, translation, filtering, extension, modulation or downsampling do not extend directly to graphs and therefore require new mathematical definitions while retaining the original intuitive concepts. In this context, the authors of [14, 9, 11] have reconsidered the construction of wave operations on graphs. In [33, 28] the authors designed a technique for performing pyramid transformations on graphs on the Mutli scale. The work of [36, 26, 27] redefined uncertainty principles on graphs and showed that intuitive concepts can be lost but also produce improved graphs on the interpretation of signal principles."}, {"heading": "2.2 CNNs on Non-Euclidean Domains", "text": "The idea is that you have to look for a solution that meets people's needs."}, {"heading": "3 Proposed Technique", "text": "Generalizing CNNs to graphs requires three basic steps. The first step is designing localized Convolutionary Filters on graphs; the second step is a graph coarsening process that groups similar features on graphs; and the last step is the graph pooling operation, in which spatial resolution is exchanged for higher filter resolution."}, {"heading": "3.1 Learning Fast Localized Spectral Filters", "text": "There are two strategies for defining local filters (either from a spatial approach or from a spectral approach.) There is no clear mathematical definition of spatial translations on graphs from a spatial perspective. On the other hand, a spectral approach provides a well-defined translation operator on graphs about convolutions with a Kronecker delta function implemented in the spectral domain. However, a filter defined in the spectral domain is not naturally localized and the translations are costly with O (n2) operations due to the multiplications with the Fourier graph."}, {"heading": "3.2 Graph Coarsening", "text": "We are now interested in extracting the multi-scale hierarchical composition property of the data. In contrast, the subsampling operation on graphs or graph coarsening is not mathematically sound; it requires the construction of meaningful neighborhoods in which similar linkages are grouped without affecting system performance, which is equivalent to constructing a multi-scale cluster that preserves local geometric structures. However, graphs are known to be a hard problem in which similar linkages are formed."}, {"heading": "3.3 Fast Pooling of Graph Signals", "text": "In fact, it is the case that we will be able to solve the problem without solving it."}, {"heading": "4 Numerical Experiments", "text": "All experiments were carried out with TensorFlow, an open source library for numerical calculation using data flow graphs that are particularly suitable for deep learning [1], which has various backends, in particular CUDA, to calculate on Nvidia GPUs. All calculations are performed on an Nvidia Tesla K40c GPU. Subsequently, we refer to filters defined by (2) as non-param, i.e. the spatially unlocalized filters, and the proposed filters defined by (4) as Chebyshev. Filters designated as splines are defined by (2) = B\u03b8, (8) where B-Rn-K is the cubic B-spline base and the parameter \u03b8 RK is a vector of control points, as proposed in [4, 15]."}, {"heading": "4.1 Revisiting Standard CNNs on MNIST", "text": "To validate our model, we first apply it to the Euclidean case with the benchmark MNIST classification problem [20]. In this situation, the graph is simply a k-NN graph of the Euclidean 2D grid. This is an important reliability test for our model, which must be able to extract properties from each graph, including the regular 2D grid on which the images are located. We remember that MNIST is a 70,000-digit dataset displayed on a 28 x 28 2D grid, so that the data points are in a space of 784 dimensions. For our graph model, we construct an 8-NN graph of the 2D euclidean grid that generates a graph of n = | V | = 976 nodes (784 pixels and 192 false nodes, as explained in Section 3.3) and grid | 198-2D channel."}, {"heading": "4.2 Text Categorization with 20NEWS", "text": "To demonstrate the versatility of our model, working with diagrams generated from unstructured data, we applied our technique to the text categorization problem with the 20NEWS datasets.1https: / / www.tensorflow.org / versions / r0.8 / tutorials / mnist / pros20NEWS consists of 18,846 (11,314 for training and 7,532 for testing) text documents associated with 20 classes [16]. We extracted the 10,000 most common words from the 93,953 unique words in this corpus. Each document is represented using the bag-of-words model that has been standardized verbatim. To test our model, we constructed a 16-NN diagram of the word2vec [25], from which a diagram of n = | V | = 10 000 nodes and | E | = 132, 834 edges are represented by a CN18 without the Adam Epoder, all of which are CN18 models without the optimizer."}, {"heading": "4.3 Numerical Comparison between Spectral Filters", "text": "In this section, we compare the proposed spectral filters with the non-parametric filters and filters proposed in [4, 15] via the MNIST and 20NEWS datasets. Table 3 reports that the proposed kernel parameterization is better than [4, 15] as well as the non-parametric filters that are not localized and require O (n) parameters to learn."}, {"heading": "4.4 Computational Efficiency", "text": "Figure 3 confirms the low computational complexity of the proposed CNN technique on graphs. The training time of our model scales as O (n), while [4, 15] as O (n2). In addition, Figure 4 gives an impression of how the validation accuracy and the loss energy converge between the three filter definitions. Finally, Table 4 compares the training time on CPU and GPU. The fact that we observe the same sequence of acceleration as classical CNNs illustrates the natural parallelization capability that our model offers. This is possible because our method relies exclusively on matrix multiplications efficiently implemented by cuBLAS, the linear algebra routines of NVIDIA."}, {"heading": "4.5 Influence of Graph Quality", "text": "For the proposed method to be successful, the statistical assumptions of locality, stationarity and compositionality with respect to the data on the graph in which the data is located must be fulfilled. Therefore, the classification performance, the quality of the filters learned, depends crucially on the quality of the graph. For data located in Euclidean space, experiments in Section 4.1 show that a simple k-NN graph of the grid is good enough to restore almost exactly the performance of the standard CNNs. We have also noticed that the value of k does not have a strong impact on the results. We can observe the importance of a graph that meets the data assumptions by comparing its performance to a random graph. Table 5 reports a large decrease in accuracy when using a random graph, that is, when the data structure is lost and the evolutionary layers are no longer useful to extract significant features. While images on climatic lattice 2D spaces are not represented by the regular graphs, this is the case of course."}, {"heading": "5 Conclusion and Future Work", "text": "We have introduced an efficient implementation of CNNs on non-Euclidean manifolds, represented by graphs using tools from GSP. Experiments have shown that our model provides strict control over the local support of filters by avoiding explicit use of the graph-Fourier base and demonstrating better test accuracy experimentally. Furthermore, we have taken into account the three concerns raised by [15]. First, we have introduced a model whose computational complexity is linear with the dimensionality of the data. Second, we have confirmed that the quality of the input graph is of paramount importance. Along this line, the idea [15] of verifying the input graph from data will be highly significant, and a natural and future approach would be to learn the CNN parameters and graph as a virtual circle that we have developed for the local wave networks."}], "references": [{"title": "LSH Forest: Self-Tuning Indexes for Similarity Search", "author": ["M. Bawa", "T. Condie", "P. Ganesan"], "venue": "International Conference on World Wide Web, pages 651\u2013660,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Towards a Theoretical Foundation for Laplacian-based Manifold Methods", "author": ["M. Belkin", "P. Niyogi"], "venue": "Journal of Computer and System Sciences, 74(8):1289\u20131308,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Spectral Networks and Deep Locally Connected Networks on Graphs", "author": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun"], "venue": "arXiv:1312.6203,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Finding Good Approximate Vertex and Edge Partitions is NP-hard", "author": ["T.N. Bui", "C. Jones"], "venue": "Information Processing Letters, 42(3):153\u2013159,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "Unsupervised Deep Haar Scattering on Graphs", "author": ["X. Chen", "X. Cheng", "S. Mallat"], "venue": "Neural Information Processing Systems, pages 1709\u20131717,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Spectral Graph Theory, volume 92", "author": ["F.R.K. Chung"], "venue": "American Mathematical Society,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Selecting Receptive Fields in Deep Networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "Neural Information Processing Systems (NIPS), pages 2528\u20132536,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Diffusion Maps", "author": ["R.R. Coifman", "S. Lafon"], "venue": "Applied and Computational Harmonic Analysis, 21(1):5\u201330,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Weighted Graph Cuts Without Eigenvectors: A Multilevel Approach", "author": ["I. Dhillon", "Y. Guan", "B. Kulis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 29(11):1944\u2013 1957,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Multiscale Wavelets on Trees, Graphs and High Dimensional Data: Theory and Applications to Semi Supervised Learning", "author": ["M. Gavish", "B. Nadler", "R. Coifman"], "venue": "International Conference on Machine Learning (ICML), pages 367\u2013374,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Emergence of Complex-like Cells in a Temporal Product Network with Local Receptive Fields", "author": ["K. Gregor", "Y. LeCun"], "venue": "arXiv:1006.0448,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Image Denoising with Nonlocal Spectral Graph Wavelets", "author": ["D. Hammond", "K. Raoaroor", "L. Jacques", "P. Vandergheynst"], "venue": "SIAM Conference on Imaging Science,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Wavelets on Graphs via Spectral Graph Theory", "author": ["D. Hammond", "P. Vandergheynst", "R. Gribonval"], "venue": "Applied and Computational Harmonic Analysis, 30(2):129\u2013150,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep Convolutional Networks on Graph-Structured Data", "author": ["M. Henaff", "J. Bruna", "Y. LeCun"], "venue": "arXiv:1506.05163,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization", "author": ["T. Joachims"], "venue": "Carnegie Mellon University, Computer Science Technical Report, CMU-CS-96-118,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1996}, {"title": "A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "SIAM Journal on Scientific Computing (SISC), 20(1):359\u2013392,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv:1412.6980,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-Based Learning Applied to Document Recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11), pages 2278\u20132324,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "A Tutorial on Spectral Clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and Computing, 17(4):395\u2013416,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "A Wavelet Tour of Signal Processing", "author": ["S. Mallat"], "venue": "Academic press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "ShapeNet: Convolutional Neural Networks on Non-Euclidean Manifolds", "author": ["J. Masci", "D. Boscaini", "M. Bronstein", "P. Vandergheynst"], "venue": "arXiv:1501.06297,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Geodesic Convolutional Neural Networks on Riemannian Manifolds", "author": ["J. Masci", "D. Boscaini", "M.M. Bronstein", "P. Vandergheynst"], "venue": "InWorkshop on 3D Representation and Recognition (3dRR),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "International Conference on Learning Representations,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Toward an Uncertainty Principle for Weighted Graphs", "author": ["B. Pasdeloup", "R. Alami", "V. Gripon", "M. Rabbat"], "venue": "Signal Processing Conference (EUSIPCO), pages 1496\u20131500,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Global and Local Uncertainty Principles for Signals on Graphs", "author": ["N. Perraudin", "B. Ricaud", "D. Shuman", "P. Vandergheynst"], "venue": "arXiv:1603.03030,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Generalized Tree-based Wavelet Transform", "author": ["I. Ram", "M. Elad", "I. Cohen"], "venue": "IEEE Transactions on Signal Processing,, 59(9):4199\u20134209,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Relaxation-based Coarsening and Multiscale Graph Organization", "author": ["D. Ron", "I. Safro", "A. Brandt"], "venue": "SIAM Iournal on Multiscale Modeling and Simulation, 9:407\u2013423,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Wavelets on Graphs via Deep Learning", "author": ["R. Rustamov", "L.J. Guibas"], "venue": "pages 998\u20131006,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Normalized Cuts and Image Segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 22(8):888\u2013905,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and other Irregular Domains", "author": ["D. Shuman", "S. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Processing Magazine, 30(3):83\u201398,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "A Multiscale Pyramid Transform for Graph Signals", "author": ["D.I. Shuman", "M.J. Faraji", "P. Vandergheynst"], "venue": "IEEE Transactions on Signal Processing, 64(8):2119\u20132134,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Accelerated Filtering on Graphs using Lanczos Method", "author": ["A. Susnjara", "N. Perraudin", "D. Kressner", "P. Vandergheynst"], "venue": "preprint arXiv:1509.04537,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressive Spectral Clustering", "author": ["N. Tremblay", "G. Puy", "R. Gribonval", "P. Vandergheynst"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "On the Degrees of Freedom of Signals on Graphs", "author": ["M. Tsitsvero", "S. Barbarossa"], "venue": "Signal Processing Conference (EUSIPCO), pages 1506\u20131510,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Convolutional neural networks [20] offer an efficient architecture to extract highly meaningful statistical patterns in large-scale and high-dimensional datasets.", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "This has led to the breakthrough of CNNs in image, video, and sound recognition tasks [19] as these data lie on low-dimensional regular lattices.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "This work leverages the recent works of [12, 8, 4, 15, 14, 32] to introduce an efficient implementation of CNNs on irregular domains represented here as graphs.", "startOffset": 40, "endOffset": 62}, {"referenceID": 6, "context": "This work leverages the recent works of [12, 8, 4, 15, 14, 32] to introduce an efficient implementation of CNNs on irregular domains represented here as graphs.", "startOffset": 40, "endOffset": 62}, {"referenceID": 2, "context": "This work leverages the recent works of [12, 8, 4, 15, 14, 32] to introduce an efficient implementation of CNNs on irregular domains represented here as graphs.", "startOffset": 40, "endOffset": 62}, {"referenceID": 13, "context": "This work leverages the recent works of [12, 8, 4, 15, 14, 32] to introduce an efficient implementation of CNNs on irregular domains represented here as graphs.", "startOffset": 40, "endOffset": 62}, {"referenceID": 12, "context": "This work leverages the recent works of [12, 8, 4, 15, 14, 32] to introduce an efficient implementation of CNNs on irregular domains represented here as graphs.", "startOffset": 40, "endOffset": 62}, {"referenceID": 30, "context": "This work leverages the recent works of [12, 8, 4, 15, 14, 32] to introduce an efficient implementation of CNNs on irregular domains represented here as graphs.", "startOffset": 40, "endOffset": 62}, {"referenceID": 5, "context": "Graphs can encode complex geometric structures, and can be studied with strong mathematical tools such as spectral graph theory [7], a blend between graph theory and harmonic analysis.", "startOffset": 128, "endOffset": 131}, {"referenceID": 30, "context": "We will make use of recent tools developed in the context of graph signal processing (GSP) [32] to achieve such goals.", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "Enhancing [4, 15], the proposed spectral filters are provable to be strictly localized in a ball of radius K, i.", "startOffset": 10, "endOffset": 17}, {"referenceID": 13, "context": "Enhancing [4, 15], the proposed spectral filters are provable to be strictly localized in a ball of radius K, i.", "startOffset": 10, "endOffset": 17}, {"referenceID": 2, "context": "We present many experiments that ultimately show that our formulation is (1) a useful model, (2) computationally efficient and (3) superior both in accuracy and complexity to the pioneer spectral graph CNNs introduced in [4, 15].", "startOffset": 221, "endOffset": 228}, {"referenceID": 13, "context": "We present many experiments that ultimately show that our formulation is (1) a useful model, (2) computationally efficient and (3) superior both in accuracy and complexity to the pioneer spectral graph CNNs introduced in [4, 15].", "startOffset": 221, "endOffset": 228}, {"referenceID": 10, "context": "Eventually, generic and efficient implementations of CNNs on graphs, as proposed here and related works [12, 8, 24, 23, 4, 15], is essential to pave the way to the development of a new class of deep learning techniques for graph-based data.", "startOffset": 104, "endOffset": 126}, {"referenceID": 6, "context": "Eventually, generic and efficient implementations of CNNs on graphs, as proposed here and related works [12, 8, 24, 23, 4, 15], is essential to pave the way to the development of a new class of deep learning techniques for graph-based data.", "startOffset": 104, "endOffset": 126}, {"referenceID": 22, "context": "Eventually, generic and efficient implementations of CNNs on graphs, as proposed here and related works [12, 8, 24, 23, 4, 15], is essential to pave the way to the development of a new class of deep learning techniques for graph-based data.", "startOffset": 104, "endOffset": 126}, {"referenceID": 21, "context": "Eventually, generic and efficient implementations of CNNs on graphs, as proposed here and related works [12, 8, 24, 23, 4, 15], is essential to pave the way to the development of a new class of deep learning techniques for graph-based data.", "startOffset": 104, "endOffset": 126}, {"referenceID": 2, "context": "Eventually, generic and efficient implementations of CNNs on graphs, as proposed here and related works [12, 8, 24, 23, 4, 15], is essential to pave the way to the development of a new class of deep learning techniques for graph-based data.", "startOffset": 104, "endOffset": 126}, {"referenceID": 13, "context": "Eventually, generic and efficient implementations of CNNs on graphs, as proposed here and related works [12, 8, 24, 23, 4, 15], is essential to pave the way to the development of a new class of deep learning techniques for graph-based data.", "startOffset": 104, "endOffset": 126}, {"referenceID": 20, "context": "1 Graph Signal Processing GSP is an emerging field that aims at bridging the gap between signal processing techniques like wavelet analysis [22] and graph theory such as spectral graph analysis [3, 21].", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "1 Graph Signal Processing GSP is an emerging field that aims at bridging the gap between signal processing techniques like wavelet analysis [22] and graph theory such as spectral graph analysis [3, 21].", "startOffset": 194, "endOffset": 201}, {"referenceID": 19, "context": "1 Graph Signal Processing GSP is an emerging field that aims at bridging the gap between signal processing techniques like wavelet analysis [22] and graph theory such as spectral graph analysis [3, 21].", "startOffset": 194, "endOffset": 201}, {"referenceID": 30, "context": "We refer the reader to [32] for an introduction of the field.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "In this context, the authors of [14, 9, 11] revisited the construction of wavelet operators on graphs.", "startOffset": 32, "endOffset": 43}, {"referenceID": 7, "context": "In this context, the authors of [14, 9, 11] revisited the construction of wavelet operators on graphs.", "startOffset": 32, "endOffset": 43}, {"referenceID": 9, "context": "In this context, the authors of [14, 9, 11] revisited the construction of wavelet operators on graphs.", "startOffset": 32, "endOffset": 43}, {"referenceID": 31, "context": "In [33, 28], the authors designed a technique to perform mutli-scale pyramid transforms on graphs.", "startOffset": 3, "endOffset": 11}, {"referenceID": 26, "context": "In [33, 28], the authors designed a technique to perform mutli-scale pyramid transforms on graphs.", "startOffset": 3, "endOffset": 11}, {"referenceID": 34, "context": "The works of [36, 26, 27] redefined uncertainty principles on graphs, and showed that intuitive concepts may be lost, but can also produce enhanced localization principles for signals on graphs.", "startOffset": 13, "endOffset": 25}, {"referenceID": 24, "context": "The works of [36, 26, 27] redefined uncertainty principles on graphs, and showed that intuitive concepts may be lost, but can also produce enhanced localization principles for signals on graphs.", "startOffset": 13, "endOffset": 25}, {"referenceID": 25, "context": "The works of [36, 26, 27] redefined uncertainty principles on graphs, and showed that intuitive concepts may be lost, but can also produce enhanced localization principles for signals on graphs.", "startOffset": 13, "endOffset": 25}, {"referenceID": 11, "context": "In [13], it was shown how to carry out lasso-based signal regularization on graphs, and studied the intertwined relationships between smoothness and sparsity on graphs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "In [35], the authors investigated compressed sensing recovery conditions for graph spectral signal analysis.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "A preliminary work was proposed with the so-called local reception fields [12, 8], successfully applied to image recognition.", "startOffset": 74, "endOffset": 81}, {"referenceID": 6, "context": "A preliminary work was proposed with the so-called local reception fields [12, 8], successfully applied to image recognition.", "startOffset": 74, "endOffset": 81}, {"referenceID": 2, "context": "A first attempt to design CNNs on graphs was introduced in [4] by adopting a spatial approach.", "startOffset": 59, "endOffset": 62}, {"referenceID": 22, "context": "ShapeNet in [24, 23] is a generalization of CNNs to 3D-meshes, a class of smooth low-dimensional non-Euclidean spaces.", "startOffset": 12, "endOffset": 20}, {"referenceID": 21, "context": "ShapeNet in [24, 23] is a generalization of CNNs to 3D-meshes, a class of smooth low-dimensional non-Euclidean spaces.", "startOffset": 12, "endOffset": 20}, {"referenceID": 4, "context": "In [6] and [30], the authors investigated the construction of Haar wavelet transforms on graphs using a deep hierarchical architecture.", "startOffset": 3, "endOffset": 6}, {"referenceID": 28, "context": "In [6] and [30], the authors investigated the construction of Haar wavelet transforms on graphs using a deep hierarchical architecture.", "startOffset": 11, "endOffset": 15}, {"referenceID": 2, "context": "Finally, [4] and [15] introduced a generalization of CNNs on graphs using graph counterparts of grid convolution and downsampling.", "startOffset": 9, "endOffset": 12}, {"referenceID": 13, "context": "Finally, [4] and [15] introduced a generalization of CNNs on graphs using graph counterparts of grid convolution and downsampling.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "They were able to learn convolutional filters on graphs using the convolution theorem [22] that defines convolutions as linear operators that diagonalize in the Fourier basis (represented by the eigenvectors of the Laplacian operator).", "startOffset": 86, "endOffset": 90}, {"referenceID": 2, "context": "However, although graph convolution directly in the spatial domain is conceivable, it also faces the challenge of matching local neighbourhoods, as pointed out in [4].", "startOffset": 163, "endOffset": 166}, {"referenceID": 30, "context": "On the other side, a spectral approach provides a well-defined translation operator on graphs via convolutions with a Kronecker delta function implemented in the spectral domain [32].", "startOffset": 178, "endOffset": 182}, {"referenceID": 5, "context": "An essential operator in spectral graph analysis is the graph Laplacian [7], which combinatorial definition is L = D \u2212W \u2208 Rn\u00d7n where D \u2208 Rn\u00d7n is the diagonal degree matrix with Dii = \u2211 jWij , and its normalized definition is L = In \u2212 D\u22121/2WD\u22121/2 where In is the identity matrix.", "startOffset": 72, "endOffset": 75}, {"referenceID": 30, "context": "We can now define the graph Fourier transform (GFT) of a spatial signal x \u2208 R as x\u0302 = Ux \u2208 R, and its inverse as x = Ux\u0302 [32].", "startOffset": 121, "endOffset": 125}, {"referenceID": 12, "context": "One such polynomial, traditionally used in GSP to approximate kernels (like wavelets), is the Chebyshev expansion [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 32, "context": "Another option, the Lanczos algorithm [34], which constructs an orthonormal basis of the Krylov subspace KK(L, x) = span{x, Lx, .", "startOffset": 38, "endOffset": 42}, {"referenceID": 3, "context": "It is however well-known that graph clustering is a NP-hard problem [5] and that approximations must be used.", "startOffset": 68, "endOffset": 71}, {"referenceID": 19, "context": "the popular spectral clustering [21]), we are most interested in multilevel clustering algorithms, where each level produces a coarser graph which corresponds to the data domain seen at a different resolution.", "startOffset": 32, "endOffset": 36}, {"referenceID": 8, "context": "In this work, we make use of the coarsening phase of the Graclus multilevel clustering algorithm [10], which has been shown to be extremely efficient at clustering a large variety of graphs.", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "Algebraic multigrid techniques on graphs [29] and the Kron reduction [33] are two methods worth exploring in future works.", "startOffset": 41, "endOffset": 45}, {"referenceID": 31, "context": "Algebraic multigrid techniques on graphs [29] and the Kron reduction [33] are two methods worth exploring in future works.", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "Graclus [10], built on Metis [17], uses a greedy algorithm to compute successive coarser versions of a given graph and is able to minimize several popular spectral clustering objectives.", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "Graclus [10], built on Metis [17], uses a greedy algorithm to compute successive coarser versions of a given graph and is able to minimize several popular spectral clustering objectives.", "startOffset": 29, "endOffset": 33}, {"referenceID": 29, "context": "We chose the normalized cut [31], which is an excellent clustering energy.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "g\u03b8(\u039b) = B\u03b8, (8) where B \u2208 Rn\u00d7K is the cubic B-spline basis and the parameter \u03b8 \u2208 R is a vector of control points, as proposed in [4, 15].", "startOffset": 129, "endOffset": 136}, {"referenceID": 13, "context": "g\u03b8(\u039b) = B\u03b8, (8) where B \u2208 Rn\u00d7K is the cubic B-spline basis and the parameter \u03b8 \u2208 R is a vector of control points, as proposed in [4, 15].", "startOffset": 129, "endOffset": 136}, {"referenceID": 2, "context": "2 rather than the simple agglomerative method of [4, 15].", "startOffset": 49, "endOffset": 56}, {"referenceID": 13, "context": "2 rather than the simple agglomerative method of [4, 15].", "startOffset": 49, "endOffset": 56}, {"referenceID": 18, "context": "1 Revisiting Standard CNNs on MNIST To validate our model, we first apply it to the Euclidean case with the benchmark MNIST classification problem [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "20NEWS consists of 18,846 (11,314 for training and 7,532 for testing) text documents associated with 20 classes [16].", "startOffset": 112, "endOffset": 116}, {"referenceID": 23, "context": "To test our model, we constructed a 16-NN graph of the word2vec [25] embedding of those words, which produced a graph of n = |V| = 10, 000 nodes and |E| = 132, 834 edges.", "startOffset": 64, "endOffset": 68}, {"referenceID": 16, "context": "All CNN models were trained for 20 epochs by the Adam optimizer [18] with a learning rate of 0.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "3 Numerical Comparison between Spectral Filters In this section, we compare the proposed spectral filters to the non-parametric filters, and those proposed in [4, 15] on the MNIST and 20NEWS datasets.", "startOffset": 159, "endOffset": 166}, {"referenceID": 13, "context": "3 Numerical Comparison between Spectral Filters In this section, we compare the proposed spectral filters to the non-parametric filters, and those proposed in [4, 15] on the MNIST and 20NEWS datasets.", "startOffset": 159, "endOffset": 166}, {"referenceID": 2, "context": "Table 3 reports that the proposed kernel parametrization outperforms [4, 15] as well as the non-parametric filters, which are not localized and require O(n) parameters to learn.", "startOffset": 69, "endOffset": 76}, {"referenceID": 13, "context": "Table 3 reports that the proposed kernel parametrization outperforms [4, 15] as well as the non-parametric filters, which are not localized and require O(n) parameters to learn.", "startOffset": 69, "endOffset": 76}, {"referenceID": 2, "context": "Accuracy Dataset Architecture Non-Param (2) Spline (8) [4, 15] Chebyshev (4) MNIST GC10 95.", "startOffset": 55, "endOffset": 62}, {"referenceID": 13, "context": "Accuracy Dataset Architecture Non-Param (2) Spline (8) [4, 15] Chebyshev (4) MNIST GC10 95.", "startOffset": 55, "endOffset": 62}, {"referenceID": 2, "context": "The training time of our model scales as O(n), while [4, 15] scales as O(n).", "startOffset": 53, "endOffset": 60}, {"referenceID": 13, "context": "The training time of our model scales as O(n), while [4, 15] scales as O(n).", "startOffset": 53, "endOffset": 60}, {"referenceID": 23, "context": "Another approach is to learn embeddings on the corpus with word2vec [25] or to use pre-trained embeddings on Google News (given by the authors).", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "We used the LSHForest [2] on the learned word2vec embedding.", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "Compared with the first works of spectral graph CNNs introduced in [4, 15], our model provides a strict control over the local support of filters, is computationally more efficient by avoiding an explicit use of the Graph Fourier basis, and experimentally shows a better test accuracy.", "startOffset": 67, "endOffset": 74}, {"referenceID": 13, "context": "Compared with the first works of spectral graph CNNs introduced in [4, 15], our model provides a strict control over the local support of filters, is computationally more efficient by avoiding an explicit use of the Graph Fourier basis, and experimentally shows a better test accuracy.", "startOffset": 67, "endOffset": 74}, {"referenceID": 13, "context": "Besides, we addressed the three concerns raised by [15].", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "Along this line, the idea of [15] to learn the input graph from data is highly meaningful.", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 118, "endOffset": 136}, {"referenceID": 7, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 118, "endOffset": 136}, {"referenceID": 9, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 118, "endOffset": 136}, {"referenceID": 4, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 118, "endOffset": 136}, {"referenceID": 28, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 118, "endOffset": 136}, {"referenceID": 31, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 175, "endOffset": 179}, {"referenceID": 34, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 218, "endOffset": 230}, {"referenceID": 24, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 218, "endOffset": 230}, {"referenceID": 25, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 218, "endOffset": 230}], "year": 2016, "abstractText": "Convolutional neural networks (CNNs) have greatly improved state-of-the-art performances in a number of fields, notably computer vision and natural language processing. In this work, we are interested in generalizing the formulation of CNNs from low-dimensional regular Euclidean domains, where images (2D), videos (3D) and audios (1D) are represented, to high-dimensional irregular domains such as social networks or biological networks represented by graphs. This paper introduces a formulation of CNNs on graphs in the context of spectral graph theory. We borrow the fundamental tools from the emerging field of signal processing on graphs, which provides the necessary mathematical background and efficient numerical schemes to design localized graph filters efficient to learn and evaluate. As a matter of fact, we introduce the first technique that offers the same computational complexity than standard CNNs, while being universal to any graph structure. Numerical experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs, as long as the graph is well-constructed.", "creator": "LaTeX with hyperref package"}}}