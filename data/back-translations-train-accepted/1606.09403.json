{"id": "1606.09403", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Learning Crosslingual Word Embeddings without Bilingual Corpora", "abstract": "Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task.", "histories": [["v1", "Thu, 30 Jun 2016 09:18:53 GMT  (208kb,D)", "http://arxiv.org/abs/1606.09403v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["long duong", "hiroshi kanayama", "tengfei ma", "steven bird", "trevor cohn"], "accepted": true, "id": "1606.09403"}, "pdf": {"name": "1606.09403.pdf", "metadata": {"source": "CRF", "title": "Learning Crosslingual Word Embeddings without Bilingual Corpora", "authors": ["Long Duong", "Hiroshi Kanayama", "Tengfei Ma", "Steven Bird", "Trevor Cohn"], "emails": [], "sections": [{"heading": null, "text": "Cross-lingual word embedding represents lexical elements from different languages in the same vector space and enables the transfer of NLP tools. However, previous attempts have had costly resource requirements, difficulties in integrating monolingual data or have been unable to handle polysemy. We address these drawbacks with our method, which relies on a comprehensive dictionary in an EM-like training algorithm compared to monolingual corpora in two languages. Our model achieves state-of-the-art performance in bilingual lexicon introduction tasks that exceed models with large bilingual corpora, and competitive results in monolingual word similarity and cross-language classification of documents."}, {"heading": "1 Introduction", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "2 Related work", "text": "This is often done in the form of parallel bilingual text that uses word alignments as a bridge between tokens in the source and target languages, so that translations are assigned similar embedding vectors (Luong et al., 2015; Klementiev et al., 2012). These approaches are influenced by errors from automatic word alignments, which motivates other approaches that operate at the sentence level (Chandar A et al., 2014; Hermann and Blunsom, 2014; Gouws et al., 2015) by learning compositional vector representations of sentences so that sentences and their translations closely match representations. Word embeddings learned this way capture translation equivalence, although they do not use explicit word alignments. Nevertheless, these approaches require large parallel companies that are not available for many language pairs. Vulic and Moens (2015), comparable approaches we use."}, {"heading": "3 Word2Vec", "text": "Our model is an extension of the Contextual Word Bag (CBOW) model by Mikolov et al. (2013b), a method for learning vector representations of words based on their distribution context. Specifically, her model describes the probability of a token wi at position i by means of logistic regression with factored parameterization, p (wi | wi \u00b1 k\\ i) = exp (u > wihi), w \u00b2 W exp (u > whi), (1) where hi = 12k \u2211 kj = \u2212 k; j 6 = 0 vwi + j is a vector encoding the context around position i over a window of size k, W is the vocabulary and the parameters V and U \u00b2 R | W | d are matrices designated as context and word bedding. The model is trained to maximize the log pseudo-probability of a training corpus, but due to the high complexity of the negative data (2013b)."}, {"heading": "4 Our Model", "text": "Our approach expands the CBOW to modelling bilingual text using two monolingual corpora and one bilingual dictionary. We believe that this data condition is less stringent than the requirement for parallel or comparable texts as the source of the bilingual signal. It is common for field linguists to create a bilingual dictionary when studying a new language as one of the first steps in the language documentation process. Translation dictionaries are a rich source of information that captures much of the lexical ambiguity in a language through translation.1: Randomly initialize V, U 2: for i < Iter Do 3: for i De Df Do 4: s \u2190 wi + hi 5: w i = argmaxw dict (wi) cos (s, vw): for i < Iter Do 3: for i De Df Do 4: s \u2190 wi + 5: argw = maxw (wi)."}, {"heading": "4.1 Dictionary replacement", "text": "To learn bilingual relationships, we use the context in one language to predict the translation of the middle word into another language. This is motivated by the fact that the context is an excellent means of clearly determining the translation of a word. Our method is closely related to Gouws and S\u00f8gaard (2015), but we only replace the middle word wi with a translation w \u0435i while keeping the context stable. We replace each middle word with a translation on the fly during the training, predicting instead of p (w) p (wi \u00b1 k\\ i), but using the same formulation as (1), but with an extended U matrix to cover word types in both languages. The translation w \u044bi is selected from the possible translations of wi listed in the dictionary. The problem of selecting the correct translation from the many options is reminiscent of the problem we face in maximizing expectations (EM)."}, {"heading": "4.2 Joint Training", "text": "Words and their translations should appear in very similar contexts, and one way to achieve this is to learn together to predict both the word and its translation from its monolingual context, hence the following common objective function, O = \u2211 i-Df (\u03b1 log \u03c3 (u > wihi) + (1 \u2212 \u03b1) log \u03c3 (u > w-ihi) + p \u00b2 j = 1 Ewj \u0445 Pn (w) log \u03c3 (\u2212 u > wjhi))), (3) where \u03b1 controls the contribution of the two terms. For our experiments, we use \u03b1 = 0.5. The negative examples come from a combined vocabulary grammar distribution calculated from combined data De-Df."}, {"heading": "4.3 Combining Embeddings", "text": "Many vector learning methods learn two embedding spaces V and U. Typically, only V is used in the application, with the use of U being insufficiently investigated as opposed to V alone (Levy and Goldberg, 2014), with the exception of Pennington et al. (2014), which use a linear combination U + V, with slight improvements over V alone. We argue that with our Model V we are better at capturing the monolingual regularities, and U is better at capturing bilingual signals. The intuition for this is as follows: Assuming that we finance the word and its Italian translation finance from the context (money, loans, bank, debt, credit) as shown in Figure 1. In V, only the contextual representations are updated, and in U only the representations of finances and negative examples such as tree and dog are updated. CBOW learns good embedding because they update the parameters each time, 1With both embedding we also give a small comparison of the use of the 2h in the context."}, {"heading": "5 Experiment Setup", "text": "We will test the lingual property, the monolingual property and the downstream usefulness of our cross-lingual word embedding (CLWE). For the cross-lingual property, we will perform the bilingual lexicon induction task of Vulic \u0301 and Moens (2015). For the monolingual property, we will perform the word similarity task on common data sets such as WordSim353 and Rareword. To demonstrate the usefulness of our CLWE, we will also evaluate the conventional cross-lingual document classification."}, {"heading": "5.1 Monolingual Data", "text": "Most of the monolingual data comes from the pre-edited Wikipedia dump by Al-Rfou et al. (2013). The data has already been cleaned and tokenized, and we have all the words low-packaged. Normally, the monolingual word embeddings are formed on billions of words. However, obtaining so much monolingual data for a language with limited resources is also a challenge, so we only select the top 5 million sentences (about 100 million words) for each language."}, {"heading": "5.2 Dictionary", "text": "The bilingual dictionary is the only source of bilingual correspondence in our technology. We want a dictionary that covers many languages, so our approach can be applied to many low-resource languages. We use Panlex, a dictionary that currently covers about 1300 language variants with about 12 million expressions. PanLex translations come from various sources such as glossaries, dictionaries, automatic inferences from other languages, etc. Accordingly, Panlex has a high language coverage, but often loud translations. 3"}, {"heading": "6 Bilingual Lexicon Induction", "text": "Considering a word in a source language, we leave it for future work only in a single model. We need to predict its translation in the target language. The difficulty of this task is that we have also experimented with a growing crow source dictionary from Wiktionary. (Our first observation is that the translation quality is better, but a lower translation rate is used.) We observe similar trends with Panlex and Wiktionary in our model. However, we can run the model with Panlex results in much better performance. (We can apply the model to the combined language of Panlex and Wiktionary in both languages.) We can apply it to the combined language of Panlex and Wiktionary, but we leave it for future work. (We can run the model on the combined language of Panlex and Wiktionary, but we can only run it in much better performance.) We can run the model on the combined language of Panlex and Wiktionary, but we leave it for future work."}, {"heading": "7 Monolingual Word Similarity", "text": "Now we are looking at the effectiveness of our CLWE on monolingual word similarity. Our experimental situation is similar to Luong et al. (2015). We evaluated monolingual similarity in English on WordSim353 (WS-EN), RareWord (RW-En) and German version of WordSim353 (WS-De) (Finkelstein et al., 2001; Luong et al., 2013; Luong et al., 2015). However, each of these datasets contains many tuples (w1, w2, Score) where score is given by annotators showing the semantic similarity between Wordw1 and w2. The system must give the score correlate with human judgments. We train the model as described in Equation (5), which is exactly the same model as the embedding in Table 2. Since the evaluation includes German and English word similarity, we train the CLWE for English - German pairs."}, {"heading": "8 Model selection", "text": "The combination of context embedding and word embedding results in an improvement of both monolingual similarity and bilingual lexicon induction. In section 4.3 we introduce several combination methods, including post-processing (interpolation and concatenation) and during training (regularization). In this section we explain our parameter and model selections. We use an English-Italian pair for tuning purposes taking into account the value of \u03b3 in equation 4. Figure 2 shows the performance using different values of \u03b3. The two extremes in which Q = 0 and Q = 1 do not correspond to any interpolation in which we only use U or V. As a compromise, we select Q = 0.5 in our experiments. Similarly, we adjust the regulation sensitivity in Equation (5), where U is better at grasping bilingual relationship and V is better at grasping the monolingual relationship. As a compromise, we select Q = 0.5 in our experiments."}, {"heading": "9 Crosslingual Document Classification", "text": "In this section, we evaluate our CLWE using a downstream crossslingual document classification (CLDC) task. In this task, the document classifier is trained on a source language and then applied directly to the classification of a document in the target language, which is convenient for a target language with limited resources where we do not have document annotations. Experimental setup is by Klementiev et al. (2012).7 Tensile and test data are taken from the Reuter RCV1 / RCV2 corpus (Lewis et al., 2004).The documents are presented as a bag with word embedding weighted by tf.idf. A multi-class classifier is trained on 1000 documents in the source language using the average perceptron algorithm and tested on 5000 documents in the target language. We use the CLWE so that document representation is embedded in the target language."}, {"heading": "10 Conclusion", "text": "Previous CLWE methods often required high levels of resources, but had low accuracy. We introduced a simple framework based on a large, loud dictionary. We modeled multilingualism using EM translation selection during training to learn bilingual corpora. Our algorithm enables us to efficiently train on vast amounts of monolingual data representing monolingual and bilingual characteristics of language, enabling us to achieve state-of-the-art performance in the bilingual lexicon introduction task, competitive results in monolingual word similarity and cross-lingual document classification. Our combination techniques during training, especially regulation, are highly effective and could be used to improve monolingual word embedding. 8We randomly execute sample documents in RCV1 and RCV2 corpus and select around 85k documents to link monolingual sentences for English and German to form them into Wikipedia, for each document we link with small-word writing:"}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,", "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Stanislas Lauly", "Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C Raykar", "Amrita Saha"], "venue": null, "citeRegEx": "P et al\\.,? \\Q2014\\E", "shortCiteRegEx": "P et al\\.", "year": 2014}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies", "author": ["Das", "Petrov2011] Dipanjan Das", "Slav Petrov"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Das et al\\.", "year": 2011}, {"title": "Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser", "author": ["Duong et al.2015] Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Duong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Dyer2014] Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Simple task-specific bilingual word embeddings", "author": ["Gouws", "S\u00f8gaard2015] Stephan Gouws", "Anders S\u00f8gaard"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2015] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": "Proceedings of the 32nd International Conference on Machine Learn-", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Hermann", "Blunsom2014] Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Panlex: Building a resource for panlingual lexical translation", "author": ["Jonathan Pool", "Susan Colowick"], "venue": "In Proceedings of the Ninth International Conference on Language Resources and Evaluation", "citeRegEx": "Kamholz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kamholz et al\\.", "year": 2014}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Ivan Titov", "Binod Bhattarai"], "venue": "In Proceedings of COLING", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Learning bilingual word representations by marginalizing alignments", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Ko\u010disk\u00fd et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ko\u010disk\u00fd et al\\.", "year": 2014}, {"title": "Neural word embedding as a factorization", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Rcv1: A new", "author": ["Lewis et al.2004] David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "venue": null, "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong et al.2013] Thang Luong", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Language", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Bilingual word representations with monolingual quality in mind", "author": ["Hieu Pham", "Christopher D. Manning"], "venue": "In NAACL Workshop on Vector Space Modeling for NLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168", "author": ["Quoc V. Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation", "author": ["Resnik", "Yarowsky1999] Philip Resnik", "David Yarowsky"], "venue": null, "citeRegEx": "Resnik et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Resnik et al\\.", "year": 1999}, {"title": "Improvements in part-of-speech tagging with an application to german", "author": ["Helmut Schmid"], "venue": "Proceedings of the ACL SIGDAT-Workshop,", "citeRegEx": "Schmid.,? \\Q1995\\E", "shortCiteRegEx": "Schmid.", "year": 1995}, {"title": "Swivel: Improving embeddings by noticing what\u2019s missing. CoRR, abs/1602.02215", "author": ["Shazeer et al.2016] Noam Shazeer", "Ryan Doherty", "Colin Evans", "Chris Waterson"], "venue": null, "citeRegEx": "Shazeer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shazeer et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Inverted indexing for cross-lingual nlp", "author": ["Bohnet", "Anders Johannsen."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long", "citeRegEx": "Bohnet and Johannsen.,? 2015", "shortCiteRegEx": "Bohnet and Johannsen.", "year": 2015}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Bilingual word embeddings from nonparallel document-aligned data applied to bilingual lexicon induction", "author": ["Vuli\u0107", "Moens2015] Ivan Vuli\u0107", "Marie-Francine Moens"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2015}, {"title": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning, chapter Distributed Word Representation Learning for CrossLingual Dependency Parsing, pages 119\u2013129", "author": ["Xiao", "Guo2014] Min Xiao", "Yuhong Guo"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2014}, {"title": "Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora", "author": ["Yarowsky", "Ngai2001] David Yarowsky", "Grace Ngai"], "venue": "In Proceedings of the Second Meeting of the North American Chapter of the Association", "citeRegEx": "Yarowsky et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2001}, {"title": "Measuring word relatedness using heterogeneous vector space models", "author": ["Yih", "Qazvinian2012] Wen-tau Yih", "Vahed Qazvinian"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Yih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 23, "context": "Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 5, "context": ", 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 1, "context": ", 2015), machine translation (Bahdanau et al., 2014).", "startOffset": 29, "endOffset": 52}, {"referenceID": 25, "context": "A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2012; Duong et al., 2015).", "startOffset": 104, "endOffset": 195}, {"referenceID": 4, "context": "A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2012; Duong et al., 2015).", "startOffset": 104, "endOffset": 195}, {"referenceID": 11, "context": "Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012).", "startOffset": 152, "endOffset": 177}, {"referenceID": 12, "context": "Moreover, many prior work (Chandar A P et al., 2014; Ko\u010disk\u00fd et al., 2014) used bilingual or comparable corpus which is also expensive for many low-resource languages.", "startOffset": 26, "endOffset": 74}, {"referenceID": 16, "context": "To capture the monolingual distributional properties of words it is crucial to train on large monolingual corpora (Luong et al., 2015).", "startOffset": 114, "endOffset": 134}, {"referenceID": 2, "context": "Moreover, many prior work (Chandar A P et al., 2014; Ko\u010disk\u00fd et al., 2014) used bilingual or comparable corpus which is also expensive for many low-resource languages. S\u00f8gaard et al. (2015) impose a less onerous data condition in the form of linked Wikipedia entries across several languages, however this approach tends to underperform other methods.", "startOffset": 37, "endOffset": 190}, {"referenceID": 10, "context": "Our model uses a bilingual dictionary from Panlex (Kamholz et al., 2014) as the source of bilingual signal.", "startOffset": 50, "endOffset": 72}, {"referenceID": 16, "context": "This is often in the form of a parallel bilingual text, using word alignments as a bridge between tokens in the source and target languages, such that translations are assigned similar embedding vectors (Luong et al., 2015; Klementiev et al., 2012).", "startOffset": 203, "endOffset": 248}, {"referenceID": 11, "context": "This is often in the form of a parallel bilingual text, using word alignments as a bridge between tokens in the source and target languages, such that translations are assigned similar embedding vectors (Luong et al., 2015; Klementiev et al., 2012).", "startOffset": 203, "endOffset": 248}, {"referenceID": 7, "context": "These approaches are affected by errors from automatic word alignments, motivating other approaches which operate at the sentence level (Chandar A P et al., 2014; Hermann and Blunsom, 2014; Gouws et al., 2015) through learning compositional vector representations of sentences, in order that sentences and their translations representations closely match.", "startOffset": 136, "endOffset": 209}, {"referenceID": 17, "context": "Mikolov et al. (2013a), Xiao and Guo (2014) and Faruqui and Dyer (2014) filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary.", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "Mikolov et al. (2013a), Xiao and Guo (2014) and Faruqui and Dyer (2014) filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary.", "startOffset": 0, "endOffset": 44}, {"referenceID": 17, "context": "Mikolov et al. (2013a), Xiao and Guo (2014) and Faruqui and Dyer (2014) filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary.", "startOffset": 0, "endOffset": 72}, {"referenceID": 17, "context": "Mikolov et al. (2013a) and Faruqui and Dyer (2014) use a cascade style of training where the word embeddings in both source and target language are trained separately and then combined later using the dictionary.", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "Mikolov et al. (2013a) and Faruqui and Dyer (2014) use a cascade style of training where the word embeddings in both source and target language are trained separately and then combined later using the dictionary.", "startOffset": 0, "endOffset": 51}, {"referenceID": 7, "context": "mance over cascade training (Gouws et al., 2015).", "startOffset": 28, "endOffset": 48}, {"referenceID": 17, "context": "Our model is an extension of the contextual bag of words (CBOW) model of Mikolov et al. (2013b), a method for learning vector representations of words based on their distributional contexts.", "startOffset": 73, "endOffset": 96}, {"referenceID": 17, "context": "The model is trained to maximise the log-pseudo likelihood of a training corpus, however due to the high complexity of computing the denominator of (1), Mikolov et al. (2013b) propose negative sampling as an approximation, by instead learning to differentiate data from noise (negative examples).", "startOffset": 153, "endOffset": 176}, {"referenceID": 19, "context": "The use of U, on the other hand, is understudied (Levy and Goldberg, 2014) with the exception of Pennington et al. (2014) who use a linear combination U + V, with minor improvement over V alone.", "startOffset": 97, "endOffset": 122}, {"referenceID": 19, "context": "First, we can follow Pennington et al. (2014) to interpolate V and U in the post-processing step.", "startOffset": 21, "endOffset": 46}, {"referenceID": 0, "context": "The monolingual data is mainly from the preprocessed Wikipedia dump from Al-Rfou et al. (2013). The data is already cleaned and tokenized.", "startOffset": 73, "endOffset": 95}, {"referenceID": 7, "context": "9 BilBOWA: Gouws et al. (2015) 51.", "startOffset": 11, "endOffset": 31}, {"referenceID": 7, "context": "9 BilBOWA: Gouws et al. (2015) 51.6 - 55.7 - 57.5 - 54.9 Vuli\u0107 and Moens (2015) 68.", "startOffset": 11, "endOffset": 80}, {"referenceID": 21, "context": "For a fairer comparison to their work, we also use the same Treetagger (Schmid, 1995) to lemmatize the output of our combined model before evaluation.", "startOffset": 71, "endOffset": 85}, {"referenceID": 15, "context": "We evaluated on English monolingual similarity on WordSim353 (WS-EN), RareWord (RW-En) and German version of WordSim353 (WS-De) (Finkelstein et al., 2001; Luong et al., 2013; Luong et al., 2015).", "startOffset": 128, "endOffset": 194}, {"referenceID": 16, "context": "We evaluated on English monolingual similarity on WordSim353 (WS-EN), RareWord (RW-En) and German version of WordSim353 (WS-De) (Finkelstein et al., 2001; Luong et al., 2013; Luong et al., 2015).", "startOffset": 128, "endOffset": 194}, {"referenceID": 15, "context": "Our experiment setting is similar with Luong et al. (2015). We evaluated on English monolingual similarity on WordSim353 (WS-EN), RareWord (RW-En) and German version of WordSim353 (WS-De) (Finkelstein et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 10, "context": "B as el in es Klementiev et al. (2012) 23.", "startOffset": 14, "endOffset": 39}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 34.", "startOffset": 12, "endOffset": 28}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 34.6 39.8 20.5 Hermann and Blunsom (2014) 28.", "startOffset": 12, "endOffset": 70}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 34.6 39.8 20.5 Hermann and Blunsom (2014) 28.3 19.8 13.6 Luong et al. (2015) 47.", "startOffset": 12, "endOffset": 105}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 34.6 39.8 20.5 Hermann and Blunsom (2014) 28.3 19.8 13.6 Luong et al. (2015) 47.4 49.3 25.3 Gouws and S\u00f8gaard (2015) 67.", "startOffset": 12, "endOffset": 145}, {"referenceID": 15, "context": "performed both Luong et al. (2015) and Gouws and S\u00f8gaard (2015)6 which represent the best published crosslingual embeddings trained on bitext and monolingual data respectively.", "startOffset": 15, "endOffset": 35}, {"referenceID": 15, "context": "performed both Luong et al. (2015) and Gouws and S\u00f8gaard (2015)6 which represent the best published crosslingual embeddings trained on bitext and monolingual data respectively.", "startOffset": 15, "endOffset": 64}, {"referenceID": 22, "context": "However, the best monolingual methods use massive monolingual data (Shazeer et al., 2016), WordNet and output of commercial search engines (Yih and Qazvinian, 2012).", "startOffset": 67, "endOffset": 89}, {"referenceID": 14, "context": "7 The train and test data are from Reuter RCV1/RCV2 corpus (Lewis et al., 2004).", "startOffset": 59, "endOffset": 79}, {"referenceID": 11, "context": "The experimental setup is from Klementiev et al. (2012).7 The train and test data are from Reuter RCV1/RCV2 corpus (Lewis et al.", "startOffset": 31, "endOffset": 56}, {"referenceID": 11, "context": "8 Klementiev et al. (2012) 77.", "startOffset": 2, "endOffset": 27}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 91.", "startOffset": 12, "endOffset": 28}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 91.8 74.2 83.0 Hermann and Blunsom (2014) 86.", "startOffset": 12, "endOffset": 70}, {"referenceID": 2, "context": "3 Chandar A P et al. (2014) 91.8 74.2 83.0 Hermann and Blunsom (2014) 86.4 74.7 80.6 Luong et al. (2015) 88.", "startOffset": 12, "endOffset": 105}, {"referenceID": 11, "context": "prior work, we also use monolingual data from the RCV1/RCV2 corpus (Klementiev et al., 2012; Gouws et al., 2015; Chandar A P et al., 2014).", "startOffset": 67, "endOffset": 138}, {"referenceID": 7, "context": "prior work, we also use monolingual data from the RCV1/RCV2 corpus (Klementiev et al., 2012; Gouws et al., 2015; Chandar A P et al., 2014).", "startOffset": 67, "endOffset": 138}], "year": 2016, "abstractText": "Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-theart performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task.", "creator": "LaTeX with hyperref package"}}}