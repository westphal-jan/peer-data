{"id": "1607.00030", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "HUME: Human UCCA-Based Evaluation of Machine Translation", "abstract": "Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the MT output, providing a more fine-grained analysis of translation quality, and enables the construction and tuning of semantics-based MT. We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme. HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output. We experiment with four language pairs, demonstrating HUME's broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores.", "histories": [["v1", "Thu, 30 Jun 2016 20:35:47 GMT  (1078kb,D)", "https://arxiv.org/abs/1607.00030v1", null], ["v2", "Tue, 27 Sep 2016 13:39:42 GMT  (1126kb,D)", "http://arxiv.org/abs/1607.00030v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexandra birch", "omri abend", "ondrej bojar", "barry haddow"], "accepted": true, "id": "1607.00030"}, "pdf": {"name": "1607.00030.pdf", "metadata": {"source": "CRF", "title": "HUME: Human UCCA-Based Evaluation of Machine Translation", "authors": ["Alexandra Birch", "Omri Abend", "Ond\u0159ej Bojar", "Barry Haddow"], "emails": ["a.birch@ed.ac.uk,", "oabend@cs.huji.ac.il", "bojar@ufal.mff.cuni.cz,", "bhaddow@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, this is reflected in the decreasing internationalization rates of people. All authors have contributed equally to this work. (Bojar et al., 2011) Secondly, the quality of the sentences does not appear to be able to translate the parts of the sentence poorly, and so developers cannot be put in a position to fix these errors. (Bojar et al., 2011) These problems are partly addressed by measures that decompress the rated translation. (Bojar et al., 2011)"}, {"heading": "2 Background", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "3 The HUME Measure", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Annotation Procedure", "text": "This year, it is more than ever in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place."}, {"heading": "3.2 Composite Score", "text": "We start with a very simple procedure and calculate an accuracy value. Let Green (s, t), Appropriate (s, t) and Orange (s, t) be the number of green or adequate or orange units. Let Units be the number of units marked with one of the labels. Then, the composite value of HUME is: HUME (s, t) = Green (s, t) + Appropriate (s, t) + 0.5 \u00b7 Orange (s, t) units."}, {"heading": "3.3 Annotation Interface", "text": "The user is asked to select a label for each semantic unit by clicking on the \"A,\" \"B,\" \"Green,\" \"Orange\" or \"Red\" buttons to avoid duplication, and the interface displays for each unit the translation segment aimed at it (as in \"Tom\" in Figure 2), allowing the user, especially in long sentences, to draw attention to the parts that are most likely for their judgment but can only be commented on to avoid duplicate counting. As the alignments are derived automatically and are therefore loud, the annotator is instructed to treat the text as a whole (annotation)."}, {"heading": "4 Experiments", "text": "To validate the HUME metric, we conducted an annotation experiment with one source language (English) and four target languages (Czech, German, Polish and Romanian) using public health texts. Semantically accurate translations are crucial in this area, making them particularly suitable for semantic MT assessment. HUME is evaluated for consistency (Interannotator Agreement), efficiency (time of annotation) and validity (by comparison with crowdsourcing adequacy assessments)."}, {"heading": "4.1 Datasets and Translation Systems", "text": "Using Moses (Koehn et al., 2007), we have developed phrase-based MT systems for each of the four eligible language pairs, which are based on large parallel sets of data, depending on the language pair from OPUS (Tiedemann, 2009) and the medical translation task WMT14 (Bojar et al., 2014), and provide between 45 and 85 million sets of training data. These translation systems have been used to translate texts derived from both NHS 2465 and Cochrane7 into the four languages. NHS 24 is a public body providing health and health service information in Scotland; Cochrane is an international non-governmental organisation that provides independent systematic reviews of health-related research. NHS 24 texts are taken from the \"Health A-Z\" section in NHS Inform, Cochrane and their summaries come from the website."}, {"heading": "4.2 HUME Annotation Statistics", "text": "In fact, most of us will be able to play by the rules that they have imposed on themselves, and they will be able to play by the rules that they have imposed on themselves, \"he told the German Press Agency."}, {"heading": "5 Comparison with Direct Assessment", "text": "In fact, most of the people who are able to move, are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance,"}, {"heading": "6 Comparison with HMEANT", "text": "This year is the highest in the history of the country."}, {"heading": "7 Conclusion", "text": "We have introduced HUME, a human semantic MT rating metric that addresses a wide range of semantic phenomena. We have shown that it can be commented reliably and efficiently in multiple languages, and that the quality of annotations is robust against sentence length. Comparing it to direct reviews further supports the validity of HUME. We believe that HUME and a future automated version of HUME will enable a finer-grained analysis of translation quality and will be useful in developing a more semantically conscious approach to MT.All the annotation data collected in this project, along with analysis scripts, are available online."}, {"heading": "Acknowledgments", "text": "This project was funded under the Horizon 2020 research and innovation programme of the European Union under the grant agreement 644402 (HimL). 9https: / / github.com / bhaddow / hume-emnlp16"}], "references": [{"title": "Universal conceptual cognitive annotation (ucca)", "author": ["Abend", "Rappoport2013] Omri Abend", "Ari Rappoport"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Abend et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Abend et al\\.", "year": 2013}, {"title": "Martha Palmer", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn"], "venue": "and Nathan Schneider.", "citeRegEx": "Banarescu et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "author": ["Banerjee", "Lavie2005] Satanjeev Banerjee", "Alon Lavie"], "venue": "In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Mea-", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Christian Buck", "author": ["Alexandra Birch", "Barry Haddow", "Ulrich Germann", "Maria Nadejde"], "venue": "and Philipp Koehn.", "citeRegEx": "Birch et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards a Predicate-Argument Evaluation for MT", "author": ["Bojar", "Wu2012] Ond\u0159ej Bojar", "Dekai Wu"], "venue": "In Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation,", "citeRegEx": "Bojar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2012}, {"title": "and Omar F", "author": ["Ond\u0159ej Bojar", "Milo\u0161 Ercegov\u010devi\u0107", "Martin Popel"], "venue": "Zaidan.", "citeRegEx": "Bojar et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Lucia Specia", "author": ["Ondrej Bojar", "Christian Buck", "Christian Federmann", "Barry Haddow", "Philipp Koehn", "Johannes Leveling", "Christof Monz", "Pavel Pecina", "Matt Post", "Herve Saint-Amand", "Radu Soricut"], "venue": "and Ale\u0161 Tamchyna.", "citeRegEx": "Bojar et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Alexander Tarelkin", "author": ["Alexander Chuchunkov"], "venue": "and Irina Galinskaya.", "citeRegEx": "Chuchunkov et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "author": ["George Doddington"], "venue": "In Proceedings of the second international conference on Human Language Technology Research,", "citeRegEx": "Doddington.,? \\Q2002\\E", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "Linguistic features for automatic evaluation of heterogenous mt systems", "author": ["Gim\u00e9nez", "M\u00e0rquez2007] Jes\u00fas Gim\u00e9nez", "Llu\u00eds M\u00e0rquez"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation,", "citeRegEx": "Gim\u00e9nez et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gim\u00e9nez et al\\.", "year": 2007}, {"title": "Can machine translation systems be evaluated by the crowd alone", "author": ["Timothy Baldwin", "Alistair Moffat", "Justin Zobel"], "venue": "Natural Language Engineering,", "citeRegEx": "Graham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Graham et al\\.", "year": 2015}, {"title": "Accurate evaluation of segment-level machine translation metrics", "author": ["Nitika Mathur", "Timothy Baldwin"], "venue": "In Proc. of NAACL-HLT,", "citeRegEx": "Graham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Graham et al\\.", "year": 2015}, {"title": "Improving evaluation of machine translation quality estimation", "author": ["Yvette Graham"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan-", "citeRegEx": "Graham.,? \\Q2015\\E", "shortCiteRegEx": "Graham.", "year": 2015}, {"title": "Nicola Bertoldi", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico"], "venue": "et al.", "citeRegEx": "Koehn et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Syntactic features for evaluation of machine translation", "author": ["Liu", "Gildea2005] Ding Liu", "Daniel Gildea"], "venue": "In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,", "citeRegEx": "Liu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2005}, {"title": "Structured vs. flat semantic role representations for machine translation evaluation", "author": ["Lo", "Wu2011] Chi-kiu Lo", "Dekai Wu"], "venue": "In Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation,", "citeRegEx": "Lo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lo et al\\.", "year": 2011}, {"title": "On the Reliability and Inter-Annotator Agreement of Human Semantic MT Evaluation via HMEANT", "author": ["Lo", "Wu2014] Chi-Kiu Lo", "Dekai Wu"], "venue": null, "citeRegEx": "Lo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lo et al\\.", "year": 2014}, {"title": "Maja Popovic", "author": ["Arle Richard Lommel"], "venue": "and Aljoscha Burchardt.", "citeRegEx": "Lommel et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluating Machine Translation Quality Using Short Segments Annotations", "author": ["Mach\u00e1\u010dek", "Bojar2015] Matou\u0161 Mach\u00e1\u010dek", "Ond\u0159ej Bojar"], "venue": "The Prague Bulletin of Mathematical Linguistics,", "citeRegEx": "Mach\u00e1\u010dek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mach\u00e1\u010dek et al\\.", "year": 2015}, {"title": "Measuring semantic preservation in machine translation with HCOMET: human cognitive metric for evaluating translation", "author": ["Pedro Marinotti"], "venue": null, "citeRegEx": "Marinotti.,? \\Q2014\\E", "shortCiteRegEx": "Marinotti.", "year": 2014}, {"title": "Discriminant-based mrs banking", "author": ["Oepen", "L\u00f8nning2006] Stephan Oepen", "Jan Tore L\u00f8nning"], "venue": "In Proceedings of LREC,", "citeRegEx": "Oepen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Oepen et al\\.", "year": 2006}, {"title": "Josef van Genabith", "author": ["Karolina Owczarzak"], "venue": "and Andy Way.", "citeRegEx": "Owczarzak et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Todd Ward", "author": ["Kishore Papineni", "Salim Roukos"], "venue": "and Wei-Jing Zhu.", "citeRegEx": "Papineni et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Eva Haji\u010dov\u00e1", "author": ["Petr Sgall"], "venue": "and Jarmila Panevov\u00e1.", "citeRegEx": "Sgall et al.1986", "shortCiteRegEx": null, "year": 1986}, {"title": "Linnea Micciulla", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz"], "venue": "and John Makhoul.", "citeRegEx": "Snover et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Omri Abend", "author": ["Elior Sulem"], "venue": "and Ari Rappoport.", "citeRegEx": "Sulem et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "News from OPUS \u2013 a collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann"], "venue": "In Recent Advances in Natural Language Processing,", "citeRegEx": "Tiedemann.,? \\Q2009\\E", "shortCiteRegEx": "Tiedemann.", "year": 2009}], "referenceMentions": [], "year": 2016, "abstractText": "Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the MT output, thus providing a more fine-grained analysis of translation quality, and enabling the construction and tuning of semantics-based MT. We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme. HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output. We experiment with four language pairs, demonstrating HUME\u2019s broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores.", "creator": "LaTeX with hyperref package"}}}