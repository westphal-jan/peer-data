{"id": "1607.00360", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "A scaled Bregman theorem with applications", "abstract": "Bregman divergences play a central role in the design and analysis of a range of machine learning algorithms. This paper explores the use of Bregman divergences to establish reductions between such algorithms and their analyses. We present a new scaled isodistortion theorem involving Bregman divergences (scaled Bregman theorem for short) which shows that certain \"Bregman distortions'\" (employing a potentially non-convex generator) may be exactly re-written as a scaled Bregman divergence computed over transformed data. Admissible distortions include geodesic distances on curved manifolds and projections or gauge-normalisation, while admissible data include scalars, vectors and matrices.", "histories": [["v1", "Fri, 1 Jul 2016 19:27:28 GMT  (1273kb,D)", "http://arxiv.org/abs/1607.00360v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["richard nock", "aditya krishna menon", "cheng soon ong"], "accepted": true, "id": "1607.00360"}, "pdf": {"name": "1607.00360.pdf", "metadata": {"source": "CRF", "title": "A scaled Bregman theorem with applications", "authors": ["Richard Nock", "Aditya Krishna Menon", "Cheng Soon Ong"], "emails": ["chengsoon.ong}@data61.csiro.au"], "sections": [{"heading": null, "text": "Our theorem enables us to take advantage of the wealth and convenience of Bregman divergences in analyzing algorithms based on the above-mentioned Bregman distortions, illustrated by three novel applications of our theorem: a reduction of the density ratio of multiple classes to an estimate of class probability, a new adaptive projection yet standard-enforcing dual-standard mirror descendant algorithm, and a reduction of clusters to flat manifolds on clusters to curved manifolds. Experiments in each of these areas confirm the analyses and suggest that the scaled Bregman theorem could be a worthy addition to the popular handful of Bregman divergences that are ubiquitous in machine learning."}, {"heading": "1 Introduction: Bregman divergences as a reduction tool", "text": "Bregman divergences play a central role in the conception and analysis of a series of machine learning algorithms that we have developed over the past few years in various areas (including the United Kingdom, France, Great Britain, Great Britain, France, Great Britain, Great Britain, Italy, Italy, Italy, Italy, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, France, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, France, Great Britain, Great Britain, Great Britain, France, Great Britain, Great Britain, France, Great Britain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain,"}, {"heading": "2 Main result: the scaled Bregman theorem", "text": "In the remaining, [k] n, [k] n and [k] n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n n (n) n (n) n (n) n (n) n (n (n) n (n (n) n (n (n) n (n (n) n (n (n) n n (n (n) n n n n n (n (n) n n n n n (n (n) n n n n n (n) n n n n n n (n) n n n n n n n (n (n) n) n n n n n n n n n (n n n n (n) n) n n n n) n n n n n n n n (n n) n) n n n n n n (n) n n n (n) n) n n n) n) n n n n (n) n n n) n n n n (n n n n n) n n) n n n (n) n n) n n n n n (n n n) n) n n n) n n n n) n n n (n n (n) n) n n n) n n n) n (n n n n) n n (n) n n) n (n) n n) n (n) n n) n (n) n n n n (n) n (n n n n) n) n) n (n) n (n) n) n n (n n) n) n (n n) n n (n) n) n n (n) n (n n n n) n (n) n n (n"}, {"heading": "3 Multiclass density-ratio estimation via class-probability estimation", "text": "Given a number of density ratios, it is important to estimate the ratio between each density and any reference density, which has implications for the covariate shift problem, in which distributions differ across instances. However, our first application of Theorem 1 is to show how the density ratio can be reduced to class probability. [Buja et al.] is not negative [Boyd and Vandenberghe, 2004, Section 3.1.3].2We stress that this condition only needs to be kept at Xg. [It would not really be interesting to be homogeneous in its domain."}, {"heading": "4 Dual norm mirror descent: projection-free online learning on Lp balls", "text": "We must predict a target value that we will vectorwt with our current weight \u2212 1. The true target value will be revealed where we do not yet know whether we can achieve it. \u2212 We must predict a target value that we will achieve. \u2212 We must predict a target value y = w > t with our current weight value vectorwt \u2212 1. The true target value will then be revealed where we do not know it. \u2212 We must predict a target value y = w > t-1xt with our current target value vectorwt \u2212 1. The actual target value will be revealed when we do not know it. \u2212 We must predict a target value y = w > t \u2212 t with our current target value vectorwt \u2212 1."}, {"heading": "5 Clustering on a manifold via data transformation", "text": "In fact, it is not so that one sees oneself in a position to abide by the rules. (...) It is not so that one must abide by the rules. (...) It is not so that one must abide by the rules. (...) It is not so that one must abide by the rules. (...) It is so that one must abide by the rules. (...) \"It is not so that one must abide by the rules. (...)\" It is not so that one must abide by the rules. \"(...) It is as if one must abide by the rules.\" (...) \"It is so.\" (...). \"(...).\" (...). \"(It.\" (...). \"(It.\" (...). \"(It.).\" (It.). \"(It.).\" (It. \"(It.).\" (It. \"(It.).\" (It. \"(It.).\" (It.). \"(It.\" (It.). \"(It.\" (It.). \"(It.\" (It.). \"(It.\" (It.). \"(It.\" (It.). \"(It.\" (It.). \"(It.\" (It.). \"(It.\" (It.). \"(It.\" (It.). \"(It.\" (It.). \"(It.\" (it.). \"(it.\" (it. \"(it.).\" (it.). \"(it.\" (it.). \"(it.\" (it.). \"(it.). (it.\" (it.). \"(it.\" (it.). \"(it.\" (it.). (it. \"(it.).\" (it.). (it. \"(it.).\" (it. (it. \"(it.).\" (it.). (it.). \"(it. (it. (it.). (it.). (it.\" (it.). (it.). \"(it.).\" (it. \"(it."}, {"heading": "6 Experimental validation", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "7 Conclusion", "text": "We presented a new scaled Bregman identity (Theorem 1) and used it to derive new results in estimating the polydensity ratio, adaptive filtering and clustering on curved manifolds. We believe that, like other established properties of Bregman divergences, there is potential for several other applications of the result; Appendix E, F present preliminary thoughts in this direction."}, {"heading": "A Additional helper lemmas", "text": "This implies a useful relationship between the gradients and the gradients. The first identity becomes in the second episode of the second episode of the second episode of the second episode of the second episode of the second episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode of the third episode. The first episode of the second episode of the third episode of the third episode of the third episode is the third episode of the third episode. The second episode of the third episode of the third episode is the third episode of the third episode."}, {"heading": "B Proofs of results in main body", "text": "We present evidence of all the results of the main body. \u2212 Proof of theorem-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-y-"}, {"heading": "C Working out examples of Table 7", "text": "We also provide the form of the corresponding divergences (x) and distortions (x) in the extended table 7 row I - for X = x = x x x x x q = x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "D Going deep: higher-order identities", "text": "We can generalize theorem 1 to \"identities of higher order.\" Let's look at k > 0 as an integer, and let's allow g1, g2,..., gk: \"x,\" \"R,\" a sequence of different functions. \"For each,\" \"each,\" \"each,\" \"each,\" \"each,\" \"each,\" \"each,\" \"each,\" \"\" each, \"\" \"each,\" \"\" each, \"\" each, \"\" \"each,\" \"each,\" \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\" each, \"each,\""}, {"heading": "E Additional application: exponential families", "text": "Let us be the cumulative function of a regular exponential family with pdf's (. | \u03b8) in which \u03b8's natural parameter is. Let. (.) be a norm on X. Let X\u044b be the reflection of the application of X to the ball of the uniform norm defined by x 7. (1 / \u0432 (x) \u00b7 x. Let it be the reflection of system X. For two systems, let it be the KL divergence between the two densities p\u0432 (. | \u03b8) and p\u0432 (. | TB). Lemma 18 For each convex form that is positively 1-homogeneous, the KL divergence between two members of the same exponential family can be. (.) Lemma 18 For each convex form that is positively 1-homogeneous, the KL divergence between two members of the same exponential family."}, {"heading": "F Additional application: computational geometry, nearest neighbor rules", "text": "Two important objects of central importance in (120) It turns out that each divergence produces a sphere of the second type, which is not necessarily a sector. (12x) There are two types of (dual) balls that can be defined, the first or second types, in which the variable x is placed in the left or right position respectively. (12x) The first type of balls is convex, while the second type of balls is not necessarily convex. (12x) It turns out that the second type of balls is not necessarily convex. (120) It turns out that each divergence is a sphere of the second type (with center c and \"radius\" r) is defined as B. (c, r) The first type of balls is each placed in the left or right position. (12x) The second type of balls is convex, while the second type of balls is not necessarily convex of the second sphere, each one is (120)."}, {"heading": "G Review: binary density ratio estimation", "text": "For completeness, let's quickly consider the central result of Menon and Ong [2016 = P = P = P (Y = P = P). Let's (P, Q, \u03c0) Densities (P = P) (X \u2212 Y = P), P (Y = 1) and M (X = x) (X = x), respectively. Let's let r (X = x) be the class probability function. = Then we have the following, which refers to [Menon and Ong, 2016, Proposition 6] in the case of number 6 = 12.Lemma 21 Given a class probability estimator digit: X \u2192 [0, 1], let's come to the density estimator r (x)."}, {"heading": "H Additional experiments", "text": "We consider a distribution in which the class conditions Pr (X | Y = c) are multivariate Gaussians with mean values \u00b5c and covariance \u03c32c \u00b7 Id. Since the class conditions each have a closed form, we can explicitly draw both the density ratio r to the reference class c \u0445 = C. For fixed class values before \u03c0 = Pr (Y = c), we draw NTr samples from Pr (X, Y), and the density ratio r to the reference class c \u0445 = C. For fixed class targets before that, we prefer NTr samples from Pr (Y = c), we draw NTr samples from Pr (X, Y). From this, we estimate class profiling by means of multiclass logistic regression. This can be regarded as minimizing the EX class M [DKs (X)."}, {"heading": "I Comment: Theorem 1 is a scaled isometry in disguise (sometimes)", "text": "Theorem 1 does indeed mean an isometry, but an adaptive one in the sense that all the metrics involved depend on all parameters, in particular on the points involved in the divergences (see Figure 5). Indeed, a simple Taylor expansion of the equation (2) (main file) shows that such a Bregman distortion can be expressed with a doubly differentiated generator as follows: D (x-y) = 1 2 \u00b7 (x-y) > H (x-y), (135) for a Hessian value as a function of x, y (see, for example, [Kivinen et al., 2006, Appendix I], [Amari and Nagaoka, 2000]). Considering that both E and E are differentiable twice, this results in eq (3) as a function of x, y-y (not)."}], "references": [{"title": "Bregman divergences and triangle inequality", "author": ["S. Acharyya", "A. Banerjee", "D. Boley"], "venue": "In SDM,", "citeRegEx": "Acharyya et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Acharyya et al\\.", "year": 2013}, {"title": "Loss functions for binary class probability estimation and classification", "author": ["J.-D. Boissonnat", "F. Nielsen", "R. Nock"], "venue": "Bregman Voronoi diagrams. DCG,", "citeRegEx": "Boissonnat et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Boissonnat et al\\.", "year": 2003}, {"title": "Spherical averages and applications to spherical splines and interpolation", "author": ["S.-R. Buss", "J.-P. Fillmore"], "venue": "Structure and applications,", "citeRegEx": "Buss and Fillmore.,? \\Q2005\\E", "shortCiteRegEx": "Buss and Fillmore.", "year": 2005}, {"title": "Spherical k-means++ clustering", "author": ["Y. Endo", "S. Miyamoto"], "venue": "In Proc. of the 12 MDAI, pages 103\u2013114,", "citeRegEx": "Endo and Miyamoto.,? \\Q2008\\E", "shortCiteRegEx": "Endo and Miyamoto.", "year": 2008}, {"title": "Revisiting Frank-Wolfe: Projection-free sparse convex optimization", "author": ["M. Jaggi"], "venue": "ICML,", "citeRegEx": "Jaggi.,? \\Q2013\\E", "shortCiteRegEx": "Jaggi.", "year": 2013}, {"title": "Linking losses for class-probability and density ratio estimation", "author": ["Menon", "C.-S. Ong"], "venue": "In ICML,", "citeRegEx": "Menon and Ong.,? \\Q2009\\E", "shortCiteRegEx": "Menon and Ong.", "year": 2009}, {"title": "Bregman divergences and surrogates for learning", "author": ["R. Nock", "F. Nielsen"], "venue": "Machine Learning,", "citeRegEx": "Nock and Nielsen.,? \\Q2005\\E", "shortCiteRegEx": "Nock and Nielsen.", "year": 2005}, {"title": "Information, divergence and risk for binary experiments", "author": ["M. Reid", "R. Williamson"], "venue": "JMLR, 12:731\u2013817,", "citeRegEx": "Reid and Williamson.,? \\Q2011\\E", "shortCiteRegEx": "Reid and Williamson.", "year": 2011}, {"title": "Hyperbolic centroidal Voronoi tessellation", "author": ["G. Rong", "M. Jin", "X. Guo"], "venue": "th ACM SPM,", "citeRegEx": "2010", "shortCiteRegEx": "2010", "year": 2010}, {"title": "The dynamics of coarsening in highly anisotropic systems: Si particles in Al\u2212Si liquids", "author": ["A.-J. Shahani", "E.-B. Gulsoy", "V.-J. Roussochatzakis", "J.-W. Gibbs", "J.-L. Fife", "P.-W. Voorhees"], "venue": "Acta Materialia,", "citeRegEx": "Shahani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shahani et al\\.", "year": 2015}, {"title": "A mixture of Manhattan frames: Beyond the Manhattan world", "author": ["J. Straub", "G. Rosman", "O. Freifeld", "J.-J. Leonard", "J.-W. Fisher III"], "venue": "In Proc. of the 27 IEEE CVPR,", "citeRegEx": "Straub et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Straub et al\\.", "year": 2014}, {"title": "Real-time Manhattan world rotation estimation in 3d", "author": ["J. Straub", "N. Bhandari", "J.-J. Leonard", "J.-W. Fisher III"], "venue": "In Proc. of the 27 IROS,", "citeRegEx": "Straub et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Straub et al\\.", "year": 2015}, {"title": "Small-variance nonparametric clustering on the hypersphere", "author": ["J. Straub", "T. Campbell", "J.-P. How", "J.-W. Fisher III"], "venue": "In Proc. of the 28 IEEE CVPR,", "citeRegEx": "Straub et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Straub et al\\.", "year": 2015}, {"title": "A Dirichlet process mixture model for spherical data", "author": ["J. Straub", "J. Chang", "O. Freifeld", "J.-W. Fisher III"], "venue": "In Proc. of the 18 AISTATS,", "citeRegEx": "Straub et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Straub et al\\.", "year": 2015}, {"title": "Subspace clustering", "author": ["R. Vidal"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Vidal.,? \\Q2011\\E", "shortCiteRegEx": "Vidal.", "year": 2011}, {"title": "First-order methods for geodesically convex optimization", "author": ["H. Zhang", "S. Sra"], "venue": null, "citeRegEx": "Zhang and Sra.,? \\Q2016\\E", "shortCiteRegEx": "Zhang and Sra.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": ", 2016], and computational geometry [Boissonnat et al., 2010]. Despite these being very different applications, many of these algorithms and their analyses basically rely on three beautiful analytic properties of Bregman divergences, properties that we summarize for differentiable scalar convex functions \u03c6 with derivative \u03c6\u2032, conjugate \u03c6, and divergence D\u03c6: \u2022 the triangle equality: D\u03c6(x\u2016y) +D\u03c6(y\u2016z)\u2212D\u03c6(x\u2016z) = (\u03c6\u2032(z)\u2212 \u03c6\u2032(y))(x\u2212 y); \u2022 the dual symmetry property: D\u03c6(x\u2016y) = D\u03c6?(\u03c6\u2032(y)\u2016\u03c6\u2032(x)); \u2022 the right-centroid (population minimizer) is the average: arg min\u03bc E[D\u03c6(X\u2016\u03bc)] = E[X]. Casting a problem as a Bregman minimisation allows one to employ these properties to simplify analysis; for example, by interpreting mirror descent as applying a particular Bregman regulariser, Beck and Teboulle [2003] relied on the triangle equality above to simplify its proof of convergence.", "startOffset": 37, "endOffset": 799}, {"referenceID": 1, "context": ", 2016], and computational geometry [Boissonnat et al., 2010]. Despite these being very different applications, many of these algorithms and their analyses basically rely on three beautiful analytic properties of Bregman divergences, properties that we summarize for differentiable scalar convex functions \u03c6 with derivative \u03c6\u2032, conjugate \u03c6, and divergence D\u03c6: \u2022 the triangle equality: D\u03c6(x\u2016y) +D\u03c6(y\u2016z)\u2212D\u03c6(x\u2016z) = (\u03c6\u2032(z)\u2212 \u03c6\u2032(y))(x\u2212 y); \u2022 the dual symmetry property: D\u03c6(x\u2016y) = D\u03c6?(\u03c6\u2032(y)\u2016\u03c6\u2032(x)); \u2022 the right-centroid (population minimizer) is the average: arg min\u03bc E[D\u03c6(X\u2016\u03bc)] = E[X]. Casting a problem as a Bregman minimisation allows one to employ these properties to simplify analysis; for example, by interpreting mirror descent as applying a particular Bregman regulariser, Beck and Teboulle [2003] relied on the triangle equality above to simplify its proof of convergence. Another intriguing possibility is that one may derive reductions amongst learning problems by connecting their underlying Bregman minimisations. Menon and Ong [2016] recently established how (binary) density ratio estimation (DRE) can be exactly reduced to class-probability estimation (CPE).", "startOffset": 37, "endOffset": 1041}, {"referenceID": 0, "context": "Hence, Bregman divergences can embed several distances in a different \u2014 and arguably less involved \u2014 way than the transformations known to date [Acharyya et al., 2013].", "startOffset": 144, "endOffset": 167}, {"referenceID": 0, "context": "Hence, Bregman divergences can embed several distances in a different \u2014 and arguably less involved \u2014 way than the transformations known to date [Acharyya et al., 2013]. As with the aforementioned key properties of Bregman divergences, Theorem 1 has potentially wide applicability. We present three such novel applications (see Table 1) to vastly different problems: \u2022 a reduction of multiple density ratio estimation to multiclass-probability estimation (\u00a73), generalising the results of Menon and Ong [2016] for the binary label case, \u2022 a projection-free yet norm-enforcing mirror gradient algorithm (enforced norms are those of mirrored vectors and of the offset) with guarantees for adaptive filtering (\u00a74), and \u2022 a seeding approach for clustering on positively or negatively (constant) curved manifolds based on a popular seeding for flat manifolds and with the same approximation guarantees (\u00a75).", "startOffset": 145, "endOffset": 509}, {"referenceID": 5, "context": "For the special case where X = R, and g(x) = 1 + x, Theorem 1 is exactly Menon and Ong [2016, Lemma 2] (c.f. Equation 1). We wish to highlight a few points with regard to our more general result. First, the \u201cdistortion\u201d generator \u03c6\u2020 may be1 non-convex, as the following illustrates. Example. Suppose \u03c6(x) = 2\u2016x\u20162 corresponds to the generator for squared Euclidean distance. Then, for g(x) = 1 + 1>x, we have \u03c6\u2020(x) = 12 \u00b7 \u2016x\u201622 1+1>x , which is non-convex on X = R . When \u03c6\u2020 is non-convex, the right hand side in Equation 3 is an object that ostensibly bears only a superficial similarity to a Bregman divergence; it is somewhat remarkable that Theorem 1 shows this general \u201cdistortion\u201d between a pair (x,y) to be entirely equivalent to a (scaling of a) Bregman divergence between some transformation of the points. Second, when g is linear, Equation 3 holds for any convex \u03c6. (This was the case considered in Menon and Ong [2016].) When g is non-linear, however, \u03c6 must be chosen carefully so that (\u03c6, g) satisfies the restricted homogeneity conditon2 of Equation 5.", "startOffset": 73, "endOffset": 930}, {"referenceID": 14, "context": "Our final application can be related to two problems that have received a steadily growing interest over the past decade in unsupervised machine learning: clustering on a non-linear manifold [Dhillon and Modha, 2001], and subspace custering [Vidal, 2011].", "startOffset": 241, "endOffset": 254}, {"referenceID": 10, "context": "We emphasize the fact that the clustering problem has significant practical impact for d as small as 2 in computer vision [Straub et al., 2014].", "startOffset": 122, "endOffset": 143}, {"referenceID": 15, "context": "Second, the fact that the manifold has non-zero curvature essentially prevents the direct use of Euclidean optimization algorithms [Zhang and Sra, 2016] \u2014 put simply, the average of two points", "startOffset": 131, "endOffset": 152}, {"referenceID": 6, "context": "Our final application can be related to two problems that have received a steadily growing interest over the past decade in unsupervised machine learning: clustering on a non-linear manifold [Dhillon and Modha, 2001], and subspace custering [Vidal, 2011]. We consider two fundamental manifolds investigated by Galperin [1993] to compute centers of mass from relativistic theory: the sphere S and the hyperboloid H, the former being of positive curvature, and the latter of negative curvature.", "startOffset": 242, "endOffset": 326}, {"referenceID": 3, "context": "The key to using the approximation property of k-means++ relies on the existence of a coordinate system on the sphere for which the cluster centroid is just the average of the cluster points (polar coordinates), an average that eventually has to be rescaled if the coordinate system is not that one [Dhillon and Modha, 2001, Endo and Miyamoto, 2015]. The existence of this coordinate system makes that the proof of Arthur and Vassilvitskii [2007] (and in particular the key Lemmata 3.", "startOffset": 325, "endOffset": 447}, {"referenceID": 3, "context": "The key to using the approximation property of k-means++ relies on the existence of a coordinate system on the sphere for which the cluster centroid is just the average of the cluster points (polar coordinates), an average that eventually has to be rescaled if the coordinate system is not that one [Dhillon and Modha, 2001, Endo and Miyamoto, 2015]. The existence of this coordinate system makes that the proof of Arthur and Vassilvitskii [2007] (and in particular the key Lemmata 3.2 and 3.3) can be carried out without modification to yield the same approximation ratio as that of Arthur and Vassilvitskii [2007] if the distortion at hand is the squared Euclidean distance, which turns out to be Drec(.", "startOffset": 325, "endOffset": 616}, {"referenceID": 2, "context": "Row III \u2014 As in Buss and Fillmore [2001], we assume \u2016x\u20162 \u2264 \u03c0, or we renormalize or change the radius of the ball) We first lift the data points using the Sphere lifting map R 3 x 7\u2192 x \u2208 R: x .", "startOffset": 16, "endOffset": 41}, {"referenceID": 1, "context": "Proof We know that KL(\u03b8\u2016\u03b8\u2032) = D\u03c6(\u03b8\u2032\u2016\u03b8) Boissonnat et al. [2010]. Hence, D\u03c6\u2020(\u03b8 \u2032\u2016\u03b8) = \u03a9(\u03b8) \u00b7D\u03c6 ( 1 \u03a9(\u03b8) \u00b7 \u03b8\u2032\u2016 1 \u03a9(\u03b8\u2032) \u00b7 \u03b8 )", "startOffset": 39, "endOffset": 64}, {"referenceID": 6, "context": ", 2010]; \u2022 range search using ball trees on D\u03c6\u2020 can be efficiently implemented using Bregman divergence D\u03c6 on Xg [Cayton, 2009]; \u2022 the minimum enclosing ball problem, the one-class clustering problem (an important problem in machine learning), with balls of the second type on D\u03c6\u2020 can be solved via the minimum Bregman enclosing ball problem on D\u03c6 [Nock and Nielsen, 2005].", "startOffset": 348, "endOffset": 372}], "year": 2016, "abstractText": "Bregman divergences play a central role in the design and analysis of a range of machine learning algorithms. This paper explores the use of Bregman divergences to establish reductions between such algorithms and their analyses. We present a new scaled isodistortion theorem involving Bregman divergences (scaled Bregman theorem for short) which shows that certain \u201cBregman distortions\u201d (employing a potentially non-convex generator) may be exactly re-written as a scaled Bregman divergence computed over transformed data. Admissible distortions include geodesic distances on curved manifolds and projections or gauge-normalisation, while admissible data include scalars, vectors and matrices. Our theorem allows one to leverage to the wealth and convenience of Bregman divergences when analysing algorithms relying on the aforementioned Bregman distortions. We illustrate this with three novel applications of our theorem: a reduction from multi-class density ratio to class-probability estimation, a new adaptive projection free yet norm-enforcing dual norm mirror descent algorithm, and a reduction from clustering on flat manifolds to clustering on curved manifolds. Experiments on each of these domains validate the analyses and suggest that the scaled Bregman theorem might be a worthy addition to the popular handful of Bregman divergence properties that have been pervasive in machine learning.", "creator": "LaTeX with hyperref package"}}}