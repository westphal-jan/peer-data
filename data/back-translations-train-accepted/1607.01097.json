{"id": "1607.01097", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2016", "title": "AdaNet: Adaptive Structural Learning of Artificial Neural Networks", "abstract": "We present a new theoretical framework for analyzing and learning artificial neural networks. Our approach simultaneously and adaptively learns both the structure of the network as well as its weights. The methodology is based upon and accom- panied by strong data-dependent theoretical learning guarantees, so that the final network architecture provably adapts to the complexity of any given problem.", "histories": [["v1", "Tue, 5 Jul 2016 02:51:33 GMT  (65kb,D)", "http://arxiv.org/abs/1607.01097v1", null], ["v2", "Sat, 19 Nov 2016 00:46:26 GMT  (66kb,D)", "http://arxiv.org/abs/1607.01097v2", null], ["v3", "Tue, 28 Feb 2017 02:58:11 GMT  (239kb,D)", "http://arxiv.org/abs/1607.01097v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["corinna cortes", "xavier gonzalvo", "vitaly kuznetsov", "mehryar mohri", "scott yang"], "accepted": true, "id": "1607.01097"}, "pdf": {"name": "1607.01097.pdf", "metadata": {"source": "CRF", "title": "AdaNet: Adaptive Structural Learning of Artificial Neural Networks", "authors": ["Corinna Cortes", "Xavi Gonzalvo"], "emails": ["corinna@google.com", "xavigonzalvo@google.com", "vitalyk@google.com", "mohri@cims.nyu.edu", "yangs@cims.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Work", "text": "Extensive work has been done on structural learning for neural networks (e.g. [Kwok and Yeung, 1997, Leung et al., 2003, Islam et al., 2003, Lehtokangas, 1999, Islam et al., 2009]) all of these publications attempt to grow and curtail the architecture of neural networks using some heuristic (e.g. genetic, information theory, or correlation) algorithms based directly on the optimization of generalization performance, which is precisely the learning goal in batch setting.From a theoretical perspective, there are several important research lines on the theoretical understanding of neural networks, the first of which deals with understanding the properties of objective function used in the training of neural networks. (e.g. [Choromanska et al., 2014, Sagun et al., 2014, Zhang et al., 2015, Livni et al., 2014]."}, {"heading": "3 Preliminaries", "text": "Let X designate the input room. We consider the standard monitored binary classification szenario and assume that training and test points will be drawn according to any distribution. (...) Each x-X-X-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "4 Theoretical properties of artificial neural networks", "text": "We measure the performance of a hypothesis f-H based on its expected loss over the data distribution D, also known as a generalization error: R (f) = E (x, y) \u0445 D [1yf (x) \u2264 0]. Typically, we also want to measure the performance of the model using our training sample S. This will be done using the empirical margin loss: R-S, \u03c1 (f) = 1m \u00b2 i = 1 1yif (xi) \u2264 \u03c1, where the margin refers to the respective term. Hypothesis functions that allow a large margin loss with a small empirical margin loss intuitively represent classifiers with high accuracy. In view of a hypothesis that H sets the function assignment from X to R, we call R-S (H) the empirical radiator complexity of H for the sample S: R-S (H) = 1 m Ekl."}, {"heading": "4.1 Learning bounds on the Rademacher complexity of network hypothesis sets", "text": "Since neural networks are constructed as compositions of these layers, it is natural that we analyze the complexity of each layer in relation to the complexity of its previous layer. (...) Our first result shows that this can actually be done, and that the empirical complexity of each intermediate layer k in the network is limited by the empirical complexity of its input times, which depends on a power of the size of the layer: Lemma 1. Let us leave 1p + 1 q = 1. Then the empirical level k in the network will be limited by the empirical complexity of H (p) k for a sample S of size m, as follows: H (p) k \u2212 1: R (H) k) k (p) k (k)."}, {"heading": "5 Algorithm", "text": "This section describes our adaptive deep learning algorithm, ADANET. ADANET adaptively extends the structure of a neural network by balancing the complexity of the model with margin maximization, starting with a high-level description of the algorithm before moving on to a more detailed representation."}, {"heading": "5.1 Overview", "text": "Our algorithm starts with the network reduced to the input layer, according to the input feature vector and an output unit that is fully connected, and expands or modifies the network via T-rounds. At each turn, the network is either expanded by a new node or, in all cases, the function h-H (p) k of an existing node in layer k is selected. A new node may already be populated in each layer k-1, l) or begin on a new layer, but in all cases, it is selected by linking only to existing nodes in the layer below and connecting to the output unit. Existing nodes are either those of the input layer or nodes previously added to the network by the algorithm. Figure 1 (a) illustrates this design. Choosing the node to construct or update in each round is a key associated aspect of our algorithm."}, {"heading": "5.2 Objective function", "text": "Remember the definition of our hypotheses space conv (H) = l k = 1 (H (p) k = 1 (H (p) k)), which is the convex shell of all neural networks down to depth l and, of course, includes all neural networks of depth l - the common hypotheses space in deep learning. Note, however, that the set of functions in H is infinite, since the weights corresponding to each function are any real value within their respective lp balls.Despite this challenge, we will efficiently discover a finite subset of H, which will serve as the basis for our convex combination. Here, N also represents the maximum number of nodes in our network. Thus, N = 1 k = 1 nk, and N is also generally assumed to be very large. In addition, we will define and update our set {h1,."}, {"heading": "5.3 Coordinate descent", "text": "Let us specify wt = (wt, 1st, wt, N) > the vector obtained after t \u2265 1 iterations, and let us specify w0 = 0. Let us specify the kth vector in RN, k [1, N]. The direction and the step selected in the tth round are those which minimize F (wt \u2212 1 + \u03b7ek). Let us specify ft \u2212 1 = x N = 1 wt \u2212 1, jhj. Then we can write F (wt \u2212 1 + \u03b7ek) = 1 m \u00b2 (1 \u2212 yift \u2212 1 (xi) \u2212 \u03b7ihk (xi) + x j 6 = k \u0441j | wt \u2212 1, k | + wt \u2212 1, For each square metre \u00b2 [1, T] we will consider the following distribution Dt (1) = x \u00b2 (1 \u2212 xi) \u2212 xi \u2212 xi), where a normalization factor is a + + CISTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTY"}, {"heading": "5.4 Search and active coordinates", "text": "As already mentioned, a key aspect of our AdaNet algorithm is the construction of new hypotheses in each round. We do not list all the N hypotheses at the beginning of the algorithm because it would be extremely difficult to select good candidates before we see the data. At the same time, searching through all possible node combinations using the data would be a mathematically impossible task. Instead, our search process will be online, building on the nodes we already have in our network, selecting at most one node at a time. Specifically, the algorithm selects an anode from the following \"active\" candidates for each round: existing nodes in our network or new nodes connecting to existing nodes in one layer. There are many potential methods to construct new nodes, and at first glance, any new node with connections to existing nodes can appear as a computational obstacle."}, {"heading": "5.4.1 New candidate nodes", "text": "For a distribution D over the sample S and a tuple of hypotheses hk = (hq, 1,.), hk, nk), hk (p) k, we denote by margin (D, hk, j), and we denote by margin (D, hk), the vector of the weighted margins of all nodes in layer k: Margin (D, hk, j) = (Ei \u00b2 D [yi (k, hk, j) (xi)], and we denote by margin (D, hk), the vector of the weighted margins of all nodes in layer k: Margin (D, hk) = (Ei \u00b2 hk, 1), a vector u \u2212 Rnk, a node that is of the previous nodes in the previous layer k \u2212 k, and the new node (xi)."}, {"heading": "5.5 Pseudocode", "text": "In this section, we introduce the pseudo code for our ADANET algorithm, which applies the greedy coordinate-wise optimization method described in Section 5.3 to the target described in Section 5.2. As input, the algorithm takes the sample S, the maximum number of nodes per layer (nk) lk = 1, the complexity penalties (\u0441k) lk = 1, the lp standards of the weights defining new nodes (\u0394k) l k = 1, and the upper limits for the nodes in each layer (Ck) lk = 1. Subsequently, ADANET initializes all weights to zero, sets the distribution uniformly, and considers the active coordinate set as the starting layer in the network. Subsequently, the algorithm repeats the following sequence of steps: It compiles the results of the existing nodes in the EXISTINGNODES method, the new candidate nodes in NEWNODES, and finds the following section before determining the best precedent node (and)."}, {"heading": "5.6 Convergence of ADANET", "text": "Remarkably, the neural network that ADANET issues is competitive with the optimal weights for each sub-network it sees during training. Furthermore, it achieves this guarantee in linear time. The exact statement and proof are contained in Appendix D."}, {"heading": "5.7 Large-scale implementation of AdaNet", "text": "We describe a large-scale implementation of the ADANET optimization problem using state-of-the-art techniques from stochastic optimization in Appendix E."}, {"heading": "6 Conclusion", "text": "Our methodology optimizes the performance of generalizations and explicitly and automatically addresses the trade-off between model architecture and empirical risk mitigation - ideas that have been little explored in the field of deep learning. Our techniques are general and can be applied to other architectures of neural networks such as CNNs and LSTMs, as well as to other learning situations such as multi-class classification and regression, all of which serve as interesting avenues for future work."}, {"heading": "A Proofs of theoretical guarantees", "text": "Lemma (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p (1) p) p (1) p (1) p) p (1) p (1) p) p (1) p (1) p) p (1) p (1) p (1) p (1) p (1) p (1) p) p (1) p (1) p) p (1) p (1) p) p (1) p (1) p) p (1) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p) p (1) p) p (1) p) p (1) p) p (1) p) p) p (1) p) p (1) p) p (1) p) p (1) p) p) p (1) p (1) p) p (1) p (1) p) p (1) p) p (1) p (1) p (1) p (1) p) p (1) p) p (1) p) p (1) p (1) p (1) p (1) p) p (1) p (1) p (1) p (1) p (1) p (1) p) p) p (1) p (1) p (1) p (1) p (1) p (1) p) p) p"}, {"heading": "B Proofs for algorithmic design", "text": "B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B."}, {"heading": "C Other components of pseudocode", "text": "Figures 5, 6, 7, 8 show the remaining components of the pseudo-code for ADANET with exponential losses. The initial weight vector w0 is initialized to 0, and the initial weight distribution D1 is equal by the coordinates. Simply, the best node is the one with the highest score between all existing nodes and the new candidate nodes. The step size performed in each turn is the optimal step in the calculated direction. For exponential loss functions, this can be precisely calculated, and in general, it can be approached numerically using line search methods (since the target is convex). The updated distribution at the time t will be proportional (1 \u2212 yift \u2212 1 (xi), as explained in section 5.3."}, {"heading": "D Convergence of ADANET", "text": "Theorem 6 (w) = 1 (m). D \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i. \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i. \"i\" i \"i\" i \"i\" i. \"i\" i \"i\" i. \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i. \"i\" i \"i\" i \"i\" i \"i\" i. \"i\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i. \"i\" i \"i\" i \"i\" i. \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i. \"i\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i. \"i\" i \"i\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i \"i.\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i. \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \""}, {"heading": "E Large-scale implementation: ADANET+", "text": "In fact, most of us are able to play by the rules and do not have to play by the rules."}, {"heading": "Appendix References", "text": "A. Beck and M. Teboulle. A fast iterative shrinkage-threshold algorithm for linear inverse problems. SIAMjournal on imaging sciences, 2 (1): 183-202, 2009.R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315-323, 2013.Z.-Q. Luo and P. Tseng. On the convergence of coordinate descent method for convex differentiable minimization. Journal of Optimization Theory and Applications, 72 (1): 7 - 35, 1992.L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24 (4): 2057-2075, 2014.T. Zhao, Y. Wang, R. Arora, and H. Liu. Accelerated mini-batch randomized block coordination, 24 (4): H. 2057-20ora, Zhao, 2014.T."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We present a new theoretical framework for analyzing and learning artificial neural networks. Our approach simultaneously and adaptively learns both the structure of the network as well as its weights. The methodology is based upon and accompanied by strong data-dependent theoretical learning guarantees, so that the final network architecture provably adapts to the complexity of any given problem.", "creator": "LaTeX with hyperref package"}}}