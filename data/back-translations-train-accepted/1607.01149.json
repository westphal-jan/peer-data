{"id": "1607.01149", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2016", "title": "Target-Side Context for Discriminative Models in Statistical Machine Translation", "abstract": "Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.", "histories": [["v1", "Tue, 5 Jul 2016 08:51:21 GMT  (31kb,D)", "http://arxiv.org/abs/1607.01149v1", "Accepted as a long paper for ACL 2016"]], "COMMENTS": "Accepted as a long paper for ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ales tamchyna", "alexander m fraser", "ondrej bojar", "marcin junczys-dowmunt"], "accepted": true, "id": "1607.01149"}, "pdf": {"name": "1607.01149.pdf", "metadata": {"source": "CRF", "title": "Target-Side Context for Discriminative Models in Statistical Machine Translation", "authors": ["Ale\u0161 Tamchyna", "Alexander Fraser", "Ond\u0159ej Bojar", "Marcin Junczys-Dowmunt"], "emails": ["tamchyna@ufal.mff.cuni.cz", "bojar@ufal.mff.cuni.cz", "fraser@cis.uni-muenchen.de", "junczys@amu.edu.pl"], "sections": [{"heading": null, "text": "We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly into the decoding process. Our approach scales to large educational data sizes, resulting in consistent improvements in translation quality across four language pairs. We also offer an analysis comparing the strengths of the source context model with our extended source context and target context model, and we show that our extension allows us to better understand morphological coherence. Our work is freely available as part of Moses."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to move to another world, in which they are able, in which they are able to move, and in which they are able to move."}, {"heading": "2 Discriminative Model with Target-Side Context", "text": "Various ways to use feature-rich models in MT have been suggested, see Section 6. We describe our approach in this section."}, {"heading": "2.1 Model Definition", "text": "We designate source phrases (given a particular phrase segmentation) (f-1,.., f-m) and the individual words (f1,..., fn). We use a similar notation for target words / phrases. To simplify matters, eprev, eprev \u2212 1 denotes the words that precede the current target phrase. Assuming the target context size of two, we model the following probability distribution: P (e-f), (e-i, f-i), (e-i), p (e-i-f), f, eprev \u2212 1) (1) The probability of a translation is the product of phrasal translation probabilities conditioned on the source phrase, the full source sentence and several previous targets."}, {"heading": "2.2 Global Model", "text": "Tamchyna et al. (2014) have already integrated VW into Moses. We started from its implementation to do our work. The features of the classifiers are divided into two \"namespaces\": \u2022 S. Features that do not depend on the current phrasal translation (i.e., source and targetcontext features). \u2022 T. Features of the current phrasal translation. We use to a large extent the feature processing available in VW, namely square feature expansion1http: / / hunch.net / \u02dc vw / and label-dependent features. When generating features for a specific group of translations, we first create the common features (namespace S), which depend only on the (source and target) context and are therefore constant for all possible translations of a particular phrase. (Note that the target context naturally depends on the current partial translation."}, {"heading": "2.3 Extraction of Training Examples", "text": "The landing page of the extracted phrase is a positive designation, and all other observed phrases depend on the extracted phrase (somewhere in the training data) are the negative designations. We train our model in a similar way: For each sentence in the parallel training data, we look at all possible phrase segmentations. We then create a training example for each source span. Since we train both our model and the standard phrase table on the same data set, we use \"leave-one-out\" in the phrase table. Each translation is defined by its label-dependent characteristics and we associate with it a loss: 0 loss for the correct translation and 1 for all others. Since we train both our model and the standard phrase table on the same data set, we use a single omission (\"leave-out-one-out\") in the training to avoid fitting the phrase. We look at numbers of phrases and the occurrence counts in the training data."}, {"heading": "2.4 Training", "text": "We use Vowpal Wabbit in the --csoaa ldf mc setting, which reduces our multi-class problem to a one-against-all binary classification. Our goal is logistical loss. We experimented with various settings of L2 regularization, but could not find any improvement compared to not using regularization. We train each model with 10 iterations of the data. We evaluate all our models using a pre-assigned set. We use the same data set as for MT system tuning because it closely matches the range of our test set. We evaluate model accuracy after each run over the training data to detect mismatches and select the model with the highest accuracy."}, {"heading": "2.5 Feature Set", "text": "We use the factor setting MT (Koehn and Hoang, 2007) and represent each type of information as an individual factor. On the source page, we use only the word surface shape, its lemmas, the morphological tag, the analytical function (like Subj for the subjects), and the parent node's lemmas in the parse tree of dependence. On the target page, we use only word terminology and morphological tags.Table 1 lists our attributes for each language pair. We implemented indicator attributes for both the source and target pages; these are simply concatenations of the words in the current phrase in a single feature. Internal attributes describe words within the current phrase. Context attributes are made either from a window of a fixed size around the current phrase (on the source page) or from a limited left context."}, {"heading": "3 Efficient Implementation", "text": "Initially, we thought that using target-side context functions in decoding would be too expensive, since we would have to question our model about as often as the language model. So, in preliminary experiments, we focused on re-evaluating the n-best lists. We made small gains, but all our results were significantly worse than with the integrated model, so we omit them from the paper. We find that decoding with a feature-rich target context model is actually feasible. In this section, we describe optimizations at various stages of our pipeline that make training and conclusions about our model feasible."}, {"heading": "3.1 Feature Extraction", "text": "We have implemented feature extraction code only once, identical code is used for training and decoding, during training the generated features are written into a file, while during testing they are fed directly into the classifier via the library interface. This design decision not only ensures consistency in feature presentation, but also makes the feature extraction process efficient. In training we are easily able to use multithreading (already implemented in Moses), and since processing training data is a trivially parallel task, we can also use distributed calculations and perform separate instances of (multithreaded) Moses on multiple machines, allowing us to easily generate training files from millions of parallel sets in a short time."}, {"heading": "3.2 Model Training", "text": "VW is itself a very fast classifier, but for very large data, its training can be further accelerated by parallelization. We use the implementation of the AllReduce scheme, which we use in a grid engine environment. We mix and shred the data and then assign each shard to a workplace. AllReduce has a master job where the learned weight vector is synchronized with all workers. We compared this approach with standard one-thread one-process training and found that we get identical model accuracy. We usually use about 10-20 training jobs. In this way, we can quickly edit our large training files and train the entire model (using multiple runs over the data) within hours; effectively, neither feature extraction nor model training becomes a significant bottleneck in the entire training pipeline of the MT system."}, {"heading": "3.3 Decoding", "text": "The fact is that we are able to assert ourselves, that we are able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, and that we are going to be able to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to put ourselves in."}, {"heading": "4 Experimental Evaluation", "text": "To ensure that our method is applicable to other language pairs, we also present experiments in English, German, Polish and Romanian. In all experiments, we use Treex (Popel and Z-abokrtsky \u0301, 2010) to lemmatize and mark the source data and also to obtain dependency analyses of all English sentences."}, {"heading": "4.1 English-Czech Translation", "text": "As parallel training data we use (subsets) of the CzEng 1.0 corpus (Bojar et al., 2012). For coordination we use the WMT13 test set (Bojar et al., 2013) and we evaluate the systems on the WMT14 test set (Bojar et al., 2014). We lemmatize and mark the Czech data with Morphodita (Strakova) et al., 2014. Our baseline system is a standard phrase-based Moses setup. The phrase table in both cases is factored and also gives lemmas and morphological markings. We train a 5 gram LM on the landing side of the parallel data. We evaluate three settings in our experiment - Vanilla Phrase-based system. \u2022 + Source - our classifying system - our classification contexts and outputs. \""}, {"heading": "4.2 Additional Language Pairs", "text": "We are experimenting with the translation from English into German, Polish and Romanian. Our English-German system is based on the data available for the WMT14 translation task: Europarl (Koehn, 2005) and the Common Crawl Corpus, 3 in total about 4.3 million sentence pairs. We are tuning the system on the WMT13 test set and testing on the WMT14 set. We are using TreeTagger (Schmid, 1994) to lemmatize and mark the German data.English-Polish has not been included in the common WMT tasks so far, but was available as a language pair for several IWSLT editions focusing on the TED Talk translation. Full test sets are only available for 2010, 2011 and 2012. References for 2013 and 2014 have not been published. We are using the development set and test set from 2010 as development dates for the parameter setting. The remaining two test sets Natil (2011, 2012) are our test dates."}, {"heading": "5 Analysis", "text": "We manually analyze the results of the English systems. Figure 3 shows an example sentence from the WMT14 test sentence, which is translated by all system variants. The baseline system makes an error in the verb valence; the Czech verb \"dos-lo\" could be used, but this verb already has an (implicit) subject and the translation of \"mining\" (\"te-z-ba\") would have to be in a different case and at a different position in the sentence. However, the second error is more interesting: the baseline system cannot correctly identify the meaning of the particle \"and translates it for the purpose of\" getting around the context. \"The source context model takes the context (span of years) and disambiguates the translation of\" to, \"selects the choice of the word.\""}, {"heading": "6 Related Work", "text": "Carpuat and Wu (2007) trained a maximum entropy classifier for each type of source phrase that used source context information to make its translations unique; the models did not capture target-side information and were independent; no parameters were shared between classifiers for different phrases; they used a strong feature set originally developed for word disambiguation; Gimpel and Smith (2008) also used broader source context information but did not train a classifier; instead, the features were incorporated directly into the log-linear model of the decoder. Mauser et al. (2009) introduced the \"discriminatory word lexicon\" and trained a binary classifier for each target word, using only the bag of words (from the entire source set) as features; training sets in which the target word occurred were used as positive examples; sentences other than negative examples used for each target word were used as a discriminatory solution (2010)."}, {"heading": "7 Conclusions", "text": "We have shown that such a model can be used relatively efficiently directly during decoding. We have shown that this model continuously significantly improves the quality of English-Czech translation compared to a strong baseline with large training data. We have confirmed the effectiveness of our model on several additional language pairs. We have presented an analysis that shows concrete examples of improved lexical selection and morphological coherence. Our work is available to other researchers in the main branch of Moses."}, {"heading": "Acknowledgements", "text": "This work was funded within the framework of the Horizon 2020 research and innovation programme of the European Union under funding agreements no. 644402 (HimL) and no. 645452 (QT21), the European Research Council (ERC) under funding agreement no. 640550 and the SIA project no. 260 333. Language resources used for this work were stored and distributed within the LINDAT / CLARIN project of the Ministry of Education, Youth and Sport of the Czech Republic (project LM2015071)."}], "references": [{"title": "Findings of the 2013", "author": ["Lucia Specia"], "venue": null, "citeRegEx": "Specia.,? \\Q2013\\E", "shortCiteRegEx": "Specia.", "year": 2013}, {"title": "Better hypothesis testing", "author": ["Noah A. Smith"], "venue": null, "citeRegEx": "Smith.,? \\Q2011\\E", "shortCiteRegEx": "Smith.", "year": 2011}, {"title": "Factored translation models", "author": ["Philipp Koehn", "Hieu Hoang."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 868\u2013876,", "citeRegEx": "Koehn and Hoang.,? 2007", "shortCiteRegEx": "Koehn and Hoang.", "year": 2007}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Conference Proceedings: the tenth Machine Translation Summit, pages 79\u201386, Phuket, Thailand. AAMT, AAMT.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models", "author": ["Arne Mauser", "Sasa Hasan", "Hermann Ney."], "venue": "pages 210\u2013218, Suntec, Singapore.", "citeRegEx": "Mauser et al\\.,? 2009", "shortCiteRegEx": "Mauser et al\\.", "year": 2009}, {"title": "Wider Context by Using Bilingual Language Models in Machine Translation", "author": ["Jan Niehues", "Teresa Herrmann", "Stephan Vogel", "Alex Waibel."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 198\u2013206, Edinburgh,", "citeRegEx": "Niehues et al\\.,? 2011", "shortCiteRegEx": "Niehues et al\\.", "year": 2011}, {"title": "Minimum Error Rate Training in Statistical Machine Translation", "author": ["Franz Josef Och."], "venue": "Proc. of ACL, pages 160\u2013167, Sapporo, Japan. ACL.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "TectoMT: Modular NLP Framework", "author": ["Martin Popel", "Zden\u011bk \u017dabokrtsk\u00fd."], "venue": "Hrafn Loftsson, Eirikur R\u00f6gnvaldsson, and Sigrun Helgadottir, editors, IceTAL 2010, volume 6233 of Lecture Notes in Computer Science, pages 293\u2013304. Iceland Cen-", "citeRegEx": "Popel and \u017dabokrtsk\u00fd.,? 2010", "shortCiteRegEx": "Popel and \u017dabokrtsk\u00fd.", "year": 2010}, {"title": "A comparison of two morphosyntactic tagsets of Polish", "author": ["Adam Przepi\u00f3rkowski."], "venue": "Violetta Koseska-Toszewa, Ludmila Dimitrova, and Roman Roszko, editors, Representing Semantics in Digital Lexicography: Proceedings of MONDILEX Fourth", "citeRegEx": "Przepi\u00f3rkowski.,? 2009", "shortCiteRegEx": "Przepi\u00f3rkowski.", "year": 2009}, {"title": "A tiered CRF tagger for Polish", "author": ["Adam Radziszewski."], "venue": "Robert Bembenik, Lukasz Skonieczny, Henryk Rybinski, Marzena Kryszkiewicz, and Marek Niezgodka, editors, Intelligent Tools for Building a Scientific Information Platform, volume", "citeRegEx": "Radziszewski.,? 2013", "shortCiteRegEx": "Radziszewski.", "year": 2013}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["Helmut Schmid."], "venue": "International Conference on New Methods in Language Processing, pages 44\u201349, Manchester, UK.", "citeRegEx": "Schmid.,? 1994", "shortCiteRegEx": "Schmid.", "year": 1994}, {"title": "Open-Source Tools for Morphology, Lemmatization, POS Tagging and Named Entity Recognition", "author": ["Jana Strakov\u00e1", "Milan Straka", "Jan Haji\u010d"], "venue": null, "citeRegEx": "Strakov\u00e1 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Strakov\u00e1 et al\\.", "year": 2014}, {"title": "An exponential translation model for target language morphology", "author": ["Michael Subotin."], "venue": "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011,", "citeRegEx": "Subotin.,? 2011", "shortCiteRegEx": "Subotin.", "year": 2011}, {"title": "Integrating a discriminative classifier into phrase-based and hierarchical decoding", "author": ["Ale\u0161 Tamchyna", "Fabienne Braune", "Alexander Fraser", "Marine Carpuat", "Hal Daum\u00e9 III", "Chris Quirk."], "venue": "The Prague Bulletin of Mathematical Linguistics,", "citeRegEx": "Tamchyna et al\\.,? 2014", "shortCiteRegEx": "Tamchyna et al\\.", "year": 2014}, {"title": "News from OPUS - A collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing, vol-", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "Racai\u2019s linguistic web services", "author": ["Dan Tufis", "Radu Ion", "Alexandru Ceausu", "Dan Stefanescu."], "venue": "Proceedings of the International Conference on Language Resources and Evaluation, LREC 2008, 26 May - 1 June 2008, Marrakech, Morocco.", "citeRegEx": "Tufis et al\\.,? 2008", "shortCiteRegEx": "Tufis et al\\.", "year": 2008}, {"title": "Word-Sense Disambiguation for Machine Translation", "author": ["D. Vickrey", "L. Biewald", "M. Teyssier", "D. Koller."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), Vancouver, Canada, October.", "citeRegEx": "Vickrey et al\\.,? 2005", "shortCiteRegEx": "Vickrey et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 16, "context": "For sense disambiguation, source context is the main source of information, as has been shown in previous work (Vickrey et al., 2005), (Carpuat and", "startOffset": 111, "endOffset": 133}, {"referenceID": 13, "context": "Tamchyna et al. (2014) already integrated VW into Moses.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "We use the factored MT setting (Koehn and Hoang, 2007) and we represent each type of information as an individual factor.", "startOffset": 31, "endOffset": 54}, {"referenceID": 5, "context": "Bilingual context features are concatenations of target-side context words and their sourceside counterparts (according to word alignment); these features are similar to bilingual tokens in bilingual LMs (Niehues et al., 2011).", "startOffset": 204, "endOffset": 226}, {"referenceID": 7, "context": "In all experiments, we use Treex (Popel and \u017dabokrtsk\u00fd, 2010) to lemmatize and tag the source data and also to obtain dependency parses of all", "startOffset": 33, "endOffset": 61}, {"referenceID": 11, "context": "We lemmatize and tag the Czech data using Morphodita (Strakov\u00e1 et al., 2014).", "startOffset": 53, "endOffset": 76}, {"referenceID": 6, "context": "For each setting, we run system weight optimization (tuning) using minimum error rate training (Och, 2003) five times and report the average BLEU score.", "startOffset": 95, "endOffset": 106}, {"referenceID": 3, "context": "data available for the WMT14 translation task: Europarl (Koehn, 2005) and the Common Crawl corpus,3 roughly 4.", "startOffset": 56, "endOffset": 69}, {"referenceID": 10, "context": "Tagger (Schmid, 1994) to lemmatize and tag the German data.", "startOffset": 7, "endOffset": 21}, {"referenceID": 9, "context": "The Polish half has been tagged using WCRFT (Radziszewski, 2013) which produces full morphological tags compatible with the NKJP tagset (Przepi\u00f3rkowski, 2009).", "startOffset": 44, "endOffset": 64}, {"referenceID": 8, "context": "The Polish half has been tagged using WCRFT (Radziszewski, 2013) which produces full morphological tags compatible with the NKJP tagset (Przepi\u00f3rkowski, 2009).", "startOffset": 136, "endOffset": 158}, {"referenceID": 14, "context": "We train our system using the available parallel data \u2013 Europarl and SETIMES2 (Tiedemann, 2009), roughly 600 thousand sentence pairs.", "startOffset": 78, "endOffset": 95}, {"referenceID": 14, "context": "We train our system using the available parallel data \u2013 Europarl and SETIMES2 (Tiedemann, 2009), roughly 600 thousand sentence pairs. We tune the English-Romanian system on the official development set and we test on the WMT16 test set. We use the online tagger by Tufis et al. (2008) to preprocess the data.", "startOffset": 79, "endOffset": 285}, {"referenceID": 1, "context": "Gimpel and Smith (2008) also", "startOffset": 11, "endOffset": 24}, {"referenceID": 4, "context": "Mauser et al. (2009) introduced the \u201cdiscriminative word lexicon\u201d and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sentence).", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "Mauser et al. (2009) introduced the \u201cdiscriminative word lexicon\u201d and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sentence). Training sentences where the target word occurred were used as positive examples, other sentences served as negative examples. Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike our work, their model only used source-context features.", "startOffset": 0, "endOffset": 341}], "year": 2016, "abstractText": "Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.", "creator": "LaTeX with hyperref package"}}}