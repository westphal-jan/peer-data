{"id": "1607.01432", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2016", "title": "Global Neural CCG Parsing with Optimality Guarantees", "abstract": "We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a new objective that encourages the parser to explore a tiny fraction of the search space. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees.", "histories": [["v1", "Tue, 5 Jul 2016 22:25:10 GMT  (36kb,D)", "https://arxiv.org/abs/1607.01432v1", null], ["v2", "Sat, 24 Sep 2016 01:41:43 GMT  (33kb,D)", "http://arxiv.org/abs/1607.01432v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kenton lee", "mike lewis", "luke zettlemoyer"], "accepted": true, "id": "1607.01432"}, "pdf": {"name": "1607.01432.pdf", "metadata": {"source": "CRF", "title": "Global Neural CCG Parsing with Optimality Guarantees", "authors": ["Kenton Lee", "Mike Lewis", "Luke Zettlemoyer"], "emails": ["kentonl@cs.washington.edu", "mlewis@cs.washington.edu", "lsz@cs.washington.edu"], "sections": [{"heading": null, "text": "To support global features, we forego dynamic programs and instead search directly in the space of all possible sub-trees. Although this space is exponentially large in record length, we show that it is possible to learn an efficient A * parser. We extend existing parsing models that have informative boundaries in the outer score to include a global model that has loose boundaries but only needs to model non-local phenomena. The global model is trained with a novel goal that encourages the parser to search both efficiently and accurately. The approach is applied to CCG parsing and improves state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parser for 99.9% of the records held and examines an average of only 190 sub-trees."}, {"heading": "1 Introduction", "text": "Recursive neural models work well for many structured prediction problems, in part because of their ability to learn representations that depend globally on all parts of production structures. However, global models of this kind are incompatible with existing exact sequential algorithms because they do not decompose via substructures in a way that enables effective dynamic programming. Existing work has therefore used greedy sequencing techniques such as beam search (Vinyals et al., 2015; Dyer et al., 2015) or reranking (Socher et al., 2013). We are introducing the first global recursive neural parses with optimal guarantees for decoding and using a state-of-the-art CCG parser.To enable learning of global representations, we modify the parser to search directly in the space of all possible parse trees with no dynamic programming."}, {"heading": "2 Overview", "text": "In fact, most of them will be able to go to another world, where they can go to another world, where they can go to another world, where they can go to another world."}, {"heading": "3 Model", "text": "We have the ability to apply the hidden terms of LSTMs to the hidden terms of the substructure and the whole set containing the hidden representations for a parse.Formally, given a sentence < w1, w2, w2, w2, w2 etc., wn >, we compute hidden states in the front LSTM for 1 < w1, w2, w2, w2, w2 etc., we compute hidden states and cell states in the front LSTM for 1 < w2, w2, w2, w2 etc., we compute hidden states and cell states in the front LSTM for 1 < t >, we compute hidden states and cell states."}, {"heading": "4 Inference", "text": "With the hyperedge scoring model s (e) described in section 3, we can find the highest scoring path deriving a complete parse tree by using the parser algorithm described in section 2. > Fruit-flies like banana-NP (S) / NP-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-miss-ultmi-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-mills-"}, {"heading": "5 Learning", "text": "During training (algorithm 1), we assume that we have access to the sentences labeled with gold parses and gold derivatives. (We) are only able to address the challenges to learning (for example, it is not possible in practice to use the standard, structured perception that is closely linked to the search method, because the search method rarely ends early in training. (Huang et al., 2012), and not in optimizing efficiency.Instead, we optimize a new objective that is closely linked to the search method. (During parsing, we would appear like hyperedges from gold derivatives at the top of the agenda.) If this condition is not met, it is inefficient, and we refer to it as a violation of the agenda that we formally define as: v (E). (A) = max. (A)"}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Data", "text": "We trained our parser in Sections 02-21 of CCGbank (Hockenmaier and Steedman, 2007), using Section 00 for development and Section 23 for testing. To restore a single gold derivative for each sentence we use during training, we find the most branched parser that satisfies the gold dependencies."}, {"heading": "6.2 Experimental Setup", "text": "For the local model, we use the supertag factored model by Lewis et al. (2016). Here, slocal (e) equals a supertag score if a HEAD (e) is a leaf and otherwise zero. External heuristic evaluation is calculated by adding the maximum supertag score for each word outside each span. In the reported results, we revert to the supertag factored model after the forest size exceeds 500,000, the agenda size exceeds 2 million, or we build more than 200,000 recursive units in the neural mesh. Our entire system is trained with all-violation updates. During the training, we lower the forest size limit to 2000 to reduce training times. The model is trained with ADAM for 30 epochs (Kingma and Ba, 2014), and we use early stops based on the development of F1. The LSTM cells and hidden states have 64 dimensions. We initialize word settings with pre-embedded tuginalized values from 2010-50 dimensions also for the embedded ones."}, {"heading": "6.3 Baselines", "text": "We compare our parser with several basic CCG parsers: the C & C parser (Clark and Curran, 2007); C & C + RNN (Xu et al., 2015), the C & C parser with an RNN supertagger; Xu (2016), an LSTM shift reduction parser; Vaswani et al. (2016), which combines a bidirectional LSTM supertagger with a beam detector that uses global functions (Clark et al., 2015); and Supertag factored (Lewis et al., 2016), which uses deterministic A decoding and an LSTM supertagging model."}, {"heading": "6.4 Parsing Results", "text": "Table 2 shows parsing results on the test set. Our global characteristics allow us to improve by 0.6 F1 over the Supertagtored model. Vaswani et al. (2016) also 1https: / / github.com / clab / cnn 2https: / / github.com / mikelewis0 / EasySRL 3https: / / github.com / kentonl / neuralccguse global characteristics, but our optimal decoding results in an improvement of 0.4 F1. Although we observed a general improvement in parser performance, the superday accuracy differed after using the parser. On the test data, the parser finds the optimal parser for 99.9% sentences before we reach our computational limits. On average, we parse 27.1 sets per second, 4 while we examine only 190.2 subtrees."}, {"heading": "6.5 Model Ablations", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to make them a reality."}, {"heading": "6.6 Update Comparisons", "text": "Table 4 compares the various injury-based learning objectives discussed in Section 5. Our novel All Violation updates outperform the alternatives, and we attribute this improvement to the robustness to bad search spaces that the greedy update lacks, and the incentive to explore good parses early on, which the Max Violation update lacks. Learning curves in Figure 5 show that the All Violation update converges faster, too."}, {"heading": "6.7 Decoder Comparisons", "text": "To show that our parser is both more accurate and more efficient than other decoding methods, we decipher our complete model by searching for the best first fraction, reranking and bar search. Table 5 shows the F1 values with and without the backoff model, the proportion of sentences each decoder can analyze, and the time spent decoding compared to the A * series. In the best-first search comparison, we do not include the informative A * heuristicians, and the parser completes very few parses before reaching computational limits - demonstrating the importance of heuristics in large search spaces. In the reranking comparison, we get n-best lists from the backoff model and tie each result to the complete model. In the bar search comparison, we use the approach of Clark et al. (2015), which greedily finds the top-n parsers for each span in bottom-up manners less than the ones that indicate that both A and A are accurate."}, {"heading": "7 Related Work", "text": "Many structured prediction problems are based on dynamic programs that are incompatible with recursive neural networks due to their newly evaluated latent variables. Some newer models have neural factors (Durrett and Klein, 2015), but these cannot be applied to the global structure of analysis, making them less meaningful. Our research investigates fewer nodes than dynamic programs, despite an exponentially larger search space, by allowing the recursive neural network to direct the search. Previous work on structured predictions with recursive or recurrent neural models has used beam searching - for example, when shifting from parsing (Dyer et al., 2015), string to tree transduction (Vinyals et al., 2015), or reranking (Socher et al., 2013) - at the expense of potentially restoring suboptimal solutions. For our model, beam searching is both less efficient and less precise than decoding A."}, {"heading": "8 Conclusion", "text": "We have shown for the first time that a parsing model with global characteristics can be decoded with optimum guarantees, enabling the use of powerful recursive neural networks for parsing without having to resort to approximate decoding methods. Experiments show that this approach to CCG parsing is effective, leading to a new state-of-the-art parser. In the future, we will apply our approach to other structured prediction tasks where neural networks - and greedy beam searching - are ubiquitous."}, {"heading": "Acknowledgements", "text": "We thank Luheng He, Julian Michael and Mark Yatskar for valuable discussions and anonymous critics for feedback and comments. This work was supported by NSF (IIS1252835, IIS-1562364), DARPA under the DEFT program by AFRL (FA8750-13-2-0019), an Allen Distinguished Investigator Award and a gift from Google."}], "references": [{"title": "Globally Normalized Transition-Based Neural Networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proceedings of the 54th Annual Meeting of the Association for", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Efficient CCG parsing: A* versus Adaptive Supertagging", "author": ["Michael Auli", "Adam Lopez."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1.", "citeRegEx": "Auli and Lopez.,? 2011", "shortCiteRegEx": "Auli and Lopez.", "year": 2011}, {"title": "Widecoverage Efficient Statistical Parsing with CCG and Log-Linear Models", "author": ["Stephen Clark", "James R Curran."], "venue": "Computational Linguistics, 33(4).", "citeRegEx": "Clark and Curran.,? 2007", "shortCiteRegEx": "Clark and Curran.", "year": 2007}, {"title": "The Java Version of the C&C Parser: Version 0.95", "author": ["Stephen Clark", "Darren Foong", "Luana Bulat", "Wenduan Xu"], "venue": "Technical report,", "citeRegEx": "Clark et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2015}, {"title": "Incremental Parsing with the Perceptron Algorithm", "author": ["Michael Collins", "Brian Roark."], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 111. Association for Computational Linguistics.", "citeRegEx": "Collins and Roark.,? 2004", "shortCiteRegEx": "Collins and Roark.", "year": 2004}, {"title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms", "author": ["Michael Collins."], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Association for", "citeRegEx": "Collins.,? 2002", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Learning as search optimization: Approximate Large Margin Methods for Structured Prediction", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Proceedings of the 22nd international conference on Machine learning, pages 169\u2013176. ACM.", "citeRegEx": "III and Marcu.,? 2005", "shortCiteRegEx": "III and Marcu.", "year": 2005}, {"title": "Search-based structured prediction", "author": ["Hal Daum\u00e9 III", "John Langford", "Daniel Marcu."], "venue": "Machine learning, 75(3):297\u2013325.", "citeRegEx": "III et al\\.,? 2009", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Neural CRF Parsing", "author": ["Greg Durrett", "Dan Klein."], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Durrett and Klein.,? 2015", "shortCiteRegEx": "Durrett and Klein.", "year": 2015}, {"title": "Transitionbased Dependency Parsing with Stack Long ShortTerm Memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proc. ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Long Short-term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "CCGbank: a Corpus of CCG derivations and Dependency Structures Extracted from the Penn Treebank", "author": ["Julia Hockenmaier", "Mark Steedman."], "venue": "Computational Linguistics, 33(3).", "citeRegEx": "Hockenmaier and Steedman.,? 2007", "shortCiteRegEx": "Hockenmaier and Steedman.", "year": 2007}, {"title": "Structured Perceptron with Inexact Search", "author": ["Liang Huang", "Suphan Fayong", "Yang Guo."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "A* Parsing: Fast Exact Viterbi Parse Selection", "author": ["Dan Klein", "Christopher D Manning."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "A* CCG Parsing with a Supertag-factored Model", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Lewis and Steedman.,? 2014", "shortCiteRegEx": "Lewis and Steedman.", "year": 2014}, {"title": "Joint A* CCG Parsing and Semantic Role Labelling", "author": ["Mike Lewis", "Luheng He", "Luke Zettlemoyer."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Lewis et al\\.,? 2015", "shortCiteRegEx": "Lewis et al\\.", "year": 2015}, {"title": "LSTM CCG Parsing", "author": ["Mike Lewis", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Lewis et al\\.,? 2016", "shortCiteRegEx": "Lewis et al\\.", "year": 2016}, {"title": "K-best A* Parsing", "author": ["Adam Pauls", "Dan Klein."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 958\u2013966. As-", "citeRegEx": "Pauls and Klein.,? 2009", "shortCiteRegEx": "Pauls and Klein.", "year": 2009}, {"title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning", "author": ["St\u00e9phane Ross", "Geoffrey J. Gordon", "Drew Bagnell."], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS", "citeRegEx": "Ross et al\\.,? 2011", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Parsing with Compositional Vector Grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the ACL conference.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved Semantic Representations from Tree-structured Long Short-term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Word representations: A Simple and General Method for Semi-supervised Learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Efficient Structured Inference for Transition-Based Parsing", "author": ["Ashish Vaswani", "Kenji Sagae"], "venue": null, "citeRegEx": "Vaswani and Sagae.,? \\Q2016\\E", "shortCiteRegEx": "Vaswani and Sagae.", "year": 2016}, {"title": "Supertagging With LSTMs", "author": ["Ashish Vaswani", "Yonatan Bisk", "Kenji Sagae", "Ryan Musa."], "venue": "Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Vaswani et al\\.,? 2016", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Sequenceto-Sequence Learning as Beam-Search Optimization", "author": ["Sam Wiseman", "Alexander M Rush."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}, {"title": "CCG Supertagging with a Recurrent Neural Network", "author": ["Wenduan Xu", "Michael Auli", "Stephen Clark."], "venue": "Volume 2: Short Papers, page 250.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "LSTM Shift-Reduce CCG Parsing", "author": ["Wenduan Xu."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Xu.,? 2016", "shortCiteRegEx": "Xu.", "year": 2016}, {"title": "Greed is Good if Randomized: New Inference for Dependency Parsing", "author": ["Yuan Zhang", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "Existing work has therefore used greedy inference techniques such as beam search (Vinyals et al., 2015; Dyer et al., 2015) or reranking (Socher et al.", "startOffset": 81, "endOffset": 122}, {"referenceID": 9, "context": "Existing work has therefore used greedy inference techniques such as beam search (Vinyals et al., 2015; Dyer et al., 2015) or reranking (Socher et al.", "startOffset": 81, "endOffset": 122}, {"referenceID": 20, "context": ", 2015) or reranking (Socher et al., 2013).", "startOffset": 21, "endOffset": 42}, {"referenceID": 14, "context": "Generalizing A\u2217 to global models is challenging; these models also break the locality assumptions used to efficiently compute existing A\u2217 heuristics (Klein and Manning, 2003; Lewis and Steedman, 2014).", "startOffset": 149, "endOffset": 200}, {"referenceID": 15, "context": "Generalizing A\u2217 to global models is challenging; these models also break the locality assumptions used to efficiently compute existing A\u2217 heuristics (Klein and Manning, 2003; Lewis and Steedman, 2014).", "startOffset": 149, "endOffset": 200}, {"referenceID": 17, "context": "In our experiments, we use a recent factored A\u2217 CCG parser (Lewis et al., 2016) for the local model, and we train a Tree-LSTM (Tai et al.", "startOffset": 59, "endOffset": 79}, {"referenceID": 21, "context": ", 2016) for the local model, and we train a Tree-LSTM (Tai et al., 2015) to model global structure.", "startOffset": 54, "endOffset": 72}, {"referenceID": 5, "context": "Traditional structured prediction objectives focus on ensuring that the gold parse has the highest score (Collins, 2002; Huang et al., 2012).", "startOffset": 105, "endOffset": 140}, {"referenceID": 12, "context": "Traditional structured prediction objectives focus on ensuring that the gold parse has the highest score (Collins, 2002; Huang et al., 2012).", "startOffset": 105, "endOffset": 140}, {"referenceID": 14, "context": "A\u2217 parsing A\u2217 parsing has been successfully applied in locally factored models (Klein and Manning, 2003; Lewis and Steedman, 2014; Lewis et al., 2015; Lewis et al., 2016).", "startOffset": 79, "endOffset": 170}, {"referenceID": 15, "context": "A\u2217 parsing A\u2217 parsing has been successfully applied in locally factored models (Klein and Manning, 2003; Lewis and Steedman, 2014; Lewis et al., 2015; Lewis et al., 2016).", "startOffset": 79, "endOffset": 170}, {"referenceID": 16, "context": "A\u2217 parsing A\u2217 parsing has been successfully applied in locally factored models (Klein and Manning, 2003; Lewis and Steedman, 2014; Lewis et al., 2015; Lewis et al., 2016).", "startOffset": 79, "endOffset": 170}, {"referenceID": 17, "context": "A\u2217 parsing A\u2217 parsing has been successfully applied in locally factored models (Klein and Manning, 2003; Lewis and Steedman, 2014; Lewis et al., 2015; Lewis et al., 2016).", "startOffset": 79, "endOffset": 170}, {"referenceID": 16, "context": "In the experiments, we apply our model to CCG parsing, using the locally factored model and A\u2217 heuristic from Lewis et al. (2016).", "startOffset": 110, "endOffset": 130}, {"referenceID": 21, "context": "We use a variant of the Tree-LSTM (Tai et al., 2015) connected to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) at the leaves.", "startOffset": 34, "endOffset": 52}, {"referenceID": 10, "context": ", 2015) connected to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) at the leaves.", "startOffset": 42, "endOffset": 76}, {"referenceID": 5, "context": "This creates challenges for learning\u2014for example, it is not possible in practice to use the standard structured perceptron update (Collins, 2002), because the search procedure rarely terminates early in training.", "startOffset": 130, "endOffset": 145}, {"referenceID": 12, "context": "Other common loss functions assume inexact search (Huang et al., 2012), and do not optimize efficiency.", "startOffset": 50, "endOffset": 70}, {"referenceID": 12, "context": "In existing work on violation-based updates, comparisons are only made between derivations with the same number of steps (Huang et al., 2012; Clark et al., 2015)\u2014whereas our definition allows subtrees of arbitrary spans to compete with each other, because hyperedges are not explored in a fixed order.", "startOffset": 121, "endOffset": 161}, {"referenceID": 3, "context": "In existing work on violation-based updates, comparisons are only made between derivations with the same number of steps (Huang et al., 2012; Clark et al., 2015)\u2014whereas our definition allows subtrees of arbitrary spans to compete with each other, because hyperedges are not explored in a fixed order.", "startOffset": 121, "endOffset": 161}, {"referenceID": 12, "context": "The greedy and max-violation updates are roughly analogous to the violationfixing updates proposed by Huang et al. (2012), but adapted to exact agenda-based parsing.", "startOffset": 102, "endOffset": 122}, {"referenceID": 11, "context": "We trained our parser on Sections 02-21 of CCGbank (Hockenmaier and Steedman, 2007), using Section 00 for development and Section 23 for test.", "startOffset": 51, "endOffset": 83}, {"referenceID": 16, "context": "For the local model, we use the supertag-factored model of Lewis et al. (2016). Here, slocal(e) corresponds to a supertag score if a HEAD(e) is a leaf and zero otherwise.", "startOffset": 59, "endOffset": 79}, {"referenceID": 27, "context": "0 Xu (2016) 87.", "startOffset": 2, "endOffset": 12}, {"referenceID": 24, "context": "8 Vaswani et al. (2016) 87.", "startOffset": 2, "endOffset": 24}, {"referenceID": 13, "context": "The model is trained for 30 epochs using ADAM (Kingma and Ba, 2014), and we use early stopping based on development F1.", "startOffset": 46, "endOffset": 67}, {"referenceID": 13, "context": "The model is trained for 30 epochs using ADAM (Kingma and Ba, 2014), and we use early stopping based on development F1. The LSTM cells and hidden states have 64 dimensions. We initialize word representations with pre-trained 50-dimensional embeddings from Turian et al. (2010). Embeddings for categories have 16 dimensions and are randomly initialized.", "startOffset": 47, "endOffset": 277}, {"referenceID": 2, "context": "We compare our parser to several baseline CCG parsers: the C&C parser (Clark and Curran, 2007); C&C + RNN (Xu et al.", "startOffset": 70, "endOffset": 94}, {"referenceID": 27, "context": "We compare our parser to several baseline CCG parsers: the C&C parser (Clark and Curran, 2007); C&C + RNN (Xu et al., 2015), which is the C&C parser with an RNN supertagger; Xu (2016), a LSTM shift-reduce parser; Vaswani et al.", "startOffset": 106, "endOffset": 123}, {"referenceID": 3, "context": "(2016) who combine a bidirectional LSTM supertagger with a beam search parser using global features (Clark et al., 2015); and supertag-factored (Lewis et al.", "startOffset": 100, "endOffset": 120}, {"referenceID": 17, "context": ", 2015); and supertag-factored (Lewis et al., 2016), which uses deterministic A\u2217 decoding and an LSTM supertagging model.", "startOffset": 31, "endOffset": 51}, {"referenceID": 2, "context": "We compare our parser to several baseline CCG parsers: the C&C parser (Clark and Curran, 2007); C&C + RNN (Xu et al., 2015), which is the C&C parser with an RNN supertagger; Xu (2016), a LSTM shift-reduce parser; Vaswani et al.", "startOffset": 71, "endOffset": 184}, {"referenceID": 2, "context": "We compare our parser to several baseline CCG parsers: the C&C parser (Clark and Curran, 2007); C&C + RNN (Xu et al., 2015), which is the C&C parser with an RNN supertagger; Xu (2016), a LSTM shift-reduce parser; Vaswani et al. (2016) who combine a bidirectional LSTM supertagger with a beam search parser using global features (Clark et al.", "startOffset": 71, "endOffset": 235}, {"referenceID": 24, "context": "Vaswani et al. (2016) also", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "spans, as in Durrett and Klein (2015). In this model, the recursive unit uses, instead of latent states from its children, the latent states of the backward LSTM at the start of the span and the latent states of the forward LSTM at the end of the span.", "startOffset": 13, "endOffset": 38}, {"referenceID": 3, "context": "In the beam search comparison, we use the approach from Clark et al. (2015) which greedily finds the top-n parses for each span in a bottom-up manner.", "startOffset": 56, "endOffset": 76}, {"referenceID": 8, "context": "Some recent models have neural factors (Durrett and Klein, 2015), but these cannot condition on global parse structure, making them less expressive.", "startOffset": 39, "endOffset": 64}, {"referenceID": 9, "context": "in shift reduce parsing (Dyer et al., 2015), string-to-tree transduction (Vinyals et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 25, "context": ", 2015), string-to-tree transduction (Vinyals et al., 2015), or reranking (Socher et al.", "startOffset": 37, "endOffset": 59}, {"referenceID": 20, "context": ", 2015), or reranking (Socher et al., 2013)\u2013at the cost of potentially recovering suboptimal solutions.", "startOffset": 22, "endOffset": 43}, {"referenceID": 14, "context": "A\u2217 parsing has been previously proposed for locally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014).", "startOffset": 68, "endOffset": 164}, {"referenceID": 18, "context": "A\u2217 parsing has been previously proposed for locally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014).", "startOffset": 68, "endOffset": 164}, {"referenceID": 1, "context": "A\u2217 parsing has been previously proposed for locally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014).", "startOffset": 68, "endOffset": 164}, {"referenceID": 15, "context": "A\u2217 parsing has been previously proposed for locally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014).", "startOffset": 68, "endOffset": 164}, {"referenceID": 6, "context": "Some recent models have neural factors (Durrett and Klein, 2015), but these cannot condition on global parse structure, making them less expressive. Our search explores fewer nodes than dynamic programs, despite an exponentially larger search space, by allowing the recursive neural network to guide the search. Previous work on structured prediction with recursive or recurrent neural models has used beam search\u2013e.g. in shift reduce parsing (Dyer et al., 2015), string-to-tree transduction (Vinyals et al., 2015), or reranking (Socher et al., 2013)\u2013at the cost of potentially recovering suboptimal solutions. For our model, beam search is both less efficient and less accurate than optimal A\u2217 decoding. In the non-neural setting, Zhang et al. (2014) showed that global features with greedy inference can improve dependency parsing.", "startOffset": 40, "endOffset": 752}, {"referenceID": 2, "context": "The CCG beam search parser of Clark et al. (2015), most related to this work, also uses global features.", "startOffset": 30, "endOffset": 50}, {"referenceID": 1, "context": "A\u2217 parsing has been previously proposed for locally factored models (Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011; Lewis and Steedman, 2014). We generalize these methods to enable global features. Vaswani and Sagae (2016) apply best-first search to an unlabeled shift-reduce parser.", "startOffset": 117, "endOffset": 246}, {"referenceID": 19, "context": ", 2009) and DAGGER (Ross et al., 2011) learn greedy policies to predict structure by sampling classification examples over actions from single states.", "startOffset": 19, "endOffset": 38}, {"referenceID": 4, "context": "Other learning objectives that update parameters based on a beam or agenda of partial structures have also been proposed (Collins and Roark, 2004; Daum\u00e9 III and Marcu, 2005; Huang et al., 2012; Andor et al., 2016; Wiseman and Rush, 2016), but the impact of search errors is unclear.", "startOffset": 121, "endOffset": 237}, {"referenceID": 12, "context": "Other learning objectives that update parameters based on a beam or agenda of partial structures have also been proposed (Collins and Roark, 2004; Daum\u00e9 III and Marcu, 2005; Huang et al., 2012; Andor et al., 2016; Wiseman and Rush, 2016), but the impact of search errors is unclear.", "startOffset": 121, "endOffset": 237}, {"referenceID": 0, "context": "Other learning objectives that update parameters based on a beam or agenda of partial structures have also been proposed (Collins and Roark, 2004; Daum\u00e9 III and Marcu, 2005; Huang et al., 2012; Andor et al., 2016; Wiseman and Rush, 2016), but the impact of search errors is unclear.", "startOffset": 121, "endOffset": 237}, {"referenceID": 26, "context": "Other learning objectives that update parameters based on a beam or agenda of partial structures have also been proposed (Collins and Roark, 2004; Daum\u00e9 III and Marcu, 2005; Huang et al., 2012; Andor et al., 2016; Wiseman and Rush, 2016), but the impact of search errors is unclear.", "startOffset": 121, "endOffset": 237}], "year": 2016, "abstractText": "We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a novel objective that encourages the parser to search both efficiently and accurately. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees.", "creator": "TeX"}}}