{"id": "1607.02061", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jul-2016", "title": "Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity", "abstract": "Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, which causes the loss of important information for word expectations, such as word order and interrelations. In this paper, we present a DSM which addresses the issue by defining verb contexts as joint dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts are more efficient than single dependencies, even with a relatively small amount of training data.", "histories": [["v1", "Thu, 7 Jul 2016 16:00:33 GMT  (19kb)", "http://arxiv.org/abs/1607.02061v1", "5 pages"], ["v2", "Wed, 5 Oct 2016 10:49:58 GMT  (17kb)", "http://arxiv.org/abs/1607.02061v2", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["emmanuele chersoni", "enrico santus", "alessandro lenci", "philippe blache", "chu-ren huang"], "accepted": true, "id": "1607.02061"}, "pdf": {"name": "1607.02061.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Philippe Blache"], "emails": ["emmanuelechersoni@gmail.com", "esantus@gmail.com", "alessandro.lenci@unipi.it", "blache@lpl-aix.fr", "churen.huang@polyu.edu.hk"], "sections": [{"heading": null, "text": "ar Xiv: 160 7.02 061v 1 [cs"}, {"heading": "1 Introduction", "text": "Distribution Semantic Models (DSM) rely on the distribution hypothesis (Harris, 1954; Firth, 1957; Sahlgren, 2008), which states that words that occur in similar contexts have similar meanings. For such theoretical reasons, words extracted from corpora are used to build semantic representations in the form of vectors that have become very popular in the NLP community. Proximity between word vectors is taken as an index of meaning similarity, and vector cosine ice is generally used to measure such proximity, although other measures have been proposed (Weeds et al., 2004)."}, {"heading": "2 Related Work", "text": "A number of studies in sentence processing suggest that verbs raise expectations of their typical argumentation patterns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring in the same events (Hare et al., 2009; Bicknell et al., 2011). Experimental subjects seem to take advantage of a rich event knowledge to dynamically activate or inhibit the representations of potential arguments, a phenomenon commonly referred to as thematic adaptation (McRae et al., 1998; Matsuki et al., 2011), supporting the idea of a mental lexicon arranged as a network of mutual expectations, such as Baroni and Lenci, 2011; Sayeed and Demberg, 2014."}, {"heading": "3 Syntactic Joint Contexts", "text": "A common context, as defined in Melamud et al. (2014), is a word window of order n around a target word. The target is replaced by a placeholder, and the value of the attribute for a word w is the probability that w takes the position of the placeholder. Assuming n = 3, a word like love would be represented by a collection of contexts, such as the new students visiting the school campus, my cousin Bob, playing basketball, etc. Such a representation leads to a data scarcity that in previous studies could include disjointed words, even differentiating contexts that essentially describe the same event (e.g., Luis the red ball and Luis the blue ball). For these reasons, we are introducing the term syntactical community context by continuing to syntactical word windows."}, {"heading": "4 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Corpus and DSMs", "text": "We trained our DSMs on the RCV1 corpus, which contains approximately 150 million words (Lewis et al., 2004). The corpus was selected for two reasons: i) to allow a direct comparison with the results reported by Melamud et al., 2014; ii) to show that our common context-based representation can handle data poverty even with a training corpus of limited size. All DSMs use Positive Pointwise Mutual Information (PPMI; (Church and Hanks, 1990)) as a context weighting scheme and differ according to three main parameters: i) Type of contexts; ii) Number of dimensions; iii) Application of Singular Value Decomposition (SVD; see (Landauer and Dumais, 1997; Landauer et., 1998). DSMs that go beyond the total number of dimensions used (iii)."}, {"heading": "4.2 Similarity Measures", "text": "Melamud et al. (2014) have proposed the Probabilistic Distributional Similarity (PDS), which is based on the intuition that two words, w1 and w2, are similar if they are likely to occur in each other's contexts. The authors define them in such a way that they assign a high similarity value when both p (w1 - contexts of w2) and p (w2 - contexts of w1) are high. We have tried to test variations of this measurement with our syntactic joint contexts, but we have not been able to achieve satisfactory results. Therefore, we have decided to report only the values with the cosine here."}, {"heading": "4.3 Datasets", "text": "The DSMs are evaluated on the basis of two test sets: VerbSim (Yang and Powers, 2006) and the verb subset of the opulent SimLex-999 (Hill et al., 2015), the former comprising 130 verb pairs, while the latter comprises 222 verb pairs. We focus on verb similarity because the verb meaning is highly context sensitive and therefore the verb representation should benefit more from the introduction of common features (Melamud et al., 2014). Both data sets are provided with similarity judgments, so we measured the spearman correlation between them and the values assigned by the model. The verbsim dataset allows a direct comparison with Melamud et al. (2014), as they also evaluated their model on the basis of this test set, achieving a Spearman correlation value of 0.616 and exceeding the results of the verb subset of Simlex-999."}, {"heading": "4.4 Results", "text": "Table 1 reports on the Spearman correlation values for the vector cosine on multiple DSMs, with individual dependencies and syntactic joint contexts being implemented. At a glance, we can see the discrepancy between the results obtained in the two datasets, as Simlex verbs confirm that it is very difficult to model them. However, we can see a trend in both datasets that is related to the number of contexts considered: the higher the number, the higher the performance. Individual dependencies and common context work very similarly, and none has an advantage over the other. It is evident that the value achieved in VerbSim by the common context model with 100K dimensions is that of Melamud et al. (2014) (0.616).A clear advantage of the joint contexts can instead be found in Table 2 and Table 3, when SVD is used to reduce the matrix. Independent of the individual contexts, the contextual validity of the models is almost always greater than the contextual validity achieved by using them."}, {"heading": "4.5 Conclusions", "text": "In this paper, we presented our proposal for a new type of vector representation based on common characteristics, which should correspond more closely to the general knowledge of participants in events, which seems to be the organizational principle of our mental lexicon. A central theme of previous studies was the challenge of data sparseness, and we tackled it by means of a more abstract, syntactic notion of a common context. Models that used common dependencies proved to be competitive and able to be comparable to - and in some cases even better than - some of the best results reported in literature. More importantly, they were able to outperform models that relied on individual dependencies in multiple parameter settings, especially after using SVD. Similarly, Agirre et al. (2009) showed that large word windows had a higher discriminatory power than independent features, but they did so by using a huge training corpus. The fact that we achieved our possible dependencies with a small corpus like R1 represented a common tactic, the fact that our results were related to a small corpus."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Agirre et al.2009] Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa"], "venue": "In Proceedings of the 2009 conference of the NAACL-", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Accurate dependency parsing with a stacked multilayer perceptron", "author": ["Felice Dell\u2019Orletta", "Maria Simi", "Joseph Turian"], "venue": "In Proceedings of EVALITA,", "citeRegEx": "Attardi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Attardi et al\\.", "year": 2009}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Baroni", "Lenci2010] Marco Baroni", "Alessandro Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Effects of event knowledge in processing verbal arguments", "author": ["Jeffrey L Elman", "Mary Hare", "Ken McRae", "Marta Kutas"], "venue": "Journal of Memory and Language,", "citeRegEx": "Bicknell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bicknell et al\\.", "year": 2010}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Church", "Hanks1990] Kenneth Ward Church", "Patrick Hanks"], "venue": "Computational linguistics,", "citeRegEx": "Church et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Church et al\\.", "year": 1990}, {"title": "DISSECT-DIStributional SEmantics Composition Toolkit", "author": ["Dinu et al.2013] Georgiana Dinu", "Nghia The Pham", "Marco Baroni"], "venue": "Proceedings of the System Demonstrations of ACL 2013,", "citeRegEx": "Dinu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2013}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Improving unsupervised vector-space thematic fit evaluation via role-filler prototype clustering", "author": ["Asad Sayeed", "Vera Demberg"], "venue": "In Proceedings of the 2015 conference of the NAACL-HLT,", "citeRegEx": "Greenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greenberg et al\\.", "year": 2015}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2015] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Kneser", "Ney1995] Reinhard Kneser", "Hermann Ney"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Landauer", "Dumais1997] Thomas K Landauer", "Susan Dumais"], "venue": "Psychological review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "An introduction to latent semantic analysis", "author": ["Peter W Foltz", "Darrell Laham"], "venue": "Discourse processes,", "citeRegEx": "Landauer et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1998}, {"title": "Composing and updating verb argument expectations: A distributional semantic model", "author": ["Alessandro Lenci"], "venue": "In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics,", "citeRegEx": "Lenci.,? \\Q2011\\E", "shortCiteRegEx": "Lenci.", "year": 2011}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["Lewis et al.2004] David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Event-based plausibility immediately influences on-line language comprehension", "author": ["Tracy Chow", "Mary Hare", "Jeffrey L Elman", "Christoph Scheepers", "Ken McRae"], "venue": "Journal of Experimental Psychology,", "citeRegEx": "Matsuki et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Matsuki et al\\.", "year": 2011}, {"title": "Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension", "author": ["McRae et al.1998] Ken McRae", "Michael J SpiveyKnowlton", "Michael K Tanenhaus"], "venue": "Journal of Memory and Language,", "citeRegEx": "McRae et al\\.,? \\Q1998\\E", "shortCiteRegEx": "McRae et al\\.", "year": 1998}, {"title": "A basis for generating expectancies for verbs from nouns", "author": ["McRae et al.2005] Ken McRae", "Mary Hare", "Jeffrey L Elman", "Todd Ferretti"], "venue": "Memory & Cognition,", "citeRegEx": "McRae et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McRae et al\\.", "year": 2005}, {"title": "Modeling Word Meaning in Context with Substitute Vectors", "author": ["Melamud et al.2015] Oren Melamud", "Ido Dagan", "Jacob Goldberger"], "venue": "In Proceedings of the 2015 conference of the NAACL-HLT,", "citeRegEx": "Melamud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2015}, {"title": "Probabilistic modeling of joint-context in distributional similarity", "author": ["Melamud et al.2014] Oren Melamud", "Ido Dagan", "Jacob Goldberger", "Idan Szpektor", "Deniz Yuret"], "venue": "In CoNLL,", "citeRegEx": "Melamud et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2014}, {"title": "Efficient estimation", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Using contextwindow overlapping in synonym discovery and ontology extension", "author": ["Enrique Alfonseca", "Pablo Castells"], "venue": "In Proceedings of RANLP,", "citeRegEx": "Ruiz.Casado et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ruiz.Casado et al\\.", "year": 2005}, {"title": "The distributional hypothesis", "author": ["Magnus Sahlgren"], "venue": "Italian Journal of Linguistics,", "citeRegEx": "Sahlgren.,? \\Q2008\\E", "shortCiteRegEx": "Sahlgren.", "year": 2008}, {"title": "Combining unsupervised syntactic and semantic models of thematic fit", "author": ["Sayeed", "Demberg2014] Asad Sayeed", "Vera Demberg"], "venue": null, "citeRegEx": "Sayeed et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sayeed et al\\.", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D Turney", "Patrick Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Characterising measures of lexical distributional similarity", "author": ["Weeds et al.2004] Julie Weeds", "David Weir", "Diana McCarthy"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Weeds et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 22, "context": "Distributional Semantic Models (DSMs) rely on the Distributional Hypothesis (Harris, 1954; Firth, 1957; Sahlgren, 2008), stating that words occurring in similar contexts have similar meanings.", "startOffset": 76, "endOffset": 119}, {"referenceID": 25, "context": "(Weeds et al., 2004).", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "This is why works like Ruiz-Casado et al. (2005), Agirre et al.", "startOffset": 23, "endOffset": 49}, {"referenceID": 0, "context": "(2005), Agirre et al. (2009), and Melamud et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "(2005), Agirre et al. (2009), and Melamud et al. (2014) proposed to introduce richer contexts in distributional spaces, i.", "startOffset": 8, "endOffset": 56}, {"referenceID": 16, "context": "A number of studies in sentence processing suggest that verbs activate expectations on their typical argument nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al.", "startOffset": 131, "endOffset": 171}, {"referenceID": 17, "context": "A number of studies in sentence processing suggest that verbs activate expectations on their typical argument nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al.", "startOffset": 131, "endOffset": 171}, {"referenceID": 3, "context": ", 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al., 2009; Bicknell et al., 2010).", "startOffset": 92, "endOffset": 134}, {"referenceID": 16, "context": "(McRae et al., 1998; Matsuki et al., 2011), supports", "startOffset": 0, "endOffset": 42}, {"referenceID": 15, "context": "(McRae et al., 1998; Matsuki et al., 2011), supports", "startOffset": 0, "endOffset": 42}, {"referenceID": 21, "context": "A first example was the compositefeature model by Ruiz-Casado et al. (2005), who extracted word windows through a Web Search engine.", "startOffset": 50, "endOffset": 76}, {"referenceID": 12, "context": "50 in the TOEFL synonym detection test, outperforming the Latent Semantic Analysis (LSA; see (Landauer and Dumais, 1997; Landauer et al., 1998)) and several other methods.", "startOffset": 93, "endOffset": 143}, {"referenceID": 7, "context": "Their model outperformed a traditional DSM on the similarity subset of the WordSim-353 test set (Finkelstein et al., 2001).", "startOffset": 96, "endOffset": 122}, {"referenceID": 18, "context": "Finally, in a recent paper, Melamud et al. (2015)", "startOffset": 28, "endOffset": 50}, {"referenceID": 18, "context": "A joint context, as defined in Melamud et al. (2014), is a word window of order n around a target word.", "startOffset": 31, "endOffset": 53}, {"referenceID": 14, "context": "We trained our DSMs on the RCV1 corpus, which contains approximately 150 million words (Lewis et al., 2004).", "startOffset": 87, "endOffset": 107}, {"referenceID": 1, "context": "pus was tagged with the tagger described in (Dell\u2019Orletta, 2009) and dependency-parsed with DeSR (Attardi et al., 2009).", "startOffset": 97, "endOffset": 119}, {"referenceID": 12, "context": "All DSMs adopt Positive Pointwise Mutual Information (PPMI; (Church and Hanks, 1990)) as a context weighting scheme and vary according to three main parameters: i) type of contexts; ii) number of dimensions; iii) application of Singular Value Decomposition (SVD; see (Landauer and Dumais, 1997; Landauer et al., 1998)).", "startOffset": 267, "endOffset": 317}, {"referenceID": 6, "context": "of the DISSECT toolkit (Dinu et al., 2013).", "startOffset": 23, "endOffset": 42}, {"referenceID": 18, "context": "Melamud et al. (2014) have proposed the Probabilistic Distributional Sim-", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "popular SimLex-999 (Hill et al., 2015).", "startOffset": 19, "endOffset": 38}, {"referenceID": 19, "context": "We focus on verb similarity because verb meaning is highly context sensitive and, therefore, verb representation should benefit more from the introduction of joint features (Melamud et al., 2014).", "startOffset": 173, "endOffset": 195}, {"referenceID": 18, "context": "The Verbsim dataset allows for direct comparison with Melamud et al. (2014), since they also evaluated their model on this test set, achieving a Spearman correlation score of 0.", "startOffset": 54, "endOffset": 76}, {"referenceID": 9, "context": "reported by Hill et al. (2015), the average per-", "startOffset": 12, "endOffset": 31}, {"referenceID": 7, "context": "are much lower than on alternative benchmarks like WordSim (Finkelstein et al., 2001) and MEN (Bruni et al.", "startOffset": 59, "endOffset": 85}, {"referenceID": 4, "context": ", 2001) and MEN (Bruni et al., 2014).", "startOffset": 16, "endOffset": 36}, {"referenceID": 18, "context": "3, the same of Melamud et al. (2014)) and for 214 pairs in the SimLex verbs dataset (96.", "startOffset": 15, "endOffset": 37}, {"referenceID": 18, "context": "It is noticeable that the score obtained on VerbSim by the joint context model with 100K dimensions goes very close to the result reported by Melamud et al. (2014) (0.", "startOffset": 142, "endOffset": 164}, {"referenceID": 17, "context": "650, which is above the result of Melamud et al. (2014). In the verb subset of SimLex-999, we obtain 0.", "startOffset": 34, "endOffset": 56}, {"referenceID": 9, "context": "283, which is very close to the score obtained by Hill et al. (2015) by applying word2vec", "startOffset": 50, "endOffset": 69}, {"referenceID": 20, "context": "(Mikolov et al., 2013) on the full dataset (0.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Similarly, Agirre et al. (2009) showed that large word windows had a higher discriminative power than indipendent features, but they did it by using a huge training corpus.", "startOffset": 11, "endOffset": 32}], "year": 2017, "abstractText": "Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, which causes the loss of important information for word expectations, such as word order and interrelations. In this paper, we present a DSM which addresses the issue by defining verb contexts as joint dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts are more efficient than single dependencies, even with a relatively small amount of training data.", "creator": "LaTeX with hyperref package"}}}