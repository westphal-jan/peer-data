{"id": "1607.02535", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2016", "title": "Learning from Multiway Data: Simple and Efficient Tensor Regression", "abstract": "Tensor regression has shown to be advantageous in learning tasks with multi-directional relatedness. Given massive multiway data, traditional methods are often too slow to operate on or suffer from memory bottleneck. In this paper, we introduce subsampled tensor projected gradient to solve the problem. Our algorithm is impressively simple and efficient. It is built upon projected gradient method with fast tensor power iterations, leveraging randomized sketching for further acceleration. Theoretical analysis shows that our algorithm converges to the correct solution in fixed number of iterations. The memory requirement grows linearly with the size of the problem. We demonstrate superior empirical performance on both multi-linear multi-task learning and spatio-temporal applications.", "histories": [["v1", "Fri, 8 Jul 2016 21:40:44 GMT  (447kb,D)", "http://arxiv.org/abs/1607.02535v1", "10 pages, Proceedings of the 33rd International Conference on Machine Learning (ICML-16), 2016"]], "COMMENTS": "10 pages, Proceedings of the 33rd International Conference on Machine Learning (ICML-16), 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rose yu", "yan liu"], "accepted": true, "id": "1607.02535"}, "pdf": {"name": "1607.02535.pdf", "metadata": {"source": "CRF", "title": "Learning from Multiway Data: Simple and Efficient Tensor Regression", "authors": ["Rose Yu", "Yan Liu"], "emails": ["QIYU@USC.EDU", "YANLIU.CS@USC.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, the time has come to put yourself in a position to be at the forefront in order to find the way to the future."}, {"heading": "2. Preliminary", "text": "On paper, we use calligraphic fonts for tensors such as X, Y, bold uppercase letters for matrices such as X, Y, and bold lowercase letters for vectors such as x, y.Tensor Unfolding Each dimension of a tensor is a mode. Indexing follows the convention in (Kolda & Bader, 2009), also known as tensor matriculation. N-mode product The n-mode product between tensor W and matrix U on mode n is represented as W \u00b7 n U and is defined as (W \u00b7 n U) = UW (n). Tucker decomposition Tucker decomposition factores a tensor W = S \u00b7 1 \u00b7 Urional or Unitonal = 1 \u00b7 Unitonal, where all matrices are equal."}, {"heading": "3. Related Work", "text": "Several algorithms have been proposed for tensor regression. For example, (Zhou et al., 2013) suggests using the Alternating Least Square (ALS) algorithm. (Romera-Paredes et al., 2013) uses ALS and an Alternating Direction Method of Multiplier (ADMM) technique to solve the problem of nuclear-regulated optimization. (Signoretto et al., 2014) proposes a more general version of these methods (Yu et al., 2014). Both ADMM-based algorithms attempt to solve a convex loosening of the original optimization problem by means of singular value soft threshold solving. To address the scalability problem of these methods, (Yu et al., 2014) suggests a greedy algorithm that follows the Orthogonal Matching Pursuit (OMP)."}, {"heading": "4. Simple and Efficient Tensor Regression", "text": "We start with a detailed description of the problem of tensor regression and our proposed algorithm. For a simple explanation we use a three-mode tensor. Our method and analysis is directly applicable to cases of higher order."}, {"heading": "4.1. Tensor Regression", "text": "Faced with a predictive tensor X and a response tensor Y, the tensor regression aims at the following problem: W? = argmin W L (W; X, Y) \u00b7 \u00b7 rank (W) \u2264 R (1) The problem aims to estimate a model tensor W = S \u00b7 RD1 \u00b7 D2 \u00b7 D3 that minimizes the empirical loss L, subject to the limitation that the tucker rank of W is equal to R. The model tensor W has a low-dimensional factorization W = S \u00b7 1 \u00b7 U1 \u00b7 2 \u00b7 3 U3 with the core S-RRR1 \u00b7 R2 \u00b7 R3 \u00b7 R3 and orthonorthonormal projection matrices {Un-RDn \u00b7 Rn}. The dimensionality of S is at most R. The reason why we prefer tucker over others is the fact that it is a high generalization of matrix models."}, {"heading": "4.2. Tensor Projected Gradient", "text": "In fact, it is as if it were a matter of a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about"}, {"heading": "5. Theoretical Analysis", "text": "S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S 1 S (1) S 1 S) S (1 S) S (1 S) S 1 S (1 S) S (1 S) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1 S) S (1) S (1) S (1 S (1) S (1) S (1) S (1) S (1) S (1) S (1 S (1) S (1) S (1) S (1) S (1) S (1) S (1 S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1 S (1) S (1) S (1) S (1) S (1 S (1) S (1) S (1) S (1) S (1) S (1) S (1 S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1) S (1 S (1) S (1) S (1) S (1 S (1 S (1) S (1) S (1) S ("}, {"heading": "6. Applications of Tensor Regression", "text": "Tensor regression is used in many areas. We present two examples: the multilinear multi-task learning problem in the machine learning community and the spatio-temporal prediction problem in the field of time series analysis."}, {"heading": "6.1. Multi-linear Multi-task Learning", "text": "Multilinear Multi-Task Learning (Romera-Paredes et al., 2013; Wimalawarne et al., 2014) deals with the scenario in which the tasks to be learned are references through multiple indices, i.e. contain multimodal relationships. Considering the predictor and the answer for each task: (Xt-Rmt \u00b7 dt, Yt-Rmt \u00b7 1), traditional multi-task learning concatenates the parameter vector wt-Rdt \u00b7 1 to a matrix. Here, the model stacks the coefficient vectors with additional information about task indices to a model tensor W. The empirical loss is defined as the summary of the least square loss for all tasks, i.e. L (W; X, Y) = \u2211 T t = 1-Yt \u2212 Xtwt-2F, weighted as a t-ter column of W (1). The multilinear multi-task learning problem can be described as follows: min {t-2T-2T (W-gt 2T) {T (XT-2T)."}, {"heading": "6.2. Spatio-temporal Forecasting", "text": "The spatio-temporal prediction (Cressie & Wikle, 2015) is the prediction of future values based on their historical measurements. Suppose we get access to measurements X-RT \u00b7 P \u00b7 M of the T timestamps of M variables across P locations as well as the geographical coordinates of P locations. We can model the time series using a vector auto-regressive (VAR) model of delay L, in which we assume the generative process as Xt,:, m = Xt,:, mW::, m, for m = 1,..., M and t = L + 1,..., T. Here Xt, m = [X > t \u2212 1,:, m,.., X > t \u2212 L,:, m] denotes the concatenation of L-lag historical data ahead of time. We learn a model coefficient sorW, RPL \u00d7 M for simultaneous prediction of multiple variables."}, {"heading": "7. Experiments", "text": "We perform a series of experiments with a synthetic dataset and two real-world applications. In this section, we present and analyze the results obtained. We compare TGP with the following basic methods: \u2022 OLS: ordinary estimator of the smallest square without low-level constraints \u2022 THOSVD (De Lathauwer et al., 2000b): a two-stage heuristic that first solves the smallest square and then performs an abbreviated decomposition of the singular value \u2022 Greedy (Yu et al., 2014): a rapid tensor regression solution that evaluates a subspace sequentially based on the orthogonal matching pursuit \u2022 ADMM (Gandy et al., 2011): Method of changing direction of multipliers for a regulated optimization of the nuclear standard"}, {"heading": "7.1. Synthetic Dataset", "text": "We construct a model coefficient tensor W of size 30 x 30 x 20 with Tucker rank equal to 2 for all modes. We then generate the observations Y and X according to the multivariate regression model Y:,:, m = X:,:, mW:,:, m + E::, m for m = 1,.., M, where E is a noise tensor with zero Gaussian elements. Since the true model is low, simple OLS suffer from poor performance. Other methods can converge to the correct solution. The main difference results for small sketch size RMSE and the runtime error bar in relation to the sketch size. Since the true model is low, simple OLS suffer from poor performance. Other methods can converge to the correct solution."}, {"heading": "7.2. Real Data", "text": "In this section, we will test the approaches described using two real-world application datasets: multilinear multi-task learning and spatio-temporal prediction."}, {"heading": "7.2.1. MULTI-LINEAR MULTI-TASK LEARNING", "text": "We compare TPG with the state-of-the-art multi-task multilinear baseline. Our evaluation follows the same experimental setting (Romera-Paredes et al., 2013) on the restaurant and consumer dataset provided by the authors in the thesis. The dataset contains consumer ratings for different restaurants. The data contains 138 consumers who submitted three types of ratings for restaurant services. Each restaurant has 45 attributes for the evaluation of predictions. The total number of ratings for all tasks was 3483. The problem results in a model tensor of the dimension 45 x 138 x 3. We divide the training and test set in a different ratio of 0.1 to 0.9 and randomly select the training datasets. If the training size is small, many tasks did not contain a training example. We also select 200 cases as a validation set. We compare with MLMTLC and multi-task feature learning baselines in the original work ML-21 Tlierer and Tralierer."}, {"heading": "7.2.2. SPATIO-TEMPORAL FORECASTING", "text": "For the spatio-temporal prediction task, we are experimenting with the following two sets of data. \u00b7 The Foursquare Check-In The Foursquare Check-In dataset contains the user check-in datasets in the Pittsburgh area from February 24 to May 23, 2012, categorized by different venues such as Art & Entertainment, College & University and Food. We extract hourly check-in datasets from 739 users in 34 different locations categories over a period of 3-474 hours, as well as user friendship network.USHCN Measurements The U.S. Historical Climatology Network (USHCN) daily (http: / / cdiac.ornl. gov / ftp / ushcn _ daily /) contains daily measurements of 5 climate variables for more than 100 years. The five climate variables correspond with temperature, precipitation, snowfall, snowfall and snow height."}, {"heading": "8. Discussion", "text": "The implication of our approach has several interesting aspects that could shed light on future algorithmic design. (1) The projection step in TPG is not data-dependent, so it combines with tensor decomposition techniques such as high order orthogonal iteration (HOOI) (De Lathauwer et al., 2000a). However, there is a subtle difference in that regression would require an early stop of iterative projection, as it searches sequentially for orthogonal subspaces. (2) TPG behaves similarly to first order methods. The convergence rate can be further improved with the second order Newton method. This can easily be done by replacing the gradient with Hessisch. This change does not affect the theoretical properties of the proposed algorithm, but would lead to significant empirical improvements (Jain et al., 2010)."}, {"heading": "9. Conclusion", "text": "In this paper, we examine tensor regression as a tool for analyzing reusable data. We present Tensor Projected Gradient to solve the problem. Our approach is based on the method of projected gradients and generalizes iterative hard threshold methods to high order tensors. The algorithm is very simple and general, which can easily be applied to many tensor regression models. It also shares the efficiency of the iterative hard threshold method. We demonstrate that the algorithm converges within a constant number of iterations and the achievable estimation error is linear to the magnitude of noise. We evaluate our method on multilinear multi-task learning as well as spatio-temporal forecasting applications. The results show that our method is significantly faster and impressively robust against noise."}, {"heading": "10. Acknowledgment", "text": "This work is partially supported by the US Army Research Office under grant number W911NF-15-1-0491, NSF Research Grant IIS-1254206 and IIS-1134990. The views and conclusions are those of the authors and should not be interpreted as representing the official policy of the funding agency or the US government."}], "references": [{"title": "Multiway analysis of epilepsy", "author": ["Acar", "Evrim", "Aykut-Bingol", "Canan", "Bingol", "Haluk", "Bro", "Rasmus", "Yener", "B\u00fclent"], "venue": "tensors. Bioinformatics,", "citeRegEx": "Acar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Acar et al\\.", "year": 2007}, {"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["Achlioptas", "Dimitris"], "venue": "Journal of computer and System Sciences,", "citeRegEx": "Achlioptas and Dimitris.,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas and Dimitris.", "year": 2003}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["Blumensath", "Thomas", "Davies", "Mike E"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Blumensath et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Blumensath et al\\.", "year": 2009}, {"title": "Projected gradient methods for linearly constrained problems", "author": ["Calamai", "Paul H", "Mor\u00e9", "Jorge J"], "venue": "Mathematical programming,", "citeRegEx": "Calamai et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Calamai et al\\.", "year": 1987}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["Candes", "Emmanuel J", "Romberg", "Justin K", "Tao", "Terence"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "Candes et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2006}, {"title": "Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation", "author": ["Cichocki", "Andrzej", "Zdunek", "Rafal", "Phan", "Anh Huy", "Amari", "Shun-ichi"], "venue": null, "citeRegEx": "Cichocki et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2009}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Clarkson", "Kenneth L", "Woodruff", "David P"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Clarkson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson et al\\.", "year": 2013}, {"title": "Statistics for spatiotemporal data", "author": ["Cressie", "Noel", "Wikle", "Christopher K"], "venue": null, "citeRegEx": "Cressie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cressie et al\\.", "year": 2015}, {"title": "On the best rank-1 and rank-(r 1, r 2,..., rn) approximation of higher-order tensors", "author": ["De Lathauwer", "Lieven", "De Moor", "Bart", "Vandewalle", "Joos"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Lathauwer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2000}, {"title": "A multilinear singular value decomposition", "author": ["De Lathauwer", "Lieven", "De Moor", "Bart", "Vandewalle", "Joos"], "venue": "SIAM journal on Matrix Analysis and Applications,", "citeRegEx": "Lathauwer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2000}, {"title": "The approximation of one matrix by another of lower rank", "author": ["Eckart", "Carl", "Young", "Gale"], "venue": null, "citeRegEx": "Eckart et al\\.,? \\Q1936\\E", "shortCiteRegEx": "Eckart et al\\.", "year": 1936}, {"title": "Tensor completion and low-n-rank tensor recovery via convex optimization", "author": ["Gandy", "Silvia", "Recht", "Benjamin", "Yamada", "Isao"], "venue": "Inverse Problems,", "citeRegEx": "Gandy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gandy et al\\.", "year": 2011}, {"title": "Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property", "author": ["Garg", "Rahul", "Khandekar", "Rohit"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Garg et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Garg et al\\.", "year": 2009}, {"title": "Multi-pattern fingerprint method for detection and attribution of climate change", "author": ["Hasselmann", "Klaus"], "venue": "Climate Dynamics,", "citeRegEx": "Hasselmann and Klaus.,? \\Q1997\\E", "shortCiteRegEx": "Hasselmann and Klaus.", "year": 1997}, {"title": "Best low multilinear rank approximation of higher-order tensors, based on the riemannian trust-region scheme", "author": ["Ishteva", "Mariya", "Absil", "P-A", "Van Huffel", "Sabine", "De Lathauwer", "Lieven"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Ishteva et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ishteva et al\\.", "year": 2011}, {"title": "Tucker compression and local optima", "author": ["Ishteva", "Mariya", "Absil", "P-A", "Van Huffel", "Sabine", "De Lathauwer", "Lieven"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "Ishteva et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ishteva et al\\.", "year": 2011}, {"title": "Guaranteed rank minimization via singular value projection", "author": ["Jain", "Prateek", "Meka", "Raghu", "Dhillon", "Inderjit S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jain et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2010}, {"title": "Tensor decompositions and applications", "author": ["Kolda", "Tamara G", "Bader", "Brett W"], "venue": "SIAM review,", "citeRegEx": "Kolda et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolda et al\\.", "year": 2009}, {"title": "Tensor completion in hierarchical tensor representations", "author": ["Rauhut", "Holger", "Schneider", "Reinhold", "Stojanac", "\u017deljka"], "venue": "In Compressed Sensing and its Applications,", "citeRegEx": "Rauhut et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rauhut et al\\.", "year": 2015}, {"title": "Monotone operators and the proximal point algorithm", "author": ["Rockafellar", "R Tyrrell"], "venue": "SIAM journal on control and optimization,", "citeRegEx": "Rockafellar and Tyrrell.,? \\Q1976\\E", "shortCiteRegEx": "Rockafellar and Tyrrell.", "year": 1976}, {"title": "Multilinear multitask learning", "author": ["Romera-Paredes", "Bernardino", "Aung", "Hane", "Bianchi-Berthouze", "Nadia", "Pontil", "Massimiliano"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Romera.Paredes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Romera.Paredes et al\\.", "year": 2013}, {"title": "Learning with tensors: a framework based on convex optimization and spectral regularization", "author": ["Signoretto", "Marco", "Dinh", "Quoc Tran", "De Lathauwer", "Lieven", "Suykens", "Johan AK"], "venue": "Machine Learning,", "citeRegEx": "Signoretto et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Signoretto et al\\.", "year": 2014}, {"title": "Multilinear analysis of image ensembles: Tensorfaces", "author": ["Vasilescu", "M Alex O", "Terzopoulos", "Demetri"], "venue": "VisionECCV", "citeRegEx": "Vasilescu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Vasilescu et al\\.", "year": 2002}, {"title": "Multitask learning meets tensor factorization: task imputation via convex optimization", "author": ["Wimalawarne", "Kishan", "Sugiyama", "Masashi", "Tomioka", "Ryota"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wimalawarne et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wimalawarne et al\\.", "year": 2014}, {"title": "Fast multivariate spatio-temporal analysis via low rank tensor learning", "author": ["Yu", "Rose", "Bahadori", "Mohammad Taha", "Liu", "Yan"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Accelerated online low rank tensor learning for multivariate spatiotemporal streams", "author": ["Yu", "Rose", "Cheng", "Dehua", "Liu", "Yan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Multilinear subspace regression: An orthogonal tensor decomposition approach", "author": ["Zhao", "Qibin", "Caiafa", "Cesar F", "Mandic", "Danilo P", "Zhang", "Liqing", "Ball", "Tonio", "Schulze-Bonhage", "Andreas", "Cichocki", "Andrzej"], "venue": "In NIPS,", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}, {"title": "Tensor regression with applications in neuroimaging data analysis", "author": ["Zhou", "Hua", "Li", "Lexin", "Zhu", "Hongtu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Massive multiway data emerge from many fields: spacetime measurements on several variables in climate dynamics (Hasselmann, 1997), multichannel EEG signals in neurology (Acar et al., 2007) and natural images sequences in computer vision (Vasilescu & Terzopoulos, 2002).", "startOffset": 169, "endOffset": 188}, {"referenceID": 26, "context": "Many tensor regression methods have been proposed (Zhao et al., 2011; Zhou et al., 2013; Romera-Paredes et al., 2013; Wimalawarne et al., 2014; Signoretto et al., 2014), leading to a broad range of successful applications ranging from neural science to climate research to social network analysis.", "startOffset": 50, "endOffset": 168}, {"referenceID": 27, "context": "Many tensor regression methods have been proposed (Zhao et al., 2011; Zhou et al., 2013; Romera-Paredes et al., 2013; Wimalawarne et al., 2014; Signoretto et al., 2014), leading to a broad range of successful applications ranging from neural science to climate research to social network analysis.", "startOffset": 50, "endOffset": 168}, {"referenceID": 20, "context": "Many tensor regression methods have been proposed (Zhao et al., 2011; Zhou et al., 2013; Romera-Paredes et al., 2013; Wimalawarne et al., 2014; Signoretto et al., 2014), leading to a broad range of successful applications ranging from neural science to climate research to social network analysis.", "startOffset": 50, "endOffset": 168}, {"referenceID": 23, "context": "Many tensor regression methods have been proposed (Zhao et al., 2011; Zhou et al., 2013; Romera-Paredes et al., 2013; Wimalawarne et al., 2014; Signoretto et al., 2014), leading to a broad range of successful applications ranging from neural science to climate research to social network analysis.", "startOffset": 50, "endOffset": 168}, {"referenceID": 21, "context": "Many tensor regression methods have been proposed (Zhao et al., 2011; Zhou et al., 2013; Romera-Paredes et al., 2013; Wimalawarne et al., 2014; Signoretto et al., 2014), leading to a broad range of successful applications ranging from neural science to climate research to social network analysis.", "startOffset": 50, "endOffset": 168}, {"referenceID": 5, "context": "ALS displays unstable convergence properties and outputs sub-optimal solutions (Cichocki et al., 2009).", "startOffset": 79, "endOffset": 102}, {"referenceID": 11, "context": "Trace-norm minimization suffers from slow convergence (Gandy et al., 2011).", "startOffset": 54, "endOffset": 74}, {"referenceID": 27, "context": "For example, (Zhou et al., 2013) proposes to use Alternating least square (ALS) algorithm.", "startOffset": 13, "endOffset": 32}, {"referenceID": 20, "context": "(Romera-Paredes et al., 2013) employs ALS as well as an Alternating Direction Method of Multiplier (ADMM) technique to solve the nuclear norm regularized optimization problem.", "startOffset": 0, "endOffset": 29}, {"referenceID": 21, "context": "(Signoretto et al., 2014) proposes a more general version of ADMM based on Douglas-Rachford splitting method.", "startOffset": 0, "endOffset": 25}, {"referenceID": 24, "context": "To address the scalability issue of these methods, (Yu et al., 2014) proposes a greedy algorithm following the Orthogonal Matching Pursuit (OMP) scheme.", "startOffset": 51, "endOffset": 68}, {"referenceID": 16, "context": "Our work is closely related to iterative hard thresholding in compressive sensing (Blumensath & Davies, 2009), sparsified gradient descent in sparse recovery (Garg & Khandekar, 2009) or singular value projection method in lowrank matrix completion (Jain et al., 2010).", "startOffset": 248, "endOffset": 267}, {"referenceID": 20, "context": "For example, in multi-linear multitask learning (Romera-Paredes et al., 2013), given the predictor and response for each task (X,Y), the empirical loss is defined as the summarization of the loss for all the tasks, i.", "startOffset": 48, "endOffset": 77}, {"referenceID": 27, "context": "For the univariate GLM model in (Zhou et al., 2013), the model is defined as Y = vec(X ) vec(W)+E .", "startOffset": 32, "endOffset": 51}, {"referenceID": 11, "context": "A common approach is to use nuclear norm as a convex surrogate to approximate the rank constraint (Gandy et al., 2011).", "startOffset": 98, "endOffset": 118}, {"referenceID": 4, "context": "We prove that TPG guarantees optimality of the estimated solution, under the assumption that the predictor tensor satisfies Restricted Isometry Property (RIP) (Candes et al., 2006).", "startOffset": 159, "endOffset": 180}, {"referenceID": 18, "context": "Similar assumption can be found in (Rauhut et al., 2015).", "startOffset": 35, "endOffset": 56}, {"referenceID": 20, "context": "Multi-linear multi-task learning (Romera-Paredes et al., 2013; Wimalawarne et al., 2014) tackles the scenario where the tasks to be learned are references by multiple indices, thus contain multi-modal relationship.", "startOffset": 33, "endOffset": 88}, {"referenceID": 23, "context": "Multi-linear multi-task learning (Romera-Paredes et al., 2013; Wimalawarne et al., 2014) tackles the scenario where the tasks to be learned are references by multiple indices, thus contain multi-modal relationship.", "startOffset": 33, "endOffset": 88}, {"referenceID": 24, "context": "\u2022 Greedy (Yu et al., 2014): a fast tensor regression solution that sequentially estimates rank one subspace based on Orthogonal Matching Pursuit", "startOffset": 9, "endOffset": 26}, {"referenceID": 11, "context": "\u2022 ADMM (Gandy et al., 2011): alternating direction method of multipliers for nuclear norm regularized optimization", "startOffset": 7, "endOffset": 27}, {"referenceID": 20, "context": "Our evaluation follows the same experiment setting in (Romera-Paredes et al., 2013) on the restaurant & consumer dataset, provided by the authors in the paper.", "startOffset": 54, "endOffset": 83}, {"referenceID": 16, "context": "This modification does not affect the theoretical properties of the proposed algorithm, but would lead to significant empirical improvement (Jain et al., 2010).", "startOffset": 140, "endOffset": 159}], "year": 2016, "abstractText": "Tensor regression has shown to be advantageous in learning tasks with multi-directional relatedness. Given massive multiway data, traditional methods are often too slow to operate on or suffer from memory bottleneck. In this paper, we introduce subsampled tensor projected gradient to solve the problem. Our algorithm is impressively simple and efficient. It is built upon projected gradient method with fast tensor power iterations, leveraging randomized sketching for further acceleration. Theoretical analysis shows that our algorithm converges to the correct solution in fixed number of iterations. The memory requirement grows linearly with the size of the problem. We demonstrate superior empirical performance on both multi-linear multi-task learning and spatio-temporal applications.", "creator": "LaTeX with hyperref package"}}}