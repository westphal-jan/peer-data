{"id": "1607.02586", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jul-2016", "title": "Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks", "abstract": "We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose a novel approach that models future frames in a probabilistic manner. Our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image. Future frame synthesis is challenging, as it involves low- and high-level image and motion understanding. We propose a novel network structure, namely a Cross Convolutional Network to aid in synthesizing future frames; this network structure encodes image and motion information as feature maps and convolutional kernels, respectively. In experiments, our model performs well on synthetic data, such as 2D shapes and animated game sprites, as well as on real-wold videos. We also show that our model can be applied to tasks such as visual analogy-making, and present an analysis of the learned network representations.", "histories": [["v1", "Sat, 9 Jul 2016 08:41:40 GMT  (767kb,D)", "http://arxiv.org/abs/1607.02586v1", "The first two authors contributed equally to this work"]], "COMMENTS": "The first two authors contributed equally to this work", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["tianfan xue", "jiajun wu 0001", "katherine l bouman", "bill freeman"], "accepted": true, "id": "1607.02586"}, "pdf": {"name": "1607.02586.pdf", "metadata": {"source": "CRF", "title": "Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks", "authors": ["Tianfan Xue", "Jiajun Wu", "Katherine L. Bouman", "William T. Freeman"], "emails": ["billf}@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it's not that we're able to show ourselves what we want to save the world, but that we're able to change the world, \"he said in an interview with The New York Times.\" It's not that we're able to save the world, \"he said.\" But it's not that we're able to save the world. \"\" It's not that we need to save the world. \"\" It's not that we can save the world, \"he said.\" It's not that we need to save the world. \"\" It's not that we need to save the world. \"\" It's not that we need to save it. \"It's not that we need to save it.\" \"It's not that we can save it.\" It's not that we need to save the world. \"\" It's not that we need to save the world. \"\" It's not that we need to save it. \"It's not that we need to save it.\""}, {"heading": "2 Related Work", "text": "In this context, we must also mention the fact that the persons who have appeared in the United States in recent years are persons who are able to identify themselves, namely persons who are able to identify themselves, and persons who are able to identify themselves. (...) It is a fact that these persons are persons who are able to identify themselves. (...) It is a fact that they are persons who are able to identify themselves. (...) It is a fact that they are persons who are able to identify themselves. (...). () () (). () (). (). () (). () (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). ().). (). (). (). (). ().). (). (). ().). (). (). (). (). (). ().). (). ()). (). (). ().). (). (). ().). (). ()). (). ()). ()). (). ()). ()). ()). (). ()). (). (). ()). (). ()). ()). (). ()).). (). ().). (). (). ()). (). (). (). ()). ().)).). (). (). ()). (). ()). (). ()). (). (). (). (). ())))). ()). (). ().)). (). ()).)). ()). (). ()). (). (). ()))))))). (). (). (). ())))).)))). ()))))). ()))). ().)). (). ())))"}, {"heading": "3 Formulation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Problem Definition", "text": "In this section, we will describe how to scan future frames from a current observation image. Here, we will focus on the synthesis of the next frame; in view of an RGB image I have observed at t, our goal is to model the conditional distribution of possible frames observed at t + 1. Formally, {(I (1), J (1),.., (I (n), J (n)} is to be the set of image pairs in our training set where I (i) and J (i) are images observed in two consecutive time steps. On the basis of this data, our task is to model the distribution of all possible next frames J for a new, hitherto invisible test image I and to sample new images from this distribution.In practice, we choose not to predict the next frame directly, but instead to predict the differential image v = J \u2212 I, also known as the Eulerian motion, between the observed frame and the future frame I; both problems are equal to predict J."}, {"heading": "3.2 A Toy Example", "text": "We have a simple example of how to learn how to distribute future frames in t + 1 time. It is a world of circles and squares with corresponding images that contain exactly one shape. All circles move vertically, as shown in Figure 2."}, {"heading": "3.3 Conditional Variational Autoencoder", "text": "In this section, we formally derive the educational goal of our model by following the similar derivatives as in Kingma and Welling (2014), Kingma et al. [2014], Gregor et al. [2015], Yan et al. [2015]. Let us consider the following generative process, which is a future framework (based on a model parameterized in 2001, conditioned by an observed Fig. I (see the graphic model in Fig. 2 (d)). In this thesis, we assume that pz (z) is a multivariate Gaussian model in which each dimension is i.i.d. with zero mean and unit variance. Then, faced with a value of z, the algorithms stitch the image of the intensity difference v from the conditional distribution (v | I, z). The final image, J = I + v, is then returned as Output.Objective function."}, {"heading": "4 Method", "text": "In this section, we present a traceable neural network structure that defines the generative functions fmean and the recognition functions gmean and gvar. Once these functions are trained, they can be used in conjunction with an input image to test future frames. First, we describe our newly proposed cross-convolutional layer, which naturally characterizes a layered motion representation [Wang and Adelson, 1993]. Then, we explain our network structure and demonstrate how to integrate the cross-convolutional layer into the network for future frame synthesis. \u0445 Here, the bold I denotes an identity matrix, while the normal I denotes the observed image."}, {"heading": "4.1 Layered Motion Representations and Cross Convolution Networks", "text": "Intuitively, different semantic segments in an image should have different distributions of all possible movements; for example, a building is often static, but a flow flows. To model the layered movement, we propose a novel Convolutionary Network (Figure 3). The network first splits an input image pyramid through an image encoder into several feature cards (Figure 3 (c)). Subsequently, it wraps these cards using different Convolutionary Nuclei (Figure 3 (d)) and uses the outputs to synthesize a differential image (Figure 3 (e)). This network structure naturally fits the layered motion representation, since each feature card characterizes an image layer (note that it differs from a network layer) and the corresponding core characterizes the movement of that layer. In other words, we model movements as Convolutionary Nuclei, which are therefore applied to a traditional image."}, {"heading": "4.2 Network Structure", "text": "This year, we are talking about just one person who is able to establish himself in the region without being able to establish himself in the region."}, {"heading": "5 Evaluations", "text": "We start with a dataset of 2D shapes, which is used to measure our model against objects with simple but non-trivial motion distributions. Following Reed et al. [2015], we then test our method on a dataset of video game sprites \u2020 with different motion. In addition to these synthetic datasets, we evaluate our framework using a new real video dataset. Also note that our model uses consecutive frames for training that do not require monitoring. Experimental results are also available on our project page \u00b2 and can be found there for better visualization."}, {"heading": "5.1 Movement of 2D Shapes", "text": "The synthetic 2D shape dataset contains three types of objects: circles, squares and triangles, where circles always move vertically, squares horizontally and triangles diagonally. The movement of circles and squares are independent, but the movement of circles and triangles are strongly correlated. The shapes can be strongly closed, and their sizes, positions and colors are chosen randomly. We synthesize 20, 000 image pairs for training and 500 for testing. The results are shown in Figure 4 (a) and (b) show a sample of consecutive frames in the dataset, and Figure 4 (c) shows the reconstruction of the second frame after coding and decoding with the bottom truth image."}, {"heading": "5.2 Movement of Video Game Sprites", "text": "The dataset consists of 672 unique characters, and for each character there are 5 animations (spelling, sliding, walking, slash, shooting) from 4 different angles. Each animation ranges from 6 to 13 frames. We collect 102, 364 pairs of adjacent frames for training and 3, 140 pairs for the test. In building the dataset, we make sure that both the training and the test sets do not show the same character. Synthesized sample frames are shown in Figure 5. The result shows that our algorithm is able to capture various possible movements from a single input screen that match the movements in the training set.For quantitative evaluation, we conduct behavioral experiments on Amazon Mechanical Turk."}, {"heading": "5.3 Movement in Real Videos Captured in the Wild", "text": "To show that our algorithm can also process real videos, we collect 20 training videos from YouTube, each of which is about 30 to 60 minutes long. First, we apply motion stabilization to the training data as a pre-processing step to remove camera movements. Then, we extract 56, 838 image pairs for training and 6,243 image pairs for testing. The training and test pairs come from different video sequences. Figure 6 shows that our frame works well at predicting the movement of legs and torso."}, {"heading": "5.4 Zero-Shot Visual Analogy-Making", "text": "Inspired by some recent work on visual analogy [Reed et al., 2015, Sadeghi et al., 2015], we show that even without monitoring analogies during training, our framework can easily be applied to the same task. Specifically, Reed et al. [2015] investigated the problem of inferring the relationship between a pair of images and synthesizing a new image by applying the inferred relationship to a new input image. Our motion encoder, which aims to extract motion information from two consecutive frames, can also be used to extract and synthesize relationships between pairs of images, as shown in Figure 7. In addition to qualitative experiments, we also quantitatively evaluate our evolutionary network for zero-shot visual analogies and show the results in Table 2. Although our method does not require analogy monitoring, it still works better than those in Reed et al. [2015], which requires visual analogies during training."}, {"heading": "5.5 Visualizing Feature Maps", "text": "We visualize the learned characteristic maps (see Figure 3 (b)) in Figure 8. Even without supervision, our network learns to recognize objects or contours in the image. For example, we see that the network automatically learns object (triangle and circle) detectors and edge detectors on the shape data set. It also learns a hair and body detector on the sprites or on the exercise data sets."}, {"heading": "5.6 Dimension of Latent Representation z", "text": "Although our latent motion representation z has 3,200 dimensions, its intrinsic dimensionality is much smaller. Table 1 shows the number of non-zero elements in predicted Zmean for 1,000 test samples. Note: Zmean is very sparse. Furthermore, we perform Principal Component Analysis (PCA) on the Zmeans and find that less than 30 principle components are required to cover 95% of the variance, suggesting that our network has learned a sparse representation of motion in an unattended manner and encodes high-level knowledge with a small number of bits, rather than simply remembering the differential images. It learns this sparse representation automatically by using the CLL divergence criterion Equation 2, which forces the latent representation z to carry minimal information, as discussed by Hinton and Van Camp [1993] and at the same time by Higgins et al. [2016]."}, {"heading": "6 Conclusion", "text": "We have shown that our framework works well on both synthetic and real videos. More broadly, the results suggest that our model of likely visual dynamics could be useful for additional applications, such as retracing higher order relationships of objects by studying correlations in their motion distributions. Furthermore, this learned representation could potentially be used as advanced motion ahead of other applications of computer vision and computational photography. Recognition The authors thank Yining Wang for helpful discussions. This work is partially supported by NSF Robust Intelligence 1212849 Reconstructive Recognition, ONR MURI 6923196, Adobe, and Shell Research."}], "references": [{"title": "Panoramic video textures", "author": ["References Aseem Agarwala", "Ke Colin Zheng", "Chris Pal", "Maneesh Agrawala", "Michael Cohen", "Brian Curless", "David Salesin", "Richard Szeliski"], "venue": "ACM TOG,", "citeRegEx": "Agarwala et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Agarwala et al\\.", "year": 2005}, {"title": "Deep generative image models using an laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In NIPS,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["Chelsea Finn", "Ian Goodfellow", "Sergey Levine"], "venue": "arXiv preprint arXiv:1605.07157,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Design and use of linear models for image motion analysis", "author": ["David J Fleet", "Michael J Black", "Yaser Yacoob", "Allan D Jepson"], "venue": null, "citeRegEx": "Fleet et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Fleet et al\\.", "year": 2000}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Early visual concept learning with unsupervised deep learning", "author": ["Irina Higgins", "Loic Matthey", "Xavier Glorot", "Arka Pal", "Benigno Uria", "Charles Blundell", "Shakir Mohamed", "Alexander Lerchner"], "venue": "arXiv preprint arXiv:1606.05579,", "citeRegEx": "Higgins et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Higgins et al\\.", "year": 2016}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["Geoffrey E Hinton", "Drew Van Camp"], "venue": "In COLT,", "citeRegEx": "Hinton and Camp.,? \\Q1993\\E", "shortCiteRegEx": "Hinton and Camp.", "year": 1993}, {"title": "Cliplets: juxtaposing still and dynamic imagery", "author": ["Neel Joshi", "Sisil Mehta", "Steven Drucker", "Eric Stollnitz", "Hugues Hoppe", "Matt Uyttendaele", "Michael Cohen"], "venue": "In UIST,", "citeRegEx": "Joshi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2012}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "In ICLR,", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Automated video looping with progressive dynamism", "author": ["Zicheng Liao", "Neel Joshi", "Hugues Hoppe"], "venue": "ACM TOG,", "citeRegEx": "Liao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2013}, {"title": "Beyond pixels: exploring new representations and applications for motion analysis", "author": ["Ce Liu"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Liu.,? \\Q2009\\E", "shortCiteRegEx": "Liu.", "year": 2009}, {"title": "Sift flow: Dense correspondence across scenes and its applications", "author": ["Ce Liu", "Jenny Yuen", "Antonio Torralba"], "venue": "IEEE TPAMI,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Ideal Observers for Detecting Motion: Correspondence Noise", "author": ["Hongjing Lu", "Alan L Yuille"], "venue": "In NIPS,", "citeRegEx": "Lu and Yuille.,? \\Q2006\\E", "shortCiteRegEx": "Lu and Yuille.", "year": 2006}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Michael Mathieu", "Camille Couprie", "Yann LeCun"], "venue": "In ICLR,", "citeRegEx": "Mathieu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2016}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["Junhyuk Oh", "Xiaoxiao Guo", "Honglak Lee", "Richard L Lewis", "Satinder Singh"], "venue": "In NIPS,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Dejavu: Motion prediction in static images", "author": ["S.L. Pintea", "J.C. van Gemert", "A.W.M. Smeulders"], "venue": "In ECCV,", "citeRegEx": "Pintea et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pintea et al\\.", "year": 2014}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "In ICLR,", "citeRegEx": "Radford et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Deep visual analogy-making", "author": ["Scott E Reed", "Yi Zhang", "Yuting Zhang", "Honglak Lee"], "venue": "In NIPS,", "citeRegEx": "Reed et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2015}, {"title": "On the spatial statistics of optical flow", "author": ["S. Roth", "M.J. Black"], "venue": "In ICCV,", "citeRegEx": "Roth and Black.,? \\Q2005\\E", "shortCiteRegEx": "Roth and Black.", "year": 2005}, {"title": "Visalogy: Answering visual analogy questions", "author": ["Fereshteh Sadeghi", "C Lawrence Zitnick", "Ali Farhadi"], "venue": "In NIPS,", "citeRegEx": "Sadeghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sadeghi et al\\.", "year": 2015}, {"title": "Anticipating visual representations from unlabeled video", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": "In CVPR,", "citeRegEx": "Vondrick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vondrick et al\\.", "year": 2016}, {"title": "An uncertain future: Forecasting from static images using variational autoencoders", "author": ["Jacob Walker", "Carl Doersch", "Abhinav Gupta", "Martial Hebert"], "venue": "arXiv preprint arXiv:1606.07873,", "citeRegEx": "2015", "shortCiteRegEx": "2015", "year": 2016}, {"title": "Layered representation for motion analysis", "author": ["John YA Wang", "Edward H Adelson"], "venue": "In CVPR,", "citeRegEx": "Wang and Adelson.,? \\Q1993\\E", "shortCiteRegEx": "Wang and Adelson.", "year": 1993}, {"title": "Slow and Smooth: a Bayesian theory for the combination of local motion signals in human vision", "author": ["Yair Weiss", "Edward H. Adelson"], "venue": "Center for Biological and Computational Learning Paper,", "citeRegEx": "Weiss and Adelson.,? \\Q1998\\E", "shortCiteRegEx": "Weiss and Adelson.", "year": 1998}, {"title": "Space-time video completion", "author": ["Yonatan Wexler", "Eli Shechtman", "Michal Irani"], "venue": "In CVPR,", "citeRegEx": "Wexler et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wexler et al\\.", "year": 2004}, {"title": "Eulerian video magnification for revealing subtle changes in the world", "author": ["Hao-Yu Wu", "Michael Rubinstein", "Eugene Shih", "John Guttag", "Fr\u00e9do Durand", "William Freeman"], "venue": "ACM TOG,", "citeRegEx": "Wu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2012}, {"title": "Synthesizing dynamic textures and sounds by spatial-temporal generative convnet", "author": ["Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "venue": "arXiv preprint arXiv:1606.00972,", "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee"], "venue": "arXiv preprint arXiv:1512.00570,", "citeRegEx": "Yan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "In this work, we propose a neural network structure, based on a variational autoencoder [Kingma and Welling, 2014] and our newly proposed cross convolutional layer, to tackle this problem.", "startOffset": 88, "endOffset": 114}, {"referenceID": 27, "context": ", 2015], our network finds an intrinsic representation of intensity changes between two images, also known as the difference image or Eulerian motion [Wu et al., 2012].", "startOffset": 150, "endOffset": 167}, {"referenceID": 22, "context": "Pioneering work by Weiss and Adelson [1998] found that the human visual system prefers slow and smooth motion fields.", "startOffset": 19, "endOffset": 44}, {"referenceID": 13, "context": "More recent work by Lu and Yuille [2006] found that humans make similar motion predictions as a Bayesian ideal observer.", "startOffset": 20, "endOffset": 41}, {"referenceID": 13, "context": "More recent work by Lu and Yuille [2006] found that humans make similar motion predictions as a Bayesian ideal observer. Roth and Black [2005] analyzed the response of spatial filters applied to optical flow fields and concluded that the spatial distribution of motion resembles that of a heavy-tailed distribution.", "startOffset": 20, "endOffset": 143}, {"referenceID": 3, "context": "Fleet et al. [2000] found that a local motion field can be represented by a linear combination of a small number of bases.", "startOffset": 0, "endOffset": 20}, {"referenceID": 17, "context": "Motion Prediction Our problem is closely related to the motion prediction problem, whose goal is to predict a motion field [Pintea et al., 2014] or trajectory of objects [Walker et al.", "startOffset": 123, "endOffset": 144}, {"referenceID": 12, "context": "For example, Liu et al. [2011] predicted a motion field of an image by transferring a similar motion field from a database, Pintea et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 12, "context": "For example, Liu et al. [2011] predicted a motion field of an image by transferring a similar motion field from a database, Pintea et al. [2014] learned a random-forest based mapping from image content to a motion field, and Vondrick et al.", "startOffset": 13, "endOffset": 145}, {"referenceID": 12, "context": "For example, Liu et al. [2011] predicted a motion field of an image by transferring a similar motion field from a database, Pintea et al. [2014] learned a random-forest based mapping from image content to a motion field, and Vondrick et al. [2016] estimated the variation of future image features from observed frames.", "startOffset": 13, "endOffset": 248}, {"referenceID": 12, "context": "For example, Liu et al. [2011] predicted a motion field of an image by transferring a similar motion field from a database, Pintea et al. [2014] learned a random-forest based mapping from image content to a motion field, and Vondrick et al. [2016] estimated the variation of future image features from observed frames. However, as demonstrated in Figure 1, deterministic prediction is often impossible due to the intrinsic ambiguity of the problem. In order to model a distribution of possible motions, Walker et al. [2015] posed the motion prediction problem as a classification task and predicted the motion class label for each pixel in the image.", "startOffset": 13, "endOffset": 524}, {"referenceID": 12, "context": "For example, Liu et al. [2011] predicted a motion field of an image by transferring a similar motion field from a database, Pintea et al. [2014] learned a random-forest based mapping from image content to a motion field, and Vondrick et al. [2016] estimated the variation of future image features from observed frames. However, as demonstrated in Figure 1, deterministic prediction is often impossible due to the intrinsic ambiguity of the problem. In order to model a distribution of possible motions, Walker et al. [2015] posed the motion prediction problem as a classification task and predicted the motion class label for each pixel in the image. This model was, however, not designed to capture pixel-wise correlations in the motion field, i.e., neighboring pixels belonging to the same object may move in opposite directions. Recently, and concurrently with our own work, Walker et al. [2016] introduced a variational autoencoder to model pixel-wise correlations in the motion field.", "startOffset": 13, "endOffset": 899}, {"referenceID": 8, "context": "These ideas were later extended to generate cinemagraphies [Joshi et al., 2012], seamlessly looping videos containing a variety of objects with different motion patterns [Agarwala et al.", "startOffset": 59, "endOffset": 79}, {"referenceID": 26, "context": ", 2013], or video inpainting [Wexler et al., 2004].", "startOffset": 29, "endOffset": 50}, {"referenceID": 0, "context": ", 2012], seamlessly looping videos containing a variety of objects with different motion patterns [Agarwala et al., 2005, Liao et al., 2013], or video inpainting [Wexler et al., 2004]. While high-resolution and realistic looking videos are generated using these techniques, they are often limited to periodic motion and require an input reference video. In contrast, we build an image generation model that does not require a reference video at test time. Recently, several network structures have been proposed to synthesize a new frame from observed frames. Srivastava et al. [2015] designed a LSTM network that synthesized future frames in a sequence from set of observed frames.", "startOffset": 99, "endOffset": 585}, {"referenceID": 0, "context": ", 2012], seamlessly looping videos containing a variety of objects with different motion patterns [Agarwala et al., 2005, Liao et al., 2013], or video inpainting [Wexler et al., 2004]. While high-resolution and realistic looking videos are generated using these techniques, they are often limited to periodic motion and require an input reference video. In contrast, we build an image generation model that does not require a reference video at test time. Recently, several network structures have been proposed to synthesize a new frame from observed frames. Srivastava et al. [2015] designed a LSTM network that synthesized future frames in a sequence from set of observed frames. Mathieu et al. [2016] proposed to synthesize the next frame in a sequence from a few previous frames, using a multi-scale network, and Oh et al.", "startOffset": 99, "endOffset": 705}, {"referenceID": 0, "context": ", 2012], seamlessly looping videos containing a variety of objects with different motion patterns [Agarwala et al., 2005, Liao et al., 2013], or video inpainting [Wexler et al., 2004]. While high-resolution and realistic looking videos are generated using these techniques, they are often limited to periodic motion and require an input reference video. In contrast, we build an image generation model that does not require a reference video at test time. Recently, several network structures have been proposed to synthesize a new frame from observed frames. Srivastava et al. [2015] designed a LSTM network that synthesized future frames in a sequence from set of observed frames. Mathieu et al. [2016] proposed to synthesize the next frame in a sequence from a few previous frames, using a multi-scale network, and Oh et al. [2015] and Finn et al.", "startOffset": 99, "endOffset": 835}, {"referenceID": 0, "context": ", 2012], seamlessly looping videos containing a variety of objects with different motion patterns [Agarwala et al., 2005, Liao et al., 2013], or video inpainting [Wexler et al., 2004]. While high-resolution and realistic looking videos are generated using these techniques, they are often limited to periodic motion and require an input reference video. In contrast, we build an image generation model that does not require a reference video at test time. Recently, several network structures have been proposed to synthesize a new frame from observed frames. Srivastava et al. [2015] designed a LSTM network that synthesized future frames in a sequence from set of observed frames. Mathieu et al. [2016] proposed to synthesize the next frame in a sequence from a few previous frames, using a multi-scale network, and Oh et al. [2015] and Finn et al. [2016] proposed to synthesize a future frame assuming a certain action is taken.", "startOffset": 99, "endOffset": 858}, {"referenceID": 0, "context": ", 2012], seamlessly looping videos containing a variety of objects with different motion patterns [Agarwala et al., 2005, Liao et al., 2013], or video inpainting [Wexler et al., 2004]. While high-resolution and realistic looking videos are generated using these techniques, they are often limited to periodic motion and require an input reference video. In contrast, we build an image generation model that does not require a reference video at test time. Recently, several network structures have been proposed to synthesize a new frame from observed frames. Srivastava et al. [2015] designed a LSTM network that synthesized future frames in a sequence from set of observed frames. Mathieu et al. [2016] proposed to synthesize the next frame in a sequence from a few previous frames, using a multi-scale network, and Oh et al. [2015] and Finn et al. [2016] proposed to synthesize a future frame assuming a certain action is taken. Specifically, concurrent work from Finn et al. [2016] also discussed the idea of learning output convolutional kernels.", "startOffset": 99, "endOffset": 986}, {"referenceID": 9, "context": "(2) Motion prior Variational autoencoders [Kingma and Welling, 2014] can be used to model the distribution of motion fields, as shown in Figure 2(c).", "startOffset": 42, "endOffset": 68}, {"referenceID": 8, "context": "In this section, we will formally derive the training objective of our model, following the similar derivations as those in Kingma and Welling [2014], Kingma et al.", "startOffset": 124, "endOffset": 150}, {"referenceID": 8, "context": "In this section, we will formally derive the training objective of our model, following the similar derivations as those in Kingma and Welling [2014], Kingma et al. [2014], Gregor et al.", "startOffset": 124, "endOffset": 172}, {"referenceID": 5, "context": "[2014], Gregor et al. [2015], Yan et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 5, "context": "[2014], Gregor et al. [2015], Yan et al. [2015]. Consider the following generative process that samples a future frame from a \u03b8 parametrized model, conditioned on an observed image I (see the graphical model in Figure 2(d)).", "startOffset": 8, "endOffset": 48}, {"referenceID": 9, "context": "Directly maximizing this marginal distribution is hard, thus we instead maximize its variational upper-bound, as proposed by Kingma and Welling [2014]. Each term in the marginal distribution is upper-bounded by L(\u03b8, \u03c6, v|I) = \u2212DKL(q\u03c6(z|v, I)||pz(z)) + Eq\u03c6(z|v(i),I(i)) [ log p\u03b8(v |z, I) ] , (1)", "startOffset": 125, "endOffset": 151}, {"referenceID": 9, "context": "Using the reparameterization trick [Kingma and Welling, 2014], we approximate both distributions as Gaussian, where the mean and variance of the distributions are functions specified by a generative network and a recognition network, respectively.", "startOffset": 35, "endOffset": 61}, {"referenceID": 24, "context": "We first describe our newly proposed cross convolutional layer, which naturally characterizes a layered motion representation [Wang and Adelson, 1993].", "startOffset": 126, "endOffset": 150}, {"referenceID": 24, "context": "Motion can often be decomposed in a layer-wise manner [Wang and Adelson, 1993].", "startOffset": 54, "endOffset": 78}, {"referenceID": 19, "context": "Following Reed et al. [2015], we then test our method on a dataset of video game sprites\u2020 with diverse motions.", "startOffset": 10, "endOffset": 29}, {"referenceID": 12, "context": "We sampled 50, 000 images and used the optical flow package by Liu [2009] to calculate the speed of each object.", "startOffset": 63, "endOffset": 74}, {"referenceID": 19, "context": "We then evaluate our framework on a video game sprites dataset, also used by Reed et al. [2015], where characters have more complicated motion.", "startOffset": 77, "endOffset": 96}, {"referenceID": 19, "context": "Model spellcast thrust walk slash shoot average Add [Reed et al., 2015] 41.", "startOffset": 52, "endOffset": 71}, {"referenceID": 19, "context": "0 Dis [Reed et al., 2015] 40.", "startOffset": 6, "endOffset": 25}, {"referenceID": 19, "context": "5 Dis + Cls [Reed et al., 2015] 13.", "startOffset": 12, "endOffset": 31}, {"referenceID": 19, "context": "Specifically, Reed et al. [2015] studied the problem of inferring the relationship between a pair of images and synthesizing a new image by applying the inferred relationship to a new input image.", "startOffset": 14, "endOffset": 33}, {"referenceID": 19, "context": "Specifically, Reed et al. [2015] studied the problem of inferring the relationship between a pair of images and synthesizing a new image by applying the inferred relationship to a new input image. Our motion encoder, which aims to extract motion information from two consecutive frames, can also be used to extract and synthesize relationships between pairs of images, as shown in Figure 7. In addition to qualitative experiments, we also evaluate our cross-convolutional network on zero-shot visual analogy-making quantitatively, and show the results in Table 2. Although our method requires no analogy supervision, it still performs better than those introduced in Reed et al. [2015], which required visual analogy labels during training.", "startOffset": 14, "endOffset": 686}, {"referenceID": 6, "context": "2, which forces the latent representation z to carry minimal information, as discussed by Hinton and Van Camp [1993] and concurrently by Higgins et al. [2016].", "startOffset": 137, "endOffset": 159}], "year": 2016, "abstractText": "We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose a novel approach that models future frames in a probabilistic manner. Our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image. Future frame synthesis is challenging, as it involves lowand high-level image and motion understanding. We propose a novel network structure, namely a Cross Convolutional Network to aid in synthesizing future frames; this network structure encodes image and motion information as feature maps and convolutional kernels, respectively. In experiments, our model performs well on synthetic data, such as 2D shapes and animated game sprites, as well as on realwold videos. We also show that our model can be applied to tasks such as visual analogy-making, and present an analysis of the learned network representations.", "creator": "LaTeX with hyperref package"}}}