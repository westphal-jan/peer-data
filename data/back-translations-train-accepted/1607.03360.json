{"id": "1607.03360", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jul-2016", "title": "Approximate maximum entropy principles via Goemans-Williamson with applications to provable variational methods", "abstract": "The well known maximum-entropy principle due to Jaynes, which states that given mean parameters, the maximum entropy distribution matching them is in an exponential family, has been very popular in machine learning due to its \"Occam's razor\" interpretation. Unfortunately, calculating the potentials in the maximum-entropy distribution is intractable \\cite{bresler2014hardness}. We provide computationally efficient versions of this principle when the mean parameters are pairwise moments: we design distributions that approximately match given pairwise moments, while having entropy which is comparable to the maximum entropy distribution matching those moments.", "histories": [["v1", "Tue, 12 Jul 2016 14:09:03 GMT  (22kb)", "http://arxiv.org/abs/1607.03360v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["andrej risteski", "yuanzhi li"], "accepted": true, "id": "1607.03360"}, "pdf": {"name": "1607.03360.pdf", "metadata": {"source": "CRF", "title": "Approximate maximum entropy principles via Goemans-Williamson with applications to provable variational methods", "authors": ["Yuanzhi Li", "Andrej Risteski"], "emails": ["risteski@cs.princeton.edu."], "sections": [{"heading": null, "text": "ar Xiv: 160 7,03 360v 1 [cs.L G] 12 JuWe also offers surprising applications of the approximate maximum entropy principle in the development of detectable variation methods for calculating the partition functions of Ising models without any assumptions about the potentials of the model. More specifically, we show that at any temperature we obtain approximate guarantees for the protocol partition function, which are comparable to those in the low temperature limit, which is setting the optimization of square shapes over the hypercube. (Alon and Naor, 2006)"}, {"heading": "1 Introduction", "text": "The Maximum Entropy Principle (Jaynes, 1957) states that we already know many parameters in individual cases, i.e. that the entropy distribution mechanisms are usually an exponential distribution in the family, i.e., the maximum distribution in the family is one of the reasons for the popularity of graphical models in machine learning: \"maximum entropy\" is interpreted as \"minimal assumptions\" about the distribution of others than what is known about it. However, this principle is problematic from a computational point of view. Due to the results of (Bresler et al), 2014; Singh and Vishnoi, 2014), the potentials of the JT that we know about it, we are from a computational point of view."}, {"heading": "2 Statements of results and prior work", "text": "The most important theorems in this section are the following: Theorem 2.1. For each covariance matrix \u03a3 of a centered distribution \u00b5: {\u2212 1} n \u2192 R, i.e., E\u00b5 [xixj] = 0, there is an efficiently sampleable distribution \u00b5, which can be considered as a sign (g) where g \u00b2 N (0, \u03a3 + \u03b2I) can be considered as follows: G 1 + \u03b2, j \u2264 E\u00b5 [XiXj] \u2264 1 + \u03b2 [XiXj] \u2264 1, j) for a fixed constant G, and entropy H (\u00b5) \u2265 n 25. \"There are two previous papers at the computational level that refer to maximum entropy principles, both proving hardness results. (Bresler et al, 2014) considers the\" hard core model \"in which functional distribution is not possible."}, {"heading": "3 Approximate maximum entropy principles", "text": "We remember what the problem is that we want to solve: Adopted maximum entropy principles We have a positive-semi-definitive matrix principle (1). (1). (1). (2). (2). (2). (2). (2). (2). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (3). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (5). (5). (5). (4). (5). (5). (5). (5). (5). (5). (5). (4). (5). (5). (5). (5). (4). (5). (5). (5). (5). (5). (4). (5). (5). (5). (4). (5). (4). (5). (4). (5). (4). (5). (4). (5). (4). (4). (4). (5). (5). (5). (4). (4). (5). (4). (4). (5). (4). (5). (4). (5). (5). (5). (4). (5). (5). (4). (5). (5). (5). (5). (5). (5). (5). (4). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5"}, {"heading": "4 Provable bounds for variational methods", "text": "In this section we will consider applications of the approximate maximum entropy principles that we have developed for calculating the partition functions of Ising models. (Before we delve into the results, we will give brief preliminary remarks on the different methods and pseudo-momentary relaxation mechanisms.) We will base ourselves on the following simple questions that characterize logZ as a solution to an optimization problem. (It essentially goes back to Gibbs (Ellis, 2012), who used it in the context of statistical mechanics although it was rediscovered by machine learning researchers (Wainwright and Jordan, 2008): Lemma 4.1 (Variative Characterization of logZ). We will name the polytopy of distributions via {\u2212 1} n. (Then, logZ = max \u00b5 M {2} n)."}, {"heading": "4.1 Ising models", "text": "We proceed with the main results of this section to the Ising models, which is the case in which we calculate the potentials Ji, j, and we want to calculate the partition function of the Ising model. We will provide a constant relaxation that has a constant factor approximation in this case. First, we remember the famous First Griffiths inequality due to Griffiths (Griffiths, 1967), which states that in the ferromagnetic case the potentials Ji, j > 0."}, {"heading": "5 Conclusion", "text": "In summary, we presented computationally efficient approximate versions of the classic Max Entropy Principle of (Jaynes, 1957): efficient random distributions that maintain certain pairs of moments up to a multiplicative constant factor, while entropy lies within a constant factor of the maximum entropy distribution corresponding to those moments. Furthermore, we applied our findings to the development of detectable variation methods for the output of models that offer comparable guarantees for the approximation of the log partition to those in the optimization setting. Our methods are based on convex relaxations of the standard variation principle due to Gibbs and are extremely general, and we hope that they will find applications for other exponential families."}], "references": [{"title": "Approximating the cut-norm via grothendieck\u2019s inequality", "author": ["Noga Alon", "Assaf Naor"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Alon and Naor.,? \\Q2006\\E", "shortCiteRegEx": "Alon and Naor.", "year": 2006}, {"title": "Rounding sum-of-squares relaxations", "author": ["Boaz Barak", "Jonathan A Kelner", "David Steurer"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Barak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2014}, {"title": "Near-maximum entropy models for binary neural representations of natural images", "author": ["Matthias Bethge", "Philipp Berens"], "venue": null, "citeRegEx": "Bethge and Berens.,? \\Q2007\\E", "shortCiteRegEx": "Bethge and Berens.", "year": 2007}, {"title": "Hardness of parameter estimation in graphical models", "author": ["Guy Bresler", "David Gamarnik", "Devavrat Shah"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bresler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bresler et al\\.", "year": 2014}, {"title": "Maximizing quadratic programs: extending grothendieck\u2019s inequality", "author": ["Moses Charikar", "Anthony Wirth"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Charikar and Wirth.,? \\Q2004\\E", "shortCiteRegEx": "Charikar and Wirth.", "year": 2004}, {"title": "Entropy, large deviations, and statistical mechanics, volume 271", "author": ["Richard S Ellis"], "venue": "Springer Science & Business Media,", "citeRegEx": "Ellis.,? \\Q2012\\E", "shortCiteRegEx": "Ellis.", "year": 2012}, {"title": "The statistics of curie-weiss models", "author": ["Richard S Ellis", "Charles M Newman"], "venue": "Journal of Statistical Physics,", "citeRegEx": "Ellis and Newman.,? \\Q1978\\E", "shortCiteRegEx": "Ellis and Newman.", "year": 1978}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["Michel X Goemans", "David P Williamson"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Goemans and Williamson.,? \\Q1995\\E", "shortCiteRegEx": "Goemans and Williamson.", "year": 1995}, {"title": "Correlations in ising ferromagnets", "author": ["Robert B Griffiths"], "venue": "i. Journal of Mathematical Physics,", "citeRegEx": "Griffiths.,? \\Q1967\\E", "shortCiteRegEx": "Griffiths.", "year": 1967}, {"title": "Information theory and statistical mechanics", "author": ["Edwin T Jaynes"], "venue": "Physical review,", "citeRegEx": "Jaynes.,? \\Q1957\\E", "shortCiteRegEx": "Jaynes.", "year": 1957}, {"title": "Polynomial-time approximation algorithms for the ising model", "author": ["Mark Jerrum", "Alistair Sinclair"], "venue": "SIAM Journal on computing,", "citeRegEx": "Jerrum and Sinclair.,? \\Q1993\\E", "shortCiteRegEx": "Jerrum and Sinclair.", "year": 1993}, {"title": "How to compute partition functions using convex programming hierarchies: provable bounds for variational methods", "author": ["Andrej Risteski"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "Risteski.,? \\Q2016\\E", "shortCiteRegEx": "Risteski.", "year": 2016}, {"title": "Entropy, optimization and counting", "author": ["Mohit Singh", "Nisheeth K Vishnoi"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Singh and Vishnoi.,? \\Q2014\\E", "shortCiteRegEx": "Singh and Vishnoi.", "year": 2014}, {"title": "The computational hardness of counting in two-spin models on d-regular graphs", "author": ["Allan Sly", "Nike Sun"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Sly and Sun.,? \\Q2012\\E", "shortCiteRegEx": "Sly and Sun.", "year": 2012}, {"title": "Tree-reweighted belief propagation algorithms and approximate ml estimation by pseudo-moment", "author": ["Martin J Wainwright", "Tommi S Jaakkola", "Alan S Willsky"], "venue": null, "citeRegEx": "Wainwright et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2003}, {"title": "A new class of upper bounds on the log partition function", "author": ["Martin J Wainwright", "Tommi S Jaakkola", "Alan S Willsky"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Wainwright et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2005}, {"title": "Correctness of local probability propagation in graphical models with loops", "author": ["Yair Weiss"], "venue": "Neural computation,", "citeRegEx": "Weiss.,? \\Q2000\\E", "shortCiteRegEx": "Weiss.", "year": 2000}], "referenceMentions": [{"referenceID": 3, "context": "Unfortunately, calculating the potentials in the maximum-entropy distribution is intractable (Bresler et al., 2014).", "startOffset": 93, "endOffset": 115}, {"referenceID": 0, "context": "(Alon and Naor, 2006)", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "1 Introduction Maximum entropy principle The maximum entropy principle (Jaynes, 1957) states that given mean parameters, i.", "startOffset": 71, "endOffset": 85}, {"referenceID": 3, "context": "Due to results of (Bresler et al., 2014; Singh and Vishnoi, 2014), the potentials Jt of the Ising model, in many cases, are impossible to estimate well in polynomial time, unless NP = RP \u2013 so merely getting the description of the maximum entropy distribution is already hard.", "startOffset": 18, "endOffset": 65}, {"referenceID": 12, "context": "Due to results of (Bresler et al., 2014; Singh and Vishnoi, 2014), the potentials Jt of the Ising model, in many cases, are impossible to estimate well in polynomial time, unless NP = RP \u2013 so merely getting the description of the maximum entropy distribution is already hard.", "startOffset": 18, "endOffset": 65}, {"referenceID": 2, "context": "This provides theoretical explanation for the phenomenon observed by the computational neuroscience community (Bethge and Berens, 2007) that this distribution (named dichotomized Gaussian there) has near-maximum entropy.", "startOffset": 110, "endOffset": 135}, {"referenceID": 14, "context": "(Wainwright et al., 2003; 2005; Weiss, 2000)).", "startOffset": 0, "endOffset": 44}, {"referenceID": 16, "context": "(Wainwright et al., 2003; 2005; Weiss, 2000)).", "startOffset": 0, "endOffset": 44}, {"referenceID": 4, "context": "In the graphical model community, these are known as Ising models, and in the optimization community this is the problem of optimizing quadratic forms and has been studied by (Charikar and Wirth, 2004; Alon and Naor, 2006; Alon et al., 2006).", "startOffset": 175, "endOffset": 241}, {"referenceID": 0, "context": "In the graphical model community, these are known as Ising models, and in the optimization community this is the problem of optimizing quadratic forms and has been studied by (Charikar and Wirth, 2004; Alon and Naor, 2006; Alon et al., 2006).", "startOffset": 175, "endOffset": 241}, {"referenceID": 13, "context": "construct a FPRAS) (Sly and Sun, 2012), but nothing is known about coarser approximations.", "startOffset": 19, "endOffset": 38}, {"referenceID": 3, "context": "(Bresler et al., 2014) considers the \u201chard-core\u201d model where the functionals \u03c6t are such that the distribution \u03bc(x) puts zero mass on configurations x which are not independent sets with respect to some graph G.", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "(Singh and Vishnoi, 2014) prove an equivalence between calculating the mean parameters and calculating partition functions.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "(Ellis, 2012) As outlined before, we will be concerned with a particularly popular exponential family: Ising models.", "startOffset": 0, "endOffset": 13}, {"referenceID": 11, "context": "Moreover, other than a recent paper by (Risteski, 2016), no other work has provided provable bounds for variational methods that proceed via a convex relaxation and a rounding thereof.", "startOffset": 39, "endOffset": 55}, {"referenceID": 11, "context": "3 (Risteski, 2016) provides guarantees in the case of Ising models that are also based on pseudo-moment relaxations of the variational principle, albeit only in the special case when the graph is \u201cdense\u201d in a suitably defined sense.", "startOffset": 2, "endOffset": 18}, {"referenceID": 10, "context": "Finally, we mention that in the special case of the ferromagnetic Ising models, an algorithm based on MCMC was provided by (Jerrum and Sinclair, 1993), which can give an approximation factor of (1 + \u01eb) to the partition function and runs in time O(npoly(1/\u01eb)).", "startOffset": 123, "endOffset": 150}, {"referenceID": 7, "context": "(Goemans and Williamson, 1995) We will prove Theorem 3.", "startOffset": 0, "endOffset": 30}, {"referenceID": 7, "context": "Similarly as in the analysis of Goemans-Williamson (Goemans and Williamson, 1995), if v\u0304i = 1 \u2016vi\u2016vi, we have G\u3008v\u0304i, v\u0304j\u3009 \u2264 E\u03bc\u0303[XiXj] = 2 \u03c0 arcsin(\u3008v\u0304i, v\u0304j\u3009) \u2264 \u3008v\u0304i, v\u0304j\u3009.", "startOffset": 51, "endOffset": 81}, {"referenceID": 5, "context": "It essentially dates back to Gibbs (Ellis, 2012), who used it in the context of statistical mechanics, though it has been rediscovered by machine learning researchers (Wainwright and Jordan, 2008): Lemma 4.", "startOffset": 35, "endOffset": 48}, {"referenceID": 1, "context": "To define M\u2032 more precisely we will need the following notion: (for a more in-depth review of moment-based convex hierarchies, the reader can consult (Barak et al., 2014)) Definition 4.", "startOffset": 150, "endOffset": 170}, {"referenceID": 8, "context": "First, recall the famous First Griffiths inequality due to Griffiths (Griffiths, 1967) which states that in the ferromagnetic case, E\u03bc[xixj ] \u2265 0, \u2200i, j.", "startOffset": 69, "endOffset": 86}, {"referenceID": 4, "context": "We will do a similar thing here, by modifying roundings due to (Charikar and Wirth, 2004) and (Alon et al.", "startOffset": 63, "endOffset": 89}, {"referenceID": 4, "context": "2, we review the rounding used by (Charikar and Wirth, 2004) in the case of maximizing quadratic forms: Algorithm 1 Quadratic form rounding by (Charikar and Wirth, 2004) 1: Input: A pseudo-moment matrix \u03a3i,j = E\u03bd [xixj ] 2: Output: A sample x from a distribution \u03c1 3: Sample g from the standard Gaussian N(0, I).", "startOffset": 34, "endOffset": 60}, {"referenceID": 4, "context": "2, we review the rounding used by (Charikar and Wirth, 2004) in the case of maximizing quadratic forms: Algorithm 1 Quadratic form rounding by (Charikar and Wirth, 2004) 1: Input: A pseudo-moment matrix \u03a3i,j = E\u03bd [xixj ] 2: Output: A sample x from a distribution \u03c1 3: Sample g from the standard Gaussian N(0, I).", "startOffset": 143, "endOffset": 169}, {"referenceID": 4, "context": "Our rounding will essentially be the same as (Charikar and Wirth, 2004), except in step 3, we will produce a vector r\u2032 i by scaling down the vector ri by 2 coordinate-wise.", "startOffset": 45, "endOffset": 71}, {"referenceID": 4, "context": "By Theorem 1 in (Charikar and Wirth, 2004), we have", "startOffset": 16, "endOffset": 42}, {"referenceID": 4, "context": "i,j Ji,jE\u03bd [xixj ] Additional, both our and the (Charikar and Wirth, 2004) roundings are such that E\u03c1[xixj ] = ErEx|r[xixj ] and E\u03bc\u0303[xixj ] = Er\u2032Ex|r\u2032 [xixj ].", "startOffset": 48, "endOffset": 74}, {"referenceID": 4, "context": "Furthermore, as noted in (Charikar and Wirth, 2004), it is easy to check that E[xixj |r\u2032] = r\u2032 ir \u2032 j and obviously r \u2032 i = 2ri, \u2200i in distribution, so we have: E\u03bc\u0303[xixj ] = Er\u2032Ex|r\u2032 [xixj ] = 1 4 ErEx|r[xixj ] = 1 4 E\u03c1[xixj ] But, this directly implies", "startOffset": 25, "endOffset": 51}], "year": 2016, "abstractText": "The well known maximum-entropy principle due to Jaynes, which states that given mean parameters, the maximum entropy distribution matching them is in an exponential family, has been very popular in machine learning due to its \u201cOccam\u2019s razor\u201d interpretation. Unfortunately, calculating the potentials in the maximum-entropy distribution is intractable (Bresler et al., 2014). We provide computationally efficient versions of this principle when the mean parameters are pairwise moments: we design distributions that approximately match given pairwise moments, while having entropy which is comparable to the maximum entropy distribution matching those moments. We additionally provide surprising applications of the approximate maximum entropy principle to designing provable variational methods for partition function calculations for Ising models without any assumptions on the potentials of the model. More precisely, we show that in every temperature, we can get approximation guarantees for the log-partition function comparable to those in the low-temperature limit, which is the setting of optimization of quadratic forms over the hypercube. (Alon and Naor, 2006)", "creator": "LaTeX with hyperref package"}}}