{"id": "1607.03474", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jul-2016", "title": "Recurrent Highway Networks", "abstract": "Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with such 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory networks. We introduce a novel theoretical analysis of recurrent networks based on Ger\\v{s}gorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks (RHN), which are long not only in time but also in space, generalizing LSTMs to larger step-to-step depths. Experiments indicate that the proposed architecture results in complex but efficient models, beating previous models for character prediction on the Hutter Prize dataset with less than half of the parameters.", "histories": [["v1", "Tue, 12 Jul 2016 19:36:50 GMT  (126kb,D)", "http://arxiv.org/abs/1607.03474v1", "9 pages, 5 figures. Submitted to NIPS conference 2016"], ["v2", "Thu, 11 Aug 2016 17:07:42 GMT  (136kb,D)", "http://arxiv.org/abs/1607.03474v2", "11 pages, 5 figures, 3 tables. Submitted to NIPS conference 2016"], ["v3", "Thu, 27 Oct 2016 19:39:22 GMT  (133kb,D)", "http://arxiv.org/abs/1607.03474v3", "13 pages, 6 figures, 2 tables"], ["v4", "Fri, 3 Mar 2017 21:10:42 GMT  (145kb,D)", "http://arxiv.org/abs/1607.03474v4", "12 pages, 6 figures, 3 tables"], ["v5", "Tue, 4 Jul 2017 19:29:23 GMT  (145kb,D)", "http://arxiv.org/abs/1607.03474v5", "12 pages, 6 figures, 3 tables"]], "COMMENTS": "9 pages, 5 figures. Submitted to NIPS conference 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["julian georg zilly", "rupesh kumar srivastava", "jan koutn\u00edk", "j\u00fcrgen schmidhuber"], "accepted": true, "id": "1607.03474"}, "pdf": {"name": "1607.03474.pdf", "metadata": {"source": "CRF", "title": "Recurrent Highway Networks", "authors": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u00edk", "J\u00fcrgen Schmidhuber"], "emails": ["jzilly@ethz.ch", "juergen}@idsia.ch"], "sections": [{"heading": "1 Introduction & Previous Work", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "2 Revisiting Gradient Flow in Recurrent Networks", "text": "Let us specify the total loss for an input sequence of length T. Let us specify x [t] n [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or. [t] or \"or\" or \"or\" or \"or [t] or [t] or [t] or [t] or [t] or [t] or or [t] or [t] or [t] or [t] or [t] or\" or \"or\" or \"or\" or [t] or [t] or [t] or or [t] or or [t] or [t] or [t] or or [t] or or [t] or [t] or or [t] or [t] or [t] or or [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or [t] or] or [t] or [t] or [t] or [t] or [t] or [t] or] or [t] or [t] or [t] or] or [t] or [t] or [t] or [t] or] or [t] or] or [t] or [t] or [t] or [t] or or or or or or or or or [t] or or or or [t] or or or or or or or or or or or or [or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or [or or or or or or or or or or [or or or or or or or or or or or or or or or [or or or or or or or or or or or or"}, {"heading": "3 Recurrent Highway Networks (RHN)", "text": "Highway layer computation is defined asy = h + x \u00b7 c) where. \"Recall that the recurrent state transition in a standard RNN is described by y [t] = f (Wx] + Ry. Let h = H (x, WH), t = T (x, WT), c = C (x, WC) be outputs of nonlinear transforms H, T and C with associated weight matrices (including biases) WH, T and C. Highway layer computation is defined asy = h + x \u00b7 c (5) where\" means elementary multiplication.Recall that the recurrent transition of the transput via H or the carry over the original input x. The highway layer computation is defined asy = h + x \u00b7 c) where. \"Recall that the recurrent state transition in a standard RNN is described by y [t] = f (Wx] + Ry."}, {"heading": "4 Experiments", "text": "In this paper, the carry gate has been coupled with the transformate gate by setting C (\u00b7) = 1n \u2212 T (\u00b7) similar to the Highway Networks proposal, which prevents an unlimited swelling of state values leading to more stable training. Alternatively, an output nonlinearity similar to LSTM networks could be used to combat this problem. Additionally, we set the initial distortion of gates at the beginning of training to negative values to facilitate the flow of information. All networks use a single hidden RHN layer, as we are only interested in investigating the effects of repeat depth, not stacking multiple layers, which is already useful. Detailed configurations for all experiments are included in the accompanying material. Code and database of results for the experiments will be published in the near future."}, {"heading": "4.1 Optimization", "text": "RHN is an architecture designed to enable the optimization of recurrent networks with deep transitions. Therefore, the primary experimental verification we are seeking is whether RHNs with higher repetition depth are easier to optimize compared to other alternatives, preferably using simple gradient-based methods. We compare the optimization of RHNs with DT-RNNs and DT (S) -RNNNs [6]. Networks with a repetition depth of 1, 2, 4 and 6 are trained to predict the next step based on the polyphonic music prediction data set JSB Chorales [27]. The network size is chosen so that the total number of network parameters increases with increasing repetition depth, but remains the same in all architectures. A hyperparameter search is then performed for the SGD-based optimization of each architecture and depth combination, in order to achieve comparisons of smaller networks with similar losses."}, {"heading": "4.2 Sequence Modeling", "text": "The task is the next symbol prediction with a total of 205 Unicode symbols. The training is performed with SGD with dynamics and weight noise as used by Graves [29]. Due0 50 100 150 200 epoches20406080100120140160180200P erpl exityDepth 1 Depth 3 Training Validation Figures 4: Training and Validation set perplexity over the course of training on Penn Treebank word-level language modeling using RHNs with fixed parameter budget and increasing recurrence depth.Table 1: Bits per character (BPC) on the Hutter Wikipedia dataset (without dynamic evaluation).Architecture BPC # Param. Test DataStacked LSTM 1.67 27.0 M 4 MB 1.MB 1.M RMB 1.47 different grills."}, {"heading": "5 Analysis", "text": "We analyze the inner workings of RHNs by inspecting Tor activations and their effects on network performance. For the 6-layer repetition RHN, which is optimized on the JSB Chorales dataset (subsection 4.1), Figure 5 (a) shows the mean transformation activity in each layer over time steps for 4 sample sequences. We note that the gates tend toward zero (white) during initialization, but all layers in the trained network are used in different ways. Goal activity in the first layer of the recurring transition is typically high on average, indicating that at least one layer of the recurring transition is almost always used. Goals in other layers have different behavior and change their activity over time for each sequence. The contributions of the layers to network performance can be quantified by a tension experiment similar to Srivastaal [25]."}, {"heading": "6 Conclusion", "text": "We developed a new analysis of the behavior of RNNs based on the Ger\u0161gorin Circle Theorem. The analysis provides insights into other recently proposed architectures such as IRNNs and Highway Networks. A key result is the representation of inherent limitations of RNNs and the ability of gates to variably influence learning. In addition, we introduced Recurrent Highway Networks, a powerful new model designed to exploit the greater depth of the recurring transition without causing additional training problems. Experiments confirmed the theoretical optimization benefits as well as improved performance in sequence modeling of tasks due to increased repetition depth. Future work will compare RHN with a pure LSTM with multiple microticks per time step. Findings: This research was supported in part by the H2020 project \"Intuitive Natural Prothesis UTilization\" (INPUT; 687795) and SNF funding \"Advanced Reincement Learning\" (# 1582)."}, {"heading": "7 Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Details of Experimental Setups", "text": "In these experiments, we compare RHNs with Deep Transition RNNs (DT-RNNs) and Deep Transition RNNs with Skip connections (DT (S) -RNNNs) introduced by [6]. A total number of 60 runs per architecture and depth is recorded. The number of units in each level of repetition is set to {1.5 \u00d7 105, 6 \u00d7 105, 9 \u00d7 105} for repetition depths of 1, 2, 4 and 6. The number of units in each level of repetition is used as an activation function for the nonlinear layers. A maximum of 1000 epochs with stop after 100 epochs without improvement are specified. Random search is performed by using the initial transformation preset of preconceptions of {0, \u2212 1, \u2212 2, \u2212 3}."}], "references": [{"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "author": ["Monica Bianchini", "Franco Scarselli"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "The utility driven dynamic error propagation network", "author": ["A.J. Robinson", "F. Fallside"], "venue": "Technical Report CUED/F-INFENG/TR.1,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1987}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Paul J Werbos"], "venue": "Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Complexity of exact gradient computation algorithms for recurrent neural networks", "author": ["R.J. Williams"], "venue": "Technical Report NU-CCS-89-27,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "ArXiv e-prints,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "The representation of the cumulative rounding error of an algorithm as a taylor expansion of the local rounding errors", "author": ["S. Linnainmaa"], "venue": "Master\u2019s thesis, Univ. Helsinki,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1970}, {"title": "Taylor expansion of the accumulated rounding error", "author": ["Seppo Linnainmaa"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1976}, {"title": "System Modeling and Optimization: Proceedings of the 10th IFIP Conference New York City, USA, August 31 \u2013 September 4, 1981, chapter Applications of advances in nonlinear sensitivity analysis, pages 762\u2013770", "author": ["Paul J. Werbos"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1982}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["S Hochreiter"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1991}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ArXiv e-prints,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Highway long short-term memory RNNS for distant speech recognition", "author": ["Yu Zhang", "Guoguo Chen", "Dong Yu", "Kaisheng Yaco", "Sanjeev Khudanpur", "James Glass"], "venue": "In 2016 IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Adaptive Computation Time for Recurrent Neural Networks", "author": ["A. Graves"], "venue": "ArXiv e-prints,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Reinforcement learning in markovian and non-markovian environments", "author": ["J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems 3. Morgan-Kaufmann,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1991}, {"title": "Sequence labelling in structured domains with hierarchical recurrent neural networks", "author": ["S. Fernandez", "A. Graves", "J. Schmidhuber"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "\u00dcber die Abgrenzung der Eigenwerte einer Matrix", "author": ["S. Ger\u0161gorin"], "venue": "Bulletin de l\u2019Acade\u0300mie des Sciences de l\u2019URSS. Classe des sciences mathe\u0300matiques,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1931}, {"title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Training very deep networks", "author": ["Rupesh K Srivastava", "Klaus Greff", "Juergen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "LSTM: A Search Space Odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u00edk", "B. R Steunebrink", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "The human knowledge compression", "author": ["M. Hutter"], "venue": "contest. http://prize.hutter1.net/,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "ArXiv e-prints,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Comput. Linguist.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1993}, {"title": "Brainstorm: Fast, Flexible and Fun Neural Networks, Version", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Recurrent Neural Network Regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "ArXiv e-prints,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Network depth is of central importance in the resurgence of neural networks as a powerful machine learning paradigm [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "[2] and references therein).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "However, certain internal function mappings in RNNs usually do not take advantage of depth [6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "Unfortunately, increased depth represents a challenge when neural network parameters are optimized by means of error backpropagation [7\u20139].", "startOffset": 133, "endOffset": 138}, {"referenceID": 7, "context": "Unfortunately, increased depth represents a challenge when neural network parameters are optimized by means of error backpropagation [7\u20139].", "startOffset": 133, "endOffset": 138}, {"referenceID": 8, "context": "Unfortunately, increased depth represents a challenge when neural network parameters are optimized by means of error backpropagation [7\u20139].", "startOffset": 133, "endOffset": 138}, {"referenceID": 9, "context": "Deep networks suffer from what are commonly referred to as the vanishing and exploding gradient problems [10\u201312], since the magnitude of the gradients may shrink or explode exponentially during backpropagation.", "startOffset": 105, "endOffset": 112}, {"referenceID": 10, "context": "Deep networks suffer from what are commonly referred to as the vanishing and exploding gradient problems [10\u201312], since the magnitude of the gradients may shrink or explode exponentially during backpropagation.", "startOffset": 105, "endOffset": 112}, {"referenceID": 11, "context": "Deep networks suffer from what are commonly referred to as the vanishing and exploding gradient problems [10\u201312], since the magnitude of the gradients may shrink or explode exponentially during backpropagation.", "startOffset": 105, "endOffset": 112}, {"referenceID": 14, "context": "These layers have been used to improve performance in speech recognition [16] and language modeling [17], and a variant of Highway networks called Residual networks is currently the state-of-the-art model for image recognition [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "These layers have been used to improve performance in speech recognition [16] and language modeling [17], and a variant of Highway networks called Residual networks is currently the state-of-the-art model for image recognition [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "These layers have been used to improve performance in speech recognition [16] and language modeling [17], and a variant of Highway networks called Residual networks is currently the state-of-the-art model for image recognition [18].", "startOffset": 227, "endOffset": 231}, {"referenceID": 5, "context": "[6] introduced Deep Transition RNNs (DT-RNNs) and Deep Transition RNNs with Skip connections (DT(S)-RNNs).", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "A known way of creating large step-by-step depth is to let an RNN tick for several \"micro time steps\u201d per step of the sequence [19\u201321], where the RNN could be an LSTM with forget gates.", "startOffset": 127, "endOffset": 134}, {"referenceID": 18, "context": "A known way of creating large step-by-step depth is to let an RNN tick for several \"micro time steps\u201d per step of the sequence [19\u201321], where the RNN could be an LSTM with forget gates.", "startOffset": 127, "endOffset": 134}, {"referenceID": 19, "context": "Recurrent Highway Networks enable a powerful alternative method to make recurrent networks more expressive and complement other approaches such as stacked LSTM layers [22].", "startOffset": 167, "endOffset": 171}, {"referenceID": 9, "context": "We can now obtain conditions for the gradients to vanish [10\u201312].", "startOffset": 57, "endOffset": 64}, {"referenceID": 10, "context": "We can now obtain conditions for the gradients to vanish [10\u201312].", "startOffset": 57, "endOffset": 64}, {"referenceID": 11, "context": "We can now obtain conditions for the gradients to vanish [10\u201312].", "startOffset": 57, "endOffset": 64}, {"referenceID": 20, "context": "Ger\u0161gorin circle theorem (GCT) [23]: For any square matrix A \u2208 Rn\u00d7n,", "startOffset": 31, "endOffset": 35}, {"referenceID": 21, "context": "[24] proposed to initialize R with identity and small random values on the off-diagonals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Highway layers [25] enable easy training of very deep feedforward networks through the use of adaptive computation.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "Similar to other variants such as those studied by [26], it retains the essential components of the LSTM \u2013 multiplicative gating units controlling the flow of information through additive cells.", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "This flexibility is the likely reason behind the performance improvement from Highway layers even in cases where network depth is not high [17].", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "We compare optimization of RHNs to DT-RNNs and DT(S)-RNNs [6].", "startOffset": 58, "endOffset": 61}, {"referenceID": 24, "context": "Networks with recurrence depth of 1, 2, 4 and 6 are trained for next step prediction on the JSB Chorales polyphonic music prediction dataset [27].", "startOffset": 141, "endOffset": 145}, {"referenceID": 22, "context": "These results are similar to those obtained in an optimization study on feedforward Highway networks [25].", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "Wikipedia: We train an RHN with recurrence depth of 5 and 864 units on the challenging Hutter Prize Wikipedia dataset (enwik8) [28].", "startOffset": 127, "endOffset": 131}, {"referenceID": 26, "context": "Training is performed using SGD with momentum and weight noise as used by Graves [29].", "startOffset": 81, "endOffset": 85}, {"referenceID": 27, "context": "Penn Treebank: To examine the effect of recurrence depth we trained RHNs with one RHN layer and fixed total parameters (10 M) but with recurrence depths of 1, 2 and 3 for word level language modeling on the Penn TreeBank dataset [30].", "startOffset": 229, "endOffset": 233}, {"referenceID": 22, "context": "[25].", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with such \u201cdeep\" transition functions remain difficult to train, even when using Long Short-Term Memory networks. We introduce a novel theoretical analysis of recurrent networks based on Ger\u0161gorin\u2019s circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks (RHN), which are long not only in time but also in space, generalizing LSTMs to larger step-to-step depths. Experiments indicate that the proposed architecture results in complex but efficient models, beating previous models for character prediction on the Hutter Prize dataset with less than half of the parameters.", "creator": "LaTeX with hyperref package"}}}