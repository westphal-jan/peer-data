{"id": "1607.03542", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jul-2016", "title": "Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge", "abstract": "Semantic parsers map language onto executable statements in a fixed schema. This mapping allows them to effectively leverage the information contained in large, formal knowledge bases (e.g., Freebase) to answer questions, but it is also fundamentally limiting---semantic parsers can only represent language that falls within their manually produced schema. Recently proposed methods for open vocabulary semantic parsing overcome this limitation by learning execution models for arbitrary language. However, all prior approaches to open vocabulary semantic parsing are purely distributional, making no use of any underlying knowledge base. We show how to combine the benefits of both of these approaches by incorporating knowledge base information into open vocabulary semantic parsing models, improving mean average precision on an open-domain natural language query task by more than 120%.", "histories": [["v1", "Tue, 12 Jul 2016 23:13:26 GMT  (29kb)", "http://arxiv.org/abs/1607.03542v1", null], ["v2", "Mon, 28 Nov 2016 21:44:30 GMT  (27kb)", "http://arxiv.org/abs/1607.03542v2", "Re-written abstract and intro, other minor changes throughout. This version published at AAAI 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["matt gardner", "jayant krishnamurthy"], "accepted": true, "id": "1607.03542"}, "pdf": {"name": "1607.03542.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mattg@allenai.org", "jayantk@allenai.org"], "sections": [{"heading": null, "text": "ar Xiv: 160 7.03 542v 1 [cs.C L] 12 Ju"}, {"heading": "1 Introduction", "text": "It is the task to project a phrase in natural language onto formal statements in a particular scheme, which can then be executed against a database (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005). For example, the phrase \"Who is the President of the United States?\" could be applied to the formal statement \"Who is the President?\" to the formal statement \"Who is the President?\" to the formal statement \"Who is the President?\" to the formal statement \"Who is the President?\" to the formal statement \"Who is the President?\" to the formal statement \"Who is the President?\" to the formal statement \"Who is the President?\" to the formal statement \"Who is the President?\" to the formal statement \"Who is the President?\" to the formal statement. \""}, {"heading": "2 Open vocabulary semantic parsing", "text": "In this section, we briefly describe the current open semantic parsing models according to Krishnamurthy and Mitchel (2015), which introduced a technique for learning semantic parsers with open predicate vocabulary. Instead of mapping text to freebase queries, their method analyzes text in a superficial logical form whose predicates are derived directly from the words in the text (see Figure 1). Next, a distribution of the denotations for each predicate is learned using a matrix factoring approach similar to that of Riedel et al. (2013). This distribution is concisely represented by a probability database that also enables efficient probability execution of logical form queries. Matrix factorization has two sets of parameters: Each category or relationship has a learned cdimensional embedding, and each entity or entity pair has a learned k-dimensional embedding capability."}, {"heading": "3 Converting Freebase queries to features", "text": "The most important finding of this work is that we can convert the executable queries used by traditional semantic parsers into traits that provide KB information for the learned execution models of open semantic vocabulary parsers. Here, we describe how this happens. Traditional semantic parsers arrange words on distributions of executable queries and then select one to actually execute them, returning sets of units or pairs of entities from a knowledge base as a result. 1Note that this is implicitly factorizing a co-event matrix between predicates and entities (Levy and Goldberg, 2014). There are two problems with this approach: (1) the set of all possible queries is inseparable from any lexicon, so we need a small lexicon to execute a small mechanism for each word."}, {"heading": "3.1 Subgraph feature extraction", "text": "SFE is a technique for generating attribute matrices by node pairs in a graph with labeled edges. If the graph corresponds to a formal KB such as Freebase, the attribute matrices generated by SFE are isomorphic compared to statements in the KB scheme (Gardner, 2015). This means that we can use SFE to create a attribute vector for each unit (and entity pair) that concisely captures the set of statements 2 in whose denotations the entity (or entity pair) appears. Using this attribute vector as part of the entity model of the semantic parser solves the above problem (2) and performing a attribute selection via these attribute vectors for each predicate problem (1). Some sample attribute extracted by SFE are shown in Figure 2. For entity pairs, these attribute sequences contain the attribute (s) corresponding to the edges (s)."}, {"heading": "3.2 Feature selection", "text": "The feature vectors produced by SFE contain tens of millions of potential formal statements. Of these tens of millions of formal statements, only a handful represent relevant freebase queries for a particular predicate. Therefore, we select a small number of 2In a restricted class that contains horn clauses and a few other things; see Gardner (2015) for more details. PALLADIO ITALYARCHITECT COUNTRYVILLA CAPRANATIONALITYT Y Y P ELO CATE DINFeatures between PALLADIO and ITALY: ITALITALY: < NATIONALITY COUNT > < DESIGNED < DESIGNED < DNEESIGNED >; NATIONALITY DINITALITY >: ITALITY < TYPE >: ARCHITECT < DESIGNED < DNEESIGNED >; DNEESIGNED < NATIONALITY DINITALITY >: ITALITY < TYPE >: ARCHITECT < DESIGNED."}, {"heading": "4 Combined predicate models", "text": "After describing how we use SFE to generate features that correspond to the statements of a formal scheme, adding these features to the models described in Section 2 is straightforward. We saw in Section 2 that open vocabulary semantic parser learns distribution vectors for each category, relationship, entity and entity pair. We extend these vectors by the ones in Section 3. Each category and relationship is given a weighting for each selected freebase query, and each entity and entity pair has a corresponding feature vector. The truth probability of a category instance c (e) or relation instance r (e1, e2) is thus expressed by: p (e) = unit (frequency asenquery) = unit c (e)) and entity c (e))) p (r (e1, e2)) = predisposition T (e2) + predisposition (2) and (2)."}, {"heading": "5 Making full use of KB information", "text": "In addition to improving predicate models, as just described, the addition of information to the knowledge base to open semantic vocabulary parsers suggests two other simple improvements: (1) the use of more specific logical forms and (2) the generation of candidate units from the knowledge base."}, {"heading": "5.1 Logical form generation", "text": "Krishnamurthy and Mitchell (2015), whose work on open semantic vocabulary we follow in this paper, generate logical forms from natural language statements by calculating a syntactic CCG parse and then applying a set of rules to generate logical forms. However, their logical form analyses do not model the relationship between noun and noun well. In this case, the N / N predicate is a generic noun-modifier relationship; however, this relationship is too vague for the predicate model to learn its precise denotation. A similar problem occurs with prepositions and possessives, for example, it is similarly difficult to learn the denotation of the predicate. Our system improves the analysis of non-mediated relationships by simply including the predicate word in the predicate name."}, {"heading": "5.2 Candidate entity generation", "text": "A major advantage of our predicate models is that they are able to assign values to entity pairs that were never seen in the training data. Distribution models have no basis for assigning these values and therefore assume p (r (e1, e2) = 0 for invisible entity pairs (e1, e2).This assumption limits the retrieval of these models when applied to answering questions, since entity pairs have not been observed for many correct entities but for rare entities. However, since our models have access to a large knowledge base, the formal component of the model can always give a score to each entity pair in the knowledge base. This advantage allows our model to considerably improve the answering of questions in rare entities. However, this advantage has a related disadvantage: in view of a logical form query, it is mathematically and statistically undesirable that all entities are considered as entities, entities."}, {"heading": "6 Evaluation", "text": "Each test example is an open vocabulary phrase that contains at least two freebase units, one of which is held up. The system must suggest a ranking of freebase units to fill the void left by the unit at hand, and the predicted units are then manually assessed for accuracy. We compare our proposed models, which combine distribution and formula elements, with a purely distribution-based baseline from the previous work. All data and codes used in these experiments are available at [url withheld for review]. 3Freebase mediators are used to capture relationships with more than two arguments, such as the employment relationship that has one employer and one employee, a start date and an end date."}, {"heading": "6.1 Data", "text": "We use the data set introduced by Krishnamurthy and Mitchell (2015), which consists of the ClueWeb09 webcorpus4 together with Google's FACC unit that links this corpus to Freebase (Gabrilovich et al., 2013). For training data, 3 million websites from this corpus were processed with a CCG parser to generate logical forms (Krishnamurthy and Mitchell, 2014), which produced 2.1 million predicate cases with 142k entity pairs and 184k entities. After rare predicates were removed (which were seen less than 6 times), there were 25k categories and 4.2k relations. 5We also used the test set created by Krishnamurthy and Mitchell, which contains 220 queries generated in the same way as the training data from a separate section of ClueWeb. However, since they did not publish a development set with their data, we used this set as a Webset, which we put together as a final assessment of the ClueWeb set, and for both of us, we used the same development results."}, {"heading": "6.2 Models", "text": "In our experiments, we compare three models: (1) the distribution model described in section 2; (2) a formal model (new in this paper) in which the distribution parameters \u03b8 and \u03c6 are set to zero in section 4; and (3) the combined model described in section 4 (also new in this paper). Except for the exceptions, all experiments use our modified logical forms (section 5.1) and our mechanism for entity suggestions (section 5.2)."}, {"heading": "6.3 Methodology", "text": "To compare the results of the systems, we follow a pooled evaluation protocol commonly used in terms of extraction and information retrieval (West et al., 2014; Riedel et al., 2013).4http: / / www.lemuproject.org / clueweb09.php 5The differences in the numbers reported here compared to those reported by Krishnamurthy and Mitchell (2015) are due to our improved logical form generation discussed in Section 5.1. We take the 30 best predictions from each system and manually comment on whether they are correct, and use these annotations to calculate the average precision (AP) and reprocedural rank (RR) of each system in the query. Average precision is presented as 1 m \u00b2 m \u00b2 m = 1 Prec (k) \u00d7 Correct (k), with Prec (prognostics) being the precision at the point where AP (RAP) is the precision (RAP) at the number of the response (MAP)."}, {"heading": "6.4 Results", "text": "As can be seen in Table 1, all models with our improved logical forms are better able to grasp the semantics of the language. This improvement is most pronounced in the formal models, which have more capacity to obtain specific characteristics from the freebase with the new logical forms. Since our logical forms are able to give better performance to all models, the remaining experiments we present use all of these logical forms. Next, we will show the improvement achieved by using the simple candidate generation in Section 5.2. Simply appending the list of connected units in the freebase to the bottom of the ranking that the distribution model provides, MAP improves by 40% (see Table 2). The linkage of an entity pair in the freebase is very informative, especially for rare units that are not seen together during the training. Table 3 shows a comparison between the semantic parsing models that we have discussed on the Relative Model, where improvements can be clearly seen in the previous work < as seen in the Combined Model."}, {"heading": "6.5 Discussion", "text": "Our model tends to exceed the distribution model on requests that have predicates with exact or partially correlated in Freebase. For example, our model gets a near-perfect average precision on the queries \"French Newspaper\" and \"Israeli Prime Minister,\" both of which can be expressed accurately in Freebase. The top features for Newspaper (x) all indicate that x has the type NEWSPAPER in Freebase, and the top features for Newspaper N / N (x, y) indicate that y is a newspaper, and that x is either the circulation range of y or the language of y. The model also performs well on queries with partial freebase corrections, such as \"Microsoft Head Honcho,\" \"\" s closest ally \"and\" Patriot Linebacker, \"although with slightly lower average precision. The high weight characteristics in these cases tend to provide useful clues, although there are no direct corrections; for example, the model says,\" people are learning that \"and that they have problems.\""}, {"heading": "7 Related work", "text": "In addition to working on traditional and open vocabulary semantics analyses, which we have already discussed, there are two further classes of work related to what we have presented in this paper. First, the task of integrating a probabilistic database into an open semantic vocabulary parser has a strong link to the task of completing the knowledge base. In addition to SFE (Gardner and Mitchell, 2015), our work draws inspiration from work on embedding units and relationships in a knowledge base (Riedel et al., 2013; Nickel et al., 2011; Bordes et al., 2013; Nickel et al., 2014; Toutanova et al., 2015) and from work on graph-based methods of reasoning with knowledge bases (Lao and Cohen, 2010; Gardner et al., 2014; Neelakantan et al., 2015). In fact, our combination of embedding methods with graph-based methods in this paper is suggestive of how the two could be combined."}, {"heading": "8 Conclusion", "text": "In this paper, we have shown how we can achieve these two benefits by converting queries generated by traditional semantic parsers into traits that can then be used in open semantic vocabulary analysis models. We have also presented a technique to perform this conversion in a way that is scalable using graph-based methods for trait extraction; our combined model achieved relative gains of over 50% in average precision, meaning a reciprocal ranking compared to a purely distributive approach; we have also introduced a better mapping of interface text to logical forms; and a simple method of using a knowledge base to find candidates during inference. Taken together, the methods introduced in this paper improved the average accuracy of our task from.163.370, a relative improvement over previous work."}], "references": [{"title": "Retrofitting word vectors to", "author": ["Noah A. Smith"], "venue": null, "citeRegEx": "Smith.,? \\Q2015\\E", "shortCiteRegEx": "Smith.", "year": 2015}, {"title": "Fast query execution for retrieval models based on path-constrained random walks", "author": ["Lao", "Cohen2010] Ni Lao", "William W Cohen"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Lao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2010}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Combined distributional and logical semantics", "author": ["Lewis", "Steedman2013] Mike Lewis", "Mark Steedman"], "venue": null, "citeRegEx": "Lewis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2013}, {"title": "Compositional vector space models for knowledge base completion", "author": ["Benjamin Roth", "Andrew McCallum"], "venue": "ACL", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Reducing the rank in relational factorization models by including observable patterns", "author": ["Xueyan Jiang", "Volker Tresp"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Nickel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2014}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M Marlin"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Injecting logical background knowledge into embeddings for relation extraction", "author": ["Sameer Singh", "Sebastian Riedel"], "venue": null, "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Symmetric patterns and coordinations: Fast and enhanced representations of verbs and adjectives", "author": ["Roi Reichart", "Ari Rappoport"], "venue": null, "citeRegEx": "Schwartz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schwartz et al\\.", "year": 2015}, {"title": "Observed versus latent features for knowledge base and text inference", "author": ["Toutanova", "Chen2015] Kristina Toutanova", "Danqi Chen"], "venue": "In ACL workshop on on Continuous Vector Space Models and Their Compositionality", "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon"], "venue": null, "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Knowledge base completion via search-based question answering", "author": ["West et al.2014] Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin"], "venue": null, "citeRegEx": "West et al\\.,? \\Q2014\\E", "shortCiteRegEx": "West et al\\.", "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["Zelle", "Mooney1996] John M. Zelle", "Raymond J. Mooney"], "venue": null, "citeRegEx": "Zelle et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Zelle et al\\.", "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Zettlemoyer", "Collins2005] Luke S. Zettlemoyer", "Michael Collins"], "venue": "In UAI", "citeRegEx": "Zettlemoyer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 7, "context": "Next, a distribution over denotations for each predicate is learned using a matrix factorization approach similar to that of Riedel et al. (2013). This distribution is concisely represented using a probabilistic database, which also enables efficient probabilistic", "startOffset": 125, "endOffset": 146}, {"referenceID": 12, "context": "commonly used in relation extraction and information retrieval (West et al., 2014; Riedel et al., 2013).", "startOffset": 63, "endOffset": 103}, {"referenceID": 7, "context": "commonly used in relation extraction and information retrieval (West et al., 2014; Riedel et al., 2013).", "startOffset": 63, "endOffset": 103}, {"referenceID": 7, "context": "In addition to SFE (Gardner and Mitchell, 2015), our work draws inspiration from work on embedding the entities and relations in a knowledge base (Riedel et al., 2013; Nickel et al., 2011; Bordes et al., 2013; Nickel et al., 2014; Toutanova et al., 2015), as well as work on graph-based methods for reasoning with knowledge bases (Lao and Cohen, 2010;", "startOffset": 146, "endOffset": 254}, {"referenceID": 5, "context": "In addition to SFE (Gardner and Mitchell, 2015), our work draws inspiration from work on embedding the entities and relations in a knowledge base (Riedel et al., 2013; Nickel et al., 2011; Bordes et al., 2013; Nickel et al., 2014; Toutanova et al., 2015), as well as work on graph-based methods for reasoning with knowledge bases (Lao and Cohen, 2010;", "startOffset": 146, "endOffset": 254}, {"referenceID": 6, "context": "In addition to SFE (Gardner and Mitchell, 2015), our work draws inspiration from work on embedding the entities and relations in a knowledge base (Riedel et al., 2013; Nickel et al., 2011; Bordes et al., 2013; Nickel et al., 2014; Toutanova et al., 2015), as well as work on graph-based methods for reasoning with knowledge bases (Lao and Cohen, 2010;", "startOffset": 146, "endOffset": 254}, {"referenceID": 10, "context": "In addition to SFE (Gardner and Mitchell, 2015), our work draws inspiration from work on embedding the entities and relations in a knowledge base (Riedel et al., 2013; Nickel et al., 2011; Bordes et al., 2013; Nickel et al., 2014; Toutanova et al., 2015), as well as work on graph-based methods for reasoning with knowledge bases (Lao and Cohen, 2010;", "startOffset": 146, "endOffset": 254}, {"referenceID": 8, "context": "Second, our work is conceptually related to many methods that aim to learn word embeddings that are informed by some kind of external knowledge (Faruqui et al., 2015; Rockt\u00e4schel et al., 2015; Schwartz et al., 2015; Yu and Dredze, 2014).", "startOffset": 144, "endOffset": 236}, {"referenceID": 9, "context": "Second, our work is conceptually related to many methods that aim to learn word embeddings that are informed by some kind of external knowledge (Faruqui et al., 2015; Rockt\u00e4schel et al., 2015; Schwartz et al., 2015; Yu and Dredze, 2014).", "startOffset": 144, "endOffset": 236}], "year": 2017, "abstractText": "Semantic parsers map language onto executable statements in a fixed schema. This mapping allows them to effectively leverage the information contained in large, formal knowledge bases (e.g., Freebase) to answer questions, but it is also fundamentally limiting\u2014semantic parsers can only represent language that falls within their manuallyproduced schema. Recently proposed methods for open vocabulary semantic parsing overcome this limitation by learning execution models for arbitrary language. However, all prior approaches to open vocabulary semantic parsing are purely distributional, making no use of any underlying knowledge base. We show how to combine the benefits of both of these approaches by incorporating knowledge base information into open vocabulary semantic parsing models, improving mean average precision on an open-domain natural language query task by more than 120%.", "creator": "LaTeX with hyperref package"}}}