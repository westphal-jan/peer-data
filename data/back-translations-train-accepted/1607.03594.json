{"id": "1607.03594", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jul-2016", "title": "Estimating Uncertainty Online Against an Adversary", "abstract": "Assessing uncertainty within machine learning systems is an important step towards ensuring their safety and reliability. Existing uncertainty estimation techniques may fail when their modeling assumptions are not met, e.g. when the data distribution differs from the one seen at training time. Here, we propose techniques that assess a classification algorithm's uncertainty via calibrated probabilities (i.e. probabilities that match empirical outcome frequencies in the long run) and which are guaranteed to be reliable (i.e. accurate and calibrated) on out-of-distribution input, including input generated by an adversary. Our methods admit formal bounds that can serve as confidence intervals, and process data in an online manner, which obviates the need for a separate calibration dataset. We establish theoretical guarantees on our methods' accuracies and convergence rates, and we validate them on two real-world problems: question answering and medical diagnosis from genomic data.", "histories": [["v1", "Wed, 13 Jul 2016 05:07:33 GMT  (1202kb,D)", "https://arxiv.org/abs/1607.03594v1", null], ["v2", "Sun, 22 Jan 2017 04:25:30 GMT  (543kb,D)", "http://arxiv.org/abs/1607.03594v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["volodymyr kuleshov", "stefano ermon"], "accepted": true, "id": "1607.03594"}, "pdf": {"name": "1607.03594.pdf", "metadata": {"source": "CRF", "title": "Estimating Uncertainty Online Against an Adversary", "authors": ["Volodymyr Kuleshov", "Stefano Ermon"], "emails": ["kuleshov@cs.stanford.edu", "ermon@cs.stanford.edu"], "sections": [{"heading": "Introduction", "text": "rE \"s rf\u00fc eid rf\u00fc ide rf\u00fc ide rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf"}, {"heading": "Background", "text": "In the following, we will use IE for the indicator function of E, [N] and [N] 0 to denote (respectively) the sets {1, 2,..., N} and {0, 1, 2,..., N}, and \u2206 d for the d-dimensional simplex."}, {"heading": "Learning with Expert Advice", "text": "Learning with expert advice (Cesa-Bianchi and Lugosi 2006) is a special case of the general online optimization framework (Shalev-Shwartz 2007) on which the online calibration algorithms are based. At any time, the forecaster F receives advice from N experts and chooses a distribution wt-N-1. Nature then shows a result yt and F suffers an expected loss of N i = 1 wti '(yt, ait), where \"(yt, ait) is the loss under expert opinion ait. The performance in this setting is measured with two terms of regret. Definition 1. External regret RextT and internal regret RintT are defined as RextT = T-1. (yt, ait) \u2212 min i-y \u2212 min i-y is the performance (N] t-t-p' (yt, ait) p-p's the ability to regret. \u2212 1. Regret is defined as internal regret RextT and internal regret RintT =."}, {"heading": "Calibration in Online Learning", "text": "Intuitively, calibration means that the true and predicted frequencies of an event should match. For example, if an algorithm predicts a 60% chance of rain 100 times in a given year, then we should see rain on about 60 of these 100 days. Formally, F cal is a forecaster making predictions in the quantity {iN | i = 0,..., N}, with 1 / N being the resolution of F cal; consider the quantities \u03c1T (p) = 1 ytIpt = p \u00b2 T = 1 ytIpt = 1 Ipt = p and CpT = N \u00b2 i = 0."}, {"heading": "Online Recalibration", "text": "Unfortunately, the existing online calibration methods are not directly applicable to the real world. On the one hand, they do not take into account covariates available to improve the prediction of yt. As a result, they cannot provide accurate predictions: for example, they would be constantly 0.5 based on a sequence of 01010... formed by alternating 0s and 1s. To correct these shortcomings, we define here a new problem called online calibration, in which the task is to turn a sequence of uncalibrated predictions pFt into predictions pt that are calibrated and almost as accurate as the original pFt. The predictions p F t can come from any existing machine learning system F; our methods treat it as a black box and preserve its advantageous convergence properties. Formally, we define the online calibration task as a generalization of the classical online optimization framework (Shalev-Shwartz 2007; Cesa-Bichi and Lugosi 2006)."}, {"heading": "Algorithms for Online Recalibration", "text": "Next, we propose an algorithm for performing an online probability recalibration; we refer to our approach as a meta-algorithm because it repeatedly calls a regular online calibration algorithm as a black box subroutine. Algorithm 1 outlines this procedure.At a high level, algorithm 1 partitions the uncalibrated forecasts pFt into M-buckets / intervals I = algorithm 1 Online recalibrationRequired: Online calibration subroutine F cal and number of buckets M1: Let I = {[0, 1M], [1 M, 2 M), [M \u2212 1 M, 1]} a series of intervals to be partitioned. 2: Let F = {F calj | j = 0, M} a series of minor instances of F cal."}, {"heading": "Calibration and Accuracy of Online Recalibration", "text": "In this section, we assume that the successive F-calibration is used in the algorithm (je) C (j) T (i / N) T (i / N) T (i / N) T (i) T (i) T (i) T (i) T (j) T (i) T (i) T (i) T (i) T (i) T (i) T (n) T (i) T (i) S (i) T (i) T (i) S (i) T (i) T (i) S (i) S (i) S (i) S (i) S (i) S (n) S) S (i) S (i) S) S (i) S) S (i) S) S (i) S (i) S) S (i) S (i) S (i) S (i) S (S) S (S) S) S (S) S (S) S) S (S) S (S) S (S) S) S (S) S (S) S (S) S) S (S) S (S) S (S) S) (S) S (S) S (S) S (S) S (i) T (i) T (i) T (i) T (i) T (i (i) T (i (i) T (i (i (i) T (i (i) T (i (i) T (i (i) T (i (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) T (i) S (i) S (i (i) T (i (i) T (i) T (i) T (i) S (i) T (i) T (i) T (i (i"}, {"heading": "Experiments", "text": "We start with a simple setting in which we observe an i.i.d. sequence of the yt system, but it is essentially a perfect predictor that is not calibrated. In Figure 2, we compare the performance of pFt) T t = 1, which is equal to 0.3 and 0.7 when we make a perfect prediction. Forecast F is essentially a perfect predictor, but is not calibrated."}, {"heading": "Previous Work", "text": "Such probabilities are achieved by recalibration methods, of which Platt scaling (Platt 1999) and isotonic regression (NiculescuMizil and Caruana 2005) are by far the most popular. Recalibration methods also feature multi-class extensions, typically training multiple one-against-all predictors (Zadrozny and Elkan 2002), as well as extensions of ranking losses (Menon et al. 2012), combinations of estimators (Zhong and Kwok 2013) and structured predictions (Kuleshov and Liang 2015). In the online setting, the calibration problem was formalized by Dawid; online calibration techniques were first proposed by Foster and Vohra. Existing algorithms are based on internal repentance minimization (Cesa-Bianchi and Lugosi 2006) or on Blackwell accessibility (Foster 1997); these approaches were recently revised and re-calibrated."}, {"heading": "Discussion and Conclusion", "text": "This technique divides the pFt into N containers and estimates the average in each bin. By i.i.d. assumption, the output probabilities are calibrated; the sharpness is determined by the width of the bin. Note that the inequality in a given bin is faster than the O (1 / 2) rate of Abernethy, Bartlett, and Hazan at a rate of O (1 / \u221a) (Devroye, Gy\u00f6rfi, and Lugosi 1996), suggesting that calibration is more challenging in the online settings. An alternative way to avoid uninformative predictions (e.g. 0.5 to 010101...) is beyond the scope of rules review (Cesa-Bianchi and Lugosi 2006)."}], "references": [{"title": "and Mannor", "author": ["J.D. Abernethy"], "venue": "S.", "citeRegEx": "Abernethy and Mannor 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "P", "author": ["Abernethy, J.", "Bartlett"], "venue": "L.; and Hazan, E.", "citeRegEx": "Abernethy. Bartlett. and Hazan 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Liang", "author": ["J. Berant"], "venue": "P.", "citeRegEx": "Berant and Liang 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic parsing on freebase from questionanswer pairs", "author": ["Berant"], "venue": null, "citeRegEx": "Berant,? \\Q2013\\E", "shortCiteRegEx": "Berant", "year": 2013}, {"title": "Loss functions for binary class probability estimation and classification: Structure and applications", "author": ["Stuetzle Buja", "A. Shen 2005] Buja", "W. Stuetzle", "Y. Shen"], "venue": null, "citeRegEx": "Buja et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Buja et al\\.", "year": 2005}, {"title": "and Lugosi", "author": ["N. Cesa-Bianchi"], "venue": "G.", "citeRegEx": "Cesa.Bianchi and Lugosi 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "A", "author": ["Dawid"], "venue": "P.", "citeRegEx": "Dawid 1982", "shortCiteRegEx": null, "year": 1982}, {"title": "A probabilistic theory of pattern recognition", "author": ["Gy\u00f6rfi Devroye", "L. Lugosi 1996] Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "Applications of mathematics", "citeRegEx": "Devroye et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 1996}, {"title": "R", "author": ["D.P. Foster", "Vohra"], "venue": "V.", "citeRegEx": "Foster and Vohra 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "D", "author": ["Foster"], "venue": "P.", "citeRegEx": "Foster 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "A", "author": ["T. Gneiting", "F. Balabdaoui", "Raftery"], "venue": "E.", "citeRegEx": "Gneiting. Balabdaoui. and Raftery 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "S", "author": ["E. Hazan", "Kakade"], "venue": "M.", "citeRegEx": "Hazan and Kakade 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Calibrating predictive model estimates to support personalized medicine", "author": ["Jiang"], "venue": "JAMIA", "citeRegEx": "Jiang,? \\Q2012\\E", "shortCiteRegEx": "Jiang", "year": 2012}, {"title": "and Liang", "author": ["V. Kuleshov"], "venue": "P.", "citeRegEx": "Kuleshov and Liang 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Stoltz", "author": ["S. Mannor"], "venue": "G.", "citeRegEx": "Mannor and Stoltz 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "A", "author": ["Menon"], "venue": "K.; Jiang, X.; Vembu, S.; Elkan, C.; and Ohno-Machado, L.", "citeRegEx": "Menon et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "A", "author": ["Murphy"], "venue": "H.", "citeRegEx": "Murphy 1973", "shortCiteRegEx": null, "year": 1973}, {"title": "and O\u2019Connor", "author": ["K. Nguyen"], "venue": "B.", "citeRegEx": "Nguyen and O.Connor 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Caruana", "author": ["A. Niculescu-Mizil"], "venue": "R.", "citeRegEx": "Niculescu.Mizil and Caruana 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "J", "author": ["Platt"], "venue": "C.", "citeRegEx": "Platt 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "G", "author": ["V. Vovk", "A. Takemura", "Shafer"], "venue": "2005. Defensive forecasting. In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, AISTATS 2005, Bridgetown, Barbados, January 6-8,", "citeRegEx": "Vovk. Takemura. and Shafer 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Calibration of confidence measures in speech recognition", "author": ["Li Yu", "D. Deng 2011] Yu", "J. Li", "L. Deng"], "venue": "Trans. Audio, Speech and Lang. Proc. 19(8):2461\u20132473", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}, {"title": "and Elkan", "author": ["B. Zadrozny"], "venue": "C.", "citeRegEx": "Zadrozny and Elkan 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "J", "author": ["L.W. Zhong", "Kwok"], "venue": "T.", "citeRegEx": "Zhong and Kwok 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [], "year": 2017, "abstractText": "Assessing uncertainty is an important step towards ensuring the safety and reliability of machine learning systems. Existing uncertainty estimation techniques may fail when their modeling assumptions are not met, e.g. when the data distribution differs from the one seen at training time. Here, we propose techniques that assess a classification algorithm\u2019s uncertainty via calibrated probabilities (i.e. probabilities that match empirical outcome frequencies in the long run) and which are guaranteed to be reliable (i.e. accurate and calibrated) on out-of-distribution input, including input generated by an adversary. This represents an extension of classical online learning that handles uncertainty in addition to guaranteeing accuracy under adversarial assumptions. We establish formal guarantees for our methods, and we validate them on two real-world problems: question answering and medical diagnosis from genomic data.", "creator": "LaTeX with hyperref package"}}}