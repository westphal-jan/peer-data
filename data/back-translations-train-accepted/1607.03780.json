{"id": "1607.03780", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jul-2016", "title": "A Vector Space for Distributional Semantics for Entailment", "abstract": "Distributional semantics creates vector-space representations that capture many forms of semantic similarity, but their relation to semantic entailment has been less clear. We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment. Using a mean-field approximation, we develop approximate inference procedures and entailment operators over vectors of probabilities of features being known (versus unknown). We use this framework to reinterpret an existing distributional-semantic model (Word2Vec) as approximating an entailment-based model of the distributions of words in contexts, thereby predicting lexical entailment relations. In both unsupervised and semi-supervised experiments on hyponymy detection, we get substantial improvements over previous results.", "histories": [["v1", "Wed, 13 Jul 2016 15:08:26 GMT  (58kb,D)", "http://arxiv.org/abs/1607.03780v1", "To appear in Proc. 54th Annual Meeting of the Association Computational Linguistics (ACL 2016)"]], "COMMENTS": "To appear in Proc. 54th Annual Meeting of the Association Computational Linguistics (ACL 2016)", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["james henderson", "diana nicoleta popa"], "accepted": true, "id": "1607.03780"}, "pdf": {"name": "1607.03780.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Nicoleta Popa"], "emails": ["james.henderson@xrce.xerox.com", "diana.popa@xrce.xerox.com"], "sections": [{"heading": null, "text": "Distribution semantics creates vector space representations that capture many forms of semantic similarity, but their relationship to semantic dissociation is less clear. We propose a vector space model that provides a formal basis for a distributional semantics of dissociation. Using a mid-range approach, we develop approximate inference procedures and dissociation operators over vectors of probabilities of known (versus unknown) characteristics. We use this framework to reinterpret an existing distribution semantics model (Word2Vec) so that it resembles a dissociation model of word distributions in contexts, predicting lexical dissociation relationships. In both unattended and semi-supervised hyponymic experiments, we achieve significant improvements over previous results."}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Modelling Entailment in a Vector Space", "text": "In order to develop a model of disenchantment in a vector space, we must begin with the logical definition of disenchantment in relation to vectors of discrete known characteristics: y contains x if and only if all known characteristics in x are also contained in y. We formalize this relationship with binary vectors x, y where 1 means are known and 0 means are unknown, so that this discrete disenchantment relationship (y) can be defined via these vectors. The exact joint and marginal probability is: P (y).yk Given previous probability distributions P (x), y) via these vectors, the distribution ratios can be defined."}, {"heading": "2.1 A Mean-Field Approximation", "text": "A mean field approximation approaches the trailing P using a factoralized distribution Q. Q = Q (Q = Q) Q (Q = Q | Q (Q = Q) (Q = Q) (Q = Q)) gives us a concise description of the trailing P (x |.) as a vector of the continuous values Q (x = 1), where Q (x = 1) Q (Q = Q (xk = 1) \u2248 EP (x |...) xk = P (xk = 1 |.) (\u2212 \u2212 \u2212 Log of the marginal probabilities of each bit). Secondly, this results in efficient methods for approximate inference of vectors in a model. First, we consider the simple case in which we want to approximate the trailing distribution P (x, y | y \u00b2 \u00b2). In a center approximation, we want to find a factored distribution Q (x, y) that minimizes the KL divergence DKL (x, y)."}, {"heading": "2.2 Inference in Entailment Graphs", "text": "In this section, we will generalize the approach outlined above so that this approach can also be factoralized. Instead, we will assume that the previous P (x) itself is a graphical model that can be approximated using an approach from the center field. In view of a number of variables representing both variables, we will assume that the approach of the approach of the approach of the approach of the approach of the approach of each variable xik, a set of approaches of the approach of the approach of the approach of the xik, a set of approaches of the approach of the approach of the approach xik, a set of approaches of the approach of the approach of the approach xik, a set of approaches of the approach of the approach of the approach of the approach xik, a set of approaches of the approach of the approach of the approach of the series xik, a set of steps of the approach of the approach of the approach of the approach of the approach xik, a set of steps of the approach of the approach of the approach of the series Xi, a set of steps of the series of the series of the series of the series Xi."}, {"heading": "3 Interpreting Word2Vec Vectors", "text": "In order to assess how well the proposed framework provides a formal basis for the distributional semantics of conditionality, we use it to reinterpret an1It. It is interesting to note that \u2212 log \u03c3 (\u2212 Xj) is a nonnegative transformation of Xj, similar to the ReLU nonlinearity that is popular in deep neural networks (Glorot et al., 2011). Logbook \u03c3 (Xj) is the analog non-positive transformation. Existing model of distributional semantics in relation to semantic conditionality. There has been a lot of work on how to use the distribution of contexts in which a word occurs to induce a vector representation of the semantics of words. In this paper, we use this earlier work in relation to distributional semantics by reinterpreting a preceding semantic model and using this understanding to map its vector space-word embedding within the framework proposed."}, {"heading": "Y = \u03b8c \u2212 log \u03c3(\u2212Xc)\u2212 log \u03c3(\u2212Xm)", "text": "= X'c \u2212 log \u03c3 (\u2212 Xm) Since the unification of Y of the context and middle word characteristics of Y > Y = is negated, we use the backward inference operator > \u00a9 to calculate how successful this unification was. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 This gives us the final evaluation: logP (y, y \u21d2 xm, y \u21d2 xc) \u2248 Y > \u00a9 Xm + Y > Xc + \u2212 \u03c3 (\u2212 Y) \u2212 \u2212 Xm + \u2212 cThis is a natural interpretation, but ignores the equivalence in Word2Vec between pairs of positive values and pairs of negative values due to the use of the Dot product. \u2212 As a more accurate interpretation, we interpret each Word2Vec dimension as indicating whether its characteristic is known to be true or false. Transferring this Word2Vector into a vector in our entailment vector space, we get a copy of the Y + vector that is known to be true."}, {"heading": "4 Related Work", "text": "In fact, it is such that most of them are able to surpass themselves, both in terms of the way in which they move and in terms of the way in which they move, as well as in terms of the way in which they move, as well as in terms of the way in which they move, in the way in which they move, in the way in which they move, in the way in which they move, in the way in which they move themselves, in the way in which they move, in the way in which they live, in which they live the world and in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they, in"}, {"heading": "5 Evaluation", "text": "To assess whether the proposed framework is an effective model for entanglement in vector spaces, we apply the interpretations from Section 3 to publicly available word embedding and use them to predict the hyponymic relationships between words in a benchmark dataset. This framework predicts that the more accurate interpretations of Word2Vec will lead to more precise, uncontrolled models of hyponymy. We evaluate the recognition of hyponymic relationships between words, because hyponymy is the canonical type of lexical entanglement; most semantic characteristics of a hypernym (e.g., \"animal\") must be included in the semantic characteristics of the hyponym (e.g., \"dog\")."}, {"heading": "5.1 Hyponymy with Word2Vec Vectors", "text": "For our evaluation on hyponymy, each word is removed from the test set, so we always use the common learning space of Weeds et al. (2014), using their selection of word pairs from the BLESS dataset (Baroni and Lenci, 2011).32https: / / github.com / SussexCompSem / learninghypernyms3Of the 1667 word pairs in these data, 24 were removed because we do not have embedding for any of the words. These word pairs include positive hyponymy pairs, plus negative pairs consisting of some other hyponymy pairs, some pairs in other semantic relationships, and some random pairs. Their selection is balanced between positive and negative examples, so that accuracy can be used as a performance metric. For their semi-supervised experiments, tenfold cross-validation is used, where one word is removed from the test set for each test set."}, {"heading": "5.2 Unsupervised Hyponymy Detection", "text": "The proposed models are compared to the Dot product because it is the standard vector space operator and has been shown to capture semantic similarity very well. However, since the Dot product is a symmetrical operator, it always performs a random direction classification. Another vector space operator that has received a lot of attention recently are vector differences, which are used (with vector sum) to perform semantic transformations, such as \"king - male + female = queen,\" and were previously used for hyponymy modeling (Vylomova et al., 2015; Weeds et al., 2014). For our purposes, we sum up the paired differences to obtain a score that we use for hyponymy detection."}, {"heading": "5.3 Semi-supervised Hyponymy Detection", "text": "Since unattended learning of word embedding can reflect many context-word correlations that have nothing to do with hyponymy, we also consider a semi-supervised setting. Adding some supervisors helps distinguish features that capture semantic properties from other features that are not relevant to hyponymic recognition, but even with supervision, we want the resulting model to be captured in a vector space, rather than in a parameterized scoring function. Therefore, we train mappings from the Word2Vec word vectors for new word vectors, and then apply the endment operators in this new vector space to predict hyponymy maps, rather than in a parameterized scoring function. Since the words in the test set are always detached from the words in the training set, this experiment measures how well the original unattended vector-space captures are generalized, and not the words are generalizations."}, {"heading": "6 Conclusion", "text": "In this paper, we propose a vector space model that forms a formal basis for a distributional semantics of dislocation; we developed a mid-range approach to probabilistic dislocation between vectors representing known versus unknown characteristics; and we used this framework to derive vector operators for dissociation and vector sequence equations for dissociation graphs. This framework allows us to reinterpret Word2Vec to approximate a dissociation-based semantic word model; a key distinction between the semi-supervised models here and many previous work is that it learns a mapping into a vector space that represents dissociation, rather than learning a parameterized dissociation classifier."}], "references": [{"title": "How we blessed distributional semantic evaluation", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, GEMS \u201911, pages 1\u201310. Association for Computa-", "citeRegEx": "Baroni and Lenci.,? 2011", "shortCiteRegEx": "Baroni and Lenci.", "year": 2011}, {"title": "Entailment above the word level in distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan."], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics", "citeRegEx": "Baroni et al\\.,? 2012", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman."], "venue": "Journal of the American Society for Information Science, 41(6):391\u2013407.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Learning semantic hierarchies: A continuous vector space approach", "author": ["Ruiji Fu", "Jiang Guo", "Bing Qin", "Wanxiang Che", "Haifeng Wang", "Ting Liu."], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, 23(3):461\u2013471.", "citeRegEx": "Fu et al\\.,? 2015", "shortCiteRegEx": "Fu et al\\.", "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "International Conference on Artificial Intelligence and Statistics, pages 315\u2013323.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Incremental sigmoid belief networks for grammar learning", "author": ["James Henderson", "Ivan Titov."], "venue": "Journal of Machine Learning Research, 11(Dec):3541\u20133570.", "citeRegEx": "Henderson and Titov.,? 2010", "shortCiteRegEx": "Henderson and Titov.", "year": 2010}, {"title": "Directional distributional similarity for lexical inference", "author": ["Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-geffet."], "venue": "Natural Language Engineering, 16(4):359\u2013389.", "citeRegEx": "Kotlerman et al\\.,? 2010", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2010}, {"title": "Deriving boolean structures from distributional vectors", "author": ["Germn Kruszewski", "Denis Paperno", "Marco Baroni."], "venue": "Transactions of the Association for Computational Linguistics, 3:375\u2013388.", "citeRegEx": "Kruszewski et al\\.,? 2015", "shortCiteRegEx": "Kruszewski et al\\.", "year": 2015}, {"title": "Identifying hypernyms in distributional semantic spaces", "author": ["Alessandro Lenci", "Giulia Benotto."], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics, SemEval \u201912, pages 75\u201379. Association for Computational Linguistics.", "citeRegEx": "Lenci and Benotto.,? 2012", "shortCiteRegEx": "Lenci and Benotto.", "year": 2012}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Focused entailment graphs for open ie propositions", "author": ["Omer Levy", "Ido Dagan", "Jacob Goldberger."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages", "citeRegEx": "Levy et al\\.,? 2014", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Automatic retrieval and clustering of similar words", "author": ["Dekang Lin."], "venue": "Proceedings of the 17th International Conference on Computational Linguistics Volume 2, COLING \u201998, pages 768\u2013774. Association for Computational Linguistics.", "citeRegEx": "Lin.,? 1998", "shortCiteRegEx": "Lin.", "year": 1998}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Ad-", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Reading between the lines: Overcoming data sparsity for accurate classification of lexical relationships", "author": ["Silvia Necsulescu", "Sara Mendes", "David Jurgens", "N\u00faria Bel", "Roberto Navigli."], "venue": "Proceedings of the Fourth Joint Conference on Lexical and", "citeRegEx": "Necsulescu et al\\.,? 2015", "shortCiteRegEx": "Necsulescu et al\\.", "year": 2015}, {"title": "Looking for hyponyms in vector space", "author": ["Marek Rei", "Ted Briscoe."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages 68\u201377, Ann Arbor, Michigan. Association for Computational Linguistics.", "citeRegEx": "Rei and Briscoe.,? 2014", "shortCiteRegEx": "Rei and Briscoe.", "year": 2014}, {"title": "Inclusive yet selective: Supervised distributional hypernymy detection", "author": ["Stephen Roller", "Katrin Erk", "Gemma Boleda."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1025\u2013", "citeRegEx": "Roller et al\\.,? 2014", "shortCiteRegEx": "Roller et al\\.", "year": 2014}, {"title": "Chasing hypernyms in vector spaces with entropy", "author": ["Enrico Santus", "Alessandro Lenci", "Qin Lu", "Sabine Schulte im Walde."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Santus et al\\.,? 2014", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Word space", "author": ["Hinrich Sch\u00fctze."], "venue": "Advances in Neural Information Processing Systems 5, pages 895\u2013902. Morgan Kaufmann.", "citeRegEx": "Sch\u00fctze.,? 1993", "shortCiteRegEx": "Sch\u00fctze.", "year": 1993}, {"title": "Experiments with three approaches to recognizing lexical entailment", "author": ["Peter D. Turney", "Saif M. Mohammad."], "venue": "CoRR, abs/1401.8269.", "citeRegEx": "Turney and Mohammad.,? 2014", "shortCiteRegEx": "Turney and Mohammad.", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "J. Artif. Int. Res., 37(1):141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Word representations via Gaussian embedding", "author": ["Luke Vilnis", "Andrew McCallum."], "venue": "Proceedings of the International Conference on Learning Representations 2015 (ICLR).", "citeRegEx": "Vilnis and McCallum.,? 2015", "shortCiteRegEx": "Vilnis and McCallum.", "year": 2015}, {"title": "Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning", "author": ["Ekaterina Vylomova", "Laura Rimell", "Trevor Cohn", "Timothy Baldwin."], "venue": "CoRR 2015.", "citeRegEx": "Vylomova et al\\.,? 2015", "shortCiteRegEx": "Vylomova et al\\.", "year": 2015}, {"title": "A general framework for distributional similarity", "author": ["Julie Weeds", "David Weir."], "venue": "Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201903, pages 81\u2013", "citeRegEx": "Weeds and Weir.,? 2003", "shortCiteRegEx": "Weeds and Weir.", "year": 2003}, {"title": "Characterising measures of lexical distributional similarity", "author": ["Julie Weeds", "David Weir", "Diana McCarthy."], "venue": "Proceedings of the 20th International Conference on Computational Linguistics, COLING \u201904, pages 1015\u20131021. Association for Computa-", "citeRegEx": "Weeds et al\\.,? 2004", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}, {"title": "Learning to distinguish hypernyms and co-hyponyms", "author": ["Julie Weeds", "Daoud Clarke", "Jeremy Reffin", "David Weir", "Bill Keller."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages", "citeRegEx": "Weeds et al\\.,? 2014", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}, {"title": "Learning term embeddings for hypernymy identification", "author": ["Zheng Yu", "Haixun Wang", "Xuemin Lin", "Min Wang."], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015. AAAI Press / International Joint", "citeRegEx": "Yu et al\\.,? 2015", "shortCiteRegEx": "Yu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "There has been a lot of interest in modelling entailment in a vector-space, but most of this work takes an empirical, often ad-hoc, approach to this problem, and achieving good results has been difficult (Levy et al., 2015).", "startOffset": 204, "endOffset": 223}, {"referenceID": 2, "context": "(Deerwester et al., 1990; Sch\u00fctze, 1993; Mikolov et al., 2013a)).", "startOffset": 0, "endOffset": 63}, {"referenceID": 19, "context": "(Deerwester et al., 1990; Sch\u00fctze, 1993; Mikolov et al., 2013a)).", "startOffset": 0, "endOffset": 63}, {"referenceID": 13, "context": "(Deerwester et al., 1990; Sch\u00fctze, 1993; Mikolov et al., 2013a)).", "startOffset": 0, "endOffset": 63}, {"referenceID": 11, "context": "We surmise that this is why distributional semantic models have had difficulty modelling lexical entailment (Levy et al., 2015).", "startOffset": 108, "endOffset": 127}, {"referenceID": 13, "context": "We validate this framework by using it to reinterpret existing Word2Vec (Mikolov et al., 2013a) word embedding vectors as approximating an entailment-based model of the distribution of words in contexts.", "startOffset": 72, "endOffset": 95}, {"referenceID": 5, "context": "Given its dependence on mean-field approximations, it is an empirical question to what extent we should view this model as computing real entailment probabilities and to what extent we should view it as a well-motivated non-linear mapping for which we simply optimise the input-output behaviour (as for neural networks (Henderson and Titov, 2010)).", "startOffset": 319, "endOffset": 346}, {"referenceID": 4, "context": "It is interesting to note that \u2212 log \u03c3(\u2212Xj) is a nonnegative transform of Xj , similar to the ReLU nonlinearity which is popular in deep neural networks (Glorot et al., 2011).", "startOffset": 153, "endOffset": 174}, {"referenceID": 13, "context": "In this section we motivate three different ways to interpret the Word2Vec (Mikolov et al., 2013a; Mikolov et al., 2013b) distributional semantic model as an approximation to an entailment-based model of the semantic relationship between a word and its context.", "startOffset": 75, "endOffset": 121}, {"referenceID": 14, "context": "In this section we motivate three different ways to interpret the Word2Vec (Mikolov et al., 2013a; Mikolov et al., 2013b) distributional semantic model as an approximation to an entailment-based model of the semantic relationship between a word and its context.", "startOffset": 75, "endOffset": 121}, {"referenceID": 27, "context": "(Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)).", "startOffset": 0, "endOffset": 125}, {"referenceID": 15, "context": "(Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)).", "startOffset": 0, "endOffset": 125}, {"referenceID": 23, "context": "(Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)).", "startOffset": 0, "endOffset": 125}, {"referenceID": 26, "context": "(Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)).", "startOffset": 0, "endOffset": 125}, {"referenceID": 3, "context": "(Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)).", "startOffset": 0, "endOffset": 125}, {"referenceID": 16, "context": "(Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)).", "startOffset": 0, "endOffset": 125}, {"referenceID": 12, "context": "symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al.", "startOffset": 24, "endOffset": 35}, {"referenceID": 24, "context": "symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al.", "startOffset": 69, "endOffset": 111}, {"referenceID": 25, "context": "symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al.", "startOffset": 69, "endOffset": 111}, {"referenceID": 6, "context": ", 2004), balAPinc (Kotlerman et al., 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 8, "context": ", 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al.", "startOffset": 15, "endOffset": 40}, {"referenceID": 18, "context": ", 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al., 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al.", "startOffset": 75, "endOffset": 96}, {"referenceID": 3, "context": ", 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)).", "startOffset": 106, "endOffset": 165}, {"referenceID": 17, "context": ", 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)).", "startOffset": 106, "endOffset": 165}, {"referenceID": 1, "context": ", 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)).", "startOffset": 106, "endOffset": 165}, {"referenceID": 26, "context": "(Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics.", "startOffset": 0, "endOffset": 89}, {"referenceID": 23, "context": "(Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics.", "startOffset": 0, "endOffset": 89}, {"referenceID": 20, "context": "(Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics.", "startOffset": 0, "endOffset": 89}, {"referenceID": 10, "context": "(Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics.", "startOffset": 0, "endOffset": 89}, {"referenceID": 24, "context": "For these reasons, in our evaluations we replicate the experimental setup of Weeds et al. (2014), for both unsupervised and semi-supervised models.", "startOffset": 77, "endOffset": 97}, {"referenceID": 24, "context": "For these reasons, in our evaluations we replicate the experimental setup of Weeds et al. (2014), for both unsupervised and semi-supervised models. Within this setup, we compare to the results of the models evaluated by Weeds et al. (2014) and to previously proposed vector-space operators.", "startOffset": 77, "endOffset": 240}, {"referenceID": 16, "context": "This includes one vector space operator for hyponymy which doesn\u2019t have trained parameters, proposed by Rei and Briscoe (2014), called weighted cosine.", "startOffset": 104, "endOffset": 127}, {"referenceID": 13, "context": "We base this evaluation on the Word2Vec (Mikolov et al., 2013a; Mikolov et al., 2013b) distributional semantic model and its publicly available word embeddings.", "startOffset": 40, "endOffset": 86}, {"referenceID": 14, "context": "We base this evaluation on the Word2Vec (Mikolov et al., 2013a; Mikolov et al., 2013b) distributional semantic model and its publicly available word embeddings.", "startOffset": 40, "endOffset": 86}, {"referenceID": 21, "context": "(Turney and Pantel, 2010)).", "startOffset": 0, "endOffset": 25}, {"referenceID": 9, "context": "Levy and Goldberg (2014) showed that it is closely related to the previous PMI-based distributional semantic models (e.", "startOffset": 0, "endOffset": 25}, {"referenceID": 22, "context": "The most similar previous work, in terms of motivation and aims, is that of Vilnis and McCallum (2015). They also model entailment directly using a vector space, without training a classifier.", "startOffset": 76, "endOffset": 103}, {"referenceID": 1, "context": "They evaluate their models on the hyponymy data from (Baroni et al., 2012).", "startOffset": 53, "endOffset": 74}, {"referenceID": 7, "context": "The semi-supervised model of Kruszewski et al. (2015) also models entailment in a vector space, but they use a discrete vector space.", "startOffset": 29, "endOffset": 54}, {"referenceID": 0, "context": "(2014), using their selection of word pairs2 from the BLESS dataset (Baroni and Lenci, 2011).", "startOffset": 68, "endOffset": 92}, {"referenceID": 24, "context": "For our evaluation on hyponymy detection, we replicate the experimental setup of Weeds et al. (2014), using their selection of word pairs2 from the BLESS dataset (Baroni and Lenci, 2011).", "startOffset": 81, "endOffset": 101}, {"referenceID": 25, "context": "We could not replicate the word embeddings used in Weeds et al. (2014), so instead we use publicly available word embeddings.", "startOffset": 51, "endOffset": 71}, {"referenceID": 25, "context": "Table 3: Accuracies on the BLESS data from Weeds et al. (2014), for hyponymy detection (50% Acc) and hyponymy direction classification (Dir Acc), in the unsupervised (upper box) and semisupervised (lower box) experiments.", "startOffset": 43, "endOffset": 63}, {"referenceID": 23, "context": "the best results from Weeds et al. (2014), who try a number of unsupervised and semi-supervised models, and use the same testing methodology and hyponymy data.", "startOffset": 22, "endOffset": 42}, {"referenceID": 23, "context": "the best results from Weeds et al. (2014), who try a number of unsupervised and semi-supervised models, and use the same testing methodology and hyponymy data. However, note that their word embeddings are different. For the semisupervised models, Weeds et al. (2014) trains classifiers, which are potentially more powerful than our linear vector mappings.", "startOffset": 22, "endOffset": 267}, {"referenceID": 12, "context": "(2014) trains classifiers, which are potentially more powerful than our linear vector mappings. We also compare the proposed operators to the dot product (dot),5 vector differences (dif ), and the weighted cosine of Rei and Briscoe (2014) (weighted cos), all computed with the same word embeddings as for the proposed operators.", "startOffset": 72, "endOffset": 239}, {"referenceID": 23, "context": "This is used (with vector sum) to perform semantic transforms, such as \u201cking - male + female = queen\u201d, and has previously been used for modelling hyponymy (Vylomova et al., 2015; Weeds et al., 2014).", "startOffset": 155, "endOffset": 198}, {"referenceID": 26, "context": "This is used (with vector sum) to perform semantic transforms, such as \u201cking - male + female = queen\u201d, and has previously been used for modelling hyponymy (Vylomova et al., 2015; Weeds et al., 2014).", "startOffset": 155, "endOffset": 198}, {"referenceID": 25, "context": "For the unsupervised results in the upper box of table 3, the best unsupervised model of Weeds et al. (2014), and the operators dot, dif and weighted cos all perform similarly on accuracy, as does the log-odds factorised entailment calculation (logodds \u21d2\u0303).", "startOffset": 89, "endOffset": 109}, {"referenceID": 1, "context": "To allow a direct comparison to the model of Vilnis and McCallum (2015), we also evaluated the unsupervised models on the hyponymy data from (Baroni et al., 2012).", "startOffset": 141, "endOffset": 162}, {"referenceID": 21, "context": "To allow a direct comparison to the model of Vilnis and McCallum (2015), we also evaluated the unsupervised models on the hyponymy data from (Baroni et al.", "startOffset": 45, "endOffset": 72}, {"referenceID": 1, "context": "To allow a direct comparison to the model of Vilnis and McCallum (2015), we also evaluated the unsupervised models on the hyponymy data from (Baroni et al., 2012). Our best model achieved 81% average precision on this dataset, non-significantly better than the 80% achieved by the best model of Vilnis and McCallum (2015).", "startOffset": 142, "endOffset": 322}, {"referenceID": 23, "context": "6 Previous work on using vector differences for semi-supervised hyponymy detection has used a linear SVM (Vylomova et al., 2015; Weeds et al., 2014), which is mathematically equivalent to our vector-differences model, except that we use cross entropy loss and they use a large-margin loss and SVM training.", "startOffset": 105, "endOffset": 148}, {"referenceID": 26, "context": "6 Previous work on using vector differences for semi-supervised hyponymy detection has used a linear SVM (Vylomova et al., 2015; Weeds et al., 2014), which is mathematically equivalent to our vector-differences model, except that we use cross entropy loss and they use a large-margin loss and SVM training.", "startOffset": 105, "endOffset": 148}, {"referenceID": 24, "context": "The mapped > \u00a9 model has the best accuracy, followed by the factorised entailment operator mapped \u21d2\u0303 and Weeds et al. (2014). Direction accuracies of all the proposed operators (mapped > \u00a9, mapped \u21d2\u0303, mapped < \u00a9) reach into the 90\u2019s.", "startOffset": 105, "endOffset": 125}], "year": 2016, "abstractText": "Distributional semantics creates vectorspace representations that capture many forms of semantic similarity, but their relation to semantic entailment has been less clear. We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment. Using a mean-field approximation, we develop approximate inference procedures and entailment operators over vectors of probabilities of features being known (versus unknown). We use this framework to reinterpret an existing distributionalsemantic model (Word2Vec) as approximating an entailment-based model of the distributions of words in contexts, thereby predicting lexical entailment relations. In both unsupervised and semi-supervised experiments on hyponymy detection, we get substantial improvements over previous results.", "creator": "TeX"}}}