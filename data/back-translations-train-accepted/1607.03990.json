{"id": "1607.03990", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jul-2016", "title": "Fast Algorithms for Segmented Regression", "abstract": "We study the fixed design segmented regression problem: Given noisy samples from a piecewise linear function $f$, we want to recover $f$ up to a desired accuracy in mean-squared error.", "histories": [["v1", "Thu, 14 Jul 2016 04:52:53 GMT  (606kb,D)", "http://arxiv.org/abs/1607.03990v1", "27 pages, appeared in ICML 2016"]], "COMMENTS": "27 pages, appeared in ICML 2016", "reviews": [], "SUBJECTS": "cs.LG cs.DS math.ST stat.TH", "authors": ["jayadev acharya", "ilias diakonikolas", "jerry li 0001", "ludwig schmidt"], "accepted": true, "id": "1607.03990"}, "pdf": {"name": "1607.03990.pdf", "metadata": {"source": "CRF", "title": "Fast Algorithms for Segmented Regression", "authors": ["Jayadev Acharya", "Ilias Diakonikolas", "Jerry Li"], "emails": ["jayadev@csail.mit.edu", "ilias.d@ed.ac.uk", "jerryzli@csail.mit.edu", "ludwigs@mit.edu"], "sections": [{"heading": null, "text": "Previous rigorous approaches to this problem have been based on Dynamic Programming (DP) and, while sample-efficient, have a square runtime in sample size. As our main contribution, we provide new, near-linear time algorithms for the problem that - although not minimax-optimal - achieve a significantly better sample time trade-off for large data sets compared to the DP approach. Our experimental evaluation shows that our algorithms deliver a convergence rate that is only deactivated by a factor of 2 to 4 compared to the DP approach, while achieving accelerations of three orders of magnitude. ar Xiv: 160 7.03 990v 1 [cs.L G] 14 Ju"}, {"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Preliminaries", "text": "In this thesis we examine the problem of the fixed definition of segmented regression. (1) We assume that we have the vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector"}, {"heading": "2.1 Our Contributions", "text": "Our main contributions are new, fast algorithms to achieve a comparable MSE rate. We now informally state our main results and refer to later sections for more precise theorem.Theorem 2 (1): There is an algorithm scheme in which we have at least 0.99, we have a target number of pieces, and the variance of noise is in time O (nd2) and gives an O (k) -piecewise linear function f, so that with a probability of at least 0.99, we haveMSE (f) \u2264 O (2 krn +). That is, our algorithm runs in time that is almost linear and yet achieves a reasonable rate of convergence. While this rate is asymptotically lower than the rate of dynamic programming, our algorithm is significantly faster than the DP, so we achieve a comparable rate of convergence."}, {"heading": "2.2 Comparison to prior work", "text": "The main advantage of our approach is the effectiveness of our work with statistical guarantees [BP98, YP13], while it relies on the dynamic programming of algorithms to solve the problem of the insufficient number of incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, incomplete, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely, infinitely,"}, {"heading": "2.3 Agnostic guarantees", "text": "We also look at the agnostic model, also known as learning with model misspecification. So far, we have assumed that the basic truth is exactly a piecemeal linear function. In reality, such an idea is probably only an approximation. While the basic truth may come close to a piecemeal linear function, we generally do not believe that it follows exactly a piecemeal linear function. In this case, our goal should be to restore a piecemeal linear function that competes with the best piecemeal linear approach to the basic truth. Formally, we are looking at the following problem. However, we assume that the same generative model as in (1) no longer assumes that f is a piecemeal linear function. Instead, the function f can now be arbitrary. We define OPTk = min g-Lk MSE (g) as an error of the most appropriate k-piecemeal linear function as a guarantee for f, and we let f-piecemeal linear function be that achieves this minimum."}, {"heading": "2.4 Mathematical Preliminaries", "text": "In this section, we note that our analysis works at all levels. (...) We need the following data to define the smallest random variables. (...) We need to define the smallest parameters K so that (...) the smallest random parameters are K, that (...) the smallest random variables are Kp for all p (...).Fact 6 (...), if Y is a random variable with variance 2, then Y 2 \u2212 2 is a centered random variable with a random random generator. (...) We have a random inequality, c.pief.Proposition 5.16 in [Ver10]. Let X1,., XN be centered subexponential random variables, and leave K = maxi."}, {"heading": "2.5 Runtime of linear least squares", "text": "The attraction of linear smallest squares lies not only in their statistical properties, but also in the fact that the estimator can be efficiently calculated. In our algorithm, we refer several times to linear smallest squares as black box subrountins. First, we use the following sentence: Fact 13. Let A-Rn-d be any data matrix and let y be the set of answers. Then, there is an algorithm called LeastSquares (A-y), which expires in time O (nd2) and calculates the smallest squares that fit this data. Faster approximate algorithms have been worked on that would be sufficient for our purposes in theory. These algorithms offer a better dependence on dimension d in exchange for slightly more complicated approximation guarantees and slightly more complicated algorithms that fit this data (for example, see [CW13])."}, {"heading": "3 Finding the least squares estimator via dynamic programming", "text": "In this section, we first present a Dynamic Programming Approach (DP) to piecemeal linear regression. We do not believe these results are new, but to the best of our knowledge, these results appear primarily as folklore in literature. For completeness, we show the fastest DP we know, and we also prove its statistical warranties. This not only serves as a good warm-up for the later, more complex evidence, but we also need a variant of this result in later analyses."}, {"heading": "3.1 The exact DP", "text": "We will first describe the exact dynamic program. It will simply find the k-piecewise linear function that minimizes the sum-squared errors to the data."}, {"heading": "3.2 Error analysis for the exact DP", "text": "We now turn our attention to the learning rate of the exact DP. We show: Theorem 16 > Let \u03b4 > 0, and let fLS be the k-part linear estimator returned by the exact DP. Then, with probability 1 \u2212 \u03b4, we have this k-part linear function. Then, by defining the smallest squares, we match that we match the evidence techniques for the convergence of linear smallest squares as shown in [Rig15]. Callback f is the true k-part linear function. Then, by defining the smallest squares, we match that we match these y-fLS squares, that we match these y-fLS-2 \u2264 y \u2212 fLS-i.n extension y \u2212 fLS-2, we have this y \u2212 fLS-part linear function."}, {"heading": "4 A simple greedy merging algorithm", "text": "In this section we present a novel greedy algorithm that runs much faster than the DP but achieves a slightly lower learning rate. However, we show both theoretically and experimentally that the trade-off between speed and statistical accuracy is significantly better for this algorithm than for the exact DP."}, {"heading": "4.1 The greedy merging algorithm", "text": "The overall structure of the algorithm is quite similar to [ADH + 15], but the merging criterion is different, and as explained above, the guarantees in this paper are insufficient to provide non-trivial learning guarantees for regression. Here, too, our algorithm requires a slightly more complicated algorithm that does not require knowledge of the variance of the i variables, i.e., s2 = E [2i]. In the pseudo code, we provide two additional tuning parameters. This is because our algorithm generally cannot provide a k histogram, but an O (k) histogram that is sufficient for most practical applications."}, {"heading": "J = {I \u2208 I : f has a jump on I} .", "text": "\"We have engaged with the intervals in J that have never been merged, and we let J2 be the remaining intervals, since these intervals have never been merged, the interval contains a point, call it i. Because we can assume that xi 6 = 0, we know that for this one point our estimator f (xi) = yi, as this is clearly the least that fits a linear estimator to a zero point."}, {"heading": "5 A variance-free merging algorithm", "text": "2.Algorithm 2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2 algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2.Algorithm 2."}, {"heading": "5.2 Postprocessing", "text": "One unsatisfactory aspect of BucketGreedyMerge is that it comes to the intermediate points of the break points that I am striving for. (K log-n) I (K log-n) I (K log-n) I (K log-n) I (K log-n) I (K log-n) I (K log-n) I (K) I (K) I (K) I (K) I (K) I (K) I (K) I (K) I (K) I) I (K) I (K) I) I (K) I (K) I (K) I (K) I (K) I (K) I (K) I (K) I (K) I (K) I) I (K) I (K) I (K) I (K) I (K) I (K) I (K) I (K) I (K) I (I) I (K) I (I) I (K) I (I) I (K) I (I) I (K) I (K) I (K) I (K) I (K) I (K) I (I (K) I (K) I (I (K) I (I (K) I (K) I (I (I (I) I (I (I) I (I ((K) I (K) I (I (K) I (I (I ((I) I (K) I ((K) I (I (I (((I) I) I ((I (I (I (((((I)) I (I (I (I (((I) I (I ((I (((I) I (I ((I (((I)) I (I (I (I (I ((I (((I) I (I (I (I ((I (I ((((I))) I (I (I (I (I ((I (I ((((I)))) I (I (I (I (I (I (I (I (I (I (I (I (I (I (I ((((I)))))))) I (I (I (I (I ("}, {"heading": "6 Obtaining agnostic guarantees", "text": "In this section we show how we can show the algorithms in the previous sections. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \u00b7 p \u00b7 p \u00b7 p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p."}, {"heading": "7 Experiments", "text": "This year, we will be able to look for a solution that is capable, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution. \""}, {"heading": "Acknowledgements", "text": "Some of this research was done while Ilias Diakonikolas was a student at the University of Edinburgh, Jerry Li was an intern at Microsoft Research Cambridge (UK) and Ludwig Schmidt attended the EECS department at UC Berkeley. Jayadev Acharya was supported by a scholarship from the MIT-Shell Energy Initiative. Ilias Diakonikolas was partially supported by the EPSRC scholarship EP / L021749 / 1, a Marie Curie Career Integration Grant and a SICSA scholarship. Jerry Li was supported by the NSF scholarship CCF-1217921, the DOE scholarship DE-SC0008923, the NSF CAREER Award CCF-145326 and an NSF Graduate Research Fellowship. Ludwig Schmidt was supported by scholarships from the MIT-Shell Energy Initiative, MADALGO and the Simons Foundation.4As the results are no different qualitatively, we only report the case k = 10."}], "references": [{"title": "Fast and near-optimal algorithms for approximating distributions by histograms", "author": ["J. Acharya", "I. Diakonikolas", "C. Hegde", "J.Z. Li", "L. Schmidt"], "venue": "In PODS,", "citeRegEx": "Acharya et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2015}, {"title": "Sample-optimal density estimation in nearly-linear time", "author": ["J. Acharya", "I. Diakonikolas", "J. Zheng Li", "L Schmidt"], "venue": "CoRR, abs/1506.00671,", "citeRegEx": "Acharya et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2015}, {"title": "Sketching structured matrices for faster nonlinear regression", "author": ["H. Avron", "V. Sindhwani", "D. Woodruff"], "venue": "In NIPS,", "citeRegEx": "Avron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2013}, {"title": "Estimating and testing linear models with multiple structural changes", "author": ["J. Bai", "P. Perron"], "venue": null, "citeRegEx": "Bai and Perron.,? \\Q1998\\E", "shortCiteRegEx": "Bai and Perron.", "year": 1998}, {"title": "On risk bounds in isotonic and other shape restricted regression problems", "author": ["S. Chatterjee", "A. Guntuboyina", "B. Sen"], "venue": "Annals of Statistics, 43(4):1774\u20131800,", "citeRegEx": "Chatterjee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chatterjee et al\\.", "year": 2015}, {"title": "Introduction to Algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": "3rd edition,", "citeRegEx": "Cormen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 2009}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["K.L. Clarkson", "D.P. Woodruff"], "venue": "In STOC,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "On asymptotic distribution theory in segmented regression problems\u2013 identified case", "author": ["P.I. Feder"], "venue": "Annals of Statistics, 3(1):49\u201383,", "citeRegEx": "Feder.,? \\Q1975\\E", "shortCiteRegEx": "Feder.", "year": 1975}, {"title": "Multivariate adaptive regression splines", "author": ["J.H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman.,? \\Q1991\\E", "shortCiteRegEx": "Friedman.", "year": 1991}, {"title": "Fitting segmented polynomial regression models whose join points have to be estimated", "author": ["A.R. Gallant", "Fuller W. A"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gallant and A.,? \\Q1973\\E", "shortCiteRegEx": "Gallant and A.", "year": 1973}, {"title": "Approximation and streaming algorithms for histogram construction problems", "author": ["S. Guha", "N. Koudas", "K. Shim"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "Guha et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Guha et al\\.", "year": 2006}, {"title": "Optimal histograms with quality guarantees", "author": ["H.V. Jagadish", "Nick Koudas", "S. Muthukrishnan", "Viswanath Poosala", "Kenneth C. Sevcik", "Torsten Suel"], "venue": "In VLDB", "citeRegEx": "Jagadish et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jagadish et al\\.", "year": 1998}, {"title": "On statistics, computation and scalability", "author": ["M.I. Jordan"], "venue": "Bernoulli, 19(4):1378\u20131390,", "citeRegEx": "Jordan.,? \\Q2013\\E", "shortCiteRegEx": "Jordan.", "year": 2013}, {"title": "Fast, provable algorithms for isotonic regression in all lp-norms", "author": ["R. Kyng", "A. Rao", "S. Sachdeva"], "venue": "In NIPS,", "citeRegEx": "Kyng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kyng et al\\.", "year": 2015}, {"title": "Inference using shape-restricted regression splines", "author": ["M.C. Meyer"], "venue": "Annals of Applied Statistics, 2(3):1013\u20131033,", "citeRegEx": "Meyer.,? \\Q2008\\E", "shortCiteRegEx": "Meyer.", "year": 2008}, {"title": "Data analysis and regression: a second course in statistics", "author": ["F. Mosteller", "J.W. Tukey"], "venue": null, "citeRegEx": "Mosteller and Tukey.,? \\Q1977\\E", "shortCiteRegEx": "Mosteller and Tukey.", "year": 1977}, {"title": "High dimensional statistics", "author": ["P. Rigollet"], "venue": null, "citeRegEx": "Rigollet.,? \\Q2015\\E", "shortCiteRegEx": "Rigollet.", "year": 2015}, {"title": "Polynomial splines and their tensor products in extended linear modeling: 1994 wald memorial lecture", "author": ["C.J. Stone", "M.H. Hansen", "C. Kooperberg", "Y.K. Truong"], "venue": "Annals of Statistics,", "citeRegEx": "Stone et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Stone et al\\.", "year": 1997}, {"title": "The use of polynomial splines and their tensor products in multivariate function estimation", "author": ["C.J. Stone"], "venue": "Annals of Statistics,", "citeRegEx": "Stone.,? \\Q1994\\E", "shortCiteRegEx": "Stone.", "year": 1994}, {"title": "Introduction to the non-asymptotic analysis of random matrices. Chapter 5 of: Compressed Sensing, Theory and Applications", "author": ["Roman Vershynin"], "venue": "Edited by Y. Eldar and G. Kutyniok", "citeRegEx": "Vershynin.,? \\Q2012\\E", "shortCiteRegEx": "Vershynin.", "year": 2012}, {"title": "Splines in statistics", "author": ["E.J. Wegman", "I.W. Wright"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Wegman and Wright.,? \\Q1983\\E", "shortCiteRegEx": "Wegman and Wright.", "year": 1983}, {"title": "Estimating and testing multiple structural changes in linear models using band spectral regressions", "author": ["Y. Yamamoto", "P. Perron"], "venue": "Econometrics Journal,", "citeRegEx": "Yamamoto and Perron.,? \\Q2013\\E", "shortCiteRegEx": "Yamamoto and Perron.", "year": 2013}], "referenceMentions": [], "year": 2016, "abstractText": "We study the fixed design segmented regression problem: Given noisy samples from a piecewise linear function f , we want to recover f up to a desired accuracy in mean-squared error. Previous rigorous approaches for this problem rely on dynamic programming (DP) and, while sample efficient, have running time quadratic in the sample size. As our main contribution, we provide new sample near-linear time algorithms for the problem that \u2013 while not being minimax optimal \u2013 achieve a significantly better sample-time tradeoff on large datasets compared to the DP approach. Our experimental evaluation shows that, compared with the DP approach, our algorithms provide a convergence rate that is only off by a factor of 2 to 4, while achieving speedups of three orders of magnitude. ar X iv :1 60 7. 03 99 0v 1 [ cs .L G ] 1 4 Ju l 2 01 6", "creator": "LaTeX with hyperref package"}}}