{"id": "1607.04423", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jul-2016", "title": "Attention-over-Attention Neural Networks for Reading Comprehension", "abstract": "Cloze-style queries are representative problems in reading comprehension. Over the past few months, we have seen much progress that utilizing neural network approach to solve Cloze-style questions. In this work, we present a novel model for Cloze-style reading comprehension tasks, called attention-over-attention reader. Our model aims to place another attention mechanism over the document-level attention, and induces \"attended attention\" for final predictions. Unlike the previous works, our neural network model requires less pre-defined hyper-parameters and uses an elegant architecture for modeling. Experimental results show that the proposed attention-over-attention model significantly outperforms various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test datasets.", "histories": [["v1", "Fri, 15 Jul 2016 09:10:11 GMT  (69kb,D)", "http://arxiv.org/abs/1607.04423v1", "8+1 pages. arXiv admin note: substantial text overlap witharXiv:1607.02250"], ["v2", "Mon, 18 Jul 2016 09:46:02 GMT  (68kb,D)", "http://arxiv.org/abs/1607.04423v2", "8+1 pages. some typos and descriptions fixed, closely related toarXiv:1607.02250"], ["v3", "Thu, 4 Aug 2016 06:17:42 GMT  (72kb,D)", "http://arxiv.org/abs/1607.04423v3", "8+1 pages. fixed errors in Fig.1 and Table.3, closely related toarXiv:1607.02250"], ["v4", "Tue, 6 Jun 2017 02:51:54 GMT  (251kb,D)", "http://arxiv.org/abs/1607.04423v4", "8+2 pages. accepted as a conference paper at ACL2017 (long paper)"]], "COMMENTS": "8+1 pages. arXiv admin note: substantial text overlap witharXiv:1607.02250", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["yiming cui", "zhipeng chen", "si wei", "shijin wang", "ting liu", "guoping hu"], "accepted": true, "id": "1607.04423"}, "pdf": {"name": "1607.04423.pdf", "metadata": {"source": "CRF", "title": "Attention-over-Attention Neural Networks for Reading Comprehension", "authors": ["Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu"], "emails": ["ymcui@iflytek.com", "zpchen@iflytek.com", "siwei@iflytek.com", "sjwang3@iflytek.com", "gphu@iflytek.com", "tliu@ir.hit.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Unlike other countries in the world, where most people are able to understand the world and understand what they are doing, we are able to understand the world. Unlike other countries in the world, where people are able to understand the world, we are able to understand the world. Unlike other countries in the world, where people are able to understand the world, we are able to understand and understand the world. Unlike other countries in the world, where people are able to understand the world, we are able to understand the world. Unlike other countries in the world, where people are able to understand and understand the world, we are able to understand the world."}, {"heading": "2 Cloze-style Reading Comprehension", "text": "In this section, we will begin with a brief introduction to the Cloze-style reading comprehension task, followed by a detailed description of some existing public records."}, {"heading": "2.1 Task Description", "text": "Among the various reading comprehension problems, Cloze-style reading comprehension is a primary and representative type of question. The Cloze-style reading comprehension problem (Taylor, 1953) aims to understand the given context or document and then answer the questions based on the nature of the document, while the answer is a single word in the document. Formally, a general Cloze-style reading comprehension problem can be presented as a triple problem: < D, Q, A >, where D is the document, Q is the question, and A is the answer to the question."}, {"heading": "2.2 Existing Public Datasets", "text": "In recent years, it has been shown that the number of people who are able to, are able to, are able to move, are able to, to be able to move, to be able to, to be able to feel, to be able to feel, to be able to feel, to be able to be able, to be able to be able, to be able to feel, to be able to be able, to be able to feel, to be able to feel, to be able to be able, to be able to be able to act, to be able to act, to be able to be able, to be able to be able to be able, to be able to be able to act on their own, and to be able to act, to be able to be able to be able to be able, to be able to be able to be able to be able to be able to act on their own."}, {"heading": "3 Attention-over-Attention Reader", "text": "In this section, we will first introduce our Attention-Over-Attention Reader (AoA Reader), an attention-based neural network model for cloze-style reading comprehension tasks, primarily developed by Kadlec et al., (2016), which aims to evaluate the response directly from the document level rather than calculating blended representations of the document. In this paper, we propose a new work that puts a different focus on primary attention to indicate the \"importance\" of each attention. Now, we will give a formal description of our proposed model. Faced with a series of training sessions triple < D >, A >, the proposed model will be constructed in the following steps."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental Setups", "text": "The dimensions of the embedding and the hidden layer for each task are listed in Table 1. We trained each model for several eras and selected the best one according to the performance of the validation kit. All models are trained on the Tesla K40 GPU. Implementation is done with Theano (Theano Development Team, 2016) and Keras (Chollet, 2015)."}, {"heading": "4.2 Results", "text": "In order to increase the effectiveness of our actions, we must strive to find a solution that meets the needs of our customers."}, {"heading": "5 Related Work", "text": "The results of this study show that most of them are able to develop and develop their skills and abilities, both in terms of the way in which they work and the way in which they work, as well as in terms of the way in which they work. We will conduct a brief review of our work to date and make it possible to build a reliable neural network to examine the relationships within these triples. They have suggested using an attention-based neural network to accomplish this task. Review on CNN / DailyMail datasets has shown that their approach is more effective than traditional baselines.Hill et al (2015) also proposed a similar approach for large-scale training data."}, {"heading": "6 Conclusion", "text": "The proposed Attention-over-Attention model aims to calculate attention not only to the document, but also to the query page, which will benefit from the mutual information. Afterwards, a weighted sum of attention is executed to obtain a visited attention about the document for the final predictions. Among several public records, our model could offer consistent and significant improvements over various state-of-the-art systems by a wide margin. A highlight in our model is that we propose to use the attention mechanism to \"get attention.\" In addition, our model is elegant and easy to perform, but shows promising results in this task. Future work will be carried out in the following aspects: We believe that our model is general and could also be applicable to other tasks, so that we will first thoroughly investigate the use of this architecture in other tasks. Second, we plan to examine hybrid reading comprehension models to address the problems that are based on a set of reasonable sentences."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734. Association for Computational Linguistics.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github.com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Consensus attention-based neural networks for chinese reading comprehension", "author": ["Yiming Cui", "Ting Liu", "Zhipeng Chen", "Shijin Wang", "Guoping Hu."], "venue": "arXiv preprint arXiv:1607.02250.", "citeRegEx": "Cui et al\\.,? 2016", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W Cohen", "Ruslan Salakhutdinov."], "venue": "arXiv preprint arXiv:1606.01549.", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1684\u20131692.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1511.02301.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1603.01547.", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Generating and exploiting large-scale pseudo training data for zero pronoun resolution", "author": ["Ting Liu", "Yiming Cui", "Qingyu Yin", "Shijin Wang", "Weinan Zhang", "Guoping Hu."], "venue": "arXiv preprint arXiv:1606.01603.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "ICML (3), 28:1310\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli."], "venue": "arXiv preprint arXiv:1312.6120.", "citeRegEx": "Saxe et al\\.,? 2013", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Iterative alternating neural attention for machine reading", "author": ["Alessandro Sordoni", "Phillip Bachman", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1606.02245.", "citeRegEx": "Sordoni et al\\.,? 2016", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u2013 1958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["Wilson L Taylor."], "venue": "Journalism and Mass Communication Quarterly, 30(4):415.", "citeRegEx": "Taylor.,? 1953", "shortCiteRegEx": "Taylor.", "year": 1953}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints, abs/1605.02688, May.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Natural language comprehension with the epireader", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1606.02270.", "citeRegEx": "Trischler et al\\.,? 2016", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Advances in Neural Information Processing Systems, pages 2692\u20132700.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Similar to the general reading comprehension problems, the Cloze-style queries (Taylor, 1953) are raised based on the nature of the document, while the answer is a single word inside of the document.", "startOffset": 79, "endOffset": 93}, {"referenceID": 0, "context": "By adopting attention-based neural network approaches (Bahdanau et al., 2014), the machine can learn these patterns in large-scale training data.", "startOffset": 54, "endOffset": 77}, {"referenceID": 4, "context": "Unlike the previous works, that are using heuristic merging functions (Cui et al., 2016), or setting various hyper-parameters (Trischler et al.", "startOffset": 70, "endOffset": 88}, {"referenceID": 17, "context": ", 2016), or setting various hyper-parameters (Trischler et al., 2016), our model could automatically generate an \u201cattended attention\u201d over the various document-level attentions, and make a mutual look not only from query-to-document but also document-to-query, which will benefit from the interactive information.", "startOffset": 45, "endOffset": 69}, {"referenceID": 0, "context": "By adopting attention-based neural network approaches (Bahdanau et al., 2014), the machine can learn these patterns in large-scale training data. To create large-scale training data, Hermann et al. (2015) published the CNN/Daily Mail news corpus for Cloze-style reading comprehensions, where the content is formed by the news articles and its summary.", "startOffset": 55, "endOffset": 205}, {"referenceID": 0, "context": "By adopting attention-based neural network approaches (Bahdanau et al., 2014), the machine can learn these patterns in large-scale training data. To create large-scale training data, Hermann et al. (2015) published the CNN/Daily Mail news corpus for Cloze-style reading comprehensions, where the content is formed by the news articles and its summary. Hill et al. (2015) released the Children\u2019s Book Test (CBT) dataset afterwards, where the training samples are generated through automatic approaches.", "startOffset": 55, "endOffset": 371}, {"referenceID": 0, "context": "By adopting attention-based neural network approaches (Bahdanau et al., 2014), the machine can learn these patterns in large-scale training data. To create large-scale training data, Hermann et al. (2015) published the CNN/Daily Mail news corpus for Cloze-style reading comprehensions, where the content is formed by the news articles and its summary. Hill et al. (2015) released the Children\u2019s Book Test (CBT) dataset afterwards, where the training samples are generated through automatic approaches. Also, Cui et al. (2016) has released the Chinese reading comprehension datasets for future research.", "startOffset": 55, "endOffset": 526}, {"referenceID": 15, "context": "The Cloze-style reading comprehension problem (Taylor, 1953) aims to comprehend the given context or document, and then answer the questions based on the nature of the document, while the answer is a single word in the document.", "startOffset": 46, "endOffset": 60}, {"referenceID": 5, "context": "1 Hermann et al. (2015) have firstly published two datasets: CNN and Daily Mail news data.", "startOffset": 2, "endOffset": 24}, {"referenceID": 1, "context": "But as Chen et al. (2016)\u2019s studies on these datasets showed that the anonymization of the entity word is less useful than expected.", "startOffset": 7, "endOffset": 26}, {"referenceID": 7, "context": "2 There was also a dataset called the Children\u2019s Book Test (CBTest) released by Hill et al. (2015), which is built on the children\u2019s book story through Project Gutenberg.", "startOffset": 80, "endOffset": 99}, {"referenceID": 4, "context": "Some researchers also suggested that by just concatenating the final representations of the query RNN states is not enough for representing the full information of query (Cui et al., 2016).", "startOffset": 170, "endOffset": 188}, {"referenceID": 2, "context": "In our implementation, we utilized the bi-directional Gated Recurrent Unit (GRU) (Cho et al., 2014).", "startOffset": 81, "endOffset": 99}, {"referenceID": 6, "context": "Our model is primarily motivated by Kadlec et al., (2016), which aims to directly estimate the answer from the document-level attention instead of calculating blended representations of the document.", "startOffset": 36, "endOffset": 58}, {"referenceID": 4, "context": "Different from Cui et al. (2016), we use a more \u201cwise\u201d way to combine these document-level attentions into a final attention, while the previous work used naive heuristics, such as summing or averaging over individual attention \u03b1(t).", "startOffset": 15, "endOffset": 33}, {"referenceID": 8, "context": "Following Kadlec et al. (2016), we map the attention results s to the vocabulary space V , and sum the attention value where the candidate word occurs in different places of the document.", "startOffset": 10, "endOffset": 31}, {"referenceID": 8, "context": "Note that, as our model mainly adds limited steps of calculations to the AS Reader (Kadlec et al., 2016) and do not employ any additional weights, the computational complexity is similar to the AS Reader.", "startOffset": 83, "endOffset": 104}, {"referenceID": 14, "context": "1 (Srivastava et al., 2014).", "startOffset": 2, "endOffset": 27}, {"referenceID": 16, "context": "While we were implementing the AS Reader, we observed that it is easy to overfit the training data within two epochs, where a similar conclusion was also made in Trischler et al. (2016). For regularization purpose and handling overfitting problems, we adopted l2-regularization to 0.", "startOffset": 162, "endOffset": 186}, {"referenceID": 12, "context": "\u2022 Hidden Layer: The GRU units are initialized with random orthogonal matrices (Saxe et al., 2013).", "startOffset": 78, "endOffset": 97}, {"referenceID": 11, "context": "To prevent the gradient exploding problem, we set gradient clipping threshold to 5 in our experiments (Pascanu et al., 2013) .", "startOffset": 102, "endOffset": 124}, {"referenceID": 9, "context": "\u2022 Optimization: We used the ADAM update rule (Kingma and Ba, 2014) with an initial learning rate lr = 0.", "startOffset": 45, "endOffset": 66}, {"referenceID": 3, "context": "Implementation is done with Theano (Theano Development Team, 2016) and Keras (Chollet, 2015).", "startOffset": 77, "endOffset": 92}, {"referenceID": 6, "context": "Table 2: Statistics of public Cloze-style reading comprehension datasets: CNN news data (Hermann et al., 2015) and CBTest NE(Named Entites) / CN(Common Nouns) (Hill et al.", "startOffset": 88, "endOffset": 110}, {"referenceID": 7, "context": ", 2015) and CBTest NE(Named Entites) / CN(Common Nouns) (Hill et al., 2015).", "startOffset": 56, "endOffset": 75}, {"referenceID": 6, "context": "Our evaluation is carried out on CNN news datasets (Hermann et al., 2015) and CBTest NE/CN datasets (Hill et al.", "startOffset": 51, "endOffset": 73}, {"referenceID": 7, "context": ", 2015) and CBTest NE/CN datasets (Hill et al., 2015).", "startOffset": 34, "endOffset": 53}, {"referenceID": 5, "context": "We also added pretty new works to compare with (Dhingra et al., 2016; Sordoni et al., 2016; Trischler et al., 2016), which came to our attention when we are writing this paper.", "startOffset": 47, "endOffset": 115}, {"referenceID": 13, "context": "We also added pretty new works to compare with (Dhingra et al., 2016; Sordoni et al., 2016; Trischler et al., 2016), which came to our attention when we are writing this paper.", "startOffset": 47, "endOffset": 115}, {"referenceID": 17, "context": "We also added pretty new works to compare with (Dhingra et al., 2016; Sordoni et al., 2016; Trischler et al., 2016), which came to our attention when we are writing this paper.", "startOffset": 47, "endOffset": 115}, {"referenceID": 6, "context": "Results marked with 1 are taken from (Hermann et al., 2015), and 2 are taken from (Hill et al.", "startOffset": 37, "endOffset": 59}, {"referenceID": 7, "context": ", 2015), and 2 are taken from (Hill et al., 2015), and 3 are taken from (Kadlec et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 8, "context": ", 2015), and 3 are taken from (Kadlec et al., 2016), and 4 are taken from (Cui et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 4, "context": ", 2016), and 4 are taken from (Cui et al., 2016), and 5 are taken from (Chen et al.", "startOffset": 30, "endOffset": 48}, {"referenceID": 1, "context": ", 2016), and 5 are taken from (Chen et al., 2016), and 6 are taken from (Dhingra et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 5, "context": ", 2016), and 6 are taken from (Dhingra et al., 2016), and 7 are taken from (Trischler et al.", "startOffset": 30, "endOffset": 52}, {"referenceID": 17, "context": ", 2016), and 7 are taken from (Trischler et al., 2016), and 8 are taken from (Sordoni et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 13, "context": ", 2016), and 8 are taken from (Sordoni et al., 2016).", "startOffset": 30, "endOffset": 52}, {"referenceID": 1, "context": "Also, the Stanford AR (Chen et al., 2016) and GA Reader (Dhingra et al.", "startOffset": 22, "endOffset": 41}, {"referenceID": 5, "context": ", 2016) and GA Reader (Dhingra et al., 2016) utilized pre-trained word embeddings for initialization, while our model does not adopt any pre-trained model for initialization.", "startOffset": 22, "endOffset": 44}, {"referenceID": 1, "context": "Also, the Stanford AR (Chen et al., 2016) and GA Reader (Dhingra et al., 2016) utilized pre-trained word embeddings for initialization, while our model does not adopt any pre-trained model for initialization. Furthermore, we do not optimize for a certain type of dataset, unlike the Stanford AR only normalized the probabilities over the named entities, rather than all the words in the document, which demonstrate that our model is more general and powerful than previous works. We have also noticed that it is fairly hard for a single model to reach above 75%, as indicated in Chen et al. (2016)\u2019s study showed that the coreference errors (roughly takes up 25%) make the questions \u201cunanswerable\u201d even for the humans.", "startOffset": 23, "endOffset": 598}, {"referenceID": 18, "context": "The proposed model is typically motivated by Pointer Network (Vinyals et al., 2015).", "startOffset": 61, "endOffset": 83}, {"referenceID": 6, "context": "Hermann et al. (2015) have proposed a methodology for obtaining large quantities of \u3008D,Q,A\u3009 triples.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "Hermann et al. (2015) have proposed a methodology for obtaining large quantities of \u3008D,Q,A\u3009 triples. By using their method, a large number of training data can be obtained without much human intervention, and make it possible to train a reliable neural network to study the relationships inside of these triples. They proposed to use an attention-based neural network for tackling this task. Evaluation on CNN/DailyMail datasets showed that their approach is effective than traditional baselines. Hill et al. (2015) also proposed a similar approach for large-scale training data collections for children\u2019s book reading comprehension task.", "startOffset": 0, "endOffset": 516}, {"referenceID": 6, "context": "Hermann et al. (2015) have proposed a methodology for obtaining large quantities of \u3008D,Q,A\u3009 triples. By using their method, a large number of training data can be obtained without much human intervention, and make it possible to train a reliable neural network to study the relationships inside of these triples. They proposed to use an attention-based neural network for tackling this task. Evaluation on CNN/DailyMail datasets showed that their approach is effective than traditional baselines. Hill et al. (2015) also proposed a similar approach for large-scale training data collections for children\u2019s book reading comprehension task. They proposed to use window-based memory network and self-supervision heuristics have been investigated. The results showed that their model had surpassed all other methods in predicting named entities(NE) and common nouns(CN) on both the CBT and the CNN QA benchmark. Kadlec et al. (2016) proposed to use a simple model that using the document-level attention result to directly pick the answer from the document, rather than computing the weighted sum representation of the document like the previous works did.", "startOffset": 0, "endOffset": 929}, {"referenceID": 13, "context": "We also have noticed two very recent works (Sordoni et al., 2016) and (Trischler et al.", "startOffset": 43, "endOffset": 65}, {"referenceID": 17, "context": ", 2016) and (Trischler et al., 2016), during our writing of this article.", "startOffset": 12, "endOffset": 36}, {"referenceID": 13, "context": "(2016) , where the latter model is widely applied to many follow-up works (Sordoni et al., 2016; Trischler et al., 2016; Cui et al., 2016).", "startOffset": 74, "endOffset": 138}, {"referenceID": 17, "context": "(2016) , where the latter model is widely applied to many follow-up works (Sordoni et al., 2016; Trischler et al., 2016; Cui et al., 2016).", "startOffset": 74, "endOffset": 138}, {"referenceID": 4, "context": "(2016) , where the latter model is widely applied to many follow-up works (Sordoni et al., 2016; Trischler et al., 2016; Cui et al., 2016).", "startOffset": 74, "endOffset": 138}, {"referenceID": 8, "context": "Liu et al. (2016) proposed an effective way to generate and exploit large-scale pseudo training data for zero pronoun resolution task.", "startOffset": 0, "endOffset": 18}, {"referenceID": 8, "context": "Liu et al. (2016) proposed an effective way to generate and exploit large-scale pseudo training data for zero pronoun resolution task. The main idea behind their approach is to automatically generate largescale pseudo training data and then using the neural network model to resolve zero pronouns. They also propose a two-step training: a pre-training phase and an adaptation phase, and this can also be applied to other tasks as well. The experimental results on OntoNotes 5.0 corpus is encouraging, and the proposed approach significantly outperforms the state-of-the-art methods. We also have noticed two very recent works (Sordoni et al., 2016) and (Trischler et al., 2016), during our writing of this article. Sordoni et al. (2016) have proposed an iterative alternating attention mechanism and gating strategies to accumulatively optimize the attention after several hops, where the number of hops is defined heuristically.", "startOffset": 0, "endOffset": 737}, {"referenceID": 8, "context": "Liu et al. (2016) proposed an effective way to generate and exploit large-scale pseudo training data for zero pronoun resolution task. The main idea behind their approach is to automatically generate largescale pseudo training data and then using the neural network model to resolve zero pronouns. They also propose a two-step training: a pre-training phase and an adaptation phase, and this can also be applied to other tasks as well. The experimental results on OntoNotes 5.0 corpus is encouraging, and the proposed approach significantly outperforms the state-of-the-art methods. We also have noticed two very recent works (Sordoni et al., 2016) and (Trischler et al., 2016), during our writing of this article. Sordoni et al. (2016) have proposed an iterative alternating attention mechanism and gating strategies to accumulatively optimize the attention after several hops, where the number of hops is defined heuristically. Trischler et al. (2016) adopted a re-ranking strategy into the neural networks and used a joint-training method to optimize the neural network.", "startOffset": 0, "endOffset": 954}, {"referenceID": 4, "context": "Our work is primarily inspired by Cui et al. (2016) and Kadlec et al.", "startOffset": 34, "endOffset": 52}, {"referenceID": 4, "context": "Our work is primarily inspired by Cui et al. (2016) and Kadlec et al. (2016) , where the latter model is widely applied to many follow-up works (Sordoni et al.", "startOffset": 34, "endOffset": 77}, {"referenceID": 4, "context": "Our work is primarily inspired by Cui et al. (2016) and Kadlec et al. (2016) , where the latter model is widely applied to many follow-up works (Sordoni et al., 2016; Trischler et al., 2016; Cui et al., 2016). Unlike the CAS Reader proposed by Cui et al. (2016), we do not assume any heuristics to our model, such as using merge functions: sum, avg etc.", "startOffset": 34, "endOffset": 262}], "year": 2017, "abstractText": "Cloze-style queries are representative problems in reading comprehension. Over the past few months, we have seen much progress that utilizing neural network approach to solve Cloze-style questions. In this work, we present a novel model for Cloze-style reading comprehension tasks, called attention-over-attention reader. Our model aims to place another attention mechanism over the document-level attention, and induces \u201cattended attention\u201d for final predictions. Unlike the previous works, our neural network model requires less pre-defined hyper-parameters and uses an elegant architecture for modeling. Experimental results show that the proposed attentionover-attention model significantly outperforms various state-of-the-art systems by a large margin in public datasets, such as CNN and Children\u2019s Book Test datasets.", "creator": "TeX"}}}