{"id": "1607.04606", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jul-2016", "title": "Enriching Word Vectors with Subword Information", "abstract": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Many popular models to learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for morphologically rich languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skip-gram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram, words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpus quickly. We evaluate the obtained word representations on five different languages, on word similarity and analogy tasks.", "histories": [["v1", "Fri, 15 Jul 2016 18:27:55 GMT  (18kb)", "http://arxiv.org/abs/1607.04606v1", "Submitted to EMNLP 2016"], ["v2", "Mon, 19 Jun 2017 17:41:07 GMT  (150kb,D)", "http://arxiv.org/abs/1607.04606v2", "Accepted to TACL. The two first authors contributed equally"]], "COMMENTS": "Submitted to EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["piotr bojanowski", "edouard grave", "armand joulin", "tomas mikolov"], "accepted": true, "id": "1607.04606"}, "pdf": {"name": "1607.04606.pdf", "metadata": {"source": "CRF", "title": "Enriching Word Vectors with Subword Information", "authors": ["Piotr Bojanowski"], "emails": ["bojanowski@fb.com", "egrave@fb.com", "ajoulin@fb.com", "tmikolov@fb.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 7.04 606v 1 [cs.C L] 15 Ju"}, {"heading": "1 Introduction", "text": "Learning continuous word representation has a long history in the processing of natural language (Rumelhart et al., 1988), and these representations are typically derived from large, unlabeled corpora that use statistics on the occurrence of words (Deerwester et al., 1990; Sch\u00fctze et al., 1992; Lund and Burgess, 1996). In the Neural Networks community, Collobert and Weston (2008) proposed learning word embeddings using a forward component. These authors made an equivalent contribution to the neural network by predicting one word on the left and two words on the right. More recently, Mikolov et al. (2013b) proposed simple logbilinear models to efficiently learn continuous word representation on very large corporations.Most of these techniques represent each word of the vocabulary by distinguishing words on the left and the right."}, {"heading": "2 Related work", "text": "In order to better model rare words, Alexandrescu and Kirchhoff (2006) introduced neural language models in which words are represented as groups of characteristics, which could include morphological information, and this technique has been successfully applied to morphologically rich languages, such as Turk-ish (Sak et al., 2010). Recently, several papers have proposed different compositional functions to derive representations of words from morphemes (Lazaridou et al., 2013; Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014). These different approaches are based on a morphological decomposition of words, whereas ours are not. Similarly, Chen et al. (2015) introduced a method to jointly learn embeddings for Chinese words and characters."}, {"heading": "3 Model", "text": "In this context, it is also possible that in the USA, in Europe and in the USA, the number of people residing in the USA continues to increase. (...) In the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "4 Experiments", "text": "In fact, it is as if most people are able to understand themselves and what they are doing. (...) It is not as if people are able to understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if. (...) It is as if. (...) It is as if. (...) It is as if. (...) It is. (...) It is as if. (...) It is as if. (...) It is."}, {"heading": "5 Discussion", "text": "In this paper, we examine a very simple method of learning word representation with subword information in mind. Our approach, which includes n-grams in the skip gram model, is linked to an old idea (Sagittarius, 1993) that has not received much attention in the last decade. We show that our method surpasses baselines that do not take subword information, rare words, morphologically rich languages, and small training datasets into account. We will open the implementation of our model as open source to facilitate comparison of future work on learning subword information."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Continuous word representations, trained on<lb>large unlabeled corpora are useful for many<lb>natural language processing tasks. Many pop-<lb>ular models to learn such representations ig-<lb>nore the morphology of words, by assigning a<lb>distinct vector to each word. This is a limita-<lb>tion, especially for morphologically rich lan-<lb>guages with large vocabularies and many rare<lb>words. In this paper, we propose a new ap-<lb>proach based on the skip-gram model, where<lb>each word is represented as a bag of charac-<lb>ter n-grams. A vector representation is as-<lb>sociated to each character n-gram, words be-<lb>ing represented as the sum of these represen-<lb>tations. Our method is fast, allowing to train<lb>models on large corpus quickly. We evaluate<lb>the obtained word representations on five dif-<lb>ferent languages, on word similarity and anal-<lb>ogy tasks.", "creator": "LaTeX with hyperref package"}}}