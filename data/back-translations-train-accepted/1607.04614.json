{"id": "1607.04614", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jul-2016", "title": "Guided Policy Search as Approximate Mirror Descent", "abstract": "Guided policy search algorithms can be used to optimize complex nonlinear policies, such as deep neural networks, without directly computing policy gradients in the high-dimensional parameter space. Instead, these methods use supervised learning to train the policy to mimic a \"teacher\" algorithm, such as a trajectory optimizer or a trajectory-centric reinforcement learning method. Guided policy search methods provide asymptotic local convergence guarantees by construction, but it is not clear how much the policy improves within a small, finite number of iterations. We show that guided policy search algorithms can be interpreted as an approximate variant of mirror descent, where the projection onto the constraint manifold is not exact. We derive a new guided policy search algorithm that is simpler and provides appealing improvement and convergence guarantees in simplified convex and linear settings, and show that in the more general nonlinear setting, the error in the projection step can be bounded. We provide empirical results on several simulated robotic navigation and manipulation tasks that show that our method is stable and achieves similar or better performance when compared to prior guided policy search methods, with a simpler formulation and fewer hyperparameters.", "histories": [["v1", "Fri, 15 Jul 2016 18:54:15 GMT  (192kb,D)", "http://arxiv.org/abs/1607.04614v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.RO", "authors": ["william montgomery", "sergey levine"], "accepted": true, "id": "1607.04614"}, "pdf": {"name": "1607.04614.pdf", "metadata": {"source": "CRF", "title": "Guided Policy Search as Approximate Mirror Descent", "authors": ["William Montgomery", "Sergey Levine"], "emails": ["wmonty@cs.washington.edu", "svlevine@cs.washington.edu"], "sections": [{"heading": "1 Introduction", "text": "Political search algorithms based on supervised learning from a computational or human \"teacher\" have become increasingly important in recent years due to their ability to optimize complex strategies for autonomous flying. [16], playing video game games [15, 4], and two-pedal locomotion [11]. Among these methods, guided political search algorithms [6] are particularly attractive due to their ability to adapt the teacher to produce data that is best suited to forming final policies with supervised learning. Such algorithms have been used to train complex in-depth neural network policies for vision-based robotic manipulation [6], as well as a variety of other tasks [19, 11]. However, convergence outcomes for these methods typically follow from their formulation as limited optimization, in which the teacher is gradually forced to adjust the policies learned, and to adjust guarantees to the performance of final policies."}, {"heading": "2 Guided Policy Search Algorithms", "text": "The First Review of Guided Political Search Methods and the Background. Political search algorithms aim to optimize a parameterized political system that guides physical examples [ut | xt] of policies tied to the state. Given stochastic dynamics p (xt + 1 | ut, ut) and costs' (xt, ut), the goal is to minimize the expected costs within the framework of policy course distribution given by J (\u03b8) = p = 1 p (xt + 1 | ut), where we use overloaded methods to name the marginalities of local distribution (xt, ut) to name the marginalities of local learning methods."}, {"heading": "2.1 Efficiently Optimizing Local Policies", "text": "A common and simple decision for local politics pi (ut | xt) is to use time-varying linear-Gaussian strategies (ut | xt) = N (Ktxt + kt, Ct), although other options are also possible [12, 11, 19]. Linear-Gaussian controllers represent individual paths with linear stabilization and Gaussian noise and are convenient in areas where any local politics can be trained from a different (but consistent) starting state xi1p (x1). This is an additional assumption that goes beyond standard RL, but allows an extremely efficient and convenient local model-based RL algorithm based on iterative LQR [9]. The algorithm assumes the generation of N examples for the real physical system from any local politics pi (ut | xt) during the C step, using these samples to adapt to local linear-Gaussian dynamics."}, {"heading": "2.2 Prior Convergence Results", "text": "Prior to working on the guided policy search, convergence through construction typically shows itself by defining the C-step and S-step as block coordinate ascent on the (extended) Lagrangian of the problem in Equation (1) with the surrogate costs for local politics corresponding to the (extended) Lagrangian, and the overall algorithm being an instance of double gradient descent [8], ADMM [12], or Bregman ADMM [6]. Since these methods force the constraint pi (ut | xt) = \u03c0\u03b8 (ut | xt) in convergence (up to linearization or sample error, depending on the method), we know that 1 N-N-N i = 1Epi (xt, ut) ['(xt, ut)] between the anticipation (xt, ut)] and the policy (ut) [' ut, ut) in convergence (xt, ut)."}, {"heading": "3 Mirror Descent Guided Policy Search", "text": "In this section, we propose our new simplified guided policy search, which we call Mirrored Parentage Guided Policy Search (MDGPS). This algorithm uses the limited LQR optimization in Equation (2) to optimize each of the local measures, but instead of restricting each local policy, we set it directly against global policy (ut | xt). The method is summarized in Algorithm 2. In the case of linear dynamics and quadratic costs (i.e. the LQR setting), and assuming that supervised learning can solve a global problem, we can show that this method corresponds to an instance of mirror parentage."}, {"heading": "3.1 Implementation for Nonlinear Global Policies and Unknown Dynamics", "text": "In practice, we aim to optimize complex policies for non-linear systems with unknown dynamics, which requires a few practical considerations. The C-step requires a local quadratic cost function, which we can obtain either through Taylor expansion or through local linear Gaussian Dynamics p (xt + 1 | xt) = N (fxtxt + futut | fct, Ft), which we can obtain either by analytical differentiation of policies, or by applying the same linear regression method that we use to estimate p (xt + 1 | xt), which is the approach in our implementation. In both cases, we get a different global policy linearization around each local policy."}, {"heading": "3.2 Analysis of Prior Guided Policy Search Methods as Approximate Mirror Descent", "text": "The main difference between the proposed method and previous search methods is that the constraint DKL (pi (\u03c4) \u0445 \u03c0 \u03b8i (\u03c4)) \u2264 is enforced on each iteration at the local level, while in previous methods this constraint is enforced iteratively by a dual descendence procedure across multiple iterations. This means that the previous methods perform approximate mirroring with increments that are adapted (by adjusting the Lagrange multipliers) but not exactly limited. In our empirical evaluation, we show that our approach is somewhat more stable, though sometimes slower than these previous methods. This empirical observation agrees with our intuition: earlier methods can sometimes be faster because they do not exactly restrict the step size, but our method is simpler, requires less coordination and always takes limited steps in terms of global policy in the trajectory."}, {"heading": "4 Analysis in the Nonlinear Case", "text": "Although the S-step under non-linear dynamics is not an optimal projection of the multiplicity of constraints, we can tie the additional costs incurred by this projection to the KL divergence between pi (ut | xt) and \u03c0\u03b8 (ut | xt). This analysis also shows why earlier search algorithms, which only have asymptotic convergence guarantees, still achieve good results in practice even after a small number of iterations. We will omit the subscript i in this section for reasons of conciseness, although the same analysis can be repeated for several local policies pi (ut | xt)."}, {"heading": "4.1 Bounding the Global Policy Cost", "text": "???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????)??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4.2 Step Size Selection", "text": "In the previous paper [8], the step size in local policy optimization is adjusted by taking into account the difference between the predicted change in the cost of local policy p (ut | xt) under the adjusted dynamic and the actual cost. Intuition is that, since the linearized dynamic is local, the further we deviate from the previous policy, the higher the cost. \"We can adjust the step size by estimating the rate at which the additional cost occurs and choosing the optimal compromise. Let's call\" k \u2212 1 the expected cost under the previous local policy p (ut | xt), \"kk \u2212 1 the cost under the current local policy p (ut | xt) and the previous adjusted dynamic (using examples from p), and to optimize p (ut | xt) and\" kk the cost of the current local policy p. \""}, {"heading": "5 Relation to Prior Work", "text": "While we have discussed the links between the MDGPS and previous linear search methods, in this section we will also discuss the links between our method and other political search methods. A popular supervised policy learning method is DAGGER [15], which also trains policy makers on the basis of supervised learning, but does not attempt to adapt the teacher to provide better training data. MDGPS reverses the assumption in DAGGER that the supervised learning phase limits errors compared to an arbitrary teacher policy. MDGPS does not need to make this assumption, since the teacher can be adapted to the limits of global political learning. This is especially important when global policy has limits of arithmetic or observation, e.g. when learning to use camera images for partially observed control tasks or, as shown in our assessment, blind insertion into the binding. If we sample (ut | xt) global policy guidelines, our method resembles political gradient methods with KL disparity constraints [14, 13, 17]."}, {"heading": "6 Experimental Evaluation", "text": "We compare several variants of MDGPS and a previous policy search method based on Bregman ADMM (BADMM), physically and effectively. We evaluate all methods based on a simulated robotic navigation task and two manipulation tasks. This strategy requires a consistent implementation of strategies and strategies based on a unified strategy, but which requires all other methods to achieve a goal (shown in green), using velocities and positions relative to the goal. We use N = 5 initial states, with 5 examples per starting state per iteration. The goal and the obstacles are fixed, but the starting position varies. This task, which is more complex, requires controlling a 7 DoF 3D arm to insert a close-fitting level into a hole. The hole can be in different positions, and the state consists of common matters, but the differences in the starting position can be varied. This task is more complex, requiring controlling a 7 DoF hole to insert a 3D hole into one."}, {"heading": "7 Discussion and Future Work", "text": "We presented a new method of guided policy search, which corresponds to the mirror descent under linearity and convexity assumptions, and demonstrated how earlier guided policy search methods can be seen as approximating the mirror descent. We set a limit to the return of global policy in the non-linear case, arguing that an appropriate step size can also lead to an improvement in global policy in this case. Our analysis provides us with the intuition to design an automatic step size adjustment rule, and we empirically illustrate that our method achieves good results in a complex simulated robotic manipulation task, while requiring significantly less tuning and hyperparameter optimization than previous policy search methods. Manual tuning and hyperparameter search are a major challenge for a number of in-depth learning algorithms and the development of scalable policy search methods that are simple and reliable to enable further progress. As discussed in Section 5, MGPS points to interesting links to other policy search methods."}, {"heading": "A KL Divergence Between Gaussian Trajectory Distributions", "text": "In this appendix we derive the KL divergence between two Gaussian trajectory distributions (step) q (step) q (step) (step) q (step) corresponding to temporally variable linear-Gaussian dynamics (with block diagonal covariances) up (point) = p (x1) T divergence down (point) = p (point). We can therefore derive their KL divergence after DKL (point) q (point) q (point))) p (point), q (point) q), q (point), q (point), q (point), q (point), q (point), q (point), q (point), q (point), q (point), q (point), q (point), p (point), p (point), p (point), p (point), p (point)."}, {"heading": "B Details of the MDGPS Algorithm", "text": "A summary of the MDGPS algorithm appears in Algorithm 2, and is repeated below for convenience: Algorithm 4 Mirror Descent Guided Political Search (MDGPS): unknown nonlinear dynamics 1: for iteration k = 1,.., K) do 2: Generate samples Di = {\u03c4i, j} by running either pi-step: Fit linear-Gaussian dynamics pi (xt + 1, ut) using samples in Di 4: Fit linearized global policy. (ut) using samples in Tue 5: C-step: pi \u2190 arg minpi dynamics pi pi (pts) [xt) [xt, ut) such search that DKL (pi). (pi)"}, {"heading": "C Global Policy Cost Bounds", "text": "In this appendix, we demonstrate the limits of the political costs discussed in Section 4.1. The evidence combines the earlier findings of Ross et al. (15) and Schulman et al. (17), and extends them to the case of the finite horizon episode task. (1) The evidence begins with the proof of Lemma 4.1, which we reproduce below in slightly simplified notation, first introducing a problem requiring the introduction of the whole variation maxxt p (ut | xt) q (ut | xt) q (ut | xt) q (ut) xt (xt) q (xt) q (xt) q (xt) q (xt) q (xt) q (xt) 2) xt. The proof first requires the introduction of a problem requiring the whole variation maxxt p (ut) xt (ut | xt) xt (ut | xt) xt (ut | xt) xt (ut | xt) xt (ut | xt) xt between two strategies on the likelihood that the policy will take the same action in a discrete setting (extensions to the continuous setting)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Guided policy search algorithms can be used to optimize complex nonlinear poli-<lb>cies, such as deep neural networks, without directly computing policy gradients<lb>in the high-dimensional parameter space. Instead, these methods use supervised<lb>learning to train the policy to mimic a \u201cteacher\u201d algorithm, such as a trajectory<lb>optimizer or a trajectory-centric reinforcement learning method. Guided policy<lb>search methods provide asymptotic local convergence guarantees by construction,<lb>but it is not clear how much the policy improves within a small, finite number of<lb>iterations. We show that guided policy search algorithms can be interpreted as an<lb>approximate variant of mirror descent, where the projection onto the constraint<lb>manifold is not exact. We derive a new guided policy search algorithm that is sim-<lb>pler and provides appealing improvement and convergence guarantees in simplified<lb>convex and linear settings, and show that in the more general nonlinear setting, the<lb>error in the projection step can be bounded. We provide empirical results on several<lb>simulated robotic navigation and manipulation tasks that show that our method is<lb>stable and achieves similar or better performance when compared to prior guided<lb>policy search methods, with a simpler formulation and fewer hyperparameters.", "creator": "LaTeX with hyperref package"}}}