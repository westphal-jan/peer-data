{"id": "1607.05002", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jul-2016", "title": "Geometric Mean Metric Learning", "abstract": "We revisit the task of learning a Euclidean metric from data. We approach this problem from first principles and formulate it as a surprisingly simple optimization problem. Indeed, our formulation even admits a closed form solution. This solution possesses several very attractive properties: (i) an innate geometric appeal through the Riemannian geometry of positive definite matrices; (ii) ease of interpretability; and (iii) computational speed several orders of magnitude faster than the widely used LMNN and ITML methods. Furthermore, on standard benchmark datasets, our closed-form solution consistently attains higher classification accuracy.", "histories": [["v1", "Mon, 18 Jul 2016 10:14:46 GMT  (38kb)", "http://arxiv.org/abs/1607.05002v1", "7 pages, 4 figures"]], "COMMENTS": "7 pages, 4 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["pourya zadeh", "reshad hosseini", "suvrit sra"], "accepted": true, "id": "1607.05002"}, "pdf": {"name": "1607.05002.pdf", "metadata": {"source": "CRF", "title": "Geometric Mean Metric Learning", "authors": ["Suvrit Sra"], "emails": ["reshad.hosseini}@ut.ac.ir", "suvrit@mit.edu"], "sections": [{"heading": null, "text": "The problem of learning linear maps has been introduced in 3 \"metric learning processes,\" as we have experienced in 3 \"metric learning processes.\" (\"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we,"}, {"heading": "A. Related work", "text": "We recall some related work to put GMML into perspective. We omit a discussion of non-linear methods and other variations of the basic Euclidean task outlined above; instead, we refer the reader to both kernel-based metric learning processes [5] and other techniques used in recent surveys by Kulis [1] and Bellet et al. [6] Probably the earliest work to formulate metric learning processes [3] is sometimes referred to as MMC, which minimizes the sum of distances over similar points while trying to ensure that different points are far apart. Using sentences S and D, MMC solves the optimization problem in A 0 [xi, xj)."}, {"heading": "II. GMML: FORMULATION AND SOLUTION", "text": "As discussed above, the basic idea behind euclidean metric learning methods is to ultimately obtain a metric that yields \"small\" distances for similar points and \"large\" distances for unequal points. As one of the earliest metric learning methods MMC, we propose to find a matrix A that reduces the sum of distances across all similar points, but unlike all previous methods, instead of treating the unequal points asymmetrically, we suggest to measure their point spacing using A-1 and add their contribution to the overall target. More specifically, we propose the following novel target function: (xi, xj) that the distance between SdA (xi, xj) and the x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"}, {"heading": "B. Optimization problem and its solution", "text": "In the following, we assume that S is a problem that is a realistic situation."}, {"heading": "C. Regularized version", "text": "We have seen that the solution of our method is the geometric mean between S \u2212 1 and D. In practice, however, the matrix S can sometimes be non-reversible or almost singular. To address this concern, we propose to add a regularizing term to the objective function. This regularizing term can also be used to include previous knowledge of the distance function. In particular, we propose to use the symmetrized LogDet divergence: Dld (A, A0) + Dld (A0, A), where A0 is the \"previous\" (SPD matrix) and Dsld (A, A0) is the symmetrized LogDet divergence: Dld (A, A0) + Dld (A0, A) + Dld (A0, A), which is equal to Dsld (A, A0)."}, {"heading": "D. Extension to weighted geometric mean", "text": "The geodesic point of view is also the key to deciding how to assign different \"weights\" to the matrices S and D when calculating the GMML solution. (This point of view is important because simply scaling the costs in (8) to change the balance between S and D is not sensible, since it scales only the resulting solution A by a constant quantity. (Given the geometric nature of the GMML solution, we replace the linear costs in (8) with a nonlinear solution guided by the Riemannian geometry of the SPD mannix. (16) The key finding in the observation of a weighted version of GMML comes from a crucial geometric observation. (8) The minimum of (8) is also the following optimization problem: min A, 0, S \u2212 1) + 2R (A, D), where we consider 2R (A, D), where we consider the weighted version of GML as a geographical observation."}, {"heading": "III. RESULTS", "text": "In this section we compare the performance of the proposed GMML method (algorithm 1) with some well-known metric learning algorithms: \u2022 ITML [5]; \u2022 LMNN [2]; and \u2022 FlatGeo with flat batch geometry [8]. We use the commonly used criterion to compare the performance of different methods, i.e. the rate of classification error for a k-NN classifier on different data sets. We select k = 5 and estimate a full-fledged matrix A in all methods."}, {"heading": "A. Experiment 1", "text": "Suppose c were the number of classes, it is common practice to generate 40c (c \u2212 1) number of constraints by randomly selecting 40c (c \u2212 1) pairs of points in a dataset. In our first experiment, shown in Figure 2, we used this number of constraints in our method in addition to ITML and FlatGeo methods. The LMNN method does not have this number of constraints parameters and we used a new version of its toolbox that uses Bayesian optimization to optimize the model hyperparameter. We use the default parameters that are used in ITML and FlatGeo, except we also use a minimum iteration of 104 for the FlatGeo method, because we have observed that sometimes FlatGeo stops prematurely results in very poor performance. ITML has a regulation parameter that is set by using cross-validations."}, {"heading": "B. Experiment 2", "text": "The results can be summarized in Figure 4. The data sets in this experiment are Isolet, Letters [20], MNIST3 [21] and USPS [22]. Figure 4 reports the average classification error over 5 runs of random splitting of the data. We use triple cross validation to adjust the parameter. As the similarity matrices of the MNIST data were not immutable, we use the regularized version of our method with regulation parameters \u03bb = 0.1. The previous matrix A0 is set to the identity matrix. On two of the large data sets, letters and USPS, our method achieves the same performance as the best competing method, which is LMNN."}, {"heading": "IV. CONCLUSION AND FUTURE WORK", "text": "Building on geometric intuition, we approached the task of learning a symmetrical positive definitive matrix by formulating it as a smooth, strictly convex optimization problem (thus ensuring a unique solution). Remarkably, our formulation proved to have a closed form solution. In all cases, we considered our formulation to be an optimization problem based on the belt-like multiplicity of SPD matrices, a viewpoint that proved crucial to obtaining a weighted generalization of the basic formulation. We also presented a regulated version of our problem. In all cases, the solution could be achieved as a closed form of \"geometric mean of the matrix,\" which explained our choice of nomenclature. We experimented with multiple sets of data, both large and small, in which we compared the classification accuracy of a k-NN classifier using various competing learned methods."}, {"heading": "A. Future work", "text": "Below we list some promising directions: \u2022 To consider our metric learning methods as a method of dimensionality reduction; here the links in [4] may be helpful. \u2022 Extensions of our simple geometric framework for learning non-linear and local metrics. \u2022 Application of the idea of applying the Mahalanobis distance dA to the other problems of machine learning at the same time as its counterpart d \u0441A."}], "references": [{"title": "Metric learning: A survey", "author": ["B. Kulis"], "venue": "Foundations and Trends in Machine Learning, vol. 5, no. 4, pp. 287\u2013364, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 207\u2013244, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "M.I. Jordan", "S. Russell", "A.Y. Ng"], "venue": "Advances in neural information processing systems, 2002, pp. 505\u2013512.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Linear dimensionality reduction: Survey, insights, and generalizations", "author": ["J.P. Cunningham", "Z. Ghahramani"], "venue": "Journal of Machine Learning Research, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "Proceedings of the 24th international conference on Machine learning. ACM, 2007, pp. 209\u2013216.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "arXiv preprint arXiv:1306.6709, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S.T. Roweis"], "venue": "Advances in neural information processing systems, 2005, pp. 451\u2013458.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Regression on fixed-rank positive semidefinite matrices: A riemannian  7 approach", "author": ["G. Meyer", "S. Bonnabel", "R. Sepulchre"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 593\u2013625, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng"], "venue": "Proceedings of the twenty-first international conference on Machine learning, 2004, p. 94.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Online metric learning and fast similarity search", "author": ["P. Jain", "B. Kulis", "I.S. Dhillon", "K. Grauman"], "venue": "Advances in neural information processing systems, 2009, pp. 761\u2013 768.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast solvers and efficient implementations for distance metric learning", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Proceedings of the 25th international conference on Machine learning, 2008, pp. 1160\u20131167.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Online learning in the embedded manifold of low-rank matrices", "author": ["U. Shalit", "D. Weinshall", "G. Chechik"], "venue": "The Journal of Machine Learning Research, vol. 13, no. 1, pp. 429\u2013458, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Metric spaces, convexity and nonpositive curvature", "author": ["A. Papadopoulos"], "venue": "European Mathematical Society,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Geodesic convexity in nonlinear optimization", "author": ["T. Rapcs\u00e1k"], "venue": "Journal of Optimization Theory and Applications, vol. 69, no. 1, pp. 169\u2013183, 1991.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1991}, {"title": "Positive definite matrices", "author": ["R. Bhatia"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Conic geometric optimization on the manifold of positive definite matrices", "author": ["S. Sra", "R. Hosseini"], "venue": "SIAM Journal on Optimization, vol. 25, no. 1, pp. 713\u2013739, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Convex analysis", "author": ["R.T. Rockafellar"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1970}, {"title": "The geometric mean of two matrices from a computational viewpoint", "author": ["B. Iannazzo"], "venue": "arXiv preprint arXiv:1201.0101, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "UCI machine learning repository", "author": ["A. Asuncion", "D. Newman"], "venue": "2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u2013 2324, 1998.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["B.B. Le Cun", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Advances in neural information processing systems, 1990.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1990}], "referenceMentions": [{"referenceID": 0, "context": "When supervised or weakly supervised information is available, selection of the distance function can itself be cast as a learning problem called \u201cmetric learning\u201d [1, 2].", "startOffset": 164, "endOffset": 170}, {"referenceID": 1, "context": "When supervised or weakly supervised information is available, selection of the distance function can itself be cast as a learning problem called \u201cmetric learning\u201d [1, 2].", "startOffset": 164, "endOffset": 170}, {"referenceID": 2, "context": "The problem of learning linear maps was introduced in [3] as \u201cMahalanobis metric learning.", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "More broadly, the idea of linearly transforming input features is a bigger theme across machine learning and statistics; encompassing whitening transforms, linear dimensionality reduction, Euclidean metric learning, and more [1, 4].", "startOffset": 225, "endOffset": 231}, {"referenceID": 3, "context": "More broadly, the idea of linearly transforming input features is a bigger theme across machine learning and statistics; encompassing whitening transforms, linear dimensionality reduction, Euclidean metric learning, and more [1, 4].", "startOffset": 225, "endOffset": 231}, {"referenceID": 4, "context": "We omit a discussion of nonlinear methods, and other variations of the basic Euclidean task outlined above; for these, we refer the reader to both kernelized metric learning [5] and other techniques as summarized in the recent surveys of Kulis [1] and Bellet et al.", "startOffset": 174, "endOffset": 177}, {"referenceID": 0, "context": "We omit a discussion of nonlinear methods, and other variations of the basic Euclidean task outlined above; for these, we refer the reader to both kernelized metric learning [5] and other techniques as summarized in the recent surveys of Kulis [1] and Bellet et al.", "startOffset": 244, "endOffset": 247}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Probably the earliest work to formulate metric learning is [3], sometimes referred to as MMC.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "[3] use \u221a dA instead of the distance dA because under dA, problem (2) has a trivial rank-one solution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "(xi,xj)\u2208S dA(xi,xj) is also used in the other metric learning methods like LMNN [2] and MCML [7] as a part of their cost functions.", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "(xi,xj)\u2208S dA(xi,xj) is also used in the other metric learning methods like LMNN [2] and MCML [7] as a part of their cost functions.", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "Information-Theoretic Metric Learning (ITML) [5], aims to satisfy the similarity and dissimilarity constraints while staying as \u201cclose\u201d as possible to a predefined matrix.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "[8] propose the formulation", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "There exist several attempts for achieving high scalability with both the dimensionality and the number of constraints in the metric learning methods; some examples include [9, 10, 11, 12].", "startOffset": 173, "endOffset": 188}, {"referenceID": 9, "context": "There exist several attempts for achieving high scalability with both the dimensionality and the number of constraints in the metric learning methods; some examples include [9, 10, 11, 12].", "startOffset": 173, "endOffset": 188}, {"referenceID": 10, "context": "There exist several attempts for achieving high scalability with both the dimensionality and the number of constraints in the metric learning methods; some examples include [9, 10, 11, 12].", "startOffset": 173, "endOffset": 188}, {"referenceID": 11, "context": "There exist several attempts for achieving high scalability with both the dimensionality and the number of constraints in the metric learning methods; some examples include [9, 10, 11, 12].", "startOffset": 173, "endOffset": 188}, {"referenceID": 12, "context": "Geodesic convexity is the generalization of ordinary (linear) convexity to (nonlinear) manifolds and metric spaces [13, 14].", "startOffset": 115, "endOffset": 123}, {"referenceID": 13, "context": "Geodesic convexity is the generalization of ordinary (linear) convexity to (nonlinear) manifolds and metric spaces [13, 14].", "startOffset": 115, "endOffset": 123}, {"referenceID": 0, "context": "A\u266ftB = A 1/2 ( ABA t A, t \u2208 [0, 1].", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "f(A\u266ftB) \u2264 tf(A) + (1\u2212 t)f(B), t \u2208 [0, 1].", "startOffset": 34, "endOffset": 40}, {"referenceID": 15, "context": "We refer the reader to [16] for more on geodesic convexity for SPD matrices.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "for t \u2208 [0, 1].", "startOffset": 8, "endOffset": 14}, {"referenceID": 17, "context": "There are several approaches for fast computation of Riemannian geodesics for SPD matrices, for instance, CholeskySchur and scaled Newton methods [19].", "startOffset": 146, "endOffset": 150}, {"referenceID": 4, "context": "In this section, we compare the performance of the proposed method GMML (Algorithm 1) to some well-known metric learning algorithms: \u2022 ITML [5]; \u2022 LMNN [2]; and \u2022 FlatGeo with batch flat geometry [8].", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "In this section, we compare the performance of the proposed method GMML (Algorithm 1) to some well-known metric learning algorithms: \u2022 ITML [5]; \u2022 LMNN [2]; and \u2022 FlatGeo with batch flat geometry [8].", "startOffset": 152, "endOffset": 155}, {"referenceID": 7, "context": "In this section, we compare the performance of the proposed method GMML (Algorithm 1) to some well-known metric learning algorithms: \u2022 ITML [5]; \u2022 LMNN [2]; and \u2022 FlatGeo with batch flat geometry [8].", "startOffset": 196, "endOffset": 199}, {"referenceID": 18, "context": "The datasets are obtained from the well-known UCI repository [20].", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "are Isolet, Letters [20], MNIST3 [21] and USPS [22].", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "are Isolet, Letters [20], MNIST3 [21] and USPS [22].", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "are Isolet, Letters [20], MNIST3 [21] and USPS [22].", "startOffset": 47, "endOffset": 51}], "year": 2016, "abstractText": "We revisit the task of learning a Euclidean metric from data. We approach this problem from first principles and formulate it as a surprisingly simple optimization problem. Indeed, our formulation even admits a closed form solution. This solution possesses several very attractive properties: (i) an innate geometric appeal through the Riemannian geometry of positive definite matrices; (ii) ease of interpretability; and (iii) computational speed several orders of magnitude faster than the widely used LMNN and ITML methods. Furthermore, on standard benchmark datasets, our closed-form solution consistently attains higher classification accuracy.", "creator": "LaTeX with hyperref package"}}}