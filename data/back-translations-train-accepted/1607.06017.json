{"id": "1607.06017", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jul-2016", "title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition", "abstract": "We study $k$-GenEV, the problem of finding the top $k$ generalized eigenvectors, and $k$-CCA, the problem of finding the top $k$ vectors in canonical-correlation analysis. We propose algorithms $\\mathtt{LazyEV}$ and $\\mathtt{LazyCCA}$ to solve the two problems with running times linearly dependent on the input size and on $k$.", "histories": [["v1", "Wed, 20 Jul 2016 16:43:18 GMT  (591kb,D)", "http://arxiv.org/abs/1607.06017v1", "arXiv admin note: text overlap witharXiv:1607.03463"], ["v2", "Sat, 26 Nov 2016 03:18:24 GMT  (668kb,D)", "http://arxiv.org/abs/1607.06017v2", "We have now stated more clearly why this paper has outperformed relevant previous results, and included discussions for doubly-stochastic methods. arXiv admin note: text overlap witharXiv:1607.03463"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1607.03463", "reviews": [], "SUBJECTS": "math.OC cs.DS cs.LG stat.ML", "authors": ["zeyuan allen-zhu", "yuanzhi li"], "accepted": true, "id": "1607.06017"}, "pdf": {"name": "1607.06017.pdf", "metadata": {"source": "CRF", "title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition", "authors": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "emails": ["zeyuan@csail.mit.edu", "yuanzhil@cs.princeton.edu"], "sections": [{"heading": null, "text": "In addition, our algorithms are doubly accelerated: Our runtimes depend only on the square root of the matrix condition number and on the square root of the eigengap. This is the first such result for both k-GenEV and k-CCA. In addition, we provide the first gapless results, which provide runtimes that depend on 1 / \u221a \u03b5 and not on the eigengap."}, {"heading": "1 Introduction", "text": "The generalized eigenvector (GenEV) problem and the Canonical Correlation Analysis (CCA) = > problem (CCA) are two fundamental problems in scientific computation, machine learning, operational research and statistics. Algorithms that solve these problems are often used to compare gaps, as well as for problems in regression; the GenEV problem consists in finding generalized eigenvectors v1,. vd, where each vi is satisfactory, and many others."}, {"heading": "2 Preliminaries", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3 Leading Eigenvector via Two-Sided Shift-and-Invert", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "4 Main Algorithm for Generalized Eigendecomposition", "text": "In this section, we propose that LazyEV (see Algorithm 2) calculates approximately the k \"leading\" eigenvectors that correspond to the k \"largest absolute eigenvalues of any symmetric matrix M\" Rd \"d. Later, we solve the k\" leading eigenvectors of A with respect to B. Our LazyEV algorithm is formally specified in Algorithm 2, using k \"mal\" AppxPCA \"\u00b1, each time with a multiplicative error of M\" 2, and projecting the matrix M into orthogonal space with respect to the obtained leading eigenvector. We specify our most important approximations. Theorem 4.1 (approximation of LazyEV). Let M \"Rd\" d \"d\" be a symmetric matrix with eigenvalues."}, {"heading": "4.1 Application to GenEV", "text": "Our top theories imply the following corollaries: corollaries 4,3 (gap-dependent k-GenEV). Let us allow A, B-Rd \u00b7 d to be two symmetrical matrices, which are B 0 and \u2212 B-A. Let us suppose that the generalized eigenvalue and eigenvalue pairs of A in relation to B {(\u03bbi, ui)} are 1, and it is satisfied 1 \u2265 | \u2265 \u00b7 \u00b7 \u2265 | 2 | Let us leave gap = 1 / 2, gap, O (\u03b54 \u00b7 gap k3 (\u03c31 / k) 4), > 0, and consider the outputVk \u2190 B \u2212 1 / 2LazyEV (A, 1 / 2LazyEV \u2212 1 / 2, gap, O (\u03b54 \u00b7 gap k3 (\u03c31 / k) 4), and consider the outputVk \u2212 1 / 2LazyEV, 1 / 2LazyEV, gap = 1 / 2, gap (\u03b54 \u00b7 gap k3 (\u03c31 / 1 / k)), and let us consider the outputVector (A)."}, {"heading": "5 High Level Ideas Behind Theorems 4.1 and 4.2", "text": "Our LazyEV algorithm reduces the problem of finding generalized eigenvectors to finding regular eigenvectors of M = B \u2212 1 / 2AB \u2212 1 / 2. In Section 5.1 we discuss how to ensure accuracy: that is, why LazyEV guarantees to find approximately the uppermost absolute eigenvectors of M; and in Section 5.2 we discuss how to implement LazyEV without ever having to calculate B1 / 2 or B \u2212 1 / 2."}, {"heading": "5.1 Ideas Behind Theorem 4.1: Approximation Guarantee of GenEV", "text": "Our approximate guarantee in Theorem 4.1 is a natural generalization of the recent work on fast iterative methods to find the uppermost k eigenvectors of a PSD matrix M. This method is called LazySVD. At a high level, LazySVD finds the uppermost k eigenvectors of M one by one, but only approximately. Starting from M0 = M, in the s-th iteration, in the s [k], LazySVD roughly calculates the leading eigenvector of the matrix Ms \u2212 1 and names it using displacement and inversion [9]. LazySVD then performs a projection Ms (I \u2212 vsv > s) Ms \u2212 1 (I \u2212 vsv > s) and proceeds to the next iteration. While the algorithmic idea of LazySVD is simple, the analysis requires some careful linear algebraic lemmas."}, {"heading": "5.2 Proof of Theorem 4.2: Fast Implementation of GenEV", "text": "We can implement LazyEV effectively without the need of calculating B1 / 2 or B-1 / 2 (= > Vs = > Vs = > Vs = > Vs = > Vs = > Vs = 1 / 2). In each iteration of LazyEV we will not explicitly store the Vs (Vs 1 / 2V) and the Vs (Vs 1 / 2Vs). (Vs 1 / 2Vs). (Vs 1 / 2Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs). (Vs"}, {"heading": "6 Main Algorithm for Canonical Correlation Analysis", "text": "In this section we propose LazyCCA (see algorithm 3), a variant of LazyEV, which is symmetrical especially for matrices M = B \u2212 1 / 2, where A and B are affected by a CCA problem according to term 2.3.More precisely, the eigenvectors of the matrices M \u2212 1, which arise from CCA instances, are: if it is a normalized eigenvector of M with eigenvalue, where the eigenvalues of Rdx, Rdy, Rdy, then (\u2212 2,) is also a normalized eigenvector, but with eigenvalue \u2212 2, because (,) is an orthogonal eigenvector of M with eigenvalue, where Rdx, Rdx and Rdy, then (-3,) it is a normalized eigenvector, which represents the eigenvalues of vector, but with eigenvalue \u2212 3, (\u2212 3) is a normalized eigenvector of M with eigenvalue."}, {"heading": "A Missing Miscellaneous Proofs", "text": "First of all, the calculation of B \u2212 1Aw is equivalent to the minimization of f (x) def = 12x > Bx \u2212 x > Aw. Suppose we write x = (x1, x2) and w = (w1, w2), where x1, w1, w2, Rdx, x2, w2, Rdy, then we can rewrite f (x) as f (x) = 12n (x), Xx1 \u2212 Y w2 + (w1, w1 \u2212 Y x2) + C, where C is a fixed constant. Therefore, we can also rewrite f (x) = 12nn, i = 1 (< Xi, x1 > \u2212 < Yi, w2 + (< Yi, x2 > \u2212 Y), where C is a fixed constant."}, {"heading": "B Proof Details for Section 3: Two-Sided Shift-and-Invert", "text": "B.1 Inexact Power MethodIn this subsection, we examine some classical convergence dilemmas with respect to the Power Method and its inaccurate variant. These dilemmas follow almost directly from previous results such as [9, 11] and are more similar [2]. We skip the proofs in this essay. Let us consider the Power Method, which begins with a random unit vector w0 \u2190 RanInit (d), and let us apply the random eigenvectors u1 / 2 Mwt \u2212 1% iteratively.Lemma B.1 (Exact Power Method). Let us consider M as a PSD matrix with eigenvalues \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 270 Mwt and associated eigenvectors u1,.. Let us fix a fault tolerance. > 0, parameter \u04321, and error probability p > 0, defined error probability p > 0, defined error probability p > 0, defined error probability p > 0, defined error probability."}, {"heading": "B.2 Proof of Theorem 3.1", "text": "We prove Theorem 3.1 by first pointing out the following problem. \u2212 \u2212 \u2212 Most of these properties are analogous to their original variants in [9, 10], but at this point we also pay attention to negative eigenvalues and allow M \u2212 \u00b7 \u00b7 non-PSD. Lemma B.4 (useful properties of AppxPCA \u00b1). With a probability of at least 1 \u2212 p it applies that (by allowing it): (a) \u2264 \u2212 1 \u2264 \u2212 132\u0445 (s \u2212 1) I \u2212 M) \u2212 1 \u2264 (useful properties of AppxPCA \u00b1) \u2212 1 \u2264 (s) 1 \u2264 (s) I + M) \u2212 1, m1) for each repetition s s s s s s s s s s s s s s s s s s s s s s s s s s s s 1; (b) \u2264 2 \u2264 \u03b54\u044b (f) I \u2212 M) \u2212 1, m) and we have no solution (f)."}, {"heading": "C Lemmas Needed for Proving Our Main Theorem", "text": "In this section we provide some necessary matrix algebra lemmas, which should become indispensable for our proof of theorem 4.1. Many of these lemmas are analogous to the lemmas used in the SVD algorithm by the same authors of this essay [2], but we need a little more care in this essay, since the underlying matrix M is no longer PSD sentence C.1. Let A, B be two (column) orthonormal matrix, so that for each task a value of 0, A > BB > A (1 \u2212 \u03b7) IThen we have: There is a matrix Q, HQ \u00b2 2 \u2264 1 such that HQ = (BB > + B \u00b2) > Proof."}, {"heading": "C.1 Approximate Projection Lemma", "text": "The next problem is that if we project a symmetrical matrix M into the orthogonal space of Vs (> > Q > > Q > > s into the orthogonal space of Vs (> V > s U) (> V > s) (> V > s) (> V > s) (> V > s) (> V > s) (> V > s) (> V > s) (> V > s) (> V > s) (> V > s) (> V > s) (> V > s) (> V > s), this problem is obvious if \"small\" means zero correlation: if Vs were completely orthogonal to U, then Qs would be equal to Vs, so that M has a symmetrical matrix with (not necessarily sorted) eigenvalues 1,., Qs and the corresponding eigenvectors u1,."}, {"heading": "C.2 Gap-Free Wedin Theorem", "text": "Lemma C.3 (double-sided slit-free Wedin theorem) For \u03b5 \u2265 0, A, B should consist of two symmetrical matrices, so that \"A-B-2 \u2264 \u03b5.\" For each \u00b5-free Wedin matrix, \"U\" should be an orthonormal column matrix consisting of eigenvectors of A with absolute eigenvalues \u2264 \u00b5, \"V\" should be an orthonorthonormal column matrix consisting of eigenvectors of B with absolute eigenvalues \u2265 \u00b5 + \u03c4, then we have \"U\" > V. \"Proof Lemma C.3. We write\" A \"and\" B in relation to intrinsic value substitution: A = UW \"U\" > + U. \"\u2212 V \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > R > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > V > R > V > V > V > V > V > R > V > V > R > V > R > V > R > V > R > R > R > V > R > V > R > R > R > R > R > V > V > V > V > V > V\" V."}, {"heading": "C.3 Eigenvector Projection Lemma", "text": "Our next technical problem deals with the projection of a matrix M in the orthogonal direction of a vector v > > Q >, where v has little correlation with M's leading eigenvectors below a certain threshold \u00b5 (denoted by U). \u2212 The conclusion of the term C4 is that after the projection, if we have the leading eigenvectors of M \"= (I \u2212 vv >) M (I \u2212 vv >) below a certain threshold \u00b5 and denote them by V1, then U is roughly embedded in V1, which means that V1 could be of greater dimension of U, but there is a matrix Q with spectral standard no more than 1, so that\" U \u2212 V1Q \"2 is smaller. Lemma C4 is a symmetric matrix with eigenvalues of C.1,..., eigenvalues of eigenvalues of V1,."}, {"heading": "D Matrix Inversion via Approx Accelerated Gradient Descent", "text": "In view of a positive matrix vector multiplication, we have a problem that we can only solve if we can only approximate matrix vector multiplications. (In this section, we will revisit this problem by compiling matrix vector multiplications only roughly. (We emphasize that this is not an easy task.) If matrix vector multiplications are reduced to T vector multiplications, then a standard analysis implies that each of these multiplications must be compiled to a very small error (T)."}, {"heading": "D.2 Proof of Theorem D.1", "text": "The proof for theorem D.1. We first check the accuracy."}, {"heading": "E Proof Details for Section 4: GenEV Theorems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "E.1 Proof of Theorem 4.1", "text": "In this section we prove that theorem 4.1 is formal. Theorem 4.1 (redefined) is then the same. Let's create a symmetrical matrix with eigenvalues (redefined). \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 Therefore, the eigenvectors U1,., is assumed, without loss of generality, that the eigenvectors Vk = (v1,.., vk), which with probability at least 1 \u2212 p can fulfill all the following properties. (Denote by Mk = (I \u2212 VkV > k) M (I \u2212 VkV > k).) (a) Core dilemma: if the eigenvectors."}, {"heading": "E.2 Proofs of Corollaries 4.3 and 4.4", "text": "The definition of generalized eigenvectors shows that B1 / 2u1,..., B 1 / 2ud eigenvectors of M def = B \u2212 1 / 2AB \u2212 1 / 2 with eigenvalues \u03bb1,.. \u2212 \u2212 d Application Theorem 4.1.a, we have the smallest index that is satisfactory. \u2212 V > k U \u00b2, where U = (B1 / 2uj,.., B 1 / 2ud) is an orthonormal matrix (column) and j is the smallest index that is satisfactory. \u2212 V > k U \u00b2, where U = (B1 / 2uj, B 1 / 2ud) is an orthonorthonormal matrix and j is the smallest indicator that is satisfactory."}, {"heading": "F Proof Details for Section 6: CCA Theorems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "F.1 The Main Convergence Theorem", "text": "Since LazyCCA allows only minor changes at the top of LazyEV, the next theorem is an almost identical copy of Theorem 4.1. To make this work concise, we only need to outline the most important changes in the new Proof.Theorem F.1. Let M = B \u2212 1 / 2AB \u2212 1 / 2 - Rd \u00b7 d be a symmetrical matrix in which A \u2212 and B \u2212 matrices come from a CCA instance using Lemma 2.3. Suppose M has eigenvalues in the order of magnitude 1,."}, {"heading": "F.2 Fast Implementation of CCA", "text": "Theorem F. 2 (runtime of LazyCCA) Let X-Rn \u00b7 dx, Y-Rn \u00b7 dy be two matrices and define A and B according to Lemma 2.3. Suppose M = B-1 / 2AB \u2212 1 / 2, and RanInit (d) is the random vector generator defined in Proposition 3.3, and we want to calculate V-R-B-1 / 2LazyCCA (A, M, k, \u03b4 \u00d7, \u03b5pca, p). Then this method can be implemented to get into time6This is because matrix Ms always has the form D-1 / 2CD1 / 2, in which D-diag is defined diagonally and positively, while C = [0, C1]; [C > 1, 0] has only zero on its two block diagonal positions. The same evidence of Lemma 2.3 shows that the eigenvectors of LazyCCA must be symmetric CD1 / 2."}, {"heading": "F.3 Proofs of Corollaries 6.2 and 6.3", "text": "The two conclusions result from theorem F. 1 for the reason similar to the conclusions 4.3 and 4.4 from theorem 4.1.Sketch proof conclusions 6.2. The approximation guarantee results from theorem F. 2. Sketch proof of the conclusions 6.3. The approximation guarantee guarantees max\u03c6, as well as the approximation guarantee Rdx, as well as the approximation guarantee Rdy (1 \u2212 2) and Rdy (2 \u2212 3). The approximation guarantee results from theorem F.1.b and the definition of M, and the approximation guarantee (1 \u2212 4) results from theorem F.1.c and the fact that time results from theorem F.1.c."}], "references": [{"title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "author": ["Zeyuan Allen-Zhu"], "venue": "ArXiv e-prints,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Even Faster SVD Decomposition Yet Without Agonizing Pain", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Linear coupling: An ultimate unification of gradient and mirror descent", "author": ["Zeyuan Allen-Zhu", "Lorenzo Orecchia"], "venue": "ArXiv e-prints,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Even faster accelerated coordinate descent using non-uniform sampling", "author": ["Zeyuan Allen-Zhu", "Peter Richt\u00e1rik", "Zheng Qu", "Yang Yuan"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Expander flows, geometric embeddings and graph partitioning", "author": ["Sanjeev Arora", "Satish Rao", "Umesh V. Vazirani"], "venue": "Journal of the ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "A survey of preconditioned iterative methods for linear systems of algebraic equations", "author": ["Owe Axelsson"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1985}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["Kamalika Chaudhuri", "Sham M Kakade", "Karen Livescu", "Karthik Sridharan"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Multi-view learning of word embeddings via cca", "author": ["Paramveer Dhillon", "Dean P Foster", "Lyle H Ungar"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Fast and simple PCA via convex optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": "ArXiv e-prints,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["Daniel Garber", "Elad Hazan", "Chi Jin", "Sham M. Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis", "author": ["Rong Ge", "Chi Jin", "Sham M. Kakade", "Praneeth Netrapalli", "Aaron Sidford"], "venue": "ArXiv e-prints,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["Sham M Kakade", "Dean P Foster"], "venue": "In Learning theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Discriminative features via generalized eigenvectors", "author": ["Nikos Karampatziakis", "Paul Mineiro"], "venue": "In ICML, pages 494\u2013502,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Large scale canonical correlation analysis with iterative least squares", "author": ["Yichao Lu", "Dean P Foster"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Zhuang Ma", "Yichao Lu", "Dean Foster"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Zhuang Ma", "Yichao Lu", "Dean Foster"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Nonparametric canonical correlation analysis", "author": ["Tomer Michaeli", "Weiran Wang", "Karen Livescu"], "venue": "arXiv preprint,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition", "author": ["Cameron Musco", "Christopher Musco"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k2)", "author": ["Yurii Nesterov"], "venue": "In Doklady AN SSSR (translated as Soviet Mathematics Doklady),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1983}, {"title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate", "author": ["Ohad Shamir"], "venue": "In icml,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "author": ["Jonathan Richard Shewchuk"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "Large-scale approximate kernel canonical correlation analysis", "author": ["Weiran Wang", "Karen Livescu"], "venue": "arXiv preprint,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis", "author": ["Weiran Wang", "Jialei Wang", "Dan Garber", "Nathan Srebro"], "venue": "ArXiv e-prints,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["Daniela M Witten", "Robert Tibshirani", "Trevor Hastie"], "venue": "Biostatistics, page kxp008,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}], "referenceMentions": [{"referenceID": 11, "context": "Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.", "startOffset": 144, "endOffset": 148}, {"referenceID": 6, "context": "Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.", "startOffset": 161, "endOffset": 164}, {"referenceID": 12, "context": "Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.", "startOffset": 181, "endOffset": 185}, {"referenceID": 7, "context": "Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.", "startOffset": 203, "endOffset": 206}, {"referenceID": 14, "context": "breakthrough result of Ma, Lu and Foster [16], they proposed to study algorithms to find top k generalized eigenvectors or top k canonical-correlation vectors.", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "As three concrete examples: \u2022 Block Krylov method computes top eigenvectors with a running time linearly in 1/\u221agap rather than 1/gap [12], or with a gap-free running time that depends on 1/ \u221a \u03b5 rather than 1/gap or 1/ \u221a gap, where \u03b5 is the approximation error [19].", "startOffset": 260, "endOffset": 264}, {"referenceID": 20, "context": "\u2022 Conjugate gradient [22], Chebyshev method [6], and Nesterov\u2019s method [20] compute B\u22121w for a vector w with a running time linearly in \u221a \u03ba rather than \u03ba, where \u03ba is the condition number of matrix B.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "\u2022 Conjugate gradient [22], Chebyshev method [6], and Nesterov\u2019s method [20] compute B\u22121w for a vector w with a running time linearly in \u221a \u03ba rather than \u03ba, where \u03ba is the condition number of matrix B.", "startOffset": 44, "endOffset": 47}, {"referenceID": 18, "context": "\u2022 Conjugate gradient [22], Chebyshev method [6], and Nesterov\u2019s method [20] compute B\u22121w for a vector w with a running time linearly in \u221a \u03ba rather than \u03ba, where \u03ba is the condition number of matrix B.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "Indeed, two groups of authors independently attempted to answer such questions [11, 24].", "startOffset": 79, "endOffset": 87}, {"referenceID": 22, "context": "Indeed, two groups of authors independently attempted to answer such questions [11, 24].", "startOffset": 79, "endOffset": 87}, {"referenceID": 10, "context": "\u2022 For the k-GenEV problem, GenELin [11] improved the dependency of \u03ba to \u221a\u03ba.", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "In a separate work, SI [24] also obtained the \u221a \u03ba dependency but only for the simpler k = 1 case; at the same time, SI enjoys a \u221a gap dependency but paying an additional factor \u03bb1.", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "The two groups of authors proposed three methods: CCALin [11], ALS [24] and SI [24].", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "The two groups of authors proposed three methods: CCALin [11], ALS [24] and SI [24].", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "The two groups of authors proposed three methods: CCALin [11], ALS [24] and SI [24].", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "1-GenEV GenELin [11] \u00d5 ( nnz(B)\u03baB gap + nnz(A) gap ) no no", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "SI [24] \u00d5 ( nnz(B)\u03baB \u221a gap\u00b7\u03bb1 + nnz(A) \u221a gap\u00b7\u03bb1 ) no no", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "1-CCA AppGrad [16] nnz(X,Y ) \u00b7 \u00d5 ( \u03ba gap ) no no CCALin [11] nnz(X,Y ) \u00b7 \u00d5 (\u221a\u03ba gap ) no no", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "1-CCA AppGrad [16] nnz(X,Y ) \u00b7 \u00d5 ( \u03ba gap ) no no CCALin [11] nnz(X,Y ) \u00b7 \u00d5 (\u221a\u03ba gap ) no no", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "ALS [24] nnz(X,Y ) \u00b7 \u00d5 ( \u221a\u03ba gap2 ) no no", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "SI [24] nnz(X,Y ) \u00b7 \u00d5 ( \u221a\u03ba \u221a gap\u00b7\u03c31 ) no no", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "CCALin [11] nnz(X,Y ) \u00b7 \u00d5 ( 1+\u221a\u03ba\u2032/n gap ) no yes", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "ALS [24] nnz(X,Y ) \u00b7 \u00d5 ( 1+\u221a\u03ba\u2032/n gap2 ) no yes", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "SI [24] nnz(X,Y ) \u00b7 \u00d5 ( 1+ \u221a \u03ba\u2032/ \u221a n \u221a gap\u00b7\u03c31 ) no yes", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "In GenEV, gap = \u03bb1\u2212\u03bb2 \u03bb1 \u2208 [0, 1], \u03bb1 \u2208 [0, 1], and \u03baB = \u03bbmax(B) \u03bbmin(B) > 1.", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": "In GenEV, gap = \u03bb1\u2212\u03bb2 \u03bb1 \u2208 [0, 1], \u03bb1 \u2208 [0, 1], and \u03baB = \u03bbmax(B) \u03bbmin(B) > 1.", "startOffset": 40, "endOffset": 46}, {"referenceID": 0, "context": "In CCA, gap = \u03c31\u2212\u03c32 \u03c31 \u2208 [0, 1], \u03c31 \u2208 [0, 1], \u03ba = \u03bbmax(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) > 1, and \u03ba \u2032 = Tr(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) \u2208 [\u03ba, d\u03ba].", "startOffset": 25, "endOffset": 31}, {"referenceID": 0, "context": "In CCA, gap = \u03c31\u2212\u03c32 \u03c31 \u2208 [0, 1], \u03c31 \u2208 [0, 1], \u03ba = \u03bbmax(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) > 1, and \u03ba \u2032 = Tr(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) \u2208 [\u03ba, d\u03ba].", "startOffset": 38, "endOffset": 44}, {"referenceID": 10, "context": "k-GenEV GenELin [11] \u00d5 (knnz(B)\u221a\u03baB gap + knnz(A)+kd gap ) no no", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "k-CCA AppGrad [16] \u00d5 (knnz(X,Y )\u00b7\u03ba+kd gap ) (local converge) no no CCALin [11] \u00d5 (knnz(X,Y )\u00b7\u221a\u03ba+k2d gap ) no no", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "k-CCA AppGrad [16] \u00d5 (knnz(X,Y )\u00b7\u03ba+kd gap ) (local converge) no no CCALin [11] \u00d5 (knnz(X,Y )\u00b7\u221a\u03ba+k2d gap ) no no", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "CCALin [11] \u00d5 (knnz(X,Y )\u00b7 ( 1+ \u221a \u03ba\u2032/n ) +kd gap ) no yes", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "In GenEV, gap = \u03bbk\u2212\u03bbk+1 \u03bbk \u2208 [0, 1] and \u03baB = \u03bbmax(B) \u03bbmin(B) > 1.", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "In CCA, gap = \u03c3k\u2212\u03c3k+1 \u03c3k \u2208 [0, 1], \u03ba = \u03bbmax(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) > 1, and \u03ba \u2032 = Tr(diag{Sxx,Syy}) \u03bbmin(diag{Sxx,Syy}) \u2208 [\u03ba, d\u03ba].", "startOffset": 27, "endOffset": 33}, {"referenceID": 22, "context": "In contrast, previous results for the k > 1 case rely on more sophisticated nonconvex optimization; and the previous work of [24] \u2014although uses convex optimization to solve 1-CCA\u2014 requires one to work with a sum-of-non-convex function which is less efficient to minimize.", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "For the easier problem of PCA and SVD, the first gap-free result was obtained by Musco and Musco [19], the first stochastic result was obtained by Shamir [21], and the first accelerated stochastic result was obtained by Garber et al.", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "For the easier problem of PCA and SVD, the first gap-free result was obtained by Musco and Musco [19], the first stochastic result was obtained by Shamir [21], and the first accelerated stochastic result was obtained by Garber et al.", "startOffset": 154, "endOffset": 158}, {"referenceID": 8, "context": "[9, 10].", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[9, 10].", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].", "startOffset": 75, "endOffset": 95}, {"referenceID": 15, "context": "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].", "startOffset": 75, "endOffset": 95}, {"referenceID": 16, "context": "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].", "startOffset": 75, "endOffset": 95}, {"referenceID": 21, "context": "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].", "startOffset": 75, "endOffset": 95}, {"referenceID": 23, "context": "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].", "startOffset": 75, "endOffset": 95}, {"referenceID": 15, "context": "Furthermore, for k > 1, the AppGrad result of [17] only provides local convergence guarantees and thus requires a warm-start whose computational complexity is not discussed in their paper.", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": ", \u03c3r satisfy \u03c3i = \u03c6 > i Sxy\u03c8i \u2208 [0, 1].", "startOffset": 32, "endOffset": 38}, {"referenceID": 0, "context": "For every w \u2208 Rd, Katyusha method [1] finds a vector w\u2032 \u2208 Rd satisfying \u2016w\u2032 \u2212B\u22121Aw\u2016 \u2264 \u03b5", "startOffset": 34, "endOffset": 37}, {"referenceID": 8, "context": "3 Leading Eigenvector via Two-Sided Shift-and-Invert In this section we define AppxPCA\u00b1, the multiplicative approximation algorithm for computing the two-sided leading eigenvector of a symmetric matrix using the shift-and-invert preconditioning framework [9, 10].", "startOffset": 255, "endOffset": 262}, {"referenceID": 9, "context": "3 Leading Eigenvector via Two-Sided Shift-and-Invert In this section we define AppxPCA\u00b1, the multiplicative approximation algorithm for computing the two-sided leading eigenvector of a symmetric matrix using the shift-and-invert preconditioning framework [9, 10].", "startOffset": 255, "endOffset": 262}, {"referenceID": 8, "context": "Our pseudo-code Algorithm 1 is a modification of Algorithm 5 appeared in [9].", "startOffset": 73, "endOffset": 76}, {"referenceID": 8, "context": "The main differences between AppxPCA\u00b1 and Algorithm 5 of [9] are two-fold.", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "Second, we provide a multiplicative-error guarantee rather than additive as originally appeared in [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 8, "context": "1 Of course, we believe the bulk of the credit for conceiving AppxPCA\u00b1 belongs to the original authors of [9, 10].", "startOffset": 106, "endOffset": 113}, {"referenceID": 9, "context": "1 Of course, we believe the bulk of the credit for conceiving AppxPCA\u00b1 belongs to the original authors of [9, 10].", "startOffset": 106, "endOffset": 113}, {"referenceID": 22, "context": "This is why the only known CCA result using shift-and-invert preconditioning [24] depends on 1 gap\u00b7\u03bb1 in Table 1.", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "Let gap = |\u03bbk|\u2212|\u03bbk+1| |\u03bbk| \u2208 [0, 1] be the relative gap.", "startOffset": 29, "endOffset": 35}, {"referenceID": 1, "context": "1 is a natural generalization of the recent work on fast iterative methods to find the top k eigenvectors of a PSD matrix M [2].", "startOffset": 124, "endOffset": 127}, {"referenceID": 8, "context": "Starting with M0 = M , in the s-th iteration where s \u2208 [k], LazySVD computes approximately the leading eigenvector of matrix Ms\u22121 and call it vs using shift-and-invert [9].", "startOffset": 168, "endOffset": 171}, {"referenceID": 1, "context": "This is achieved by a gap-free variant of the Wedin theorem plus a few other technical lemmas, and we recommend interested readers to see the high-level overview section of [2].", "startOffset": 173, "endOffset": 176}, {"referenceID": 20, "context": "As for the second item, we simply notice that whenever we want to compute w\u2032 \u2190 B\u22121Aw, we can first compute Aw in time O(nnz(A)), and then use Conjugate gradient [22] to compute B\u22121 applied to this vector.", "startOffset": 161, "endOffset": 165}, {"referenceID": 0, "context": "Let gap = \u03c3k\u2212\u03c3k+1 \u03c3k \u2208 [0, 1] be the relative gap, and define A = [[0, Sxy]; [S > xy, 0]] and B = diag(Sxx, Syy) following Definition 2.", "startOffset": 23, "endOffset": 29}, {"referenceID": 0, "context": "For such reason, one can apply the convergence theorem of Katyusha [1] to find an additive \u03b5\u0303 approximate minimizer of f(x) in time O ( nnz(X,Y ) \u00b7 ( 1 + \u221a \u03ba\u2032/n ) log f(x 0)\u2212f(x\u2217) \u03b5\u0303 ) where x0 is an arbitrary starting vector fed into Katyusha and x\u2217 is the exact minimizer.", "startOffset": 67, "endOffset": 70}, {"referenceID": 8, "context": "These lemmas almost directly follow from previous results such as [9, 11], and are more similar to [2].", "startOffset": 66, "endOffset": 73}, {"referenceID": 10, "context": "These lemmas almost directly follow from previous results such as [9, 11], and are more similar to [2].", "startOffset": 66, "endOffset": 73}, {"referenceID": 1, "context": "These lemmas almost directly follow from previous results such as [9, 11], and are more similar to [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 8, "context": "1 of [9]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "Most of these properties are analogous to their original variants in [9, 10], but here we take extra care also on negative eigenvalues and thus allowing M to be non-PSD.", "startOffset": 69, "endOffset": 76}, {"referenceID": 9, "context": "Most of these properties are analogous to their original variants in [9, 10], but here we take extra care also on negative eigenvalues and thus allowing M to be non-PSD.", "startOffset": 69, "endOffset": 76}, {"referenceID": 2, "context": "w> a va \u2212 \u03b5\u03031 \u2208 [7 8 \u03bbmax(C (s\u22121))\u2212 2\u03b5\u03031, \u03bbmax(C(s\u22121)) ] \u2286 [3 4 \u03bbmax(C (s\u22121)), \u03bbmax(C(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) \u2212 \u03bbmax(M) , and", "startOffset": 96, "endOffset": 106}, {"referenceID": 3, "context": "w> a va \u2212 \u03b5\u03031 \u2208 [7 8 \u03bbmax(C (s\u22121))\u2212 2\u03b5\u03031, \u03bbmax(C(s\u22121)) ] \u2286 [3 4 \u03bbmax(C (s\u22121)), \u03bbmax(C(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) \u2212 \u03bbmax(M) , and", "startOffset": 96, "endOffset": 106}, {"referenceID": 0, "context": "w> a va \u2212 \u03b5\u03031 \u2208 [7 8 \u03bbmax(C (s\u22121))\u2212 2\u03b5\u03031, \u03bbmax(C(s\u22121)) ] \u2286 [3 4 \u03bbmax(C (s\u22121)), \u03bbmax(C(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) \u2212 \u03bbmax(M) , and", "startOffset": 96, "endOffset": 106}, {"referenceID": 2, "context": "w> b vb \u2212 \u03b5\u03031 \u2286 [3 4 \u03bbmax(D (s\u22121)), \u03bbmax(D(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) + \u03bbmin(M) .", "startOffset": 53, "endOffset": 63}, {"referenceID": 3, "context": "w> b vb \u2212 \u03b5\u03031 \u2286 [3 4 \u03bbmax(D (s\u22121)), \u03bbmax(D(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) + \u03bbmin(M) .", "startOffset": 53, "endOffset": 63}, {"referenceID": 0, "context": "w> b vb \u2212 \u03b5\u03031 \u2286 [3 4 \u03bbmax(D (s\u22121)), \u03bbmax(D(s\u22121)) ] = [3 4 , 1 ] \u00b7 1 \u03bb(s\u22121) + \u03bbmin(M) .", "startOffset": 53, "endOffset": 63}, {"referenceID": 1, "context": "Many of these lemmas are analogous to those ones used in the SVD algorithm by the same authors of this paper [2], however, we need some extra care in this paper because the underlying matrix M is no longer PSD.", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "In particular, Chebyshev method [6] uses the so-called Chebyshev polynomial for this purpose, and the number of matrix-vector multiplications is determined by the degree of that polynomial.", "startOffset": 32, "endOffset": 35}, {"referenceID": 18, "context": "Our reduction is based on an inexact variant of the accelerated gradient descent (AGD) method originally put forward by Nesterov [20], which relies on some convex optimization techniques and can be proved using the linear-coupling framework [3].", "startOffset": 129, "endOffset": 133}, {"referenceID": 2, "context": "Our reduction is based on an inexact variant of the accelerated gradient descent (AGD) method originally put forward by Nesterov [20], which relies on some convex optimization techniques and can be proved using the linear-coupling framework [3].", "startOffset": 241, "endOffset": 244}, {"referenceID": 22, "context": "Indeed, for instance in the ALS algorithm of [24] for solving CCA, the authors obtained a running time proportional to 1/gap although there are only 1/gap iterations.", "startOffset": 45, "endOffset": 49}, {"referenceID": 2, "context": "2 can be proved using the linear-coupling framework of [3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "Indeed, suppose for instance \u2016\u03be\u2032 s\u2016 = c for some c \u2208 [0, 1].", "startOffset": 53, "endOffset": 59}], "year": 2017, "abstractText": "We study k-GenEV, the problem of finding the top k generalized eigenvectors, and k-CCA, the problem of finding the top k vectors in canonical-correlation analysis. We propose algorithms LazyEV and LazyCCA to solve the two problems with running times linearly dependent on the input size and on k. Furthermore, our algorithms are doubly-accelerated : our running times depend only on the square root of the matrix condition number, and on the square root of the eigengap. This is the first such result for both k-GenEV or k-CCA. We also provide the first gap-free results, which provide running times that depend on 1/ \u221a \u03b5 rather than the eigengap.", "creator": "LaTeX with hyperref package"}}}