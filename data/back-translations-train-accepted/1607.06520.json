{"id": "1607.06520", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2016", "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings", "abstract": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to \"debias\" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.", "histories": [["v1", "Thu, 21 Jul 2016 22:26:20 GMT  (577kb,D)", "http://arxiv.org/abs/1607.06520v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG stat.ML", "authors": ["tolga bolukbasi", "kai-wei chang", "james y zou", "venkatesh saligrama", "adam tauman kalai"], "accepted": true, "id": "1607.06520"}, "pdf": {"name": "1607.06520.pdf", "metadata": {"source": "CRF", "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings", "authors": ["Tolga Bolukbasi", "Kai-Wei Chang", "James Zou", "Venkatesh Saligrama", "Adam Kalai"], "emails": ["tolgab@bu.edu,", "kw@kwchang.net,", "jamesyzou@gmail.com,", "srv@bu.edu,", "adam.kalai@microsoft.com"], "sections": [{"heading": null, "text": "We show that even word embeddings trained on Google News articles have disturbingly female / male gender stereotypes, raising concerns because the widespread use we describe often tends to amplify these distortions. Geometrically, it turns out that gender stereotypes are first captured by a direction in the word embeddedness. Second, gender-neutral words are presented as linearly separable from gender-specific definition words in the word embedded.Using these characteristics, we provide a method to modify an embeddedness to eliminate gender stereotypes, such as the link between the words receptionist and female, while maintaining desired associations such as between the words queen and woman. We define metrics to quantify both direct and indirect gender-specific distortions in embeddeeds, and develop algorithms to \"impair embeddeedness.\""}, {"heading": "1 Introduction", "text": "In fact, most people are able to save themselves, and they are able to save themselves, \"he said.\" I don't think they're able to save me, \"he said.\" But I don't think they're able to save me. \"He added,\" I don't think they're able to save me. \"He added,\" I don't think they're able to save me. \"He added,\" I don't think they're able to save me. \"He added,\" I don't think I'm able to save myself. \""}, {"heading": "2 Related work", "text": "Related work can be divided into relevant literature on bias in language and bias in algorithms.2The terminology of indirect bias follows Pedreshi et al. [29], which distinguishes between direct and indirect discrimination in the rules of fair classifiers. Direct discrimination involves the direct use of sensitive traits such as gender or race, while indirect discrimination involves the use of relates that are not inherently based on sensitive traits but deliberately or inadvertently lead to disproportionate treatment. 3http: / / mturk.com"}, {"heading": "2.1 Gender bias and stereotype in English", "text": "It is important to quantify and understand bias in languages, as such biases can increase the psychological status of different groups [33]. Language bias has been studied over a number of decades in a variety of contexts (see e.g. [17]), and we highlight only some of the results here. Biases differ between people, although similarities can be found. Implicit association tests [15] have uncovered gender biases that people do not report themselves and that they may not even be aware of. Shared bias links female concepts with liberal arts and family and male concepts with science and career [28]. Biases are seen in word morphology, i.e. the fact that words like actors are by default associated with the dominant class [19], and female versions of these words, e.g. actress, are characterized by an imbalance in the number of words associated with F-M associations."}, {"heading": "2.2 Bias within algorithms", "text": "A number of online systems have shown that they exhibit various distortions, such as racial discrimination and gender bias in the ads presented to users [36, 6]. A recent study found that algorithms to predict repeat offenders exhibit indirect racial distortions [1]. However, different demographic and geographic groups also use different dialects and word choices on social media [8]. One implication of this effect is that language used by minority groups may not be processed by natural language tools trained on \"standard\" data sets. Distortions in the processing of machine learning data sets have been investigated in [37, 4]. Regardless of our work, Schmidt [34] identified the bias of word embedding and proposed distortion by completely eliminating multiple gender dimensions, one for each gender pair. His goal and approach, similar but simpler than ours, is to completely remove it from the gender."}, {"heading": "3 Preliminaries", "text": "\"It's as if he was able to put himself at the top,\" he says. \"It's as if he was able to put himself at the top.\" \"It's as if he was able to put himself at the top,\" he says. \"It's as if he was able to put himself at the top,\" he says. \"It's as if he was able to put himself at the top,\" he says."}, {"heading": "4 Gender stereotypes in word embeddings", "text": "We asked the crowd workers whether a cast was considered feminine, male-stereotypical or neutral. We asked the crowd workers whether a cast was considered female or not. We asked the crowd workers whether a cast was considered female. We asked them whether a cast was considered female or whether it was considered female. We asked the crowd workers whether a cast was considered female or whether it was considered female. We asked the crowd workers whether a cast was considered female, male-stereotypical or neutral. We asked the crowd workers whether a cast was considered female or whether it was considered female. We asked the crowd workers whether a cast was considered female, male-stereotypical or neutral. We asked the crowd workers whether a cast was considered female, male-stereotypical or neutral."}, {"heading": "5 Geometry of Gender and Bias", "text": "In this section, we examine the bias present in the geometric embedding, identify the gender direction and quantify the bias regardless of whether it is consistent with the crowd bias. We develop metrics of direct and indirect bias that more strictly quantify the observations of the previous section."}, {"heading": "5.1 Identifying the gender subspace", "text": "In fact, most of them are able to survive on their own if they do not see themselves as able to survive on their own."}, {"heading": "5.2 Direct bias", "text": "In order to measure the direct bias, we first identify words that should be gender neutral for the application in question, and how this sentence of gender-neutral words can be generated, in Section 7. Given the gender-neutral words denoted by N and the gender-specific direction we have learned from above, g, we define the direct gender bias of anembedding to beDirectBiasc = 1 | N | \u2211 w, N | cos (~ w, g) | cwhere c is a parameter that determines how strictly we want to measure bias. If c is 0, then | cos (~ w \u2212 g) | c = 0 is only if ~ w does not overlap with g, and otherwise it is 1. Such a strict measurement of bias may be desirable in contexts such as the college admission example from the introduction, where it would be unacceptable for the embedding to introduce a slight bias for a candidate against a different gender."}, {"heading": "5.3 Indirect bias", "text": "In fact, the fact is that most of us will be able to be able to be able to be able to be, and that they will be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be."}, {"heading": "6 Debiasing algorithms", "text": "The first step, called Identify gender subspace, is to identify a direction (or, more generally, a subspace) of embedding that captures the bias. For the second step, we define two options: Neutralize and Equalize or Soft. Neutralize ensures that gender-neutral words are zero in the gender subspace. Equalize perfect equalize of words outside the subspace and thus the property that every neutral word is equal."}, {"heading": "7 Determining gender neutral words", "text": "For practical purposes, since there are many less gender-specific words, it is more efficient to enumerate the set of gender-specific words S and use the gender-neutral words as a compliment, N = W\\ S. Using dictionary definitions, we derive a subset S0 of 218 words from the words in w2vNEWS. Remember that this embedding is a subset of 26.377 words out of the full 3 million words in the embedding, as described in Section 3. Note that the word choice is subjective and ideally should be tailored to the application. We generalize this list to the entire 3 million words in the embedding, as described in Section 3."}, {"heading": "8 Debiasing results", "text": "In fact, the fact is that most of them are able to play by the rules that they have established over the past few years, and that they are able to play by the rules, \"he said.\" But it's not as if they have been able to play by the rules, \"he said.\" But it's not as if they have been able to play by the rules. \""}, {"heading": "9 Discussion", "text": "In fact, we are able to put ourselves in a situation where we are able to put ourselves in a situation where we are able to survive it."}, {"heading": "A Generating analogies", "text": "We will now expand various possible methods for creating (x, y) pairs, the (a, b) for generating analogies a: x: b: y. The first and simplest metric is to consider the evaluation of an analogy by (~ a \u2212 \u2212 b) \u2212 (~ x \u2212 y) \u2212 (~ x \u2212 y), which can be called a parallelogram approach and, for the purpose of finding the best single y given unit length, b \u2212 x, it is equivalent to the most common approach to locating single word analogies, namely maximizing cos (~ y, ~ x + ~ b \u2212 a). Assuming that all vectors are unit length, x \u2212 mmys. This works well in some cases, but one weakness is that for many triples (a, b, x), the closest word to x = x, i.e. x = arg is miny: x xx."}, {"heading": "B Learning the linear transform", "text": "In the soft debiasing algorithm, we have to solve the following optimization problem. (Min.) T (TW) T (TW) T (TW) - WTW (WTW) - 2F (TN) T (TB) - 2F. (3) The first term ensures that the paired inner products are retained and the second term induces the distortions of gender-neutral words in the gender subregion to be small. The user-specific parameter \u03bb balances the two terms. (3) The direct solution of this SDP optimization problem is challenging. In practice, the dimension of the matrix W is on the scale of 300 x 400,000. The dimensions of the matrix T - 2W - 2W - 2W - W. The dimensions of the matrix F - 2W - X - W - W - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - X - W - W - W - W."}, {"heading": "C Details of gender specific words base set", "text": "In this section, we find exact details about how we derived our list of gender-neutral words, although the choice of gender-neutral words is partially subjective. Some words are most commonly associated with women or men, but have exceptions, such as beard (bearded women), estrogen (men have small amounts of the hormone estrogen), and rabbi (reformed Jewish communities recognize female rabbi). There are also many words that have multiple senses, some of which are gender-neutral and others gender-specific. For example, the profession of nursing is gender-neutral while nursing a baby (i.e., breastfeeding is performed only by women). To derive the subset of words from w2vNEWS, for each of the 26.377 words in the filtered embed, we have selected words, whose definitions include the genders, whose definitions include the genders, male, female, female, female, female, female, female, female, female, female, female, female, female, female, female, female, male, male, male, male, male, male, male, male, male, female, female, female, male."}, {"heading": "D Questionnaire for generating gender stereotypical words", "text": "Task: For each category, please enter 10 or more words separated by commas. We are looking for a variety of creative answers - this is a mentally challenging HIT that will make you think. \u2022 10 or more comma-separated words that are definitely associated with men. Examples: dude, men's clothing, king, penis,... \u2022 10 or more comma-separated words that are definitely associated with women. Examples: queen, Jane, girl,... \u2022 10 or more comma-separated words that are stereotypically associated with men Examples: football, janitor, cocky,... \u2022 10 or more comma-separated words that are stereotypically associated with women Examples: pink, sewing, caring, naughty, nurse,... Thank you for your help in making artificially intelligent systems that have no bias.: -)"}, {"heading": "E Questionnaire for generating gender stereotypical analogies", "text": "An analogy describes two pairs of words where the relationship between the two words in each pair is the same. An example of an analogy is apple is fruit as asparagus is vegetable (referred to as apple: fruit:: asparagus: vegetables). We need your help to improve our analogy generating system.Task: Please enter 10 or more analogies that reflect gender stereotypes, separated by quotes. We are looking for a variety of creative answers - this is a mentally challenging HIT that makes you think. Examples of stereotypes \u2022 large: man:: in short: woman reflects a cultural stereotype that men are tall and women are short. \u2022 Doctor: man: nurse: woman reflects a stereotype that doctors are typically men and nurses are typically women."}, {"heading": "F Questionnaire for rating stereotypical analogies", "text": "An analogy describes two pairs of words in which the relationship between the two words is the same in each pair. An example of an analogy is apple is fruit as asparagus is vegetable (referred to as apple: fruit:: asparagus: vegetables). We need your help to improve our analogy generation.Task: What analogies are stereotypes? What are appropriate analogies? \u2022 Examples of stereotypical analogies large: man::: short: saleswoman: man:: nurse: woman \u2022 Examples of appropriate analogies King: man:: queen: wife brother: man:: sister: womenJohn: man:: Maria: womenSeine: man::: Hers: female saleswoman: man:: saleswoman penis: man:: vagina: woman WARNING: This HIT may contain adult content."}, {"heading": "G Analogies Generated by Word Embeddings", "text": "Women: women: women: women: women: women: women: women: women: women: women: women: women: women: women: women: women: women: women: women: men: women: women: women: men: women: women: women: women: women: women: women: women: women: men: men: women: men: women: men: women: men: women: men: women: men: women: men: women: men: women: men: women: men: women: men: women: men: women: men: women: men: women"}, {"heading": "H Debiasing the full w2vNEWS embedding.", "text": "In the main text, we focused on the results of a cleansed version of w2vNEWS, consisting of 26,377 lowercase words. Furthermore, we applied our hard debugging algorithm to the complete w2vNEWS dataset. Evaluation based on the standard metrics shows that the disadvantage does not affect the usefulness of the embedding (Table 3)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>The blind application of machine learning runs the risk of amplifying biases present in data. Such a<lb>danger is facing us with word embedding, a popular framework to represent text data as vectors which<lb>has been used in many machine learning and natural language processing tasks. We show that even<lb>word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing<lb>extent. This raises concerns because their widespread use, as we describe, often tends to amplify these<lb>biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding.<lb>Second, gender neutral words are shown to be linearly separable from gender definition words in the word<lb>embedding. Using these properties, we provide a methodology for modifying an embedding to remove<lb>gender stereotypes, such as the association between between the words receptionist and female, while<lb>maintaining desired associations such as between the words queen and female. We define metrics to<lb>quantify both direct and indirect gender biases in embeddings, and develop algorithms to \u201cdebias\u201d the<lb>embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate<lb>that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties<lb>such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can<lb>be used in applications without amplifying gender bias.", "creator": "LaTeX with hyperref package"}}}