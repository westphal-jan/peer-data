{"id": "1607.07057", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jul-2016", "title": "Latent Tree Language Model", "abstract": "It this paper we introduce Latent Tree Language Model (LTLM), a novel approach to language modeling that encodes syntax and semantics of a given sentence as a tree of word roles.", "histories": [["v1", "Sun, 24 Jul 2016 15:40:36 GMT  (178kb,D)", "https://arxiv.org/abs/1607.07057v1", "Submitted to EMNLP 2016"], ["v2", "Mon, 29 Aug 2016 12:35:24 GMT  (178kb,D)", "http://arxiv.org/abs/1607.07057v2", "Accepted to EMNLP 2016"], ["v3", "Mon, 5 Sep 2016 14:47:18 GMT  (172kb,D)", "http://arxiv.org/abs/1607.07057v3", "Accepted to EMNLP 2016"]], "COMMENTS": "Submitted to EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tomas brychcin"], "accepted": true, "id": "1607.07057"}, "pdf": {"name": "1607.07057.pdf", "metadata": {"source": "CRF", "title": "Latent Tree Language Model", "authors": ["Tom\u00e1\u0161 Brychc\u0131\u0301n"], "emails": ["brychcin@kiv.zcu.cz"], "sections": [{"heading": null, "text": "In the learning phase, we update the trees iteratively by moving nodes to Gibbs sampling. We introduce two algorithms to infer a tree for a given set. The first is based on Gibbs sampling. It is fast but does not guarantee to find the most likely tree. The second is based on dynamic programming. It is slower but guarantees to find the most likely tree. We offer a comparison of both algorithms. We combine LTLM with a 4-gram modified Kneser-Ney language model using linear interpolation. Our experiments with English and Czech corpora show significant reductions in confusion (up to 46% for English and 49% for Czech) compared to the standalone 4-gram modified Kneser-Ney language model."}, {"heading": "1 Introduction", "text": "It is one of the core disciplines in natural language processing (NLP). Automatic speech recognition, machine translation, optical character recognition and other tasks depend heavily on the language model (LM). Improving language modelling often leads to better performance of the entire task. However, the goal of language modelling is to determine the common probability of a sentence. Currently, the dominant approach is n-gram language modelling, which decomposes the common probability of words into the product of conditional probabilities by using the word strings. In traditional n-gram LMs, the words are presented as different symbols, resulting in an enormous number of word combinations. In recent years, many researchers have tried to capture words of contextual meaning and integrate them into the LMs. Word sequences that have never before received a high probability of receiving words that are semantically similar to words that form sentences seen in training data. This ability can increase LM performance because it reduces the data saving problem."}, {"heading": "2 Latent Tree Language Model", "text": "In this section, we describe Latent Tree Language Model (LTLM). LTLM is a generative statistical model that reveals the tree structures hidden in the text corpus. Let L be a word vocabulary with the totality of the different words. Suppose we have a sequence of words divided into S sentences. The goal of LTLM or other LMs is to estimate the probability of a text P (w). Let Ns specify the number of words in the s-th sentence. The s-th sentence is a sequence of words = {rs, i} Nsi = 0, where ws, i-L is a word at position i in that sentence, and ws, 0 = < s > is an artificial symbol added at the beginning of each sentence. Each sentence s is associated with the dependency graph Gs. We define the dependency graph as a highlighted directed graph where nodes correspond to the words in the sentence and there is a name."}, {"heading": "3 Parameter Estimation", "text": "In this section we will introduce the learning algorithm for LTLM. The aim is to estimate the predictive power and dependence in a way that maximizes the predictive power of the model (generates the corpus with maximum common probability P (w)). Let's let the predictive power (i, j) be an operation that changes the treeGs (i, j): Gs \u2192 G \u2032 s, (3) so that the newly created tree G \u2032 s, a = s, r \u2032 s) consists of: \u2022 V \u2032 s = V s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s"}, {"heading": "4 Inference", "text": "In this section, we present two approaches to finding the most likely tree for a given proposition, provided we have already estimated the parameters \u03b8 and \u03c6."}, {"heading": "4.1 Non-deterministic Inference", "text": "We use the same scanning technique as for estimating parameters (equation 8), i.e. we use random partial variations \u03c7k (i, j). However, we use equations 9 and 10 for the predictive distributions of Dirichlet multinomials instead of 5 and 6. In fact, these equations correspond to the predictive distributions for the newly added words, i with the role rs, i in the corpus, conditioned by w and r. This scanning technique rarely finds the best solution, but often it is very close."}, {"heading": "4.2 Deterministic Inference", "text": "Here we present the deterministic algorithm that guarantees to find the most likely tree for a given sentence = ci = a = b = | b = | | we were inspired by the algorithm of Cocke-YoungerKasami (CYK) (Lange and Lei\u00df, 2009).Let T ns, a, c denote the sub-tree of Gs (sub-tree of Gs, which is also a tree), which contains a sub-sequence of nodes {a, a + 1,..., c}.The high proposition n denotes the number of children that have root of this sub-tree. We denote the common probability of a sub-tree from position a to position c with the corresponding words sequenced by the root roll k as Pn ({ws, i},..., c} s."}, {"heading": "5 Side-dependent LTLM", "text": "So far, we have presented LTLM in its simplified version. In role-by-role probabilities (role-by-role determined by their parent role), we have not distinguished whether the role is on the left or on the right side of the parent. However, this position contains important information about the syntax of words (and their roles). We assume separate multinomial distributions for roles that are on the left and on the right side, each of which has its own funnel with hyperparameters \u03b1 or \u03b1 beforehand. The process of estimating LTLM parameters is almost the same. The only difference is that we have to redefine the predictive distribution for the role-by-role distribution (Eq.6) in order to count only the roles on the corresponding page. Also, we have to distinguish pages every time the role-by-role probability is used: P (rs, i | rs, hs (i)) = {."}, {"heading": "6 Experimental Results and Discussion", "text": "In this section, we present the experiments with LTLM in two languages that are used less frequently than the words we use. (It is only a part of them, because there are several uses that have a reasonable length, and it is paradoxical that we have removed all sentences of more than 30 words.) The reason was that the complexity of the learning phase and the process of searching for the most likely trees depends on the length of the sentences. It has resulted in about a quarter of all sentences being removed. The corpus is available in a tokenized form, so the only processing stage we use is lowercasing. We keep the vocabularies of 100,000 most common words in the Corpus language that we use."}, {"heading": "7 Conclusion and Future Work", "text": "Our experiments with English and Czech corpora showed dramatic improvements in predictive ability compared to stand-alone Modified Kneser-Ney LM. Our Java implementation is available for research purposes at https: / / github.com / brychcin / LTLM. It was outside the scope of this work to explicitly test the semantic and syntactic properties of the model. As the main direction for future work, we plan to investigate these properties, for example, by comparing them with humanly assigned judgments. In addition, we would like to test our model in various NLP tasks (e.g. speech recognition, machine translation, etc.).We believe that the role-by-role distribution should depend on the distance between parent and child, but our preliminary experiments have not been successful."}, {"heading": "Acknowledgments", "text": "This publication was supported by the LO1506 project of the Czech Ministry of Education, Youth and Sport. Computer resources were provided by CESNET LM2015042 and CERIT Scientific Cloud LM2015085 as part of the \"Projects of Large Research, Development and Innovation Infrastructures\" programme. Finally, we would like to thank the anonymous reviewers for their insightful feedback."}], "references": [{"title": "A neural probabilistic lan", "author": ["Christian Janvin"], "venue": null, "citeRegEx": "Janvin.,? \\Q2003\\E", "shortCiteRegEx": "Janvin.", "year": 2003}, {"title": "Correlated topic models", "author": ["David M. Blei", "John D. Lafferty."], "venue": "In Proceedings of the 23rd International Conference on Machine Learning, pages 113\u2013 120. MIT Press.", "citeRegEx": "Blei and Lafferty.,? 2006", "shortCiteRegEx": "Blei and Lafferty.", "year": 2006}, {"title": "The joy of parallelism with czeng 1.0", "author": ["Ond\u0159ej Bojar", "Zden\u011bk \u017dabokrtsk\u00fd", "Ond\u0159ej Du\u0161ek", "Petra Galu\u0161\u010d\u00e1kov\u00e1", "Martin Majli\u0161", "David Mare\u010dek", "Ji\u0159\u0131\u0301 Mar\u0161\u0131\u0301k", "Michal Nov\u00e1k", "Martin Popel", "Ale\u0161 Tamchyna"], "venue": "In Proceedings of the Eight International Conference", "citeRegEx": "Bojar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2012}, {"title": "Classbased n-gram models of natural language", "author": ["Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai."], "venue": "Computational Linguistics, 18:467\u2013479.", "citeRegEx": "Brown et al\\.,? 1992", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Semantic spaces for improving language modeling", "author": ["Tom\u00e1\u0161 Brychc\u0131\u0301n", "Miloslav Konop\u0131\u0301k"], "venue": "Computer Speech & Language,", "citeRegEx": "Brychc\u0131\u0301n and Konop\u0131\u0301k.,? \\Q2014\\E", "shortCiteRegEx": "Brychc\u0131\u0301n and Konop\u0131\u0301k.", "year": 2014}, {"title": "Latent semantics in language models", "author": ["Tom\u00e1\u0161 Brychc\u0131\u0301n", "Miloslav Konop\u0131\u0301k"], "venue": "Computer Speech & Language,", "citeRegEx": "Brychc\u0131\u0301n and Konop\u0131\u0301k.,? \\Q2015\\E", "shortCiteRegEx": "Brychc\u0131\u0301n and Konop\u0131\u0301k.", "year": 2015}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F. Chen", "Joshua T. Goodman."], "venue": "Technical report, Computer Science Group, Harvard University.", "citeRegEx": "Chen and Goodman.,? 1998", "shortCiteRegEx": "Chen and Goodman.", "year": 1998}, {"title": "Logistic normal priors for unsupervised probabilistic grammar induction", "author": ["Shay B. Cohen", "Kevin Gimpel", "Noah A. Smith."], "venue": "Advances in Neural Information Processing Systems 21, pages 1\u20138.", "citeRegEx": "Cohen et al\\.,? 2009", "shortCiteRegEx": "Cohen et al\\.", "year": 2009}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["Arthur P. Dempster", "N.M. Laird", "D.B. Rubin."], "venue": "Journal of the Royal Statistical Society. Series B, 39(1):1\u201338.", "citeRegEx": "Dempster et al\\.,? 1977", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "The latent words language model", "author": ["Koen Deschacht", "Jan De Belder", "Marie-Francine Moens."], "venue": "Computer Speech & Language, 26(5):384\u2013409.", "citeRegEx": "Deschacht et al\\.,? 2012", "shortCiteRegEx": "Deschacht et al\\.", "year": 2012}, {"title": "Distributional structure", "author": ["Zellig Harris."], "venue": "Word, 10(23):146\u2013162.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Improving unsupervised dependency parsing with richer contexts and smoothing", "author": ["William P. Headden III", "Mark Johnson", "David McClosky."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of", "citeRegEx": "III et al\\.,? 2009", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Dependency parsing", "author": ["Sandra K\u00fcbler", "Ryan McDonald", "Joakim Nivre."], "venue": "Synthesis Lectures on Human Language Technologies, 2(1):1\u2013127.", "citeRegEx": "K\u00fcbler et al\\.,? 2009", "shortCiteRegEx": "K\u00fcbler et al\\.", "year": 2009}, {"title": "To cnf or not to cnf? an efficient yet presentable version of the cyk algorithm", "author": ["Martin Lange", "Hans Lei\u00df."], "venue": "Informatica Didactica, 8.", "citeRegEx": "Lange and Lei\u00df.,? 2009", "shortCiteRegEx": "Lange and Lei\u00df.", "year": 2009}, {"title": "Dependencybased word embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302\u2013308, Baltimore, Maryland, June. Association for Computa-", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Stopprobability estimates computed on a large corpus improve unsupervised dependency parsing", "author": ["David Mare\u010dek", "Milan Straka."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Mare\u010dek and Straka.,? 2013", "shortCiteRegEx": "Mare\u010dek and Straka.", "year": 2013}, {"title": "Algorithms for bigram and trigram word clustering", "author": ["Sven Martin", "Jorg Liermann", "Hermann Ney."], "venue": "Speech Communication, 24(1):19\u201337.", "citeRegEx": "Martin et al\\.,? 1998", "shortCiteRegEx": "Martin et al\\.", "year": 1998}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["Ryan McDonald", "Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d."], "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pages", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Estimating a dirichlet distribution", "author": ["Thomas P. Minka."], "venue": "Technical report.", "citeRegEx": "Minka.,? 2003", "shortCiteRegEx": "Minka.", "year": 2003}, {"title": "Dependencybased construction of semantic space models", "author": ["Sebastian Pad\u00f3", "Mirella Lapata."], "venue": "Computational Linguistics, 33(2):161\u2013199, June.", "citeRegEx": "Pad\u00f3 and Lapata.,? 2007", "shortCiteRegEx": "Pad\u00f3 and Lapata.", "year": 2007}, {"title": "Perplexity of n-gram and dependency language models", "author": ["Martin Popel", "David Mare\u010dek."], "venue": "Proceedings of the 13th International Conference on Text, Speech and Dialogue, TSD\u201910, pages 173\u2013180, Berlin, Heidelberg. Springer-Verlag.", "citeRegEx": "Popel and Mare\u010dek.,? 2010", "shortCiteRegEx": "Popel and Mare\u010dek.", "year": 2010}, {"title": "Unsupervised dependency parsing without gold part-of-speech tags", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Angel X. Chang", "Daniel Jurafsky."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1281\u20131290, Ed-", "citeRegEx": "Spitkovsky et al\\.,? 2011", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2011}, {"title": "Language modelling for russian and english using words and classes", "author": ["Edward W.D. Whittaker", "Philip C. Woodland."], "venue": "Computer Speech & Language, 17(1):87\u2013104.", "citeRegEx": "Whittaker and Woodland.,? 2003", "shortCiteRegEx": "Whittaker and Woodland.", "year": 2003}], "referenceMentions": [{"referenceID": 10, "context": "It suggests that two words are expected to be semantically similar if they occur in similar contexts (they are similarly distributed in the text) (Harris, 1954).", "startOffset": 146, "endOffset": 160}, {"referenceID": 18, "context": "The most commonly used architectures are neural network LMs (Bengio et al., 2003; Mikolov et al., 2010; Mikolov et al., 2011) and class-based LMs.", "startOffset": 60, "endOffset": 125}, {"referenceID": 19, "context": "The most commonly used architectures are neural network LMs (Bengio et al., 2003; Mikolov et al., 2010; Mikolov et al., 2011) and class-based LMs.", "startOffset": 60, "endOffset": 125}, {"referenceID": 4, "context": "A similar approach was presented in (Brychc\u0131\u0301n and Konop\u0131\u0301k, 2014; Brychc\u0131\u0301n and Konop\u0131\u0301k, 2015), where the word clusters derived from various semantic spaces were used to improve LMs.", "startOffset": 36, "endOffset": 96}, {"referenceID": 5, "context": "A similar approach was presented in (Brychc\u0131\u0301n and Konop\u0131\u0301k, 2014; Brychc\u0131\u0301n and Konop\u0131\u0301k, 2015), where the word clusters derived from various semantic spaces were used to improve LMs.", "startOffset": 36, "endOffset": 96}, {"referenceID": 7, "context": "Deschacht et al. (2012) used the same idea and introduced Latent Words Language Model (LWLM), where word classes are latent variables in a graphical model.", "startOffset": 0, "endOffset": 24}, {"referenceID": 21, "context": "Resulting word representations are usually less topical and exhibit more functional similarity (they are more syntactically oriented) as shown in (Pad\u00f3 and Lapata, 2007; Levy and Goldberg, 2014).", "startOffset": 146, "endOffset": 194}, {"referenceID": 14, "context": "Resulting word representations are usually less topical and exhibit more functional similarity (they are more syntactically oriented) as shown in (Pad\u00f3 and Lapata, 2007; Levy and Goldberg, 2014).", "startOffset": 146, "endOffset": 194}, {"referenceID": 12, "context": "Dependency-based methods for syntactic parsing have become increasingly popular in NLP in the last years (K\u00fcbler et al., 2009).", "startOffset": 105, "endOffset": 126}, {"referenceID": 7, "context": "Recently, unsupervised algorithms for dependency parsing appeared in (Headden III et al., 2009; Cohen et al., 2009; Spitkovsky et al., 2010; Spitkovsky et al., 2011; Mare\u010dek and Straka, 2013) offering new possibilities even for poorly-resourced languages.", "startOffset": 69, "endOffset": 191}, {"referenceID": 23, "context": "Recently, unsupervised algorithms for dependency parsing appeared in (Headden III et al., 2009; Cohen et al., 2009; Spitkovsky et al., 2010; Spitkovsky et al., 2011; Mare\u010dek and Straka, 2013) offering new possibilities even for poorly-resourced languages.", "startOffset": 69, "endOffset": 191}, {"referenceID": 15, "context": "Recently, unsupervised algorithms for dependency parsing appeared in (Headden III et al., 2009; Cohen et al., 2009; Spitkovsky et al., 2010; Spitkovsky et al., 2011; Mare\u010dek and Straka, 2013) offering new possibilities even for poorly-resourced languages.", "startOffset": 69, "endOffset": 191}, {"referenceID": 10, "context": "Dependency-based methods for syntactic parsing have become increasingly popular in NLP in the last years (K\u00fcbler et al., 2009). Popel and Mare\u010dek (2010) showed that these methods are promising direction of improving LMs.", "startOffset": 106, "endOffset": 153}, {"referenceID": 9, "context": "In our work we were inspired by class-based LMs (Deschacht et al., 2012), unsupervised dependency parsing (Mare\u010dek and Straka, 2013), and tree-based DSMs (Levy and Goldberg, 2014).", "startOffset": 48, "endOffset": 72}, {"referenceID": 15, "context": ", 2012), unsupervised dependency parsing (Mare\u010dek and Straka, 2013), and tree-based DSMs (Levy and Goldberg, 2014).", "startOffset": 41, "endOffset": 67}, {"referenceID": 14, "context": ", 2012), unsupervised dependency parsing (Mare\u010dek and Straka, 2013), and tree-based DSMs (Levy and Goldberg, 2014).", "startOffset": 89, "endOffset": 114}, {"referenceID": 20, "context": "We use a fixed point iteration technique described in (Minka, 2003) to estimate them.", "startOffset": 54, "endOffset": 67}, {"referenceID": 13, "context": "We were inspired by Cocke-YoungerKasami (CYK) algorithm (Lange and Lei\u00df, 2009).", "startOffset": 56, "endOffset": 78}, {"referenceID": 2, "context": "0 (Bojar et al., 2012) of the sentence-parallel Czech-English corpus.", "startOffset": 2, "endOffset": 22}, {"referenceID": 6, "context": "We experiment with Modified Kneser-Ney (MKN) interpolation (Chen and Goodman, 1998), with Recurrent Neural Network LM (RNNLM) (Mikolov et al.", "startOffset": 59, "endOffset": 83}, {"referenceID": 18, "context": "We experiment with Modified Kneser-Ney (MKN) interpolation (Chen and Goodman, 1998), with Recurrent Neural Network LM (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2011)3, and with LWLM (Deschacht et al.", "startOffset": 126, "endOffset": 170}, {"referenceID": 19, "context": "We experiment with Modified Kneser-Ney (MKN) interpolation (Chen and Goodman, 1998), with Recurrent Neural Network LM (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2011)3, and with LWLM (Deschacht et al.", "startOffset": 126, "endOffset": 170}, {"referenceID": 9, "context": ", 2011)3, and with LWLM (Deschacht et al., 2012)4.", "startOffset": 24, "endOffset": 48}, {"referenceID": 17, "context": "MST parser (McDonald et al., 2005).", "startOffset": 11, "endOffset": 34}, {"referenceID": 8, "context": "We use the expectation maximization algorithm (Dempster et al., 1977) for the maximum likelihood estimate of \u03bb parameter on the development part of the corpus.", "startOffset": 46, "endOffset": 69}, {"referenceID": 14, "context": "Also it seems that LTLM is more syntactically oriented, which confirms claims from (Levy and Goldberg, 2014; Pad\u00f3 and Lapata, 2007), but to draw such conclusions a deeper analysis is required.", "startOffset": 83, "endOffset": 131}, {"referenceID": 21, "context": "Also it seems that LTLM is more syntactically oriented, which confirms claims from (Levy and Goldberg, 2014; Pad\u00f3 and Lapata, 2007), but to draw such conclusions a deeper analysis is required.", "startOffset": 83, "endOffset": 131}, {"referenceID": 1, "context": "For example, Blei and Lafferty (2006) showed that the logistic-normal distribution works well for topic modeling because it captures the correlations between topics.", "startOffset": 13, "endOffset": 38}], "year": 2016, "abstractText": "In this paper we introduce Latent Tree Language Model (LTLM), a novel approach to language modeling that encodes syntax and semantics of a given sentence as a tree of word roles. The learning phase iteratively updates the trees by moving nodes according to Gibbs sampling. We introduce two algorithms to infer a tree for a given sentence. The first one is based on Gibbs sampling. It is fast, but does not guarantee to find the most probable tree. The second one is based on dynamic programming. It is slower, but guarantees to find the most probable tree. We provide comparison of both algorithms. We combine LTLM with 4-gram Modified Kneser-Ney language model via linear interpolation. Our experiments with English and Czech corpora show significant perplexity reductions (up to 46% for English and 49% for Czech) compared with standalone 4-gram Modified Kneser-Ney language model.", "creator": "TeX"}}}