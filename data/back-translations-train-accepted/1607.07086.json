{"id": "1607.07086", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jul-2016", "title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a \\textit{critic} network that is trained to predict the value of an output token, given the policy of an \\textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.", "histories": [["v1", "Sun, 24 Jul 2016 20:05:07 GMT  (43kb,D)", "http://arxiv.org/abs/1607.07086v1", null], ["v2", "Tue, 26 Jul 2016 16:08:30 GMT  (43kb,D)", "http://arxiv.org/abs/1607.07086v2", null], ["v3", "Fri, 3 Mar 2017 15:43:52 GMT  (73kb,D)", "http://arxiv.org/abs/1607.07086v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dzmitry bahdanau", "philemon brakel", "kelvin xu", "anirudh goyal", "ryan lowe", "joelle pineau", "aaron courville", "yoshua bengio"], "accepted": true, "id": "1607.07086"}, "pdf": {"name": "1607.07086.pdf", "metadata": {"source": "CRF", "title": "An Actor-Critic Algorithm for Sequence Prediction", "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In many important areas of machine learning, the task is to develop a system that produces a sequence of discrete characters that require input. Recent work has shown that recurring neural networks (RNs) can provide excellent performance in many such tasks if trained to predict the next issue. This approach has been successfully applied in practice (Sutskever et al., 2015; Karpathy and Fei-Foucault et al., 2015), the labeling generation (Kiros et al., 2014; Vinyals et al., 2015; Xu et al., 2015; Foux and Foux., 2015)."}, {"heading": "2 Background", "text": "We look at the problem of learning to generate a source sequence Y = (y1,., yT). (yT) We look at the problem of learning to generate a source sequence Y = (y1,., yT). (yT) We look at the problem of learning to generate a source sequence Y = (y1). (yT) We look at the source sequence Y = (y) on the test group in which Y = h (X) is the predictive pair (X, Y). (X) It is assumed that the predictive pairs (X) are available for both training and testing. (RNN) produces a sequence of state vectors (s1,...., sT) in the face of a sequence of input vectors (e1,.,., eT) by starting from an initial s0 state."}, {"heading": "3 Actor-Critic for Sequence Prediction", "text": "It is not the first time that we have dealt with the question of whether we should deal with the question of whether we can answer this question at all. (...) It is not the first time that we have asked this question. (...) It is the second time that we have asked this question. (...) It is not the first time that we have asked this question. (...) It is the second time that we have asked this question. (...) It is the second time that we have asked this question. (...) It is the second time that we have asked this question. (...) It is the second time that we have asked the question. (...) It is the second time that we have asked the question. (...) It is the second time that we have asked the question. (...) It is the first time that we have asked the question. (...) It is the second time that we have asked the question. (...) It is the second time that we have asked the question. (...) It is the second time that we have asked the question. (...) It is the second time that we have asked the question. (... It is the second time that we have asked the question."}, {"heading": "4 Related Work", "text": "In other recent RL-inspired work on sequence prediction, Ranzato et al. (2015) have trained a translation model by gradually learning from maximum probability to optimize BLEU- or ROUGE-results using the REINFORCE algorithm. However, it is known that this policy gradient estimator has a very high variance and does not exploit the availability of soil truth in the way that the critical network does. Again, these systems do not use the basic truth for value prediction. Imitation learning has also been applied to structured predictions (Vlachos, 2012). Successful methods of this kind have also been applied to structured predictions (Maes et al., 2009)."}, {"heading": "5 Experiments", "text": "To validate our approach, we conducted two experiments: First, we trained the proposed model for recovering strings of natural text from their corrupted versions. Specifically, we consider each figure in a natural language corpus and, with some probability, as a substitute for a random character. We call this synthetic task spell correction, even though the number of misspellings is much higher than for the average person. A desirable feature of this synthetic task is that the data is essentially infinite and does not need to be overly taken. Our second experiment is performed on the task of translating from German to English. We use data from the One Billion Word dataset for the spell task (Chelba et al, 2013), which has predefined training and test sentences. The training data was abundant, and we have never used any example twice. We evaluate trained models on a section of test data containing 6075 sentences."}, {"heading": "6 Discussion", "text": "We have shown that our method leads to significant improvements over the highest probability training for both a synthetic task and a machine translation benchmark. Compared to the REINFORCE training for machine translation, the actor-critic matches the training data much faster and also leads to significantly better final performance. Finally, we have qualitatively shown that the critic actually learns to assign high values to words that make sense in the given context. On the synthetic task, we found that the difference in the character error rate between maximum probability and our method increases when we increase the difficulty of the task. This is in line with our expectations, as the maximum probability should suffer the most if there is a greater discrepancy between the basic probability sequences it has trained and the sequences it can produce. Our system has also achieved a significantly higher BLEU score than the basic data of our task."}, {"heading": "Acknowledgments", "text": "We thank the developers of Theano (Theano Development Team, 2016) and Blocks (van Merrie \ufffd nboer et al., 2015) for their great work. We thank NSERC, Compute Canada, Calcul Quebe \ufffd c, Canada Research Chairs, CIFAR and Samsung Institute of Advanced Techonology for their support."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["Barto et al.1983] Andrew G Barto", "Richard S Sutton", "Charles W Anderson"], "venue": "Systems, Man and Cybernetics, IEEE Transactions", "citeRegEx": "Barto et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1983}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks. arXiv preprint arXiv:1506.03099", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Report on the 11th iwslt evaluation campaign", "author": ["Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Marcello Federico"], "venue": "In Proc. of IWSLT", "citeRegEx": "Cettolo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Listen, attend and spell", "author": ["Chan et al.2015] William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "arXiv preprint arXiv:1312.3005", "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Attention-based models for speech recognition. CoRR, abs/1506.07503", "author": ["Dzmitry Bahdanau", "Dmitriy Serdyuk", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Search-based structured prediction", "author": ["John Langford", "Daniel Marcu"], "venue": "Machine learning,", "citeRegEx": "Iii et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Iii et al\\.", "year": 2009}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": null, "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Minimum bayes-risk automatic speech recognition", "author": ["Goel", "Byrne2000] Vaibhava Goel", "William J Byrne"], "venue": "Computer Speech & Language,", "citeRegEx": "Goel et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Goel et al\\.", "year": 2000}, {"title": "Firstpass large vocabulary continuous speech recognition using bi-directional recurrent dnns", "author": ["Hannun et al.2014] Awni Y Hannun", "Andrew L Maas", "Daniel Jurafsky", "Andrew Y Ng"], "venue": "arXiv preprint arXiv:1408.2873", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Direct loss minimization for structured prediction", "author": ["Hazan et al.2010] Tamir Hazan", "Joseph Keshet", "David A McAllester"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hazan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Fei-Fei2015] Andrej Karpathy", "Li FeiFei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik P Kingma", "Jimmy Ba"], "venue": "In International Conference on Learning Representation", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Automatic evaluation of summaries using ngram co-occurrence statistics", "author": ["Lin", "Hovy2003] Chin-Yew Lin", "Eduard Hovy"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Structured prediction with reinforcement learning", "author": ["Maes et al.2009] Francis Maes", "Ludovic Denoyer", "Patrick Gallinari"], "venue": "Machine learning,", "citeRegEx": "Maes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maes et al\\.", "year": 2009}, {"title": "Neural networks for control", "author": ["Paul J Werbos", "Richard S Sutton"], "venue": null, "citeRegEx": "Miller et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1995}, {"title": "Human-level control through deep reinforcement", "author": ["Mnih et al.2015] Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732", "author": ["Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross et al.2010] St\u00e9phane Ross", "Geoffrey J Gordon", "J Andrew Bagnell"], "venue": "arXiv preprint arXiv:1011.0686", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Minimum risk training for neural machine translation", "author": ["Shen et al.2015] Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "arXiv preprint arXiv:1512.02433", "citeRegEx": "Shen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to reinforcement learning, volume 135", "author": ["Sutton", "Barto1998] Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "In NIPS,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Temporal credit assignment in reinforcement learning", "author": ["Richard Stuart Sutton"], "venue": null, "citeRegEx": "Sutton.,? \\Q1984\\E", "shortCiteRegEx": "Sutton.", "year": 1984}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Richard S Sutton"], "venue": "Machine learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Td-gammon, a self-teaching backgammon program, achieves masterlevel play", "author": ["Gerald Tesauro"], "venue": "Neural computation,", "citeRegEx": "Tesauro.,? \\Q1994\\E", "shortCiteRegEx": "Tesauro.", "year": 1994}, {"title": "An analysis of temporaldifference learning with function approximation", "author": ["Tsitsiklis", "Van Roy1997] John N Tsitsiklis", "Benjamin Van Roy"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Tsitsiklis et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis et al\\.", "year": 1997}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "An investigation of imitation learning algorithms for structured prediction", "author": ["Andreas Vlachos"], "venue": "In EWRL,", "citeRegEx": "Vlachos.,? \\Q2012\\E", "shortCiteRegEx": "Vlachos.", "year": 2012}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Learning simple algorithms from examples. arXiv preprint arXiv:1511.07275", "author": ["Tomas Mikolov", "Armand Joulin", "Rob Fergus"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "This approach has been applied successfully in machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Kiros et al.", "startOffset": 67, "endOffset": 114}, {"referenceID": 0, "context": "This approach has been applied successfully in machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Kiros et al.", "startOffset": 67, "endOffset": 114}, {"referenceID": 9, "context": ", 2015), caption generation (Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015), and speech recognition (Chorowski et al.", "startOffset": 28, "endOffset": 137}, {"referenceID": 36, "context": ", 2015), caption generation (Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015), and speech recognition (Chorowski et al.", "startOffset": 28, "endOffset": 137}, {"referenceID": 39, "context": ", 2015), caption generation (Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015), and speech recognition (Chorowski et al.", "startOffset": 28, "endOffset": 137}, {"referenceID": 7, "context": ", 2015; Karpathy and Fei-Fei, 2015), and speech recognition (Chorowski et al., 2015; Chan et al., 2015).", "startOffset": 60, "endOffset": 103}, {"referenceID": 4, "context": ", 2015; Karpathy and Fei-Fei, 2015), and speech recognition (Chorowski et al., 2015; Chan et al., 2015).", "startOffset": 60, "endOffset": 103}, {"referenceID": 2, "context": "During this search, the model is conditioned on its own guesses, which may be incorrect and thus lead to a compounding of errors (Bengio et al., 2015).", "startOffset": 129, "endOffset": 150}, {"referenceID": 2, "context": "Due to this discrepancy, it has been shown that maximum likelihood training can be suboptimal (Bengio et al., 2015; Ranzato et al., 2015).", "startOffset": 94, "endOffset": 137}, {"referenceID": 23, "context": "Due to this discrepancy, it has been shown that maximum likelihood training can be suboptimal (Bengio et al., 2015; Ranzato et al., 2015).", "startOffset": 94, "endOffset": 137}, {"referenceID": 38, "context": "(2015) rely on the REINFORCE algorithm (Williams, 1992) to decide whether or not the tokens from a sampled prediction lead to a high task-specific score, such as BLEU (Papineni et al.", "startOffset": 39, "endOffset": 55}, {"referenceID": 22, "context": "(2015) rely on the REINFORCE algorithm (Williams, 1992) to decide whether or not the tokens from a sampled prediction lead to a high task-specific score, such as BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003).", "startOffset": 167, "endOffset": 190}, {"referenceID": 2, "context": "During this search, the model is conditioned on its own guesses, which may be incorrect and thus lead to a compounding of errors (Bengio et al., 2015). This can become especially prominent as the sequence length becomes large. Due to this discrepancy, it has been shown that maximum likelihood training can be suboptimal (Bengio et al., 2015; Ranzato et al., 2015). In these works, the authors argue that the network should be trained to continue generating correctly given the outputs already produced by the model, rather than the ground-truth outputs as in teacher forcing. This gives rise to the challenging problem of determining the target for the next output of the network. Bengio et al. (2015) use the token k from the ground-truth answer as the target for the network at step k, whereas Ranzato et al.", "startOffset": 130, "endOffset": 703}, {"referenceID": 2, "context": "During this search, the model is conditioned on its own guesses, which may be incorrect and thus lead to a compounding of errors (Bengio et al., 2015). This can become especially prominent as the sequence length becomes large. Due to this discrepancy, it has been shown that maximum likelihood training can be suboptimal (Bengio et al., 2015; Ranzato et al., 2015). In these works, the authors argue that the network should be trained to continue generating correctly given the outputs already produced by the model, rather than the ground-truth outputs as in teacher forcing. This gives rise to the challenging problem of determining the target for the next output of the network. Bengio et al. (2015) use the token k from the ground-truth answer as the target for the network at step k, whereas Ranzato et al. (2015) rely on the REINFORCE algorithm (Williams, 1992) to decide whether or not the tokens from a sampled prediction lead to a high task-specific score, such as BLEU (Papineni et al.", "startOffset": 130, "endOffset": 819}, {"referenceID": 31, "context": "Our approach draws inspiration and borrows the terminology from the field of reinforcement learning (RL) (Sutton and Barto, 1998), in particular from the actor-critic approach (Sutton, 1984; Sutton et al., 1999; Barto et al., 1983).", "startOffset": 176, "endOffset": 231}, {"referenceID": 30, "context": "Our approach draws inspiration and borrows the terminology from the field of reinforcement learning (RL) (Sutton and Barto, 1998), in particular from the actor-critic approach (Sutton, 1984; Sutton et al., 1999; Barto et al., 1983).", "startOffset": 176, "endOffset": 231}, {"referenceID": 1, "context": "Our approach draws inspiration and borrows the terminology from the field of reinforcement learning (RL) (Sutton and Barto, 1998), in particular from the actor-critic approach (Sutton, 1984; Sutton et al., 1999; Barto et al., 1983).", "startOffset": 176, "endOffset": 231}, {"referenceID": 32, "context": "To train the critic, we adapt the temporal difference methods from the RL literature (Sutton, 1988) to our setup.", "startOffset": 85, "endOffset": 99}, {"referenceID": 33, "context": "While RL methods with non-linear function approximators are not new (Tesauro, 1994; Miller et al., 1995), they have recently surged in popularity, giving rise to the field of \u2018deep RL\u2019 (Mnih et al.", "startOffset": 68, "endOffset": 104}, {"referenceID": 19, "context": "While RL methods with non-linear function approximators are not new (Tesauro, 1994; Miller et al., 1995), they have recently surged in popularity, giving rise to the field of \u2018deep RL\u2019 (Mnih et al.", "startOffset": 68, "endOffset": 104}, {"referenceID": 20, "context": ", 1995), they have recently surged in popularity, giving rise to the field of \u2018deep RL\u2019 (Mnih et al., 2015).", "startOffset": 88, "endOffset": 107}, {"referenceID": 6, "context": "Popular choices for the mapping f are the Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Units (Cho et al., 2014), the latter of which we use for our models.", "startOffset": 130, "endOffset": 148}, {"referenceID": 28, "context": "(Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 0, "endOffset": 42}, {"referenceID": 6, "context": "(Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 0, "endOffset": 42}, {"referenceID": 25, "context": "In our models, the sequence of vectors is produced by either a bidirectional RNN (Schuster and Paliwal, 1997) or a convolutional encoder (Rush et al., 2015).", "startOffset": 137, "endOffset": 156}, {"referenceID": 0, "context": "We use a soft attention mechanism (Bahdanau et al., 2015) that uses a weighted sum of a sequence of vectors in which the attention weights determine the relative importance of each vector.", "startOffset": 34, "endOffset": 57}, {"referenceID": 30, "context": "We note that this way of re-writing the gradient of the expected reward is known in RL under the names policy gradient theorem (Sutton et al., 1999) and stochastic actor-critic (Sutton, 1984).", "startOffset": 127, "endOffset": 148}, {"referenceID": 31, "context": ", 1999) and stochastic actor-critic (Sutton, 1984).", "startOffset": 36, "endOffset": 50}, {"referenceID": 23, "context": "This is similar in spirit to the REINFORCE algorithm that Ranzato et al. (2015) use in the same context.", "startOffset": 58, "endOffset": 80}, {"referenceID": 30, "context": "We note, that result itself is not new and was first derived in (Sutton et al., 1999).", "startOffset": 64, "endOffset": 85}, {"referenceID": 32, "context": "In this work we draw inspiration from temporal difference (TD) methods for policy evaluation (Sutton, 1988).", "startOffset": 93, "endOffset": 107}, {"referenceID": 16, "context": "Similarly to (Lillicrap et al., 2015), we update the parameters \u03c6\u2032 of the target critic by linearly interpolating them with the parameters of the trained one: \u03c6\u2032 \u2190 (1\u2212 \u03c4)\u03c6\u2032+ \u03c4\u03c6, where \u03c4 1 is a hyperparameter.", "startOffset": 13, "endOffset": 37}, {"referenceID": 16, "context": "This is inspired by (Lillicrap et al., 2015), where a delayer actor is used for a similar purpose.", "startOffset": 20, "endOffset": 44}, {"referenceID": 40, "context": "A similar trick was also recently used in the context of learning simple algorithms with Q learning (Zaremba et al., 2015).", "startOffset": 100, "endOffset": 122}, {"referenceID": 18, "context": "Standard value-based RL algorithms like SARSA and OLPOMDP have also been applied to structured prediction (Maes et al., 2009).", "startOffset": 106, "endOffset": 125}, {"referenceID": 22, "context": "In other recent RL inspired work on sequence prediction, Ranzato et al. (2015) trained a translation model by gradually transitioning from maximum likelihood learning into optimizing BLEU or ROUGE scores using the REINFORCE algorithm.", "startOffset": 57, "endOffset": 79}, {"referenceID": 37, "context": "Imitation learning has also been applied to structured prediction (Vlachos, 2012).", "startOffset": 66, "endOffset": 81}, {"referenceID": 24, "context": ", 2009) and DAGGER (Ross et al., 2010) algorithms.", "startOffset": 19, "endOffset": 38}, {"referenceID": 12, "context": "One such approach is \u2018Direct Loss Minimization\u2019 (Hazan et al., 2010) in which the inference procedure is adapted to take both the model likelihood and task-specific score into account.", "startOffset": 48, "endOffset": 68}, {"referenceID": 27, "context": "Another popular approach is to replace the domain over which the task score expectation is defined with a small subset of it, as is done in Minimum (Bayes) Risk Training (Goel and Byrne, 2000; Shen et al., 2015; Och, 2003).", "startOffset": 170, "endOffset": 222}, {"referenceID": 21, "context": "Another popular approach is to replace the domain over which the task score expectation is defined with a small subset of it, as is done in Minimum (Bayes) Risk Training (Goel and Byrne, 2000; Shen et al., 2015; Och, 2003).", "startOffset": 170, "endOffset": 222}, {"referenceID": 2, "context": "Finally, a method called \u2018scheduled sampling\u2019 was proposed to address the train-test discrepancy problems of maximum likelihood training for sequence generation (Bengio et al., 2015).", "startOffset": 161, "endOffset": 182}, {"referenceID": 5, "context": "Data We use data from the One Billion Word dataset for the spelling correction task (Chelba et al., 2013), which has pre-defined training and testing sets.", "startOffset": 84, "endOffset": 105}, {"referenceID": 3, "context": "For the machine translation experiment, we use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014), as used in Ranzato et al.", "startOffset": 140, "endOffset": 162}, {"referenceID": 3, "context": "For the machine translation experiment, we use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014), as used in Ranzato et al. (2015), and closely follow the pre-processing described in that work.", "startOffset": 141, "endOffset": 197}, {"referenceID": 0, "context": "We use the same attention mechanism as proposed in (Bahdanau et al., 2015), which effectively makes our actor network a smaller version of the model used in that work.", "startOffset": 51, "endOffset": 74}, {"referenceID": 0, "context": "We use the same attention mechanism as proposed in (Bahdanau et al., 2015), which effectively makes our actor network a smaller version of the model used in that work. For machine translation, we use a different convolutional encoder to make our results more comparable with Ranzato et al. (2015). For the same reason, we use 256 hidden units in all machine translation networks.", "startOffset": 52, "endOffset": 297}, {"referenceID": 23, "context": "In addition to the actor-critic training we also implemented REINFORCE with a baseline as described in (Ranzato et al., 2015).", "startOffset": 103, "endOffset": 125}, {"referenceID": 11, "context": "Similarly to (Hannun et al., 2014), we subtracted \u03b2T from the negative log-likelihood of each candidate sentence, where T is the candidate\u2019s length, and \u03b2 is a hyperparameter tuned on the validation set.", "startOffset": 13, "endOffset": 34}, {"referenceID": 23, "context": "8 BLEU points better than what Ranzato et al. (2015) report for their MIXER approach.", "startOffset": 31, "endOffset": 53}], "year": 2016, "abstractText": "We present an approach to training neural networks to generate sequences using actorcritic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a critic network that is trained to predict the value of an output token, given the policy of an actor network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a taskspecific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.", "creator": "LaTeX with hyperref package"}}}