{"id": "1607.07195", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jul-2016", "title": "Higher-Order Factorization Machines", "abstract": "Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy. We demonstrate the proposed approaches on four different link prediction tasks.", "histories": [["v1", "Mon, 25 Jul 2016 10:19:27 GMT  (110kb,D)", "http://arxiv.org/abs/1607.07195v1", null], ["v2", "Fri, 14 Oct 2016 06:32:13 GMT  (517kb,D)", "http://arxiv.org/abs/1607.07195v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["mathieu blondel", "akinori fujino", "naonori ueda", "masakazu ishihata"], "accepted": true, "id": "1607.07195"}, "pdf": {"name": "1607.07195.pdf", "metadata": {"source": "CRF", "title": "Higher-Order Factorization Machines", "authors": ["Mathieu Blondel", "Akinori Fujino", "Naonori Ueda", "Masakazu Ishihata"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Factorization machines (FMs) [12, 13] are a supervised learning approach that can efficiently use second-order functional combinations, even if the data is very high-dimension.The key idea of FMs is to model the weights of functional combinations using a low-level matrix. This has two major advantages: Firstly, FMs can achieve empirical accuracy on par with polynomial regression or core methods, but with smaller and faster models; secondly, FMs can derive the weights of functional combinations that have not been observed in the training environment.This second characteristic is crucial, for example, in receiver systems where FMs are becoming increasingly popular."}, {"heading": "2 Factorization machines (FMs)", "text": "The FMs are an important feature (1), which is only considered in comparison. (2) The FMs are an increasingly popular method for efficiently using second-order features (1). (3) The FMs are one (2). (3) The FMs are one (2). (3) The FMs are one (2). (3) The FMs are one (2). (3) The FMs are one (2). (3) The FMs are one (2). (3) The FMs are one (2). (2). (3) The FMs are one (2). (3) The FMs are one (2). (3). (4) The FMs are one (2). (2). (3). The FMs are one (2). (4). (4). The FMs are one (4). (4). (4)."}, {"heading": "3 Linear-time stochastic gradient algorithms for HOFMs", "text": "The quantity shown in Section 2 allows us to focus on the ANOVA kernels as the most important \"computing units\" for training. (1) In this section, we develop dynamic programming procedures (DP) for the evaluation and calculation of its gradients in only O (dm) time, i.e., linear time tracking. (1) The main observation is that we can (7) opt for recursive removal of properties until the computation of the kernel. (1) Then we have a subvector of p 1: j Rj and similar for x. In addition, let us imagine the abstract (7). (p1: j, x1: j). Then we have a subvector of p 1: j Rj and similar for x. (t: j)."}, {"heading": "4 Coordinate descent algorithm for arbitrary-order HOFMs", "text": "We describe a coordinate descend (CD) solver for arbitrary order (p, x) where we define (j). CD is a good choice for learning HOFMs, as its objective function is coordinate-wise convex, thanks to the multilinearity of the ANOVA kernel. However, our algorithm can be considered a generalization of the CD algorithms proposed in [13, 4]. An alternative recursion, which we used in the previous section, is usually not suitable for a CD algorithm implementation, because it would require the maintenance of statistics for each training instance, such as predictions for the current iteration. If a coordinate is updated, the statistics must then be synchronized. Unfortunately, the recursion we used in the previous section is not suitable for a CD algorithm, because it would require maintenance and synchronization of the DP table for each training instance."}, {"heading": "5 HOFMs with shared parameters", "text": "HOFMs, as originally defined in [12, 13], model each degree with separate matrices P (2),..., P (m). Assuming that we use the same rank k for all matrices, the total model size of m-order HOFMs is therefore O (kdm). Furthermore, the cost of calculating predictions even using our O (dm) DP algorithm O (k (2d + \u00b7 + md)) = O (kdm2). Therefore, HOFMs tend to build large, expensive models. To reduce model size and prediction times, we introduce two new cores that allow us to divide parameters between each degree: the inhomogeneous ANOVA core and the entire subset core. Since both cores are derived from the ANOVA core, they have the same attractive properties: multilinearity, economical gradients, and economical data friendliness."}, {"heading": "5.1 Inhomogeneous ANOVA kernel", "text": "It is known that a sum of the cores corresponds to the concatenation of their associated characteristics (17, section 3.4). Let \u03b8 = [\u03b81,. \u2212 Am] T. In fact, to combine different degrees, a natural kernel is converted A1 \u2192 m (p, x; \u03b8): = m \u2211 t = 1 \u03b8tAt (p, x). (13) The kernel uses all the combinations of characteristics of degree 1 up to m. We call it an inhomogeneous ANOVA kernel because it is an inhomogeneous polynomial of x. In contrast, Am (p, x) is homogeneous. The main difference between (13) and (6) is that all ANOVA kernels share the same parameters in sum. However, in order to increase the modelling performance, we allow each kernel to have different weights."}, {"heading": "5.2 All-subsets kernel", "text": "We will now consider a closely related kernel called All-subsets kernel [17, definition 9,5]: S (p, x): = d (p, x) = 1 (1 + pjxj).The main difference to the traditional use of this kernel is that we learn p. Interestingly, it can be shown that S (p, x) = 1 + A1 \u2192 d (p, x; 1) = 1 + A1 \u2192 nz (x) (p, x; 1), where nz (x) is the number of non-zero features in x. To learn the parameters, the kernel uses all combinations of different features up to the order nz (x) with uniform weights. Even if d is very large, the kernel can be a good choice if each training instance contains only a few non-zero elements. To learn the parameters, we simply replace it with S in (10). In SG or CD algorithms, everything it brings with it, on the xx (kp)."}, {"heading": "6 Experimental results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Application to link prediction", "text": "We can therefore assume that we have two different types of application examples for the application examples of A and B. We assume that the application examples of A and B are each represented by a matrix Y."}, {"heading": "6.2 Solver comparison", "text": "We compared AdaGrad [5], L-BFGS and Coordinate Descent (CD) to minimize (10) the variation of degree m on the NIPS dataset with \u03b2 = 0.1 and k = 30. We constructed the data in the same way as described in the previous section and added m \u2212 1 dummy characteristics, resulting in n = 8, 280 sparse samples of dimension d = 27, 298 + m \u2212 1. For AdaGrad and L-BFGS, we calculated the (stochastic) gradients using algorithm 2. All solvers used the same initialization. The results are shown in Figure 1. We see that our CD algorithm works very well when m \u2264 3 starts to deteriorate, but when m \u2265 4 becomes advantageous, in which case L-BFGS becomes. As shown in Figure 1 d, the cost per epoch of AdaGrad and L-BFGS is given early with m, an advantage of our DP algorithm for calculating the target rate is not necessarily higher than the S001."}, {"heading": "7 Conclusion and future directions", "text": "In this paper, we introduced the first training algorithms for HOFMs and introduced new HOFM variants with common parameters. A popular way to handle a large number of negative samples is to use an objective function that directly maximizes AUC [8, 14]. This is particularly easy with SG algorithms, since we can scan pairs of positive and negative samples from the dataset every SG update. Therefore, we expect that the algorithms developed in Section 3 will be particularly useful in this environment. [7] Recently, a distributed SG algorithm has been proposed for the formation of second-order FMs. It should be easy to extend this algorithm to HOFMs based on our contributions in Section 3. Finally, it should be possible to integrate algorithms 1 and 2 into a deep learning framework such as TensorFlow [1] in order to easily compose ANOVA cores with other layers (e.g. Convolutionary)."}, {"heading": "A Dataset descriptions", "text": "\u2022 NIPS: Co-author graph of the authors in the first twelve issues of NIPS, obtained from [16]. For this dataset, as well as the enzyme dataset below, we have A = B. The number of positive samples is n + = 4, 140. \u2022 Enzyme: metabolic networks from [20]. The network includes nA = nB = 668 enzymes, which are represented by three sets of characteristics: a 157-dimensional vector of phylogenetic information, a 145-dimensional vector of gene expression information, and a 23-dimensional vector of gene location information. We link the three sets of information to form characteristics dA = 325."}, {"heading": "B Additional experiments", "text": "B.1 Solution Comparison We also compared AdaGrad, L-BFGS and Coordinate Parentage (CD) on the datasets Enzymes, Gene Disease (GD) and Movielens 100K. Results are given in Figures 2, 3 and 4 respectively. B.2 Recommended System Experiments We compared HOFMs, HOFMs (with common parameters), All-subsets and Polynomial Networks on the following two receiver system datasets. The design matrix was constructed as follows [12, 13]. Namely, for each rating yi, the corresponding xi is set on the concatenation of the most uniform user codes and item indices. We divided the samples consistently between 75% for training and 25% for the test.Dataset n d Movielens 1M 1,000.209 (ratings) 9,940 = 6,040 (user) + 3,900 films load day (128,078) + 108,078 (user) (user) + 3,900 movies load day."}, {"heading": "C Reverse-mode differentiation on the alternative recursion", "text": "s use the abbreviations: (p, x) and dt: (p, x). We can then use the recursion at: (a1d1 \u2212 a0d2) and a3 (a2d1 \u2212 a1d2 \u2212 a0d3).We see that a2 influences a2 and a3. Likewise, d3 influences a3, d2 influences a2 and a3, and d1 influences a1."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Factorization machines (FMs) are a supervised learning approach that can use<lb>second-order feature combinations even when the data is very high-dimensional.<lb>Unfortunately, despite increasing interest in FMs, there exists to date no efficient<lb>training algorithm for higher-order FMs (HOFMs). In this paper, we present<lb>the first generic yet efficient algorithms for training arbitrary-order HOFMs. We<lb>also present new variants of HOFMs with shared parameters, which greatly re-<lb>duce model size and prediction times while maintaining similar accuracy. We<lb>demonstrate the proposed approaches on four different link prediction tasks.", "creator": "LaTeX with hyperref package"}}}