{"id": "1607.08810", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jul-2016", "title": "Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms", "abstract": "Polynomial networks and factorization machines are two recently-proposed models that can efficiently use feature interactions in classification and regression tasks. In this paper, we revisit both models from a unified perspective. Based on this new view, we study the properties of both models and propose new efficient training algorithms. Key to our approach is to cast parameter learning as a low-rank symmetric tensor estimation problem, which we solve by multi-convex optimization. We demonstrate our approach on regression and recommender system tasks.", "histories": [["v1", "Fri, 29 Jul 2016 13:54:51 GMT  (664kb,D)", "http://arxiv.org/abs/1607.08810v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["mathieu blondel", "masakazu ishihata", "akinori fujino", "naonori ueda"], "accepted": true, "id": "1607.08810"}, "pdf": {"name": "1607.08810.pdf", "metadata": {"source": "META", "title": "Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms", "authors": ["Mathieu Blondel", "Masakazu Ishihata", "Akinori Fujino", "Naonori Ueda"], "emails": ["MATHIEU.BLONDEL@LAB.NTT.CO.JP", "ISHIHATA.MASAKAZU@LAB.NTT.CO.JP", "FUJINO.AKINORI@LAB.NTT.CO.JP", "UEDA.NAONORI@LAB.NTT.CO.JP"], "sections": [{"heading": "1. Introduction", "text": "One of the simplest approaches to using such interactions is to explicitly express characteristics of this type of approach: the number of parameters is estimated as O (dm), where d takes into account the number of characteristics and m takes into account the order of interactions. As a result, it is usually limited to the second or third type of interactions; another popular approach is to implicitly map the data using the kernel trick; the main advantage of this approach is that the number of parameters in the model is actually independent of d and m."}, {"heading": "2. Related work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Polynomial networks", "text": "Polynomic networks (PNs) (Livni et al., 2014) of degree m = 2 predict the output y-R associated with x-Rd-byy-PN (x; w, \u03bb, P): = < w, x > + < \u03c3 (PTx), \u03bb >, (2) where w-Rd, P-Rd \u00b7 k, \u03bb-Rk and \u03c3 (u): = u2 are evaluated elementally. Intuitively, the right term can be interpreted as a feedforward neural network with a hidden layer of k units and activation function \u03c3 (u). Livni et al. (2014) also extend (2) to case m = 3 and theoretically show that PNs can approach feedforward networks with sigmoidal activation."}, {"heading": "2.2. Factorization machines", "text": "One of the simplest methods to effectively use interactions between characteristics is polynomial regression (PR). For example, for second-order interactions, we calculate predictions on the principle of \"PR\" (x; w, W): = < w, x > + \u2211 j \"> j\" W, \"j\" xjxj, \"with w\" Rd \"and W\" Rd2. \"Obviously, the model size in PR does not scale well w.r.t. The basic idea of the (secondary) factoring machines (FMs) (Rendle, 2010; 2012) is to replace W with a factored matrix PPT: y\" FM \"(x; w, P): = < w, x > +\" j \"> j\" (PPT) jj \"ctuxj\" (PT), with P \"Rd\" k. FMs \"increasingly popular for efficiently modeling interactions between characteristics in high-dimensional data, we show in the Rendle that in 2012 is the first order of the MS references."}, {"heading": "3. Polynomial and ANOVA kernels", "text": "In this section, we show that the prediction functions used by polynomial networks and factorization machines are very useful (1) for a specific selection of kernels (1).The prediction function is a popular kernel for using combinations of characteristics. The kernel is defined as Pm\u03b3 (p, x): = (\u03b3 + < p, x > m, where the kernel (p, x >) x x x x x x, x x x x x, x x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, etc."}, {"heading": "4. Direct approach", "text": "The most natural approach to learn the models of form (1) is to \"(yi, y, i) choose a convex loss function to minimize some error functions. Note that (4) is a convex lens, regardless of K. However, it is generally not convex w.r.t. P. Fortunately, if K = Am is, we can show that (4) is a convex loss function. Theorem 1 Multi-convex of (4) is a convex lens w.r.t. P is generally not convex. If K = Am is, we can show that (4) is multiconvex. Theorem 1 is multi-convex of (4), if RDAm is convex."}, {"heading": "5. Lifted approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Conversion to low-rank tensor estimation problem", "text": "If we put K = Hm in (4), the resulting optimization problem is neither convex nor multi-convex w.r.t. < W = > For (Blondel et al., 2015), for m = 2, it has been proposed to convert the parameter estimation as a low-ranking symmetric matrix estimation problem. < W \u00b7 s: a similar idea has been used in the context of phase retrieval in (Cande), which leads to a goal that is multi-convex for both K = Am and K = Hm (Section 5,2). We start by rewriting the core definitions using a symmetric tensor. ForHm (p, x), it is easy to see thatHm (p, x). & lt."}, {"heading": "5.2. Multi-convex formulation", "text": "\"We are not in a position to put ourselves in a position.\" < P diag. < P diag (2) PT diag. < P diag (2) PT diag. < P diag (2) P diag. < P diag (3) P diag. < P diag (3) P diag. < P diag (3) P diag. < P diag. < P diag (2) P diag. < P diag (2) P diag. < P diag (2) P diag. < P diag (2) P diag. < P diag. < P diag (2) P diag. < P diag. < P diag (2) P diag. < P diag."}, {"heading": "6. Regularization", "text": "In some applications, the number of basics or the ranking limitation is not sufficient to achieve a good generalization performance, and it is necessary to consider an additional form of regulation. For the raised target with K = H2 or A2, we use the typical Frobenius standard regularization L-K (U, V): = LK (S (UV T))) + \u03b22 (VP), (12) where \u03b2 > 0 is a regularization parameter. For the direct target, we introduce the new regularization D-K (\u03bb, P): = DK (\u03bb, P) + \u03b2 k-s = 1-ps-2. (13) This allows us to regulate regulation with a single hyperparameter and P with a single hyperparameter. Let us define the following nuclear standard, which is penalized as a minimum target: L-K (M): = LK (S (M) + M) + \u03b2-Prom-12 with a single parameter."}, {"heading": "7. Coordinate descent algorithms", "text": "We now describe how to learn the model parameters by descending coordinates, which is a state in which the learning rate is free for multi-convex problems (e.g., Yu et al. (2012)). In the following, we assume that \"it is a simple change of variables.\" Therefore, we focus on minimizing. (P.) Let us denote the elements of P by Pjs. (P.) Then our algorithm cyclically performs the following update for all s '1-regulated convex target. (P.) Let us denote the elements of P by Pjs. (P.) Cyclically performs the following update for all s. (D.): Pjs."}, {"heading": "8. Inhomogeneous polynomial models", "text": "The algorithms presented so far are designed for homogeneous polynomic nuclei Hm and Am. These nuclei only use monomials of the same degree m. However, in many applications we would like to use monomials of the same degree up to a certain degree. In this section we propose a simple idea to do this using the algorithms presented so far, unmodified. Our main observation is that we can easily turn homogeneous polynomials into inhomogeneous ones by extending the dimensions of the training data by stupid features. We start by explaining how to learn inhomogeneous polynomial models using Hm. Let us designate p. \u2212 kD: = [1, pT] and x."}, {"heading": "9. Experimental results", "text": "In this section, we present experimental results, focusing on regression tasks. Data sets are described in Appendix E. In all experiments, we place \"(y, y) on quadratic loss."}, {"heading": "9.1. Direct optimization: is it useful to fit \u03bb?", "text": "As explained in section 4, there is no advantage in adjusting \u03bb if m is odd, since Am and Hm can absorb \u03bb in P. However, this is not the case if m is even: Am and Hm can absorb absolute values, but not negative characters (unless complex numbers are allowed for parameters). If m is uniform, the class of functions we can represent with models of form (1) may be smaller if we fix \u03bb = 1 (as in FMs). To verify that this is actually the case, we minimize (13) with m = 2 as follows: a) minimize w.r.t. both \u03bb and P alternately b) fix \u03bbs = 1 for s [k] and minimize w.r.t. P, c) fix \u03bbs = \u00b1 1 with proba. 0.5 and minimize w.r.t. P. We initialize elements of P by pjs \u00b2 N (0, 0.01) for all j \u00b2."}, {"heading": "9.2. Direct vs. lifted optimization", "text": "In this section, we compare the direct and upscale optimization approaches on high-dimensional data, if m = 2. To compare the two approaches fairly, we propose the following initialization scheme. Let's remember that both approaches essentially learn a low symmetrical matrix: W = S (UV T) for uplifted and W = P diag (\u03bb) PT for direct optimization. This indicates that we can easily convert the matrices U, V-Rd \u00b7 r, which are used to initialize the uplifted optimization, into P-Rd \u00b7 k and \u03bb-Rd \u00b7 k by calculating the (reduced) self-decomposition of S (UV T). Note that since we solve the uplifted optimization problem by coordinate descent, UV-T is never symmetrical and therefore the rank of S (UV T) is usually twice as high as that of UV Hence, in practice we have this solution initiated by directly solving the GS = GS with our objective S1 CD = GS solution."}, {"heading": "9.3. Recommender system experiment", "text": "To confirm the proposed framework's ability to derive the weights of unobserved feature interactions, we conducted experiments with Last.fm and Movielens 1M, two standard recommendation system datasets. Following (Rendle, 2012), matrix factorization can be reduced to FMs by creating a dataset of (xi, yi) pairs in which xi contains the uniform encoding of user and object, and yi is the corresponding rating (i.e. the number of training cases corresponds to the number of ratings).We compared four models: a) K = A2 (augment): y = y attributes A2 (x factor), with x-T: = [1, xT], b) K = A2 (linear combination): y = < w, x > + y attributes A2 (x factor), c) K = H2 (augment): y value H2 (x value) and d = (2): y-combination (x)."}, {"heading": "9.4. Low-budget non-linear regression experiment", "text": "In this experiment, we demonstrated the proposed framework's ability to achieve good regression performance with a small number of bases k. We compared: a) Proposal with K = H3 (with extended characteristics), b) Proposal with K = A3 (with extended characteristics), c) Nystro \ufffd m method with K = P31 and d) Random selection: Selection of p1,... pk uniformly random from the training set and use of K = P31. For a) and b) we used the upscale approach. For a fair comparison in terms of model size (number of floats used), we set r = k / 3. Results for individuals, cadata and small datasets are shown in Figure 4. We see that i) the proposed framework achieves the same performance as kernel ridge regression with much less bases than other methods and ii) H3 tends to perform better than A3 in these tasks. Similar trends were observed when using A2 or H2."}, {"heading": "10. Discussion", "text": "It is not as if it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about and a way in which it is about a way in which it is about and a way in which it is about and a way in which it is about and a way in which it is about and a way in which it is about and a way in which it is about and a way in which it is about and a way in which it is about a way in which it is about and a way in which it is about a way in which it is about and a way in which it is about a way in which it is about and a way in which it is not a way in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is not in which it is in which it is not in which it is not in which it is not in which it is not in which it is in which it is not in which it is not in which it is not in which it is not in which it is"}, {"heading": "11. Conclusion", "text": "In this paper, we examined polynomial networks (Livni et al., 2014) and factorization machines (Rendle, 2010; 2012) from a unified perspective. We proposed direct and upscale optimization approaches and demonstrated their equivalence in the regularized case for m = 2. With regard to PNs, we proposed the first CD solver with support for any integer \u2265 2. With regard to FMs, we made several novel contributions, including a connection to the ANOVA kernel, which demonstrated important properties of objective function and derived the first CD solver for third-order FMs. Empirically, we demonstrated that the proposed algorithms perform excellently in nonlinear regression and recommendation system tasks."}, {"heading": "Acknowledgments", "text": "This work was partly carried out in the framework of \"Research and Development on Fundamental and Applied Technologies for Social Big Data,\" commissioned by the National Institute of Information and Communications Technology (NICT), Japan. We also thank Vlad Niculae, Olivier Grisel, Fabian Pedregosa and Joseph Salmon for their valuable comments."}, {"heading": "A. Symmetric tensors", "text": "A.1. BackgroundLet Rd1 \u00b7 \u00b7 \u00b7 dm be the set of d1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 dm realm-order tensors. < < < < < < > dm = >. We denote the set of cubic tensors of Rd m. We denote the elements of M-Rdm of Mj1,..., jm, where j1,.., jm, [d]. Let us denote the set of cubic tensors of Rd.,..,. ltm). Given M-Rdm, we define M-Rdm as a tensor so that (M\u03c3) j1,..., jm: = Mj\u03c31,."}, {"heading": "B. Proofs related to ANOVA kernels", "text": "B. 1. Proof of multi-linearity (Lemma 2 = 1) For m = 1, we have A1 (p, x) = 1 (p, x) = 1 (p, x) = 1 (p, x) = 1 (p, x) = 1 (p, x) = 1 (p, x) = 1 (p, x) = 1 (p, x) = 1. For 1 < m = 1 (p, x) = 1 (p, x) = 1 (p, x) = 1 (p, x) = 1 (p, x) = 1 (p, x) = 1 (p, x) = 1 (p, x) = 1 (p, x)."}, {"heading": "C. Proof of equivalence between regularized problems (Theorem 2)", "text": "First, we will prove that the optimal solution of the nuclear standard is a symmetrical problem that has a symmetrical matrix. (For this reason, we must use the following formula: \"M + MT,\" \"S (M),\" \"M,\" \"M,\" \"M,\" \"M,\" \"M,\" \"M,\" \"\" M, \"\" \"M,\" \"\" M, \"\" \"M,\" \"\" M, \"\" M, \"\" \"M,\" \"\" M, \"\" \"M,\" \"\" M, \"\" \".,\"., \".,\".,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,,.,.,.,.,.,.,.,.,.,.,.,.,,.,.,.,.,.,.,.,.,.,,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,."}, {"heading": "D. Efficient coordinate descent algorithms", "text": "In this section we develop an efficient minimization algorithm (13).It is easy to see that minimization can be reduced to a standard. \"In this section we therefore develop our attention to minimization. (13) It is easy to see that minimization can be reduced to a standard.\" (1). (1). (1). (2). We therefore focus on minimization. (2). (2). (2). As a reminder: We want Minimizef (1). (1). (1). (1). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (3). (3). (3). (3). (3). (). (3). ().). (3). (). (3).). (3). ()."}, {"heading": "E. Datasets", "text": "For regression experiments we used the following public data sets. Data set n (train) n (test) d Description alone 3,132 1,045 8 Prediction of age of abalons from physical measurements cadata 15,480 5,160 8 Prediction of house prices from economic covariate pusmall 6,144 2,048 12 Prediction of computer system activity from system performance measures diabetes 331 111 10 Prediction of disease progression from baseline measures E2006-tfidf 16,087 3,308 150,360 Prediction Volatility of stock returns from financial reportThe diabetes data set is available in scikit-learn (Pedregosa et al., 2011). Other data sets are from http: / www. csie.edu.edu.tw / cjlin / libsvmtools / datasets /.For recommendder system experiments we forecast the following two public data sets."}], "references": [{"title": "A new approach to collaborative filtering: Operator estimation with spectral regularization", "author": ["Abernethy", "Jacob", "Bach", "Francis", "Evgeniou", "Theodoros", "Vert", "Jean-Philippe"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Abernethy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2009}, {"title": "Subspace embeddings for the polynomial kernel", "author": ["Avron", "Haim", "Nguyen", "Huy", "Woodruff", "David"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Avron et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2014}, {"title": "Convex factorization machines", "author": ["Blondel", "Mathieu", "Fujino", "Akinori", "Ueada", "Naonori"], "venue": "In Proceedings of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD),", "citeRegEx": "Blondel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2015}, {"title": "Phase retrieval via matrix completion", "author": ["Cand\u00e8s", "Emmanuel J", "Eldar", "Yonina C", "Strohmer", "Thomas", "Voroninski", "Vladislav"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2013}, {"title": "Training and testing low-degree polynomial data mappings via linear svm", "author": ["Chang", "Yin-Wen", "Hsieh", "Cho-Jui", "Kai-Wei", "Ringgaard", "Michael", "Lin", "Chih-Jen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Danqi", "Manning", "Christopher D"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Symmetric tensors and symmetric tensor rank", "author": ["Comon", "Pierre", "Golub", "Gene", "Lim", "Lek-Heng", "Mourrain", "Bernard"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Comon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Comon et al\\.", "year": 2008}, {"title": "Random feature maps for dot product kernels", "author": ["Kar", "Purushottam", "Karnick", "Harish"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Kar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kar et al\\.", "year": 2012}, {"title": "On the computational efficiency of training neural networks", "author": ["Livni", "Roi", "Shalev-Shwartz", "Shai", "Shamir", "Ohad"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Livni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Livni et al\\.", "year": 2014}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["Mazumder", "Rahul", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["Pham", "Ninh", "Pagh", "Rasmus"], "venue": "In Proceedings of the 19th KDD conference,", "citeRegEx": "Pham et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2013}, {"title": "Factorization machines", "author": ["Rendle", "Steffen"], "venue": "In Proceedings of International Conference on Data Mining,", "citeRegEx": "Rendle and Steffen.,? \\Q2010\\E", "shortCiteRegEx": "Rendle and Steffen.", "year": 2010}, {"title": "Factorization machines with libfm", "author": ["Rendle", "Steffen"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "Rendle and Steffen.,? \\Q2012\\E", "shortCiteRegEx": "Rendle and Steffen.", "year": 2012}, {"title": "Kernel Methods for Pattern Analysis", "author": ["Shawe-Taylor", "John", "Cristianini", "Nello"], "venue": null, "citeRegEx": "Shawe.Taylor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor et al\\.", "year": 2004}, {"title": "Coffin: A computational framework for linear svms", "author": ["Sonnenburg", "S\u00f6ren", "Franc", "Vojtech"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Sonnenburg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2010}, {"title": "Support vector regression with anova decomposition kernels", "author": ["Stitson", "Mark", "Gammerman", "Alex", "Vapnik", "Vladimir", "Vovk", "Volodya", "Watkins", "Chris", "Weston", "Jason"], "venue": null, "citeRegEx": "Stitson et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Stitson et al\\.", "year": 1997}, {"title": "Statistical learning theory", "author": ["Vapnik", "Vladimir"], "venue": null, "citeRegEx": "Vapnik and Vladimir.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik and Vladimir.", "year": 1998}, {"title": "Multi-class pegasos on a budget", "author": ["Z. Wang", "K. Crammer", "S. Vucetic"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML),", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["Williams", "Christopher K. I", "Seeger", "Matthias"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Williams et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2001}, {"title": "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems", "author": ["Yu", "Hsiang-Fu", "Hsieh", "Cho-Jui", "Si", "Dhillon", "Inderjit S"], "venue": "In ICDM,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Although fast linear model solvers can be used (Chang et al., 2010; Sonnenburg & Franc, 2010), an obvious drawback of this kind of approach is that the number of parameters to estimate scales as O(d), where d is the number of features and m is the order of interactions considered.", "startOffset": 47, "endOffset": 93}, {"referenceID": 17, "context": "This is sometimes called the curse of kernelization (Wang et al., 2010).", "startOffset": 52, "endOffset": 71}, {"referenceID": 1, "context": "clude the Nystr\u00f6m method (Williams & Seeger, 2001), random features (Kar & Karnick, 2012) and sketching (Pham & Pagh, 2013; Avron et al., 2014).", "startOffset": 104, "endOffset": 143}, {"referenceID": 8, "context": "We show (Section 3) that choosing one kernel or the other allows us to recover polynomial networks (PNs) (Livni et al., 2014) and, surprisingly, factorization machines (FMs) (Rendle, 2010; 2012).", "startOffset": 105, "endOffset": 125}, {"referenceID": 8, "context": "Polynomial networks Polynomial networks (PNs) (Livni et al., 2014) of degree m = 2 predict the output y \u2208 R associated with x \u2208 R by \u0177PN(x;w,\u03bb,P ) := \u3008w,x\u3009+ \u3008\u03c3(Px),\u03bb\u3009, (2) where w \u2208 R, P \u2208 Rd\u00d7k, \u03bb \u2208 R and \u03c3(u) := u is evaluated element-wise.", "startOffset": 46, "endOffset": 66}, {"referenceID": 8, "context": "Polynomial networks Polynomial networks (PNs) (Livni et al., 2014) of degree m = 2 predict the output y \u2208 R associated with x \u2208 R by \u0177PN(x;w,\u03bb,P ) := \u3008w,x\u3009+ \u3008\u03c3(Px),\u03bb\u3009, (2) where w \u2208 R, P \u2208 Rd\u00d7k, \u03bb \u2208 R and \u03c3(u) := u is evaluated element-wise. Intuitively, the right-hand term can be interpreted as a feedforward neural network with one hidden layer of k units and with activation function \u03c3(u). Livni et al. (2014) also extend (2) to the case m = 3 and show theoretically that PNs can approximate feedforward networks with sigmoidal activation.", "startOffset": 47, "endOffset": 414}, {"referenceID": 15, "context": "A much lesser known kernel is the ANOVA kernel (Stitson et al., 1997; Vapnik, 1998).", "startOffset": 47, "endOffset": 83}, {"referenceID": 2, "context": "In (Blondel et al., 2015), for m = 2, it was proposed to cast parameter estimation as a low-rank symmetric matrix estimation problem.", "startOffset": 3, "endOffset": 25}, {"referenceID": 3, "context": "A similar idea was used in the context of phase retrieval in (Cand\u00e8s et al., 2013).", "startOffset": 61, "endOffset": 82}, {"referenceID": 6, "context": "Lemma 5 Link between tensors and kernel expansions Let W \u2208 Sdm have a symmetric outer product decomposition (Comon et al., 2008)", "startOffset": 108, "endOffset": 128}, {"referenceID": 3, "context": "Following (Cand\u00e8s et al., 2013), we call this approach lifted.", "startOffset": 10, "endOffset": 31}, {"referenceID": 0, "context": "In addition to Theorem 2, from (Abernethy et al., 2009), we also know that every local minimum U ,V of (12) gives a global solution UV T of (14) provided that rank(M\u2217) \u2264 r.", "startOffset": 31, "endOffset": 55}, {"referenceID": 19, "context": ", Yu et al. (2012)).", "startOffset": 2, "endOffset": 19}, {"referenceID": 8, "context": "Conclusion In this paper, we revisited polynomial networks (Livni et al., 2014) and factorization machines (Rendle, 2010; 2012) from a unified perspective.", "startOffset": 59, "endOffset": 79}], "year": 2016, "abstractText": "Polynomial networks and factorization machines are two recently-proposed models that can efficiently use feature interactions in classification and regression tasks. In this paper, we revisit both models from a unified perspective. Based on this new view, we study the properties of both models and propose new efficient training algorithms. Key to our approach is to cast parameter learning as a low-rank symmetric tensor estimation problem, which we solve by multi-convex optimization. We demonstrate our approach on regression and recommender system tasks.", "creator": "LaTeX with hyperref package"}}}