{"id": "1608.00112", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jul-2016", "title": "Supervised Attentions for Neural Machine Translation", "abstract": "In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the \"true\" alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system.", "histories": [["v1", "Sat, 30 Jul 2016 12:39:19 GMT  (2108kb,D)", "http://arxiv.org/abs/1608.00112v1", "6 pages. In Proceedings of EMNLP 2016. arXiv admin note: text overlap witharXiv:1605.03148"]], "COMMENTS": "6 pages. In Proceedings of EMNLP 2016. arXiv admin note: text overlap witharXiv:1605.03148", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haitao mi", "zhiguo wang", "abe ittycheriah"], "accepted": true, "id": "1608.00112"}, "pdf": {"name": "1608.00112.pdf", "metadata": {"source": "CRF", "title": "Supervised Attentions for Neural Machine Translation", "authors": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah"], "emails": ["abei}@us.ibm.com"], "sections": [{"heading": null, "text": "In this paper, we improve the attention or alignment accuracy of neural machine translation by using the alignment of training set pairs. We simply calculate the distance between machine attention and the \"true\" alignment and minimize this cost in the training sequence. Our experiments with large-scale tasks from Chinese to English show that our model significantly improves both the translation and alignment qualities compared to the large-language neural machine translation system, and even surpasses a modern traditional syntax-based system."}, {"heading": "1 Introduction", "text": "Neural Machine Translation (NMT) has grown in popularity over the past two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al. (2014). The attention model plays a crucial role in NMT, as it shows on which source word (s) the model should focus in order to predict the next target word. However, the attention or alignment quality of NMT is still very low (Mi et al., 2016a; Tu et al., 2016). In this paper, we alleviate the above problem by using the alignment (human annotated data or machine alignment) of the training set. Given the alignment of all training sets, we add alignment distance costs to the target function. Therefore, we not only maximize the log translation probabilities, but also minimize alignment costs."}, {"heading": "2 Neural Machine Translation", "text": "As shown in Figure 1, the attention-based NMT (Bahdanau et al., 2014) is an encoder-decoder network, using a bidirectional recursive neural network to encode the source sentence x = (x1,..., xl), where l translates the sentence length (including the sentence < eos >) into a sequence of hidden states h = (h1,..., hl), each hi is a concatenation of a left-right \u2212 \u2192 Hi and a right-left-left sentence \u2212 hi. Given h, the decoder says the target translation by maximizing the conditional logic probability of the correct translation y = (y \u00b2 m), where m is the sentence length (including the endofentence). At any given time t is the probability of each word yt from a target vocabulary: y (y \u00b2, including the sentence length y \u00b2)."}, {"heading": "3 Alignment Component", "text": "Attention, \u03b1t, 1... \u03b1t, l, plays an important role in each step t in the NMT. However, accuracy in relation to the alignment F1 score is still far behind the traditional MaxEnt alignment model (Mi et al., 2016b; Tu et al., 2016). Therefore, in this section we explicitly add an alignment distance to the objective function. 5. The \"truth\" alignments for each set pair may come from human-commented data, unattended or supervised alignments (e.g. GIZA + + (Och and Ney, 2000) or MaxEnt (Ittycheriah and Roukos, 2005). In an alignment matrix A for a set pair (x, y) in Figure 2 (a), where we have an end-of-section token < eos > = xl, and we align all unaligned target words (x) to the probability (y) from Figure 2 (a)."}, {"heading": "3.1 Simple Transformation", "text": "The first transformation simply normalizes each line. Figure 2 (b) shows the result matrix A \u0445. The last column in red dashed lines shows the orientation of the special end of sentence token < eos >."}, {"heading": "3.2 Smoothed Transformation", "text": "Considering the original alignment matrix A, we create a matrixA \u0445, at which all points are initialized with zero. Then, for each alignment point, we update At, i = 1, A \u0443, adding a Gaussian distribution, g (\u00b5, \u03c3), with a window size (t-w,... t + w). For example, if we take A1.1 = 1, we have A * 1,1 + = 1, A * 1,2 + = 0.61 and A * 1,3 + = 0.14 with w = 2, g (\u00b5, \u03c3) = g (0, 1). Then we normalize each row and get (c). In our experiments, we use a form distribution where \u03c3 = 0.5."}, {"heading": "3.3 Objectives", "text": "Alignment Objective: Considering the \"true\" alignment A * and the machine attention A \u00b2 generated by the NMT model, we calculate the Euclidean distance between A * and A \u00b2.d (A \u00b2, A \u00b2) = 270 \u00b0 \u00b2 m \u00b2 t = 1l \u00b2 i = 1 (A \u00b2 t, i \u2212 A \u0445 t, i) 2. (6) NMT Objective: We put Il. 6 on Il. 5, we have two parts: Translation and Alignment so that we can optimize them jointly or separately (e.g. we first optimize the alignment, then the optimization of the translation). (7) There are two parts: Translation and Alignment so that we can optimize them jointly or separately (e.g. we only optimize the alignment, then we optimize the translation)."}, {"heading": "4 Related Work", "text": "In order to improve attention or accuracy of alignment, Cheng et al. (2016) adapted agreement-based learning (Liang et al., 2006; Liang etal., 2008) and introduced a combined target that takes into account both translation directions (source-to-target and target-to-source) and an agreement between the two alignments. In contrast, our approach uses and optimizes NMT parameters directly using the \"monitored\" alignments."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data Preparation", "text": "We conduct our experiments in Chinese to English assignment. The Training Corpus consists of approximately 5 million sentences within the DARPA BOLT Sino-English assignment. The Corpus comprises a mix of newswire, broadcast news, and web blog. We do not include HK Law, HK Hansard, and UN data. Chinese text is segmented with a segmentator who trains on conditioned random fields (CRF). Our development set is the concatenation of multiple tuning sets (GALE Dev, P1R6 Dev, and Dev 12), initially released under the DARPA GALE program. The development set is 4491 sentences in total. Our test sets are NIST MT06 (1664 sets), MT08 News, and MT08 (666 sets).For all NMT systems, the complete vocabulary of the training is 300k."}, {"heading": "5.2 Translation Results", "text": "The syntax-based statistical translation model achieves an average (TER-BLEU) / 2 of 13.36 on three test categories. The Cov. LVNMT system achieves an average (TER-BLEU) / 2 of 14.24, which is about 0.9 points worse than the tree-to-string SMT system. Please note that all systems are individual systems. It is very possible that the group of NMT systems with different random seeds may lead to better results. We test three different orientations: \u2022 En \u2192 En (One Direction of GIZA + +), GDFA Finale and \"Heuristic Summary of the Two Directions of GIZA + +.\""}, {"heading": "5.3 Alignment Results", "text": "Table 2 shows the alignment F1 values based on the alignment test set (447 hand-aligned sets). The MaxEnt model is designed for 67k hand-aligned sets and reaches an F1 value of 75.96. In NMT systems, we drop the alignment matrices and convert them into alignments with the following steps. For each target word, we sort the alphabets and add the maximum probability link if it is higher than 0.2. Interestingly, if we only align the alignment component (A in line 3), we improve the alignment F1 value from 45.76 to 47.87. And we further increase the value to 50.97 by matching alignment and translation together (J in line 7). Interestingly, the system with MaxEnt produces more alignments in the output and results in a higher alignment F1 score. This indicates that using Maxt links to a more align the probabilities we can use."}, {"heading": "6 Conclusion", "text": "In this paper, we use the \"monitored\" alignments and set the alignment costs on the NMT lens function. In this way, we optimize the attention model directly in a monitored way. Experiments show significant improvements in both translation and alignment tasks compared to a very strong LVNMT system."}, {"heading": "Acknowledgment", "text": "We thank the anonymous reviewers for their useful comments."}], "references": [{"title": "Automatic evaluation measures for statistical machine translation system optimization", "author": ["Arne Mauser", "Ney2008] Sasa Hasan Arne Mauser", "Hermann Ney"], "venue": "In Proceedings of LREC", "citeRegEx": "Mauser et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mauser et al\\.", "year": 2008}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Bahdanau et al.2014] D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Agreement-based joint training for bidirectional attention-based neural machine translation", "author": ["Cheng et al.2016] Yong Cheng", "Shiqi Shen", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Flexible and efficient hypergraph interactions for joint hierarchical and forest-to-string decoding", "author": ["Haitao Mi", "Bowen Zhou"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cmejrek et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cmejrek et al\\.", "year": 2013}, {"title": "Clause restructuring for statistical machine translation", "author": ["Philipp Koehn", "Ivona Kucerova"], "venue": "In Proceedings of ACL,", "citeRegEx": "Collins et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Tuning as ranking", "author": ["Hopkins", "May2011] Mark Hopkins", "Jonathan May"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Hopkins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2011}, {"title": "A maximum entropy word aligner for arabic-english machine translation", "author": ["Ittycheriah", "Roukos2005] Abraham Ittycheriah", "Salim Roukos"], "venue": "In HLT \u201905: Proceedings of the HLT and EMNLP,", "citeRegEx": "Ittycheriah et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ittycheriah et al\\.", "year": 2005}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean et al.2015] S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of ACL,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Alignment by agreement", "author": ["Liang et al.2006] P. Liang", "B. Taskar", "D. Klein"], "venue": "In North American Association for Computational Linguistics (NAACL),", "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Joint decoding with multiple translation models", "author": ["Liu et al.2009] Yang Liu", "Haitao Mi", "Yang Feng", "Qun Liu"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "2016a. A coverage embedding model for neural machine translation. ArXiv e-prints", "author": ["Mi et al.2016a] Haitao Mi", "Baskaran Sankaran", "Zhiguo Wang", "Abe Ittycheriah"], "venue": null, "citeRegEx": "Mi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "2016b. Vocabulary manipulation for neural machine translation", "author": ["Mi et al.2016b] Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah"], "venue": "In Proceedings of ACL,", "citeRegEx": "Mi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Improved statistical alignment models", "author": ["Och", "Ney2000] Franz Josef Och", "Hermann Ney"], "venue": "In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Och et al\\.", "year": 2000}, {"title": "Coverage-based Neural Machine Translation", "author": ["Tu et al.2016] Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li"], "venue": null, "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "ADADELTA: an adaptive learning rate method. CoRR", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Generalizing local and non-local word-reordering patterns for syntax-based machine translation", "author": ["Zhao", "Al-onaizan2008] Bing Zhao", "Yaser Alonaizan"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Zhao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al.", "startOffset": 75, "endOffset": 137}, {"referenceID": 8, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al.", "startOffset": 75, "endOffset": 137}, {"referenceID": 11, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al.", "startOffset": 75, "endOffset": 137}, {"referenceID": 15, "context": "However, the attention or alignment quality of NMT is still very low (Mi et al., 2016a; Tu et al., 2016).", "startOffset": 69, "endOffset": 104}, {"referenceID": 1, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al. (2014). The attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word.", "startOffset": 76, "endOffset": 207}, {"referenceID": 1, "context": "As shown in Figure 1, attention-based NMT (Bahdanau et al., 2014) is an encoder-decoder network.", "startOffset": 42, "endOffset": 65}, {"referenceID": 1, "context": "Figure 1: The architecture of attention-based NMT (Bahdanau et al., 2014).", "startOffset": 50, "endOffset": 73}, {"referenceID": 15, "context": "However, the accuracy is still far behind the traditional MaxEnt alignment model in terms of alignment F1 score (Mi et al., 2016b; Tu et al., 2016).", "startOffset": 112, "endOffset": 147}, {"referenceID": 9, "context": "(2016) adapted the agreementbased learning (Liang et al., 2006; Liang et al., 2008), and introduced a combined objective that takes into account both translation directions (source-to-target and target-to-source) and an agreement term between the two alignment directions.", "startOffset": 43, "endOffset": 83}, {"referenceID": 2, "context": "In order to improve the attention or alignment accuracy, Cheng et al. (2016) adapted the agreementbased learning (Liang et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 16, "context": "In the training procedure, we use AdaDelta (Zeiler, 2012) to update model parameters with a mini-batch size 80.", "startOffset": 43, "endOffset": 57}, {"referenceID": 12, "context": "Following Mi et al. (2016a), the output vocabulary for each mini-batch or sentence is a sub-set of the full vo-", "startOffset": 10, "endOffset": 28}, {"referenceID": 5, "context": "For each source sentence, the sentencelevel target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from \u2018fast align\u2019 (Dyer et al., 2013).", "startOffset": 209, "endOffset": 228}, {"referenceID": 10, "context": "Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013).", "startOffset": 144, "endOffset": 184}, {"referenceID": 3, "context": "Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013).", "startOffset": 144, "endOffset": 184}, {"referenceID": 4, "context": "For each source sentence, the sentencelevel target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from \u2018fast align\u2019 (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. The Cov. LVNMT system is a re-implementation of the enhanced NMT system of Mi et al. (2016a), which employs a coverage embedding model and achieves better performance over large vocabulary NMT Jean et al.", "startOffset": 210, "endOffset": 454}, {"referenceID": 4, "context": "For each source sentence, the sentencelevel target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from \u2018fast align\u2019 (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. The Cov. LVNMT system is a re-implementation of the enhanced NMT system of Mi et al. (2016a), which employs a coverage embedding model and achieves better performance over large vocabulary NMT Jean et al. (2015). The coverage embedding dimension of each source word is 100.", "startOffset": 210, "endOffset": 573}, {"referenceID": 4, "context": "For each source sentence, the sentencelevel target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from \u2018fast align\u2019 (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. The Cov. LVNMT system is a re-implementation of the enhanced NMT system of Mi et al. (2016a), which employs a coverage embedding model and achieves better performance over large vocabulary NMT Jean et al. (2015). The coverage embedding dimension of each source word is 100. Following Jean et al. (2015), we dump the alignments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word.", "startOffset": 210, "endOffset": 664}, {"referenceID": 4, "context": "In terms of BLEU scores, we conduct the statistical significance tests with the signtest of Collins et al. (2005), the results show that the improvements of our J + Gau.", "startOffset": 92, "endOffset": 114}], "year": 2016, "abstractText": "In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the \u201ctrue\u201d alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system.", "creator": "LaTeX with hyperref package"}}}