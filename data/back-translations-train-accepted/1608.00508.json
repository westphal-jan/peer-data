{"id": "1608.00508", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Aug-2016", "title": "Blind phoneme segmentation with temporal prediction errors", "abstract": "Phonemic segmentation of speech is a critical step of speech recognition systems. We propose a novel unsupervised algorithm based on sequence prediction models such as Markov chains and recurrent neural network. Our approach consists in analyzing the error profile of a model trained to predict speech features frame-by-frame. Specifically, we try to learn the dynamics of speech in the MFCC space and hypothesize boundaries from local maxima in the prediction error. We evaluate our system on the TIMIT dataset, with improvements over similar methods.", "histories": [["v1", "Mon, 1 Aug 2016 17:51:03 GMT  (435kb,D)", "http://arxiv.org/abs/1608.00508v1", "5 pages 3 figures. Submitted to SLT"], ["v2", "Sat, 27 May 2017 04:01:13 GMT  (444kb,D)", "http://arxiv.org/abs/1608.00508v2", "7 pages 3 figures. Presented at ACL SRW 2017"]], "COMMENTS": "5 pages 3 figures. Submitted to SLT", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["paul michel", "okko r\\\"as\\\"anen", "roland thiolli\\`ere", "emmanuel dupoux"], "accepted": true, "id": "1608.00508"}, "pdf": {"name": "1608.00508.pdf", "metadata": {"source": "CRF", "title": "IMPROVING PHONEME SEGMENTATION WITH RECURRENT NEURAL NETWORKS", "authors": ["Paul Michel", "Okko Rasanen", "Roland Thiolli\u00e8re", "Emmanuel Dupoux"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Recurrent Neural Networks, Statistical Learning, Phone Segmentation, Language Segmentation"}, {"heading": "1. INTRODUCTION", "text": "One of the main difficulties of speech processing as opposed to word processing is the continuous, time-dependent nature of the signal. As a result, pre-segmentation of the speech signal into words or subword units such as phonemes syllables or words is an essential first step in a variety of speech recognition tasks. Phonematic segmentation is useful for a number of applications (annotation of language for phonetic analysis, calculation of speech rate, keyword spotting, etc.) and can be done in two ways. Monitored methods are based on an existing phoneme or word recognition system that is used to decipher incoming language into phonemes. Phonemal boundaries can then be extracted as a by-product of aligning the phoneme models with speech. Unmonitored methods (also referred to as blind segmentation) consist of predicting the boundaries of language using categorical signals."}, {"heading": "2. RELATED WORK", "text": "Most of the previous work on the segmentation of blind phonemes [2, 3, 4, 5, 6] focused on analyzing the rate of change in the spectral domain. The idea is to design robust acoustic characteristics to remain stable within a phoneme, and to change as the phoneme transitions from one to the next. The algorithm then defines a measure of change that is then used to detect phoneme boundaries. Apart from this line of research, three main approaches have been studied, the first being to use short-term statistical dependencies. In [7], the idea was to first discredit the signal using a cluster algorithm and then calculate discrete sequence statistics to define a threshold. This is the idea we are following in the current work. The second approach is to use dynamic programming methods inspired by text segmentation [8] to obtain optimal segmentation."}, {"heading": "3. SYSTEM", "text": "The two types of language characteristics: 13-dimensional MFCCs [15] (with 12 MFCCs) are used to avoid three phase transmissions, which are then applied to a random subset of MFCCs (10,000 frames were randomly selected), with a target number of 8 clusters of 8, then each frame is represented by a cluster number. Each frame is then replaced by a random subset of MFCCs (10,000 frames), or alternatively by the corresponding vector of dimension 8. These hyperparameters are based on [7].Figure 1 allows a visual comparison of the three signals (waveform, MFCC, categorical).The entire dataset is split between a training and an examination."}, {"heading": "4. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Dataset", "text": "We evaluated our methods using the TIMIT dataset [20]. The TIMIT dataset consists of 6300 utterances (\u0445 5.4 hours) of 630 speakers from 8 dialects of the English language. The corpus was divided into a training and test set according to the standard split, containing 4620 utterances (172,460 limits) and the test set 1680 (65,825 limits)."}, {"heading": "4.2. Evaluation", "text": "The performance assessment of our system is based on precision and recall. Since Ng is the number of gold limits, Nh the number of hypothetical limits, and Nc the number of correctly detected limits, we define precision (P), callback (R), and F score (F) as follows: P = NcNhR = NcNgF = 2PRP + RThe F score, defined as the harmonic mean of precision and recall, returns a single value, which is often used as a comparison for different algorithms. A disadvantage of this metric is that a high recall, low precision results, such as those generated by the hypothesis of a limit every 5 ms (P: 58%, R: 91%), results in a high F score (70%). Other metrics have been designed to address this problem. One such example is the R value [21]: R \u2212 val = 1 \u2212 setpoint (1 \u2212 OS + 2 + OS2 + 2 + OS2 + OS2."}, {"heading": "4.3. Results", "text": "The current state of the art in the segmentation of blind phonemes on the TIMIT corpus is provided by [6]. It evaluates our best results with 78.16% F score and 81.11% R value on the training part of the data set using a similar evaluation method to ours. In Tables 1 and 2, we compare our best results with the previous statistical approach evaluated in [7] and the na\u00efve segmentation of periodic boundaries (one boundary every 5ms). Since [7] an evaluation method was used that allows for overlaps between tolerance windows, we provide our results for consistency with both evaluation methods (full and truncated windows). Figure 2 provides an overview of the accuracy / retrieval values when variing the maximum detection threshold (and in the case of periodic boundaries of the period). This gives us some insight into the actual behavior of the different algorithms, especially in the highly precise, MN region where the distinction region appears."}, {"heading": "5. DISCUSSION", "text": "In terms of optimal F-score and R-score values, the simple Markov model exceeded the previously published work using short-term sequential statistics [7], as well as recurrent neural networks. However, these optimal values may obscure the differential behavior of these algorithms in different sections of the precision / retrieval curve. In particular, it is interesting to note that the neural network model trained on the raw MFCCs yielded very good results in the low retrieval and precision domain. In fact, the precision can reach 90% with a retrieval curve of 40%. Such a regime could be useful, for example, when using blind phoneme segmentation to help with word segmentation. The reason for the increase in precision of neural networks could be that it combines the sensitivity of this model with sequential statistical regularities of the signal, but also for spectral variations, i.e. errors associated with the Xtron dimensions, which means that some changes in the Xtron dimension are also associated with high."}, {"heading": "6. CONCLUSIONS", "text": "We have presented a fast, lightweight method for segmenting blind phonemes that predicts the limits of peak predictive loss of transition probability models, and the various models we have tested have produced satisfactory results while remaining mathematically tractable. Our recurrent neural network, trained on language characteristics, points in particular to a possibility of combining both statistical and spectral information in a single model.From the perspective of machine learning, we emphasized the use of lateral channel information (in this case, the test error) to extract structure from raw data in an unattended environment.Future work could include exploring various RNN models, evaluating the stability of these methods using simpler features such as raw spectrograms, or exploring the representation of each image in the hidden layers of the networks."}, {"heading": "7. ACKNOWLEDGEMENTS", "text": "This project is supported by the European Research Council (ERC-2011-AdG-295810 BOOTPHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR10-IDEX-0001-02 PSL *), the Fondation de France, the Ecole de Neurosciences de Paris, the Ile de France Region (DIM cerveau et pense'e) and an AWS in Education Research Grant."}, {"heading": "8. REFERENCES", "text": "[1] Sorin Dusan and Lawrence R Rabiner, \"On the relation between maximum spectral transition positions and phone boundaries,\" in INTERSPEECH. Citeseer, 2006. [2] Anna Esposito and Guido Aversano, \"Text independent methods for speech segmentation,\" in Nonlinear Speech Modeling and Applications, pp. 261-290. [3] Yago Pereiro Estevan, Vincent Wan, and Odette Scharenborg, \"Finding maximum margin segments in speech,\" in 2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP '07. IEEE, 2007, vol. 4, pp. IV-937] George Almpanidis and Constantine Kotropoulos, \"Phonemic Okamba distribution using the alized gamma information criterion.\""}], "references": [{"title": "On the relation between maximum spectral transition positions and phone boundaries", "author": ["Sorin Dusan", "Lawrence R Rabiner"], "venue": "INTERSPEECH. Citeseer, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Text independent methods for speech segmentation", "author": ["Anna Esposito", "Guido Aversano"], "venue": "Nonlinear Speech Modeling and Applications, pp. 261\u2013290. Springer, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Finding maximum margin segments in speech", "author": ["Yago Pereiro Estevan", "Vincent Wan", "Odette Scharenborg"], "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP\u201907. IEEE, 2007, vol. 4, pp. IV\u2013937.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Phonemic segmentation using the generalised gamma distribution and small sample bayesian information criterion", "author": ["George Almpanidis", "Constantine Kotropoulos"], "venue": "Speech Communication, vol. 50, no. 1, pp. 38\u2013 55, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Blind segmentation of speech using non-linear filtering methods, INTECH", "author": ["Okko Rasanen", "Toomas Altosaar", "Unto Laine"], "venue": "Open Access Publisher,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Blind phone segmentation based on spectral change detection using legendre polynomial approximation", "author": ["Dac-Thang Hoang", "Hsiao-Chuan Wang"], "venue": "The Journal of the Acoustical Society of America, vol. 137, no. 2, pp. 797\u2013805, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Basic cuts revisited: Temporal segmentation of speech into phone-like units with statistical learning at a pre-linguistic level", "author": ["Okko R\u00e4s\u00e4nen"], "venue": "Proceedings of the 36th Annual Conference of the Cognitive Science Society. Quebec, Canada, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "The concave least-weight subsequence problem revisited", "author": ["Robert Wilber"], "venue": "Journal of Algorithms, vol. 9, no. 3, pp. 418\u2013425, 1988.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1988}, {"title": "Unsupervised optimal phoneme segmentation: Objectives, algorithm and comparisons", "author": ["Yu Qiao", "Naoya Shimomura", "Nobuaki Minematsu"], "venue": "2008 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2008, pp. 3989\u20133992.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Fully unsupervised small-vocabulary speech recognition using a segmental bayesian model", "author": ["Herman Kamper", "Aren Jansen", "Sharon Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "A probabilistic framework for segmentbased speech recognition", "author": ["James R Glass"], "venue": "Computer Speech & Language, vol. 17, no. 2, pp. 137\u2013152, 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Unsupervized training of an HMM-based self-organizing recognizer with applications to topic classification and keyword discovery", "author": ["Man-hung Siu", "Herbert Gish", "Arthur Chan", "William Belfield", "Steve Lowe"], "venue": "Computer Speech & Language, vol. preprint, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning to segment speech using multiple cues: A connectionist model", "author": ["Morten H Christiansen", "Joseph Allen", "Mark S Seidenberg"], "venue": "Language and cognitive processes, vol. 13, no. 2-3, pp. 221\u2013268, 1998.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["Steven Davis", "Paul Mermelstein"], "venue": "IEEE transactions on acoustics, speech, and signal processing, vol. 28, no. 4, pp. 357\u2013366, 1980.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1980}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1990}, {"title": "Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, no. 2, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1929}, {"title": "The darpa speech recognition research database: Specifications and status", "author": ["William M. Fischer", "George R. Doddington", "Kathleen M Goudie-Marshall"], "venue": "Proceedings of DARPA Workshop on Speech Recognition, 1986, pp. 93\u201399.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1986}, {"title": "An improved speech segmentation quality measure: the r-value", "author": ["Okko R\u00e4s\u00e4nen", "Unto Kalervo Laine", "Toomas Altosaar"], "venue": "Proceedings of Interspeech, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "This last model is especially interesting in that it couples our statistical approach with more common spectral transition based methods ([1] for instance).", "startOffset": 138, "endOffset": 141}, {"referenceID": 1, "context": "Most previous work on blind phoneme segmentation [2, 3, 4, 5, 6] has focused on the analysis of the rate of change in the spectral domain.", "startOffset": 49, "endOffset": 64}, {"referenceID": 2, "context": "Most previous work on blind phoneme segmentation [2, 3, 4, 5, 6] has focused on the analysis of the rate of change in the spectral domain.", "startOffset": 49, "endOffset": 64}, {"referenceID": 3, "context": "Most previous work on blind phoneme segmentation [2, 3, 4, 5, 6] has focused on the analysis of the rate of change in the spectral domain.", "startOffset": 49, "endOffset": 64}, {"referenceID": 4, "context": "Most previous work on blind phoneme segmentation [2, 3, 4, 5, 6] has focused on the analysis of the rate of change in the spectral domain.", "startOffset": 49, "endOffset": 64}, {"referenceID": 5, "context": "Most previous work on blind phoneme segmentation [2, 3, 4, 5, 6] has focused on the analysis of the rate of change in the spectral domain.", "startOffset": 49, "endOffset": 64}, {"referenceID": 6, "context": "In [7], the idea was to first discretize the signal using a clustering algorithm and then compute discrete sequence statistics, over which a threshold can be defined.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "The second approach is to use dynamic programming methods inspired by text segmentation [8], in order to derive optimal segmentation [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "The second approach is to use dynamic programming methods inspired by text segmentation [8], in order to derive optimal segmentation [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 9, "context": "The third approach consists in jointly segmenting and learning the acoustic models for phonemes [10, 11, 12].", "startOffset": 96, "endOffset": 108}, {"referenceID": 10, "context": "The third approach consists in jointly segmenting and learning the acoustic models for phonemes [10, 11, 12].", "startOffset": 96, "endOffset": 108}, {"referenceID": 11, "context": "The third approach consists in jointly segmenting and learning the acoustic models for phonemes [10, 11, 12].", "startOffset": 96, "endOffset": 108}, {"referenceID": 12, "context": "from previous work by Elman [13] and Christiansen et al.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "[14] published in the 90s.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "\u201d [13].", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "We used two kinds of speech features : 13 dimensional MFCCs [15] (with 12 mel-cepstrum coefficients and 1 energy coefficient) and categorical one-hot vectors derived from MFCCs inspired by [7].", "startOffset": 60, "endOffset": 64}, {"referenceID": 6, "context": "We used two kinds of speech features : 13 dimensional MFCCs [15] (with 12 mel-cepstrum coefficients and 1 energy coefficient) and categorical one-hot vectors derived from MFCCs inspired by [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 6, "context": "The latter are computed according to [7] : K-means clustering is performed on a random subset of the MFCCs (10000 frames were selected at random), with a target number of clusters of 8, then each MFCC is identified to the closest centroid.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "These hyper-parameters were chosen according to [7].", "startOffset": 48, "endOffset": 51}, {"referenceID": 15, "context": ", x0)) is minimized using back propagation through time [16] and stochastic gradient descent or a variant thereof (we have found RMSProp [17] to give the best results).", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": ", x0)) is minimized using back propagation through time [16] and stochastic gradient descent or a variant thereof (we have found RMSProp [17] to give the best results).", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": "In our case, the network itself consists of two LSTM layers [18] stacked on one another followed by a linear layer and a softmax.", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "Dropout [19] with probability 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "9 Rasanen, 2014[7] 68.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "8 Rasanen, 2014[7] 74.", "startOffset": 15, "endOffset": 18}, {"referenceID": 19, "context": "We evaluated our methods on the TIMIT dataset [20].", "startOffset": 46, "endOffset": 50}, {"referenceID": 20, "context": "One such example is the R-value [21] :", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "Further details can be found in [21].", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "We decided to use the procedure described in [21] to match gold boundaries and hypothesized boundaries : overlapping tolerance windows are cropped in the middle of the two boundaries.", "startOffset": 45, "endOffset": 49}, {"referenceID": 5, "context": "The current state of the art in blind phoneme segmentation on the TIMIT corpus is provided by [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "In Tables 1 and 2 we compare our best results to the previous statistical approach evoked in [7] and the naive periodic boundaries segmentation (one boundary each 5ms).", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "Since [7] used an evaluation method allowing for tolerance windows to overlap, we provide our results with both evaluation methods (full windows and cropped windows) for the sake of consistency.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "In terms of optimal F-score and R values, the simple Markov model outperformed the previously published paper using short term sequential statistics [7], as well as the recurrent", "startOffset": 149, "endOffset": 152}], "year": 2016, "abstractText": "Phonemic segmentation of speech is a critical step of speech recognition systems. We propose a novel unsupervised algorithm based on sequence prediction models such as Markov chains and recurrent neural network. Our approach consists in analyzing the error profile of a model trained to predict speech features frame-by-frame. Specifically, we try to learn the dynamics of speech in the MFCC space and hypothesize boundaries from local maxima in the prediction error. We evaluate our system on the TIMIT dataset, with improvements over similar methods.", "creator": "LaTeX with hyperref package"}}}