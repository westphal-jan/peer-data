{"id": "1608.00612", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Aug-2016", "title": "Structured prediction models for RNN based sequence labeling in clinical text", "abstract": "Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In clinical domain one major application of sequence labeling involves extraction of medical entities such as medication, indication, and side-effects from Electronic Health Record narratives. Sequence labeling in this domain, presents its own set of challenges and objectives. In this work we experimented with various CRF based structured learning models with Recurrent Neural Networks. We extend the previously studied LSTM-CRF models with explicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methodologies for structured prediction in order to improve the exact phrase detection of various medical entities.", "histories": [["v1", "Mon, 1 Aug 2016 20:54:22 GMT  (304kb,D)", "http://arxiv.org/abs/1608.00612v1", "To appear in the proceedings of EMNLP 2016"]], "COMMENTS": "To appear in the proceedings of EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["abhyuday jagannatha", "hong yu"], "accepted": true, "id": "1608.00612"}, "pdf": {"name": "1608.00612.pdf", "metadata": {"source": "CRF", "title": "Structured prediction models for RNN based sequence labeling in clinical text", "authors": ["Abhyuday N Jagannatha", "Hong Yu"], "emails": ["abhyuday@cs.umass.edu", "hong.yu@umassmed.edu"], "sections": [{"heading": null, "text": "Sequence tagging is a widely used method for identifying named entities and extracting information from unstructured natural speech data. In clinical settings, one of the most important uses of sequence tagging involves extracting medical entities such as drugs, indications and adverse events from electronic health narratives. Sequence tagging in this area presents its own challenges and objectives. In this work, we have experimented with various CRF-based structured learning models with recursive neural networks. We are extending the previously studied LSTM-CRF models with explicit modelling of pairwise potentials. We also propose an approximate version of CRF inference with RNN potentials. We use these methods for structured prediction to improve the accurate phrase recognition of various medical entities."}, {"heading": "1 Introduction", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2 Related Work", "text": "As mentioned in the previous sections, both Neural Networks and Conditional Random Fields have widespread sequence marking tasks in NLP. Specifically, CRFs (Lafferty et al., 2001) have a long history of use for various sequence marking tasks in general and designated entity findings in particular. Some early notable work includes McCallum et. al. (2003), Sarawagi et al. (2004) and Sha et. al. (2003). Hammerton et. al. (2003) used Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber) for designated entity findings. Several recent work on both image and text domains have used structured conclusions to improve the performance of Neural Network Models. In NLP, Collobert et al al al al al al al al al al."}, {"heading": "3 Methods", "text": "We use Bi-RNNs as feature extractors from the word order. We evaluate three different methods of structured learning. The starting point is a bidirectional relapsing neural network as described in Section 3.1."}, {"heading": "3.1 Bi-LSTM (baseline)", "text": "This model is a bidirectional LSTM neural network with word embedding input and a softmax output layer. The raw natural speech input set is fed into a sequence of tokens x = [xt] T1 using a tokenizer of regular expressions. The token sequence is fed into the embedding layer, which generates a dense vector representation of words. Output of the bidirectional RNN then generates a bidirectional vector sequence. This bidirectional RNN, together with the embedding layer, is the main machine responsible for learning a good feature representation of the data. Output of the bidirectional RNN generates a vector sequence \u03c9 (x) = [\u03c9 (x)] T1 with the same length as the input sequence x. In this basic model, we do not use structured inference. Therefore, this model can be used solely to predict the label sequence by normalizing the seismic sequence."}, {"heading": "3.2 Bi-LSTM CRF", "text": "This model is adapted to the Bi-LSTM CRF model described in Huang et. al. (2015). It combines the framework of the bidirectional RNN layer [\u03c9 (x)] T1 with the linear chain CRF inference. For a general linear chain CRF, the probability of a label sequence y for a given sentence x can be described as follows: P (y, yt, yt + 1) is the paired potential between positions t, t + 1. Similar to Huang et. al. (2015), the results of the bidirectional RNN layer \u03c9 (x) are the uniform potential for the label position t and vice versa (yt, yt + 1) is the paired potential between positions t, t + 1. Similar to Huang et. al. (2015), the results of the bidirectional RNN layer \u03c9 (x) are used to model the non-uniform potentials of a linear chain CRF."}, {"heading": "3.3 Bi-LSTM CRF with pairwise modeling", "text": "In the previous section, the paired potential is calculated by a transition probability matrix [A] regardless of the current context or word. For the reasons mentioned in Section 1, this may not be an effective strategy. Some medical facilities are relatively rare. Therefore, the transition from an outside label to a medical label could not be effectively modeled by a fixed parameter matrix. In this method, the paired potentials are modeled by a non-linear neural network depending on the current word and context. Specifically, the paired potential in Equation 2 is calculated by using a one-dimensional CNN with 1-D filter size 2 and tanh nonlinearity. At each label position t, it takes [\u03c9 (x) t; \u03c9 (x) t + 1] as input and produces a paired potential in Equation 2 by using nn (yt, yt + 1) and tanh nonlinearity."}, {"heading": "3.4 Approximate Skip-chain CRF", "text": "It is not only the way in which we move in the world, but also the way in which we move in the world, in which we move in the world, in which we move in the world, in which we move in the world, in which we move in the world, in which we live in the world, in which we live in the world, in which we live in the world, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live"}, {"heading": "4 Dataset", "text": "We use an annotated corpus of 1154 English electronic health records of cancer patients. Each note has been commented on by two commentators who divide medical facilities into several categories. These categories can be roughly divided into two groups, medical events and attributes. Medical events encompass any specific event that leads or could contribute to a change in a patient's medical status. Attributes are formulations that describe certain important characteristics about the events.Medical event categories in this corpus are adverse drug events (ADE), drugs, indications and other signs and illnesses (Other SSD).ADE, indication and other SSDs are events that share a common vocabulary of signs, symptoms and diseases (SSD) and can be distinguished depending on the context in which they are used. A specific SSD should be referred to as ADE if it is a side effect of a drug."}, {"heading": "5 Experiments", "text": "The models work on the tokenized sets. To do this, we limited the set length to 50 tokens. All sets longer than 50 tokens were trimmed to size, and the shorter sets were pre-lined with masks. The first layer for all models was a 200-dimensional word embedding layer. To improve performance, we initialized layer values in all models with a Skip-gram word embedding (Mikolov et al., 2013). The first layer for all models was a 200-dimensional word embedding layer. To improve the layer values in all models with a Skip-gram word embedding, we embedded the word in the drawer."}, {"heading": "6 Results", "text": "As shown in Table 2, the best performance is Skip-Chain CRF (0.8210 for strict and 0.8632 for relaxed evaluation). Models that use an exact CRF inference improve the accuracy of the strict evaluation by 2 to 5 percentage points. Bi-LSTM CRF pair achieved the highest precision for exact match. However, the (both strict and relaxed) recall for exact CRF models is less than Bi-LSTM. This reduction in the recall is much lower in the Bi-LSTMpair model. In relaxed evaluation, only the Skip-Chain model has a better F score than the base model LSTM. Overall, Bi-LSTM-CRF pair and ApproxSkip-Chain models show significant performance improvements."}, {"heading": "7 Discussion", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "8 Conclusion", "text": "We have shown that the modelling of paired potentials and the use of an approximate version of skip chain inference increases the performance of the Bi-LSTMCRF model. These results suggest that the structured prediction models are a good direction to improve accurate phrase extraction for medical facilities."}, {"heading": "Acknowledgments", "text": "We thank the UMassMed Comments Team: Elaine Freund, Wiesong Liu, Steve Belknap, Nadya Frid, Alex Granillo, Heather Keating, and Victoria Wang for creating the gold standard evaluation kit used in this work. We also thank the anonymous reviewers for their comments and suggestions. This work was partially supported by the National Institutes of Health (NIH) HL125089 Grant. We also thank the United States Department of Veterans Affairs (VA) for its support with the 1I01HX001457 Award. This work was also partially supported by the Center for Intelligent Information Retrieval. The content of this paper does not represent the views of CIIR, NIH, VA, or the U.S. government."}], "references": [{"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Chiu", "Nichols2015] Jason PC Chiu", "Eric Nichols"], "venue": "arXiv preprint arXiv:1511.08308", "citeRegEx": "Chiu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "An empirical evaluation of resources for the identification of diseases and adverse effects in biomedical literature", "author": ["Roman Klinger", "Martin Hofmann-Apitius", "Juliane Fluck"], "venue": null, "citeRegEx": "Gurulingappa et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gurulingappa et al\\.", "year": 2010}, {"title": "Named entity recognition with long short-term memory", "author": ["James Hammerton"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume", "citeRegEx": "Hammerton.,? \\Q2003\\E", "shortCiteRegEx": "Hammerton.", "year": 2003}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167", "author": ["Ioffe", "Szegedy2015] Sergey Ioffe", "Christian Szegedy"], "venue": null, "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando CN Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "arXiv preprint arXiv:1603.01360", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Biomedical named entity recognition based on extended recurrent neural networks", "author": ["Li et al.2015] Lishuang Li", "Liuke Jin", "Zhenchao Jiang", "Dingxin Song", "Degen Huang"], "venue": "In Bioinformatics and Biomedicine (BIBM),", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Deeply learning the messages in message passing inference", "author": ["Lin et al.2015] Guosheng Lin", "Chunhua Shen", "Ian Reid", "Anton van den Hengel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Early results for named entity recognition with conditional random fields, feature induction and webenhanced lexicons", "author": ["McCallum", "Li2003] Andrew McCallum", "Wei Li"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL", "citeRegEx": "McCallum et al\\.,? \\Q2003\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2003}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Deep patient: An unsupervised representation to predict the future of patients from the electronic health records", "author": ["Li Li", "Brian A Kidd", "Joel T Dudley"], "venue": "Scientific reports,", "citeRegEx": "Miotto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miotto et al\\.", "year": 2016}, {"title": "A novel method of adverse event detection can accurately identify venous thromboembolisms (vtes) from narrative electronic health record", "author": ["Aman D Verma", "Tewodros Eguale", "Todd C Lee", "David L Buckeridge"], "venue": null, "citeRegEx": "Rochefort et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rochefort et al\\.", "year": 2015}, {"title": "Semi-markov conditional random fields for information extraction", "author": ["Sarawagi", "Cohen2004] Sunita Sarawagi", "William W Cohen"], "venue": null, "citeRegEx": "Sarawagi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sarawagi et al\\.", "year": 2004}, {"title": "Biomedical named entity recognition using conditional random fields and rich feature sets", "author": ["Burr Settles"], "venue": "In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications,", "citeRegEx": "Settles.,? \\Q2004\\E", "shortCiteRegEx": "Settles.", "year": 2004}, {"title": "Shallow parsing with conditional random fields", "author": ["Sha", "Pereira2003] Fei Sha", "Fernando Pereira"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-", "citeRegEx": "Sha et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sha et al\\.", "year": 2003}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "An introduction to conditional random fields for relational learning. Introduction to statistical relational learning, pages 93\u2013128", "author": ["Sutton", "McCallum2006] Charles Sutton", "Andrew McCallum"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 6, "context": "Recent works on Named Entity Recognition by (Huang et al., 2015) and others have combined the benefits of Neural Networks with CRF by modeling the unary potential functions of a CRF as NN models.", "startOffset": 44, "endOffset": 64}, {"referenceID": 8, "context": "Specially, CRFs (Lafferty et al., 2001) have a long history of being used for various sequence labeling tasks in general and named entity recognition in particular.", "startOffset": 16, "endOffset": 39}, {"referenceID": 7, "context": "Specially, CRFs (Lafferty et al., 2001) have a long history of being used for various sequence labeling tasks in general and named entity recognition in particular. Some early notable works include McCallum et. al. (2003), Sarawagi et al.", "startOffset": 17, "endOffset": 222}, {"referenceID": 7, "context": "Specially, CRFs (Lafferty et al., 2001) have a long history of being used for various sequence labeling tasks in general and named entity recognition in particular. Some early notable works include McCallum et. al. (2003), Sarawagi et al. (2004) and Sha et.", "startOffset": 17, "endOffset": 246}, {"referenceID": 7, "context": "Specially, CRFs (Lafferty et al., 2001) have a long history of being used for various sequence labeling tasks in general and named entity recognition in particular. Some early notable works include McCallum et. al. (2003), Sarawagi et al. (2004) and Sha et. al. (2003). Hammerton et.", "startOffset": 17, "endOffset": 269}, {"referenceID": 4, "context": "Hammerton et. al. (2003) and Chiu et.", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": "Hammerton et. al. (2003) and Chiu et. al. (2015) used Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) for named entity recognition.", "startOffset": 0, "endOffset": 49}, {"referenceID": 9, "context": "Specifically for Recurrent Neural Networks, Lample et al. (2016) and Huang et.", "startOffset": 44, "endOffset": 65}, {"referenceID": 9, "context": "Specifically for Recurrent Neural Networks, Lample et al. (2016) and Huang et. al. (2015) used LSTMs to model the unary potentials of a CRF.", "startOffset": 44, "endOffset": 90}, {"referenceID": 17, "context": "Settles (2004) used Conditional Random Fields to extract occurrences of protein, DNA and similar biological entity classes.", "startOffset": 0, "endOffset": 15}, {"referenceID": 17, "context": "Settles (2004) used Conditional Random Fields to extract occurrences of protein, DNA and similar biological entity classes. Li et. al. (2015) recently used LSTM for named entity recognition or protein/gene names from BioCreative corpus.", "startOffset": 0, "endOffset": 142}, {"referenceID": 17, "context": "Settles (2004) used Conditional Random Fields to extract occurrences of protein, DNA and similar biological entity classes. Li et. al. (2015) recently used LSTM for named entity recognition or protein/gene names from BioCreative corpus. Gurulingappa et. al. (2010) evaluated various existing biomedical dictionaries on extraction of adverse effects and diseases from a corpus of Medline abstracts.", "startOffset": 0, "endOffset": 265}, {"referenceID": 15, "context": "Other works using a real world medical corpus include Rochefort et al. (2015), who", "startOffset": 54, "endOffset": 78}, {"referenceID": 13, "context": "In order to improve performance, we initialized embedding layer values in all models with a skip-gram word embedding (Mikolov et al., 2013).", "startOffset": 117, "endOffset": 139}, {"referenceID": 19, "context": "We use dropout (Srivastava et al., 2014) with a probability of 0.", "startOffset": 15, "endOffset": 40}, {"referenceID": 2, "context": "All models are trained in an end-to-end fashion using Adagrad (Duchi et al., 2011) with momentum.", "startOffset": 62, "endOffset": 82}], "year": 2016, "abstractText": "Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In clinical domain one major application of sequence labeling involves extraction of medical entities such as medication, indication, and side-effects from Electronic Health Record narratives. Sequence labeling in this domain, presents its own set of challenges and objectives. In this work we experimented with various CRF based structured learning models with Recurrent Neural Networks. We extend the previously studied LSTM-CRF models with explicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methodologies1 for structured prediction in order to improve the exact phrase detection of various medical entities.", "creator": "LaTeX with hyperref package"}}}