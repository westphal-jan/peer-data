{"id": "1608.00778", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Aug-2016", "title": "Exponential Family Embeddings", "abstract": "Word embeddings are a powerful approach for capturing semantic similarity among terms in a vocabulary. In this paper, we develop exponential family embeddings, a class of methods that extends the idea of word embeddings to other types of high-dimensional data. As examples, we studied neural data with real-valued observations, count data from a market basket analysis, and ratings data from a movie recommendation system. The main idea is to model each observation conditioned on a set of other observations. This set is called the context, and the way the context is defined is a modeling choice that depends on the problem. In language the context is the surrounding words; in neuroscience the context is close-by neurons; in market basket data the context is other items in the shopping cart. Each type of embedding model defines the context, the exponential family of conditional distributions, and how the latent embedding vectors are shared across data. We infer the embeddings with a scalable algorithm based on stochastic gradient descent. On all three applications - neural activity of zebrafish, users' shopping behavior, and movie ratings - we found exponential family embedding models to be more effective than other types of dimension reduction. They better reconstruct held-out data and find interesting qualitative structure.", "histories": [["v1", "Tue, 2 Aug 2016 11:44:19 GMT  (703kb,D)", "http://arxiv.org/abs/1608.00778v1", null], ["v2", "Mon, 21 Nov 2016 15:12:54 GMT  (709kb,D)", "http://arxiv.org/abs/1608.00778v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["maja r rudolph", "francisco j r ruiz", "stephan mandt", "david m blei"], "accepted": true, "id": "1608.00778"}, "pdf": {"name": "1608.00778.pdf", "metadata": {"source": "CRF", "title": "Exponential Family Embeddings", "authors": ["Maja R. Rudolph", "Francisco J. R. Ruiz", "Stephan Mandt", "David M. Blei"], "emails": ["maja@cs.columbia.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of us are able to survive ourselves, and that they are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag, \"in which he dealt with the question:\" What kind of person is this?, \"he asked in an interview with\" Welt am Sonntag \"and added:\" What kind of person is this?, \"\" What kind of person is this?, \"\" What kind of person is this?, \"\" what kind of person is this?, \"\" what kind of person is this?, \"\" what kind of person is this?, \"\" what kind of person is this?, \"\" what kind of person is this?, \"\" what kind of person?, \"\" what kind of person?, \"what kind of person?,\" what kind of person?, \"what kind of person?,\" what kind of person?, \"what kind of person?,\" what, \"what kind of person?,\" what kind, \"what kind of person?,\" what kind of person?, \"what kind of person?,\" what kind, \"what kind of person?,\" what kind, \"what kind of person?,\" what kind of person?, \"what kind of person?,\" what kind, \"what kind,\" what kind of person?, \"what kind of person?,\" what kind of person?, \"what kind of person?,\" what kind, \"what kind,\" what kind of person?, \"what kind,\" what kind, \"what kind,\" what kind, \"what kind of person?,\" what kind, \"what kind,\" what kind, \"what kind,\" what kind, \"what,\" what kind, \"what kind,\" what, \"what,\" what, \"a,\" a, \"what,\" what, \"what,\" a, \"what,\" a, \"what,\" \"what,\" what, \"a,\" what, \"what,\" what, \"what,\" what, \"what,\" a, \"what,\" a, \"what,\" what, \"what,\" what, \"what,\" what, \"what,\" what, \"a,\" what, \"what,\" what, \"what,\" what, \"what,\" what, \"what,\" what, \"what,"}, {"heading": "2 Exponential Family Embeddings", "text": "We consider a matrix x x = x1: I of the I observations, where each x i is a D vector. As an example, in language x i, an indicator vector for the word at position i and D is the size of the vocabulary. As another example of exponential data embedding x i, neuronal activity is measured by the index pair i = (n, t), where n indices represent a neuron and t indices represent a point in time; each measurement is a scalar (D = 1).The goal of exponential data embedding (EF-EMB) is to derive useful properties of the data. There are three components: a context function, a conditional exponential structure, and an embedding structure. These components work together to form the target. The EF-EMB models depend on their context; the context function determines which other data points are in the game."}, {"heading": "2.1 Examples", "text": "We develop the Gaussian Embedding Models (G-EMB) for the analysis of real observations from a neuroscience application; we also present a non-negative version of the non-negative Gaussian Embedding Data (NG-EMB) for which we have different linking functions; we present a categorical embedding model that corresponds to continuous word embedding (CBOW); Word Embedding (Mikolov et al); and finally, we present a Bernoulli Embedding (B-EMB) for binary data."}, {"heading": "2.2 Inference", "text": "We adjust the embedding (i) and context vectors (i) by maximizing the objective function in Equation (3). We use stochastic gradient lineage (SGD). We first calculate the gradient by using the identity for objective family distributions that the derivative of the log normalizer is equal to the expectation of sufficient statistics, i.e., E [t (x) = [n). With this result, the gradient is equal to the embedding (j) of the objective distributions (j) even though the derivative of the log normalizer corresponds to the expectations of sufficient statistics (x i) \u2212 E [t (x i)] [t (j)]]] n (j [j] log p (j]))))))). (4) The gradient is equal to the embedding (j). In Appendix A we detail this expression for the respective models we empirically examine."}, {"heading": "3 Empirical Study", "text": "We examine exponential family embedding models (EF-EMB) on real and numbered data and in various fields of application - computer neuroscience, purchasing behaviour and film ratings. We present quantitative comparisons to other dimension reduction methods and show how we can gain qualitative insights from the embedded models."}, {"heading": "3.1 Real Valued Data: Neural Data Analysis", "text": "We analyze the neuronal activity of a zebrafish that is able to remain in a single cell (Ahrens et al., 2013). Through genetic modification, the individual neurons push a calcium indicator in time (Friedrich et al., 2015). With this method, the neurons (out of a total of 200,000) are transformed. We adapt all models to the downstream data x-x to filter out the correlation between the individual neurons."}, {"heading": "3.2 Count data: Market Basket Analysis and Movie Ratings", "text": "This year it is more than ever before."}, {"heading": "4 Discussion", "text": "We described exponential family embeddings (EF-EMBs), conditionally specified latent variable models for extracting distributed representations from high-dimensional data. We showed that a continuous bag of words (CBOW) (Mikolov et al., 2013b) is a specific case of EF-EMB, and we provided examples that go beyond the text: brain activity of zebrafish, purchase data, and film ratings. We fit the EF-EMB target and use stochastic gradients. Our empirical study shows that an EF-EMB can reconstruct data better than existing dimension reduction techniques based on matrix factorization. In addition, the learned embeddings capture interesting semantic structures."}, {"heading": "A Stochastic Gradient Descent", "text": "To specify the gradients in Equation 4 for the stochastic gradient (SGD), we need the sufficient statistic t (x), the expected sufficient statistic E [t (x)], the gradients of the natural parameter relative to the embedded vectors and the gradients of the embedded vector (G-EMB). In this appendix, we specify these quantities for the models that we empirically analyze in Section 3.A.1 of Embedding Gaussian (G-EMB) using the notation i = (n, t) and reflect the embedded structure."}, {"heading": "B Algorithm Details", "text": "Model Minibatch Size Regularization Parameters Number of Iterations Negative SamplesNeuro G-EMB 100 10 500 n / a Neuro NG-EMB 100 0.1 500 n / a Shopping all models n / a 1 3000 10 Films all models n / a 1 3000 10Table 5: Algorithm details for the models examined in Section 3."}, {"heading": "C Complements and Substitutes in the Shopping", "text": "DataTable 6 shows some pairs of articles with high internal product of embedding vectors and context vectors. The articles in the first column have a higher probability of being bought if the article is in the second column in the shopping cart. We can observe that they coincide with articles that are often bought together (potato chips and beer, potato chips and frozen pizza, two different sodas).Likewise, Table 7 shows some pairs of articles with low internal product. The articles in the first column have a lower probability of being bought if the article is in the second column in the shopping cart. We can observe that they coincide with articles that are rarely bought together (detergent and toast crunch, milk and toothbrush) or that replace each other (two different brands of snacks, soup or pasta sauce)."}, {"heading": "D Movie Rating Results", "text": "Tables 8 and 9 show clusters of rankings of films learned through our P-EMB model. These rankings are created as follows: For each latent dimension k {1, \u00b7 \u00b7 \u00b7, K} we sort the context vectors by their value in this dimension. This gives us a ranking of context vectors for each k. Tables 8 and 9 show the 10 top positions of the ranking for two different values of k. Similar to theme modeling, the latent dimensions have the interpretation of themes. We see that the sorting of the context vectors in this way reveals the thematic structure in the film collection. While Table 8 contains a table of films for children, Table 9 (with a few outliers) shows a cluster of science fiction and action films."}], "references": [{"title": "Whole-brain functional imaging at cellular resolution using light-sheet microscopy", "author": ["M.B. Ahrens", "M.B. Orger", "D.N. Robson", "J.M. Li", "P.J. Keller"], "venue": "Nature Methods, 10(5):413\u2013420.", "citeRegEx": "Ahrens et al\\.,? 2013", "shortCiteRegEx": "Ahrens et al\\.", "year": 2013}, {"title": "Conditionally specified distributions: an introduction (with comments and a rejoinder by the authors)", "author": ["B.C. Arnold", "E. Castillo", "Sarabia", "J. M"], "venue": null, "citeRegEx": "Arnold et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Arnold et al\\.", "year": 2001}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "Sen\u00e9cal", "J.-S.", "F. Morin", "Gauvain", "J.-L."], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Database paper: The IRI marketing data set", "author": ["B.J. Bronnenberg", "M.W. Kruger", "C.F. Mela"], "venue": "Marketing Science, 27(4):745\u2013748.", "citeRegEx": "Bronnenberg et al\\.,? 2008", "shortCiteRegEx": "Bronnenberg et al\\.", "year": 2008}, {"title": "Fundamentals of statistical exponential families with applications in statistical decision theory", "author": ["L.D. Brown"], "venue": "Lecture Notes-Monograph Series, 9:i\u2013279.", "citeRegEx": "Brown,? 1986", "shortCiteRegEx": "Brown", "year": 1986}, {"title": "A generalization of principal components analysis to the exponential family", "author": ["M. Collins", "S. Dasgupta", "R.E. Schapire"], "venue": "Neural Information Processing Systems, pages 617\u2013624.", "citeRegEx": "Collins et al\\.,? 2001", "shortCiteRegEx": "Collins et al\\.", "year": 2001}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, 12:2121\u2013 2159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Fast constrained non-negative matrix factorization for whole-brain calcium imaging data", "author": ["J. Friedrich", "D. Soudry", "L. Paninski", "Y. Mu", "J. Freeman", "M. Ahrens"], "venue": "NIPS workshop on Neural Systems.", "citeRegEx": "Friedrich et al\\.,? 2015", "shortCiteRegEx": "Friedrich et al\\.", "year": 2015}, {"title": "Scalable recommendation with hierarchical Poisson factorization", "author": ["P. Gopalan", "J. Hofman", "D.M. Blei"], "venue": "Uncertainty in Artificial Intelligence.", "citeRegEx": "Gopalan et al\\.,? 2015", "shortCiteRegEx": "Gopalan et al\\.", "year": 2015}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research.", "citeRegEx": "Gutmann and Hyv\u00e4rinen,? 2010", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen", "year": 2010}, {"title": "The MovieLens datasets: History and context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS), 5(4):19.", "citeRegEx": "Harper and Konstan,? 2015", "shortCiteRegEx": "Harper and Konstan", "year": 2015}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word, 10(2-3):146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "Data Mining.", "citeRegEx": "Hu et al\\.,? 2008", "shortCiteRegEx": "Hu et al\\.", "year": 2008}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Neural Information Processing Systems, pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg,? 2014", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Generalized linear models, volume 37", "author": ["P. McCullagh", "J.A. Nelder"], "venue": "CRC press.", "citeRegEx": "McCullagh and Nelder,? 1989", "shortCiteRegEx": "McCullagh and Nelder", "year": 1989}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshop Proceedings. arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "Yih", "W.-T. a.", "G. Zweig"], "venue": "HLT-NAACL, pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noisecontrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "Neural Information Processing Systems, pages 2265\u20132273.", "citeRegEx": "Mnih and Kavukcuoglu,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "International Conference on Machine Learning, pages 1751\u20131758.", "citeRegEx": "Mnih and Teh,? 2012", "shortCiteRegEx": "Mnih and Teh", "year": 2012}, {"title": "Learning stochastic feedforward networks", "author": ["R.M. Neal"], "venue": "Department of Computer Science, University of Toronto.", "citeRegEx": "Neal,? 1990", "shortCiteRegEx": "Neal", "year": 1990}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Conference on Empirical Methods on Natural Language Processing, volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Deep exponential families", "author": ["R. Ranganath", "L. Tang", "L. Charlin", "D.M. Blei"], "venue": "Artificial Intelligence and Statistics.", "citeRegEx": "Ranganath et al\\.,? 2015", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hintont", "R.J. Williams"], "venue": "Nature, 323:9.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Distributed multinomial regression", "author": ["M Taddy"], "venue": "The Annals of Applied Statistics, 9(3):1394\u20131414.", "citeRegEx": "Taddy,? 2015", "shortCiteRegEx": "Taddy", "year": 2015}, {"title": "Word representations via Gaussian embedding", "author": ["L. Vilnis", "A. McCallum"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Vilnis and McCallum,? 2015", "shortCiteRegEx": "Vilnis and McCallum", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Word embeddings are a powerful approach for analyzing language (Bengio et al., 2006; Mikolov et al., 2013a,b; Pennington et al., 2014).", "startOffset": 63, "endOffset": 134}, {"referenceID": 21, "context": "Word embeddings are a powerful approach for analyzing language (Bengio et al., 2006; Mikolov et al., 2013a,b; Pennington et al., 2014).", "startOffset": 63, "endOffset": 134}, {"referenceID": 23, "context": "method discovers distributed representations of words; these representations capture the semantic similarity between the words and reflect a variety of other linguistic regularities (Rumelhart et al., 1986; Bengio et al., 2006; Mikolov et al., 2013c).", "startOffset": 182, "endOffset": 250}, {"referenceID": 2, "context": "method discovers distributed representations of words; these representations capture the semantic similarity between the words and reflect a variety of other linguistic regularities (Rumelhart et al., 1986; Bengio et al., 2006; Mikolov et al., 2013c).", "startOffset": 182, "endOffset": 250}, {"referenceID": 17, "context": "method discovers distributed representations of words; these representations capture the semantic similarity between the words and reflect a variety of other linguistic regularities (Rumelhart et al., 1986; Bengio et al., 2006; Mikolov et al., 2013c).", "startOffset": 182, "endOffset": 250}, {"referenceID": 18, "context": "There are many variants, adaptations, and extensions of word embeddings (Mikolov et al., 2013a,b; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Vilnis and McCallum, 2015), but each reflects the same main ideas.", "startOffset": 72, "endOffset": 202}, {"referenceID": 13, "context": "There are many variants, adaptations, and extensions of word embeddings (Mikolov et al., 2013a,b; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Vilnis and McCallum, 2015), but each reflects the same main ideas.", "startOffset": 72, "endOffset": 202}, {"referenceID": 21, "context": "There are many variants, adaptations, and extensions of word embeddings (Mikolov et al., 2013a,b; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Vilnis and McCallum, 2015), but each reflects the same main ideas.", "startOffset": 72, "endOffset": 202}, {"referenceID": 25, "context": "There are many variants, adaptations, and extensions of word embeddings (Mikolov et al., 2013a,b; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Vilnis and McCallum, 2015), but each reflects the same main ideas.", "startOffset": 72, "endOffset": 202}, {"referenceID": 11, "context": "In language, this is the foundational idea that words with similar meanings will appear in similar contexts (Harris, 1954).", "startOffset": 108, "endOffset": 122}, {"referenceID": 4, "context": "We use the tools of exponential families (Brown, 1986) and generalized linear models (GLMs) (McCullagh and Nelder, 1989) to adapt this idea beyond language.", "startOffset": 41, "endOffset": 54}, {"referenceID": 14, "context": "We use the tools of exponential families (Brown, 1986) and generalized linear models (GLMs) (McCullagh and Nelder, 1989) to adapt this idea beyond language.", "startOffset": 92, "endOffset": 120}, {"referenceID": 15, "context": "We show how existing methods, such as continuous bag of words (CBOW) (Mikolov et al., 2013a) and negative sampling (Mikolov et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 16, "context": ", 2013a) and negative sampling (Mikolov et al., 2013b), can each be viewed as an EF-EMB.", "startOffset": 31, "endOffset": 54}, {"referenceID": 5, "context": "Mirroring the success of word embeddings, EF-EMB models outperform traditional dimensionality reduction, such as exponential family principal component analysis (PCA) (Collins et al., 2001) and Poisson factorization (Gopalan et al.", "startOffset": 167, "endOffset": 189}, {"referenceID": 8, "context": ", 2001) and Poisson factorization (Gopalan et al., 2015), and find interpretable features of the data.", "startOffset": 34, "endOffset": 56}, {"referenceID": 15, "context": "EF-EMB models generalize CBOW (Mikolov et al., 2013a) in the same way that exponential family PCA (Collins et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 5, "context": ", 2013a) in the same way that exponential family PCA (Collins et al., 2001) generalizes PCA, GLMs (McCullagh and Nelder, 1989) generalize regression, and deep exponential families (Ranganath et al.", "startOffset": 53, "endOffset": 75}, {"referenceID": 14, "context": ", 2001) generalizes PCA, GLMs (McCullagh and Nelder, 1989) generalize regression, and deep exponential families (Ranganath et al.", "startOffset": 30, "endOffset": 58}, {"referenceID": 22, "context": ", 2001) generalizes PCA, GLMs (McCullagh and Nelder, 1989) generalize regression, and deep exponential families (Ranganath et al., 2015) generalize sigmoid belief networks (Neal, 1990).", "startOffset": 112, "endOffset": 136}, {"referenceID": 20, "context": ", 2015) generalize sigmoid belief networks (Neal, 1990).", "startOffset": 43, "endOffset": 55}, {"referenceID": 15, "context": "A linear EF-EMB (which we define precisely below) relates to contextwindow-based embedding methods such as CBOW or the vector log-bilinear language model (VLBL) (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013), which model a word given its context.", "startOffset": 161, "endOffset": 212}, {"referenceID": 18, "context": "A linear EF-EMB (which we define precisely below) relates to contextwindow-based embedding methods such as CBOW or the vector log-bilinear language model (VLBL) (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013), which model a word given its context.", "startOffset": 161, "endOffset": 212}, {"referenceID": 15, "context": "The more general EF-EMB relates to embeddings with a nonlinear component, such as the skip-gram (Mikolov et al., 2013a) or the inverse vector log-bilinear language model (IVLBL) (Mnih and Kavukcuoglu, 2013).", "startOffset": 96, "endOffset": 119}, {"referenceID": 18, "context": ", 2013a) or the inverse vector log-bilinear language model (IVLBL) (Mnih and Kavukcuoglu, 2013).", "startOffset": 67, "endOffset": 95}, {"referenceID": 9, "context": "These include noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010; Mnih and Teh, 2012), hierarchical softmax (Mikolov et al.", "startOffset": 43, "endOffset": 92}, {"referenceID": 19, "context": "These include noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010; Mnih and Teh, 2012), hierarchical softmax (Mikolov et al.", "startOffset": 43, "endOffset": 92}, {"referenceID": 16, "context": "These include noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010; Mnih and Teh, 2012), hierarchical softmax (Mikolov et al., 2013b), and negative sampling (Mikolov et al.", "startOffset": 115, "endOffset": 138}, {"referenceID": 15, "context": ", 2013b), and negative sampling (Mikolov et al., 2013a).", "startOffset": 32, "endOffset": 55}, {"referenceID": 1, "context": "However, in general it does not have a consistent joint distribution (Arnold et al., 2001).", "startOffset": 69, "endOffset": 90}, {"referenceID": 14, "context": "Equation (3) can be seen as a likelihood function for a bank of GLMs (McCullagh and Nelder, 1989).", "startOffset": 69, "endOffset": 97}, {"referenceID": 15, "context": "We present a categorical embedding model that corresponds to the continuous bag of words (CBOW) word embedding (Mikolov et al., 2013a).", "startOffset": 111, "endOffset": 134}, {"referenceID": 16, "context": "2 we show that negative sampling (Mikolov et al., 2013b) corresponds to biased stochastic gradients of the B-EMB objective.", "startOffset": 33, "endOffset": 56}, {"referenceID": 0, "context": "Consider the (calcium) expression of a large population of zebrafish neurons (Ahrens et al., 2013).", "startOffset": 77, "endOffset": 98}, {"referenceID": 15, "context": "EF-EMBs are inspired by word embeddings, such as CBOW (Mikolov et al., 2013a).", "startOffset": 54, "endOffset": 77}, {"referenceID": 16, "context": "2 we show that biased stochastic gradients of the B-EMB objective recovers negative sampling (Mikolov et al., 2013b).", "startOffset": 93, "endOffset": 116}, {"referenceID": 6, "context": "We use Adagrad (Duchi et al., 2011) to set the step-size.", "startOffset": 15, "endOffset": 35}, {"referenceID": 16, "context": "This relates to negative sampling, which is used to approximate the skip-gram objective (Mikolov et al., 2013b).", "startOffset": 88, "endOffset": 111}, {"referenceID": 0, "context": "We analyze the neural activity of a larval zebrafish, recorded at single cell resolution for 3000 time frames (Ahrens et al., 2013).", "startOffset": 110, "endOffset": 131}, {"referenceID": 7, "context": "The resulting calcium imaging data is preprocessed by a nonnegative matrix factorization to identify neurons, their locations, and the fluorescence activity x\u2217 t \u2208 R N of the individual neurons over time (Friedrich et al., 2015).", "startOffset": 204, "endOffset": 228}, {"referenceID": 3, "context": "We analyze the IRI dataset3 (Bronnenberg et al., 2008), 3We thank IRI for making the data available.", "startOffset": 28, "endOffset": 54}, {"referenceID": 8, "context": "Table 3: Comparison of predictive log-likelihood between P-EMB, AP-EMB, HPF (Gopalan et al., 2015), and Poisson PCA (Collins et al.", "startOffset": 76, "endOffset": 98}, {"referenceID": 5, "context": ", 2015), and Poisson PCA (Collins et al., 2001) on held out data.", "startOffset": 25, "endOffset": 47}, {"referenceID": 10, "context": "We also analyze the MovieLens-100K dataset (Harper and Konstan, 2015), which contains movie ratings on a scale from 1 to 5.", "startOffset": 43, "endOffset": 69}, {"referenceID": 12, "context": "1, as done by Hu et al. (2008) for matrix factorization.", "startOffset": 14, "endOffset": 31}, {"referenceID": 8, "context": "We compare the predictive performance with HPF (Gopalan et al., 2015) and Poisson PCA (Collins et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 5, "context": ", 2015) and Poisson PCA (Collins et al., 2001).", "startOffset": 24, "endOffset": 46}, {"referenceID": 16, "context": "We showed that continuous bag of words (CBOW) (Mikolov et al., 2013b) is a special case of EF-EMB and we provided examples beyond text: the brain activity of zebrafish, shopping data, and movie ratings.", "startOffset": 46, "endOffset": 69}], "year": 2016, "abstractText": "Word embeddings are a powerful approach for capturing semantic similarity among terms in a vocabulary. In this paper, we develop exponential family embeddings, a class of methods that extends the idea of word embeddings to other types of high-dimensional data. As examples, we studied neural data with real-valued observations, count data from a market basket analysis, and ratings data from a movie recommendation system. The main idea is to model each observation conditioned on a set of other observations. This set is called the context, and the way the context is defined is a modeling choice that depends on the problem. In language the context is the surrounding words; in neuroscience the context is close-by neurons; in market basket data the context is other items in the shopping cart. Each type of embedding model defines the context, the exponential family of conditional distributions, and how the latent embedding vectors are shared across data. We infer the embeddings with a scalable algorithm based on stochastic gradient descent. On all three applications\u2014neural activity of zebrafish, users\u2019 shopping behavior, and movie ratings\u2014we found exponential family embedding models to be more effective than other types of dimensionality reduction. They better reconstruct held-out data and find interesting qualitative structure.", "creator": "LaTeX with hyperref package"}}}