{"id": "1608.01056", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Aug-2016", "title": "Morphological Priors for Probabilistic Neural Word Embeddings", "abstract": "Word embeddings allow natural language processing systems to share statistical information across related words. These embeddings are typically based on distributional statistics, making it difficult for them to generalize to rare or unseen words. We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features. Unlike previous work that constructs word embeddings directly from morphemes, we combine morphological and distributional information in a unified probabilistic framework, in which the word embedding is a latent variable. The morphological information provides a prior distribution on the latent word embeddings, which in turn condition a likelihood function over an observed corpus. This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging.", "histories": [["v1", "Wed, 3 Aug 2016 02:21:16 GMT  (78kb,D)", "http://arxiv.org/abs/1608.01056v1", "Accepted to Empirical Methods in Natural Language Processing (EMNLP 2016, Austin)"], ["v2", "Sat, 24 Sep 2016 01:28:00 GMT  (78kb,D)", "http://arxiv.org/abs/1608.01056v2", "Appeared at the Conference on Empirical Methods in Natural Language Processing (EMNLP 2016, Austin)"]], "COMMENTS": "Accepted to Empirical Methods in Natural Language Processing (EMNLP 2016, Austin)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["parminder bhatia", "robert guthrie", "jacob eisenstein"], "accepted": true, "id": "1608.01056"}, "pdf": {"name": "1608.01056.pdf", "metadata": {"source": "CRF", "title": "Morphological Priors for Probabilistic Neural Word Embeddings", "authors": ["Parminder Bhatia", "Yik Yak", "Robert Guthrie", "Jacob Eisenstein"], "emails": ["parminder@yikyakapp.com", "jacobe}@gatech.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people who live in the USA, live in the USA, in the USA, in Europe, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "2 Model", "text": "The general modeling framework is illustrated in Figure 1, with the emphasis on the word sesquipedalianism. < The confusion is resolved by analyzing the second example morphologically as (in + flame) + capable, but this requires hierarchical morphological analysis, not just segmentation.word is relatively rare, but morphology points to several of its properties: the -ism suffix suggests that the word is a noun that probably describes an abstract property; the sesqui- prefix refers to one and a half and so on. If the word is unknown, we must rely heavily on these intuitions, but if the word is well documented, we can rely instead on its examples. It is this reasoning that our modeling framework aims to formalize a word, and so on. We treat word embedding as latent variables in a common probability model."}, {"heading": "2.1 Prior distribution", "text": "The most important distinguishing feature of this model is that we do not evaluate word embedding directly, but treat it as a latent variable, with a previous distribution reflecting the morphological properties of the word. To characterize this previous distribution, each morpheme m is associated with its own embedding, where k is again the embedding size. At the next position i of the word embedding bw we have the following previous, bw, i Bernoulli (2) = Vw w w logP (bw; Mw, u) (3) = Vw w w k i logP (bw, i \u2212 Mw, u) (4) = Vw w w w logP (bw; Mw, u) (3) = Vw w w w w, u (1 \u2212 m)."}, {"heading": "2.2 Expected likelihood", "text": "The corpus probability is calculated using a recursive neural network language model (Mikolov et al., 2010, RNNLM), which is a generative model of token sequences. In the RNNLM, the probability of each word is bound to all preceding words by a recursively updated state vector. (7) The state vector in turn depends on the embedding of the preceding words, namely by the following update equations: ht = f (bxt, ht \u2212 1) (6) xt + 1 \u0445 Multinomial (Softmax [Vht])). (7) The function f (\u00b7) is a recursive update equation; in the RNN, it corresponds to the ratio equations (\u0442ht \u2212 1 + bxt), which is the elementary sigmoid function (\u00b7), which is the logmoid function: the matrix-V-Rv-k contains the \"output embedding\" of each word in the vocabulary."}, {"heading": "2.3 Variational approximation", "text": "Conclusion to the limit probability P (x1: N) = Q (x1: N, b) db is insoluble. We address this problem by making a variable approximation, logP (x) = logP (x | b) P (b) (b) (10) = logP (b) Q (b) Q (b) Q (x | b) P (b) (11) = logEq (x | b) P (b) P (b) Q (b)] (12) \u2265 Eq (x | b) + logP (b) \u2212 logQ (b) (13) The variable distribution Q (b) is defined by a fully factorized center approximation, Q (b) = vw (b) k) q (bw) q (bw), i); the variable distribution is a product of Bernoullis, with parameters from which we assess the variation."}, {"heading": "3 Implementation", "text": "The objective function is given by the lower limit of variation in equation 20, using the approximation of the conditional probability described in equation 19. This function is optimized with respect to several parameters: \u2022 the morpheme embeddings, {um} m-1... vm; \u2022 the variation parameters of the word embeddings, {\u03b3} w-1... vw; \u2022 the output word embeddings V; \u2022 the parameter of the repetition function. Each of these parameters is updated via the online learning algorithm RMSProp (Tieleman and Hinton, 2012). The model and the baseline (described below) are implemented in blocks (van Merrie \ufffd nboer et al., 2015). In the rest of the paper we refer to our model as VAREMBED."}, {"heading": "3.1 Data and preprocessing", "text": "All embedding is trained to 22 million characters from the North American News Text (NANT) corpus (Graff, 1995). We use an initial vocabulary of 50,000 words with a special < UNK > character for words that are not among the 50,000 most commonly used. We then make a reduction and convert all numerical characters into a special < NUM > character. After these steps, the vocabulary decreases to 48,986. Note that the method can inoculate word embedding for words outside the vocabulary under the previous distribution P (b; M, u); however, it is necessary to opt for a vocabulary size to determine the number of variation parameters and output values. Unattended morphological segmentation is performed using Morfessor (Creutz and Lagus, 2002), using a maximum of sixteen morphemes per word."}, {"heading": "3.2 Learning details", "text": "The LSTM parameters are uniformly initialized in the range [\u2212 0.08, 0.08]. Word embedding is itized using pre-trained word2vec embeddings. We train the model for 15 epochs, with an initial learning rate of 0.01 and a decay rate of 0.97 per epoch; we use size 25 minibatches. We set the standard of gradients (normalized by minibatch size) at 1. These decisions are motivated by preparatory work (Zaremba et al., 2014). Training takes approximately one hour per iteration using an NVIDIA 670 GTX, a raw material graphics processor unit (GPU) for games. This is almost identical to the training time required for our re-implementation of the Botha and Blunsom algorithm (2014)."}, {"heading": "3.3 Baseline", "text": "The most comparable modeling approach is that of Botha and Blunsom (2014). In their work, the embeddings are estimated for each morphem, as well as for each embedded word. The final embedding of a word is then the sum of all these embeddings, e.g. greenhouse = greenhouse + green + house, with the italics representing learned embeddings. We build a baseline closely inspired by this approach, which we call SUMEMBED. The crucial difference is that Botha and Blunsom (2014) build on the log bilinear language model (Mnih and Hinton, 2007), but use the same LSTM-based architecture as in our own modeling implementation. This allows our evaluation to focus on the crucial difference between the two approaches: the use of latent variables instead of summing to model the word embeddings. As with our method, we have pre-traced the embed2c model to use."}, {"heading": "3.4 Number of parameters", "text": "The predominant terms in the total number of parameters are the (expected) word embeddings themselves. The variation parameters of the input word embeddings, \u03b3, are of the size k \u00b7 vw. The output word embeddings are of the size # | h | \u00d7 vw. The morphine embeddings are of the size k \u00b7 vm, with vm vw. In our main experiments we set vw = 48, 896 (see above), k = 128 and # | h | = 128. After including the character embeddings and the parameters of the recurring models, the total number of parameters is about 12.8 million. This corresponds to the number of parameters in the SUMEMBED baseline."}, {"heading": "4 Evaluation", "text": "Our evaluation compares the following embeddings: WORD2VEC We train the popular word2vec CBOW model (Mikolov et al., 2013) using the gensim implementation.SUMEMBED We compare with the baseline described in \u00a7 3.3, which can be considered a re-implementation of the compositional model of Botha and Blunsom (2014). VAREMBED For our model, we take the expected embeddings Eq [b] and then run them through an inverse sigmoid function to get values across the entire real line."}, {"heading": "4.1 Word similarity", "text": "Our first assessment is based on two classical word similarity datasets: Wordsim353 (Finkelstein et al., 2001) and the Stanford \"rare words\" (rw) dataset (Luong et al., 2013). We report on Spearmanns \u03c1, a measure of rank correlation that evaluates both the entire vocabulary and the subset of in-vocabulary words. As shown in Table 1, VAREMBED consistently performs better than SUMEMBED on both datasets. On the subset of in-vocabulary words, WORD2VEC performs worse than the two models based on morphology and agrees with the results of Luong et al. (2013) and Botha and Blunsom (2014) in terms of this morphology."}, {"heading": "4.2 Alignment with lexical semantic features", "text": "The QVEC statistics are another intrinsic evaluation method, which is demonstrably better correlated with downstream tasks (Tsvetkov et al., 2015). This measurement measures the alignment between word embedding and a series of lexical semantic characteristics. Specifically, we use the semcornword verb Supersenses oracle, which is used in the Qvec github repository.2As shown in Table 2, VAREMBED exceeds SUMEMBED on the complete lexicon and provides a similar performance to WORD2VEC on the sentence calling words. We also look at the morph embedding alone. For SUMEMBED, this means that we construct the word embedding from the sum of the embedding / BED- / BED- / BED- / BED- / BED- / BED- / BE- / BE- / BE- / BE- / BE- / D- / BE- / BE- / D- without the additional embedding for half of the BE- / BED- / BE- / BE- / BE- / BE- / D- / D- / BE- / BE- / D- / BE- / D- / BE- / BE- / D- / BE- / D- / BE- / D- / BE- / D- / D- /"}, {"heading": "4.3 Part-of-speech tagging", "text": "Our final evaluation refers to the downstream task of part-of-speech tagging using the Penn Treebank. We are building a simple classification-based tagger that uses a feedforward-neural network. (This is not intended as an alternative to state-of-the-art tagging algorithms, but as a comparison of the syntactical use of the information encrypted in the word embedding.) The input into the network is the chained embedding of the five word neighbors (xt \u2212 2, xt \u2212 1, xt + 1, xt + 2); as in all evaluations, 128-dimensional embedding is used, so that the total size of the input is 640. This input is fed into a network with two hidden layers of size 625, and a softmax output layer over all tags. We train with RMSProp (Tieleman and Hinton, 2012). The results are presented in Table 3. Both morphologically informed embedding / BE- / BE- / G- and E- / BE- / LE- / - are clearly better than the BE- / BE- / BE- / G- and E- / G-."}, {"heading": "5 Related work", "text": "This type of work typically relies on existing lexical semantic resources such as WordNet. A similar approach is defined, for example, by Bian et al. (2014), a common training goal in which the word must predict not only adjacent word tokens in a corpus, but also related word types in a semantic resource; a similar approach is proposed by Bian et al. (2014). Alternatively, Faruqui et al. (2015) proposes to use \"retrofit\" pre-formed word embeddings in a semantic network. Their approach is similar to ours in that they also treat the true word embeddings as latent variables emitted by the preached word embeddings. In principle, such approaches could be applied to a relational network based on orthographic similarity."}, {"heading": "6 Conclusion and future work", "text": "We present a model that unifies compositional and distributive perspectives on lexical semantics through the machinery of Bayesian latent variable models. In this framework, our previous expectations of word meanings are based on internal structure, but these expectations can be overridden by distributional statistics. The model is based on the very successful long-term short-term memory (LSTM) for sequence modelling, and while it employs Bayesian justification, its conclusions and estimates are little more complicated than a standard LSTM. This demonstrates the benefits of arguing about uncertainty even when working in a \"neural\" paradigm. This work represents a first step, and we see many opportunities to improve performance through its expansion. Clearly, we would expect this model to be more effective in languages with richer morphological structures than English, and we plan to explore this possibility in future work."}, {"heading": "Acknowledgments", "text": "Thanks to Erica Briscoe, Martin Hyatt, Yangfeng Ji, Bryan Leslie Lee and Yi Yang for the helpful discussion of this work. Thanks also to the EMNLP reviewers for constructive feedback. This research is supported by the Defense Threat Reduction Agency under the award HDTRA1-15-1-0019."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Knowledgepowered deep learning for word embedding", "author": ["Jiang Bian", "Bin Gao", "Tie-Yan Liu."], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 132\u2013148. Springer.", "citeRegEx": "Bian et al\\.,? 2014", "shortCiteRegEx": "Bian et al\\.", "year": 2014}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A Botha", "Phil Blunsom."], "venue": "Proceedings of the International Conference on Machine Learning (ICML).", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "Classbased n-gram models of natural language", "author": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai."], "venue": "Computational linguistics, 18(4):467\u2013479.", "citeRegEx": "Brown et al\\.,? 1992", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of Empirical Methods for", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A recurrent latent variable model for sequential data", "author": ["Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron Courville", "Yoshua Bengio."], "venue": "Neural Information Processing Systems (NIPS), Montr\u00e9al.", "citeRegEx": "Chung et al\\.,? 2015", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the International Conference on Machine Learning (ICML), pages 160\u2013167.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Morphological word-embeddings", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL), Denver, CO, May.", "citeRegEx": "Cotterell and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Cotterell and Sch\u00fctze.", "year": 2015}, {"title": "Unsupervised discovery of morphemes", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "Proceedings of the ACL-02 workshop on Morphological and phonological learning-Volume 6, pages 21\u201330. Association for Computational Linguistics.", "citeRegEx": "Creutz and Lagus.,? 2002", "shortCiteRegEx": "Creutz and Lagus.", "year": 2002}, {"title": "Sequential monte carlo methods to train neural network models", "author": ["Jo\u00e3o FG de Freitas", "Mahesan Niranjan", "Andrew H. Gee", "Arnaud Doucet."], "venue": "Neural computation, 12(4):955\u2013993.", "citeRegEx": "Freitas et al\\.,? 2000", "shortCiteRegEx": "Freitas et al\\.", "year": 2000}, {"title": "Thematic proto-roles and argument selection", "author": ["David Dowty."], "venue": "Language, pages 547\u2013619.", "citeRegEx": "Dowty.,? 1991", "shortCiteRegEx": "Dowty.", "year": 1991}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL), Denver, CO, May.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Problems with evaluation of word embeddings using word similarity tasks", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer."], "venue": "arxiv, 1605.02276.", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "WWW, pages 406\u2013414. ACM.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Distributed morphology and the pieces of inflection", "author": ["Morris Halle", "Alec Marantz."], "venue": "Kenneth L. Hale and Samuel J. Keyser, editors, The view from building 20. MIT Press, Cambridge, MA.", "citeRegEx": "Halle and Marantz.,? 1993", "shortCiteRegEx": "Halle and Marantz.", "year": 1993}, {"title": "The structure of a semantic theory", "author": ["Jerrold J Katz", "Jerry A Fodor."], "venue": "Language, pages 170\u2013210.", "citeRegEx": "Katz and Fodor.,? 1963", "shortCiteRegEx": "Katz and Fodor.", "year": 1963}, {"title": "Character-aware neural language", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Autoencoding variational bayes", "author": ["Diederik P Kingma", "Max Welling."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Welling.,? 2014", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Deriving boolean structures from distributional vectors", "author": ["German Kruszewski", "Denis Paperno", "Marco Baroni."], "venue": "Transactions of the Association for Computational Linguistics, 3:375\u2013388.", "citeRegEx": "Kruszewski et al\\.,? 2015", "shortCiteRegEx": "Kruszewski et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": "Proceedings of Empirical Methods", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL2013, 104.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Neural variational inference for text processing", "author": ["Yishu Miao", "Lei Yu", "Phil Blunsom."], "venue": "CoRR, abs/1511.06038.", "citeRegEx": "Miao et al\\.,? 2015", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor."], "venue": "Proceedings of the International Conference on Machine Learning (ICML), pages 1791\u20131799.", "citeRegEx": "Mnih and Gregor.,? 2014", "shortCiteRegEx": "Mnih and Gregor.", "year": 2014}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "Proceedings of the 24th international conference on Machine learning, ICML \u201907, pages 641\u2013648, New York, NY, USA. ACM.", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Black box variational inference", "author": ["Rajesh Ranganath", "Sean Gerrish", "David Blei."], "venue": "Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, pages 814\u2013822.", "citeRegEx": "Ranganath et al\\.,? 2014", "shortCiteRegEx": "Ranganath et al\\.", "year": 2014}, {"title": "Semantic proto-roles", "author": ["Drew Reisinger", "Rachel Rudinger", "Francis Ferraro", "Craig Harman", "Kyle Rawlins", "Benjamin Van Durme."], "venue": "Transactions of the Association for Computational Linguistics, 3:475\u2013488.", "citeRegEx": "Reisinger et al\\.,? 2015", "shortCiteRegEx": "Reisinger et al\\.", "year": 2015}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Cicero D. Santos", "Bianca Zadrozny."], "venue": "Proceedings of the International Conference on Machine Learning (ICML), pages 1818\u20131826.", "citeRegEx": "Santos and Zadrozny.,? 2014", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "Proceedings of INTERSPEECH.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Lecture 6.5: Rmsprop", "author": ["Tijman Tieleman", "Geoffrey Hinton"], "venue": "Technical report, Coursera Neural Networks for Machine Learning", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), Lisbon, September.", "citeRegEx": "Tsvetkov et al\\.,? 2015", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio."], "venue": "CoRR, abs/1506.00619.", "citeRegEx": "Merri\u00ebnboer et al\\.,? 2015", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Word representations via gaussian embedding", "author": ["Luke Vilnis", "Andrew McCallum."], "venue": "CoRR, abs/1412.6623.", "citeRegEx": "Vilnis and McCallum.,? 2014", "shortCiteRegEx": "Vilnis and McCallum.", "year": 2014}, {"title": "Graphical models, exponential families, and variational inference", "author": ["Martin J. Wainwright", "Michael I. Jordan."], "venue": "Foundations and Trends in Machine Learning, 1(1-2):1\u2013305.", "citeRegEx": "Wainwright and Jordan.,? 2008", "shortCiteRegEx": "Wainwright and Jordan.", "year": 2008}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), pages 545\u2013550, Baltimore, MD.", "citeRegEx": "Yu and Dredze.,? 2014", "shortCiteRegEx": "Yu and Dredze.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Human behavior and the principle of least effort", "author": ["George Kingsley Zipf."], "venue": "Addison-Wesley.", "citeRegEx": "Zipf.,? 1949", "shortCiteRegEx": "Zipf.", "year": 1949}], "referenceMentions": [{"referenceID": 24, "context": "Word embeddings have been shown to improve many natural language processing applications, from language models (Mikolov et al., 2010) to information extraction (Collobert and Weston, 2008), and from parsing (Chen and Manning, 2014) to machine translation (Cho et al.", "startOffset": 111, "endOffset": 133}, {"referenceID": 8, "context": ", 2010) to information extraction (Collobert and Weston, 2008), and from parsing (Chen and Manning, 2014) to machine translation (Cho et al.", "startOffset": 34, "endOffset": 62}, {"referenceID": 5, "context": ", 2010) to information extraction (Collobert and Weston, 2008), and from parsing (Chen and Manning, 2014) to machine translation (Cho et al.", "startOffset": 81, "endOffset": 105}, {"referenceID": 6, "context": ", 2010) to information extraction (Collobert and Weston, 2008), and from parsing (Chen and Manning, 2014) to machine translation (Cho et al., 2014).", "startOffset": 129, "endOffset": 147}, {"referenceID": 4, "context": "Word embeddings leverage a classical idea in natural language processing: use distributional statistics from large amounts of unlabeled data to learn representations that allow sharing across related words (Brown et al., 1992).", "startOffset": 206, "endOffset": 226}, {"referenceID": 39, "context": "of linguistic data ensures that there will always be words that are not observed in even the largest corpus (Zipf, 1949).", "startOffset": 108, "endOffset": 120}, {"referenceID": 22, "context": "Recent work has proposed to address this issue by replacing word-level embeddings with embeddings based on subword units: morphemes (Luong et al., 2013; Botha and Blunsom, 2014) or individual characters (Santos and Zadrozny, 2014; Ling et al.", "startOffset": 132, "endOffset": 177}, {"referenceID": 3, "context": "Recent work has proposed to address this issue by replacing word-level embeddings with embeddings based on subword units: morphemes (Luong et al., 2013; Botha and Blunsom, 2014) or individual characters (Santos and Zadrozny, 2014; Ling et al.", "startOffset": 132, "endOffset": 177}, {"referenceID": 30, "context": ", 2013; Botha and Blunsom, 2014) or individual characters (Santos and Zadrozny, 2014; Ling et al., 2015; Kim et al., 2016).", "startOffset": 58, "endOffset": 122}, {"referenceID": 21, "context": ", 2013; Botha and Blunsom, 2014) or individual characters (Santos and Zadrozny, 2014; Ling et al., 2015; Kim et al., 2016).", "startOffset": 58, "endOffset": 122}, {"referenceID": 18, "context": ", 2013; Botha and Blunsom, 2014) or individual characters (Santos and Zadrozny, 2014; Ling et al., 2015; Kim et al., 2016).", "startOffset": 58, "endOffset": 122}, {"referenceID": 35, "context": "We treat word embeddings as latent variables (Vilnis and McCallum, 2014), which are conditioned on a prior distribution that is based on word morphology.", "startOffset": 45, "endOffset": 72}, {"referenceID": 16, "context": "Morphemes are viewed as adding morphosyntactic features to words: for example, in English, un- adds a negation feature (unbelievable), -s adds a plural feature, and -ed adds a past tense feature (Halle and Marantz, 1993).", "startOffset": 195, "endOffset": 220}, {"referenceID": 17, "context": "Similarly, the lexicon is often viewed as organized in terms of features: for example, the word bachelor carries the features HUMAN, MALE, and UNMARRIED (Katz and Fodor, 1963).", "startOffset": 153, "endOffset": 175}, {"referenceID": 12, "context": "Each word\u2019s semantic role within a sentence can also be characterized in terms of binary features (Dowty, 1991; Reisinger et al., 2015).", "startOffset": 98, "endOffset": 135}, {"referenceID": 29, "context": "Each word\u2019s semantic role within a sentence can also be characterized in terms of binary features (Dowty, 1991; Reisinger et al., 2015).", "startOffset": 98, "endOffset": 135}, {"referenceID": 1, "context": "However, we can also work with the expected word embeddings, which are vectors of probabilities, and can therefore be expected to hold the advantages of dense distributed representations (Bengio et al., 2013).", "startOffset": 187, "endOffset": 208}, {"referenceID": 17, "context": "As noted in the introduction, this binary representation is motivated by feature-based theories of lexical semantics (Katz and Fodor, 1963).", "startOffset": 117, "endOffset": 139}, {"referenceID": 31, "context": "This same notation can be applied to compute the likelihood under a long-short term memory (LSTM) language model (Sundermeyer et al., 2012).", "startOffset": 113, "endOffset": 139}, {"referenceID": 7, "context": "Alternative methods such as variational autoencoders (Chung et al., 2015) or sequential Monte Carlo (de Freitas et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 36, "context": "Variational bounds in the form of Equation 13 can generally be expressed as a difference between an expected log-likelihood term and a term for the Kullback-Leibler divergence between the prior distribution P (b) and the variational distribution Q(b) (Wainwright and Jordan, 2008).", "startOffset": 251, "endOffset": 280}, {"referenceID": 32, "context": "Each of these parameters is updated via the RMSProp online learning algorithm (Tieleman and Hinton, 2012).", "startOffset": 78, "endOffset": 105}, {"referenceID": 10, "context": "Unsupervised morphological segmentation is performed using Morfessor (Creutz and Lagus, 2002), with a maximum of sixteen morphemes per word.", "startOffset": 69, "endOffset": 93}, {"referenceID": 38, "context": "These choices are motivated by prior work (Zaremba et al., 2014).", "startOffset": 42, "endOffset": 64}, {"referenceID": 3, "context": "This is nearly identical to the training time required for our reimplementation of the algorithm of Botha and Blunsom (2014), described below.", "startOffset": 100, "endOffset": 125}, {"referenceID": 3, "context": "The most comparable modeling approach is that of Botha and Blunsom (2014). In their work, embeddings are estimated for each morpheme, as well as for each in-vocabulary word.", "startOffset": 49, "endOffset": 74}, {"referenceID": 27, "context": "The key difference is that while Botha and Blunsom (2014) build on the log-bilinear language model (Mnih and Hinton, 2007), we use the same LSTM-based architecture as in our own model implementation.", "startOffset": 99, "endOffset": 122}, {"referenceID": 3, "context": "The key difference is that while Botha and Blunsom (2014) build on the log-bilinear language model (Mnih and Hinton, 2007), we use the same LSTM-based architecture as in our own model implementation.", "startOffset": 33, "endOffset": 58}, {"referenceID": 25, "context": "WORD2VEC We train the popular word2vec CBOW (continuous bag of words) model (Mikolov et al., 2013), using the gensim implementation.", "startOffset": 76, "endOffset": 98}, {"referenceID": 3, "context": "3, which can be viewed as a reimplementation of the compositional model of Botha and Blunsom (2014).", "startOffset": 75, "endOffset": 100}, {"referenceID": 15, "context": "Our first evaluation is based on two classical word similarity datasets: Wordsim353 (Finkelstein et al., 2001) and the Stanford \u201crare words\u201d (rw) dataset (Luong et al.", "startOffset": 84, "endOffset": 110}, {"referenceID": 22, "context": ", 2001) and the Stanford \u201crare words\u201d (rw) dataset (Luong et al., 2013).", "startOffset": 51, "endOffset": 71}, {"referenceID": 14, "context": "Our first evaluation is based on two classical word similarity datasets: Wordsim353 (Finkelstein et al., 2001) and the Stanford \u201crare words\u201d (rw) dataset (Luong et al., 2013). We report Spearmann\u2019s \u03c1, a measure of rank correlation, evaluating on both the entire vocabulary as well as the subset of in-vocabulary words. As shown in Table 1, VAREMBED consistently outperforms SUMEMBED on both datasets. On the subset of in-vocabulary words, WORD2VEC gives slightly better results on the wordsim words that are in the NANT vocabulary, but is not applicable to the complete dataset. On the rare words dataset, WORD2VEC performs considerably worse than both morphology-based models, matching the findings of Luong et al. (2013) and Botha and Blunsom (2014) regarding the importance of morphology for doing well on this dataset.", "startOffset": 85, "endOffset": 723}, {"referenceID": 3, "context": "(2013) and Botha and Blunsom (2014) regarding the importance of morphology for doing well on this dataset.", "startOffset": 11, "endOffset": 36}, {"referenceID": 14, "context": "Recent work questions whether these word similarity metrics are predictive of performance on downstream tasks (Faruqui et al., 2016).", "startOffset": 110, "endOffset": 132}, {"referenceID": 33, "context": "The QVEC statistic is another intrinsic evaluation method, which has been shown to be better correlated with downstream tasks (Tsvetkov et al., 2015).", "startOffset": 126, "endOffset": 149}, {"referenceID": 32, "context": "We train using RMSProp (Tieleman and Hinton, 2012).", "startOffset": 23, "endOffset": 50}, {"referenceID": 34, "context": "For example, Yu and Dredze (2014) define a joint training objective, in which the word embedding must predict not only neighboring word tokens in a corpus, but also related word types in a semantic resource; a similar approach is taken by Bian et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 2, "context": "For example, Yu and Dredze (2014) define a joint training objective, in which the word embedding must predict not only neighboring word tokens in a corpus, but also related word types in a semantic resource; a similar approach is taken by Bian et al. (2014). Alternatively, Faruqui et al.", "startOffset": 239, "endOffset": 258}, {"referenceID": 2, "context": "For example, Yu and Dredze (2014) define a joint training objective, in which the word embedding must predict not only neighboring word tokens in a corpus, but also related word types in a semantic resource; a similar approach is taken by Bian et al. (2014). Alternatively, Faruqui et al. (2015) propose to \u201cretrofit\u201d pre-trained word embeddings over a semantic network.", "startOffset": 239, "endOffset": 296}, {"referenceID": 10, "context": "Word embeddings and morphology The SUMEMBED baseline is based on the work of Botha and Blunsom (2014), in which words are segmented into morphemes using MORFESSOR (Creutz and Lagus, 2002), and then word representations are", "startOffset": 163, "endOffset": 187}, {"referenceID": 3, "context": "Word embeddings and morphology The SUMEMBED baseline is based on the work of Botha and Blunsom (2014), in which words are segmented into morphemes using MORFESSOR (Creutz and Lagus, 2002), and then word representations are", "startOffset": 77, "endOffset": 102}, {"referenceID": 21, "context": "A key modeling difference from this prior work is that rather than computing word embeddings directly and deterministically from subcomponent embeddings (morphemes or characters, as in (Ling et al., 2015; Kim et al., 2016)), we use these subcomponents to define a prior distribution, which can be overridden by distributional statistics for common words.", "startOffset": 185, "endOffset": 222}, {"referenceID": 18, "context": "A key modeling difference from this prior work is that rather than computing word embeddings directly and deterministically from subcomponent embeddings (morphemes or characters, as in (Ling et al., 2015; Kim et al., 2016)), we use these subcomponents to define a prior distribution, which can be overridden by distributional statistics for common words.", "startOffset": 185, "endOffset": 222}, {"referenceID": 9, "context": "Other work exploits morphology by training word embeddings to optimize a joint objective over distributional statistics and rich, morphologically-augmented part of speech tags (Cotterell and Sch\u00fctze, 2015).", "startOffset": 176, "endOffset": 205}, {"referenceID": 0, "context": "Latent word embeddings Word embeddings are typically treated as a parameter, and are optimized through point estimation (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2010).", "startOffset": 120, "endOffset": 191}, {"referenceID": 8, "context": "Latent word embeddings Word embeddings are typically treated as a parameter, and are optimized through point estimation (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2010).", "startOffset": 120, "endOffset": 191}, {"referenceID": 24, "context": "Latent word embeddings Word embeddings are typically treated as a parameter, and are optimized through point estimation (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2010).", "startOffset": 120, "endOffset": 191}, {"referenceID": 17, "context": "This is easy to do in the latent binary framework proposed here, which is also a better fit for some theoretical models of lexical semantics (Katz and Fodor, 1963; Reisinger et al., 2015).", "startOffset": 141, "endOffset": 187}, {"referenceID": 29, "context": "This is easy to do in the latent binary framework proposed here, which is also a better fit for some theoretical models of lexical semantics (Katz and Fodor, 1963; Reisinger et al., 2015).", "startOffset": 141, "endOffset": 187}, {"referenceID": 0, "context": "Latent word embeddings Word embeddings are typically treated as a parameter, and are optimized through point estimation (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2010). Current models use word embeddings with hundreds or even thousands of parameters per word, yet many words are observed only a handful of times. It is therefore natural to consider whether it might be beneficial to model uncertainty over word embeddings. Vilnis and McCallum (2014) propose to model Gaussian densities over dense vector word embeddings.", "startOffset": 121, "endOffset": 474}, {"referenceID": 0, "context": "Latent word embeddings Word embeddings are typically treated as a parameter, and are optimized through point estimation (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2010). Current models use word embeddings with hundreds or even thousands of parameters per word, yet many words are observed only a handful of times. It is therefore natural to consider whether it might be beneficial to model uncertainty over word embeddings. Vilnis and McCallum (2014) propose to model Gaussian densities over dense vector word embeddings. They estimate the parameters of the Gaussian directly, and, unlike our work, do not consider using orthographic information as a prior distribution. This is easy to do in the latent binary framework proposed here, which is also a better fit for some theoretical models of lexical semantics (Katz and Fodor, 1963; Reisinger et al., 2015). This view is shared by Kruszewski et al. (2015), who induce binary word representations using labeled data of lexical semantic entailment relations.", "startOffset": 121, "endOffset": 931}, {"referenceID": 19, "context": "The variational autoencoder (Kingma and Welling, 2014), neural variational inference (Mnih and Gregor, 2014; Miao et al.", "startOffset": 28, "endOffset": 54}, {"referenceID": 26, "context": "The variational autoencoder (Kingma and Welling, 2014), neural variational inference (Mnih and Gregor, 2014; Miao et al., 2015), and black box variational inference (Ranganath et al.", "startOffset": 85, "endOffset": 127}, {"referenceID": 23, "context": "The variational autoencoder (Kingma and Welling, 2014), neural variational inference (Mnih and Gregor, 2014; Miao et al., 2015), and black box variational inference (Ranganath et al.", "startOffset": 85, "endOffset": 127}, {"referenceID": 28, "context": ", 2015), and black box variational inference (Ranganath et al., 2014) all propose to use a neural network to compute the variational approximation.", "startOffset": 45, "endOffset": 69}, {"referenceID": 7, "context": "These ideas are employed by Chung et al. (2015) in the variational recurrent neural network, which places a latent continuous variable at each time step.", "startOffset": 28, "endOffset": 48}, {"referenceID": 21, "context": "From a modeling perspective, our prior distribution merely sums the morpheme embeddings, but a more accurate model might account for sequential or combinatorial structure, through a recurrent (Ling et al., 2015), recursive (Luong et al.", "startOffset": 192, "endOffset": 211}, {"referenceID": 22, "context": ", 2015), recursive (Luong et al., 2013), or convolutional architecture (Kim et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 18, "context": ", 2013), or convolutional architecture (Kim et al., 2016).", "startOffset": 39, "endOffset": 57}, {"referenceID": 31, "context": "If we follow Tsvetkov et al. (2015) in the argument that word embeddings should correspond to lexical semantic features, then an inventory of such features could be used as a source of partial supervision, thus locking dimensions of the word embeddings to specific semantic properties.", "startOffset": 13, "endOffset": 36}, {"referenceID": 13, "context": "This would complement the graph-based \u201cretrofitting\u201d supervision proposed by Faruqui et al. (2015), by instead placing supervision at the level of individual words.", "startOffset": 77, "endOffset": 99}], "year": 2017, "abstractText": "Word embeddings allow natural language processing systems to share statistical information across related words. These embeddings are typically based on distributional statistics, making it difficult for them to generalize to rare or unseen words. We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features. Unlike previous work that constructs word embeddings directly from morphemes, we combine morphological and distributional information in a unified probabilistic framework, in which the word embedding is a latent variable. The morphological information provides a prior distribution on the latent word embeddings, which in turn condition a likelihood function over an observed corpus. This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging.", "creator": "TeX"}}}