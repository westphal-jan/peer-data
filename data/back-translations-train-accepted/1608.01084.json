{"id": "1608.01084", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Aug-2016", "title": "To Swap or Not to Swap? Exploiting Dependency Word Pairs for Reordering in Statistical Machine Translation", "abstract": "Reordering poses a major challenge in machine translation (MT) between two languages with significant differences in word order. In this paper, we present a novel reordering approach utilizing sparse features based on dependency word pairs. Each instance of these features captures whether two words, which are related by a dependency link in the source sentence dependency parse tree, follow the same order or are swapped in the translation output. Experiments on Chinese-to-English translation show a statistically significant improvement of 1.21 BLEU point using our approach, compared to a state-of-the-art statistical MT system that incorporates prior reordering approaches.", "histories": [["v1", "Wed, 3 Aug 2016 06:24:01 GMT  (2718kb,D)", "http://arxiv.org/abs/1608.01084v1", "7 pages, 1 figures, Proceedings of AAAI-16"]], "COMMENTS": "7 pages, 1 figures, Proceedings of AAAI-16", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["christian hadiwinoto", "yang liu", "hwee tou ng"], "accepted": true, "id": "1608.01084"}, "pdf": {"name": "1608.01084.pdf", "metadata": {"source": "META", "title": "To Swap or Not to Swap? Exploiting Dependency Word Pairs for Reordering in Statistical Machine Translation", "authors": ["Christian Hadiwinoto", "Yang Liu", "Hwee Tou Ng"], "emails": ["chrhad@comp.nus.edu.sg", "nght@comp.nus.edu.sg", "liuyang2011@tsinghua.edu.cn"], "sections": [{"heading": "Introduction", "text": "Machine translation (MT) reordering is a critical process to maintain the correct translation output of the word order, since the word order reflects meaning, and remains a major challenge, especially for language pairs with significant differences in the word order. Phrase-based MT systems (Koehn, Och and Marcu 2003) generally use a reordering model that predicts the reordering based on the span of a sentence and that of the adjacent sentence (Tillmann 2004; Xiong, Liu and Lin 2006; Galley and Manning 2008; Cherry 2013).The above methods do not explicitly preserve the relationship between words in the source sentence that reflects the sentence meaning.The word relationship in a sentence can be captured by its dependency tree, in which each word w is a tree node connected to its top node hw, another word that indicates that the formator is a dependent (child) of the source sentence."}, {"heading": "Dependency Word Pair Features", "text": "We define a number of sparse features based on dependency tree pair decoding algorithms that are learned and used in a phrase-based SMT beam search decoding algorithm."}, {"heading": "Capturing Word Pair Ordering in Phrase-Based Beam Search", "text": "The phrases-based approach is a state-of-the-art approach to SMT, in which phrases are used as a sequence of one or more words as translation units. It is executed by a beam search algorithm (Koehn 2004a), in which the search process in the translation output produces translations from left to right. It is organized into hypotheses, each of which represents a covered input sentence and its possible translation.Since the beam search can select input phrases in any order, the sequence of target-language phrases in the translation output cannot follow the original sentence sequence. The sequence determines the translation output sequence and allows translation output for language pairs with differences in the word order, such as Chinese and English.When a word fi in a source sentence f = fN1 is covered by a hypothesis, it is known that the words {fl | fl} \u2212 11, which are translated by the left side and the fi \u2212 are not translated."}, {"heading": "Dependency Swap Features", "text": "\"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another word.\" \"We define dependence on another.\" We define dependence on another. \"We define dependence on another.\" We define dependence on another. \"We define the other word.\" We define dependence on another. \"We define the other word.\" We define the other. \"We define dependence.\" We define the other word. \"We define the other.\" We define dependence. \"We define the other word.\" We define the other. \"We define dependence.\" We define the other. \"We define dependence.\" We define the other. \"We define dependence.\" We define the other. \"We define dependence.\" We define the other. \"We define the other.\" We define dependence. \"We define the other.\" We define dependence. \"We define the other.\" We define dependence. \"We define the other.\" We define dependence. \"We define the other.\""}, {"heading": "Dependency Distortion Penalty", "text": "To promote translation output that matches the dependency-parse structure, we introduce a penalty that prevents translators from complying with the dependency-parse subtree (Cherry 2008), which assigns a penalty if the translation of the current phrase results in a source-dependency-parse subtree that needs to be split up in the output translation. Figure 1a of the translation output shows that each dependency-parse subtree is merged, which does not result in a penalty. However, it is possible that the translation \"Jokowi gave a speech,\" then translates \"Hook\" into \"into\" and \"yesterday.\" This translation will break the cohesion of the source-dependency-parse subtree and is bad. This is the case where a dependency distortion penalty applies."}, {"heading": "Modified Future Cost", "text": "Phrase-based SMT beam search encoding involves future costs that do not arise from the translation hypotheses but are used to reduce the search space (Koehn 2004a), which predicts the cost of translating the remaining untranslated phrases. Based on our dependency exchange functions, we include the future cost of each untranslated dependency pair. Future costs assume that the untranslated words are sorted in the most likely order. However, if an untranslated word x is an ancestor (in the dependency parse tree) of a word covered by the current hypothesis, the future cost assumes that x precedes all associated words. This follows the assumption that each subtree will have a contiguous translation (Cherry 2008)."}, {"heading": "Other Sparse Features", "text": "This section describes other sparse features of previous work that can be compared with our method."}, {"heading": "Sparse Reordering Orientation Features", "text": "In fact, most of them are not purely political parties, but a party that is able to establish itself."}, {"heading": "Dependency Path Features", "text": "We also use the dependency path characteristics (Chang et al. 2009) for phrase components defined by the shortest path of the dependency, parse tree links between the current and the previous source phrase. Chang et al. (2009) define the dependency path characteristics based on a maximum entropy phrase orientation classification, formed on their parallel word-based text and characterized by the two possible phrases in the translation output: ordered and reversed. Meanwhile, we use the features as sparse decoding characteristics, with the following template: hpath (shortest path (plast, sfirst); o) (4) where o), where o (in order, reversed) denotes the orientation of the two phrases in the translation. Given a source sentence and its translation output, a path is defined between the last word of the previous source phrase and the preceding phrase. \""}, {"heading": "Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Data Set and Toolkits", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "Baseline System", "text": "We are building a basic phrase-based SMT system that uses a non-sparse phrase-based lexicalized reorder (PBLR), where the probability of a reorder depends on the phrase to be translated and its position in relation to the source position of the previously translated phrase (Tillmann 2004; Koehn et al. 2005), and a non-sparse hierarchical reorder (HR), where the previous unit is not only the previous phrase, but the majority of contiguous source phrases4LDC2002L27 and LDC2005T34. 5The default when executing GIZA + + with Moses. 6AFP, APW, CNA, LTW, NYT and Xinhua.having contiguous translation (Galley and Manning 2008). Furthermore, a distortion limit is set so that the reorder cannot be longer than a certain distance."}, {"heading": "Our Approach", "text": "To accommodate our sparse feature functions, our Moses code has been modified to read dependency-analyzed input records and include additional decoding functions above our baseline, namely Dependency Distortion Penalty (DDP) (Cherry 2008), sparse Dependency Path Features (Path) (Chang et al. 2009), sparse Reordering by Hierarchical Reordering (SHR) (Cherry 2013), and our sparse Dependency Swap Features (DS). DDP Feature is a single feature that resembles the distortion penalty in distance-based reordering models (Koehn, Och, and Marcu 2003), while Path, SHR, and DS are sparse features, each capturing a specific phenomenon during translation (Chiang, Knight, and Wang 2009). We always pair DS with DDP. Since the original Path features were not used, we are experimenting with DDP, and are not dependent on setting another feature in a DP."}, {"heading": "Experimental Results", "text": "The translation quality of the system output is measured on the basis of the case-insensitive BLEU (Papineni et al. 2002), for which the reduction penalty is calculated on the basis of the shortest reference (NIST-BLEU) 7. The verification of statistical significance between systems is carried out using bootstrap resampling (Koehn 2004b).7ftp: / / jaguar.ncsl.nist.gov / mt / resources / mteval-v11b.plTable 1 shows the experimental results. The distortion limit of all systems is set at 14, giving the best result on the development list for the base system. As shown in the table, the system with our DS functions and DDP at the top of the base system with and without DDP can be + 1.09 or + 0.93 BLEU points better than the other dependency-based features (Path)."}, {"heading": "Discussions", "text": "The reordering of orientation models, i.e., PBLR and HR, only takes into account the phrase pair generated by a hypothesis, not related word properties. Economical reordering of orientation features (Cherry 2013) takes advantage of this by capturing the previous phrase pair (or contiguous chunk) properties, so they are able to improve across the baseline. However, as the results suggest, the dependency sparse provides a more useful guide to reordering a source set. As shown in Figure 3, the baseline phrase-based SMT system with the two reordering orientation models (PBLR and HR) results in a flawed translation performance that \"exports are the main market,\" which is not what the source set means. This is also the case with the previously added system, which includes DDP (Cherry 2008), Path, and HR."}, {"heading": "Related Work", "text": "Source dependency trees were used in phrase-based SMTs by modelling the transition between dependency subtrees during translation (Bach, Gao and Vogel 2009), but this does not take into account dependence on the POS tag. Another work uses source and target dependency trees for the phrase-based restoration of MT output (Gimpel and Smith 2014) rather than for decoding the translation. Chang et al. (2009) introduced the dependency path as a soft constraint based on the sequence of source dependency links traversed in the phrase-based translation. It is used as a feature on a maximum entropy phrase orientation classifier, whose probability output is used as a decoding function. As the path may be arbitrarily long, it cannot be sufficiently represented in the training samples. Our sparse feature definition can mitigate this by defining features based on two words."}, {"heading": "Conclusion", "text": "We have introduced a reordering approach for phrase-based SMT, guided by sparse dependency-swap functions. We have contributed a new approach to learning and performing reordering in phrase-based MT by incorporating dependency-based features. We have shown from our experiments that using source-dependency parses to rearrange sentences contributes to significantly improving translation quality compared to a phrase-based base system with state-of-the-art reordering models."}, {"heading": "Acknowledgments", "text": "This research is supported by the Singapore National Research Foundation through its International Research Centre @ Singapore Funding Initiative and managed by the IDM Programme Office."}], "references": [{"title": "Source-side dependency tree reordering models with subtree movements and constraints", "author": ["N. Bach", "Q. Gao", "S. Vogel"], "venue": "Proceedings of MT Summit XII. Bohnet, B., and Nivre, J. 2012. A transition-based system for joint part-of-speech tagging and labeled non-projective", "citeRegEx": "Bach et al\\.,? 2009", "shortCiteRegEx": "Bach et al\\.", "year": 2009}, {"title": "The mathematics of statistical machine translation", "author": ["P.F. Brown", "S.A. Della Pietra", "V.J. Della Pietra", "R.L. Mercer"], "venue": "Computational Linguistics 19(2):263\u2013311.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Dependency-based pre-ordering for Chinese-English machine translation", "author": ["J. Cai", "M. Utiyama", "E. Sumita", "Y. Zhang"], "venue": "Proceedings of ACL 2014 (Short Papers), 155\u2013160.", "citeRegEx": "Cai et al\\.,? 2014", "shortCiteRegEx": "Cai et al\\.", "year": 2014}, {"title": "Discriminative reordering with Chinese grammatical relations features", "author": ["P.-C. Chang", "H. Tseng", "D. Jurafsky", "C.D. Manning"], "venue": "Proceedings of SSST-3, 51\u201359.", "citeRegEx": "Chang et al\\.,? 2009", "shortCiteRegEx": "Chang et al\\.", "year": 2009}, {"title": "Cohesive phrase-based decoding for statistical machine translation", "author": ["C. Cherry"], "venue": "Proceedings of ACL-08: HLT, 72\u201380.", "citeRegEx": "Cherry,? 2008", "shortCiteRegEx": "Cherry", "year": 2008}, {"title": "Improved reordering for phrase-based translation using sparse features", "author": ["C. Cherry"], "venue": "Proceedings of NAACL HLT 2013, 22\u201331.", "citeRegEx": "Cherry,? 2013", "shortCiteRegEx": "Cherry", "year": 2013}, {"title": "11,001 new features for statistical machine translation", "author": ["D. Chiang", "K. Knight", "W. Wang"], "venue": "Proceedings of NAACL HLT 2009, 218\u2013226.", "citeRegEx": "Chiang et al\\.,? 2009", "shortCiteRegEx": "Chiang et al\\.", "year": 2009}, {"title": "Clause restructuring for statistical machine translation", "author": ["M. Collins", "P. Koehn", "I. Ku\u010derov\u00e1"], "venue": "Proceedings of ACL 2005, 531\u2013540.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "A simple and effective hierarchical phrase reordering model", "author": ["M. Galley", "C.D. Manning"], "venue": "Proceedings of EMNLP 2008, 848\u2013856.", "citeRegEx": "Galley and Manning,? 2008", "shortCiteRegEx": "Galley and Manning", "year": 2008}, {"title": "Soft dependency constraints for reordering in hierarchical phrase-based translation", "author": ["Y. Gao", "P. Koehn", "A. Birch"], "venue": "Proceedings of EMNLP 2011, 857\u2013868.", "citeRegEx": "Gao et al\\.,? 2011", "shortCiteRegEx": "Gao et al\\.", "year": 2011}, {"title": "Automatically learning source-side reordering rules for large scale machine translation", "author": ["D. Genzel"], "venue": "Proceedings of COLING 2010, 376\u2013384.", "citeRegEx": "Genzel,? 2010", "shortCiteRegEx": "Genzel", "year": 2010}, {"title": "Phrase dependency machine translation with quasi-synchronous tree-to-tree features", "author": ["K. Gimpel", "N.A. Smith"], "venue": "Computational Linguistics 40(2):349\u2013401.", "citeRegEx": "Gimpel and Smith,? 2014", "shortCiteRegEx": "Gimpel and Smith", "year": 2014}, {"title": "Syntactic preprocessing for statistical machine translation", "author": ["N. Habash"], "venue": "Proceedings of MT Summit XI, 215\u2013 222.", "citeRegEx": "Habash,? 2007", "shortCiteRegEx": "Habash", "year": 2007}, {"title": "Tuning as ranking", "author": ["M. Hopkins", "J. May"], "venue": "Proceedings of EMNLP 2011, 1352\u20131362.", "citeRegEx": "Hopkins and May,? 2011", "shortCiteRegEx": "Hopkins and May", "year": 2011}, {"title": "Exploiting syntactic relationships in a phrase-based decoder: an exploration", "author": ["T. Hunter", "P. Resnik"], "venue": "Machine Translation 24(2):123\u2013140.", "citeRegEx": "Hunter and Resnik,? 2010", "shortCiteRegEx": "Hunter and Resnik", "year": 2010}, {"title": "Source-side preordering for translation using logistic regression and depth-first branch-and-bound search", "author": ["L. Jehl", "A. de Gispert", "M. Hopkins", "W. Byrne"], "venue": "In Proceedings of EACL", "citeRegEx": "Jehl et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jehl et al\\.", "year": 2014}, {"title": "Syntax-based reordering for statistical machine translation", "author": ["M. Khalilov", "J.A.R. Fonollosa"], "venue": "Computer Speech and Language 25(4):761\u2013788.", "citeRegEx": "Khalilov and Fonollosa,? 2011", "shortCiteRegEx": "Khalilov and Fonollosa", "year": 2011}, {"title": "Edinburgh system description for the 2005 IWSLT speech translation evaluation", "author": ["P. Koehn", "A. Axelrod", "A.B. Mayne", "C. Callison-Burch", "M. Osborne", "D. Talbot"], "venue": "Proceedings of IWSLT 2005.", "citeRegEx": "Koehn et al\\.,? 2005", "shortCiteRegEx": "Koehn et al\\.", "year": 2005}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of the ACL 2007 Demo and Poster Sessions, 177\u2013180.", "citeRegEx": "Zens et al\\.,? 2007", "shortCiteRegEx": "Zens et al\\.", "year": 2007}, {"title": "Statistical phrasebased translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "Proceedings of HLT-NAACL 2003, 48\u2013", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Pharaoh: A beam search decoder for phrase-based statistical machine translation models", "author": ["P. Koehn"], "venue": "Proceedings of AMTA 2004, 115\u2013124.", "citeRegEx": "Koehn,? 2004a", "shortCiteRegEx": "Koehn", "year": 2004}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["P. Koehn"], "venue": "Proceedings of EMNLP 2004, 388\u2013395.", "citeRegEx": "Koehn,? 2004b", "shortCiteRegEx": "Koehn", "year": 2004}, {"title": "Minimum Bayes-Risk decoding for statistical machine translation", "author": ["S. Kumar", "W. Byrne"], "venue": "Proceedings of HLT-NAACL 2004, 169\u2013176.", "citeRegEx": "Kumar and Byrne,? 2004", "shortCiteRegEx": "Kumar and Byrne", "year": 2004}, {"title": "Source-side classifier preordering for machine translation", "author": ["U. Lerner", "S. Petrov"], "venue": "Proceedings of EMNLP 2013, 512\u2013523.", "citeRegEx": "Lerner and Petrov,? 2013", "shortCiteRegEx": "Lerner and Petrov", "year": 2013}, {"title": "A probabilistic approach to syntax-based reordering for statistical machine translation", "author": ["C.-H. Li", "D. Zhang", "M. Li", "M. Zhou", "M. Li", "Y. Guan"], "venue": "Proceedings of ACL 2007, 720\u2013727.", "citeRegEx": "Li et al\\.,? 2007", "shortCiteRegEx": "Li et al\\.", "year": 2007}, {"title": "A maximum entropy approach to Chinese word segmentation", "author": ["J.K. Low", "H.T. Ng", "W. Guo"], "venue": "Proceedings of SIGHAN4, 161\u2013164.", "citeRegEx": "Low et al\\.,? 2005", "shortCiteRegEx": "Low et al\\.", "year": 2005}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney"], "venue": "Computational Linguistics 29(1):19\u201351.", "citeRegEx": "Och and Ney,? 2003", "shortCiteRegEx": "Och and Ney", "year": 2003}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of ACL-02, 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A unigram orientation model for statistical machine translation", "author": ["C. Tillmann"], "venue": "Proceedings of HLT-NAACL 2004: Short Papers, 101\u2013104.", "citeRegEx": "Tillmann,? 2004", "shortCiteRegEx": "Tillmann", "year": 2004}, {"title": "Chinese syntactic reordering for statistical machine translation", "author": ["C. Wang", "M. Collins", "P. Koehn"], "venue": "Proceedings of EMNLP-CoNLL 2007, 737\u2013745.", "citeRegEx": "Wang et al\\.,? 2007", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Improving a statistical MT system with automatically learned rewrite patterns", "author": ["F. Xia", "M. McCord"], "venue": "Proceedings of COLING 2004, 508\u2013514.", "citeRegEx": "Xia and McCord,? 2004", "shortCiteRegEx": "Xia and McCord", "year": 2004}, {"title": "Maximum entropy based phrase reordering model for statistical machine translation", "author": ["D. Xiong", "Q. Liu", "S. Lin"], "venue": "Proceedings of COLING/ACL 2006, 521\u2013528.", "citeRegEx": "Xiong et al\\.,? 2006", "shortCiteRegEx": "Xiong et al\\.", "year": 2006}, {"title": "Using a dependency parser to improve SMT for subject-object-verb languages", "author": ["P. Xu", "J. Kang", "M. Ringgaard", "F. Och"], "venue": "Proceedings of NAACL HLT 2009, 245\u2013253.", "citeRegEx": "Xu et al\\.,? 2009", "shortCiteRegEx": "Xu et al\\.", "year": 2009}, {"title": "A rankingbased approach to word reordering for statistical machine translation", "author": ["N. Yang", "M. Li", "D. Zhang", "N. Yu"], "venue": "Proceedings of ACL 2012, 912\u2013920.", "citeRegEx": "Yang et al\\.,? 2012", "shortCiteRegEx": "Yang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 28, "context": "Phrase-based MT systems (Koehn, Och, and Marcu 2003) generally adopt a reordering model that predicts reordering based on the span of a phrase and that of the adjacent phrase (Tillmann 2004; Xiong, Liu, and Lin 2006; Galley and Manning 2008; Cherry 2013).", "startOffset": 175, "endOffset": 254}, {"referenceID": 8, "context": "Phrase-based MT systems (Koehn, Och, and Marcu 2003) generally adopt a reordering model that predicts reordering based on the span of a phrase and that of the adjacent phrase (Tillmann 2004; Xiong, Liu, and Lin 2006; Galley and Manning 2008; Cherry 2013).", "startOffset": 175, "endOffset": 254}, {"referenceID": 5, "context": "Phrase-based MT systems (Koehn, Och, and Marcu 2003) generally adopt a reordering model that predicts reordering based on the span of a phrase and that of the adjacent phrase (Tillmann 2004; Xiong, Liu, and Lin 2006; Galley and Manning 2008; Cherry 2013).", "startOffset": 175, "endOffset": 254}, {"referenceID": 32, "context": "Dependency-based pre-ordering can be performed either by a rule-based approach based on manually specified human linguistic knowledge (Xu et al. 2009; Cai et al. 2014), or by a learning approach (Xia and McCord 2004; Habash 2007; Genzel 2010; Yang et al.", "startOffset": 134, "endOffset": 167}, {"referenceID": 2, "context": "Dependency-based pre-ordering can be performed either by a rule-based approach based on manually specified human linguistic knowledge (Xu et al. 2009; Cai et al. 2014), or by a learning approach (Xia and McCord 2004; Habash 2007; Genzel 2010; Yang et al.", "startOffset": 134, "endOffset": 167}, {"referenceID": 4, "context": "Dependency parsing has also been used in reordering approaches integrated with decoding to determine the next source phrase to translate after translating the current source phrase (Cherry 2008; Bach, Gao, and Vogel 2009; Chang et al. 2009).", "startOffset": 181, "endOffset": 240}, {"referenceID": 3, "context": "Dependency parsing has also been used in reordering approaches integrated with decoding to determine the next source phrase to translate after translating the current source phrase (Cherry 2008; Bach, Gao, and Vogel 2009; Chang et al. 2009).", "startOffset": 181, "endOffset": 240}, {"referenceID": 2, "context": "We propose sparse feature functions based on the pre-ordering rules of (Cai et al. 2014).", "startOffset": 71, "endOffset": 88}, {"referenceID": 2, "context": "However, in contrast to the manual rule-based pre-ordering approach of (Cai et al. 2014), the weights of our sparse feature functions are automatically learned and used during the actual translation process, without an explicit pre-ordering step.", "startOffset": 71, "endOffset": 88}, {"referenceID": 20, "context": "It is performed by a beam search algorithm (Koehn 2004a), in which the search process produces translation from left to right in the translation output.", "startOffset": 43, "endOffset": 56}, {"referenceID": 2, "context": "We define our dependency swap features, following the rule template definition for dependency swap rules (Cai et al. 2014), which defines rule instances based on the word pairs with head-child or sibling relationship.", "startOffset": 105, "endOffset": 122}, {"referenceID": 2, "context": "We define our dependency swap features, following the rule template definition for dependency swap rules (Cai et al. 2014), which defines rule instances based on the word pairs with head-child or sibling relationship. However, the difference is that our approach does not require manually specifying which dependency labels are the conditions to swap words, but learns them automatically. In our approach, each rule instance based on the above template becomes a Boolean sparse feature function (Chiang, Knight, and Wang 2009). The function parameters are the word pair specification and output order. While Cai et al. (2014) defined the rules only by the dependency labels, we define our feature functions for each word pair by the dependency labels, the POS tags, and the combination of both, resulting in a group of four feature functions for every word pair ordering.", "startOffset": 106, "endOffset": 626}, {"referenceID": 4, "context": "To encourage the translation output that conforms to the dependency parse structure, we impose a penalty feature that discourages translation candidates not conforming to the dependency parse subtree (Cherry 2008).", "startOffset": 200, "endOffset": 213}, {"referenceID": 20, "context": "Phrase-based SMT beam search decoding involves future cost, not accumulated over the translation hypotheses but used to reduce search space (Koehn 2004a).", "startOffset": 140, "endOffset": 153}, {"referenceID": 4, "context": "This follows the assumption that each subtree will have a contiguous translation (Cherry 2008).", "startOffset": 81, "endOffset": 94}, {"referenceID": 5, "context": "We incorporate sparse reordering orientation features (Cherry 2013).", "startOffset": 54, "endOffset": 67}, {"referenceID": 8, "context": "Cherry (2013) designed his sparse reordering orientation model following the hierarchical reordering (HR) model (Galley and Manning 2008), capturing the relative position of the current phrase (covered by the current hypothesis) with respect to the largest chunk of contiguous source phrases that form a contiguous translation before this phrase.", "startOffset": 112, "endOffset": 137}, {"referenceID": 3, "context": "We also utilize dependency path features (Chang et al. 2009) for phrase-based SMT, defined over the shortest path of dependency parse tree links bridging the current and the previous source phrase.", "startOffset": 41, "endOffset": 60}, {"referenceID": 3, "context": "We also utilize dependency path features (Chang et al. 2009) for phrase-based SMT, defined over the shortest path of dependency parse tree links bridging the current and the previous source phrase. Chang et al. (2009) defined the dependency path features on a maximum entropy phrase orientation classifier, trained on their word-aligned parallel text and labeled by the two possible phrase orderings in the translation output: in-order and swapped.", "startOffset": 42, "endOffset": 218}, {"referenceID": 26, "context": "Then the parallel corpus is word-aligned by GIZA++ (Och and Ney 2003) using IBM Models 1, 3, and 4 (Brown et al.", "startOffset": 51, "endOffset": 69}, {"referenceID": 1, "context": "Then the parallel corpus is word-aligned by GIZA++ (Och and Ney 2003) using IBM Models 1, 3, and 4 (Brown et al. 1993)5.", "startOffset": 99, "endOffset": 118}, {"referenceID": 13, "context": "Weight tuning is done by using the pairwise ranked optimization (PRO) algorithm (Hopkins and May 2011), which is also used to obtain weights of the sparse features to help determine the reordering.", "startOffset": 80, "endOffset": 102}, {"referenceID": 28, "context": "We build a phrase-based baseline SMT system, which uses non-sparse phrase-based lexicalized reordering (PBLR), in which the reordering probability depends on the phrase being translated and its position with respect to the source position of the previously translated phrase (Tillmann 2004; Koehn et al. 2005), and non-sparse hierarchical reordering (HR), in which the previous unit is not only the previous phrase, but the largest chunk of contiguous source phrases", "startOffset": 275, "endOffset": 309}, {"referenceID": 17, "context": "We build a phrase-based baseline SMT system, which uses non-sparse phrase-based lexicalized reordering (PBLR), in which the reordering probability depends on the phrase being translated and its position with respect to the source position of the previously translated phrase (Tillmann 2004; Koehn et al. 2005), and non-sparse hierarchical reordering (HR), in which the previous unit is not only the previous phrase, but the largest chunk of contiguous source phrases", "startOffset": 275, "endOffset": 309}, {"referenceID": 4, "context": "Dataset Base (Cherry 2008) (Chang et al.", "startOffset": 13, "endOffset": 26}, {"referenceID": 3, "context": "Dataset Base (Cherry 2008) (Chang et al. 2009) +DDP+Path (Cherry 2013) Ours +DDP +Path +SHR +DDP+DS +DDP+Path+DS", "startOffset": 27, "endOffset": 46}, {"referenceID": 5, "context": "2009) +DDP+Path (Cherry 2013) Ours +DDP +Path +SHR +DDP+DS +DDP+Path+DS", "startOffset": 16, "endOffset": 29}, {"referenceID": 4, "context": ", dependency distortion penalty (+DDP) (Cherry 2008), sparse dependency path features (+Path) (Chang et al.", "startOffset": 39, "endOffset": 52}, {"referenceID": 3, "context": ", dependency distortion penalty (+DDP) (Cherry 2008), sparse dependency path features (+Path) (Chang et al. 2009), the combination of both (+DDP+Path), as well as sparse reordering orientation features (+SHR) (Cherry 2013).", "startOffset": 94, "endOffset": 113}, {"referenceID": 5, "context": "2009), the combination of both (+DDP+Path), as well as sparse reordering orientation features (+SHR) (Cherry 2013).", "startOffset": 101, "endOffset": 114}, {"referenceID": 8, "context": "having contiguous translation (Galley and Manning 2008).", "startOffset": 30, "endOffset": 55}, {"referenceID": 22, "context": "We also use n-best Minimum Bayes Risk (MBR) decoding (Kumar and Byrne 2004) instead of the default maximum a-posteriori (MAP) decoding.", "startOffset": 53, "endOffset": 75}, {"referenceID": 4, "context": "To accommodate our sparse feature functions, our Moses code has been modified to read dependency-parsed input sentences and incorporate additional decoding features on top of our baseline, namely dependency distortion penalty (DDP) (Cherry 2008), sparse dependency path features (Path) (Chang et al.", "startOffset": 232, "endOffset": 245}, {"referenceID": 3, "context": "To accommodate our sparse feature functions, our Moses code has been modified to read dependency-parsed input sentences and incorporate additional decoding features on top of our baseline, namely dependency distortion penalty (DDP) (Cherry 2008), sparse dependency path features (Path) (Chang et al. 2009), sparse reordering orientation following hierarchical reordering orientation (SHR) (Cherry 2013), and our sparse dependency swap features (DS).", "startOffset": 286, "endOffset": 305}, {"referenceID": 5, "context": "2009), sparse reordering orientation following hierarchical reordering orientation (SHR) (Cherry 2013), and our sparse dependency swap features (DS).", "startOffset": 89, "endOffset": 102}, {"referenceID": 27, "context": "The translation quality of the system outputs is measured by case-insensitive BLEU (Papineni et al. 2002), for which the brevity penalty is computed based on the shortest reference (NIST-BLEU)7.", "startOffset": 83, "endOffset": 105}, {"referenceID": 21, "context": "Statistical significance testing between systems is conducted by bootstrap resampling (Koehn 2004b).", "startOffset": 86, "endOffset": 99}, {"referenceID": 5, "context": "Sparse reordering orientation features (Cherry 2013) leverage this by capturing the previous phrase (or contiguous chunk) properties.", "startOffset": 39, "endOffset": 52}, {"referenceID": 4, "context": "This is also the case with the system added with prior reordering approaches, which includes DDP (Cherry 2008), Path (Chang et al.", "startOffset": 97, "endOffset": 110}, {"referenceID": 3, "context": "This is also the case with the system added with prior reordering approaches, which includes DDP (Cherry 2008), Path (Chang et al. 2009), their combination DDP+Path, and SHR (Cherry 2013).", "startOffset": 117, "endOffset": 136}, {"referenceID": 5, "context": "2009), their combination DDP+Path, and SHR (Cherry 2013).", "startOffset": 43, "endOffset": 56}, {"referenceID": 4, "context": "Base, +DDP (Cherry 2008), +Path (Chang et al.", "startOffset": 11, "endOffset": 24}, {"referenceID": 3, "context": "Base, +DDP (Cherry 2008), +Path (Chang et al. 2009): identical output The export of high-tech products in Guangdong Province is the", "startOffset": 32, "endOffset": 51}, {"referenceID": 5, "context": "+SHR (Cherry 2013)", "startOffset": 5, "endOffset": 18}, {"referenceID": 3, "context": "Combining our sparse dependency swap features with sparse dependency path features (Chang et al. 2009) achieves the best experimental result.", "startOffset": 83, "endOffset": 102}, {"referenceID": 11, "context": "Another work exploits source and target dependency trees for phrase-based MT output reranking (Gimpel and Smith 2014), instead of for translation decoding.", "startOffset": 94, "endOffset": 117}, {"referenceID": 24, "context": "Prior work has also used constituency parse to guide reordering, by manually defining pre-ordering rules to reorder input sentences into the target language order before translation (Collins, Koehn, and Ku\u010derov\u00e1 2005; Wang, Collins, and Koehn 2007), or to automatically learn those rules (Li et al. 2007; Khalilov and Fonollosa 2011).", "startOffset": 288, "endOffset": 333}, {"referenceID": 16, "context": "Prior work has also used constituency parse to guide reordering, by manually defining pre-ordering rules to reorder input sentences into the target language order before translation (Collins, Koehn, and Ku\u010derov\u00e1 2005; Wang, Collins, and Koehn 2007), or to automatically learn those rules (Li et al. 2007; Khalilov and Fonollosa 2011).", "startOffset": 288, "endOffset": 333}, {"referenceID": 3, "context": "Chang et al. (2009) introduced dependency path as a soft constraint based on the sequence of source dependency links traversed in phrase-based translation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Chang et al. (2009) introduced dependency path as a soft constraint based on the sequence of source dependency links traversed in phrase-based translation. It is used as features on a maximum entropy phrase orientation classifier, whose probability output is used as a decoding feature function. As the path can be arbitrarily long, it may not be represented sufficiently in the training samples. Our sparse feature definition can alleviate this as features are defined on two words. In addition, capturing word pairs instead of paths enables incorporation of other word properties such as POS tags. Hunter and Resnik (2010) proposed a probability model to capture the offset of a word with respect to its head position in phrase-based MT.", "startOffset": 0, "endOffset": 625}, {"referenceID": 3, "context": "Chang et al. (2009) introduced dependency path as a soft constraint based on the sequence of source dependency links traversed in phrase-based translation. It is used as features on a maximum entropy phrase orientation classifier, whose probability output is used as a decoding feature function. As the path can be arbitrarily long, it may not be represented sufficiently in the training samples. Our sparse feature definition can alleviate this as features are defined on two words. In addition, capturing word pairs instead of paths enables incorporation of other word properties such as POS tags. Hunter and Resnik (2010) proposed a probability model to capture the offset of a word with respect to its head position in phrase-based MT. Their model does not take into account two sibling words sharing the same head. They reported negative result. Meanwhile, Gao, Koehn, and Birch (2011) defined soft constraints on hierarchical phrase-based MT, which produce translation by bottom-up constituency parsing algorithm instead of beam search.", "startOffset": 0, "endOffset": 891}], "year": 2016, "abstractText": "Reordering poses a major challenge in machine translation (MT) between two languages with significant differences in word order. In this paper, we present a novel reordering approach utilizing sparse features based on dependency word pairs. Each instance of these features captures whether two words, which are related by a dependency link in the source sentence dependency parse tree, follow the same order or are swapped in the translation output. Experiments on Chinese-to-English translation show a statistically significant improvement of 1.21 BLEU point using our approach, compared to a state-of-the-art statistical MT system that incorporates prior reordering approaches.", "creator": "TeX"}}}