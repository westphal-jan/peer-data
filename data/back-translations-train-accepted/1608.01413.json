{"id": "1608.01413", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Aug-2016", "title": "Solving General Arithmetic Word Problems", "abstract": "This paper presents a novel approach to automatically solving arithmetic word problems. This is the first algorithmic approach that can handle arithmetic problems with multiple steps and operations, without depending on additional annotations or predefined templates. We develop a theory for expression trees that can be used to represent and evaluate the target arithmetic expressions; we use it to uniquely decompose the target arithmetic problem to multiple classification problems; we then compose an expression tree, combining these with world knowledge through a constrained inference framework. Our classifiers gain from the use of {\\em quantity schemas} that supports better extraction of features. Experimental results show that our method outperforms existing systems, achieving state of the art performance on benchmark datasets of arithmetic word problems.", "histories": [["v1", "Thu, 4 Aug 2016 01:47:23 GMT  (143kb,D)", "https://arxiv.org/abs/1608.01413v1", null], ["v2", "Sat, 20 Aug 2016 11:50:41 GMT  (153kb,D)", "http://arxiv.org/abs/1608.01413v2", "EMNLP 2015"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["subhro roy", "dan roth"], "accepted": true, "id": "1608.01413"}, "pdf": {"name": "1608.01413.pdf", "metadata": {"source": "CRF", "title": "Solving General Arithmetic Word Problems", "authors": ["Subhro Roy", "Dan Roth"], "emails": ["danr}@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Related Work", "text": "The system described in (Hosseini et al., 2014) deals only with addition and subtraction and requires additional annotated data for verb categories. In contrast, our system does not require additional annotations and can handle a more general category of problems. The approach in (Roy et al., 2015) supports all four basic operations and uses a pipeline of classifiers to predict different properties of the problem. In contrast, it makes assumptions about the number of quantities mentioned in the problem text and the number of arithmetic steps required to solve the problem. In contrast, our system has no such limitations, effectively addressing problems that have multiple quantities and require multiple steps. Kushman's approach to the automatic solution of algebra word problems (Kushman et al., 2014) may be the easiest solution to this problem."}, {"heading": "3 Expression Tree and Problem Decomposition", "text": "We address the problem of the automatic solution of arithmetic word problems. The input to our system is the problem chain text P, the n quantities q1, q2,., qn. Our goal is to map this problem to a once-read arithmetic expression E, which, when evaluated, provides the solution to the problem. We define a once-read arithmetic expression as one that uses each quantity at most once. We say that E is a valid expression if it is such a once-read arithmetic expression, and in this work we only look at problems that can be solved by means of valid expressions (it is possible that they can also be solved with invalid expressions).An expression tree T for a valid expression is a binary tree whose leaves represent quantities, and each internal node represents one of the four basic operations. For a non-leaning node n, we represent the operation associated with it (its) as a right (n), and a child (n)."}, {"heading": "4 Mapping Problems to Expression Trees", "text": "Given the uniqueness demonstrated in paragraph 3, it is sufficient to identify the operation between two relevant parameters in the text to determine the only valid expression. In fact, identifying the operation between arbitrary set pairs, given the uncertainty of identifying the operation from the text, leads to much-needed redundancy, and we use it in our final common conclusion. Consequently, our general method proceeds as follows: In the face of problem text P, we recognize the sizes q1, q2,... qn. Subsequently, we use two classifiers, one for relevance and another for predicting the LCA operations for a monotonous expression tree of the solution. Our training uses the concept of set schemas described in Section 4.2. The distribution output of these classifiers is then used in a common inference method that determines the final expression tree. Our training data consists of problem text paired with a monotonous expression tree for the resolution of the text and the alignment of the quantity problems in the classification, as well as the quantity problems in the quantity of the expression in the text."}, {"heading": "4.1 Global Inference for Expression Trees", "text": "In this subset, we define the scoring functions that correspond to the decomposed problems 1. Let's show how we combine these results to draw global conclusions. For a problem P with the quantities q1, q2,.., qn, we define the following scoring functions: 1. PAIR (Qj, op): Scores the probability of LCA (Qi, qj, T) = op, where T is an irrelevant quantity, which is a monotonous expression of the solution expression of P. A multicultural classifier that can predict the LCA operations (Section 4.4), these scores can be the probability of quantity q, which is an irrelevant quantity in P, that is, q is not used to create the solution. A binary classifier trains to predict whether a quantity q is relevant or not (Section 4.3), we can provide these scores."}, {"heading": "4.2 Quantity Schema", "text": "In order to generalize the different problem types, as well as about simple manipulations of the text, it is necessary to train our system only with relevant information from the problem text. To this end, we introduce the concept of a quantity scheme, which we extract for each quantity in the text of the problem. Along with the question that is asked, the quantity scheme provides all the information needed to solve most arithmetic problems. A quantity scheme for a quantity q in problem P consists of the following components. The associated verb for each quantity q, we recognize the associated verb. We traverse the dependency tree that proceeds from the quantity, and select the first verb we reach."}, {"heading": "4.3 Relevance Classifier", "text": "We train a binary SVM classifier to determine whether q is needed in the numerical expression that produces the solution based on the problem text P and the quantity q contained therein. We train on gold annotations and use the score of the classifier as the scoring function IRR (\u00b7)."}, {"heading": "4.3.1 Features", "text": "Characteristics are extracted from the set diagrams and can be roughly divided into three groups: 1. Unit characteristics: Most questions explicitly mention the object whose quantity needs to be calculated, so questions provide valuable clues as to which quantities may be irrelevant. We add one feature to determine whether the quantity unit q is present in the question marks, and we also add a feature based on whether the units of other sets have better matches with question marks (based on the number of marks found), and one to determine the number of quantities that have the maximum number of matches with the question marks. 2. Related NP characteristics: Units are often insufficient to distinguish between relevant and irrelevant quantities. Consider the following: Example 3 Problem: There are 8 apples in a pile on the desk. Each apple comes in a package of 11. 5 apples are added to the bunch."}, {"heading": "4.4 LCA Operation Classifier", "text": "To predict life cycle operations, we train a multi-class SVM classifier. Given the problem text P and two sizes pi and pj, the classifier predicts one of the six labels described in Equation 2. We consider the trust values for each label provided by the classifier to be the scoring function PAIR (\u00b7)."}, {"heading": "4.4.1 Features", "text": "We use the following categories of characteristics: 1. Individual characteristics of quantity: Dependent verbs have shown that they play a significant role in solving addition and subtraction (Hosseini et al., 2014). Therefore, we add the dependent verb of quantity as a characteristic. Multiplication and division problems are largely dependent on the quantities described in the text. To grasp this, we add a characteristic based on whether the quantity is a rate and whether a component of the quantity unit is present in the question. In addition to these characteristics of the quantity scheme, we add selected tokens from the neighborhood of the quantity mentioned. Neighborhoods of quantities are often highly informative for LCA operations, for example: \"He got 80 more marbles,\" the term \"more\" usually gives adverbs and comparative adjectives mentioned in a size 5 window, we add the quantity characteristics of pairs (we add a pair)."}, {"heading": "5 Experimental Results", "text": "In this section, we evaluate the proposed methodology for publicly available datasets of arithmetic word problems. We evaluate the relevance and life cycle classifiers separately and show the contribution of different characteristics. Finally, we evaluate the performance of the entire system and quantify the gains achieved by the restrictions."}, {"heading": "5.1 Datasets", "text": "We evaluate our system on three sets of data, each of which includes a different category of arithmetic word problems. 1. AI2 Data Set: This is a collection of 395 addition and subtraction problems, published by (Hosseini et al., 2014) They performed a triple cross-validation, with each fold containing problems from 1 http: / / cogcomp.cs.illinois.edu / page / software view / predictoisSLdifferent sources. This helped them evaluate the robustness of domain diversity. We use the same evaluation setting. 2. IL Data Set: This is a collection of arithmetic problems published by (Roy et al., 2015). Each of these problems can be solved by performing an operation. However, there are several problems that have the same template. To counteract this, we are making some changes to the data sets."}, {"heading": "5.2 Relevance Classifier", "text": "Table 2 evaluates the performance of the Relevance Classifier on the AI2 and IL datasets. We report two accuracy values: Relax - fraction of the quantities correctly recorded by the classifier, and Strict - fraction of the mathematical problems for which all sets have been correctly classified. We report accuracy by using all characteristics and then removing each set of characteristics one at a time. We see that characteristics relating to units of quantity play the most important role in determining the relevance of sets. Nor are the associated NP characteristics helpful for the AI2 dataset."}, {"heading": "5.3 LCA Operation Classifier", "text": "Table 1 evaluates the performance of the LCA Operation Classifier on the AI2, IL and CC datasets. As before, we report two accuracies - Relax - fraction of the set pairs for which the classifier correctly predicted LCA operation, and Strict - fraction of the mathematical problems for which all set pairs were correctly classified. We report accuracy by using all the features and then removing each set of features individually. The strict and relaxed accuracies for IL datasets are identical, as each problem in the IL dataset requires only one operation. Characteristics relating to individual sets are most significant; in particular, the accuracy in the CC dataset goes to 0.0 without using individual set attributes. The question attributes are not helpful for classification in the CC dataset. This can be attributed to the fact that all problems in the CC dataset require multiple operations and questions in multi-level problems typically do not include any information required for each operation."}, {"heading": "5.4 Global Inference Module", "text": "Table 3 shows the performance of our system in correctly solving arithmetic word problems. We show the effects of different contracts and also compare with previously best known results of AI2 and IL datasets. We also show results that use each of the two constraints separately and do not use any constraints.The best result of the AI2 dataset to date is reported in (Hosseini et al., 2014). As we follow the exact same evaluation settings, our results are directly comparable. We reach the state of the art without having access to additional annotated data, as opposed to (Hosseini et al., 2014) using the data for verb categorization. For the IL dataset, we purchased the system from (Roy et al., 2015) the authors and operated it with the same folding information. We exceed their system with an absolute gain of over 20%. We believe that the improvement was mainly due to the dependence of the system of (Roy et al., 2015) on the AIX system."}, {"heading": "5.5 Discussion", "text": "The leading source of error for the classifiers is faulty quantity scheme extraction and lack of understanding of unknown or rare verbs. In the LCA operation classifier on the AI2 dataset, 25% of errors were due to errors in extracting the quantity schemes and 20% to rare verbs. In the LCA operation classifier on the same dataset, 16% of errors were due to unknown verbs and 15% to errors in extracting the schemes. In the operation classifier on the CC dataset, 8% of errors were due to verbs and 16% to faulty quantity scheme extraction. Quantity scheme extraction is a challenge as problems as well as some non-standard patterns are analyzed and it will be the $4 rate, for example, to buy future work in the form of toys."}, {"heading": "6 Conclusion", "text": "This paper presents a novel method of understanding and solving a general class of arithmetic word problems. Our approach can solve any problem whose solution can be expressed by a once-read arithmetic expression, in which a lot of the problem text appears at most once in expression. We are developing a new theoretical framework centered around the term monotonous expression trees, and showing how this representation can be used to obtain a unique decomposition of the problem. Naturally, this theory leads to a computational solution that we have shown to clearly determine the solution - determine the arithmetic operation between any two quantities identified in the text. This theory underlies our algorithmic solution - we are developing classifiers and a limited reasoning approach that utilizes the redundancy of the information, and show that this performs a strong performance on several benchmark collections. Specifically, our approach reaches the state of the art by supporting computational performance on two publicly available arithmetic problem sets and natural generalizations."}, {"heading": "Acknowledgments", "text": "This research was funded by DARPA (contract number FA8750-13-2-0008) and funded by AI2. Any opinions, findings, conclusions or recommendations are those of the authors and do not necessarily reflect the opinion of the agencies."}], "references": [{"title": "Aggregation via Set Partitioning for Natural Language Generation. In Human Language Technologies - North American Chapter of the Association for Computational Linguistics, June", "author": ["Barzilay", "Lapata2006] R. Barzilay", "M. Lapata"], "venue": null, "citeRegEx": "Barzilay et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2006}, {"title": "Modeling biological processes for reading comprehension", "author": ["Berant et al.2014] J. Berant", "V. Srikumar", "P. Chen", "A.V. Linden", "B. Harding", "B. Huang", "P. Clark", "C.D. Manning"], "venue": "Proceedings of EMNLP", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Structured learning with constrained conditional models", "author": ["Chang et al.2012] M. Chang", "L. Ratinov", "D. Roth"], "venue": "Machine Learning,", "citeRegEx": "Chang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2012}, {"title": "Elementary School Science and Math Tests as a Driver for AI: Take the Aristo Challenge", "author": ["P. Clark"], "venue": "In Proceedings of IAAI", "citeRegEx": "Clark.,? \\Q2015\\E", "shortCiteRegEx": "Clark.", "year": 2015}, {"title": "Constraint-based sentence compression: An integer programming approach", "author": ["Clarke", "Lapata2006] J. Clarke", "M. Lapata"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Clarke et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Clarke et al\\.", "year": 2006}, {"title": "An efficient algorithm for easy-first non-directional dependency parsing", "author": ["Goldberg", "Elhadad2010] Y. Goldberg", "M. Elhadad"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Goldberg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2010}, {"title": "Learning to solve arithmetic word problems with verb categorization", "author": ["H. Hajishirzi", "O. Etzioni", "N. Kushman"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Hosseini et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hosseini et al\\.", "year": 2014}, {"title": "Learning to automatically solve algebra word problems", "author": ["Kushman et al.2014] N. Kushman", "L. Zettlemoyer", "R. Barzilay", "Y. Artzi"], "venue": "In ACL,", "citeRegEx": "Kushman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kushman et al\\.", "year": 2014}, {"title": "The necessity of syntactic parsing for semantic role labeling", "author": ["D. Roth", "W. Yih"], "venue": "In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Punyakanok et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2005}, {"title": "The importance of syntactic parsing and inference in semantic role labeling", "author": ["D. Roth", "W. Yih"], "venue": "Computational Linguistics,", "citeRegEx": "Punyakanok et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2008}, {"title": "A linear programming formulation for global inference in natural language tasks", "author": ["Roth", "Yih2004] D. Roth", "W. Yih"], "venue": "In Hwee Tou Ng and Ellen Riloff, editors, Proc. of the Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Roth et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2004}, {"title": "Integer linear programming inference for conditional random fields", "author": ["Roth", "Yih2005] D. Roth", "W. Yih"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "Roth et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2005}, {"title": "Reasoning about quantities in natural language", "author": ["Roy et al.2015] S. Roy", "T. Vieira", "D. Roth"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Roy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2015}, {"title": "Viske: Visual knowledge extraction and question answering by visual verification of relation phrases", "author": ["Sadeghi et al.2015] F. Sadeghi", "S.K. Divvala", "A. Farhadi"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "Sadeghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sadeghi et al\\.", "year": 2015}, {"title": "Diagram understanding in geometry questions", "author": ["Seo et al.2014] M.J. Seo", "H. Hajishirzi", "A. Farhadi", "O. Etzioni"], "venue": "In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence, July", "citeRegEx": "Seo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Initial methods to address arithmetic word problems have mostly focussed on subsets of problems, restricting the number or the type of operations used (Roy et al., 2015; Hosseini et al., 2014) but could not deal with multi-step arithmetic problems involving all four basic operations.", "startOffset": 151, "endOffset": 192}, {"referenceID": 6, "context": "Initial methods to address arithmetic word problems have mostly focussed on subsets of problems, restricting the number or the type of operations used (Roy et al., 2015; Hosseini et al., 2014) but could not deal with multi-step arithmetic problems involving all four basic operations.", "startOffset": 151, "endOffset": 192}, {"referenceID": 7, "context": "The template based method of (Kushman et al., 2014), on the other hand, can deal with all types of problems, but implicitly assumes that the solution is generated from a set of predefined equation templates.", "startOffset": 29, "endOffset": 51}, {"referenceID": 6, "context": "The system described in (Hosseini et al., 2014) handles only addition and subtraction problems, and requires additional annotated data for verb categories.", "startOffset": 24, "endOffset": 47}, {"referenceID": 12, "context": "The approach in (Roy et al., 2015) supports all four basic operations, and uses a pipeline of classifiers to predict different properties of the problem.", "startOffset": 16, "endOffset": 34}, {"referenceID": 7, "context": "Kushman\u2019s approach to automatically solving algebra word problems (Kushman et al., 2014) might be the most related to ours.", "startOffset": 66, "endOffset": 88}, {"referenceID": 3, "context": "Our approach is related to work in understanding and solving elementary school standardized tests (Clark, 2015).", "startOffset": 98, "endOffset": 111}, {"referenceID": 1, "context": "The system described in (Berant et al., 2014) attempts to automatically answer biology questions, by extracting the structure of biological processes from text.", "startOffset": 24, "endOffset": 45}, {"referenceID": 14, "context": "There has also been efforts to solve geometry questions by jointly understanding diagrams and associated text (Seo et al., 2014).", "startOffset": 110, "endOffset": 128}, {"referenceID": 13, "context": "A recent work (Sadeghi et al., 2015) tries to answer science questions by visually verifying relations from images.", "startOffset": 14, "endOffset": 36}, {"referenceID": 2, "context": "Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012).", "startOffset": 107, "endOffset": 127}, {"referenceID": 8, "context": "This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015).", "startOffset": 187, "endOffset": 327}, {"referenceID": 9, "context": "This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015).", "startOffset": 187, "endOffset": 327}, {"referenceID": 12, "context": "This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015).", "startOffset": 187, "endOffset": 327}, {"referenceID": 2, "context": "Our final expression tree is an outcome of a constrained optimization process, following (Roth and Yih, 2004; Chang et al., 2012).", "startOffset": 89, "endOffset": 129}, {"referenceID": 2, "context": "The space of possible expressions is large, and we employ a beam search strategy to find the highest scoring constraint satisfying expression (Chang et al., 2012).", "startOffset": 142, "endOffset": 162}, {"referenceID": 6, "context": "Finally, if no unit token can be extracted, we assign the unit of the neighboring quantities as the unit of q (following previous work (Hosseini et al., 2014)).", "startOffset": 135, "endOffset": 158}, {"referenceID": 6, "context": "Individual Quantity features: Dependent verbs have been shown to play significant role in solving addition and subtraction problems (Hosseini et al., 2014).", "startOffset": 132, "endOffset": 155}, {"referenceID": 6, "context": "AI2 Dataset: This is a collection of 395 addition and subtraction problems, released by (Hosseini et al., 2014).", "startOffset": 88, "endOffset": 111}, {"referenceID": 12, "context": "IL Dataset: This is a collection of arithmetic problems released by (Roy et al., 2015).", "startOffset": 68, "endOffset": 86}, {"referenceID": 6, "context": "(Hosseini et al., 2014) 77.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "(Roy et al., 2015) - 52.", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "(Kushman et al., 2014) 64.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "The previously known best result in the AI2 dataset is reported in (Hosseini et al., 2014).", "startOffset": 67, "endOffset": 90}, {"referenceID": 6, "context": "We achieve state of the art results, without having access to any additional annotated data, unlike (Hosseini et al., 2014), who use labeled data for verb categorization.", "startOffset": 100, "endOffset": 123}, {"referenceID": 12, "context": "For the IL dataset, we acquired the system of (Roy et al., 2015) from the authors, and ran it with the same fold information.", "startOffset": 46, "endOffset": 64}, {"referenceID": 12, "context": "We believe that the improvement was mainly due to the dependence of the system of (Roy et al., 2015) on lexical and neighborhood of quantity features.", "startOffset": 82, "endOffset": 100}, {"referenceID": 7, "context": "Finally, we also compare against the template based system of (Kushman et al., 2014).", "startOffset": 62, "endOffset": 84}, {"referenceID": 6, "context": "(Hosseini et al., 2014) mentions the result of running the system of (Kushman et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": ", 2014) mentions the result of running the system of (Kushman et al., 2014) on AI2 dataset, and we report their result here.", "startOffset": 53, "endOffset": 75}, {"referenceID": 7, "context": "For IL and CC datasets, we used the system released by (Kushman et al., 2014).", "startOffset": 55, "endOffset": 77}, {"referenceID": 7, "context": "The template based system of (Kushman et al., 2014) performs on par with our system on the IL dataset.", "startOffset": 29, "endOffset": 51}], "year": 2016, "abstractText": "This paper presents a novel approach to automatically solving arithmetic word problems. This is the first algorithmic approach that can handle arithmetic problems with multiple steps and operations, without depending on additional annotations or predefined templates. We develop a theory for expression trees that can be used to represent and evaluate the target arithmetic expressions; we use it to uniquely decompose the target arithmetic problem to multiple classification problems; we then compose an expression tree, combining these with world knowledge through a constrained inference framework. Our classifiers gain from the use of quantity schemas that supports better extraction of features. Experimental results show that our method outperforms existing systems, achieving state of the art performance on benchmark datasets of arithmetic word problems.", "creator": "LaTeX with hyperref package"}}}