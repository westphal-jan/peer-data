{"id": "1608.01961", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Aug-2016", "title": "De-Conflated Semantic Representations", "abstract": "One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them.", "histories": [["v1", "Fri, 5 Aug 2016 18:14:19 GMT  (500kb,D)", "http://arxiv.org/abs/1608.01961v1", "EMNLP 2016"]], "COMMENTS": "EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["mohammad taher pilehvar", "nigel collier"], "accepted": true, "id": "1608.01961"}, "pdf": {"name": "1608.01961.pdf", "metadata": {"source": "CRF", "title": "De-Conflated Semantic Representations", "authors": ["Mohammad Taher Pilehvar", "Nigel Collier"], "emails": ["mp792@cam.ac.uk", "nhc30@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, the time has come for a realignment, in which there will be a realignment."}, {"heading": "2 De-Conflated Representations", "text": "Our proposed approach incorporates a series of presented word representations and uses the graph structure of a semantic lexical resource to weave the representations into those of the sense of words. Therefore, our approach requires, first, a series of pre-trained word representations (e.g. word embeddings). Any model that maps a particular word to a specified vector representation (i.e. a vector space model) can be used from our approach. In our experiments, we opted for a series of publicly available word embeddings (cf. \u00a7 3.1). Second, we need a lexicosemantic resource whose semantic relations allow us to view it as graph G = (V, E), with each vertex in the V group corresponding to a concept and edges in E that have lexicosemantic relationships between these vertebrae."}, {"heading": "2.1 Overview of the approach", "text": "Our goal is to calculate a semantic representation that places a particular sense of the word into an existing semantic word space. We achieve this by using word representations and knowledge derived from WordNet. The core of our approach lies in calculating a list of distorting words for a given sense of the word. To this end, we first analyze the semantic network of WordNet and extract a list of the most representative words that can effectively determine the semantics of individual synsets (\u00a7 2.2). We then use an effective technique that learns semantic representations for individual sense perceptions of the word by placing the senses close to their corresponding distorting words (\u00a7 2.3)."}, {"heading": "2.2 Determining sense biasing words", "text": "We use a graph-based algorithm to calculate the biases of words Bt for yt. Algorithm 1 Get sense biasing words for synset yt Require: Graph G = (V, E) of vertices V = {yi} mi = 1 (of m synsets) and edges E (semantic relations between synsets) Require: Function \u00b5 (yi) that returns for a given synset yi the words, the require: Graph G = (V, E) of vertices V = (of m synsets) and edges E (semantic relations between synsets) Require: Function \u00b5 (yi) that returns for a given synset yi the words: Target synset the words it."}, {"heading": "2.3 Learning sense representations", "text": "Let us digit the first and third terms of the noun digit in W.3.0.Net. \"I.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D.\" D. \"D.\" D. \"D.\" D. \"D.\" D.D. \"D.\" D.D. \"D.\" D.D. \"D.D.\" D.D. \"D.\" D.D. \"D.\" D. \"D.\" D.D. \"D.\" D \"D.D\" D. \"D\" D.D \"D. D\" D \"D.D\" D. D \"D\" D.D \"D.\" D \"D.\" D \"D\" D.D \"D.\" D. \"D\" D \"D\" D \"D.D\" D \"D.\" D. \"D\" D \"D\" D \"D.D\" D \"D\" D.D \"D\" D \"D.D.D\" D \"D.D\" D.D \"D\" D.D \"D\" D.D \"D\" D.D \"D\" D.D \"D\" D.D \"D.D\" D \"D.D\" D \"D.D\" D \"D.D\" D \"D.D\" D \"D\" D \"D.D\" D \"D.D\" D.D \"D.D.D\" D \"D.D.D\" D \"D.D\" D \"D.D\" D \"D\" D.D.D.D \"D.D\" D.D.D.D.D.D.D.D \"D.D.D.D.D.D.D.D.D.D.D.D.D.D.D.D.D.D.D.D.D.D.D.D.D.D.D.D."}, {"heading": "3 Experiments", "text": "Using several newer techniques, we have measured our approach to sensory representation against two standard tasks: word similarity (\u00a7 3.2), for which we evaluate both in-isolation and in-context similarity data sets, and semantic similarity at different levels (\u00a7 3.3)."}, {"heading": "3.1 Experimental setup", "text": "For our word representations, we used the 300-d Word2vec word embedding (Mikolov et al., 2013), which was trained on the Google News Dataset4, mainly due to its popularity in various NLP applications. However, our approach is equally applicable to all count-based representation techniques (Baroni and Lenci, 2010; Turney and Pantel, 2010) or to any other embedding approach (Pennington et al., 2014; LeCun et al., 2015). We leave the evaluation and comparison of different word-forming techniques with different training approaches, objectives and dimensions for future work. Parameter settings. Remember \u00a7 2.3 that our method for displaying the learning sense only needs one parameter to be matched, i.e. we did not perform a comprehensive adjustment to the value of this parameter and set its value to 1 / 5 after trying four different 4-ps."}, {"heading": "3.2 Word similarity", "text": "We compared our results with nine other sensory perceptions: the WordNet-based approaches of Pilehvar and Navigli (2015), Chen et al. (2014), Rothe and Protectors (2015), Jauhar et al. (2015) and Iacobacci et al. (2015) and the clustering-based approaches of Huang et al. (2012), Tian et al. (2014), Neelakantan et al. (2014), and Liu et al. (2015) (see also the approach of Faruqui et al. (2015), which uses knowledge from WordNet to improve word representation. From the different configurations used in (Faruqui et al., 2015), we chose the system that GloVe et al uses (Pennington et al., 2014), which uses the data from Lex."}, {"heading": "3.2.1 Experimental results", "text": "Tables 4 and 3 show the results of our system, DECONF, and the comparison systems on the SCWS and the other four similarity data sets. In both tables, we also report on the word vector baseline whenever available, which is calculated by directly comparing the corresponding word representations of the two words. Please note that the word-based baseline does not apply to the approach of Pilehvar and Navigli (2015), as it is based purely on the semantic network of WordNet and does not use pre-formed word embeddings. It is clear from the tables that our sensual representations make considerable improvements over those of words in the five data sets, underscoring the fact that the de-mixing of word representations into those of their individual meaning was highly advantageous. On the SCWS dataset, the DECONF dataset outperforms all the newer datasets of modern symbolization techniques (in their best settings)."}, {"heading": "3.2.2 Discussion", "text": "The similarity scale of the SimLex 999 dataset differs from our other benchmarks for word similarity in that it assigns relatively low values to antonymous pairs. For example, in this dataset, sunset-sunrise and man-woman are assigned to the respective similarities of 2.47 and 3.33 (in a [0, 10] similarity scale), which is in the same range as the similarity between word pairs with low domain affinity, such as head-nail (2.47), air molecule (3.05) or succeedtry (3.98). In fact, we observed that optimizing the similarity scale of our system in such a way that it reduces the similarity values between antonyms can lead to a significant improvement in the performance of this dataset. To this end, we conducted an experiment in which the similarity of a word pair was simply divided by five when the two words belonged to synsets linked by antonyms."}, {"heading": "3.3 Cross-Level semantic similarity", "text": "In addition to the comparative measure of word similarity, we evaluated the performance of our representations in the context of semantic similarity measurement. To this end, we chose the SemEval2014 task on semantic similarity on the same level (Jurgens et al., 2014, CLSS). The \"Word to sense similarity\" subtask of this task, with 500 instances in its test set, provides a suitable yardstick for the evaluation of synonyms. For one word sense of s and one word w, we calculate the similarity score according to four different strategies: the similarity of s with the most similar sense of w (MaxSim), the average similarity of s with individual senses of w (AvgSim), the direct similarity of s to w when the latter is modeled as word representation (Sense-to-Word or S2W) or as the center of its sensory perceptions of w (AvgSim), the direct similarity of s to w when the latter is modeled as word representation of a word representation of (S2W or S2W)."}, {"heading": "3.3.1 Experimental results", "text": "Table 5 shows the results for the word-to-sense dataset of the SemEval 2014 CLSS task = Net Deep Analysis by Pearson (r) and Spearman (\u03c1) correlations and for the four strategies. As can be seen from the low overall performance, the task is a very challenging benchmark with many WordNet terms outside the vocabulary or slang terms and rare uses. Nevertheless, DECONF offers a consistent improvement in comparison sensor representation techniques according to both measures and for all strategies. Throughout the four strategies, S2A is proving to be the most effective for DECONF and the representations of Rothe and Protectors (2015). Also, a comparison of our results on the S2W and S2W strategies shows that S2W and S2W strategies are not showing a consistent trend with relatively low performance across the four strategies."}, {"heading": "4 Related Work", "text": "Over the past few years, this type of word formation has resulted in only two main strands: (1) those that, similar to our work, extract knowledge from external need techniques to learn the need techniques; and (2) those that group the contexts in which a word appears in a given body of text and from them different representations for individual clusters. Examples of the first industry include the approach of Chen et al. (2014), Jauhar et al. (2015) and Rothe and Schu (2015), which all include different representations for individual clusters. Examples of the first industry are the approaches of Chen et al. (2014) using the content words in the definition of a word and WSD. But the sole use of glosses as meaningful contexts and the non-optimal WSD make the approach vague."}, {"heading": "5 Conclusions", "text": "We have proposed a technique of sensory representation, namely DECONF, which offers several advantages over the current state of the art: (1) The number of sensory perceptions in our technique is flexible and the calculated representations are associated with the senses of the word in WordNet; (2) DECONF is effective in providing an accurate representation of the sensory perceptions of the word, even for those senses that do not normally occur frequently in general text corpora; and (3) our approach is general, as it can easily be applied to any set of word representations and any semantic network without the need for extensive parameter adjustment. Our experimental results have shown that DECONF can surpass the current state of the art in several datasets in two task areas. We publish our calculated representations for about 118K synsets and 205K word senses in WordNet 3.0 at https: / / github.com / pivar / deconf) and we would also plan to use other constellar languages as we work with other networks."}, {"heading": "Acknowledgments", "text": "The authors thank PheneBank for supporting the MRC grant no. MR / M025160 / 1."}], "references": [{"title": "Evaluating and optimizing the parameters of an unsupervised graph-based wsd algorithm", "author": ["Agirre et al.2006] Eneko Agirre", "David Mart\u0131\u0301nez", "Oier L\u00f3pez de Lacalle", "Aitor Soroa"], "venue": "In Proceedings of the First Workshop on Graph Based Methods for", "citeRegEx": "Agirre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2006}, {"title": "Random walks for knowledgebased Word Sense Disambiguation", "author": ["Agirre et al.2014] Eneko Agirre", "Oier L\u00f3pez de Lacalle", "Aitor Soroa"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "Simcompass: Using deep learning word embeddings to assess cross-level similarity", "author": ["Banea et al.2014] Carmen Banea", "Di Chen", "Rada Mihalcea", "Claire Cardie", "Janyce Wiebe"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Banea et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Banea et al\\.", "year": 2014}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Baroni", "Lenci2010] Marco Baroni", "Alessandro Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Semi-Supervised Learning. MIT Press. chapter Label Propagation and Quadratic Criterion", "author": ["Bengio et al.2007] Yoshua Bengio", "Olivier Delalleau", "Nicolas Le Roux"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD Interna-", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam Khanh Tran", "Marco Baroni"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "NASARI: a Novel Approach to a SemanticallyAware Representation of Items", "author": ["Mohammad Taher Pilehvar", "Roberto Navigli"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Camacho.Collados et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Camacho.Collados et al\\.", "year": 2015}, {"title": "NASARI: Integrating explicit knowledge and corpus statistics for amultilingual representation of concepts and entities", "author": ["Mohammad Taher Pilehvar", "Roberto Navigli"], "venue": "Artificial Intelligence", "citeRegEx": "Camacho.Collados et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Camacho.Collados et al\\.", "year": 2016}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Chen et al.2014] Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Semi-supervised frame-semantic parsing for unknown predicates", "author": ["Das", "Smith2011] Dipanjan Das", "Noah A. Smith"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Das et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Das et al\\.", "year": 2011}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Associa-", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "PPDB: The paraphrase database", "author": ["Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "SimLex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2015] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Sensembed: Learning sense embeddings for word and relational similarity", "author": ["Mohammad Taher Pilehvar", "Roberto Navigli"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Iacobacci et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iacobacci et al\\.", "year": 2015}, {"title": "Ontologically grounded multisense representation learning for semantic vector space models", "author": ["Chris Dyer", "Eduard Hovy"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Associa-", "citeRegEx": "Jauhar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jauhar et al\\.", "year": 2015}, {"title": "SemEval-2014 task 3: Cross-level semantic similarity", "author": ["Mohammad Taher Pilehvar", "Roberto Navigli"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Jurgens et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jurgens et al\\.", "year": 2014}, {"title": "Meerkat Mafia: Multilingual and Cross-Level Semantic Textual Similarity systems", "author": ["Lushan Han", "Roberto Yus", "Jennifer Sleeman", "Taneeya W. Satyapanich", "Sunil R Gandhi", "Tim Finin"], "venue": null, "citeRegEx": "Kashyap et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kashyap et al\\.", "year": 2014}, {"title": "Topical word embeddings", "author": ["Liu et al.2015] Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Word sense clustering and clusterability", "author": ["Marianna Apidianaki", "Katrin Erk"], "venue": "Computational Linguistics,", "citeRegEx": "McCarthy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "McCarthy et al\\.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network", "author": ["Navigli", "Ponzetto2012] Roberto Navigli", "Simone Paolo Ponzetto"], "venue": "Artificial Intelligence,", "citeRegEx": "Navigli et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2012}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "The people\u2019s Web meets linguistic knowledge: Automatic sense alignment of Wikipedia and Wordnet", "author": ["Niemann", "Gurevych2011] Elisabeth Niemann", "Iryna Gurevych"], "venue": "In Proceedings of the Ninth International Conference on Computational Seman-", "citeRegEx": "Niemann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niemann et al\\.", "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "From senses to texts: An all-in-one graph-based approach for measuring semantic similarity", "author": ["Pilehvar", "Roberto Navigli"], "venue": "Artificial Intelligence,", "citeRegEx": "Pilehvar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pilehvar et al\\.", "year": 2015}, {"title": "Semantiklue: Robust semantic similarity at multiple levels using maximum weight matching", "author": ["Proisl et al.2014] Thomas Proisl", "Stefan Evert", "Paul Greiner", "Besim Kabashi"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (Se-", "citeRegEx": "Proisl et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Proisl et al\\.", "year": 2014}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Reisinger", "Mooney2010] Joseph Reisinger", "Raymond J. Mooney"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "AutoExtend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Rothe", "Sch\u00fctze2015] Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Rothe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rothe et al\\.", "year": 2015}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "John B. Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "The impact on", "author": ["Sanderson", "C.J. Van Rijsbergen"], "venue": null, "citeRegEx": "Sanderson et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sanderson et al\\.", "year": 1999}, {"title": "A probabilistic model for learning multiprototype word embeddings", "author": ["Tian et al.2014] Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu"], "venue": "In Proceedings of COLING", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Visualizing high-dimensional data using t-SNE", "author": ["van der Maaten", "Hinton2008] L.J.P van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Measuring semantic similarity in the taxonomy of wordnet", "author": ["Yang", "Powers2005] Dongqiang Yang", "David M.W. Powers"], "venue": "In Proceedings of the Twenty-eighth Australasian Conference on Computer Science,", "citeRegEx": "Yang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2005}, {"title": "WikiWalk: Random walks on Wikipedia for semantic relatedness", "author": ["Yeh et al.2009] Eric Yeh", "Daniel Ramage", "Christopher D. Manning", "Eneko Agirre", "Aitor Soroa"], "venue": "In Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language", "citeRegEx": "Yeh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yeh et al\\.", "year": 2009}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 23, "context": ", semantically unrelated words that are synonymous to different senses of a word are pulled towards each other in the semantic space (Neelakantan et al., 2014).", "startOffset": 133, "endOffset": 159}, {"referenceID": 14, "context": "sociate a word to multiple points in the semantic space by clustering its contexts in a given text corpus and learning distinct representations for individual clusters (Reisinger and Mooney, 2010; Huang et al., 2012).", "startOffset": 168, "endOffset": 216}, {"referenceID": 23, "context": "Neelakantan et al. (2014) tackled this issue by allowing the number to be dynamically adjusted for each word during training.", "startOffset": 0, "endOffset": 26}, {"referenceID": 0, "context": "all the other clustering-based techniques still suffer from the fact that the computed sense representations are not linked to any sense inventory, a linking which would require large amounts of senseannotated data (Agirre et al., 2006).", "startOffset": 215, "endOffset": 236}, {"referenceID": 9, "context": "biguation (WSD) to gather sense-specific contexts (Chen et al., 2014; Iacobacci et al., 2015) or take advantage of the properties of WordNet, such as synonymy and direct semantic relations (Rothe and Sch\u00fctze, 2015).", "startOffset": 50, "endOffset": 93}, {"referenceID": 15, "context": "biguation (WSD) to gather sense-specific contexts (Chen et al., 2014; Iacobacci et al., 2015) or take advantage of the properties of WordNet, such as synonymy and direct semantic relations (Rothe and Sch\u00fctze, 2015).", "startOffset": 50, "endOffset": 93}, {"referenceID": 11, "context": "The above criterion is similar to the frameworks of Das and Smith (2011) and Faruqui et al. (2015) which, though being convex, is", "startOffset": 77, "endOffset": 99}, {"referenceID": 4, "context": "usually solved for efficiency reasons by an iterative method proposed by Bengio et al. (2007). Following these works, we obtain the below equation for computing the representation of a word sense si:", "startOffset": 73, "endOffset": 94}, {"referenceID": 11, "context": "Following Faruqui et al. (2015), we set \u03b1 to 1.", "startOffset": 10, "endOffset": 32}, {"referenceID": 5, "context": "biasing words can be obtained for larger sense inventories, such as FreeBase (Bollacker et al., 2008) or BabelNet (Navigli and Ponzetto, 2012).", "startOffset": 77, "endOffset": 101}, {"referenceID": 21, "context": "As our word representations, we used the 300-d Word2vec (Mikolov et al., 2013) word embeddings trained on the Google News dataset4 mainly for their popularity across different NLP applications.", "startOffset": 56, "endOffset": 78}, {"referenceID": 25, "context": "However, our approach is equally applicable to any count-based representation technique (Baroni and Lenci, 2010; Turney and Pantel, 2010) or any other embedding approach (Pennington et al., 2014; LeCun et al., 2015).", "startOffset": 170, "endOffset": 215}, {"referenceID": 9, "context": "We compared our results against nine other sense representation techniques: the WordNet-based approaches of Pilehvar and Navigli (2015), Chen et al. (2014), Rothe and Sch\u00fctze", "startOffset": 137, "endOffset": 156}, {"referenceID": 14, "context": "(2015), Jauhar et al. (2015), and Iacobacci et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 14, "context": "(2015), and Iacobacci et al. (2015) and the clustering-based approaches of Huang et al.", "startOffset": 12, "endOffset": 36}, {"referenceID": 14, "context": "(2015) and the clustering-based approaches of Huang et al. (2012), Tian et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 14, "context": "(2015) and the clustering-based approaches of Huang et al. (2012), Tian et al. (2014), Neelakantan et al.", "startOffset": 46, "endOffset": 86}, {"referenceID": 14, "context": "(2015) and the clustering-based approaches of Huang et al. (2012), Tian et al. (2014), Neelakantan et al. (2014), and Liu et al.", "startOffset": 46, "endOffset": 113}, {"referenceID": 14, "context": "(2015) and the clustering-based approaches of Huang et al. (2012), Tian et al. (2014), Neelakantan et al. (2014), and Liu et al. (2015) (please see \u00a74 for more details).", "startOffset": 46, "endOffset": 136}, {"referenceID": 11, "context": "From the different configurations presented in (Faruqui et al., 2015) we chose the system that uses GloVe (Pennington et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 25, "context": ", 2015) we chose the system that uses GloVe (Pennington et al., 2014) with all WordNet relations which is their best performing monolingual system.", "startOffset": 44, "endOffset": 69}, {"referenceID": 11, "context": "approach of Faruqui et al. (2015) which uses knowledge derived from WordNet for improving word representations.", "startOffset": 12, "endOffset": 34}, {"referenceID": 11, "context": "approach of Faruqui et al. (2015) which uses knowledge derived from WordNet for improving word representations. From the different configurations presented in (Faruqui et al., 2015) we chose the system that uses GloVe (Pennington et al., 2014) with all WordNet relations which is their best performing monolingual system. As for the approach of Jauhar et al. (2015), we show the results of the EM+RETERO system which performs most consistently across different datasets.", "startOffset": 12, "endOffset": 366}, {"referenceID": 6, "context": "As our word similarity benchmark, we considered five datasets: RG-65 (Rubenstein and Goodenough, 1965), YP-130 (Yang and Powers, 2005), MEN-3K (Bruni et al., 2014), SimLex-999 (Hill et al.", "startOffset": 143, "endOffset": 163}, {"referenceID": 14, "context": "MEN-3K Iacobacci et al. (2015) \u2212 80.", "startOffset": 7, "endOffset": 31}, {"referenceID": 11, "context": "2 Faruqui et al. (2015) \u2212 75.", "startOffset": 2, "endOffset": 24}, {"referenceID": 11, "context": "2 Faruqui et al. (2015) \u2212 75.9 \u2212 73.7 Pilehvar and Navigli (2015) 61.", "startOffset": 2, "endOffset": 66}, {"referenceID": 14, "context": "1 Iacobacci et al. (2015) \u2212 87.", "startOffset": 2, "endOffset": 26}, {"referenceID": 11, "context": "2 Faruqui et al. (2015) \u2212 84.", "startOffset": 2, "endOffset": 24}, {"referenceID": 11, "context": "2 Faruqui et al. (2015) \u2212 84.2 \u2212 76.7 Pilehvar and Navigli (2015) 80.", "startOffset": 2, "endOffset": 66}, {"referenceID": 15, "context": "9 Iacobacci et al. (2015) \u2212 63.", "startOffset": 2, "endOffset": 26}, {"referenceID": 18, "context": "8 Neelakantan et al. (2014) (best) 67.", "startOffset": 2, "endOffset": 28}, {"referenceID": 9, "context": "3 Chen et al. (2014) 66.", "startOffset": 2, "endOffset": 21}, {"referenceID": 9, "context": "3 Chen et al. (2014) 66.2 68.9 Liu et al. (2015) (best) \u2212 68.", "startOffset": 2, "endOffset": 49}, {"referenceID": 9, "context": "3 Chen et al. (2014) 66.2 68.9 Liu et al. (2015) (best) \u2212 68.1 Huang et al. (2012) 62.", "startOffset": 2, "endOffset": 83}, {"referenceID": 9, "context": "3 Chen et al. (2014) 66.2 68.9 Liu et al. (2015) (best) \u2212 68.1 Huang et al. (2012) 62.8 65.7 Tian et al. (2014) (best) \u2212 65.", "startOffset": 2, "endOffset": 112}, {"referenceID": 9, "context": "3 Chen et al. (2014) 66.2 68.9 Liu et al. (2015) (best) \u2212 68.1 Huang et al. (2012) 62.8 65.7 Tian et al. (2014) (best) \u2212 65.7 Iacobacci et al. (2015) 62.", "startOffset": 2, "endOffset": 150}, {"referenceID": 9, "context": "3 Chen et al. (2014) 66.2 68.9 Liu et al. (2015) (best) \u2212 68.1 Huang et al. (2012) 62.8 65.7 Tian et al. (2014) (best) \u2212 65.7 Iacobacci et al. (2015) 62.4 \u2212 Jauhar et al. (2015) \u2212 58.", "startOffset": 2, "endOffset": 178}, {"referenceID": 14, "context": "For the SCWS dataset, we follow the past works (Reisinger and Mooney, 2010; Huang et al., 2012) and report the results according to two system configurations: (1) AvgSim: where the similarity between two words is computed as the average of all the pairwise similarities between their senses, and (2) AvgSimC: where each pairwise sense similarity is weighted by the relevance of each sense to its corresponding context.", "startOffset": 47, "endOffset": 95}, {"referenceID": 14, "context": "pare against the publicly-available sense representations of Iacobacci et al. (2015), Rothe and Sch\u00fctze (2015), Pilehvar and Navigli (2015) and Chen et al.", "startOffset": 61, "endOffset": 85}, {"referenceID": 14, "context": "pare against the publicly-available sense representations of Iacobacci et al. (2015), Rothe and Sch\u00fctze (2015), Pilehvar and Navigli (2015) and Chen et al.", "startOffset": 61, "endOffset": 111}, {"referenceID": 14, "context": "pare against the publicly-available sense representations of Iacobacci et al. (2015), Rothe and Sch\u00fctze (2015), Pilehvar and Navigli (2015) and Chen et al.", "startOffset": 61, "endOffset": 140}, {"referenceID": 9, "context": "(2015), Rothe and Sch\u00fctze (2015), Pilehvar and Navigli (2015) and Chen et al. (2014) which are linked to the WordNet sense inventory.", "startOffset": 66, "endOffset": 85}, {"referenceID": 9, "context": "The representations of Chen et al. (2014) perform best with the S2W strategy whereas those of Iacobacci et al.", "startOffset": 23, "endOffset": 42}, {"referenceID": 9, "context": "The representations of Chen et al. (2014) perform best with the S2W strategy whereas those of Iacobacci et al. (2015) do not show a consistent trend with relatively low performance across the four strategies.", "startOffset": 23, "endOffset": 118}, {"referenceID": 14, "context": "Our analysis showed that the performances of the approaches of Rothe and Sch\u00fctze (2015) and Iacobacci et al. (2015) were hampered partly due to their limited coverage.", "startOffset": 92, "endOffset": 116}, {"referenceID": 9, "context": "Chen et al. (2014) provide near-full coverage for", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "9 Iacobacci et al. (2015)\u2217 19.", "startOffset": 2, "endOffset": 26}, {"referenceID": 9, "context": "1 Chen et al. (2014)\u2217 17.", "startOffset": 2, "endOffset": 21}, {"referenceID": 15, "context": "0 \u2212 \u2212 \u2212 \u2212 Iacobacci et al. (2015) 19.", "startOffset": 10, "endOffset": 34}, {"referenceID": 9, "context": "and Sch\u00fctze (2015) and Chen et al. (2014) that use older versions of WordNet (1.", "startOffset": 23, "endOffset": 42}, {"referenceID": 18, "context": "The three best-performing systems in the task are Meerkat Mafia (Kashyap et al., 2014) (r = 37.", "startOffset": 64, "endOffset": 86}, {"referenceID": 2, "context": "3), SimCompass (Banea et al., 2014) (r = 35.", "startOffset": 15, "endOffset": 35}, {"referenceID": 18, "context": "itage Dictionary, Wiktionary and WordNet, in order to handle slang terms and rare usages, which leads to its competitive performance (Kashyap et al., 2014).", "startOffset": 133, "endOffset": 155}, {"referenceID": 9, "context": "Examples for the first branch include the approaches of Chen et al. (2014), Jauhar et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 9, "context": "Examples for the first branch include the approaches of Chen et al. (2014), Jauhar et al. (2015) and Rothe and Sch\u00fctze (2015), all of which use WordNet as an external resource and obtain sense representations for this sense inventory.", "startOffset": 56, "endOffset": 97}, {"referenceID": 9, "context": "Examples for the first branch include the approaches of Chen et al. (2014), Jauhar et al. (2015) and Rothe and Sch\u00fctze (2015), all of which use WordNet as an external resource and obtain sense representations for this sense inventory.", "startOffset": 56, "endOffset": 126}, {"referenceID": 15, "context": "Other work in this branch include SensEmbed (Iacobacci et al., 2015) and Nasari (Camacho-Collados et al.", "startOffset": 44, "endOffset": 68}, {"referenceID": 7, "context": ", 2015) and Nasari (Camacho-Collados et al., 2015; Camacho-Collados et al., 2016) which are based on the BabelNet sense inventory (Navigli and Ponzetto, 2012).", "startOffset": 19, "endOffset": 81}, {"referenceID": 8, "context": ", 2015) and Nasari (Camacho-Collados et al., 2015; Camacho-Collados et al., 2016) which are based on the BabelNet sense inventory (Navigli and Ponzetto, 2012).", "startOffset": 19, "endOffset": 81}, {"referenceID": 19, "context": "Other prominent work in the category include topical word embeddings (Liu et al., 2015) which use latent topic models for assigning topics to each word in a corpus and learn topic-", "startOffset": 69, "endOffset": 87}, {"referenceID": 14, "context": "specific word representations, and the technique proposed by Huang et al. (2012) which incorporates \u201cglobal document context.", "startOffset": 61, "endOffset": 81}, {"referenceID": 14, "context": "specific word representations, and the technique proposed by Huang et al. (2012) which incorporates \u201cglobal document context.\u201d Tian et al. (2014) modified the Skip-gram model in order to learn multiple embeddings for each word type.", "startOffset": 61, "endOffset": 146}, {"referenceID": 20, "context": "of senses, ignoring the fact that polysemy is highly dynamic across words that can range from monosemous to highly ambiguous with dozens of associated meanings (McCarthy et al., 2016).", "startOffset": 160, "endOffset": 183}, {"referenceID": 20, "context": "of senses, ignoring the fact that polysemy is highly dynamic across words that can range from monosemous to highly ambiguous with dozens of associated meanings (McCarthy et al., 2016). Neelakantan et al. (2014) tackled this issue by estimating the", "startOffset": 161, "endOffset": 211}, {"referenceID": 0, "context": "the existence of high coverage sense-annotated data (Agirre et al., 2006).", "startOffset": 52, "endOffset": 73}, {"referenceID": 12, "context": "Another notable line of research incorporates knowledge from external resources, such as PPDB (Ganitkevitch et al., 2013) and WordNet, to improve word embeddings (Yu and Dredze, 2014; Faruqui et al.", "startOffset": 94, "endOffset": 121}, {"referenceID": 11, "context": ", 2013) and WordNet, to improve word embeddings (Yu and Dredze, 2014; Faruqui et al., 2015).", "startOffset": 48, "endOffset": 91}], "year": 2016, "abstractText": "One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them.", "creator": "LaTeX with hyperref package"}}}