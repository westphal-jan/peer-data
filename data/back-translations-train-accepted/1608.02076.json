{"id": "1608.02076", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Aug-2016", "title": "Bi-directional Attention with Agreement for Dependency Parsing", "abstract": "We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of soft headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 7 languages.", "histories": [["v1", "Sat, 6 Aug 2016 07:16:31 GMT  (92kb,D)", "https://arxiv.org/abs/1608.02076v1", "EMNLP 2016"], ["v2", "Thu, 22 Sep 2016 08:52:31 GMT  (92kb,D)", "http://arxiv.org/abs/1608.02076v2", "EMNLP 2016"]], "COMMENTS": "EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["hao cheng", "hao fang", "xiaodong he", "jianfeng gao", "li deng"], "accepted": true, "id": "1608.02076"}, "pdf": {"name": "1608.02076.pdf", "metadata": {"source": "CRF", "title": "Bi-directional Attention with Agreement for Dependency Parsing", "authors": ["Hao Cheng", "Hao Fang", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "emails": ["chenghao@uw.edu", "hfang@uw.edu", "xiaohe@microsoft.com", "jfgao@microsoft.com", "deng@microsoft.com"], "sections": [{"heading": null, "text": "We are developing a novel bidirectional dependency parsing attention model that learns to agree on forward and backward keyword predictions; the parsing procedure for each direction is formulated in such a way that the memory component that stores continuous keyword embeddings is sequentially queried; the proposed parser uses soft keyword embeddings that allow the model to implicitly capture the history of high order parsing without dramatically increasing computational complexity; and we are conducting experiments in English, Chinese, and 12 other languages of the CoNLL 2006 common task, which show that the proposed model achieves state-of-the-art values for unlabeled attachments in 6 languages.1"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, and they feel able to survive on their own. Most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own."}, {"heading": "2 A MemNet-based Dependency Parser", "text": "The proposed parser first encodes each word in a sentence by continuous embedding using a bidirectional RNN and then performs two types of operations, namely 1) header predictions based on the bidirectional embedding history and 2) the relation prognosis conditioned on the current modifier and its predicted keyword both in the embedding space. In the following, we first present the construction of the token embedding, and then the key components of the proposed parser, i.e. the memory component and the query component, are discussed in detail. Finally, we describe the parsing algorithm using a bidirectional attention model with consent."}, {"heading": "2.1 Token Embeddings", "text": "In the proposed BiAtt-DP, the memory and query components share the same token embedding. We use the term additive token embedding as in (Botha and Blunsom, 2014) to use the available information about the token, such as its word form, lemma, part-of-speech (POS) tag and morphological features. In particular, the token embedding is called Eformeformi + E poseposi + E lemmaelemmaelemmai + \u00b7 \u00b7, where the egg vectors are for the ith word, and E's are parameters that need to be learned to store the continuous embedding for appropriate function. Note that these one-hot encoding vectors have different dimensions, depending on individual word sizes, and all E's have the same first dimension, but different second dimension. The additive token embedding allows us to integrate a variety of information."}, {"heading": "2.2 Components", "text": "As shown in Figure 1, the proposed BiAtt-DP has three components, i.e. one storage capacity, which we have listed in Figure 1. (The Figure 1, which we have listed in Figure 1.) (The Figure 1, which we have listed in Figure 1.) (The Figure 1, which we have listed in Figure 1.) (The Figure 1, which we have listed in Figure 1.) (The Figure 2, which we have listed in Figure 1.) (The Figure 1, which we have listed in Figure 1.) The Figure 2, which we have listed in Figure 1. (The Figure 1, which we have listed in Figure 1.) (The Figure 1, which we have in Figure 1, which we have in Figure 1.) (The 1, which we have in Figure 1, which we have in Figure 1. (The 1, which we have, which we have in Figure 1.) (The 1, which we have in Figure 1, which we have, which we have in Figure 1.) (The 1, which we have in Figure 1, which we have in Figure 1.)."}, {"heading": "2.3 Parsing by Attention with Agreement", "text": "For the bidirectional attention model, the underlying probability distributions old and a r t may not agree with each other. To promote the correspondence, we use the mathematically convenient metric, i.e. the square Hellinger distance H2 (alt | | r) to quantify the distance between these two distortions. To analyze the dependence, if the golden alignment is known during training, we can set an upper limit on the latent agreement target asH2 (alt, a r) \u2264 2 D (gt | alt) + D (gt | art), where D (\u00b7 \u00b7 \u00b7 art) the golden alignment is the KL divergence. The complete derivation occurs in scope A. During optimization, we can safely drop the constant scaler and the square root operation in the upper limit, resulting in the following loss function)."}, {"heading": "3 Model Learning", "text": "For the t-th word (modifier) wt in a set of length n, let H lt and H r t declare random variables that encompass the predicted keyword from forward (left to right) and backward (right to left). Also, let Rt declare the random variable that denotes the dependency relationship for wt. The common probability of the keyword and the relation predictions can be asP (R1: n, H l 1: n, H r 1: n) = n (Rt | w1: n) P (H lt | w1: n) P (Hrt | w1: n) n) = n, Rt \u00b7 a t, Hlt \u00b7 art, Hrt \u00b7 art (5) where at each step we assume that the header-modified relationships and headwords from both directions are independent of each other."}, {"heading": "4 Experiments", "text": "In this section we present the parsing accuracy of the proposed BiAtt-DP in 14 languages. We report on both the UAS and the marked Attachment Score (LAS) determined by the CoNLL-X eval.pl script2, which ignores punctuation symbols. Keyword predictions are generated by the MST search, which slightly improves both UAS and LAS (absolutely less than 0.3%). Overall, the proposed BiAtt-DP achieves competitive parsing accuracy in all languages as a state-of-the-art parser, achieving better UAS in 6 languages. We also show the effects of the use of POS tags and pre-trained word embeddings. In addition, in this section different variants of the full model are compared."}, {"heading": "4.1 Data", "text": "We work with the English data set Treebank-3 (PTB et al., 1999), the Chinese data set Treebank-5.1 (CTB) (Palmer et al., 2005) and 12 other languages of the joint task CoNLL 2006 (Buchholz et al., 2006). For PTB and CTB data sets, we use exactly the same setup as in (Chen et al., 2014; Dyer et al., 2015). Specifically, we convert the English and Chinese data using the Stanford parser v3.3.0 (de Marneffe et al., 2006) and the Penn2Malt tool (Zhang and Clark, 2008). For English, POS tags are converted using the Stanford POS tag v3.3.0 (Toutanova et al., 2003), 2http: / / ilk.uvt.nl / conll / software.htmlwhile for English we use gold segmentation word v3.0 (Toutwaranova et al, 2ftwart.ht.nl)."}, {"heading": "4.2 Model Configurations", "text": "The size of the hidden layer remains the same in the proposed BiAtt-DP for all RNNs. We also require that the dimension of the embedded tokens is the same as the size of the hidden layer. Note that we link the hidden layers of two RNNs for the construction of mj, and therefore we have e = 2d. The weight matrices C and D project vectors mj and qt respectively on the same dimension h, which is equivalent to English and Chinese. Since the dimension of pre-trained word embedding is 300, we use 300 x h as the dimension of the embedding parameters E. For the 12 other languages, we use square matrices for the embedding parameters E. For all languages, we match the hidden layer size and choose one according to UAS on the dev set. The selected hidden layer sizes for these languages are: 368 (English), 114 (Chinese), 160 (French) (French) (12224) (French) (126)."}, {"heading": "4.3 Results", "text": "The results are presented in Table 1 and Table 2. However, it can be seen that the BiAtt-DP outperforms all other graph-based parsers on PTB. For Chinese, we will outperform all other graph-based parsers on PTB. Compared with other pseudonymic parsers on PTB. Compared with other pseudonymic parasites, we will use the dependency-based word descriptions on https: / / goo.gl / tWke3I (Levy and Goldberg, 2014). For Chinese, we will use 192 dimensional parasites on PTB before training. Compared with other languages on English, we will use the word embeddings in Chinese Gigawords al. (2005). Transition-based parsers achieve better accuracy than Chen and Manning (2014), which uses a feed-forward network, and Dyer et al. (2015), which uses three style LSTM networks."}, {"heading": "4.4 Ablative Study", "text": "At this point, we will attempt to examine the impact of the use of pre-trained word embedding, POS tags and bidirectional query components on our model. First, we will start from our best model (Model 1 in Table 4) in English, which uses 300 as the token embedding dimension and 368 as the hidden layer size. We will keep these model parameter dimensions unchanged and analyze various factors by increasing the analytical accuracy in the PTB dev set.The results are summarized in Table 4. Comparing models 1-3, it can be observed that without pre-trained word embedding, both UAS and LAS will decrease by 0.6%, and without the use of POS tags in token embedding, the numbers in the UAS will decrease by 1.6% and in LAS by 2.6%. In terms of query components, it can be observed that the use of a single query component (Models 4-5) will decrease the UAS by 0.7-0.9% and LAS by 2.%, compared to LAS by about 1.0%."}, {"heading": "5 Related Work", "text": "The Neural Dependency Parsing Model: Recently developed neural dependency parsers are mostly transition-based models that read words sequentially from a buffer into a stack and incrementally build a parseparate tree by predicting a sequence of transitions (Yamada and Matsumoto, 2003; Nivre, 2004). However, an advanced neural network is used in the form of (Chen and Manning, 2014), in which they represent the current state with 18 selected elements such as the top words on the stack and the buffer. Each element is encrypted by concentrated embedding of words, POS tags and arc labels. Their parser dependency achieves an improvement in both accuracy and parse speed."}, {"heading": "6 Conclusion", "text": "In this paper, we develop a bidirectional attention model by promoting the agreement between the latent attention alignments q and q. By a simple and interpretable approach, we establish the link between latent and observed alignments to train the model. We apply the bidirectional attention model, which applies the match target during the training to the proposed memory network-based dependency saver. The resulting parser is able to implicitly capture the analysis history of high order without being affected by the output of high computational complexity for graph-based dependency saver. We have conducted empirical studies across 14 languages.The accuracy of the proposed model is highly competitive with the state-of-the-art dependency savers. For English, the proposed BiAttDP exceeds all graph-based parsers. It also achieves state-of-the-art performance in 6 languages with respect to the UAS and demonstrates the effectiveness of the wood directivity of its use \u00b2 with respect to the wood bidirectivity of the distance proposed."}, {"heading": "B UAS Scores of MSTParsers", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Improved transition-based parsing and tagging with neural networks", "author": ["Chris Alberti", "David Weiss", "Slav Petrov", "Slav Petrov."], "venue": "Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pages 1354\u20131359.", "citeRegEx": "Alberti et al\\.,? 2015", "shortCiteRegEx": "Alberti et al\\.", "year": 2015}, {"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL).", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. Int. Conf. Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pages 349\u2013359.", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["Bernd Bohnet", "Joakim Nivre."], "venue": "Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pages 1455\u20131465.", "citeRegEx": "Bohnet and Nivre.,? 2012", "shortCiteRegEx": "Bohnet and Nivre.", "year": 2012}, {"title": "Very high accurarcy and fast dependency parsing is not a contradiction", "author": ["Bernd Bohnet."], "venue": "Proc. Int. Conf. Computational Linguistics (COLING), pages 89\u201397.", "citeRegEx": "Bohnet.,? 2010", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A. Botha", "Phil Blunsom."], "venue": "Proc. Int. Conf. Machine Learning (ICML).", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "author": ["Sabine Buchholz", "Erwin Marsi."], "venue": "Proc. Conf. Computational Natural Language Learning (CoNLL), pages 149\u2013164.", "citeRegEx": "Buchholz and Marsi.,? 2006", "shortCiteRegEx": "Buchholz and Marsi.", "year": 2006}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahadanau", "Fethhi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proc. Conf. Empirical", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "On the shortest arborescene of a directed graph", "author": ["Yoeng-Jin Chu", "Tseng-Hong Liu."], "venue": "Science Sinica, 14:1396\u20131400.", "citeRegEx": "Chu and Liu.,? 1965", "shortCiteRegEx": "Chu and Liu.", "year": 1965}, {"title": "A fundamental algorithm for dependency parsing", "author": ["Michael A. Covington."], "venue": "Proc. Annu. ACM Southeast Conf., pages 95\u2013102.", "citeRegEx": "Covington.,? 2001", "shortCiteRegEx": "Covington.", "year": 2001}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine de Marneffe", "Bill MacCartney", "Christopher D. Manning."], "venue": "Proc. Int. Conf. Language Resources and Evaluation (LREC).", "citeRegEx": "Marneffe et al\\.,? 2006", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Transitionbased dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL), pages 334\u2013343.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Optimum branchings", "author": ["Jack Edmonds."], "venue": "Journal of Research of the National Bureau of Standards, 718(4):233\u2013240.", "citeRegEx": "Edmonds.,? 1967", "shortCiteRegEx": "Edmonds.", "year": 1967}, {"title": "Chinese Gigaword Second Edition LDC2005T14", "author": ["David Graff", "Ke Chen", "Junbo Kong", "Kazuaki Maeda."], "venue": "Web Download.", "citeRegEx": "Graff et al\\.,? 2005", "shortCiteRegEx": "Graff et al\\.", "year": 2005}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proc. Int. Conf. Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Dual decomposition for parsing with non-projective head automata", "author": ["Terry Koo", "Alexander M. Rush", "Michael Collins", "Tommi Jaakkola", "David Sontag."], "venue": "Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pages 1288\u20131298.", "citeRegEx": "Koo et al\\.,? 2010", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL), pages 1381\u20131391.", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Dependencybased word embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL), pages 302\u2013 308.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Alignment by agreement", "author": ["Percy Liang", "Ben Tasker", "Dan Klein."], "venue": "Proc. Human Language Technology Conf. and Conf. North American Chapter Assoc. for Computational Linguistics (HLT-NAACL), pages 104\u2013111.", "citeRegEx": "Liang et al\\.,? 2006", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Treebank-3 LDC99T42", "author": ["Mitchell Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz", "Ann Taylor."], "venue": "Web Download.", "citeRegEx": "Marcus et al\\.,? 1999", "shortCiteRegEx": "Marcus et al\\.", "year": 1999}, {"title": "Turbo parsers: Dependency parsing by approximate variational inference", "author": ["Andr\u00e8 F.T. Martins", "Noah A. Smith", "Eric P. Xing."], "venue": "Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pages 34\u201344.", "citeRegEx": "Martins et al\\.,? 2010", "shortCiteRegEx": "Martins et al\\.", "year": 2010}, {"title": "Turing on the turbo: Fast third-order non-projective turbo parsers", "author": ["Andr\u00e8 F.T. Martins", "Miguel B. Almeida", "Noah A. Smith."], "venue": "Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL), pages 617\u2013622.", "citeRegEx": "Martins et al\\.,? 2013", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L. Mass", "Awni Y. Hannun", "Andrew Y. Ng."], "venue": "Proc. Int. Conf. Machine Learning (ICML).", "citeRegEx": "Mass et al\\.,? 2013", "shortCiteRegEx": "Mass et al\\.", "year": 2013}, {"title": "Online learning of approximate dependency parsing algorithms", "author": ["Ryan McDonald", "Fernando Pereira."], "venue": "Proc. European Chapter Assoc. for Computational Linguistics (EACL), pages 81\u201388.", "citeRegEx": "McDonald and Pereira.,? 2006", "shortCiteRegEx": "McDonald and Pereira.", "year": 2006}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["Ryan McDonald", "Fernando Pererira", "Kiril Ribarov", "Jan Haji\u010d."], "venue": "Proc. Human Language Technology Conf. and Conf. Empirical Methods Natural Language Process. (HLT/EMNLP), pages", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proc. Workshop at Int. Conf. Learning Representations.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre."], "venue": "Proc. Int. Conf. Parsing Technologies (IWPT), pages 149\u2013160.", "citeRegEx": "Nivre.,? 2003", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Incrementality in deterministic dependency parsing: Bringing engineering and cognition together", "author": ["Joakim Nivre."], "venue": "Proc. Workshop at ACL.", "citeRegEx": "Nivre.,? 2004", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Chinese Treebank 5.0 LDC2005T01", "author": ["Martha Palmer", "Fu-Dong Chiou", "Nianwen Xue", "Tsan-Kuang Lee"], "venue": "Web Download", "citeRegEx": "Palmer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Palmer et al\\.", "year": 2005}, {"title": "A linear-time translation system for crossing interval trees", "author": ["Emily Pitler", "Ryan McDonald."], "venue": "Proc. Conf. North American Chapter Assoc. for Computational Linguistics (NAACL), pages 662\u2013671.", "citeRegEx": "Pitler and McDonald.,? 2015", "shortCiteRegEx": "Pitler and McDonald.", "year": 2015}, {"title": "Learning representations by backpropogating errors", "author": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams."], "venue": "Nature, 323(6088):533\u2013536, October.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Vine pruning for efficient multi-pass dependency parsing", "author": ["Alexander M. Rush", "Slav Petrov."], "venue": "Proc. Conf. North American Chapter Assoc. for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 498\u2013507.", "citeRegEx": "Rush and Petrov.,? 2012", "shortCiteRegEx": "Rush and Petrov.", "year": 2012}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."], "venue": "Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS), pages 2431\u20132439.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Finding optimum branchings", "author": ["Robert E. Tarjan."], "venue": "Networks, 7(1):25\u201335.", "citeRegEx": "Tarjan.,? 1977", "shortCiteRegEx": "Tarjan.", "year": 1977}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer."], "venue": "Proc. Human Language Technology Conf. and Conf. North American Chapter Assoc. for Computational Linguis-", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS), pages 2692\u20132700.", "citeRegEx": "Vinyals et al\\.,? 2015a", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS), pages 2755\u20132763.", "citeRegEx": "Vinyals et al\\.,? 2015b", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL), pages 323\u2013 333.", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "Proc. Int. Conf. Learning Representations (ICLR).", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Statistical dependency analysis with support vector machine", "author": ["Hiroyasu Yamada", "Yuji Matsumoto."], "venue": "Proc. Int. Conf. Parsing Technologies (IWPT), pages 195\u2013206.", "citeRegEx": "Yamada and Matsumoto.,? 2003", "shortCiteRegEx": "Yamada and Matsumoto.", "year": 2003}, {"title": "A tale of two parsers: investigating and combining graph-based and transition-based depdency parsing using beam-search", "author": ["Yue Zhang", "Stephen Clark"], "venue": null, "citeRegEx": "Zhang and Clark.,? \\Q2008\\E", "shortCiteRegEx": "Zhang and Clark.", "year": 2008}, {"title": "Generalized higher-order dependency parsing with cube pruning", "author": ["Hao Zhang", "Ryan McDonald."], "venue": "Proc. Conf. Empirical Methods Natural Language Process. and Computational Natural Language Learning (EMNLP-CoNLL), pages 320\u2013331.", "citeRegEx": "Zhang and McDonald.,? 2012", "shortCiteRegEx": "Zhang and McDonald.", "year": 2012}, {"title": "Enforcing structural diversity in cube-pruned dependency parsing", "author": ["Hao Zhang", "Ryan McDonald."], "venue": "Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL), pages 656\u2013661.", "citeRegEx": "Zhang and McDonald.,? 2014", "shortCiteRegEx": "Zhang and McDonald.", "year": 2014}, {"title": "Online learning for inexact hypergraph search", "author": ["Hao Zhang", "Liang Huang", "Kai Zhao", "Ryan McDonald."], "venue": "Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pages 908\u2013913.", "citeRegEx": "Zhang et al\\.,? 2013", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Steps to excellence: Simple inference with the refined scoring of dependency trees", "author": ["Yuan Zhang", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola", "Amir Golberson."], "venue": "Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL), pages 197\u2013207.", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 40, "context": "The memory network framework has been studied in the context of question answering and language modeling (Weston et al., 2015; Sukhbaatar et al., 2015), whereas the neural attention model under the encoder-decoder framework has been applied to machine translation (Bahdanau et al.", "startOffset": 105, "endOffset": 151}, {"referenceID": 34, "context": "The memory network framework has been studied in the context of question answering and language modeling (Weston et al., 2015; Sukhbaatar et al., 2015), whereas the neural attention model under the encoder-decoder framework has been applied to machine translation (Bahdanau et al.", "startOffset": 105, "endOffset": 151}, {"referenceID": 2, "context": ", 2015), whereas the neural attention model under the encoder-decoder framework has been applied to machine translation (Bahdanau et al., 2015) and constituency parsing (Vinyals et al.", "startOffset": 120, "endOffset": 143}, {"referenceID": 38, "context": ", 2015) and constituency parsing (Vinyals et al., 2015b).", "startOffset": 33, "endOffset": 56}, {"referenceID": 2, "context": "Although already used in the encoder for capturing global context information (Bahdanau et al., 2015), the bi-directional recurrent neural network (RNN) has yet to be employed in the decoder.", "startOffset": 78, "endOffset": 101}, {"referenceID": 2, "context": "Although already used in the encoder for capturing global context information (Bahdanau et al., 2015), the bi-directional recurrent neural network (RNN) has yet to be employed in the decoder. Bi-directional decoding is expected to be advantageous over the previously developed uni-directional counterpart, because the former exploits richer contextual information. Intuitively, we can use two separate uni-directional RNNs where each one constructs its respective attended encoder context vectors for computing RNN hidden states. However, the drawback of this approach is that the decoder would often produce different alignments resulting in discrepancies for the forward and backward directions. In this paper, we design a training objective function to enforce attention agreement between both directions, inspired by the alignmentby-agreement idea from Liang et al. (2006). Specifically, we develop a dependency parser (BiAtt-DP) using a bi-directional attention model based on the memory network.", "startOffset": 79, "endOffset": 877}, {"referenceID": 26, "context": "This formulation is adopted by graph-based parsers such as the MSTParser (McDonald et al., 2005).", "startOffset": 73, "endOffset": 96}, {"referenceID": 8, "context": "of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers.", "startOffset": 114, "endOffset": 225}, {"referenceID": 39, "context": "of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers.", "startOffset": 114, "endOffset": 225}, {"referenceID": 0, "context": "of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers.", "startOffset": 114, "endOffset": 225}, {"referenceID": 13, "context": "of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers.", "startOffset": 114, "endOffset": 225}, {"referenceID": 3, "context": "of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers.", "startOffset": 114, "endOffset": 225}, {"referenceID": 25, "context": "Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2) for a sentence with n words.", "startOffset": 136, "endOffset": 164}, {"referenceID": 6, "context": "We use the notion of additive token embedding as in (Botha and Blunsom, 2014) to utilize the available information about the token, e.", "startOffset": 52, "endOffset": 77}, {"referenceID": 8, "context": "Moreover, we only need to make a single decision on the dimensionality of the token embedding, rather than a combination of decisions on word embeddings and POS tag embeddings as in concatenated token embeddings used by Chen and Manning (2014), Dyer et al.", "startOffset": 220, "endOffset": 244}, {"referenceID": 8, "context": "Moreover, we only need to make a single decision on the dimensionality of the token embedding, rather than a combination of decisions on word embeddings and POS tag embeddings as in concatenated token embeddings used by Chen and Manning (2014), Dyer et al. (2015) and Weiss et al.", "startOffset": 220, "endOffset": 264}, {"referenceID": 8, "context": "Moreover, we only need to make a single decision on the dimensionality of the token embedding, rather than a combination of decisions on word embeddings and POS tag embeddings as in concatenated token embeddings used by Chen and Manning (2014), Dyer et al. (2015) and Weiss et al. (2015). It reduces the number of model parameters to be tuned, especially when lots of different features are used.", "startOffset": 220, "endOffset": 288}, {"referenceID": 24, "context": "The activation function of this projection layer is the leaky rectified linear (LReL) function (Mass et al., 2013) with 0.", "startOffset": 95, "endOffset": 114}, {"referenceID": 9, "context": "To address the vanishing gradient issue in RNNs, we use the gated recurrent unit (GRU) proposed by Cho et al. (2014), where an update gate and a reset gate are employed to control the information flow.", "startOffset": 99, "endOffset": 117}, {"referenceID": 41, "context": "On the other hand, by recursively feeding both the query vector and the soft headword embedding into the RNN, the model implicitly captures high-order parsing history information, which can potentially improve the parsing accuracy (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006).", "startOffset": 231, "endOffset": 287}, {"referenceID": 25, "context": "On the other hand, by recursively feeding both the query vector and the soft headword embedding into the RNN, the model implicitly captures high-order parsing history information, which can potentially improve the parsing accuracy (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006).", "startOffset": 231, "endOffset": 287}, {"referenceID": 25, "context": "For example, an k-th order MSTParser (McDonald and Pereira, 2006) has O(nk+1) complexity for a sentence of n words.", "startOffset": 37, "endOffset": 65}, {"referenceID": 11, "context": "This parsing procedure is also similar to the exhaustive left-to-right modifier-first search algorithm described in (Covington, 2001), but it is enhanced by an additional right-to-left search with the agreement enforcement.", "startOffset": 116, "endOffset": 133}, {"referenceID": 26, "context": "(log at,j + log a r t,j) as a score of the corresponding arc and then search for the MST to form a dependency parse tree, as proposed in (McDonald et al., 2005).", "startOffset": 137, "endOffset": 160}, {"referenceID": 10, "context": "The MST search is achieved via the ChuLiu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967), which can be implemented in O(n2) for dense graphs according to Tarjan (1977).", "startOffset": 60, "endOffset": 94}, {"referenceID": 14, "context": "The MST search is achieved via the ChuLiu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967), which can be implemented in O(n2) for dense graphs according to Tarjan (1977).", "startOffset": 60, "endOffset": 94}, {"referenceID": 10, "context": "The MST search is achieved via the ChuLiu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967), which can be implemented in O(n2) for dense graphs according to Tarjan (1977). In practice, the MST search slows down the parsing speed by 6\u201310%.", "startOffset": 61, "endOffset": 174}, {"referenceID": 32, "context": "The gradients are computed via the back-propagation algorithm (Rumelhart et al., 1986).", "startOffset": 62, "endOffset": 86}, {"referenceID": 16, "context": "We use stochastic gradient descent with the Adam algorithm proposed in (Kingma and Ba, 2015).", "startOffset": 71, "endOffset": 92}, {"referenceID": 21, "context": "We work on the English Treebank-3 (PTB) dataset (Marcus et al., 1999), the Chinese Treebank-5.", "startOffset": 48, "endOffset": 69}, {"referenceID": 30, "context": "1 (CTB) dataset (Palmer et al., 2005), and 12 other languages from the CoNLL 2006 shared task (Buchholz and Marsi, 2006).", "startOffset": 16, "endOffset": 37}, {"referenceID": 7, "context": ", 2005), and 12 other languages from the CoNLL 2006 shared task (Buchholz and Marsi, 2006).", "startOffset": 64, "endOffset": 90}, {"referenceID": 8, "context": "For PTB and CTB datasets, we use exactly the same setup as in (Chen and Manning, 2014; Dyer et al., 2015).", "startOffset": 62, "endOffset": 105}, {"referenceID": 13, "context": "For PTB and CTB datasets, we use exactly the same setup as in (Chen and Manning, 2014; Dyer et al., 2015).", "startOffset": 62, "endOffset": 105}, {"referenceID": 42, "context": ", 2006) and the Penn2Malt tool (Zhang and Clark, 2008), respectively.", "startOffset": 31, "endOffset": 54}, {"referenceID": 36, "context": "0 (Toutanova et al., 2003),", "startOffset": 2, "endOffset": 26}, {"referenceID": 19, "context": "gl/tWke3I (Levy and Goldberg, 2014).", "startOffset": 10, "endOffset": 35}, {"referenceID": 27, "context": "For Chinese, we pre-train 192-dimension skip-gram embeddings (Mikolov et al., 2013) on Chinese Gigawords (Graff et al.", "startOffset": 61, "endOffset": 83}, {"referenceID": 15, "context": ", 2013) on Chinese Gigawords (Graff et al., 2005).", "startOffset": 29, "endOffset": 49}, {"referenceID": 11, "context": "6 Dyer et al. (2015) 93.", "startOffset": 2, "endOffset": 21}, {"referenceID": 11, "context": "6 Dyer et al. (2015) 93.2 90.9 B&N (2012)\u2020 93.", "startOffset": 2, "endOffset": 42}, {"referenceID": 0, "context": "22 Alberti et al. (2015)\u2020 94.", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "22 Alberti et al. (2015)\u2020 94.23 92.41 Weiss et al. (2015)\u2020 94.", "startOffset": 3, "endOffset": 58}, {"referenceID": 0, "context": "22 Alberti et al. (2015)\u2020 94.23 92.41 Weiss et al. (2015)\u2020 94.26 92.41 Andor et al. (2016)\u2217 94.", "startOffset": 3, "endOffset": 91}, {"referenceID": 5, "context": "Graph Bohnet (2010)\u2020 92.", "startOffset": 6, "endOffset": 20}, {"referenceID": 5, "context": "Graph Bohnet (2010)\u2020 92.88 90.71 Martins et al. (2013)\u2020 92.", "startOffset": 6, "endOffset": 55}, {"referenceID": 5, "context": "Graph Bohnet (2010)\u2020 92.88 90.71 Martins et al. (2013)\u2020 92.89 90.55 Z&M (2014)\u2020 93.", "startOffset": 6, "endOffset": 79}, {"referenceID": 13, "context": "the same POS tagger as C&M (2014) and Dyer et al. (2015),", "startOffset": 38, "endOffset": 57}, {"referenceID": 0, "context": "\u2020 and \u2217 are provided in (Alberti et al., 2015) and (Andor et al.", "startOffset": 24, "endOffset": 46}, {"referenceID": 13, "context": "4 Dyer et al. (2015) 87.", "startOffset": 2, "endOffset": 21}, {"referenceID": 5, "context": "the transition-based parsers, it achieves better accuracy than Chen and Manning (2014), which uses a feed-forward neural network, and Dyer et al.", "startOffset": 63, "endOffset": 87}, {"referenceID": 5, "context": "the transition-based parsers, it achieves better accuracy than Chen and Manning (2014), which uses a feed-forward neural network, and Dyer et al. (2015), which uses three stack LSTM networks.", "startOffset": 63, "endOffset": 153}, {"referenceID": 3, "context": "Compared with the integrated parsing and tagging models, the BiAtt-DP outperforms Bohnet and Nivre (2012) but has a small gap to Alberti et al.", "startOffset": 82, "endOffset": 106}, {"referenceID": 0, "context": "Compared with the integrated parsing and tagging models, the BiAtt-DP outperforms Bohnet and Nivre (2012) but has a small gap to Alberti et al. (2015). On CTB, it achieves best UAS and similar LAS.", "startOffset": 129, "endOffset": 151}, {"referenceID": 18, "context": "Specifically, we show UAS of the 3rd-order RBGParser as reported in (Lei et al., 2014) since it also uses low-dimensional continuous embeddings.", "startOffset": 68, "endOffset": 86}, {"referenceID": 18, "context": "First, in (Lei et al., 2014), the lowdimensional continuous embeddings are derived", "startOffset": 10, "endOffset": 28}, {"referenceID": 7, "context": "Table 3: UAS on 12 languages in the CoNLL 2006 shared task (Buchholz and Marsi, 2006).", "startOffset": 59, "endOffset": 85}, {"referenceID": 18, "context": "The results of the 3rd-order RBGParser are reported in (Lei et al., 2014).", "startOffset": 55, "endOffset": 73}, {"referenceID": 31, "context": "same dataset in terms of UAS among (Pitler and McDonald, 2015), (Zhang and McDonald, 2014), (Zhang et al.", "startOffset": 35, "endOffset": 62}, {"referenceID": 44, "context": "same dataset in terms of UAS among (Pitler and McDonald, 2015), (Zhang and McDonald, 2014), (Zhang et al.", "startOffset": 64, "endOffset": 90}, {"referenceID": 45, "context": "same dataset in terms of UAS among (Pitler and McDonald, 2015), (Zhang and McDonald, 2014), (Zhang et al., 2013), (Zhang", "startOffset": 92, "endOffset": 112}, {"referenceID": 33, "context": "and McDonald, 2012), (Rush and Petrov, 2012), (Martins et al.", "startOffset": 21, "endOffset": 44}, {"referenceID": 23, "context": "and McDonald, 2012), (Rush and Petrov, 2012), (Martins et al., 2013), (Martins et al.", "startOffset": 46, "endOffset": 68}, {"referenceID": 22, "context": ", 2013), (Martins et al., 2010), and (Koo et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 17, "context": ", 2010), and (Koo et al., 2010).", "startOffset": 13, "endOffset": 31}, {"referenceID": 31, "context": "effectiveness of the parser in dealing with non-projectivity, we follow (Pitler and McDonald, 2015), to compute the recall of crossed", "startOffset": 72, "endOffset": 99}, {"referenceID": 25, "context": "Second, the RBGParser uses combined scoring of arcs by including traditional features from the MSTParser (McDonald and Pereira, 2006) / TurboParser (Martins et al.", "startOffset": 105, "endOffset": 133}, {"referenceID": 23, "context": "Second, the RBGParser uses combined scoring of arcs by including traditional features from the MSTParser (McDonald and Pereira, 2006) / TurboParser (Martins et al., 2013).", "startOffset": 148, "endOffset": 170}, {"referenceID": 46, "context": "Third, the RBGParser employs a third-order parsing algorithm based on (Zhang et al., 2014), although it also implements a first-order parsing algorithm, which achieves lower UAS in general.", "startOffset": 70, "endOffset": 90}, {"referenceID": 25, "context": "An arguably fair comparison for the BiAttDP is the MSTParser (McDonald and Pereira, 2006), since the BiAtt-DP replaces the scoring function for arcs but uses exactly the same search algorithm.", "startOffset": 61, "endOffset": 89}, {"referenceID": 18, "context": "Due to the space limit, we refer readers to (Lei et al., 2014) for results of the MSTParsers (also shown in Appendix B).", "startOffset": 44, "endOffset": 62}, {"referenceID": 31, "context": "Finally, following (Pitler and McDonald, 2015), we also analyze the performance of the BiAtt-DP on both crossed and uncrossed arcs.", "startOffset": 19, "endOffset": 46}, {"referenceID": 31, "context": "For these four languages, the BiAtt-DP achieves better UAS than that reported in (Pitler and McDonald, 2015).", "startOffset": 81, "endOffset": 108}, {"referenceID": 31, "context": "Finally, following (Pitler and McDonald, 2015), we also analyze the performance of the BiAtt-DP on both crossed and uncrossed arcs. Since the BiAttDP uses a graph-based non-projective parsing algorithm, it is interesting to evaluate the performance on crossed arcs, which result in the non-projectivity of the dependency tree. The last three columns of Table 3 show the recall of crossed arcs, that of uncrossed arcs, and the percentage of crossed arcs in the test set. Pitler and McDonald (2015) reported numbers on the same data for Dutch, German, Portuguese, and Slovene as in this paper.", "startOffset": 20, "endOffset": 497}, {"referenceID": 41, "context": "Neural Dependency Parsing: Recently developed neural dependency parsers are mostly transition-based models, which read words sequentially from a buffer into a stack and incrementally build a parse tree by predicting a sequence of transitions (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004).", "startOffset": 242, "endOffset": 296}, {"referenceID": 28, "context": "Neural Dependency Parsing: Recently developed neural dependency parsers are mostly transition-based models, which read words sequentially from a buffer into a stack and incrementally build a parse tree by predicting a sequence of transitions (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004).", "startOffset": 242, "endOffset": 296}, {"referenceID": 29, "context": "Neural Dependency Parsing: Recently developed neural dependency parsers are mostly transition-based models, which read words sequentially from a buffer into a stack and incrementally build a parse tree by predicting a sequence of transitions (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004).", "startOffset": 242, "endOffset": 296}, {"referenceID": 8, "context": "A feed-forward neural network is used in (Chen and Manning, 2014), where they represent the current state with 18 selected elements such as the top words on the stack and buffer.", "startOffset": 41, "endOffset": 65}, {"referenceID": 0, "context": "The model is extended to integrate parsing and tagging in (Alberti et al., 2015).", "startOffset": 58, "endOffset": 80}, {"referenceID": 7, "context": "A feed-forward neural network is used in (Chen and Manning, 2014), where they represent the current state with 18 selected elements such as the top words on the stack and buffer. Each element is encoded by concatenated embeddings of words, POS tags, and arc labels. Their dependency parser achieves improvement on both accuracy and parsing speed. Weiss et al. (2015) improve the parser using semi-supervised structured learning and unlabeled data.", "startOffset": 42, "endOffset": 367}, {"referenceID": 0, "context": "The model is extended to integrate parsing and tagging in (Alberti et al., 2015). On the other hand, Dyer et al. (2015) develop the stack LSTM architecture, which uses three LSTMs to respectively model the sequences of buffer states, stack states, and actions.", "startOffset": 59, "endOffset": 120}, {"referenceID": 26, "context": "The MSTParser (McDonald et al., 2005) and the TurboParser (Martins et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 22, "context": ", 2005) and the TurboParser (Martins et al., 2010) are two examples of graphbased parsers.", "startOffset": 28, "endOffset": 50}, {"referenceID": 18, "context": "The RBGParser proposed in (Lei et al., 2014) can also be viewed as a graph-based parser, which scores arcs using low-dimensional continuous features derived from low-rank tensors as well as features used by MSTParser/TurboParser.", "startOffset": 26, "endOffset": 44}, {"referenceID": 46, "context": "It also employs a sampler-based algorithm for parsing (Zhang et al., 2014).", "startOffset": 54, "endOffset": 74}, {"referenceID": 34, "context": "Neural Attention Model: The proposed BiAttDP is closely related to the memory network (Sukhbaatar et al., 2015) for question answering, as well as the neural attention models for machine translation (Bahdanau et al.", "startOffset": 86, "endOffset": 111}, {"referenceID": 2, "context": ", 2015) for question answering, as well as the neural attention models for machine translation (Bahdanau et al., 2015) and constituency parsing (Vinyals et al.", "startOffset": 95, "endOffset": 118}, {"referenceID": 38, "context": ", 2015) and constituency parsing (Vinyals et al., 2015b).", "startOffset": 33, "endOffset": 56}, {"referenceID": 37, "context": "The similar idea is employed by the pointer network in (Vinyals et al., 2015a), which is used to solve three different combinatorial optimization problems.", "startOffset": 55, "endOffset": 78}, {"referenceID": 18, "context": "We use the numbers reported in (Lei et al., 2014).", "startOffset": 31, "endOffset": 49}], "year": 2016, "abstractText": "We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of soft headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages.1", "creator": "TeX"}}}