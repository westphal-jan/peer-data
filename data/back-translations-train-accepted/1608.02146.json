{"id": "1608.02146", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Aug-2016", "title": "Leveraging Union of Subspace Structure to Improve Constrained Clustering", "abstract": "Many clustering problems in computer vision and other contexts are also classification problems, where each cluster shares a meaningful label. Subspace clustering algorithms in particular are often applied to problems that fit this description, for example with face images or handwritten digits. While it is straightforward to request human input on these datasets, our goal is to reduce this input as much as possible. We present an algorithm for active query selection that allows us to leverage the union of subspace structure assumed in subspace clustering. The central step of the algorithm is in querying points of minimum margin between estimated subspaces; analogous to classifier margin, these lie near the decision boundary. This procedure can be used after any subspace clustering algorithm that outputs an affinity matrix and is capable of driving the clustering error down more quickly than other state-of-the-art active query algorithms on datasets with subspace structure. We demonstrate the effectiveness of our algorithm on several benchmark datasets, and with a modest number of queries we see significant gains in clustering performance.", "histories": [["v1", "Sat, 6 Aug 2016 19:29:58 GMT  (1526kb)", "http://arxiv.org/abs/1608.02146v1", "17 pages, 7 figures"], ["v2", "Wed, 13 Sep 2017 21:17:33 GMT  (2900kb,D)", "http://arxiv.org/abs/1608.02146v2", "11 pages, 8 figures"]], "COMMENTS": "17 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["john lipor", "laura balzano"], "accepted": true, "id": "1608.02146"}, "pdf": {"name": "1608.02146.pdf", "metadata": {"source": "CRF", "title": "Leveraging Union of Subspace Structure to Improve Constrained Clustering", "authors": ["John Lipor"], "emails": ["lipor@umich.edu", "girasole@umich.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.02 146v 1 [cs.L G] 6A ugMany cluster problems in computer vision and other contexts are also classification problems where each cluster has a meaningful name. In particular, cluster algorithms in subspace are often applied to problems that fit this description, e.g. with facial images or handwritten numbers. Although it is easy to request human input to these data sets, our goal is to reduce this input as much as possible. We present an active query selection algorithm that allows us to use the unification of subspace structures adopted in subspace clusters. The central step of the algorithm is to query points of the minimum margin between estimated subspaces; analogous to the classification span, these are close to the decision boundary. This method can be applied to any cluster algorithm in subspace that issues an affinity matrix and is able to drive cluster errors to the latest state of the art algorithm faster than other active ones."}, {"heading": "1 Introduction", "text": "eDi eeisrrcnluiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "2 Related Work", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "3 UoS-Based Pairwise-Constrained Clustering", "text": "In this paper, we assume that all subspaces have the same dimension, but it is possible to expand our algorithm to deal with unequal dimensions. Denote the true clustering of a point x x x x x by C (x). Let the output of a cluster algorithm (such as SSC or TSC) be an affinity / similarity matrix A and a set of label estimates {C (xi)} Ni = 1. Our algorithm for PCC consists of an initialization and three main steps. To initialize, we build a set of specific sets Z using an explorer-like algorithm similar to [24]. Next, a test point is reached by using either the min margin criterion for subspaces [29] or a concept of margin based on affinity."}, {"heading": "3.1 Sample Selection via Margin", "text": "In [30], the author notes that active query of points with minimum margin (as opposed to maximum entropy or minimum trust) is a suitable choice for reducing classification errors. In [31], the authors present a margin-based binary classification algorithm that achieves an optimal convergence rate (within a logarithmic factor). In this section, we will consider two terms of margin - one based on subspace spacing and one based on the input affinity matrix."}, {"heading": "3.1.1 Residual-Based Margin", "text": "The concept of the margin for sub-spaces was first examined in [29]. For a sub-space Sk with orthogonal projection matrix Pk, leave the distance of a point to that sub-space (x, \u00b7 \u00b7, K). Then the sub-space margin of a point x-X is defined as [29] -1 (x) = maximum 6 = k-points [K] dist (x, Sk-points) dist (x, Sk-points) dist (x) dist (x, Sj-points) dist (x) dist (x) dist (x, Sj) dist (x) dist (x) (x). (1) The point of the minimum margin is then defined as arg maxx (x).1 (x). Note: The fraction is a value in [0, 1] where the larger \u00b51 (x) are closer to the decision boundary. A value of 1 implies that the point x corresponds to its two narrowest sub-spaces."}, {"heading": "3.1.2 Affinity-Based Margin", "text": "We also consider a version of the margin calculated from the entries of the affinity matrix itself, which is inspired by the non-parametric entropy estimate in [8]. Considering an affinity matrix A, we estimate the probability that a point xi is in the subspace k asP (k | xi) = \u2211 j: C (xj) = k Aij \u2211 N j = 1 Aij. Let us leave the probability that a point xi is in the subspace k asP (l | x). Let us define the affinity margin of a point x X as\u00b52 (x) = max j 6 = l \u0445, j [K] P (j | x) P (l \u0445 | x). (3) The point of the minimum margin follows again as arg maxx x \u00b52 (x)."}, {"heading": "3.2 Pairwise Constrained Clustering with SUPERPAC", "text": "The question of whether it is a matter of a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way, in which it is about which it is about a way and in which it is about a way, in which it is about which it is about a way and in which it is about which it is about a way, and in which it is about which it is about which it is about which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about which it is about which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about which it is about which it is about which it is about a way and in which it is about which it is about which it is about which is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which"}, {"heading": "4 Experimental Results", "text": "We compare the performance of our method with the non-parametric version of the URASC algorithm (URASC-N) 2 across a variety of data sets. We refer to the residual-based margin (1) form of our algorithm as SUPERPAC-R and affinity-based (3) form as SUPERPAC-A. We use a maximum query budget of 2K for UoS exploration and exploration. We also compared the algorithm from [9], but found that this algorithm performed worse than URASC with much higher computing costs. For the sake of completeness, we also compare random constraints in which queries are consistently randomly selected from the set of unqueried images. Finally, we compare against the oracle PCA classifier that we are now defining. Let Uk be the d-dimensional PCA estimate of the points whose true designation is C (x) = k."}, {"heading": "4.1 Extended Yale Face Database B", "text": "In fact, the fact is that most of them are able to move to a different world in which they are able than in another world in which they are able to live and live."}, {"heading": "4.2 Handwritten Digits", "text": "We look at the MNIST database, which consists of 10,000 centered 28 pixel images of the handwritten digits 0-9. In previous papers, only sets of K = 2 and 3 digits were considered, and then the higher values of K were considered more recently and included in our future work. In this section, we show the effectiveness of PCC in drastically reducing the number of queries. We follow the similarity with the previous section and select 100 random subsets of K, taking into account the subspace dimension d = 3."}, {"heading": "4.3 Motion Data", "text": "Finally, we will compare the performance of the various methods on the Hopkins 155 dataset. This set consists of 155 video sequences with marked characteristic points on fixed moving objects. These points have been shown to be in a union of affinity sub-spaces of dimension no more than 3 [38]. The 155 sequences contain K = 2 and 3 movements, depending on the sequence. We will compare the performance because the number of paired comparisons is increased with the input affinity matrix obtained from the SSC. Results are in Fig. 5. With this dataset, where the sub-spaces are affinity and the centres are therefore not close together, we can see that the URASC algorithm works as well as SUPERPAC, although it does not use the subspace structure. Modification of SUPERPAC to take affinity into account is a topic of our future research."}, {"heading": "4.4 Evaluation of Selection Methods", "text": "In the following numbers, we compare SUPERPAC, URASC, and random queries with \"Active Random\" queries. We also remove the Explore step during initialization. Unlike purely random label queries, Active Random uses all available query information to provide a more realistic benchmark for active PCC algorithms. We first compare the performance of URASC and both random algorithms on the Sonar, Balance, and Leaf 250 records, which are all used in the database."}, {"heading": "5 Conclusion", "text": "We have presented a method for selecting and including pair-wise constraints in subspace clusters that takes into account the underlying geometric structure of the problem. We have shown that the subspace classifier union model is often used in computer vision applications where it is possible to request input from human labelers in the form of pair-wise constraints. We have shown that labeling for subspace classifiers is often necessary to achieve a cluster error close to zero, since even an oracle PCA approach does not allow for perfect clustering; in addition, these constraints improve the clustering process as a whole and allow perfect clustering with a modest number of requests for human input. We view this work as a bridge between adaptive quantum selection to picturesquely limited clustering and adaptive sampling for structured signals (see previous work on sparse [12, 13], structural spares [14], structural spares [14], and low ranges] [40]."}, {"heading": "A Proof of Theorem 1", "text": "The proof is based on Theorem 5.2.1 of [48], which is reproduced below. Theorem 2. (concentration on the Gauss space) Let us consider a random vector X (0, \u03c32ID) and a Lipschitz function f: RD \u2192 R. Let us then consider for each t: 0, Pr: f (X) \u2212 Ef (X) \u2212 Ef (X) \u2264 2 exp (\u2212 ct2\u03c32) f: f = f: f Lip is the lipschitz constant of f: 1. Let us first consider the numerical value and note that y \u2212 P1y = P: 1 y: N (0, \u03c3 2P: 1) with E: P: 1 y: p: 2 (D: p). Let us leave f: z: z: 2 the f: p: 1 and z: p: 2 the p: 2, by the exercise 5.2.5 of [48] we can replace E: d: D: p: d."}, {"heading": "B Proof of Corollary 1", "text": "We first make a few comments to link our results to other frequently used pitch distances. < < ---------------------------------------------------------------- < If the two pitches are equal, the projection on the orthogonal complement is zero; if they are orthogonal, we get the norm of U2 alone, resulting in a distance of 1. \u2212 \u2212 \u2212 This corresponds to the more visually symmetrical 1 \u2212 1 d \u00b2 U T 1 U2 \u00b2 2 F, another common distance. Furthermore, we note that when defining main angles [32], 1 \u2212 1d \u00b2 \u2212 UT1 U2 \u00b2 \u2212 2 F = 1 \u2212 1dd \u00b2 i = 1cos2 (\u03c6i) = 1dd \u00b2 (\u0432i).Proof."}], "references": [{"title": "Lambertian reflectance and linear subspaces", "author": ["R. Basri", "D. Jacobs"], "venue": "IEEE TPAMI, vol. 25, no. 2, pp. 218\u2013233, February 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "A bayesian computer vision system for modeling human interactions", "author": ["N. Oliver", "B. Rosario", "A. Pentland"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 831\u2013843, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A. Georghiades", "P. Belhumeur", "D. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence, vol. 23, no. 6, pp. 643\u2013660, 2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "A benchmark for the comparison of 3-D motion segmentation algorithms", "author": ["R. Tron", "R. Vidal"], "venue": "IEEE Int. Conf. on Comp. Vision and Pattern Recog., 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J.C. Burges"], "venue": "2016. [Online]. Available: yann.lecun.com/exdb/mnist", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Measuring constraint-set utility for partitional clustering algorithms", "author": ["I. Davidson", "K.L. Wagstaff", "S. Basu"], "venue": "Proc. European Conf. on Machine Learning and Prinicpals and Practice of Knowledge Discovery in Databases, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Active clustering with model-based uncertainty reduction", "author": ["C. Xiong", "D.M. Johnson", "J.J. Corso"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence, 2016, accepted for publication.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Active image clustering with pairwise constraints from humans", "author": ["A. Biswas", "D. Jacobs"], "venue": "International Journal on Computer Vision, vol. 108, pp. 133\u2013147, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["E. Elhamifar", "R. Vidal"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 35, pp. 2765\u20132781, Nov. 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 31, pp. 210\u2013227, Feb. 2009. 15", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Distilled sensing: Adaptive sampling for sparse detection and estimation", "author": ["J. Haupt", "R.M. Castro", "R. Nowak"], "venue": "IEEE Transactions on Information Theory, vol. 57, no. 9, pp. 6222\u20136235, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "On the power of adaptivity in sparse recovery", "author": ["P. Indyk", "E. Price", "D.P. Woodruff"], "venue": "Foundations of Computer Science (FOCS), 2011 IEEE 52nd Annual Symposium on. IEEE, 2011, pp. 285\u2013294.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "On the fundamental limits of recovering tree sparse vectors from noisy linear measurements", "author": ["A. Soni", "J. Haupt"], "venue": "IEEE Transactions on Information Theory, vol. 60, no. 1, pp. 133\u2013149, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Subspace clustering", "author": ["R. Vidal"], "venue": "IEEE Signal Processing Magazine, vol. 28, pp. 52\u201368, Mar. 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust subspace clustering via thresholding", "author": ["R. Heckel", "H. B\u00f6lcskei"], "venue": "IEEE Trans. Inf. Theory, vol. 24, no. 11, pp. 6320\u20136342, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Hybrid linear modeling via local best-fit flats", "author": ["T. Zhang", "A. Szlam", "Y. Wang", "G. Lerman"], "venue": "International Journal of Computer Vision, vol. 100, pp. 217\u2013240, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Greedy subspace clustering", "author": ["D. Park", "C. Caramanis", "S. Sanghavi"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 2753\u20132761.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A Geometric Analysis of Subspace Clustering with Outliers", "author": ["M. Soltanolkotabi", "E.J. Candes"], "venue": "The Annals of Statistics, vol. 40, no. 4, pp. 2195\u20132238, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust Subspace Clustering", "author": ["\u2014\u2014"], "venue": "The Annals of Statistics, vol. 42, no. 2, pp. 669\u2013699, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Noisy sparse subspace clustering", "author": ["Y.-X. Wang", "H. Xu"], "venue": "Proceedings of The 30th International Conference on Machine Learning, 2013, pp. 89\u201397.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Graph connectivity in noisy sparse subspace clustering", "author": ["Y. Wang", "Y.-X. Wang", "A. Singh"], "venue": "Proceedings of The 19th International Conference on Artificial Intelligence and Statistics, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Constrained K-means clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schroedl"], "venue": "Proc. Int. Conf. on Machine Learning, 2001.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Active semi-supervision for pairwise constrained clustering", "author": ["S. Basu", "A. Banerjee", "R.J. Mooney"], "venue": "Proc. SIAM Int. Conf. on Data Mining, 2004.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Active query selection for semi-supervised clustering", "author": ["P.K. Mallapragada", "R. Jin", "A.K. Jain"], "venue": "Proc. Int. Conf. on Pattern Recognition, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Active constrained clustering by examining spectral eigenvectors", "author": ["Q. Xu", "M. desJardins", "K.L. Wagstaff"], "venue": "Proc. 8th Int. Conf. on Discovery Science, 2005.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Active spectral clustering", "author": ["X. Wang", "I. Davidson"], "venue": "Proc. 10th Int. Conf. on Data Mining, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Margin-based active subspace clustering", "author": ["J. Lipor", "L. Balzano"], "venue": "Proc. Int. Workshop on Computational Advances in Multi-Sensor Adaptive Processing, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Active Learning", "author": ["B. Settles"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Noise-adaptive margin-based active learning for multi-dimensional data and lower bounds under tsybakov noise", "author": ["Y. Wang", "A. Singh"], "venue": "Proc. AAAI Conference on Artificial Intellgence, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix Computations", "author": ["G. Golub", "C.V. Loan"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Robust principal component analysis?", "author": ["E.J. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Linearized alternating direction method with adaptive penalty for low-rank representation", "author": ["Z. Lin", "M. Chen", "L. Wu", "Y. Ma"], "venue": "Advances in Neural Information Processing Systems, 2011. 16", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalable sparse subspace clustering by orthogonal matching pursuit", "author": ["C. You", "D.P. Robinson", "R. Vidal"], "venue": "Proc. IEEE International Conference on Computer Vision and Pattern Recognition, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Oracle based active set algorithm for scalable elastic net subspace clustering", "author": ["C. You", "C.-G. Li", "D.P. Robinson", "R. Vidal"], "venue": "Proc. IEEE International Conference on Computer Vision and Pattern Recognition, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Online low-rank subspace clustering by basis dictionary pursuit", "author": ["J. Shen", "P. Li", "H. Xu"], "venue": "Proc. International Conference on Machine Learning, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Shape and motion from image streams under orthography", "author": ["C. Tomasi", "T. Kanade"], "venue": "Int\u2019l J. Computer Vision, vol. 9, no. 2, pp. 137\u2013154, 1992.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1992}, {"title": "Low-rank matrix and tensor completion via adaptive sampling", "author": ["A. Krishnamurthy", "A. Singh"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 836\u2013844.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast approximate text document clustering using compressive sampling", "author": ["L.A. Park"], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2011, pp. 565\u2013580.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Compressive clustering of high-dimensional data", "author": ["A. Ruta", "F. Porikli"], "venue": "Machine Learning and Applications (ICMLA), 2012 11th International Conference on, vol. 1. IEEE, 2012, pp. 380\u2013385.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Performance limits of compressive sensing-based signal classification", "author": ["T. Wimalajeewa", "H. Chen", "P.K. Varshney"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 6, pp. 2758\u20132770, 2012.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "A deterministic analysis of noisy sparse subspace clustering for dimensionality-reduced data", "author": ["Y. Wang", "Y.-X. Wang", "A. Singh"], "venue": "Proc. of Int. Conf. on Machine Learning (ICML), 2015, pp. 1422\u20131431.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Necessary and sufficient conditions for sketched subspace clustering", "author": ["D. Pimentel-Alarcon", "L. Balzano", "R. Nowak"], "venue": "2016, submitted to Allerton.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P. Martinsson", "J. Tropp"], "venue": "SIAM Review, vol. 53, no. 2, pp. 217\u2013288, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "RandNLA: Randomized numerical linear algebra", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "Communications of the ACM, vol. 59, no. 6, pp. 80\u201390, 2016.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "A Course in High Dimensional Probability, 2016", "author": ["R. Vershynin"], "venue": "[Online]. Available: www-personal.umich.edu/\u223cromanv/teaching/2015-16/626/HDP-book.pdf", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction The union of subspaces (UoS) model, in which data vectors lie near one of several subspaces, has been used actively in the computer vision community on datasets ranging from images of objects under various lighting conditions [1] to visual surveillance tasks [2].", "startOffset": 241, "endOffset": 244}, {"referenceID": 1, "context": "1 Introduction The union of subspaces (UoS) model, in which data vectors lie near one of several subspaces, has been used actively in the computer vision community on datasets ranging from images of objects under various lighting conditions [1] to visual surveillance tasks [2].", "startOffset": 274, "endOffset": 277}, {"referenceID": 2, "context": "These algorithms achieve excellent clustering performance on datasets such as the extended Yale Face Database B [4], Hopkins 155 [5], and the MNIST handwritten digit database [6].", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "These algorithms achieve excellent clustering performance on datasets such as the extended Yale Face Database B [4], Hopkins 155 [5], and the MNIST handwritten digit database [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "These algorithms achieve excellent clustering performance on datasets such as the extended Yale Face Database B [4], Hopkins 155 [5], and the MNIST handwritten digit database [6].", "startOffset": 175, "endOffset": 178}, {"referenceID": 5, "context": "In [7], the authors investigate the phenomenon that incorporating poorly-chosen constraints can lead to an increase in clustering error, rather than a decrease as expected, since points constrained to be in the same cluster that are otherwise dissimilar can confound the constrained clustering algorithm.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "Active methods such as [8] have been shown to significantly reduce clustering error with a modest number of pairwise constraints, and in [9] the authors receive constraints from people with no special training via Amazon Mechanical Turk.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "Active methods such as [8] have been shown to significantly reduce clustering error with a modest number of pairwise constraints, and in [9] the authors receive constraints from people with no special training via Amazon Mechanical Turk.", "startOffset": 137, "endOffset": 140}, {"referenceID": 8, "context": "We present a novel PCC algorithm that utilizes the subspace estimates provided by algorithms such as Sparse Subspace Clustering (SSC) [10] to reduce clustering error as quickly as possible.", "startOffset": 134, "endOffset": 138}, {"referenceID": 9, "context": "Structure within and between data clusters is often leveraged for unsupervised clustering [11], and that structure is also leveraged for adaptive sampling of the structured signals themselves [12, 13, 14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "Structure within and between data clusters is often leveraged for unsupervised clustering [11], and that structure is also leveraged for adaptive sampling of the structured signals themselves [12, 13, 14].", "startOffset": 192, "endOffset": 204}, {"referenceID": 11, "context": "Structure within and between data clusters is often leveraged for unsupervised clustering [11], and that structure is also leveraged for adaptive sampling of the structured signals themselves [12, 13, 14].", "startOffset": 192, "endOffset": 204}, {"referenceID": 12, "context": "Structure within and between data clusters is often leveraged for unsupervised clustering [11], and that structure is also leveraged for adaptive sampling of the structured signals themselves [12, 13, 14].", "startOffset": 192, "endOffset": 204}, {"referenceID": 13, "context": "2 Related Work A survey of recently developed subspace clustering algorithms can be found in [15] and the textbook [3].", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "Notable examples of such algorithms include Sparse Subspace Clustering (SSC) [10], Thresholded Subspace Clustering (TSC) [16], Spectral Local Best-Fit Flats (SLBF) [17], and Greedy Subspace Clustering (GSC) [18].", "startOffset": 77, "endOffset": 81}, {"referenceID": 14, "context": "Notable examples of such algorithms include Sparse Subspace Clustering (SSC) [10], Thresholded Subspace Clustering (TSC) [16], Spectral Local Best-Fit Flats (SLBF) [17], and Greedy Subspace Clustering (GSC) [18].", "startOffset": 121, "endOffset": 125}, {"referenceID": 15, "context": "Notable examples of such algorithms include Sparse Subspace Clustering (SSC) [10], Thresholded Subspace Clustering (TSC) [16], Spectral Local Best-Fit Flats (SLBF) [17], and Greedy Subspace Clustering (GSC) [18].", "startOffset": 164, "endOffset": 168}, {"referenceID": 16, "context": "Notable examples of such algorithms include Sparse Subspace Clustering (SSC) [10], Thresholded Subspace Clustering (TSC) [16], Spectral Local Best-Fit Flats (SLBF) [17], and Greedy Subspace Clustering (GSC) [18].", "startOffset": 207, "endOffset": 211}, {"referenceID": 8, "context": "Of these methods, SSC achieves the best overall performance on benchmark datasets and has the strongest theoretical guarantees, which were introduced in [10] and strengthened in numerous recent works [19, 20, 21, 22].", "startOffset": 153, "endOffset": 157}, {"referenceID": 17, "context": "Of these methods, SSC achieves the best overall performance on benchmark datasets and has the strongest theoretical guarantees, which were introduced in [10] and strengthened in numerous recent works [19, 20, 21, 22].", "startOffset": 200, "endOffset": 216}, {"referenceID": 18, "context": "Of these methods, SSC achieves the best overall performance on benchmark datasets and has the strongest theoretical guarantees, which were introduced in [10] and strengthened in numerous recent works [19, 20, 21, 22].", "startOffset": 200, "endOffset": 216}, {"referenceID": 19, "context": "Of these methods, SSC achieves the best overall performance on benchmark datasets and has the strongest theoretical guarantees, which were introduced in [10] and strengthened in numerous recent works [19, 20, 21, 22].", "startOffset": 200, "endOffset": 216}, {"referenceID": 20, "context": "Of these methods, SSC achieves the best overall performance on benchmark datasets and has the strongest theoretical guarantees, which were introduced in [10] and strengthened in numerous recent works [19, 20, 21, 22].", "startOffset": 200, "endOffset": 216}, {"referenceID": 7, "context": "For example, in [9], the authors perform experiments where comparisons between human faces are provided by users of Amazon Mechanical Turk with an error rate of 1.", "startOffset": 16, "endOffset": 19}, {"referenceID": 21, "context": "Similarly, for subspace clustering datasets such as Yale B and MNIST, a human could easily answer questions such as, \u201cAre these two faces the same person?\u201d and \u201cAre these two images the same number?\u201d An early example of PCC is found in [23], where the authors modify the K-means cost function to incorporate such constraints.", "startOffset": 236, "endOffset": 240}, {"referenceID": 22, "context": "In [24], the authors utilize active methods to initialize K-means in an intelligent Explore phase, during which neighborhoods of must-linked points are built up.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "similar explore phase is used in [25], after which a min-max approach is used to select the most uncertain sample.", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "Early work on constrained spectral clustering appears in [26, 27], in which spectral clustering is improved by examining the eigenvectors of the affinity matrix in order to determine the most informative points.", "startOffset": 57, "endOffset": 65}, {"referenceID": 25, "context": "Early work on constrained spectral clustering appears in [26, 27], in which spectral clustering is improved by examining the eigenvectors of the affinity matrix in order to determine the most informative points.", "startOffset": 57, "endOffset": 65}, {"referenceID": 6, "context": "More recently, the authors in [8, 9] improve constrained clustering by modeling which points will be most informative given the current clustering, with state-of-the-art results achieved on numerous datasets by the algorithm in [8] referred to as Uncertainty Reducing Active Spectral Clustering (URASC).", "startOffset": 30, "endOffset": 36}, {"referenceID": 7, "context": "More recently, the authors in [8, 9] improve constrained clustering by modeling which points will be most informative given the current clustering, with state-of-the-art results achieved on numerous datasets by the algorithm in [8] referred to as Uncertainty Reducing Active Spectral Clustering (URASC).", "startOffset": 30, "endOffset": 36}, {"referenceID": 6, "context": "More recently, the authors in [8, 9] improve constrained clustering by modeling which points will be most informative given the current clustering, with state-of-the-art results achieved on numerous datasets by the algorithm in [8] referred to as Uncertainty Reducing Active Spectral Clustering (URASC).", "startOffset": 228, "endOffset": 231}, {"referenceID": 22, "context": "In practice [28], the certain sets are initialized using the Explore algorithm of [24].", "startOffset": 82, "endOffset": 86}, {"referenceID": 26, "context": "To the best of our knowledge, the only such algorithm to do so is that of [29].", "startOffset": 74, "endOffset": 78}, {"referenceID": 26, "context": "The algorithm described in [29] has two major drawbacks.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "First, the authors use a modified version of the K-subspaces algorithm, which exhibits poor performance on most benchmark datasets (see for example [15]).", "startOffset": 148, "endOffset": 152}, {"referenceID": 6, "context": "We follow the framework laid out in [8] and provide an algorithm for improving the performance of subspace clustering that achieves significant benefits over the current state-of-the-art in PCC.", "startOffset": 36, "endOffset": 39}, {"referenceID": 22, "context": "To initialize, we build a set of certain sets Z using an Explore-like algorithm similar to that of [24].", "startOffset": 99, "endOffset": 103}, {"referenceID": 26, "context": "Next, a test point is obtained using either the min-margin criterion for subspaces [29] or a notion of margin based on the affinity matrix, which we define below.", "startOffset": 83, "endOffset": 87}, {"referenceID": 22, "context": "The use of certain sets relies on the assumption that the pairwise queries are answered correctly\u2014an assumption that is common in the literature [24, 25, 8].", "startOffset": 145, "endOffset": 156}, {"referenceID": 23, "context": "The use of certain sets relies on the assumption that the pairwise queries are answered correctly\u2014an assumption that is common in the literature [24, 25, 8].", "startOffset": 145, "endOffset": 156}, {"referenceID": 6, "context": "The use of certain sets relies on the assumption that the pairwise queries are answered correctly\u2014an assumption that is common in the literature [24, 25, 8].", "startOffset": 145, "endOffset": 156}, {"referenceID": 6, "context": "However, in [8], the authors demonstrate that an algorithm based on certain sets still yields significant improvements under a small error rate.", "startOffset": 12, "endOffset": 15}, {"referenceID": 27, "context": "In [30], the author notes that actively querying points of minimum margin (as opposed to maximum entropy or minimum confidence) is an appropriate choice for reducing classification error.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "In [31], the authors present a margin-based binary classification algorithm that achieves an optimal rate of convergence (within a logarithmic factor).", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "1 Residual-Based Margin The concept of margin for subspaces was first studied in [29].", "startOffset": 81, "endOffset": 85}, {"referenceID": 26, "context": "Then the subspace margin of a point x \u2208 X is defined as [29] \u03bc1(x) = max j 6=k,j\u2208[K] dist(x, Sk\u2217) dist(x, Sj) .", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "Note that the fraction is a value in [0, 1], where the larger \u03bc1(x) the closer x is to the decision boundary.", "startOffset": 37, "endOffset": 43}, {"referenceID": 10, "context": "This method of point selection is then motivated by the fact that the difficult points to cluster are those lying near the intersection of subspaces [12].", "startOffset": 149, "endOffset": 153}, {"referenceID": 9, "context": "Further, theory for SSC ([11],[15]) shows that problematic points are those having large inner product with some or all directions in other subspaces.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "Further, theory for SSC ([11],[15]) shows that problematic points are those having large inner product with some or all directions in other subspaces.", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "See [32] for a definition of principal angles.", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "2 Affinity-Based Margin We also consider a version of margin calculated from the entries of the affinity matrix itself, which is inspired by the nonparametric entropy estimation in [8].", "startOffset": 181, "endOffset": 184}, {"referenceID": 22, "context": "SUPERPAC can be thought of as an extension of ideas from PCC literature [24, 9, 8] to leverage prior knowledge about the underlying geometry of the data.", "startOffset": 72, "endOffset": 82}, {"referenceID": 7, "context": "SUPERPAC can be thought of as an extension of ideas from PCC literature [24, 9, 8] to leverage prior knowledge about the underlying geometry of the data.", "startOffset": 72, "endOffset": 82}, {"referenceID": 6, "context": "SUPERPAC can be thought of as an extension of ideas from PCC literature [24, 9, 8] to leverage prior knowledge about the underlying geometry of the data.", "startOffset": 72, "endOffset": 82}, {"referenceID": 6, "context": "The reader will note we made a choice to order the certain sets according to the UoS model; this is similar to the choice in [8] to query according to similarity, where our notion of similarity here is based on subspace distances, rather than some other metric.", "startOffset": 125, "endOffset": 128}, {"referenceID": 7, "context": "In contrast to [9, 8], where the test point is chosen according to a global improvement metric, we choose test points according to their classification margin.", "startOffset": 15, "endOffset": 21}, {"referenceID": 6, "context": "In contrast to [9, 8], where the test point is chosen according to a global improvement metric, we choose test points according to their classification margin.", "startOffset": 15, "endOffset": 21}, {"referenceID": 9, "context": "In our experiments, we explored notions of margin based on the l1 regression coefficients from SSC, similar to the Sparsity Concentration Index of [11].", "startOffset": 147, "endOffset": 151}, {"referenceID": 22, "context": "For this reason, the Explore algorithm from [24] is unlikely to quickly find points from different clusters in an efficient manner.", "startOffset": 44, "endOffset": 48}, {"referenceID": 7, "context": "We also compared with the algorithm from [9] but found this algorithm performed worse than URASC with a far greater computational cost.", "startOffset": 41, "endOffset": 44}, {"referenceID": 30, "context": "In our experiments, we also compared with oracle robust PCA [33] implemented via the augmented Lagrange multiplier method [34] but did not find any improvement in classification error.", "startOffset": 60, "endOffset": 64}, {"referenceID": 31, "context": "In our experiments, we also compared with oracle robust PCA [33] implemented via the augmented Lagrange multiplier method [34] but did not find any improvement in classification error.", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "We follow the methodology of [17] and perform clustering on 100 randomly selected subsets of size K.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "We use the output of SSC [10] as our initial affinity matrix, as SSC is known to have the best performance on this set, and choose d = 9 as is common in the literature [10, 16].", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "We use the output of SSC [10] as our initial affinity matrix, as SSC is known to have the best performance on this set, and choose d = 9 as is common in the literature [10, 16].", "startOffset": 168, "endOffset": 176}, {"referenceID": 14, "context": "We use the output of SSC [10] as our initial affinity matrix, as SSC is known to have the best performance on this set, and choose d = 9 as is common in the literature [10, 16].", "startOffset": 168, "endOffset": 176}, {"referenceID": 8, "context": "The validity of the UoS assumption for these datasets is investigated in [10, 16].", "startOffset": 73, "endOffset": 81}, {"referenceID": 14, "context": "The validity of the UoS assumption for these datasets is investigated in [10, 16].", "startOffset": 73, "endOffset": 81}, {"referenceID": 22, "context": "22 (45/153) Explore [24] 4.", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "This demonstrates the significant improvement over the results in [29], where 3Kd labels are required to surpass oracle performance; providing oracle labels (in this case, stating who a person is) requires far more expertise than pairwise comparisons (answering whether two images are of the same person).", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "Next, we show the effectiveness of the UoS-Explore algorithm over Explore used in [24, 8].", "startOffset": 82, "endOffset": 89}, {"referenceID": 6, "context": "Next, we show the effectiveness of the UoS-Explore algorithm over Explore used in [24, 8].", "startOffset": 82, "endOffset": 89}, {"referenceID": 15, "context": "In previous papers [17, 16], only sets of K = 2 and 3 digits were considered, and then only specific digits.", "startOffset": 19, "endOffset": 27}, {"referenceID": 14, "context": "In previous papers [17, 16], only sets of K = 2 and 3 digits were considered, and then only specific digits.", "startOffset": 19, "endOffset": 27}, {"referenceID": 32, "context": "Higher values of K have been considered more recently [35, 36, 37] and will be incorporated into our future work.", "startOffset": 54, "endOffset": 66}, {"referenceID": 33, "context": "Higher values of K have been considered more recently [35, 36, 37] and will be incorporated into our future work.", "startOffset": 54, "endOffset": 66}, {"referenceID": 34, "context": "Higher values of K have been considered more recently [35, 36, 37] and will be incorporated into our future work.", "startOffset": 54, "endOffset": 66}, {"referenceID": 14, "context": "methodology to the previous section and select 100 random subsets of size K, using subspace dimension d = 3 as in [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "To demonstrate that our algorithm is agnostic to the input clustering method, we use TSC [16] to obtain the input affinity matrix.", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "We use Nk = 100 points in each subspace, the regime in which TSC has been shown to outperform SSC on this dataset [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "Although not pictured, we found in our experiments that [29] again requires as many labels as SUPERPAC requires pairwise comparisons.", "startOffset": 56, "endOffset": 60}, {"referenceID": 32, "context": "During the preparation of this manuscript, we became aware of [35, 36, 37], where more scalable algorithms for subspace clustering have been developed.", "startOffset": 62, "endOffset": 74}, {"referenceID": 33, "context": "During the preparation of this manuscript, we became aware of [35, 36, 37], where more scalable algorithms for subspace clustering have been developed.", "startOffset": 62, "endOffset": 74}, {"referenceID": 34, "context": "During the preparation of this manuscript, we became aware of [35, 36, 37], where more scalable algorithms for subspace clustering have been developed.", "startOffset": 62, "endOffset": 74}, {"referenceID": 35, "context": "These points have been shown to lie in a union of affine subspaces of dimension at most 3 [38].", "startOffset": 90, "endOffset": 94}, {"referenceID": 6, "context": "We first compare the performance of URASC and both random algorithms on the Sonar, Balance, and Leaf-250 datasets, all of which are used in [8] as benchmarks.", "startOffset": 140, "endOffset": 143}, {"referenceID": 10, "context": ", see previous work on sparse [12, 13], structured sparse [14], and low rank signals [40]).", "startOffset": 30, "endOffset": 38}, {"referenceID": 11, "context": ", see previous work on sparse [12, 13], structured sparse [14], and low rank signals [40]).", "startOffset": 30, "endOffset": 38}, {"referenceID": 12, "context": ", see previous work on sparse [12, 13], structured sparse [14], and low rank signals [40]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 36, "context": ", see previous work on sparse [12, 13], structured sparse [14], and low rank signals [40]).", "startOffset": 85, "endOffset": 89}, {"referenceID": 37, "context": "Several works apply ideas of compressive sensing to clustering [41, 42] and", "startOffset": 63, "endOffset": 71}, {"referenceID": 38, "context": "Several works apply ideas of compressive sensing to clustering [41, 42] and", "startOffset": 63, "endOffset": 71}, {"referenceID": 9, "context": "classification [11, 43], and recent work in subspace clustering has also shown that it\u2019s possible to cluster columns that lie in a union of subspaces even using compressed or subsampled data [44, 45, 46, 47].", "startOffset": 15, "endOffset": 23}, {"referenceID": 39, "context": "classification [11, 43], and recent work in subspace clustering has also shown that it\u2019s possible to cluster columns that lie in a union of subspaces even using compressed or subsampled data [44, 45, 46, 47].", "startOffset": 15, "endOffset": 23}, {"referenceID": 40, "context": "classification [11, 43], and recent work in subspace clustering has also shown that it\u2019s possible to cluster columns that lie in a union of subspaces even using compressed or subsampled data [44, 45, 46, 47].", "startOffset": 191, "endOffset": 207}, {"referenceID": 41, "context": "classification [11, 43], and recent work in subspace clustering has also shown that it\u2019s possible to cluster columns that lie in a union of subspaces even using compressed or subsampled data [44, 45, 46, 47].", "startOffset": 191, "endOffset": 207}, {"referenceID": 42, "context": "classification [11, 43], and recent work in subspace clustering has also shown that it\u2019s possible to cluster columns that lie in a union of subspaces even using compressed or subsampled data [44, 45, 46, 47].", "startOffset": 191, "endOffset": 207}, {"referenceID": 43, "context": "classification [11, 43], and recent work in subspace clustering has also shown that it\u2019s possible to cluster columns that lie in a union of subspaces even using compressed or subsampled data [44, 45, 46, 47].", "startOffset": 191, "endOffset": 207}, {"referenceID": 44, "context": "1 from [48], restated below.", "startOffset": 7, "endOffset": 11}, {"referenceID": 44, "context": "5 of [48], we can replace E \u2016X\u20162 by", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "Further we note that, by the definition of principal angles [32],", "startOffset": 60, "endOffset": 64}], "year": 2016, "abstractText": "Many clustering problems in computer vision and other contexts are also classification problems, where each cluster shares a meaningful label. Subspace clustering algorithms in particular are often applied to problems that fit this description, for example with face images or handwritten digits. While it is straightforward to request human input on these datasets, our goal is to reduce this input as much as possible. We present an algorithm for active query selection that allows us to leverage the union of subspace structure assumed in subspace clustering. The central step of the algorithm is in querying points of minimum margin between estimated subspaces; analogous to classifier margin, these lie near the decision boundary. This procedure can be used after any subspace clustering algorithm that outputs an affinity matrix and is capable of driving the clustering error down more quickly than other state-ofthe-art active query algorithms on datasets with subspace structure. We demonstrate the effectiveness of our algorithm on several benchmark datasets, and with a modest number of queries we see significant gains in clustering performance.", "creator": "LaTeX with hyperref package"}}}