{"id": "1608.02214", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Aug-2016", "title": "Robsut Wrod Reocginiton via Semi-Character Recurrent Neural Network", "abstract": "The Cmabrigde Uinervtisy (Cambridge University) effect from the psycholinguistics literature has demonstrated a robust word processing mechanism in humans, where jumbled words (e.g. Cmabrigde / Cambridge) are recognized with little cost. Inspired by the findings from the Cmabrigde Uinervtisy effect, we propose a word recognition model based on a semi-character level recursive neural network (scRNN). In our experiments, we demonstrate that scRNN has significantly more robust performance in word spelling correction (i.e. word recognition) compared to existing spelling checkers. Furthermore, we demonstrate that the model is cognitively plausible by replicating a psycholinguistics experiment about human reading difficulty using our model.", "histories": [["v1", "Sun, 7 Aug 2016 13:28:46 GMT  (58kb,D)", "http://arxiv.org/abs/1608.02214v1", null], ["v2", "Tue, 7 Feb 2017 07:56:39 GMT  (103kb,D)", "http://arxiv.org/abs/1608.02214v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["keisuke sakaguchi", "kevin duh", "matt post", "benjamin van durme"], "accepted": true, "id": "1608.02214"}, "pdf": {"name": "1608.02214.pdf", "metadata": {"source": "CRF", "title": "Robsut Wrod Reocginiton via semi-Character Recurrent Neural Network", "authors": ["Keisuke Sakaguchi", "Kevin Duh", "Matt Post", "Benjamin Van Durme"], "emails": ["keisuke@cs.jhu.edu", "kevinduh@cs.jhu.edu", "post@cs.jhu.edu", "vandurme@cs.jhu.edu"], "sections": [{"heading": null, "text": "Inspired by the results of the Cmabrigde Uinervtisy effect, we propose a word recognition model based on a semi-character recursive neural network (scRNN). In our experiments, we show that scRNN performs significantly more robust spell-checking (i.e., word recognition) than existing spell-checkers. In addition, we show that the model is cognitively plausible by replicating a psycholinguistic experiment on human reading difficulties with our model."}, {"heading": "1 Introduction", "text": "This year, the number of new registrations in Germany rose by 0.2 percent compared to the previous year."}, {"heading": "2 Raeding Wrods with Jumbled Lettres", "text": "Forster et al. (1987) show that a swirling word (e.g. anwser-ANSWER) enables identity-size primates (answer-ANSWER) in a paradigm of masked primates and these results have been confirmed (Perea and Lupker, 2004; Guerrera and Forster, 2008) These results on robust human word-processing mechanisms have been further investigated by looking at other types of noise in addition to simple letter transpositions. Humphreys et al. (1990) show that deleting a letter in a word still produces a significant primation effect (e.g. blckBLACK), and similar results have been shown in other research (Peressotti and Grainger, 1999; Grainger et al al al al al al, 2006). Van Assche et Grainger et al jjinger (2006) show that a priming effect when inserting a letter remains a JEye-Eye word (e.g. Ejuaye movement)."}, {"heading": "3 Semi-Character Recurrent Neural Net", "text": "To achieve the human-like robust word-processing mechanism, we propose a semicharacter base based on a recursive neural network (scRNN), which has the same structure as a standard recursive neural network, except that the input vector consists of three sub-vectors corresponding to the position of the characters; the first and third sub-vectors (bn, en) represent the first and last characters of the n-th word, so these two subvectors are the most uniform representations; the second sub-vector represents a bag of characters of the word that do not include the beginning and end positions; for example, the word \"university\" is represented as bn = {U = 1}, en = {y = 1}, and in = {e = 2, n = 1, s = 1, r = 1, t = 1, v = 1}, with all other elements being null."}, {"heading": "4 Experiments", "text": "The input layer consists of a 76-character vector (A-Z, a-z, and 24 symbols), the hidden layer units were 650 in size, and the total vocabulary size was set to 10k. We apply some kind of noise to each word, except that all words with numbers (e.g. 1980s) and short words (length \u2264 3) are not messed up and left as they are, and that these words are excluded from evaluation. We trained the model by going through 5 stack size 20 eras. To make the training efficient, we set the backpropagation parameter over time (BPTT) to 3."}, {"heading": "4.1 Spelling correction results", "text": "We tested different types of noise: jumble, delete, and insert, where the jumble changes the internal characters (e.g. Cambridge \u2192 Cmbarigde), randomly deletes one of the internal characters (Cambridge \u2192 Camridge), and randomly inserts an alphabet into an internal position (Cambridge \u2192 Cambpridge). None of the noise types changes the first and last characters. For comparison, we performed two widely used spell corrections (Commercial3 and Hunspell4). Other spell corrections. The only error in scRNN may be that the last character (Checkearch) strongly enabled scRNN nodes toward research, rather than research.6Table 3 shows the result in terms of the noise type. Overall, scRNN outperforms the other two spell corrections in all three different types of noise. It is striking that scRNN has a robustness in the jumble noise, while the other 35% suffers from relatively large word corrections in all three models."}, {"heading": "4.2 Corroboration with psycholinguistic experiments", "text": "As shown in \u00a7 2, the position of the transposition affects the cognitive load of human word recognition. We study this phenomenon with scRNN by manipulating the structure of the input vector. We replicate the experimental paradigm in Rayner et al. (2006), but use scRNNs instead of human sub-6There is also a deletion of \"r.\" Objects. We trained scRNN on various confused conditions: INT, END and BEG. INT is the same model as \u00a7 3, END represents an input word as a concatenation of the initial letter vector (b) and a vector for the rest of the characters (i.e. the internal and last characters are subject to confusion), and BEG combines a vector for the final character (s) and a vector for the rest of the characters (i.e. the initial and internal characters are subject to confusion)."}, {"heading": "5 Related Work", "text": "Character-based recurrent neural networks were studied and used for a variety of NLP tasks such as speech modeling (Sutskever et al., 2011), segmentation (Chrupala, 2013), dependency parsing (Ballesteros et al., 2015), machine translation (Ling et al., 2015), and text normalization (Chrupa\u0142a, 2014).Although scRNN has some similarities in terms of model architecture with these recent work, our contribution is a demonstration of the robustness and cognitive plausibility of semisign-based recursive neural networks for word recognition. Character-level Convolutionary neural Networks (CNN) have also been identified (Kim et al., 2015) and used for spell correction (Schmaltz et al., 2016). While CNNs exhibit a richer representation, scRNN still achieves high accuracy in confused word identification with a simple NN structure, which is not required."}, {"heading": "6 Summary", "text": "We presented a semi-character recurrent neural network model, scRNN, inspired by the robust word recognition mechanism known in psycholinguistic literature as the cmabrigde uinervtisy effect. Despite the simplicity of the model, it clearly outperforms existing spell checkers in terms of different types of noise. We also demonstrated a similarity between scRNN and human word recognition mechanisms by showing that scRNN replicates a psycholinguistic experiment on the difficulty of word recognition in terms of the position of confused people. There are a variety of potential NLP applications for scRNN where robustness plays an important role, such as the normalization of social media texts (e.g. Cooooolll \u2192 Cool) and the modelling of morphologically rich languages."}], "references": [{"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349\u2013", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Text segmentation with character-level text embeddings", "author": ["Grzegorz Chrupala."], "venue": "arXiv preprint arXiv:1309.4628.", "citeRegEx": "Chrupala.,? 2013", "shortCiteRegEx": "Chrupala.", "year": 2013}, {"title": "Normalizing tweets with edit scripts and recurrent neural embeddings", "author": ["Grzegorz Chrupa\u0142a."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 680\u2013686, Baltimore, Mary-", "citeRegEx": "Chrupa\u0142a.,? 2014", "shortCiteRegEx": "Chrupa\u0142a.", "year": 2014}, {"title": "Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy", "author": ["Matt Davis."], "venue": "http://www.mrccbu.cam.ac.uk/people/matt.davis/cmabridge/.", "citeRegEx": "Davis.,? 2003", "shortCiteRegEx": "Davis.", "year": 2003}, {"title": "Masked priming with graphemically related forms: Repetition or partial activation", "author": ["Kenneth I Forster", "C Davis", "C Schoknecht", "R Carter"], "venue": null, "citeRegEx": "Forster et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Forster et al\\.", "year": 1987}, {"title": "Letter position information and printed word perception: the relative-position priming constraint", "author": ["Jonathan Grainger", "Jean-Pierre Granier", "Fernand Farioli", "Eva Van Assche", "Walter JB van Heuven."], "venue": "Journal of Experimental Psychology: Human Per-", "citeRegEx": "Grainger et al\\.,? 2006", "shortCiteRegEx": "Grainger et al\\.", "year": 2006}, {"title": "Masked form priming with extreme transposition", "author": ["Christine Guerrera", "Kenneth Forster."], "venue": "Language and Cognitive Processes, 23(1):117\u2013142.", "citeRegEx": "Guerrera and Forster.,? 2008", "shortCiteRegEx": "Guerrera and Forster.", "year": 2008}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Orthographic processing in visual word identification", "author": ["Glyn W Humphreys", "Lindsay J Evett", "Philip T Quinlan."], "venue": "Cognitive Psychology, 22(4):517 \u2013 560.", "citeRegEx": "Humphreys et al\\.,? 1990", "shortCiteRegEx": "Humphreys et al\\.", "year": 1990}, {"title": "Transposed-letter effects in reading: Evidence from eye movements and parafoveal preview", "author": ["Rebecca L Johnson", "Manuel Perea", "Keith Rayner."], "venue": "Journal of Experimental Psychology: Human Perception and Performance, 33(1):209.", "citeRegEx": "Johnson et al\\.,? 2007", "shortCiteRegEx": "Johnson et al\\.", "year": 2007}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "arXiv preprint arXiv:1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black."], "venue": "arXiv preprint arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Can CANISO activate casino? transposed-letter similarity effects with nonadjacent letter positions", "author": ["Manuel Perea", "Stephen J Lupker."], "venue": "Journal of Memory and Language, 51(2):231 \u2013 246.", "citeRegEx": "Perea and Lupker.,? 2004", "shortCiteRegEx": "Perea and Lupker.", "year": 2004}, {"title": "The role of letter identity and letter position in orthographic priming", "author": ["Francesca Peressotti", "Jonathan Grainger."], "venue": "Perception & Psychophysics, 61(4):691\u2013706.", "citeRegEx": "Peressotti and Grainger.,? 1999", "shortCiteRegEx": "Peressotti and Grainger.", "year": 1999}, {"title": "Raeding wrods with jubmled lettres: There is a cost", "author": ["Keith Rayner", "Sarah J. White", "Rebecca L. Johnson", "Simon P. Liversedge."], "venue": "Psychological Science, 17(3):192\u2013193.", "citeRegEx": "Rayner et al\\.,? 2006", "shortCiteRegEx": "Rayner et al\\.", "year": 2006}, {"title": "Sentence-level grammatical error identification as sequence-to-sequence correction", "author": ["Allen Schmaltz", "Yoon Kim", "Alexander M. Rush", "Stuart Shieber."], "venue": "Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Appli-", "citeRegEx": "Schmaltz et al\\.,? 2016", "shortCiteRegEx": "Schmaltz et al\\.", "year": 2016}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "A study of relative-position priming with superset primes", "author": ["Eva Van Assche", "Jonathan Grainger."], "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition, 32(2):399.", "citeRegEx": "Assche and Grainger.,? 2006", "shortCiteRegEx": "Assche and Grainger.", "year": 2006}], "referenceMentions": [{"referenceID": 3, "context": "For example, the following sentences, introduced by a psycholinguist (Davis, 2003), provide a great demonstration of the robust word recognition mechanism in humans.", "startOffset": 69, "endOffset": 82}, {"referenceID": 7, "context": "The model is based on a standard recurrent neural network with a memory cell as in LSTM (Hochreiter and Schmidhuber, 1997).", "startOffset": 88, "endOffset": 122}, {"referenceID": 14, "context": "Table 1: Example sentences and results for measures of fixation (excerpt from (Rayner et al., 2006)).", "startOffset": 78, "endOffset": 99}, {"referenceID": 12, "context": "anwser-ANSWER) facilitates primes as large as identity primes (answer-ANSWER) in a masked priming paradigm and these results have been confirmed (Perea and Lupker, 2004; Guerrera and Forster, 2008).", "startOffset": 145, "endOffset": 197}, {"referenceID": 6, "context": "anwser-ANSWER) facilitates primes as large as identity primes (answer-ANSWER) in a masked priming paradigm and these results have been confirmed (Perea and Lupker, 2004; Guerrera and Forster, 2008).", "startOffset": 145, "endOffset": 197}, {"referenceID": 4, "context": "Forster et al. (1987) show that a jumbled word (e.", "startOffset": 0, "endOffset": 22}, {"referenceID": 13, "context": "blckBLACK), and similar results have been shown in other research (Peressotti and Grainger, 1999; Grainger et al., 2006).", "startOffset": 66, "endOffset": 120}, {"referenceID": 5, "context": "blckBLACK), and similar results have been shown in other research (Peressotti and Grainger, 1999; Grainger et al., 2006).", "startOffset": 66, "endOffset": 120}, {"referenceID": 7, "context": "Humphreys et al. (1990) show that deleting a letter in a word still produces significant priming effect (e.", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": "blckBLACK), and similar results have been shown in other research (Peressotti and Grainger, 1999; Grainger et al., 2006). Van Assche and Grainger (2006) demonstrate that a priming effect remains", "startOffset": 98, "endOffset": 153}, {"referenceID": 13, "context": "With an eye-movement paradigm, Rayner et al. (2006) and Johnson et al.", "startOffset": 31, "endOffset": 52}, {"referenceID": 9, "context": "(2006) and Johnson et al. (2007) conduct more detailed experiments on the robust word recognition mechanism with jumbled letters.", "startOffset": 11, "endOffset": 33}, {"referenceID": 9, "context": "(2006) and Johnson et al. (2007) conduct more detailed experiments on the robust word recognition mechanism with jumbled letters. They show that letter transposition affects fixation time measures during reading depending on which part of the word is jumbled. Table 1 presents the result from Rayner et al. (2006). It is obvious that human can read smoothly (i.", "startOffset": 11, "endOffset": 314}, {"referenceID": 14, "context": "We replicate the experimental paradigm in Rayner et al. (2006), but using scRNNs rather than human sub-", "startOffset": 42, "endOffset": 63}, {"referenceID": 16, "context": "Character-based recurrent neural networks have been investigated and used for a variety of NLP tasks such as language modeling (Sutskever et al., 2011), segmentation (Chrupala, 2013), dependency parsing (Ballesteros et al.", "startOffset": 127, "endOffset": 151}, {"referenceID": 1, "context": ", 2011), segmentation (Chrupala, 2013), dependency parsing (Ballesteros et al.", "startOffset": 22, "endOffset": 38}, {"referenceID": 0, "context": ", 2011), segmentation (Chrupala, 2013), dependency parsing (Ballesteros et al., 2015), machine translation (Ling et al.", "startOffset": 59, "endOffset": 85}, {"referenceID": 11, "context": ", 2015), machine translation (Ling et al., 2015), and text normalization (Chrupa\u0142a, 2014).", "startOffset": 29, "endOffset": 48}, {"referenceID": 2, "context": ", 2015), and text normalization (Chrupa\u0142a, 2014).", "startOffset": 32, "endOffset": 48}, {"referenceID": 10, "context": "Character-level convolutional neural nets (CNN) have also been noted (Kim et al., 2015) and used for spelling correction (Schmaltz et al.", "startOffset": 69, "endOffset": 87}], "year": 2017, "abstractText": "The Cmabrigde Uinervtisy (Cambridge University) effect from the psycholinguistics literature has demonstrated a robust word processing mechanism in humans, where jumbled words (e.g. Cmabrigde / Cambridge) are recognized with little cost. Inspired by the findings from the Cmabrigde Uinervtisy effect, we propose a word recognition model based on a semi-character level recursive neural network (scRNN). In our experiments, we demonstrate that scRNN has significantly more robust performance in word spelling correction (i.e. word recognition) compared to existing spelling checkers. Furthermore, we demonstrate that the model is cognitively plausible by replicating a psycholinguistics experiment about human reading difficulty using our model.", "creator": "TeX"}}}