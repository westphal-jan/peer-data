{"id": "1608.03000", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Aug-2016", "title": "Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge", "abstract": "This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.", "histories": [["v1", "Tue, 9 Aug 2016 23:05:03 GMT  (1418kb,D)", "http://arxiv.org/abs/1608.03000v1", "to be published in EMNLP 2016"]], "COMMENTS": "to be published in EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["nicholas locascio", "karthik narasimhan", "eduardo deleon", "nate kushman", "regina barzilay"], "accepted": true, "id": "1608.03000"}, "pdf": {"name": "1608.03000.pdf", "metadata": {"source": "CRF", "title": "Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge", "authors": ["Nicholas Locascio", "Karthik Narasimhan", "Eduardo DeLeon", "Regina Barzilay"], "emails": ["njl@mit.edu", "karthikn@mit.edu", "edeleon04@mit.edu", "nate@kushman.org", "regina@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "This paper examines the task of translating natural text requests into regular expressions that embody their meaning. Regular expressions are built into many application interfaces, but most users of these applications have difficulty writing them (Friedl, 2002). Thus, a system for automatically generating regular expressions from natural language has been useful in many contexts. Furthermore, such technologies can ultimately be translated into other formal representations, such as program scripts (Raza et al., 2015).Prior's work has demonstrated the feasibility of this task. Kushman and Barzilay have proposed a model that performs the task of a parallel corpus of regular expressions and text descriptions. To explain the given representation disparity between formal regular expressions and natural language, their model uses a domain specification presented as a complementary material component."}, {"heading": "2 Related Work", "text": "Regular Expressions from Natural Language There have been several attempts to generate regular expressions from text descriptions, but our work is similar to that of Kushman and Barzilay (2013), who used rules-based techniques to create a natural language interface for writing regular expressions (Ranta, 1998). They learned a semantic translation model using a parallel dataset of natural language and regular expressions, and their model used a regular expression-specific semantic unification technique to decipher the meaning of natural language descriptions. Our method is similar in that we only need description and regex pairs to learn, but we treat the problem as a direct translation task without applying any domain-specific knowledge."}, {"heading": "3 Regex Generation as Translation", "text": "Our model is inspired by recent advances in the sequence of neural translation. We use a Recurrent Neural Network (RNN) with attention (Mnih et al., 2014) for both encoding and decoding (Figure 1).Let W = w1, w2... wm be the input text description where each wi is a word in the vocabulary. We want to generate the regex R = r1, r2,... rn where each ri is a character in the regex.We use LSTM (Hochreiter and Schmidhuber, 1997) cells in our model, the transition equations for which can be summarized as such: it = expanded (U (i) xt + V (i) ht \u2212 1 + b (i)), ft = conditioning (U (f) xt + V (f) ht \u2212 1).We are equal (t) xt + z function (consecrated)."}, {"heading": "4 Creating a Large Corpus of Natural Language / Regular Expression Pairs", "text": "The challenge in capturing such corpora is that typical crowdsourcing workers do not have the specialized knowledge to write regular expressions. To solve this, we use a two-step generation and paraphrasing process to collect our data, a technique similar to the methods used by Wang et al. (2015) to create a semantic analysis of corporations. In the generation step, we generate regular expressions from a small manually generated grammar (Table 1). Our grammar includes 15 nonterminal derivatives and 6 terminals, and both basic and high-grade operations, which we identify using frequency analyses of smaller datasets from previous work (Kushman and Barzilay, 2013)."}, {"heading": "5 Experiments", "text": "Datasets We divide the 10,000 regexp and description pairs in NL-RX into 65% train, 10% dev, and 25% test sets. In addition, we also evaluate our model using the dataset (KB13) used by Kushman and Barzilay (2013), although it contains far fewer data points (824). We use the 75 / 25 train / test split used in their work to directly compare our performance with theirs. Training We perform a hyper-parameter gridsearch (on the dev set) to determine our model hyper parameters: learning-rate = 1.0, encoderdepth = 2, decoderdepth = 2, batch size = 32, dropout = 0.25. We use a torch (Collobert et al., 2002) Implementation of the attention sequence to sequence networks (Kim, 2016)."}, {"heading": "6 Results", "text": "Our model significantly exceeds the baselines of the NL-RX dataset and achieves comparable performance with Semantic Unify of the KB13 dataset (Table 3). Despite the small size of KB13, our model achieves state-of-the-art results in this very resource-limited dataset (814 examples). Using NL-RX, we investigate the impact of the training data size on the accuracy of our model. Figure 3 shows how the performance of our model improves as the number of training examples increases. Differences in datasets Taking into account the previous section, a seemingly unusual finding is that the accuracy of the model is better for smaller2We in consultation with the original authors. Dataset KB13 than for the larger dataset NL-RXTurk. Further analysis has shown that the KB13 dataset expresses a much less diverse and complex dataset than NL-RX-Turk."}, {"heading": "7 Conclusions", "text": "In this paper, we show that generic neural architectures exceed custom, advanced models to generate regular expressions, and the results suggest that this technique can be used to address more difficult problems in larger families of formal languages, such as mapping between language descriptions and program scripts. We have also created a large parallel corpus of regular expressions and natural language queries with the help of typical crowd-sourced people, which we make publicly available."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Torch: A modular machine learning software", "author": ["Samy Bengio", "Johnny Marithoz"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2002}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Mastering regular expressions", "author": ["Jeffrey EF Friedl"], "venue": null, "citeRegEx": "Friedl.,? \\Q2002\\E", "shortCiteRegEx": "Friedl.", "year": 2002}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Seq2seq-attn. https:// github.com/harvardnlp/seq2seq-attn", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2016\\E", "shortCiteRegEx": "Kim.", "year": 2016}, {"title": "Using semantic unification to generate regular expressions from natural language. North American Chapter of the Association for Computational Linguistics (NAACL)", "author": ["Kushman", "Barzilay2013] Nate Kushman", "Regina Barzilay"], "venue": null, "citeRegEx": "Kushman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kushman et al\\.", "year": 2013}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Mnih et al.2014] Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "A multilingual naturallanguage interface to regular expressions", "author": ["Aarne Ranta"], "venue": "In Proceedings of the International Workshop on Finite State Methods in Natural Language Processing,", "citeRegEx": "Ranta.,? \\Q1998\\E", "shortCiteRegEx": "Ranta.", "year": 1998}, {"title": "Compositional program synthesis from natural language and examples", "author": ["Raza et al.2015] Mohammad Raza", "Sumit Gulwani", "Natasa Milic-Frayling"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "Raza et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Raza et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Building a semantic parser overnight. Association for Computational Linguistics (ACL)", "author": ["Wang et al.2015] Yushi Wang", "Jonathan Berant", "Percy Liang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Regular expressions are built into many application interfaces, yet most users of these applications have difficulty writing them (Friedl, 2002).", "startOffset": 130, "endOffset": 144}, {"referenceID": 10, "context": "program scripts (Raza et al., 2015).", "startOffset": 16, "endOffset": 35}, {"referenceID": 10, "context": "program scripts (Raza et al., 2015). Prior work has demonstrated the feasibility of this task. Kushman and Barzilay (2013) proposed a model that learns to perform the task from a parallel corpus of regular expressions and the text descrip-", "startOffset": 17, "endOffset": 123}, {"referenceID": 9, "context": "search into this task used rule-based techniques to create a natural language interface to regular expression writing (Ranta, 1998).", "startOffset": 118, "endOffset": 131}, {"referenceID": 9, "context": "search into this task used rule-based techniques to create a natural language interface to regular expression writing (Ranta, 1998). Our work, however, is closest to Kushman and Barzilay (2013). They learned a semantic parsing translation model from a", "startOffset": 119, "endOffset": 194}, {"referenceID": 0, "context": "Neural Machine Translation Recent advances in neural machine translation (NMT) (Bahdanau et al., 2014; Devlin et al., 2014; Luong et al., 2015) using the framework of sequence to sequence learning (Sutskever et al.", "startOffset": 79, "endOffset": 143}, {"referenceID": 2, "context": "Neural Machine Translation Recent advances in neural machine translation (NMT) (Bahdanau et al., 2014; Devlin et al., 2014; Luong et al., 2015) using the framework of sequence to sequence learning (Sutskever et al.", "startOffset": 79, "endOffset": 143}, {"referenceID": 7, "context": "Neural Machine Translation Recent advances in neural machine translation (NMT) (Bahdanau et al., 2014; Devlin et al., 2014; Luong et al., 2015) using the framework of sequence to sequence learning (Sutskever et al.", "startOffset": 79, "endOffset": 143}, {"referenceID": 12, "context": ", 2015) using the framework of sequence to sequence learning (Sutskever et al., 2014) have demonstrated the", "startOffset": 61, "endOffset": 85}, {"referenceID": 8, "context": "We use a Recurrent Neural Network (RNN) with attention (Mnih et al., 2014) for both encoding and decoding (Figure 1).", "startOffset": 55, "endOffset": 74}, {"referenceID": 0, "context": "We use a global attention mechanism (Bahdanau et al., 2014), which considers all hidden states of the encoder when comput-", "startOffset": 36, "endOffset": 59}, {"referenceID": 11, "context": "We perform standard dropout during training (Srivastava et al., 2014) after every LSTM layer with dropout probability equal to 0.", "startOffset": 44, "endOffset": 69}, {"referenceID": 13, "context": "This technique is similar to the methods used by Wang et al. (2015) to create a semantic parsing corpus.", "startOffset": 49, "endOffset": 68}, {"referenceID": 1, "context": "We use a Torch (Collobert et al., 2002) implementation of attention sequence to sequence networks from (Kim, 2016).", "startOffset": 15, "endOffset": 39}, {"referenceID": 5, "context": ", 2002) implementation of attention sequence to sequence networks from (Kim, 2016).", "startOffset": 71, "endOffset": 82}], "year": 2016, "abstractText": "This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus1 of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.", "creator": "LaTeX with hyperref package"}}}