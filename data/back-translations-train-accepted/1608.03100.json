{"id": "1608.03100", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Aug-2016", "title": "Estimation from Indirect Supervision with Linear Moments", "abstract": "In structured prediction problems where we have indirect supervision of the output, maximum marginal likelihood faces two computational obstacles: non-convexity of the objective and intractability of even a single gradient computation. In this paper, we bypass both obstacles for a class of what we call linear indirectly-supervised problems. Our approach is simple: we solve a linear system to estimate sufficient statistics of the model, which we then use to estimate parameters via convex optimization. We analyze the statistical properties of our approach and show empirically that it is effective in two settings: learning with local privacy constraints and learning from low-cost count-based annotations.", "histories": [["v1", "Wed, 10 Aug 2016 09:19:07 GMT  (172kb,D)", "http://arxiv.org/abs/1608.03100v1", "12 pages, 7 figures, extended and updated version of our paper appearing in ICML 2016"]], "COMMENTS": "12 pages, 7 figures, extended and updated version of our paper appearing in ICML 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["aditi raghunathan", "roy frostig", "john duchi", "percy liang"], "accepted": true, "id": "1608.03100"}, "pdf": {"name": "1608.03100.pdf", "metadata": {"source": "META", "title": "Estimation from Indirect Supervision with Linear Moments", "authors": ["Aditi Raghunathan", "Roy Frostig", "John Duchi", "Percy Liang"], "emails": ["ADITIR@STANFORD.EDU", "RF@CS.STANFORD.EDU", "JDUCHI@STANFORD.EDU", "PLIANG@CS.STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "We are interested in indirect surveillance for two reasons: firstly, a data collector could not be trusted and would wish to disclose only partial information about sensitive data (Warner, 1965; Evfimievski et al., Dwork et al., 2006; Duchi et al., 2013); secondly, when the data is generated by human users, it can often be cheaper than it does (Oded, Tomasz, 1998; Duchi et al., 2008; Liang et al., 2009; we trade with statistical efficiency for another resource: privacy."}, {"heading": "2. Setup", "text": "We use superscripts to enumerate instances in a data sample (e.g. x (1),., x (n), and square class indexing to enumerate components of a vector or sequence: x [b] denotes the component (s) of x associated examples. We model this figure using a conditional exponential distribution pattern (y) = exp (x, y) > modeling where the structured prediction task of mapping an input x to any output Y. We model this figure using a conditional exponential family type (y) = exp."}, {"heading": "3. Learning under local privacy", "text": "Suppose we want to estimate a conditional distribution curve (y | x) in which x is insensitive information about a person and contains sensitive information (such as income or disease status). Individuals may want to keep y private for a variety of reasons - mistrust, embarrassment, fear of discrimination - and instead release some o-S (\u00b7 | y). To quantify the level of privacy that S grants, we turn to the literature on privacy in databases and theoretical computer science (Evfimievski et al., 2004; Dwork et al., 2006) and say that S, if there are two y, y, has a comparable probability (up to a factor of Exp (\u03b1), o: sup o, y, y \u2032 S (o | y) S (o | y) S (o | y) \u2264 exp (\u03b1). (4) Which S should we use? First, we examine the classical randomized reaction mechanism (R) (Section 3.1), and then develop a new structure (3.2)."}, {"heading": "3.1. Classic randomized response", "text": "Warner (1965) suggested the today classic randomized response technique, which runs the following way: In a fixed (generally small) answer > 0, the respondent shows y with probability and probability 1 \u2212 draws a sample from a (known) base distribution u - generally uniform - about Y. Formally seen, the classical randomized response surveillance S (o | y) def = = I [o = y] + (1 \u2212) u (o) estimate. (5) Our goal is to construct a function that is satisfactory (3). Let us begin with this purpose with the??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "3.2. Structured randomized response", "text": "With this difficulty in mind, we realise that we must somehow exploit the structure of sufficient statistics."}, {"heading": "4. Learning with lightweight annotations", "text": "This motivates a number of papers that seek to learn from weaker forms of annotation (Mann & McCallum, 2008; Haghighi & Klein, 2006; Liang et al., 2009), focusing on the annotations of the region where an annotator is asked to examine only a certain subsequence of input and count the number of occurrences (e.g. nouns), on the grounds that it is cognitively easier to focus on a label rather than focusing on a large number of terms that are easier to hit than selecting precise locations, especially in mobile crowdsourcing interfaces (Vaish et al, 2014)."}, {"heading": "5. Asymptotic analysis", "text": "We have two estimators: the maximum marginal probability, which is difficult to calculate and requires a non-convex optimization and possibly intractable conclusion. In this section we examine and compare the statistical efficiency of the marginal probability, and we focus on the unconditional setting in which x is empty, and in this section we also allow our exponential family model to be well specified, and these are the true parameters of the marginal probability. All expectations are taken with respect to the y-shaped deviations."}, {"heading": "6. The geometry of two-step estimation", "text": "We can now provide some geometric intuitions on the differences between countries in terms of distribution statistics. (...) We can ask ourselves whether the statistics on distribution statistics actually have a finite structure and the number of distribution statistics on the distribution of all distribution statistics (...) on the distribution of all distribution statistics on the distribution of all distribution statistics (...).See Figure 4 for an example where m = 3 and d = 2. Note that in the space of distribution statistics F is a non-convex set.Let O = {1, k} be the number of observations. We can present the monitoring function S (o | y) as a matrix S-Rk \u00b7 m."}, {"heading": "7. Experiments", "text": "We take linear regression as a simple, structured model: it corresponds to a pair of random fields about the input factors and the answer. Indeed, the sufficient statistics are edge features that result in an accurate estimate. On the other hand, it is a dense vector that stimulates the coordinate system instead. We select a random model that shows the probability 12 that the oversight of the RR scheme is given. On the other hand, it is a dense vector that motivates the coordinate release instead. We select a random model that shows the probability 12 that the oversight of the RR scheme is given. On the other hand, it is a dense vector that motivates the coordinate release. We select the coordinate scheme with which we uncover both input variables x as well as the answer to the question about the answer."}, {"heading": "8. Related work and discussion", "text": "This work was motivated by two cases of indirect surveillance: local privacy and cheap annotations of other direct weights. Each trade away from statistical accuracy for a different resource: privacy or annotation costs. Local privacy has its roots in classical randomized responses to conducting surveys (Warner, 1965), but this is extended to the multicultural (Tamhane, 1981) and conditional (Matloff, 1984) ways. In the computer science community, differentiated privacy has proven to be a useful formalization of privacy (Dwork, 2006). We are working with the stronger notion of local differential privacy (Evfimievski et al., 2004; Kasiviswanathan et al., 2011; Duchi et al.) Our contribution here is twofold: We are incorporating local privacy into the graphic model, which provides an opportunity for privacy to be sensitive to the model structure. While we believe that open mechanisms are structured in the case, our question is rational in the structured one."}, {"heading": "A. Details of privacy schemes", "text": "To recognize this, we must first of all note that it is sufficient to have a differentiated privacy of observations o with respect to all possible (possibly random) data o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to privacy o with respect to"}], "references": [{"title": "Two SVDs suffice: Spectral decompositions for probabilistic topic modeling and latent Dirichlet allocation", "author": ["A. Anandkumar", "D.P. Foster", "D. Hsu", "S.M. Kakade", "Y. Liu"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": null, "citeRegEx": "Anandkumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2013}, {"title": "Visual tracking with online multiple instance learning", "author": ["B. Babenko", "M. Yang", "S. Belongie"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Babenko et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Babenko et al\\.", "year": 2009}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "S.A.D. Pietra", "V.J.D. Pietra", "R.L. Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Spectral experts for estimating mixtures of linear regressions", "author": ["A. Chaganty", "P. Liang"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Chaganty and Liang,? \\Q2013\\E", "shortCiteRegEx": "Chaganty and Liang", "year": 2013}, {"title": "Estimating latent-variable graphical models using moments and likelihoods", "author": ["A. Chaganty", "P. Liang"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Chaganty and Liang,? \\Q2014\\E", "shortCiteRegEx": "Chaganty and Liang", "year": 2014}, {"title": "Guiding semisupervision with constraint-driven learning", "author": ["M. Chang", "L. Ratinov", "D. Roth"], "venue": "In Association for Computational Linguistics (ACL), pp", "citeRegEx": "Chang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2007}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["Dempster A. P", "M.L. N", "D. R"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "P. et al\\.,? \\Q1977\\E", "shortCiteRegEx": "P. et al\\.", "year": 1977}, {"title": "Local privacy and statistical minimax rates", "author": ["J.C. Duchi", "M.I. Jordan", "M.J. Wainwright"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Duchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2013}, {"title": "Differential privacy", "author": ["C. Dwork"], "venue": "In Automata, languages and programming,", "citeRegEx": "Dwork,? \\Q2006\\E", "shortCiteRegEx": "Dwork", "year": 2006}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "In Proceedings of the 3rd Theory of Cryptography Conference,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Privacy preserving mining of association rules", "author": ["A. Evfimievski", "R. Srikant", "R. Agrawal", "J. Gehrke"], "venue": "Information Systems,", "citeRegEx": "Evfimievski et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Evfimievski et al\\.", "year": 2004}, {"title": "Expectation maximization and posterior constraints", "author": ["J. Gra\u00e7a", "K. Ganchev", "B. Taskar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gra\u00e7a et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gra\u00e7a et al\\.", "year": 2008}, {"title": "Prototype-driven learning for sequence models", "author": ["A. Haghighi", "D. Klein"], "venue": "In North American Association for Computational Linguistics (NAACL),", "citeRegEx": "Haghighi and Klein,? \\Q2006\\E", "shortCiteRegEx": "Haghighi and Klein", "year": 2006}, {"title": "Unsupervised learning of noisyor Bayesian networks", "author": ["Y. Halpern", "D. Sontag"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Halpern and Sontag,? \\Q2013\\E", "shortCiteRegEx": "Halpern and Sontag", "year": 2013}, {"title": "Learning mixtures of spherical Gaussians: Moment methods and spectral decompositions", "author": ["D. Hsu", "S.M. Kakade"], "venue": "In Innovations in Theoretical Computer Science (ITCS),", "citeRegEx": "Hsu and Kakade,? \\Q2013\\E", "shortCiteRegEx": "Hsu and Kakade", "year": 2013}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Identifiability and unmixing of latent parse trees", "author": ["D. Hsu", "S.M. Kakade", "P. Liang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "What can we learn privately", "author": ["S.P. Kasiviswanathan", "H.K. Lee", "K. Nissim", "S. Raskhodnikova", "A. Smith"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Kasiviswanathan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kasiviswanathan et al\\.", "year": 2011}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Learning from measurements in exponential families", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Liang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In Association for Computational Linguistics (ACL), pp", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Inference for imputation", "author": ["R.J. M", "W. Naisyin"], "venue": "estimators. Biometrika,", "citeRegEx": "M and Naisyin,? \\Q2000\\E", "shortCiteRegEx": "M and Naisyin", "year": 2000}, {"title": "Generalized expectation criteria for semi-supervised learning of conditional random fields. In Human Language Technology and Association for Computational Linguistics (HLT/ACL)", "author": ["G. Mann", "A. McCallum"], "venue": null, "citeRegEx": "Mann and McCallum,? \\Q2008\\E", "shortCiteRegEx": "Mann and McCallum", "year": 2008}, {"title": "Use of covariates in randomized response settings", "author": ["N.S. Matloff"], "venue": "Statistics & Probability Letters,", "citeRegEx": "Matloff,? \\Q1984\\E", "shortCiteRegEx": "Matloff", "year": 1984}, {"title": "A framework for multipleinstance learning", "author": ["M. Oded", "L. Tom\u00e1s"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Oded and Tom\u00e1s,? \\Q1998\\E", "shortCiteRegEx": "Oded and Tom\u00e1s", "year": 1998}, {"title": "Estimating labels from label proportions", "author": ["N. Quadrianto", "A.J. Smola", "T.S. Caetano", "Q.V. Le"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Quadrianto et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Quadrianto et al\\.", "year": 2008}, {"title": "Conditional random fields for object recognition", "author": ["A. Quattoni", "M. Collins", "T. Darrell"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Quattoni et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2004}, {"title": "Learning with relaxed supervision", "author": ["J. Steinhardt", "P. Liang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Steinhardt and Liang,? \\Q2015\\E", "shortCiteRegEx": "Steinhardt and Liang", "year": 2015}, {"title": "Randomized response techniques for multiple sensitive attributes", "author": ["A.C. Tamhane"], "venue": "Journal of the American Statistical Association (JASA),", "citeRegEx": "Tamhane,? \\Q1981\\E", "shortCiteRegEx": "Tamhane", "year": 1981}, {"title": "Twitch crowdsourcing: crowd contributions in short bursts of time", "author": ["R. Vaish", "K. Wyngarden", "J. Chen", "B. Cheung", "M.S. Bernstein"], "venue": "In Conference on Human Factors in Computing Systems (CHI),", "citeRegEx": "Vaish et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vaish et al\\.", "year": 2014}, {"title": "Asymptotic statistics", "author": ["A.W. van der Vaart"], "venue": null, "citeRegEx": "Vaart,? \\Q1998\\E", "shortCiteRegEx": "Vaart", "year": 1998}, {"title": "Estimating mixture models via mixture of polynomials", "author": ["S. Wang", "A. Chaganty", "P. Liang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Randomized response: A survey technique for eliminating evasive answer bias", "author": ["S.L. Warner"], "venue": "Journal of the American Statistical Association (JASA),", "citeRegEx": "Warner,? \\Q1965\\E", "shortCiteRegEx": "Warner", "year": 1965}], "referenceMentions": [{"referenceID": 33, "context": "We are interested in indirect supervision for two reasons: first, one might not trust a data collector and wish to use privacy mechanisms to reveal only partial information about sensitive data (Warner, 1965; Evfimievski et al., 2004; Dwork et al., 2006; Duchi et al., 2013).", "startOffset": 194, "endOffset": 274}, {"referenceID": 11, "context": "We are interested in indirect supervision for two reasons: first, one might not trust a data collector and wish to use privacy mechanisms to reveal only partial information about sensitive data (Warner, 1965; Evfimievski et al., 2004; Dwork et al., 2006; Duchi et al., 2013).", "startOffset": 194, "endOffset": 274}, {"referenceID": 10, "context": "We are interested in indirect supervision for two reasons: first, one might not trust a data collector and wish to use privacy mechanisms to reveal only partial information about sensitive data (Warner, 1965; Evfimievski et al., 2004; Dwork et al., 2006; Duchi et al., 2013).", "startOffset": 194, "endOffset": 274}, {"referenceID": 8, "context": "We are interested in indirect supervision for two reasons: first, one might not trust a data collector and wish to use privacy mechanisms to reveal only partial information about sensitive data (Warner, 1965; Evfimievski et al., 2004; Dwork et al., 2006; Duchi et al., 2013).", "startOffset": 194, "endOffset": 274}, {"referenceID": 26, "context": "Second, if data is generated by human annotators, say in a crowdsourcing setting, it can often be more cost-effective to solicit lightweight annotations (Oded & Tom\u00e1s, 1998; Mann & McCallum, 2008; Quadrianto et al., 2008; Liang et al., 2009).", "startOffset": 153, "endOffset": 241}, {"referenceID": 20, "context": "Second, if data is generated by human annotators, say in a crowdsourcing setting, it can often be more cost-effective to solicit lightweight annotations (Oded & Tom\u00e1s, 1998; Mann & McCallum, 2008; Quadrianto et al., 2008; Liang et al., 2009).", "startOffset": 153, "endOffset": 241}, {"referenceID": 6, "context": "Second, even the computation of the gradient or performing the E-step can be intractable, requiring probabilistic inference on a loopy graphical model induced by the indirect supervision (Chang et al., 2007; Gra\u00e7a et al., 2008; Liang et al., 2009).", "startOffset": 187, "endOffset": 247}, {"referenceID": 12, "context": "Second, even the computation of the gradient or performing the E-step can be intractable, requiring probabilistic inference on a loopy graphical model induced by the indirect supervision (Chang et al., 2007; Gra\u00e7a et al., 2008; Liang et al., 2009).", "startOffset": 187, "endOffset": 247}, {"referenceID": 20, "context": "Second, even the computation of the gradient or performing the E-step can be intractable, requiring probabilistic inference on a loopy graphical model induced by the indirect supervision (Chang et al., 2007; Gra\u00e7a et al., 2008; Liang et al., 2009).", "startOffset": 187, "endOffset": 247}, {"referenceID": 16, "context": "We lean on the method of moments (Pearson, 1894), which has recently led to advances in learning latent-variable models (Hsu et al., 2009; Anandkumar et al., 2013; Chaganty & Liang, 2014), although we do not appeal to tensor factorization.", "startOffset": 120, "endOffset": 187}, {"referenceID": 1, "context": "We lean on the method of moments (Pearson, 1894), which has recently led to advances in learning latent-variable models (Hsu et al., 2009; Anandkumar et al., 2013; Chaganty & Liang, 2014), although we do not appeal to tensor factorization.", "startOffset": 120, "endOffset": 187}, {"referenceID": 19, "context": "For concreteness, we specialize to conditional random fields (CRFs) (Lafferty et al., 2001) over collections of K-variate labels, where x = (x[1], .", "startOffset": 68, "endOffset": 91}, {"referenceID": 12, "context": "This motivates a number of relaxations (Gra\u00e7a et al., 2008; Liang et al., 2009; Steinhardt & Liang, 2015), but there are no guarantees on approximation quality.", "startOffset": 39, "endOffset": 105}, {"referenceID": 20, "context": "This motivates a number of relaxations (Gra\u00e7a et al., 2008; Liang et al., 2009; Steinhardt & Liang, 2015), but there are no guarantees on approximation quality.", "startOffset": 39, "endOffset": 105}, {"referenceID": 11, "context": "To quantify the amount of privacy afforded by S, we turn to the literature on privacy in databases and theoretical computer science (Evfimievski et al., 2004; Dwork et al., 2006) and say that S is \u03b1-differentially private if any two y, y\u2032 have comparable probability (up to a factor of exp(\u03b1)) of generating o:", "startOffset": 132, "endOffset": 178}, {"referenceID": 10, "context": "To quantify the amount of privacy afforded by S, we turn to the literature on privacy in databases and theoretical computer science (Evfimievski et al., 2004; Dwork et al., 2006) and say that S is \u03b1-differentially private if any two y, y\u2032 have comparable probability (up to a factor of exp(\u03b1)) of generating o:", "startOffset": 132, "endOffset": 178}, {"referenceID": 20, "context": "This motivates a line of work which attempts to learn from weaker forms of annotation (Mann & McCallum, 2008; Haghighi & Klein, 2006; Liang et al., 2009).", "startOffset": 86, "endOffset": 153}, {"referenceID": 30, "context": "The rationale is that it is cognitively easier for the annotator to focus on one label at a time rather than annotating from a large tag set, and physically easier to hit a single yes/no or counter button than to select precise locations, especially in mobile-based crowdsourcing interfaces (Vaish et al., 2014).", "startOffset": 291, "endOffset": 311}, {"referenceID": 3, "context": "These are admittedly strong independence assumptions similar to IBM model 1 for word alignment (Brown et al., 1993) or the unordered translation model of Steinhardt & Liang (2015).", "startOffset": 95, "endOffset": 115}, {"referenceID": 3, "context": "These are admittedly strong independence assumptions similar to IBM model 1 for word alignment (Brown et al., 1993) or the unordered translation model of Steinhardt & Liang (2015). Even though our model is fully factorized and lacks edge potentials, inference p\u03b8(y | x, o) is expensive as conditioning on the indirect supervision o couples all of the y variables.", "startOffset": 96, "endOffset": 180}, {"referenceID": 3, "context": "These are admittedly strong independence assumptions similar to IBM model 1 for word alignment (Brown et al., 1993) or the unordered translation model of Steinhardt & Liang (2015). Even though our model is fully factorized and lacks edge potentials, inference p\u03b8(y | x, o) is expensive as conditioning on the indirect supervision o couples all of the y variables. This typically calls for approximate inference techniques common to the realm of structured prediction. Steinhardt & Liang (2015) developed a relaxation to cope with this supervision, but this still requires approximate inference via sampling and non-convex optimization.", "startOffset": 96, "endOffset": 494}, {"referenceID": 31, "context": "To compute \u03a3marg, we follow standard arguments in van der Vaart (1998). If `(o; \u03b8) is the marginal log-likelihood, then a straightforward calculation yields \u2207`(o, \u03b8\u2217) = E[\u03c6(y) | o] \u2212 E[\u03c6(y)].", "startOffset": 58, "endOffset": 71}, {"referenceID": 33, "context": "Local privacy has its roots in classical randomized response for conducting surveys (Warner, 1965), which has been extended to the multivariate (Tamhane, 1981) and conditional (Matloff, 1984) settings.", "startOffset": 84, "endOffset": 98}, {"referenceID": 29, "context": "Local privacy has its roots in classical randomized response for conducting surveys (Warner, 1965), which has been extended to the multivariate (Tamhane, 1981) and conditional (Matloff, 1984) settings.", "startOffset": 144, "endOffset": 159}, {"referenceID": 24, "context": "Local privacy has its roots in classical randomized response for conducting surveys (Warner, 1965), which has been extended to the multivariate (Tamhane, 1981) and conditional (Matloff, 1984) settings.", "startOffset": 176, "endOffset": 191}, {"referenceID": 9, "context": "In the computer science community, differential privacy has emerged as a useful formalization of privacy (Dwork, 2006).", "startOffset": 105, "endOffset": 118}, {"referenceID": 11, "context": "We work with the stronger notion of local differential privacy (Evfimievski et al., 2004; Kasiviswanathan et al., 2011; Duchi et al., 2013).", "startOffset": 63, "endOffset": 139}, {"referenceID": 18, "context": "We work with the stronger notion of local differential privacy (Evfimievski et al., 2004; Kasiviswanathan et al., 2011; Duchi et al., 2013).", "startOffset": 63, "endOffset": 139}, {"referenceID": 8, "context": "We work with the stronger notion of local differential privacy (Evfimievski et al., 2004; Kasiviswanathan et al., 2011; Duchi et al., 2013).", "startOffset": 63, "endOffset": 139}, {"referenceID": 2, "context": "Multiinstance learning (Oded & Tom\u00e1s, 1998) is popular in computer vision, where it is natural to label the presence but not location of objects (Babenko et al., 2009).", "startOffset": 145, "endOffset": 167}, {"referenceID": 20, "context": "In natural language processing, there also been work on learning from structured outputs where, like this work, only counts of labels are observed (Mann & McCallum, 2008; Liang et al., 2009).", "startOffset": 147, "endOffset": 190}, {"referenceID": 2, "context": "Multiinstance learning (Oded & Tom\u00e1s, 1998) is popular in computer vision, where it is natural to label the presence but not location of objects (Babenko et al., 2009). In natural language processing, there also been work on learning from structured outputs where, like this work, only counts of labels are observed (Mann & McCallum, 2008; Liang et al., 2009). However, these works resort to likelihood-based approaches which involve non-convex optimization and approximate inference, whereas in this work, we show that linear algebra and convex optimization suffice under modeling assumptions. Quadrianto et al. (2008) showed how to learn from label proportions of groups of examples, using a linear system technique similar to ours.", "startOffset": 146, "endOffset": 620}, {"referenceID": 3, "context": "Indirect supervision arises more generally in latent-variable models, which arises in machine translation (Brown et al., 1993), semantic parsing (Liang et al.", "startOffset": 106, "endOffset": 126}, {"referenceID": 21, "context": ", 1993), semantic parsing (Liang et al., 2011), object detection (Quattoni et al.", "startOffset": 26, "endOffset": 46}, {"referenceID": 27, "context": ", 2011), object detection (Quattoni et al., 2004), and other missing data problems in statistics (M & Naisyin, 2000).", "startOffset": 26, "endOffset": 49}, {"referenceID": 17, "context": "In recent years, tensor factorization techniques have provided efficient methods for a wide class of latent-variable models (Hsu et al., 2012; Anandkumar et al., 2012; Hsu & Kakade, 2013; Anandkumar et al., 2013; Chaganty & Liang, 2013; Halpern & Sontag, 2013; Chaganty & Liang, 2014).", "startOffset": 124, "endOffset": 284}, {"referenceID": 0, "context": "In recent years, tensor factorization techniques have provided efficient methods for a wide class of latent-variable models (Hsu et al., 2012; Anandkumar et al., 2012; Hsu & Kakade, 2013; Anandkumar et al., 2013; Chaganty & Liang, 2013; Halpern & Sontag, 2013; Chaganty & Liang, 2014).", "startOffset": 124, "endOffset": 284}, {"referenceID": 1, "context": "In recent years, tensor factorization techniques have provided efficient methods for a wide class of latent-variable models (Hsu et al., 2012; Anandkumar et al., 2012; Hsu & Kakade, 2013; Anandkumar et al., 2013; Chaganty & Liang, 2013; Halpern & Sontag, 2013; Chaganty & Liang, 2014).", "startOffset": 124, "endOffset": 284}, {"referenceID": 32, "context": "One can leverage even more general polynomialsolving techniques to expand the set of models (Wang et al., 2015).", "startOffset": 92, "endOffset": 111}], "year": 2016, "abstractText": "In structured prediction problems where we have indirect supervision of the output, maximum marginal likelihood faces two computational obstacles: non-convexity of the objective and intractability of even a single gradient computation. In this paper, we bypass both obstacles for a class of what we call linear indirectly-supervised problems. Our approach is simple: we solve a linear system to estimate sufficient statistics of the model, which we then use to estimate parameters via convex optimization. We analyze the statistical properties of our approach and show empirically that it is effective in two settings: learning with local privacy constraints and learning from low-cost count-based annotations.1", "creator": "LaTeX with hyperref package"}}}