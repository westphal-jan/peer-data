{"id": "1608.03542", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Aug-2016", "title": "WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia", "abstract": "We present WikiReading, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNN-based architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%.", "histories": [["v1", "Thu, 11 Aug 2016 17:34:12 GMT  (103kb,D)", "http://arxiv.org/abs/1608.03542v1", "ACL 2016"], ["v2", "Wed, 15 Mar 2017 19:58:44 GMT  (103kb,D)", "http://arxiv.org/abs/1608.03542v2", null]], "COMMENTS": "ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniel hewlett", "alexandre lacoste", "llion jones", "illia polosukhin", "andrew fandrianto", "jay han", "matthew kelcey", "david berthelot"], "accepted": true, "id": "1608.03542"}, "pdf": {"name": "1608.03542.pdf", "metadata": {"source": "CRF", "title": "WIKIREADING: A Novel Large-scale Language Understanding Task over Wikipedia", "authors": ["Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot"], "emails": ["dhewlett@google.com", "allac@google.com", "llion@google.com", "ipolosukhin@google.com", "fto@google.com", "hanjay@google.com", "matkelcey@google.com", "dberth@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that one sees oneself in a position to solve the problems mentioned by getting a grip on them. (...) In fact, it is the case that one sees oneself in a position to get a grip on them. (...) It is as if one were able to solve them. (...) It is as if one were able to put them in a position to put them in a position. (...) It is as if one put them in a position to put them in a position to put them in a place. (...) It is as if one puts them in the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place of the place."}, {"heading": "2 WIKIREADING", "text": "We now provide background information on Wikidata, followed by a detailed description of the WIKIREADING prediction task and the dataset."}, {"heading": "2.1 Wikidata", "text": "Wikidata is a free collaborative knowledge base containing information on approximately 16 million articles (Vrandec \u0441ic \"and Kro\" tzsch, 2014). Knowledge on each article is expressed in a series of statements, each consisting of a (property, value) tuple. For example, the article could contain statements related to Paris (instance, city) or (country, France). Wikidata contains over 80 million such statements on 884 objects. Articles can be linked to articles in Wikipedia."}, {"heading": "2.2 Dataset", "text": "We constructed the WIKIREADING dataset from Wikidata and Wikipedia as follows: We consolidated all Wikidata statements with the same item and property into a single (item, property, response) triple dataset, with the answer being a set of values. Replacing each item with the text of the linked Wikipedia article (discarding unlinked items) results in a record of 18.58M (document, property, response) instances. Importantly, all elements in each instance are human-readable strings, making the task completely textual. The only change we have made to these strings was the conversion of timestamps into a human-readable format (e.g., \"July 4, 1776\").The WIKIREADING task is then to predict the response string for each tuple given to the document and object strings. This constellation can be viewed as a question similar to the information sextraction, or the property. \""}, {"heading": "2.3 Documents", "text": "The record contains 4.7 million unique Wikipedia articles, which means that about 80% of the English-language Wikipedia is represented. Multiple instances can share the same document, with an average of 5.31 instances per article (median: 4, max: 879). The most common categories of documents are human, taxon, film, album and human settlement, which comprise 48.8% of documents and 9.1% of instances."}, {"heading": "2.4 Properties", "text": "The data set contains 884 unique properties, although the distribution of properties across instances is highly distorted: the top 20 properties cover 75% of the data set, achieving a coverage of 99% by 180 properties. We roughly divide the properties into two groups: categorical properties such as instance, gender and country require the choice between a relatively small number of possible responses, while relational properties such as date of birth, parent and capital typically need to extract rare or completely unique responses from the document, i.e. to quantify this difference, we calculate the entropy of the response distribution A for each property p, scaled to the [0, 1] range, by dividing it by the entropy of a uniform distribution with the same number of values, i.e. H (p) = H (Ap) / log | Ap |. Properties that essentially represent one-to-one mappings achieve almost 1.0, while a property with a single response would achieve 0.0 points less than a sub-spectrum containing a table of 0.2 for a sub-spectrum."}, {"heading": "2.5 Answers", "text": "The distribution of the properties described above affects the response distribution. There is a relatively small number of responses with very high frequency, mainly categorical properties, and a large number of responses with very low frequency, such as names and data. In extreme cases, almost 7% of the data set accounts for the most common human response, while 54.7% of the answers in the data set are unique. There are some specific categories of answers that are systematically linked, particularly data that make up 8.9% of the data set (with 7.2% being unique). This distribution means that methods that focus on head or tail responses can each perform moderately well, but only one method that handles both types of responses can achieve maximum performance. Another consequence of the long tail of responses is that many (30.0%) of the answers in the test set never appear in the training set, meaning that they must be read from the document."}, {"heading": "3 Methods", "text": "Recently, it has been shown that neural network architectures for NLU meet or exceed the performance of traditional methods (Zhang et al., 2015; Dai et al., 2015).The move towards deep neural networks also allows for new ways to combine property and document, inspired by recent research in the field of question (with property serving as a question).In sequential models such as Recurrent Neural Networks (RNNs), the question could be prefixed to the document, whereby the model could \"read\" the document differently for each question (Hermann et al., 2015).Alternatively, the question could be used to calculate a form of attention through the document (Bahdanau et al., 2014) to effectively focus the model on the most predictive words or phrases (Sukhbaatar et al., 2015; Hermann et al., 2015).Since this is currently an ongoing field of research, we have included a number of current models in general models, and now we are applying them to fictitious categories for the first time."}, {"heading": "3.1 Answer Classification", "text": "To put WIKIREADING in this context, we consider each possible answer as a class name and integrate property-based characteristics so that the model can make different predictions for the same document. Although the number of possible responses is too large to be workable (and in principle unlimited), a significant part of the data set can be covered by a model with a comprehensible number of responses."}, {"heading": "3.1.1 Baseline", "text": "The most common approach to classifying documents is to adapt to a linear model (e.g. logistic regression) using word pods (BoW) characteristics. To serve as a basis for our task, the linear model must make different predictions depending on the property for the same Wikipedia article. We enable this behavior by calculating two Nw element BoW vectors, one for the documentation and the property, and combining them into a single 2Nw feature vector."}, {"heading": "3.1.2 Neural Network Methods", "text": "In this section, the property and the document are included in a common representation that serves as input for a final softmax layer that has a probability distribution across the top levels of Nans. (1) These models differ primarily in the way they combine ownership and the document to produce the common representation. (1) For existing models from the literature, we provide a brief description and note of all important differences in our implementation, but refer the reader to the original documents for more details.Except for character models, documents and properties are converted to words. The Nw most common words are mapped to a vector in Rdin."}, {"heading": "3.2 Answer Extraction", "text": "Relational properties involve mappings between arbitrary units (e.g. date of birth, mother, and author) and are therefore less suited to documenting a classification, for which approaches from information extraction (especially relationship extraction) are much more appropriate. In general, these methods attempt to identify a word or phrase in the text that relates to a particular (possibly implicit) topic. Section 5 contains a discussion of previous work that applied NLP techniques that include entity detection and syntactic parsing to this problem. RNNs provide a natural fit for extraction as they can predict a value at any position in a sequence, depending on the entire previous order. The simplest application to WIKIREADING is to predict the probability that a word is part of an answer at a given location. We test this approach using an RNN that works on the order of words. At each time step, we use a moid to estimate whether the word is activated or not the current part of the answer."}, {"heading": "3.3 Sequence to Sequence", "text": "These models combine two RNNs: an encoder that turns the input sequence into a vector representation, and a decoder that converts the encoder vector into a sequence of output tokens, one token at a time. Thus, in principle, they are able to approximate any function that maps sequential inputs to sequential outputs. Importantly, they are the first model that we consider to be any combination of response classification and extraction."}, {"heading": "3.3.1 Basic seq2seq", "text": "This model is similar to the LSTM reader, which has been extended by a second RNN to decode the answer as a word sequence. The embedding matrix is split between the two RNNs, but their state transition matrices differ (Figure 1b). This method expands the set of possible answers to arbitrary strings of words from the document vocabulary."}, {"heading": "3.3.2 Placeholder seq2seq", "text": "As mentioned in Section 3.2, RNN Labeler can extract any word sequence present in the document, even if some of them are OOV. We expand the basic seq2seq model to process OOV words by adding placeholders to our vocabulary, increasing the vocabulary size from Nw to Nw + Ndoc. Then, when an OOV word occurs in the document, it is randomly (without substitution) replaced by one of these placeholders. We also replace the corresponding OOV words in the target output - 4Dates have been semantically matched to increase memory. 5We chose an arbitrary threshold of 0.5 for chunking. The score of each piece is obtained from the harmonious mean of the predicted sequences."}, {"heading": "3.3.3 Basic Character seq2seq", "text": "Another way to deal with rare words is to process the input and output text as sequences of characters or bytes. RNNs have shown that working with character input is promising, including state-of-the-art performance at a Wikipedia text classification scale (Dai and Le, 2015). A model that prints answers character by character can in principle produce any of the answers in the test set, a big advantage for WIKIREADING.This model, shown in Figure 2, works only with sequences of mixed uppercase letters. Finally, the property encoder RNN transforms the property as a string into a vector of fixed length. This property encoding becomes the initial hidden state for the second layer of a two-layer document that reads the document character by character again. Finally, the response encoder RNN uses the final state of the previous RNN to retrieve the characters for the same answer multiple times in this OOV."}, {"heading": "3.3.4 Character seq2seq with Pretraining", "text": "Unfortunately, at the character level, the length of all sequences (documents, properties, and answers) is greatly increased. To address this problem in a classification context, Dai and Le (2015) demonstrated that initializing an LSTM classifier with weights from a language model (LM) improved its accuracy. Inspired by this result, we apply this principle to the seq2seq model with a two-phase training process: in the first phase, we train a LM at the character level on the input character sequences from the WIKIREADING training set (no new data are introduced); in the second phase, the weights from this LM are used to initialize the first layer of the encoder and decoder (purple and green blocks in Figure 2); after initialization, the training proceeds as in the seq2seq model."}, {"heading": "4 Experiments", "text": "An answer is correct if there is an exact match between the predicted answer and the gold answer. However, as described in Section 2.2, some answers are composed of a series of values (e.g., the third example in Table 1.) To handle this, we define the mean F1 value as follows: For each example, we calculate the F1 value (harmonic mean for precision and retrieval) as a measure of the degree of intersection between the predicted response amount and the gold rate for a given instance. The resulting perinence F1 values are then averaged to obtain a single data set value, which allows a method to obtain partial recognition for an instance when it responds with at least one value from the amount of gold. In this essay, 63 we consider only methods of answering with a single value, and most responses in the data set are also composed of a single value, so that this Mean F1 metric is closely related to accuracy, or more precisely, a formula of a single 0.63 Mean answer with a single value."}, {"heading": "4.1 Training Details", "text": "We implemented all models in a single framework based on TensorFlow (Abadi et al., 2015) with common pre-processing and comparable hyperparameters whenever possible. All documents are switched to the first 300 words, with the exception of characters seq2seq, which uses 400 characters. The embedding matrix used to encode words in the document uses din = 300 dimensions for the Nw = 100 000 most common words. Similarly, the answer classification is based on the nans = 50 000 most common answers using a response representation of the size dout = 300.7 The first 10 words of the properties are embedded using the document embedding matrix. Following Cho et al. (2014), RNNNs in seq2seq models use a GRU cell with a hidden state size of 1024. Further details on the parameters are reported in Table 4.Property optimizations."}, {"heading": "4.2 Results and Discussion", "text": "This year, the time has come for most of them not to be able to retaliate."}, {"heading": "5 Related Work", "text": "The goal of automatically extracting structured information from unstructured Wikipedia text was first advanced by Wu and Weld (2007). Since Wikidata did not exist at the time, the authors relied on the structured infoboxes included in some Wikipedia articles to relate Wikipedia content. Wikidata is a cleaner source of data, as the infobox data contains many minor variations in the scheme associated with page formatting. In part, to circumvent this problem, the authors limit their Kylin predictive model to 4 specific infobox classes and only common attributes within each class.A significant portion of the work related to extraction (RE) follows the remote monitoring paradigm (Craven and Cumlia, 1999), where sentences containing both arguments of a knowledge base (KB) are adopted triple to express the triple relationship. Broadly speaking, these models use these remote labels to identify the subject matter, syntactical characteristics in relation to the units of knowledge, and syntactical characteristics in relation to the text."}, {"heading": "6 Conclusion", "text": "We have demonstrated the complexity of the WIKIREADING task and its suitability as a benchmark for the future development of DNN models for understanding natural language. After comparing a variety of models that include classification and extraction, we conclude that end-to-end sequence models are the most promising, and that these models simultaneously learned to classify documents and copy arbitrary strings from them. In light of these findings, we propose some key areas for future research. Finally, our character model has improved significantly after language model pre-training, suggesting that further optimization in training can lead to further advancements. Document length poses a problem for RNN-based models that could be addressed with more easily parallelisable Convolutionary Neural Networks. Finally, we note that these models are not fundamentally limited to English, as they rely on little or no pre-processing with traditional NADLP systems, which effectively means that other languages should be rendered more effective by IING."}, {"heading": "Acknowledgments", "text": "We thank Jonathan Berant for many helpful comments on early drafts of the paper and Catherine Finegan-Dollak for introducing RNN Labeler early."}], "references": [{"title": "Tensorflow: Largescale machine learning on heterogeneous systems", "author": ["Abadi et al.2015] Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2015}, {"title": "How Wikipedia works: And how you can be a part of it", "author": ["Ayers et al.2008] Phoebe Ayers", "Charles Matthews", "Ben Yates"], "venue": null, "citeRegEx": "Ayers et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ayers et al\\.", "year": 2008}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD International", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning phrase representations using rnn encoder\u2013decoder", "author": ["Bart Van Merri\u00ebnboer", "\u00c7alar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Craven", "Kumlien1999] Mark Craven", "Johan Kumlien"], "venue": "In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology,", "citeRegEx": "Craven et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Craven et al\\.", "year": 1999}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Le2015] Andrew M Dai", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent convolutional neural networks for discourse compositionality", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the CVSC Workshop,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "A method for stochastic optimization", "author": ["Kingma", "Adam2015] Diederik P Kingma", "Jimmy Ba Adam"], "venue": "In International Conference on Learning Representation", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Neural randomaccess machines", "author": ["Kurach et al.2015] Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "Kurach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc V. Le", "Tomas Mikolov"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Relation extraction: Perspective from convolutional neural networks", "author": ["Nguyen", "Grishman2015] Thien Huu Nguyen", "Ralph Grishman"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Injecting Logical Background Knowledge into Embeddings for Relation Extraction", "author": ["Sameer Singh", "Sebastian Riedel"], "venue": "In Annual Conference of the North American Chapter of the Association", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Wikidata: A free collaborative knowledgebase", "author": ["Vrande\u010di\u0107", "Kr\u00f6tzsch2014] Denny Vrande\u010di\u0107", "Markus Kr\u00f6tzsch"], "venue": "Commun. ACM,", "citeRegEx": "Vrande\u010di\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vrande\u010di\u0107 et al\\.", "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy", "author": ["Weston et al.2015] Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Autonomously semantifying wikipedia", "author": ["Wu", "Weld2007] Fei Wu", "Daniel S Weld"], "venue": "In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,", "citeRegEx": "Wu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2007}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yang et al.2015] Yi Yang", "Wen-tau Yih", "Christopher Meek"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "A growing amount of research in natural language understanding (NLU) explores end-to-end deep neural network (DNN) architectures for tasks such as text classification (Zhang et al., 2015), relation extraction (Nguyen and Grishman, 2015), and question answering (Weston et al.", "startOffset": 167, "endOffset": 187}, {"referenceID": 23, "context": ", 2015), relation extraction (Nguyen and Grishman, 2015), and question answering (Weston et al., 2015).", "startOffset": 81, "endOffset": 102}, {"referenceID": 1, "context": "The task, which we call WIKIREADING, is to predict textual values from the open knowledge base Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) given text from the corresponding articles on Wikipedia (Ayers et al., 2008).", "startOffset": 191, "endOffset": 211}, {"referenceID": 25, "context": "Many natural language datasets for question answering (QA), such as WIKIQA (Yang et al., 2015), have only thousands of examples and are thus too small for train-", "startOffset": 75, "endOffset": 94}, {"referenceID": 23, "context": "The bAbI dataset (Weston et al., 2015) requires multiple forms of reasoning, but is composed of synthetically generated documents.", "startOffset": 17, "endOffset": 38}, {"referenceID": 8, "context": "Hermann et al. (2015) proposed a task similar to QA, predicting entities in news summaries from the text of the original news articles, and generated a NEWS dataset with 1M instances.", "startOffset": 0, "endOffset": 22}, {"referenceID": 26, "context": "Recently, neural network architectures for NLU have been shown to meet or exceed the performance of traditional methods (Zhang et al., 2015; Dai and Le, 2015).", "startOffset": 120, "endOffset": 158}, {"referenceID": 2, "context": "Alternatively, the question could be used to compute a form of attention (Bahdanau et al., 2014) over the document, to effectively focus the model on the most predictive words or phrases (Sukhbaatar et al.", "startOffset": 73, "endOffset": 96}, {"referenceID": 20, "context": ", 2014) over the document, to effectively focus the model on the most predictive words or phrases (Sukhbaatar et al., 2015; Hermann et al., 2015).", "startOffset": 98, "endOffset": 145}, {"referenceID": 8, "context": ", 2014) over the document, to effectively focus the model on the most predictive words or phrases (Sukhbaatar et al., 2015; Hermann et al., 2015).", "startOffset": 98, "endOffset": 145}, {"referenceID": 8, "context": "LSTM Reader: This model is a simplified version of the Deep LSTM Reader proposed by Hermann et al. (2015). In this model, an LSTM (Hochreiter and Schmidhuber, 1997) reads the property and document sequences word-by-word", "startOffset": 84, "endOffset": 106}, {"referenceID": 15, "context": "Limited experimentation with initialization from publicly-available word2vec embeddings (Mikolov et al., 2013) yielded no improvement in performance.", "startOffset": 88, "endOffset": 110}, {"referenceID": 8, "context": "Attentive Reader: This model, also presented in Hermann et al. (2015), uses an attention mechanism to better focus on the relevant part of the document for a given property.", "startOffset": 48, "endOffset": 70}, {"referenceID": 20, "context": "Memory Network: Our implementation closely follows the End-to-End Memory Network proposed in Sukhbaatar et al. (2015). This model", "startOffset": 93, "endOffset": 118}, {"referenceID": 19, "context": "Our final results use the position encoding method proposed by Sukhbaatar et al. (2015), which incorporates positional information in addition to word embeddings.", "startOffset": 63, "endOffset": 88}, {"referenceID": 19, "context": "Our final results use the position encoding method proposed by Sukhbaatar et al. (2015), which incorporates positional information in addition to word embeddings. Instead of the linearization method of Sukhbaatar et al. (2015), we applied an entropy regularizer for the softmax attention as described in Kurach et al.", "startOffset": 63, "endOffset": 227}, {"referenceID": 12, "context": "(2015), we applied an entropy regularizer for the softmax attention as described in Kurach et al. (2015).", "startOffset": 84, "endOffset": 105}, {"referenceID": 16, "context": "For training, we label all locations where any answer appears in the document with a 1, and other positions with a 0 (similar to distant supervision (Mintz et al., 2009)).", "startOffset": 149, "endOffset": 169}, {"referenceID": 14, "context": "Luong et al. (2015) developed a similar procedure for dealing with rare words in machine translation, copying their locations into the output sequence for further processing.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "This approach is comparable to entity anonymization used in Hermann et al. (2015), which replaces named entities with random ids, but simpler because we use word-level placeholders without en-", "startOffset": 60, "endOffset": 82}, {"referenceID": 0, "context": "We implemented all models in a single framework based on TensorFlow (Abadi et al., 2015) with shared pre-processing and comparable hyperparameters whenever possible.", "startOffset": 68, "endOffset": 88}, {"referenceID": 7, "context": "Gradient clipping 9 (Graves, 2013) is used to prevent instability in training RNNs.", "startOffset": 20, "endOffset": 34}, {"referenceID": 3, "context": "(2009) apply distant supervision to extracting Freebase triples (Bollacker et al., 2008) from Wikipedia text, analogous to the relational part of WIKIREADING.", "startOffset": 64, "endOffset": 88}, {"referenceID": 18, "context": "Extensions to distant supervision include explicitly modelling whether the relation is actually expressed in the sentence (Riedel et al., 2010), and jointly reasoning over larger sets of sentences and relations (Surdeanu et al.", "startOffset": 122, "endOffset": 143}, {"referenceID": 21, "context": ", 2010), and jointly reasoning over larger sets of sentences and relations (Surdeanu et al., 2012).", "startOffset": 75, "endOffset": 98}, {"referenceID": 15, "context": "Mintz et al. (2009) apply distant supervision to extracting Freebase triples (Bollacker et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "(2009) apply distant supervision to extracting Freebase triples (Bollacker et al., 2008) from Wikipedia text, analogous to the relational part of WIKIREADING. Extensions to distant supervision include explicitly modelling whether the relation is actually expressed in the sentence (Riedel et al., 2010), and jointly reasoning over larger sets of sentences and relations (Surdeanu et al., 2012). Recently, Rockt\u00e4schel et al. (2015) developed methods for reducing the num-", "startOffset": 65, "endOffset": 431}], "year": 2017, "abstractText": "We present WIKIREADING, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNNbased architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%.", "creator": "LaTeX with hyperref package"}}}